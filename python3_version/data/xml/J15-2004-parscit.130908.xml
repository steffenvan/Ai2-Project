<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997453">
A Statistical Parsing Framework for
Sentiment Classification
</title>
<author confidence="0.99878">
Li Dong*, **
</author>
<affiliation confidence="0.965538">
Beihang University
</affiliation>
<author confidence="0.972424">
Furu Wei†, ‡
</author>
<affiliation confidence="0.864595">
Microsoft Research
</affiliation>
<author confidence="0.933449">
Shujie Liu†
</author>
<affiliation confidence="0.890442">
Microsoft Research
</affiliation>
<author confidence="0.963079">
Ming Zhou†
</author>
<affiliation confidence="0.902214">
Microsoft Research
</affiliation>
<author confidence="0.839511">
Ke Xu*
</author>
<affiliation confidence="0.646884">
Beihang University
</affiliation>
<bodyText confidence="0.8015016">
We present a statistical parsing framework for sentence-level sentiment classification in this
article. Unlike previous works that use syntactic parsing results for sentiment analysis, we
develop a statistical parser to directly analyze the sentiment structure of a sentence. We show that
complicated phenomena in sentiment analysis (e.g., negation, intensification, and contrast) can
be handled the same way as simple and straightforward sentiment expressions in a unified and
probabilistic way. We formulate the sentiment grammar upon Context-Free Grammars (CFGs),
and provide a formal description of the sentiment parsing framework. We develop the parsing
model to obtain possible sentiment parse trees for a sentence, from which the polarity model
is proposed to derive the sentiment strength and polarity, and the ranking model is dedicated
to selecting the best sentiment tree. We train the parser directly from examples of sentences
annotated only with sentiment polarity labels but without any syntactic annotations or polarity
annotations of constituents within sentences. Therefore we can obtain training data easily. In
particular, we train a sentiment parser, s.parser, from a large amount of review sentences with
users’ ratings as rough sentiment polarity labels. Extensive experiments on existing benchmark
data sets show significant improvements over baseline sentiment classification approaches.
* State Key Laboratory of Software Development Environment, Beihang University, XueYuan Road No.37,
HaiDian District, Beijing, P.R. China 100191. E-mail: donglixp®gmail.com; kexu®nlsde.buaa.edu.cn.
** Contribution during internship at Microsoft Research.
† Natural Language Computing Group, Microsoft Research Asia, Building 2, No. 5 Danling Street, Haidian
District, Beijing, P.R. China 100080. E-mail: {fuwei, shujliu, mingzhou}@microsoft.com.
</bodyText>
<note confidence="0.981482333333333">
‡ Corresponding author.
Submission received: 10 December 2013; revised version received: 26 July 2014; accepted for publication:
28 January 2015.
doi:10.1162/COLI a 00221
© 2015 Association for Computational Linguistics
Computational Linguistics Volume 41, Number 2
</note>
<sectionHeader confidence="0.988841" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9991659">
Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from
both research and industry communities in recent years. Sentiment classification, which
identifies sentiment polarity (positive or negative) from text (sentence or document),
has been the most extensively studied task in sentiment analysis. Until now, there
have been two mainstream approaches for sentiment classification. The lexicon-based
approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity
of a sentence from the polarity of words or phrases found in the sentence, and the
learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity
identification as a special text classification task and focuses on building classifiers
from a set of sentences (or documents) annotated with their corresponding sentiment
polarity.
The lexicon-based sentiment classification approach is simple and interpretable,
but suffers from scalability and is inevitably limited by sentiment lexicons that are
commonly created manually by experts. It has been widely recognized that sentiment
expressions are colloquial and evolve over time very frequently. Taking tweets from
Twitter1 and movie reviews on IMDb2 as examples, people use very casual language
as well as informal and new vocabulary to comment on general topics and movies. In
practice, it is not feasible to create and maintain sentiment lexicons to capture sentiment
expressions with high coverage. On the other hand, the learning-based approach relies
on large annotated samples to overcome the vocabulary coverage and deals with varia-
tions of words in sentences. Human ratings in reviews (Maas et al. 2011) and emoticons
in tweets (Davidov, Tsur, and Rappoport 2010; Zhao et al. 2012) are extensively used
to collect a large number of training corpora to train the sentiment classifier. However,
it is usually not easy to design effective features to build the classifier. Among
others, unigrams have been reported as the most effective features (Pang, Lee, and
Vaithyanathan 2002) in sentiment classification.
Handling complicated expressions delivering people’s opinions is one of the most
challenging problems in sentiment analysis. Compositionalities such as negation, inten-
sification, contrast, and their combinations are typical cases. We show some concrete
examples here:
</bodyText>
<listItem confidence="0.990843666666667">
(1) The movie is not good. [negation]
(2) The movie is very good. [intensification]
(3) The movie is not funny at all. [negation + intensification]
(4) The movie is just so so, but i still like it. [contrast]
(5) The movie is not very good, but i still like it. [negation + intensification +
contrast]
</listItem>
<bodyText confidence="0.999891">
The negation expressions, intensification modifiers, and the contrastive conjunction
can change the polarity (Examples (1), (3), (4), (5)), strength (Examples (2), (3), (5)), or
both (Examples (3), (5)) of the sentiment of the sentences. We do not need any detailed
explanations here as they can be commonly found and easily understood in people’s
</bodyText>
<footnote confidence="0.996621">
1 http://twitter.com.
2 http://www.imdb.com.
</footnote>
<page confidence="0.98868">
266
</page>
<note confidence="0.992248">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.99929452">
daily lives. Existing works to address these issues usually rely on syntactic parsing
results either used as features (Choi and Cardie 2008; Moilanen, Pulman, and Zhang
2010) in learning-based methods or hand-crafted rules (Moilanen and Pulman 2007; Jia,
Yu, and Meng 2009; Klenner, Petrakis, and Fahrni 2009; Liu and Seneff 2009) in lexicon-
based methods. However, even with the difficulty and feasibility of deriving the senti-
ment structure from syntactic parsing results put aside, it is an even more challenging
task to generate stable and reliable parsing results for text that is ungrammatical in
nature and has a high ratio of out-of-vocabulary words. The accuracy of the linguistic
parsers trained on standard data sets (e.g., the Penn Treebank [Marcus, Marcinkiewicz,
and Santorini 1993]) drops dramatically on user-generated-content (reviews, tweets,
etc.), which is actually the prime focus of sentiment analysis algorithms. The error,
unfortunately, will propagate downstream in the process of sentiment analysis methods
building upon parsing results.
We therefore propose directly analyzing the sentiment structure of a sentence.
The nested structure of sentiment expressions can be naturally modeled in a similar
fashion as statistical syntactic parsing, which aims to find the linguistic structure of a
sentence. This idea creates many opportunities for developing sentiment classifiers from
a new perspective. The most challenging problem and barrier in building a statistical
sentiment parser lies in the acquisition of training data. Ideally, we need examples of
sentences annotated with polarity for the whole sentence as well as sentiment tags
for constituents within a sentence, as with the Penn TreeBank for training traditional
linguistic parsers. However, this is not practical as the annotations will be inevitably
time-consuming and require laborious human efforts. Therefore, it is better to learn the
sentiment parser only utilizing examples annotated with the polarity label of the whole
sentence. For example, we can collect a huge number of publicly available reviews and
rating scores on the Web. People may use the movie is gud (“gud” is a popular informal
expression of “good”) to express a positive opinion towards a movie, and not a fan to
express a negative opinion. Also, we can find review sentences such as The movie is
gud, but I am still not a fan to indicate a negative opinion. We can then use these two
fragments and the overall negative opinion of the sentence to deduce sentiment rules
automatically from data. These sentiment fragments and rules can be used to analyze
the sentiment structure for new sentences.
In this article, we propose a statistical parsing framework to directly analyze the
structure of a sentence from the perspective of sentiment analysis. Specifically, we
formulate a Context-Free Grammar (CFG)–based sentiment grammar. We then develop
a statistical parser to derive the sentiment structure of a sentence. We leverage the CYK
algorithm (Cocke 1969; Younger 1967; Kasami 1965) to conduct bottom–up parsing, and
use dynamic programming to accelerate computation. Meanwhile, we propose using
the polarity model to derive sentiment strength and polarity of a sentiment parse tree,
and the ranking model to select the best one from the sentiment parsing results. We train
the parser directly from examples of sentences annotated with sentiment polarity labels
instead of syntactic annotations and polarity annotations of constituents within sen-
tences. Therefore we can obtain training data easily. In particular, we train a sentiment
parser, named s.parser, from a large number of review sentences with users’ ratings
as rough sentiment polarity labels. The statistical parsing–based approach builds a
principled and scalable framework to support the sentiment composition and inference
which cannot be well handled by bag-of-words approaches. We show that complicated
phenomena in sentiment analysis (e.g., negation, intensification, and contrast) can be
handled the same way as simple and straightforward sentiment expressions in a unified
and probabilistic way.
</bodyText>
<page confidence="0.990056">
267
</page>
<note confidence="0.863139">
Computational Linguistics Volume 41, Number 2
</note>
<bodyText confidence="0.599016">
The major contributions of the work presented in this article are as follows.
</bodyText>
<listItem confidence="0.972027909090909">
• We propose a statistical parsing framework for sentiment analysis that is
capable of analyzing the sentiment structure for a sentence. This
framework can naturally handle compositionality in a probabilistic way. It
can be trained from sentences annotated with only sentiment polarity but
without any syntactic annotations or polarity annotations of constituents
within sentences.
• We present the parsing model, polarity model, and ranking model in the
proposed framework, which are formulated and can be improved
independently. It provides a principled and flexible approach to sentiment
classification.
• We implement the statistical sentiment parsing framework, and conduct
</listItem>
<bodyText confidence="0.929783">
experiments on several benchmark data sets. The experimental results
show that the proposed framework and algorithm can significantly
outperform baseline methods.
The remainder of this article is organized as follows. We introduce related work in
Section 2. We present the statistical sentiment parsing framework, including the parsing
model, polarity model, and ranking model, in Section 3. Learning methods for our
model are explained in Section 4. Experimental results are reported in Section 5. We
conclude this article with future work in Section 6.
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.9997485">
In this section, we give a brief introduction to related work about sentiment classi-
fication (Section 2.1) and parsing (Section 2.2). We tackle the sentiment classification
problem in a parsing manner, which is a significant departure from most previous
research.
</bodyText>
<subsectionHeader confidence="0.973375">
2.1 Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999916647058824">
Sentiment classification has been extensively studied in the past few years. In terms
of text granularity, existing works can be divided into phrase-level, sentence-level, or
document-level sentiment classification. We focus on sentence-level sentiment classifi-
cation in this article. Regardless of what granularity the task is performed on, existing
approaches deriving sentiment polarity from text fall into two major categories, namely,
lexicon-based and learning-based approaches.
The lexicon-based sentiment analysis uses dictionary matching on a predefined sen-
timent lexicon to derive sentiment polarity. These methods often use a set of manually
defined rules to deal with the negation of polarity. Turney (2002) proposed using the
average sentiment orientation of phrases, which contains adjectives or adverbs, in a
review to predict its sentiment orientation. Yu and Hatzivassiloglou (2003) calculated
a modified log-likelihood ratio for every word by the co-occurrences with positive and
negative seed words. To determine the polarity of a sentence, they compare the average
log-likelihood value with threshold. Taboada et al. (2011) presented a lexicon-based
approach for extracting sentiment from text. They used dictionaries of words with anno-
tated sentiment orientation (polarity and strength) while incorporating intensification
and negation. The lexicon-based methods often achieve high precisions and do not need
</bodyText>
<page confidence="0.995725">
268
</page>
<note confidence="0.991861">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.99958056">
any labeled samples. But they suffer from coverage and domain adaption problems.
Moreover, lexicons are often built and used without considering the context (Wilson,
Wiebe, and Hoffmann 2009). Also, hand-crafted rules are often matched heuristically.
The sentiment dictionaries used for lexicon-based sentiment analysis can be cre-
ated manually, or automatically using seed words to expand the list of words. Kamps
et al. (2004) and Williams and Anand (2009) used various lexical relations (such as
synonym and antonym relations) in WordNet to expend a set of seed words. Some other
methods learn lexicons from data directly. Hatzivassiloglou and McKeown (1997) used
a log-linear regression model with conjunction constraints to predict whether conjoined
adjectives have similar or different polarities. Combining conjunction constraints across
many adjectives, a clustering algorithm separated the adjectives into groups of different
polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010)
constructed a term similarity graph using the cosine similarity of context vectors. They
performed graph propagation from seeds on the graph, obtaining polarity words and
phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of
electrons, using the mean field approximation to compute the approximate probability
function of the system instead of the intractable actual probability function. Kanayama
and Nasukawa (2006) used tendencies for similar polarities to appear successively in
contexts. They defined density and precision of coherency to filter neutral phrases and
uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the
lexicon learning to an optimization problem, and used integer linear programming to
solve it. Kaji and Kitsuregawa (2007) defined the χ2-based polarity value and PMI-based
polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning,
and Potts (2010) utilized review data to define polarity strength as the expected rating
value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and
trained a classifier using Support Vector Machines with linear kernel. They then re-
garded the weights as polarity strengths. Krestel and Siersdorfer (2013) generated topic-
dependent lexicons from review articles by incorporating topic and rating probabilities
and defined the polarity strength based on the results. In this article, the lexical relations
defined in WordNet are not used because of its coverage. Furthermore, most of these
methods define different criteria to propagate polarity information of seeds, or use
optimization algorithms and sentence-level sentiment labels to learn polarity strength
values. Their goal is to balance the precision and recall of learned lexicons. We also
learn the polarity strength values of phrases from data. However, our primary objective
is to obtain correct sentence-level polarity labels, and use them to form the sentiment
grammar.
Learning-based sentiment analysis uses machine learning methods to classify sen-
tences or documents into two (negative and positive) or three (negative, positive, and
neutral) classes. Previous research has shown that sentiment classification is more dif-
ficult than traditional topic-based text classification, despite the fact that the number
of classes in sentiment classification is smaller than that in topic-based text classifi-
cation (Pang and Lee 2008). Pang, Lee, and Vaithyanathan (2002) investigated three
machine learning methods to produce automated classifiers to generate class labels for
movie reviews. They tested them on Naive Bayes, Maximum Entropy, and Support
Vector Machine (SVM), and evaluated the contribution of different features includ-
ing unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results
suggested that a SVM classifier with unigram presence features outperforms other
competitors. Pang and Lee (2004) separated subjective portions from the objective
by finding minimum cuts in graphs to achieve better sentiment classification perfor-
mance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to
</bodyText>
<page confidence="0.99523">
269
</page>
<note confidence="0.839646">
Computational Linguistics Volume 41, Number 2
</note>
<bodyText confidence="0.99985786">
extract frequent subsequences and dependency subtrees, and used them as features
of SVM. McDonald et al. (2007) investigated a global structured model for jointly
classifying polarity at different levels of granularity. This model allowed classification
decisions from one level in the text to influence decisions at another. Yessenalina,
Yue, and Cardie (2010) used sentence-level latent variables to improve document-level
prediction. T¨ackstr¨om and McDonald (2011a) presented a latent variable model for
only using document-level annotations to learn sentence-level sentiment labels, and
T¨ackstr¨om and McDonald (2011b) improved it by using a semi-supervised latent vari-
able model to utilize manually crafted sentence labels. Agarwal et al. (2011) and Tu et al.
(2012) explored part-of-speech tag features and tree-kernel. Wang and Manning (2012)
used SVM built over Naive Bayes log-count ratios as feature values to classify polarity.
They showed that SVM was better at full-length reviews, and Multinomial Naive Bayes
was better at short-length reviews. Liu, Agam, and Grossman (2012) proposed a set
of heuristic rules based on dependency structure to detect negations and sentiment-
bearing expressions. Most of these methods are built on bag-of-words features, and
sentiment compositions are handled by manually crafted rules. In contrast to these
models, we derive polarity labels from tree structures parsed by the sentiment grammar.
There have been several attempts to assume that the problem of sentiment analy-
sis is compositional. Sentiment classification can be solved by deriving the sentiment
of a complex constituent (sentence) from the sentiment of small units (words and
phrases) (Moilanen and Pulman 2007; Klenner, Petrakis, and Fahrni 2009; Choi and
Cardie 2010; Nakagawa, Inui, and Kurohashi 2010). Moilanen and Pulman (2007) pro-
posed using delicate written linguistic patterns as heuristic decision rules when com-
puting the sentiment from individual words to phrases and finally to the sentence. The
manually compiled rules were powerful enough to discriminate between the different
sentiments in effective remedies (positive) / effective torture (negative), and in too colorful
(negative) and too sad (negative). Nakagawa, Inui, and Kurohashi (2010) leveraged a
conditional random field model to calculate the sentiment of all the parsed elements
in the dependency tree and then generated the overall sentiment. It had an advantage
over the rule-based approach (Moilanen and Pulman 2007) in that it did not explicitly
denote any sentiment designation to words or phrases in parse trees. Instead, it modeled
their sentiment polarity as latent variables with a certain probability of being positive
or negative. Councill, McDonald, and Velikovich (2010) used a conditional random field
model informed by a dependency parser to detect the scope of negation for sentiment
analysis. Some other methods model sentiment compositionality in the vector space.
They regard the composition operator as a matrix, and use matrix-vector multiplica-
tion to obtain the transformed vector representation. Socher et al. (2012) proposed a
recursive neural network model that learned compositional vector representations for
phrases and sentences. Their model assigned a vector and a matrix to every node in a
parse tree. The vector captured the inherent meaning of the constituent, and the matrix
captured how it changes the meaning of neighboring words or phrases. Socher et al.
(2013) recently introduced a sentiment treebank based on the results of the Stanford
parser (Klein and Manning 2003). The sentiment treebank included polarity labels of
phrases that are annotated using Amazon Mechanical Turk. The authors trained recur-
sive neural tensor networks on the sentiment treebank. For a new sentence, the model
predicted polarity labels based on the syntactic parse tree, and used tensors to handle
compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple com-
position functions in recursive neural models and learning to select them adaptively.
Most previous methods are either rigid in terms of handcrafted rules, or sensitive to
the performance of existing syntactic parsers they use. This article addresses sentiment
</bodyText>
<page confidence="0.992861">
270
</page>
<note confidence="0.991417">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.990234333333333">
compositions by defining sentiment grammar and borrowing some techniques in the
parsing research field. Moreover, our method uses symbolic representations instead of
vector spaces.
</bodyText>
<subsectionHeader confidence="0.999483">
2.2 Syntactic Parsing and Semantic Parsing
</subsectionHeader>
<bodyText confidence="0.999653136363636">
The work presented in this article is close to traditional statistical parsing, as we borrow
some algorithms to build the sentiment parser. Syntactic parsers are learned from the
Treebank corpora, and find the most likely parse tree with the largest probability. In
this article, we borrow some well-known techniques from syntactic parsing methods
(Charniak 1997; Charniak and Johnson 2005; McDonald, Crammer, and Pereira 2005;
K¨ubler, McDonald, and Nivre 2009), such as the CYK algorithm and Context-Free
Grammar. These techniques are used to build the sentiment grammar and parsing
model. They provide a natural way of defining the structure of sentiment trees and
parse sentences to trees. The key difference lies in that our task is to calculate the
polarity label of a sentence, instead of obtaining the parse tree. We only have sentence-
polarity pairs as our training instances instead of annotated tree structures. Moreover,
in the decoding process, our goal is to compute correct polarity labels by representing
sentences as latent sentiment trees. Recently, Hall, Durrett, and Klein (2014) developed a
discriminative constituency parser using rich surface features, adapting it to sentiment
analysis. Besides extracting unigrams and bigrams as features, they learned interactions
between tags and words located at the beginning or the end of spans. However, their
method relies on phrase-level polarity annotations.
Semantic parsing is another body of work related to this article. A semantic parser
is used to parse meaning representations for given sentences. Most existing semantic
parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney
2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained
annotations of target logical forms, which required the supervision of experts and are
relatively expensive. To balance the performance and the amount of human annotation,
some works used only question-answer pairs or even binary correct/incorrect signals
as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database
query to map sentences to logical forms. It worked with FunQL language and trans-
formed semantic parsing as an integer linear programming (ILP) problem. In each iter-
ation, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and
Klein (2013) learned a semantic parser from question-answer pairs, where the logical
form was modeled as a latent tree-based semantic representation. Krishnamurthy and
Mitchell (2012) presented a method for training a semantic parser using a knowledge
base and an unlabeled text corpus, without any individually annotated sentences.
Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a
grounded Combinatory Categorial Grammar semantic parser, which took context into
consideration. Bao et al. (2014) presented a translation-based weakly supervised seman-
tic parsing method to translate questions to answers based on CYK parsing. A log-linear
model is defined to score derivations. All these weakly supervised semantic parsing
methods learned to transform a natural language sentence to its semantic representation
without annotated logical form. In this work, we build a sentiment parser. Specif-
ically, we use a modified version of the CYK algorithm that parses sentences in a
bottom–up fashion. We use the log-linear model to score candidates generated by beam
search. Instead of using question-answer pairs, sentence-polarity pairs are used as our
weak supervisions. We also use the parameter estimation algorithm proposed by Liang,
Jordan, and Klein (2013).
</bodyText>
<page confidence="0.993256">
271
</page>
<note confidence="0.470303">
Computational Linguistics Volume 41, Number 2
</note>
<sectionHeader confidence="0.86565" genericHeader="method">
3. Statistical Sentiment Parsing
</sectionHeader>
<bodyText confidence="0.999822466666666">
We present the statistical parsing framework for sentence-level sentiment classification
in this section. The underlying idea is to model sentiment classification as a statisti-
cal parsing process. Figure 1 shows the overview of the statistical sentiment parsing
framework. There are three major components. The input sentence s is transformed
into and represented by sentiment trees derived from the parsing model (Section 3.2),
using the sentiment grammar defined in Section 3.1. Trees are scored by the ranking
model in Section 3.3. The sentiment tree with the highest ranking score is treated as the
best derivation for s. Furthermore, the polarity model (Section 3.4) is used to compute
polarity values for the sentiment trees.
Notably, the sentiment trees t are unobserved during training. We can only observe
the sentence s and its polarity label y in training data. In other words, we train the model
directly from the examples of sentences annotated only with sentiment polarity labels
but without any syntactic annotations or polarity annotations of the constituents within
sentences. To be specific, we first learn the sentiment grammar and the polarity model
from data as described in Section 4.2. Then, given the sentence and polarity label pairs
(s, y), we search the latent sentiment trees t and estimate the parameters of the ranking
model as detailed in Section 4.1.
To better illustrate the whole process, we describe the sentiment parsing procedure
using an example sentence, The movie is not very good, but i still like it. The sentiment
polarity label of the above sentence is “positive.” There is negation, intensification, and
contrast in this example, which are difficult to capture using bag-of-words classification
methods. This sentence is a complex case that demonstrates the capability of the pro-
posed statistical sentiment parsing framework, which motivates the work in this article.
The statistical sentiment parsing algorithm may generate a number of sentiment trees
for the input sentence. Figure 2 shows the best sentiment parse tree. It shows that the
statistical sentiment parsing framework can deal with the compositionality of sentiment
in a natural way. In Table 1, we list the sentiment rules used during the parsing process.
We show the generation process of the sentiment parse tree from the bottom–up and
the calculation of sentiment strength and polarity for every text span in the parsing
process.
</bodyText>
<figure confidence="0.897031833333333">
Ranking Model Polarity Model
s t
y
Parsing Model
(sentence) (sentiment tree) (polarity label)
the movie is not very good
</figure>
<figureCaption confidence="0.995655">
Figure 1
</figureCaption>
<bodyText confidence="0.79369775">
The parsing model and ranking model are used to transform the input sentence s to the
sentiment tree t with the highest ranking score. Moreover, the polarity model defines
how to compute polarity values for the rules of the sentiment grammar. The sentiment
tree t is evaluated with respect to the polarity model to produce the polarity label y.
</bodyText>
<equation confidence="0.504540857142857">
S
N
the movie is not very good P→good N +/-
+: 0.93
N→not P
+: 0.87
... ...
-: 0.63
P
P
N
P
P
P
</equation>
<page confidence="0.906136">
272
</page>
<note confidence="0.827061">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<equation confidence="0.9610339">
P
N
N
N
P
P ε
S
P
P
The movie is not very good , but I still like it
</equation>
<figureCaption confidence="0.811603">
Figure 2
</figureCaption>
<bodyText confidence="0.995725166666667">
Sentiment structure for the sentence The movie is not very good, but i still like it. The rules used in
the derivation process include {P -+ the movie is; P -+ good; P -+ i still like it; P -+ very P;
N -+ not P; N -+ PN; N -+ NE; E -+ ,; P -+ N but P; S - +P}.
In the following sections, we first provide a formal description of the sentiment
grammar in Section 3.1. We then present the details of the parsing model in Section 3.2,
the ranking model in Section 3.3, and the polarity model in Section 3.4.
</bodyText>
<subsectionHeader confidence="0.999859">
3.1 Sentiment Grammar
</subsectionHeader>
<bodyText confidence="0.9666806">
We develop the sentiment grammar upon CFG (Context-Free Grammar) (Chomsky
1956). Let 9 =&lt; V, E, S, R &gt; denote a CFG, where V is a finite set of non-terminals,
E is a finite set of terminals (disjointed from V), S E V is the start symbol, and R is
a set of rewrite rules (or production rules) of the form A -+ c where A E V and c E
(V U E)∗. We use 9s =&lt; Vs, Es, S, Rs &gt; to denote the sentiment grammar in this article.
</bodyText>
<tableCaption confidence="0.85409">
Table 1
</tableCaption>
<bodyText confidence="0.904105666666667">
Parsing process for the sentence The movie is not very good, but i still like it. [i, Y, j] represents the
text spanning from i to j is derived to symbol Y. N and P are non-terminals in the sentiment
grammar, and N and P represent polarities of sentiment.
</bodyText>
<listItem confidence="0.188965">
Span Rule Strength Polarity
[0, P, 3]: the movie is P -+ the movie is 0.52 P
[5, P, 6]: good P -+ good 0.87 P
[6, E, 7]: , E -+ ,
[3, N, 6]: not very good N
</listItem>
<footnote confidence="0.781440111111111">
not P 0.63 N
[0, N, 6]: the movie is not very good N -+ PN 0.60 N
[0, N, 7]: the movie is not very good, N -+ NE 0.60 N
[0,P, 11]: the movie is not very good, but i still like it P -+ N but P 0.76 P
[0, S,11]: the movie is not very good, but i still like it S -+ P 0.76 P
- -
[8,P,11]: i still like it P -+ i still like it 0.85 P
[4, P, 6]: very good P -+ very P 0.93 P
-+
</footnote>
<page confidence="0.992258">
273
</page>
<note confidence="0.354958">
Computational Linguistics Volume 41, Number 2
</note>
<bodyText confidence="0.995804666666667">
The non-terminal set is denoted as Vs = {N, P,S,E}, where S is the start symbol, the
non-terminal N represents the negative polarity, and the non-terminal P represents the
positive polarity. The rules in Rs are divided into the following six categories:
</bodyText>
<listItem confidence="0.9944627">
• Dictionary rules: X → wk0, where X ∈ {N, P}, wk 0 = w0 ... wk−1, and
wk 0 ∈ E+s . These rules can be regarded as the sentiment dictionary
used in traditional approaches. They are basic sentiment units
assigned with polarity probabilities. For instance, P → good is a
dictionary rule.
• Combination rules: X → c, where c ∈ (Vs ∪ Es)+, and two successive
non-terminals are not allowed. There is at least one terminal in c.
These rules combine terminals and non-terminals, such as N → not P,
and P → N but P. They are used to handle negation, intensification,
and contrast in sentiment analysis. The number of non-terminals in a
combination rule is restricted to one and two.
• Glue rules: X → X1X2, where X, X1, X2 ∈ {N, P}. These rules combine two
text spans that are derived into X1 and X2, respectively.
• OOV rules: E → wk0, where wk0 ∈ E+. We use these rules to handle
Out-Of-Vocabulary (OOV) text spans whose polarity probabilities are not
learned from data.
• Auxiliary rules: X → EX1, X → X1E, where X, X1 ∈ {N, P}. These rules
combine a text span with polarity and an OOV text span.
• Start rules: S → Y, where Y ∈ {N, P, E}. The derivations begin with S, and S
can be derived to N, P, and E.
</listItem>
<bodyText confidence="0.998649090909091">
Here, X represents the non-terminals N or P. The dictionary rules and combi-
nations rules are automatically extracted from the data. We will describe the details
in Section 4.2. By applying these rules, we can derive the polarity label of a sen-
tence from the bottom–up. The glue rules are used to combine polarity informa-
tion of two text spans together, and it treats the combined parts as independent. In
order to tackle the OOV problem, we treat a text span that consists of OOV words as
empty text span, and derive them to E. The OOV text spans are combined with other
text spans without considering their sentiment information. Finally, each sentence is
derived to the symbol S using the start rules that are the beginnings of derivations.
We can use the sentiment grammar to compactly describe the derivation process of a
sentence.
</bodyText>
<subsectionHeader confidence="0.997998">
3.2 Parsing Model
</subsectionHeader>
<bodyText confidence="0.999932">
We present the formal description of the statistical sentiment parsing model following
deductive proof systems (Shieber, Schabes, and Pereira 1995; Goodman 1999) as used in
traditional syntactic parsing. For a concrete example,
</bodyText>
<equation confidence="0.989177">
(6)
[i, A, j]
(A → BC) [i, B, k] [k, C, j]
</equation>
<page confidence="0.99552">
274
</page>
<note confidence="0.939188">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.601273666666667">
which represents if we have the rule A -+ BC and B ∗ wki and C ∗ wjk (∗ is used to
represent the reflexive and transitive closure of immediate derivation), then we can
obtain A ∗ wji . By adding a unary rule
</bodyText>
<equation confidence="0.9673455">
(7)
[i, A, j]
</equation>
<bodyText confidence="0.8696225">
with the binary rule in Equation (6), we can express the standard CYK algorithm for
CFG in Chomsky Normal Form (CNF). And the goal is [0, S, n], in which S is the start
symbol and n is the length of the input sentence. In the given CYK example, the term in
deductive rules can be one of the following two forms:
</bodyText>
<listItem confidence="0.998116">
• [i, X, j] is an item representing a subtree rooted in X spanning from i to j, or
• (X -+ -y) is a rule in the grammar.
</listItem>
<bodyText confidence="0.586909">
Generally, we represent the form of an inference rule as:
</bodyText>
<equation confidence="0.987636">
(r) H1 ... HK (8)
[i,X,j]
</equation>
<bodyText confidence="0.999009333333333">
where, if all the terms r and Hk are true, then we can infer [i, X, j] as true. Here, r denotes
a sentiment rule, and Hk denotes an item. When we refer to both rules and items, we
employ the word terms.
Theoretically, we can convert the sentiment rules to CNF versions, and then use
the CYK algorithm to conduct parsing. Because the maximum number of non-terminal
symbols in a rule is already restricted to two, we formulate the statistical sentiment
parsing based on a customized CYK algorithm that is similar to the work of Chiang
(2007). Let X, X1, X2 represent the non-terminals N or P; the inference rules for the
statistical sentiment parsing are summarized in Figure 3.
</bodyText>
<subsectionHeader confidence="0.996646">
3.3 Ranking Model
</subsectionHeader>
<bodyText confidence="0.999901">
The parsing model generates many candidate parse trees T(s) for a sentence s. The
goal of the ranking model is to score and rank these parse trees. The sentiment tree
with the highest score is treated as the best representation for sentence s. We extract a
feature vector 4)(s, t) E Rd for the specific sentence-tree pair (s, t), where t E T(s) is the
parse tree. Let * E Rd be the parameter vector for the features. We use the log-linear
model to calculate a probability p(t|s; T,*) for each parse tree t E T(s). The probabilities
indicate how likely the trees are to produce correct predictions. Given the sentence s
and parameters *, the log-linear model defines a conditional probability:
</bodyText>
<equation confidence="0.888858727272727">
p(t|s; T,*) = exp 14)(s,t)T* − A(*; s, T)}
A(*; s, T) = log � exp 14)(s,t)T*}
tET(s)
275
(A -+ wji)
Computational Linguistics Volume 41, Number 2
(X → wji)
[i,X,j]
(X → wi1
i X1wj j1) [i1,X1,j1]
[i,X,j]
(X → wi1
i X1wi2 j1X2wj2 j ) [i1,X1,j1] [i2,X2,j2]
[i,X,j]
(X → X1X2) [i,X1,k] [k,X2,j]
[i, X,j]
(E → wij)
[i, E, j]
(X → EX1) [i, E, k] [k, X1, j]
[i,X,j]
(X → X1E) [i,X1,k] [k,E,j]
[i,X,j]
</equation>
<bodyText confidence="0.881697">
where X, X1, X2 represent N or P.
</bodyText>
<subsectionHeader confidence="0.350618">
Figure 3
</subsectionHeader>
<bodyText confidence="0.979412833333333">
Inference rules for the basic parsing model.
where A(*; s, T) is the log-partition function with respect to T(s). The log-linear
model is a discriminative model, and it is widely used in natural language pro-
cessing. We can use φ(s, t)T* as the score of the parse tree without normalization in the
decoding process, because p(t|s; T, *) ∝ φ(s, t)T*, and this will not change the ranking
order.
</bodyText>
<subsectionHeader confidence="0.877524">
3.4 Polarity Model
</subsectionHeader>
<bodyText confidence="0.9324015">
The goal of the polarity model is to model the calculation of sentiment strength and
polarity of a text span from its subspans in the parsing process. It is specified in terms of
the rules used in the parsing process. We expand the notations in the inference rule (8)
to incorporate the polarity model. The new form of inference rule is:
</bodyText>
<equation confidence="0.647716857142857">
(r) H1Φ1 ... HKΦK (11)
[i,X,j]Φ
in which r, H1, ... , HK are the terms described in Section 3.2. Every item Hk is assigned
� P(N |wjk
ik )
polarity strength Φk : ik . For the item [i, X, j], the polarity
P(P|wjk
</equation>
<bodyText confidence="0.935539">
ik ) for text span w jk
model Φ(r, Φ1,. . . , ΦK) is defined as a function that takes the rule r and polarity strength
of subspans as input.
</bodyText>
<page confidence="0.997217">
276
</page>
<note confidence="0.942875">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.8556385">
The polarity strength obtained by the polarity model should satisfy two con-
straints. First, the values calculated by the polarity model are non-negative, that is,
</bodyText>
<equation confidence="0.940985">
P(X |wji) ≥ 0, P(X |wji) ≥ 0. Second, the positive and negative polarity values are normal-
ized to 1, namely, P(X |wji) + P(X |wji) = 1. Notably, X = P, X = N is the opposite
�N, X = P
</equation>
<bodyText confidence="0.93827875">
polarity of X.
The inference rules with the polarity model are formally defined in Figure 4. In the
following part, we define the polarity model for the different types of rules. If the rule
is a dictionary rule X → wji, its sentiment strength is obtained as:
</bodyText>
<equation confidence="0.9999725">
Φ : P(X |wji) = ˜P(X |w ji) (12)
�P(X|wji) = ˜P(X|wji)
</equation>
<bodyText confidence="0.872375">
where X ∈ {N, P} denotes the sentiment polarity of the left hand side of the rule, X is
the opposite polarity of X, and ˜P(X |wji), ˜P(X |wji) indicate the sentiment polarity values
estimated from training data.
</bodyText>
<equation confidence="0.94719408">
(X → wji)
[i,X,j]P(X |wji) = ˜P(X |wji)
(X → wi1
i X1wj j1 )[i1,X1,j1]Φ1
[i, X, j]P(X |wji) = h(θ0 + θ1P(X1|wj1
i1 ))
(X → wi1
i X1wi2
j1X2wj j2) [i1,X1,j1]Φ1 [i2,X2, j2]Φ2
[i,X,j]P(X|wji) = h(θ0 + θ1P(X1|wj1
i1 ) + θ2P(X2|wj2
i2 ))
(X → X1X2) [i,X1,k]Φ1 [k,X2,j]Φ2
P(X|w k i )P(X|w j
[i,X,j]P(X|wj k)
i) = P(X|wki )P(X|wjk)+P(X|w ki )P(X|w jk)
(E → wji)
[i,E, j]◦
(X → EX1) [i,E,k] ◦ [k,X1,j]Φ1
[i,X,j]P(X|wji) = P(X |wjk)
(X → X1E) [i,X1,k]Φ1 [k,E,j]◦
[i,X,j]P(X|wji) = P(X|wki )
where h(x) = 1 + exp{−x} is a logistic function, ◦ represents the absence, and X, X1, X2
1
represent N or P. As specified in the polarity model, we have P(X |wji) = 1 − P(X |wji).
</equation>
<figureCaption confidence="0.724466">
Figure 4
</figureCaption>
<bodyText confidence="0.619432">
Inference rules with the polarity model.
</bodyText>
<page confidence="0.970919">
277
</page>
<note confidence="0.342591">
Computational Linguistics Volume 41, Number 2
</note>
<bodyText confidence="0.906853">
The glue rules X -+ X1X2 combine two spans (wki , wjk). The polarity value is calcu-
lated by their product, and normalized to 1.
</bodyText>
<equation confidence="0.99945275">
Φ : P(X|wk i )P(X|w j
{ P(X |wj k)
i) = P(X|wk i )P(X|w j k)+P(X|w k i )P(X|wj k) (13)
P(X |wji) = 1 − P(X |wji)
</equation>
<bodyText confidence="0.99740425">
For OOV text spans, the polarity model does not calculate the polarity values. When
they are combined with in-vocabulary phrases by the auxiliary rules, the polarity values
are determined by the text span with polarity and the OOV text span is ignored. More
specifically,
</bodyText>
<equation confidence="0.995801">
�
P(X |wji) = P(X |wki ) Φ :(14)
P(X |wji) = P(X |wki )
</equation>
<bodyText confidence="0.999200833333333">
The combination rules are more complicated than other types of rules. In this article,
we model the polarity probability calculation as the logistic regression. The logistic
regression can be regarded as putting linear combination of the subspans’ polarity prob-
abilities into a logistic function (or sigmoid function). We will show that the negation,
intensification, and contrast can be well modeled by the regression-based method. It is
formally shown as
</bodyText>
<equation confidence="0.99325875">
00 + �K �0kP(Xk|wjkik) (15)
P(X |wj i) = h k=1
= 1
1 + exp &lt; − (00 + Ek=1 0kP(Xk |wik)) }
</equation>
<bodyText confidence="0.9701914">
where h(x) = 1+exp {−x} is the logistic function, K is the number of non-terminals in
1
a rule, and 00, ... , 0K are the parameters that are learned from data. As a concrete
example, if the span wji can match N -+ not P and P � wji+1, the inference rule with
the polarity model is defined as
</bodyText>
<equation confidence="0.999812666666667">
[i, N, j] N -+ not P [i + 1, P, j]Φ1 �(16)
P(N|wji) = h(00 + 01P(P|wji+1))
P(P|wji) = 1 − P(N|wji)
</equation>
<bodyText confidence="0.9998528">
where polarity probability is calculated by P(N|wji) = h(00 + 01P(P|wji+1)).
To tackle negation, switch negation (Choi and Cardie 2008; Saur´ı 2008) simply re-
verses the sentiment polarity and corresponding sentiment strength. However, consider
not great and not good; flipping polarity directly makes not good more positive than
not great, which is unreasonable. Another potential problem of switch negation is that
negative polarity items interact with intensifiers in undesirable ways (Kennedy and
Inkpen 2006). For example, not very good turns out to be even more negative than not
good, given the fact that very good is more positive than good. Therefore, Taboada et al.
(2011) argue that shift negation is a better way to handle polarity negation. Instead
of reversing polarity strength, shift negation shifts it toward the opposite polarity by
</bodyText>
<page confidence="0.982741">
278
</page>
<note confidence="0.891063">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.999931380952381">
a fixed amount. This method can partially avoid the aforementioned two problems.
However, they set the parameters manually, which might not be reliable and extensible
enough to a new data set. Using the regression model, switch negation is captured by
the negative scale item 0k (k &gt; 0), and shift negation is expressed by the shift item 00.
The intensifiers are adjectives or adverbs that strengthen (amplifier) or decrease
(downtoner) the semantic intensity of its neighboring item (Quirk 1985). For example,
extremely good should obtain higher strength of positive polarity than good, because it
is modified by the amplifier (extremely). Polanyi and Zaenen (2006) and Kennedy and
Inkpen (2006) handle intensifiers by polarity addition and subtraction. This method,
termed fixed intensification, increases a fixed amount of polarity for amplifiers and
decreases for downtoners. Taboada et al. (2011) propose a method, called percentage
intensification, to associate each intensification word with a percentage scale, which is
larger than one for amplifiers, and less than one for downtoners. The regression model
can capture these two methods to handle the intensification. The shift item 00 represents
the polarity addition and subtraction directly, and the scale item 0k (k &gt; 0) can scale the
polarity by a percentage.
Table 2 illustrates how the regression based polarity model represents different
negation and intensification methods. For a specific rule, the parameters and the com-
positional method are automatically learned from data (Section 4.2.3) instead of setting
them manually as in previous work (Taboada et al. 2011). In a similar way, this method
can handle the contrast. For example, the inference rule for N → P but N is:
</bodyText>
<equation confidence="0.998032666666667">
(N → P but N) [i1, P, j1]Φ1 [i2, N,j2]Φ2 (17)
rP(N|wji) = h(00 + 01P(P*1) + 02P(N |wi2))
[i, N,j] P(P|wji) = 1 − P(N|wji)
</equation>
<bodyText confidence="0.9985278">
where the polarity probability of the rule N → P but Nis computed by P(N|wji) = h(00 +
01P(P|wj1
i1 ) + 02P(N|wj2i2)). It can express the contrast relation by specific parameters 00,
01, and 02.
It should be noted that a linear regression model could turn out to be problem-
atic, as it may produce unreasonable results. For example, if we do not add any
constraint, we may get P(N|wji) = −0.6 + P(P|wji+1). When P(P|wji+1) = 0.55, we will
get P(N|wji) = −0.6 + 0.55 = −0.05. This conflicts with the definition that the polarity
probability ranges from zero to one. Figure 5 intuitively shows that the logistic function
truncates polarity values to (0, 1) smoothly.
</bodyText>
<tableCaption confidence="0.700834">
Table 2
</tableCaption>
<bodyText confidence="0.996538333333333">
The check mark means the parameter of the polarity model can capture the corresponding
intensification type and negation type. Shift item 00 can handle shift negation and fixed
intensification, and scale item 01 can model switch negation and percentage intensification.
</bodyText>
<table confidence="0.748646333333333">
Parameter Negation Type Intensification Type
P(X|wj i) = h(00 + 01P(X|wj1 P(X|wj i) = h(00 + 01P(X|wj1
i1 )) i1 ))
Shift Switch Percentage Fixed
00 (Shift item) ✓ ✓
01 (Scale item) ✓ ✓
</table>
<page confidence="0.829878">
279
</page>
<figure confidence="0.957994090909091">
Computational Linguistics Volume 41, Number 2
h(x)
0.5
0.0
1.0
Linear Function
Logistic Function
x
Figure 5
Logistic function h(x)=1+exp{−x} truncates polarity values to (0, 1) smoothly. The computed
1
</figure>
<figureCaption confidence="0.586422">
values are used as polarity probabilities.
</figureCaption>
<subsectionHeader confidence="0.778938">
3.5 Constraints
</subsectionHeader>
<bodyText confidence="0.999516">
We incorporate additional constraints into the parsing model. Those are used as pruning
conditions in the derivation process not only to improve efficiency but also to force the
derivation towards the correct direction. We expand the inference rules in Section 3.4 as,
</bodyText>
<equation confidence="0.8790475">
(r) H1D1 ... HKDK C (18)
[i, X,j]D
</equation>
<bodyText confidence="0.99986525">
where C is a side condition. The constraints are interpreted in a Boolean manner. If
the constraint C is satisfied, the rule can be used, otherwise, it cannot. We define two
constraints in the parsing model.
First, in the parsing process, the polarity label of text span wji obtained by the
polarity model (Section 3.4) should be consistent with the non-terminal X (N or P)
on the left hand side of the rule. To distinguish between the polarity labels and the
non-terminals, we denote the corresponding polarity label of non-terminal X as X.
Following this notation, we describe the first constraint as
</bodyText>
<equation confidence="0.9231405">
C1 : P(X |wji) &gt; P(X |wji) (19)
where X is the opposite polarity of X. For instance, if rule P → not N matches the text
</equation>
<bodyText confidence="0.99923725">
span wji, the polarity calculated by the polarity model should be consistent with P, i.e.,
the polarity obtained by the polarity model should be positive (P).
Second, when we apply the combination rules, the polarity strength of subspans
needs to exceed a predefined threshold r (≥ 0.5). Specifically, for combination rules X →
</bodyText>
<equation confidence="0.979675">
wi1
i X1wi2 j1X2wj j2 and X → wi1
i X1wj j1, we define the second constraint as
C2 : P(Xk|wjk
ik ) &gt; r,k = 1,...,K (20)
</equation>
<bodyText confidence="0.9989435">
where K is the number of subspans in the rule, and Xk is the corresponding polarity
label of non-terminal Xk in the right hand side. If P(Xk|wjkik) is not larger than threshold
</bodyText>
<page confidence="0.958154">
280
</page>
<note confidence="0.869495">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.999095">
r, we regard the polarity of phrase wjk
ik as neutral. For instance, we do not want to use
the combination rule P → a lot of P or N → a lot of N for the phrase a lot of people. This
constraint avoids improperly using the combination rules for neutral phrases. Notably,
when r is set as 0.5, this constraint is the same as the first one in Equation (19).
As shown in Figure 6, we add these two constraints to the inference rules. The OOV
rules do not have any constraints, and the constraint C1 is applied for all the other rules.
The constraint C2 is only applied for the combination rules.
</bodyText>
<subsectionHeader confidence="0.997879">
3.6 Decoding Algorithm
</subsectionHeader>
<bodyText confidence="0.999956777777778">
In this section, we summarize the decoding algorithm in Algorithm 1. For a sentence s,
the CYK algorithm and dynamic programming are used to obtain the sentiment tree
with the highest score. To be specific, the modified CYK parsing model parses the input
sentence to sentiment trees in a bottom–up manner—that is, from short to long text
spans. For every text span wji, we match the rules in the sentiment grammar (Section 3.1)
to generate the candidate set. Their polarity values are calculated using the polarity
model described in Section 3.4. We also use the constraints described in Section 3.5 to
prune search paths. The constraints improve the efficiency of the parsing algorithm and
make derivations that meet our intuition.
</bodyText>
<equation confidence="0.997183090909091">
(X → wji)
C1
[i, X, j]P(X |wji) = ˜P(X |wji)
(X → wi1
i X1wj j1) [i1,X1,j1]Φ1
C1 ∧ C2
[i, X, j]P(X |wji) = h(00 + 01P(X1|wj1
i1 ))
(X → wi1
i X1wi2 j1X2wj j2 ) [i1,X1,j1]Φ1 [i2,X2,j2]Φ2 C C
[i, X, j]P(X  |wji) = h(00 + 01P(X1  |wit) + 02P(X2|wit )) 1 2
(X → X1X2) [i, X1,k]Φ1 [k,X2,j]Φ2 C1
P(X|wk i )P(X|wj
[i,X,j]P(X|wj k)
i) = P(X|wki )P(X|wjk)+P(X|wki )P(X|wjk)
(E → wji)
◦
[i,E, j]◦
(X → EX1) [i,E,k] ◦ [k,X1,j]Φ1C1
[i,X, j]P(X |wji) = P(X |wjk)
(X → X1E) [i,X1,k]Φ1 [k,E,j]◦C1
[i, X,j]P(X |wji) = P(X |wki )
</equation>
<bodyText confidence="0.727822">
where h(x) = 1 + exp{−x} is a logistic function, ◦ represents the absence, and X, X1, X2
</bodyText>
<equation confidence="0.4550945">
1
represent N or P. As specified in the polarity model, we have P(X |wji) = 1 − P(X |wji).
</equation>
<figureCaption confidence="0.595442">
Figure 6
</figureCaption>
<bodyText confidence="0.599573">
Inference rules with the polarity model and constraints.
</bodyText>
<page confidence="0.929466">
281
</page>
<figure confidence="0.657666666666667">
Computational Linguistics Volume 41, Number 2
Algorithm 1 Decoding Algorithm
Input: wn0: Sentence
</figure>
<listItem confidence="0.943898">
Output: Polarity of the input sentence
1: score[,,] +- {}
2: for l +- 1... n do &gt; Modified CYK algorithm
3: for all i, j s.t. j − i = l do
for all inferable rule (r) H1...HK
4: [i,X,j] for wji do
5: Φ +- calculate polarity value for r &gt; Polarity model
6: if constraints are satisfied then &gt; Constraint
7: sc +- compute score for this derivation by ranking model &gt; Ranking
model
8: if sc &gt; score[i, j, X] then
9: score[i, j, X] +- sc
10: return argmaxX∈{ V,P} score[0,X,n]
</listItem>
<bodyText confidence="0.9990905">
The features in the ranking model (Section 4.1.1) decompose along the structure of
the sentiment tree. So the dynamic programming technique can be used to compute the
derivation tree with the highest ranking score. For a span, the scores of its subspans
are used to calculate the local scores of its derivations. For example, the score of the
</bodyText>
<equation confidence="0.745248">
derivation (r) [i1,P,j1] [i2,N,j2] [ij] is score[i1, j1, P] + score[i2, j2, N] + scorer, where score[i, j, X] is
,X,
</equation>
<bodyText confidence="0.999604285714286">
the highest score of text span wji that is derived to the non-terminal X, and scorer is
the score of applying the rule r. As described in Section 3.3, the score of using rule r is
scorer = φ(wji,r)Tψ, where φ(wji,r) is the feature vector of using the rule r for the span
wji, and ψ is the weight vector of the ranking model. The k highest score trees satisfying
the constraints are stored in score[,, ] for decoding the longer text spans. After finishing
the CYK parsing, arg maxX∈{ V,P} score[0, n, X] is regarded as the polarity label of input
sentence. The time complexity is the same as the standard CYK’s.
</bodyText>
<sectionHeader confidence="0.873996" genericHeader="method">
4. Model Learning
</sectionHeader>
<bodyText confidence="0.999957">
We described the statistical sentiment parsing framework in Section 3. We present the
model learning process in this section. The learning process consists of two steps. First,
the sentiment grammar and the polarity model are learned from data. In other words,
the rules and the parameters used to compute polarity values are learned. These basic
sentiment building blocks are then used to build the parse trees. Second, we estimate
the parameters of the ranking model using the sentence and polarity label pairs. At this
stage, we concentrate on learning how to score the parse trees based on the learned
sentiment grammar and polarity model.
Section 4.1 shows the features and the parameter estimation algorithm used in the
ranking model. Section 4.2 illustrates how to learn the sentiment grammar and the
polarity model.
</bodyText>
<subsectionHeader confidence="0.923502">
4.1 Ranking Model Training
</subsectionHeader>
<bodyText confidence="0.9983755">
As shown in Section 3.3, we develop the ranking model on the log-linear model. In
the following subsections, we first present the features used to rank sentiment tree
</bodyText>
<page confidence="0.976628">
282
</page>
<note confidence="0.887625">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.972585125">
candidates. Then, we describe the objective function used in the optimization algorithm.
Finally, we introduce the algorithm for parameter estimation using the gradient-based
method.
4.1.1 Features. We extract a feature vector φ(s, t) E Rd for each parse tree t of sentence s.
The feature vector is used in the log-linear model. In Figure 7, we present the features
extracted for the sentence The movie is not very good, but i still like it. The features are
organized into feature templates. Each of them contains a set of features. These feature
templates are shown as follows:
</bodyText>
<listItem confidence="0.999958">
• COMBHIT: This feature is the total number of combination rules used in t.
• COMBRULE: It contains features {COMBRULE[r] : r is a combination rule},
each of which fires on the combination rule r appearing in t.
• DICTHIT: This feature is the total number of dictionary rules used in t.
• DICTRULE: It contains features {DICTRULE[r] : r is a dictionary rule}, each
</listItem>
<bodyText confidence="0.942228285714286">
of which fires on the dictionary rule r appearing in t.
These features are generic local patterns that capture the properties of the senti-
ment tree. Another intuitive lexical feature template is [combination rule + word]. For
instance, P very P(good) is a feature that lexicalizes the non-terminal P to good.
However, if this feature is fired frequently, the phrase very good would be learned as
a dictionary rule and can be used in the decoding process. So we do not use this feature
template in order to reduce the feature size. It should be noted that these features
</bodyText>
<equation confidence="0.978235">
The movie is not very good , but I still like it
P
N
N
N
P
P ε
R
P
P
The movie is not very good , but I still like it
P
N
N
N
P
P ε
R
P
P
</equation>
<table confidence="0.9710354">
(a) COMBHIT and COMBRULE Feature (b) DICTHIT and DICTRULE
Feature Template Feature Value
Number of combination rules COMBHIT 3
Combination rule COMBRULE[P very P] 1
COMBRULE[N not P] 1
COMBRULE[P N but P] 1
Number of dictionary rules DICTHIT 3
Dictionary rule DICTRULE[P the movie is] 1
DICTRULE[P good] 1
DICTRULE[P i still like it] 1
</table>
<figureCaption confidence="0.996469">
Figure 7
</figureCaption>
<bodyText confidence="0.8720405">
Feature templates used in the ranking model. The red triangles denote the features for the
example.
</bodyText>
<page confidence="0.972156">
283
</page>
<note confidence="0.333988">
Computational Linguistics Volume 41, Number 2
</note>
<bodyText confidence="0.671137857142857">
decompose along structures of sentiment trees, enabling us to use dynamic program-
ming in the CYK algorithm.
4.1.2 Objective Function. We design the ranking model upon the log-linear model to score
candidate sentiment trees. In the training data D, we only have the input sentence s and
its polarity label Ls. The forms of sentiment parse trees, which can obtain the correct
sentiment polarity, are unobserved. So we work with the marginal log-likelihood of
obtaining the correct polarity label Ls,
</bodyText>
<equation confidence="0.971796">
logp(Ls|s;T,ψ) = logp(t ∈ TLs(s)|s;T,ψ)
(21)
= A(ψ; s, TLs) − A(ψ; s, T)
</equation>
<bodyText confidence="0.990525166666667">
where TLs is the set of candidate trees whose prediction labels are Ls, and A(ψ; s, T)
(Equation (10)) is the log-partition function with respect to T(s).
Based on the marginal log-likelihood function, the objective function O(ψ, T) con-
sists of two terms. The first term is the sum of marginal log-likelihood over training
instances that can obtain the correct polarity labels. The second term is a L2-norm
regularization term on the parameters ψ. Formally,
</bodyText>
<equation confidence="0.980167">
� logp(Ls|s; T, ψ) − λ2 kψk22 (22)
O(ψ, T) =
(s,Ls)ED
TLs (s)00
</equation>
<bodyText confidence="0.999978666666667">
To learn the parameters ψ, we use a gradient-based optimization method to max-
imize the objective function O(ψ, T). According to Wainwright and Jordan (2008), the
derivative of the log-partition function is the expected feature vector
</bodyText>
<equation confidence="0.938057">
∂O(ψ,T) = E (Ep(t|s;TLs,ψ)[φ(s,t)] − Ep(t|s;T,ψ)[φ(s, t)]) − λψ (23)
∂ψ (s,Ls)ED
TLs (s)00
</equation>
<bodyText confidence="0.998673">
where Ep(x)[f (x)] = Ex p(x)f(x) for discrete x.
</bodyText>
<listItem confidence="0.8257042">
4.1.3 Parameter Estimation. The objective function O(ψ, T) is not concave (nor convex),
hence the optimization potentially results in a local optimum. Stochastic Gradient De-
scent (SGD; Robbins and Monro 1951) is a widely used optimization method. The SGD
algorithm picks up a training instance randomly, and updates the parameter vector ψ
according to
</listItem>
<equation confidence="0.9647815">
ψj(t+1) = ψj(t) + α ( ∂ 0 ) |ψ=ψ(t)) (24)
i
</equation>
<bodyText confidence="0.999838666666667">
where α is the learning rate, and ∂O(ψ) ∂ψj is the gradient of the objective function with
respect to parameter ψj. The SGD is sensitive to α, and the learning rate is the same
for all dimensions. As described in Section 4.1.1, we mix sparse features together with
dense features. We want the learning rate to be different for each dimension. We use
AdaGrad (Duchi, Hazan, and Singer 2011) to update the parameters, which sets an
adaptive per-feature learning rate. The AdaGrad algorithm tends to use smaller update
</bodyText>
<page confidence="0.993559">
284
</page>
<note confidence="0.964461">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.976654">
steps when we meet a feature many times. In order to compute efficiently, a diagonal
approximation version of AdaGrad is used. The update rule is
</bodyText>
<equation confidence="0.919547166666667">
(∂O(ψ) ) (25)
ψj(t+1) = ψj (t) + α 1
/G(t+1) ∂ψj
V j
G(t+1) = G(t) + (∂O(ψ)|ψ=ψ(t))2
j i ∂ψj
</equation>
<bodyText confidence="0.989372642857143">
where we introduce an adaptive term G(t)
j . G(t)
j becomes larger along with updating, and
decreases the update step for dimension j. Compared with SGD, the only cost is to store
and update G(t)
j for each parameter.
To train the model, we use the method proposed by Liang, Jordan, and Klein (2013).
With the candidate parse trees and objective function, the parameters ψ are updated to
make the parsing model favor correct trees and give them a higher score. Because there
are many parse trees for a sentence, we need to calculate Equation (23) efficiently. As
indicated in Section 4.1.1, the features decompose along the structure of sentiment trees.
So dynamic programming can be utilized to compute Ep(t|s;T,ψ)[φ(s, t)] of Equation (23).
However, the first expectation term Ep(t|s;TLs,ψ)[φ(s,t)] sums over the candidates that
obtain the correct polarity labels. As this constraint does not decompose along the tree
structure, there is no efficient dynamic program for this. Instead of searching all the
parse trees spanning s, we use beam search to approximate this expectation. Beam
search is a best-first search algorithm that explores at most K paths (K is the beam
size). It keeps the local optimums to reduce the huge search space. Specifically, the
beam search algorithm generates the K-best trees with the highest score φ(s,t)Tψ for
each span. These local optimums are used recursively in the CYK process. The K-best
trees for the whole span are regarded as the candidate set ˜T. Then T˜ and ˜TLs are used to
approximate Equation (23) as in Liang, Jordan, and Klein (2013).
The intuition behind this parameter estimation algorithm lies in: (1) if we have
better parameters, we can obtain better candidate trees; (2) with better candidate trees,
we can learn better parameters. Thus the optimization problem is solved in an iterative
manner. We initialize the parameters as zeros. This leads to a random search and gen-
erates random candidate trees. With the initial candidates, the two steps in Algorithm 2
lead the parameters ψ towards the direction achieving better performance.
</bodyText>
<subsectionHeader confidence="0.99861">
4.2 Sentiment Grammar Learning
</subsectionHeader>
<bodyText confidence="0.9995513">
In this section, we present the automatic learning of the sentiment grammar as defined
in Section 3.1. We need to extract the dictionary rules and the combination rules from
data. In traditional statistical parsing, grammar rules are induced from annotated parse
trees (such as the Penn TreeBank), so ideally we need examples of sentiment structure
trees, or sentences annotated with sentiment polarity for the whole sentence as well
as those for constituents within sentences. However, this is not practical, if not un-
feasible, as the annotations will be inevitably time consuming and require laborious
human effort. We show that it is possible to induce the sentiment grammar directly
from examples of sentences annotated with sentiment polarity labels without using
any syntactic annotations or polarity annotations of constituents within sentences. The
</bodyText>
<page confidence="0.98316">
285
</page>
<figure confidence="0.7181802">
Computational Linguistics Volume 41, Number 2
Algorithm 2 Ranking Model Learning Algorithm
Input: D: Training data {(s, Gs)}, S: Maximum number of iteration
Output: *: Parameters of the ranking model
1: *(0) ← (0,0,...,0)T
2: repeat
3: (s, Gs) ← randomly select a training instance in D
4: ˜T(t) ← BEAMSEACH(s,*(t)) &gt; Beam search to generate K-best candidates
5: G�t+1) G(jt) + ( ∂Oa�T(t)) |ψ=ψ(t) \2
6: �t+1) *j(t) + o /G1( ∂O(*, t)) &gt; Update parameters using
V j
AdaGrad
7: t ← t + 1
8: until t &gt; S
9: return *(T)
</figure>
<bodyText confidence="0.977084928571428">
sentences annotated with sentiment polarity labels are relatively easy to obtain, and we
use them as our input to learn dictionary rules and combination rules.
We first present the basic idea behind the algorithm we propose. People are likely to
express positive or negative opinions using very simple and straightforward sentiment
expressions again and again in their reviews. Intuitively, we can mine dictionary rules
from these massive review sentences by leveraging the redundancy characteristics.
Furthermore, there are many complicated reviews that contain complex sentiment
structures (e.g., negation, intensification, and contrast). If we already have dictionary
rules on hand, we can use them to obtain basic sentiment information for the fragments
within complicated reviews. We can then extract combination rules with the help of the
dictionary rules and the sentiment polarity labels of complicated reviews. Because the
simple and straightforward sentiment expressions are often coupled with complicated
expressions, we need to conduct dictionary rule mining and the combination rule
mining in an iterative way.
</bodyText>
<subsubsectionHeader confidence="0.562888">
4.2.1 Dictionary Rule Learning. The dictionary rules 9D are basic sentiment building
</subsubsectionHeader>
<bodyText confidence="0.998944875">
blocks used in the parsing process. Each dictionary rule in 9D is in the form X → f,
where f is a sentiment fragment. We use the polarity probabilities P(N|f ) and P(P|f ) in
the polarity model. To build 9D, we regard all the frequent fragments whose occurrence
frequencies are larger than rf and lengths range from 1 to 7 as the sentiment fragments.
We further filter the phrases formed by stop words and punctuations, which are not
used to express sentiment.
For a balanced data set, the sentiment distribution of a candidate sentiment frag-
ment f is calculated by
</bodyText>
<equation confidence="0.999465">
#(f, X ) + 1
P(X |f ) = (26)
#(f,N ) + #(f, P) + 2
</equation>
<bodyText confidence="0.993564333333333">
where X E {N, P}, and #(f, X ) denotes the number of reviews containing f with X
being the polarity. It should be noted that Laplace smoothing is used in Equation (26) to
deal with the zero frequency problem.
</bodyText>
<page confidence="0.992273">
286
</page>
<note confidence="0.96566">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.979904">
We do not learn the polarity probabilities P(N|f) and P(P|f) by directly counting
occurrence frequency. For example, in the review sentence this movie is not good (nega-
tive), the naive counting method increases the count #(good,N ) in terms of the polarity
of the whole sentence. Moreover, because of the common collocation not as good as
(negative) in movie reviews, as good as is also regarded as negative if we count the
frequency directly. The examples indicate why some polarity probabilities of phrases
counting from data are different from our intuitions. These unreasonable polarity
probabilities also make trouble for learning the polarity model. Consequently, in order
to estimate more reasonable probabilities, we need to take the compositionality into
consideration when learning sentiment fragments.
Following this motivation, we ignore the count #(f, X ), if the sentiment fragment
f is covered by a negation rule r that negates the polarity of f. The word cover here
means that f is derived within a non-terminal of the negation rule r. For instance, the
negation rule N → not P covers the sentiment fragment good in the sentence this is not
a good movie (negative), that is, the good is derived from P of this negation rule. So we
ignore the occurrence for #(good,N ) in this sentence. It should be noted that we still
increase the count for #(not good,N ), because there is no negation rule covering the
fragment not good.
As shown in Algorithm 3, we learn the dictionary rules and their polarity probabil-
ities by counting the frequencies in negative and positive classes. Only the fragments
whose occurrence numbers are larger than threshold rf are kept. Moreover, we take
the combination rules into consideration to acquire more reasonable GD. Notably, a
subsequence of a frequent fragment must also be frequent. This is similar to the key
insight in the Apriori algorithm (Agrawal and Srikant 1994). When we learn the dic-
tionary rules, we can count the sentiment fragments from short to long, and prune the
infrequent fragments in the early stages if any subsequence is not frequent. This pruning
method accelerates the dictionary rule learning process and makes the procedure fit in
memory.
Algorithm 3 Dictionary Rule Learning
</bodyText>
<listItem confidence="0.968389055555556">
Input: D: Data set, GC: Combination rules, rf: Frequency threshold
Output: GD: Dictionary rules
1: function MINEDICTIONARYRULES(D, GC)
2: GD&apos; ← {}
3: for (s, Ls) in D do &gt; s : w0w1 · · · w|s|−1, Ls: Polarity label of s
&gt; wj
4: for all i, j s.t. 0 ≤ i &lt; j ≤ |s |do i : wiwi+1 ··· wj−1
5: if no negation rule in GC covers wji then
#(wj
6: i, Ls) ++
add wj
7: i to GD&apos;
8: GD ← {}
9: for f in GD&apos; do
10: if #(f, ·) ≥ rf then
11: compute P(N|f ) and P(P|f ) using Equation (26)
12: add dictionary rule (Lf → f ) to GD &gt; Lf = arg maxXcjN,P} P(X |f )
13: return GD
</listItem>
<page confidence="0.96552">
287
</page>
<note confidence="0.463832">
Computational Linguistics Volume 41, Number 2
</note>
<subsubsectionHeader confidence="0.96882">
4.2.2 Combination Rule Learning. The combination rules 9C are generalizations for the
</subsubsectionHeader>
<bodyText confidence="0.99777719047619">
dictionary rules. They are used to handle the compositionality and process unseen
phrases. The learning of combination rules is based on the learned dictionary rules and
their polarity values. The sentiment fragments are generalized to combination rules by
replacing the subsequences of dictionary rules with their polarity labels. For instance, as
shown in Figure 8, the fragments is not (good/as expected/funny/well done) are all negative.
After replacing the subspans good, as expected, funny, and well done with their polarity
label P, we can learn the negation rule N → is not P.
We present the combination rule learning approach in Algorithm 4. Specifically,
the first step is to generate combination rule candidates. For every subsequence wj i
of sentiment fragment f, we replace it with the corresponding non-terminal Lwj if
P(Lwj |wji) is larger than the threshold τp, and we can get wi0Lwjw�f |. Next, we compare
the polarity Lwj with Lf. If Lf # Lwj, we regard the rule Lf ! wi0Lwjw�f  |as a negation
rule. Otherwise, we further compareitheir polarity values. If this rule hakes the polarity
value become larger (or smaller), it will be treated as a strengthen (or weaken) rule. To
obtain the contrast rules, we replace two subsequences with their polarity labels in a
similar way. If the polarities of these two subsequences are different, we categorize this
rule to the contrast type. Notably, these two non-terminals cannot be next to each other.
After these steps, we get the rule candidate set 9C&apos; and the occurrence number of each
rule. We then filter the rule candidates whose occurrence frequencies are too small, and
assign the rule types (negation, strengthen, weaken, and contrast) according to their
occurrence numbers.
</bodyText>
<figureCaption confidence="0.62686625">
4.2.3 Polarity Model Learning. As shown in Section 3.4, we define the polarity model
to calculate the polarity probabilities using the sentiment grammar. In this section, we
present how to learn the parameters of the polarity model for the combination rules.
Figure 8
</figureCaption>
<bodyText confidence="0.999236222222222">
We replace the subsequences with their polarity labels for frequent sentiment fragments.
As shown here, we replace good, as expected, funny, and well done with their polarity label P.
Then we compare the polarity probabilities of subfragments with the whole fragments,
such as good and is not good, to determine whether it is a negation rule, strengthen
rule, or weaken rule. After obtaining the rule, we use polarity probabilities of these
compositional examples as training data to estimate parameters of the polarity model.
In this, (P(P|good),P(N|is not good), (P(P|as expected),P(N|is not as expected),
(P(P|funny), P(N|is not funny), (P(P|well done), and P(N|is not well done)) are used to
learn the polarity model for N → is not P.
</bodyText>
<figure confidence="0.883664653846154">
𝑁 → is not P
Polarity Model
...
... is not
good
as expected
funny
well done
...
Estimate Parameters
𝑁 → is not P
...
P
288
Dong et al. A Statistical Parsing Framework for Sentiment Classification
Algorithm 4 Combination Rule Learning
Input: D: Data set, GD: Dictionary rules, rp, rΔ, rr, rc: Thresholds
Output: GC: Combination rules
1: function MINECOMBINATIONRULES(D, GD)
2: GC&apos; ← {}
3: for (X → f) in GD do Df : w0w1 ··· w|f|−1
4: for all i, j s.t. 0 ≤ i &lt; j ≤ |f  |do
5: if P(Lwj i|wji) &gt; rp then &gt; Polarity label Lwj i = argmaxX∈{N,P} P(X |wji)
6: r: X → wi0Lwjwjf &gt; Non-terminal Lwji = argmaxX∈{N,P} P(X  |wi)
if X =6 L j then
wi
8: #(r, negation) ++
9: else if P(X |f ) &gt; P(Lwj i|wji) + rΔ then
10: #(r, strengthen) ++
11: else if P(X |f ) &lt; P(L j  |wji) − rΔ then
wi
12: #(r, weaken) ++
13: add r to GC&apos;
14: for all i0, j0, i1, j1 s.t. 0 ≤ i0 &lt; j0 &lt; i1 &lt; j1 ≤ |f  |do
15: if P(Lwj0 |w10 &gt; rp and P(LWh Iwtl) &gt; rp then
i0 i1
16: r: X → w0 Lwj0w�oLwj1w�i &gt; Replace wj0 i0, wj1
i1 with the non-terminals
i0 i1
17: if Lwj0 =6 Lwj1 then
i0 i1
18: #(r, contrast) ++
19: add r to GC&apos;
20: GC ← {}
21: for r in GC&apos; do
22: if #(r, ·) &gt; rr and max
T
7:
#(r,T)
#(r) &gt; rc then
23: add r to GC
24: return GC
</figure>
<bodyText confidence="0.9591492">
As shown in Figure 8, we learn combination rules by replacing the subsequences of
frequent sentiment fragments with their polarity labels. Both the replaced fragment and
the whole fragment can be found in the dictionary rules, so their polarity probabilities
have been estimated from data. We can use them as our training examples to figure
out how context changes the polarity of replaced fragment, and learn parameters of the
polarity model.
We describe the polarity model in Section 3.4. To further simplify the notation, we
denote the input vector x = (1, P(X1|wj1
i1 ), ... , P(XK|wjKiK))T, and the response value as y.
Then we can rewrite Equation (15) as
</bodyText>
<equation confidence="0.975629">
1
hθ(x) = (27)
1 + exp{−θTx}
</equation>
<page confidence="0.98651">
289
</page>
<note confidence="0.55352">
Computational Linguistics Volume 41, Number 2
</note>
<bodyText confidence="0.9999112">
where hθ(x) is the polarity probability calculated by the polarity model, and θ =
(θ0, θ1, ... , θK)T is the parameter vector. Our goal is to estimate the parameter vector
θ of the polarity model.
We fit the model to minimize the sum of squared residuals between the predicted
polarity probabilities and the values computed from data. We define the cost function as
</bodyText>
<equation confidence="0.99149">
J (θ) = 1 2 E (hθ(xm) − ym)2 (28)
m
</equation>
<bodyText confidence="0.994632666666667">
where (xm, ym) is the m-th training instance.
The gradient descent algorithm is used to minimize the cost function J (θ). The
partial derivative of J (θ) with respect to θj is
</bodyText>
<equation confidence="0.9818426">
∂J (θ) =E (hθ(xm) − ym) ∂hθ(xm)
∂θj m ∂θj
(hθ(xm) − ym) hθ(xm) (1 − hθ(xi)) ∂θTxm (29)
∂θj
(hθ(xm) − ym) hθ(xm) (1 − hθ(xm)) xmj
</equation>
<bodyText confidence="0.998623">
We set the initial θ as zeros, and start with it. We use the Stochastic Gradient
Descend algorithm to minimize the cost function. For the instance (x, y), the parameters
are updated using
</bodyText>
<equation confidence="0.996759666666667">
C∂J (θ) J
θj(t+1) = θj(t) − α ∂θj |θ=θ(t)
= θj(t) − α(hθ(t)(x) − y)hθ(t)(x) (1 − hθ(t)(x)) xj
</equation>
<bodyText confidence="0.951606333333333">
where α is the learning rate, and it is set to 0.01 in our experiments. We summarize
the learning method in Algorithm 5. For each combination rule, we iteratively scan
Algorithm 5 Polarity Model Learning Algorithm
</bodyText>
<listItem confidence="0.922773833333333">
Input: GC: Combination rules, ε: Stopping condition, α: Learning rate
Output: θ: Parameters of the polarity model
1: function ESTIMATEPOLARITYMODEL(GC)
2: for all combination rule r ∈ GC do
3: θ(0) ← (0,0,...,0)T
4: repeat
</listItem>
<equation confidence="0.760810916666667">
(x, y) ← randomly select a training instance
(t) − α(hθ(t)(x) − y)hθ(t)(x) (1 − hθ(t)(x)) xj
6: θj(t+1) ← θj
7: t ← t + 1
� �2
8: until �θ(t+1) − θ(t)� 2 &lt; ε
9: assign θ(T) as the parameters of the polarity model for rule r
=E
m
=E
m
(30)
</equation>
<page confidence="0.990882">
290
</page>
<note confidence="0.934237">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<table confidence="0.81268575">
Algorithm 6 Sentiment Grammar Learning
Input: D: Data set {(s, Gs)}, T: Maximum number of iteration &gt; Gs: Polarity label of s
Output: 9D: Dictionary rules, 9C: Combination rules
1: 9C {}
</table>
<listItem confidence="0.988037833333333">
2: repeat
3: 9D MINEDICTIONARYRULES(D, 9C) &gt; Algorithm 3
4: 9C MINECOMBINATIONRULES(D, 9D) &gt; Algorithm 4
5: until iteration number exceeds T
6: ESTIMATEPOLARITYMODEL(9C) &gt; Algorithm 5
7: return 9D, 9C
</listItem>
<bodyText confidence="0.88049">
through the training examples (x,y) in a random order, and update the parameters
0 according to Equation (30). The stopping condition is II0(t+1) − 0(t) II2 &lt; e, which
indicates the parameters become stable.
4.2.4 Summary of the Grammar Learning Algorithm. We summarize the grammar learning
process in Algorithm 6, which learns the sentiment grammar in an iterative manner.
We first learn the dictionary rules and their polarity probabilities by counting the
frequencies in negative and positive classes. Only the fragments whose occurrence num-
bers are larger than the threshold rf are kept. As mentioned in Section 4.2.1, the context
can essentially change the distribution of sentiment fragments. We take the combination
rules into consideration to acquire more reasonable 9D. In the first iteration, the set of
combination rules is empty. Therefore, we have no information about compositionality
to improve dictionary rule learning. The initial 9D contains some inaccurate sentiment
distributions. Next, we replace the subsequences of dictionary rules to their polarity
labels, and generalize these sentiment fragments to the combination rules 9C as illus-
trated in Section 4.2.2. At the same time, we can obtain their compositional types and
learn parameters of the polarity model. We iterate over these two steps to obtain refined
9D and 9C.
</bodyText>
<sectionHeader confidence="0.976099" genericHeader="method">
5. Experimental Studies
</sectionHeader>
<bodyText confidence="0.9998975">
In this section, we describe experimental results on existing benchmark data sets with
extensive comparisons with state-of-the-art sentiment classification methods. We also
present the effects of different experimental settings in the proposed statistical senti-
ment parsing framework.
</bodyText>
<subsectionHeader confidence="0.936778">
5.1 Experiment Set-up
</subsectionHeader>
<bodyText confidence="0.9990485">
We describe the data sets in Section 5.1.1, the experimental settings in Section 5.1.2, and
the methods used for comparison in Section 5.1.3.
</bodyText>
<listItem confidence="0.5849375">
5.1.1 Data Sets. We conduct experiments on sentiment classification for sentence-level
and phrase-level data. The sentence-level data sets contain user reviews and critic
</listItem>
<page confidence="0.991436">
291
</page>
<note confidence="0.557355">
Computational Linguistics Volume 41, Number 2
</note>
<bodyText confidence="0.9998015">
reviews from Rotten Tomatoes3 and IMDb.4 We balance the positive and negative
instances in the training data set to mitigate the problem of data imbalance. Moreover,
the Stanford Sentiment Treebank5 contains polarity labels of all syntactically plausible
phrases. In addition, we use the MPQA6 data set for the phrase-level task. We describe
these data sets as follows.
RT-C: 436,000 critic reviews from Rotten Tomatoes. It consists of 218,000 negative
and 218,000 positive critic reviews. The average review length is 23.2 words. Critic
reviews from Rotten Tomatoes contain a label (Rotten: Negative, Fresh: Positive) to
indicate the polarity, which we use directly as the polarity label of corresponding
reviews.
PL05-C: The sentence polarity data set v1.0 (Pang and Lee 2005) contains 5,331
positive and 5,331 negative snippets written by critics from Rotten Tomatoes. This data
set is widely used as the benchmark data set in the sentence-level polarity classification
task. The data source is the same as RT-C.
SST: The Stanford Sentiment Treebank (Socher et al. 2013) is built upon PL05-C.
The sentences are parsed to parse trees. Then, 215,154 syntactically plausible phrases are
extracted and annotated by workers from Amazon Mechanical Turk. The experimental
settings of positive/negative classification for sentences are the same as in Socher et al.
(2013).
RT-U: 737,806 user reviews from Rotten Tomatoes. Because we focus on sentence-
level sentiment classification, we filter out user reviews that are longer than 200 char-
acters. The average length of these short user reviews from Rotten Tomatoes is 15.4
words. Following previous work on polarity classification, we use the review score to
select highly polarized reviews. For the user reviews from Rotten Tomatoes, a negative
review has a score &lt;2.5 out of 5, and a positive review has a score &gt;3.5 out of 5.
IMDB-U: 600,000 user reviews from IMDb. The user reviews in IMDb contain
comments and short summaries (usually a sentence) to summarize the overall sentiment
expressed in the reviews. We use the review summaries as the sentence-level reviews.
The average length is 6.6 words. For user reviews of IMDb, a negative review has a score
&lt;4 out of 10, and a positive review has a score &gt;7 out of 10.
C-TEST: 2,000 labeled critic reviews sampled from RT-C. We use C-TEST as the
testing data set for RT-C. Note that we exclude these from the training data set (i.e.,
RT-C).
U-TEST: 2,000 manually labeled user reviews sampled from RT-U. User reviews
often contain some noisy ratings compared with critic reviews. To eliminate the ef-
fect of noise, we sample 2,000 user reviews from RT-U, and annotate their polarity
labels manually. We use U-TEST as a testing data set for RT-U and IMDB-U, which
are both user reviews. Note that we exclude them from the training data set (i.e.,
RT-U).
MPQA: The opinion polarity subtask of the MPQA data set (Wiebe, Wilson, and
Cardie 2005). The authors manually annotate sentiment polarity labels for the ex-
pressions (i.e., sub-sentences) within a sentence. We regard the expressions as short
sentences in our experiments. There are 7,308 negative examples and 3,316 positive
examples in this data set. The average number of words per example is 3.1.
</bodyText>
<footnote confidence="0.996816">
3 http://www.rottentomatoes.com.
4 http://www.imdb.com.
5 http://nlp.stanford.edu/sentiment/treebank.html.
6 http://mpqa.cs.pitt.edu/corpora/mpqa corpus.
</footnote>
<page confidence="0.990093">
292
</page>
<note confidence="0.996273">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<tableCaption confidence="0.61803975">
Table 3
Statistical information of data sets. #Negative and #Positive are the number of negative instances
and positive instances, respectively. lavg is average length of sentences in the data set, and |V |is
the vocabulary size.
</tableCaption>
<table confidence="0.999402142857143">
Data Set Size #Negative #Positive lavg IVI
RT-C 436,000 218,000 218,000 23.2 136,006
PL05-C 10,662 5,331 5,331 21.0 20,263
SST 98,796 42,608 56,188 7.5 16,372
RT-U 737,806 368,903 368,903 15.4 138,815
IMDB-U 600,000 300,000 300,000 6.6 83,615
MPQA 10,624 7,308 3,316 3.1 5,992
</table>
<bodyText confidence="0.965543909090909">
Table 3 shows the summary of these data sets, and all of them are publicly available
athttp://goo.gl/WxTdPf.
5.1.2 Settings. To compare with other published results for PL05-C and MPQA, the
training and testing regime (10-fold cross-validation) is the same as in Pang and Lee
(2005), Nakagawa, Inui, and Kurohashi (2010) and Socher et al. (2011). For SST, the
regime is the same as in Socher et al. (2013). We use C-TEST as the testing data for RT-C,
and U-TEST as the testing data for RT-U and IMDB-U. There are a number of settings
that have trade-offs in performance, computation, and the generalization power of our
model. The best settings are chosen by a portion of training split data that serves as the
validation set. We provide the performance comparisons using different experimental
settings in Section 5.4.
Number of training examples: The size of training data has been widely recognized
as one of the most important factors in machine learning-based methods. Generally,
using more data leads to better performance. By default, all the training data is used
in our experiments. We use the same size of training data in different methods for fair
comparisons.
Number of training iterations (T): We use AdaGrad (Duchi, Hazan, and Singer
2011) as the optimization algorithm in the learning process. The algorithm starts with
randomly initialized parameters, and alternates between searching candidate sentiment
trees and updating parameters of the ranking model. We treat one-pass scan of training
data as an iteration.
Beam size (K): The beam size is used to make a trade-off between the search space
and the computation cost. Moreover, an appropriate beam size can prune unfavorable
candidates. We set K = 30 in our experiments.
Regularization (λ): The regularization parameter λ in Equation (22) is used to avoid
over-fitting. The value used in the experiments is 0.01.
Minimum fragment frequency: It is difficult to estimate reliable polarity probabili-
ties when the fragment appears very few times. Hence, a minimum fragment frequency
that is too small will introduce noise in the fragment learning process. On the other
hand, a large threshold will lose much useful information. The minimum fragment
frequency is chosen according to the size of the training data set and the validation
performance. To be specific, we set this parameter as 4 for RT-C, SST, RT-U, and
IMDB-U, and 2 for PL05-C and MPQA.
</bodyText>
<page confidence="0.994161">
293
</page>
<note confidence="0.556271">
Computational Linguistics Volume 41, Number 2
</note>
<bodyText confidence="0.9979576">
Maximum fragment length: High order n-grams are more precise and deterministic
expressions than unigrams and bigrams. So it would be useful to use long fragments to
capture polarity information. According to the experimental results, as the maximum
fragment length increases, the accuracy of sentiment classification increases. The maxi-
mum fragment length is set to 7 words in our experiments.
</bodyText>
<subsubsectionHeader confidence="0.711712">
5.1.3 Sentiment Classification Methods for Comparison. We evaluate the proposed statis-
</subsubsectionHeader>
<bodyText confidence="0.955635558139535">
tical sentiment parsing framework on the different data sets, and compare the results
with some baselines and state-of-the-art sentiment classification methods described as
follows.
SVM-m: Support Vector Machine (SVM) achieves good performance in the sen-
timent classification task (Pang and Lee 2005). Though unigrams and bigrams are
reported as the most effective features in existing work (Pang and Lee 2005), we use
high-order n-gram (1 &lt; n &lt; m) features to conduct fair comparisons. Hereafter, m has
the same meaning. We use LIBLINEAR (Fan et al. 2008) in our experiments because it
can handle well the high feature dimension and a large number of training examples.
We try different hyper-parameters C E {10−2,10−1, 1, 5,10, 20} for SVM, and select C on
the validation set.
MNB-m: As indicated in Wang and Manning (2012), Multinomial Naive Bayes
(MNB) often outperforms SVM for sentence-level sentiment classification. We uti-
lize Laplace smoothing (Manning, Raghavan, and Sch¨utze 2008) to tackle the zero
probability problem. High order n-gram (1 &lt; n &lt; m) features are considered in the
experiments.
LM-m: Language Model (LM) is a generative model calculating the probability
of word sequences. It is used for sentiment analysis in Cui, Mittal, and Datar (2006).
Probability of generating sentence s is calculated by P(s) = flis|0 1 P (wi|wo 1), where
wi−1
0 denotes the word sequence w0 ... wi−1. We use Good-Turing smoothing (Good
1953) to overcome sparsity when estimating the probability of high-order n-gram. We
train language models on negative and positive sentences separately. For a sentence,
its polarity is determined by comparing the probabilities calculated from the positive
and negative language models. The unknown-word token is treated as a regular word
(denoted by &lt;UNK&gt;). The SRI Language Modeling Toolkit (Stolcke 2002) is used in our
experiment.
Voting-w/Rev: This approach is proposed by Choi and Cardie (2009b), and is used
as a baseline in Nakagawa, Inui, and Kurohashi (2010). The polarity of a subjective
sentence is decided by the voting of each phrase’s prior polarity. The polarity of phrases
that have odd numbers of negation phrases in their ancestors is reversed. The results
are reported by Nakagawa, Inui, and Kurohashi (2010).
HardRule: This baseline method is compared by Nakagawa, Inui, and Kurohashi
(2010). The polarity of a subjective sentence is deterministically decided based on rules,
by considering the sentiment polarity of dependency subtrees. The polarity of a mod-
ifier is reversed if its head phrase has a negation word. The decision rules are applied
from the leaf nodes to the root node in a dependency tree. We use the results reported
by Nakagawa, Inui, and Kurohashi (2010).
Tree-CRF: Nakagawa, Inui, and Kurohashi (2010) present a dependency tree-based
method using conditional random fields with hidden variables. In this model, the
polarity of each dependency subtree is represented by a hidden variable. The value of
the hidden variable of the root node is identified as the polarity of the whole sentence.
The experimental results are reported by Nakagawa, Inui, and Kurohashi (2010).
</bodyText>
<page confidence="0.997871">
294
</page>
<note confidence="0.852316">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
RAE-pretrain: Socher et al. (2011) introduce a framework based on recursive auto-
</note>
<bodyText confidence="0.98480425">
encoders to learn vector space representations for multi-word phrases and predict
sentiment distributions for sentences. We use the results with pre-trained word vectors
learned on Wikipedia, which leads to better results compared with randomized word
vectors. We directly compare the results with those in Socher et al. (2011).
MV-RNN: Socher et al. (2012) try to capture the compositional meaning of
long phrases through matrix-vector recursive neural networks. This model assigns a
vector and a matrix to every node in the parse tree. Matrices are regarded as opera-
tors, and vectors capture the meaning of phrases. The results are reported by Socher
et al. (2012, 2013).
s.parser-LongMatch: The longest matching rules are utilized in the decoding pro-
cess. In other words, the derivations that contain the fewest rules are used for all text
spans. In addition, the dictionary rules are preferred to the combination rules if both
of them match the same text span. The dynamic programming algorithm is used in the
implementation.
s.parser-w/oComb: This is our method without using the combination rules (such
as N → not P) learned from data.
</bodyText>
<subsectionHeader confidence="0.984512">
5.2 Results of Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999994275862069">
We present the experimental results of the sentiment classification methods on the
different data sets in Table 4. The top three methods on each data set are in bold, and the
best methods are also underlined. The experimental results show that s.parser achieves
better performance than other methods on most data sets.
The data sets RT-C, PL05-C, and SST are critic reviews. On RT-C, the accuracy of
s.parser increases by 2 percentage points, 2.9 percentage points, and 7.1 percentage
points from the best results of SVM, MNB, and LM, respectively. On PL05-C, the
accuracy of s.parser also rises by 2.1 percentage points, 0.7 percentage points, and
4.4 percentage points from the best results of SVM, MNB, and LM, respectively.
Compared to Voting-w/Rev and HardRule, s.parser outperforms them by 16.4 per-
centage points and 16.6 percentage points. The results indicate that our method
significantly outperforms the baselines that use manual rules, as rule-based methods
lack a probabilistic way to model the compositionality of context. Furthermore, s.parser
achieves an accuracy improvement rate of 2.2 percentage points, 1.8 percentage points,
and 0.5 percentage points over Tree-CRF, RAE-pretrain, and MV-RNN, respectively. On
SST, s.parser outperforms SVM, MNB, and LM by 3.4 percentage points, 1.4 percentage
points, and 3.8 percentage points, respectively. The performance is better than MV-RNN
with an improvement rate of 1.8 percentage points. Moreover, the result is compara-
ble to the 85.4% obtained by recursive neural tensor networks (Socher et al. 2013)
without depending on syntactic parsing results.
On the user review data sets RT-U and IMDB-U, our method also achieves the best
results. More specifically, on the data set RT-U, s.parser outperforms the best results of
SVM, MNB, and LM by 1.7 percentage points, 2.9 percentage points, and 1.5 percentage
points, respectively. On the data set IMDB-U, our method brings an improved accuracy
rate of 2.1 percentage points, 3.7 percentage points, and 2.2 percentage points over
SVM, MNB, and LM, respectively. We find that MNB performs better than SVM and
LM on the critics review data sets RT-C and PL05-C. Also, SVM and LM achieve better
results on the user review data sets RT-U and IMDB-U. The s.parser is more robust for
the different genres of data sets.
</bodyText>
<page confidence="0.997221">
295
</page>
<note confidence="0.468976">
Computational Linguistics Volume 41, Number 2
</note>
<tableCaption confidence="0.8470128">
Table 4
Sentiment classification results on different data sets; The top three methods are in bold and the
best is also underlined; SVM-m = Support Vector Machine; MNB-m = Multinomial Naive Bayes;
LM-m = Language Model; Voting-w/Rev = Voting with negation rules; HardRule = Rule based
method on dependency tree; Tree-CRF = Dependency tree-based method employing conditional
random fields; RAE-pretrain = Recursive autoencoders with pre-trained word vectors;
MV-RNN = Matrix-vector recursive neural network; s.parser-LongMatch = The longest
matching rules are used; s.parser-w/oComb = Without using the combination rules; s.parser =
Our method. Some of the results are missing (indicated by “-”) in the table as there is no publicly
available implementation or they are hard to scale up.
</tableCaption>
<table confidence="0.999482833333334">
Method RT-C PL05-C SST RT-U IMDB-U MPQA
SVM-1 80.3 76.3 81.1 88.5 84.9 85.1
SVM-2 83.0 77.4 81.3 88.9 86.8 85.3
SVM-3 83.1 77.0 81.2 89.7 87.2 85.5
SVM-4 81.5 76.9 80.9 89.8 87.0 85.6
SVM-5 81.7 76.8 80.8 89.3 87.0 85.6
MNB-1 79.6 78.0 82.6 83.3 82.7 85.0
MNB-2 82.0 78.8 83.3 87.5 85.6 85.0
MNB-3 82.2 78.4 82.9 88.6 84.6 85.0
MNB-4 81.8 78.2 82.6 88.2 83.1 85.1
MNB-5 81.7 78.1 82.4 88.1 82.5 85.1
LM-1 77.6 75.1 80.9 87.6 81.8 64.0
LM-2 78.0 74.1 78.4 89.0 85.8 71.4
LM-3 77.3 74.2 78.3 89.3 87.1 71.1
LM-4 77.2 73.0 78.3 89.6 87.0 71.1
LM-5 77.0 72.9 78.2 90.0 87.1 71.1
Voting-w/Rev - 63.1 - - - 81.7
HardRule - 62.9 - - - 81.8
Tree-CRF - 77.3 - - - 86.1
RAE-pretrain - 77.7 - - - 86.4
MV-RNN - 79.0 82.9 - - -
s.parser-LongMatch 82.8 78.6 82.5 89.4 86.9 85.7
s.parser-w/oComb 82.6 78.3 82.4 89.0 86.4 85.5
s.parser 85.1 79.5 84.7 91.5 89.3 86.2
</table>
<bodyText confidence="0.998510272727273">
On the data set MPQA, the accuracy of s.parser increases by 0.5 percentage points,
1.1 percentage points, and 14.8 percentage points from the best results of SVM, MNB,
and LM, respectively. Compared with Voting-w/Rev and HardRule, s.parser achieves
4.5 percentage point and 4.4 percentage point improvements over them. As illustrated
in Table 3, the size and length of sentences in MPQA are much smaller than those in
the other four data sets. The RAE-pretrain achieves better results than other methods
on this data set, because the word embeddings pre-trained on Wikipedia can leverage
smoothing to relieve the sparsity problem in MPQA. If we do not use any external
resources (i.e., Wikipedia), the accuracy of RAE on MPQA is 85.7%, which is lower than
Tree-CRF and s.parser. The results indicate that s.parser achieves the best result if no
external resource is used.
</bodyText>
<page confidence="0.996958">
296
</page>
<note confidence="0.963382">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.999946523809524">
In addition, we compare the results of s.parser-LongMatch and s.parser-w/oComb.
The s.parser-LongMatch utilizes the dictionary rules and combination rules in the
longest matching manner, whereas s.parser-w/oComb removes the combination rules
in the parsing process. Compared with the results of s.parser, we find that both the
ranking model and the combination rules play a positive role in the model. The ranking
model learns to score parse trees by assigning larger weights to the rules that tend to ob-
tain correct labels. Also, the combination rules generalize these dictionary rules to deal
with the sentiment compositionality in a symbolic way, which enables the model to
process unseen phrases. Furthermore, s.parser-LongMatch achieves better results than
s.parser-w/oComb. This indicates that the effects of the combination rules are more
pronounced than the ranking model.
The bag-of-words classifiers work well for long documents relying on sentiment
words that appear many times in a document. The redundancy characteristics provide
strong evidence for sentiment classification. Even though some phrases of a document
are not estimated accurately, it can still result in a correct polarity label. However, for
short text, such as a sentence, the compositionality plays an important role in sentiment
classification. Tree-CRF, MV-RNN, and s.parser take compositionality into considera-
tion in different ways, and they achieve significant improvements over SVM, MNB, and
LM. We also find that the high order n-grams contribute to classification accuracy on
most of the data sets, but they harm the accuracy of LM on PL05-C. The high-order
n-grams can partially solve compositionality in a brute-force way.
</bodyText>
<subsectionHeader confidence="0.998543">
5.3 Effect of Training Data Size
</subsectionHeader>
<bodyText confidence="0.9999574">
We further investigate the effect of the size of training data for different sentiment clas-
sification methods. This is meaningful as the number of the publicly available reviews
is increasing dramatically nowadays. The methods that can take advantage of more
training data will be even more useful in practice.
We report the results of s.parser compared with SVM, MNB, and LM on the data
set RT-C using different training data size. In order to make the figure clear, we only
present the results of SVM/MNB/LM-1/5 here. As shown in Figure 9, we find that
the size of training data plays an important role for all these sentiment classification
methods. The basic conclusion is that the performance of all the methods rise as the
data size increases, especially when the data size is smaller than a certain number. It
meets our intuition that the size of data is the key factor when the size is relatively
small. When the size of data is larger, the growth of accuracy becomes slower. The
performance of the baseline methods starts to converge after the data size is larger
than 200,000. The comparisons illustrate that s.parser significantly outperforms these
baselines. And the performance of s.parser becomes even better when the data size
increases. The convergence of s.parser’s performance is slower than the others. It in-
dicates that s.parser leverages data more effectively and benefits more from a larger
data set. With more training data, s.parser learns more dictionary rules and combina-
tion rules. These rules enhance the generalization ability of our model. Furthermore,
it estimates more reliable parameters for the polarity model and ranking model. In
contrast, the bag-of-words based approaches (such as SVM, MNB, and LM) cannot
make full use of high-order information in the data set. The generalization ability
of the combination rules of s.parser leads to better performance, and take advantage
of larger data. It should be noted that there are similar trends with the other data
sets.
</bodyText>
<page confidence="0.981505">
297
</page>
<figure confidence="0.99671825">
Computational Linguistics Volume 41, Number 2
SVM-1
SVM-5
MNB-1
MNB-5
3×105
0 105 2×105 4×10
Data Size
</figure>
<figureCaption confidence="0.995286">
Figure 9
</figureCaption>
<bodyText confidence="0.999963">
The curves show the test accuracy as the number of training examples increases. Our method
s.parser significantly outperforms the other methods, which indicates s.parser can leverage data
more effectively and benefits more from larger data.
</bodyText>
<subsectionHeader confidence="0.999292">
5.4 Effect of Experimental Settings
</subsectionHeader>
<bodyText confidence="0.997842666666667">
In this section, we investigate the effects of different experimental settings. We show
the results on the data set RT-C by only changing one factor and fixing the others.
Figure 10 shows the effect of minimum fragment frequency, and maximum frag-
ment length. Specifically, Figure 10a indicates that a minimum fragment frequency
that is too small will introduce noise, and it is difficult to estimate reliable polarity
probabilities for infrequent fragments. However, a minimum fragment frequency that
is too large will discard too much useful information. As shown in Figure 10b, we find
that accuracy increases as the maximum fragment length increases. The results illustrate
that the large maximum fragment length is helpful for s.parser. We can learn more
</bodyText>
<figure confidence="0.99586725">
Accuracy 85
80
75
70
65
LM-1
LM-5
s.parser
87
86
85
Accuracy
84
83
82
81
80
90
85
Accuracy
80
75
70
2 4 8 16 32
Minimum Fragment Frequency
1 2 3 4 5 6 7
Maximum Fragment Length
(a) Effect of minimum fragment frequency (b) Effect of maximum fragment length
</figure>
<figureCaption confidence="0.976285">
Figure 10
</figureCaption>
<bodyText confidence="0.94344075">
(a) When the minimum fragment frequency is small, noise is introduced in the fragment
learning process. On the other hand, too large a threshold loses useful information. (b) As the
maximum fragment length increases, the accuracy increases monotonically. It indicates that long
fragments are useful for our method.
</bodyText>
<page confidence="0.994194">
298
</page>
<note confidence="0.802138">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<figure confidence="0.98122855882353">
Accuracy
84
90
88
86
82
80
78
76
0.001 0.003 0.01 0.03 0.1 0.3
Regularization (λ)
Accuracy
87
86
85
84
83
81
80
82
1 10 20 30 50 100
Beam size (K)
Accuracy
Average run time
250
200
50
0
150
100
Average run time (ms)
(a) Effect of regularization (b) Effect of beam size
Figure 11
(a) The test accuracy is relatively insensitive to the regularization parameter λ in Equation (22).
</figure>
<listItem confidence="0.7377925">
(b) As the beam size K increases, the test accuracy increases; however, the computation costs also
become more expensive. When K = 1, the optimization algorithm cannot learn any weights.
</listItem>
<bodyText confidence="0.999858230769231">
combination rules with a larger maximum fragment length, and long dictionary rules
capture more precise expressions than unigrams. This conclusion is the same as that in
Section 5.2.
As shown in Figure 11, we also investigate how the training iteration, regulariza-
tion, and beam size affect the results. As shown in Figure 11a, we try a wide range of
regularization parameters λ in Equation (22). The results indicate that it is insensitive
to the choice of λ. Figure 11b shows the effects of different beam size K in the search
process. When beam size K = 1, the optimization algorithm cannot learn the weights.
In this case, the decoding process is to select one search path randomly, and compute
its polarity probabilities. The results become better as the beam size K increases. On
the other hand, the computation costs increase. The proper beam size K can prune some
candidates to speed up the search procedure. It should be noted that the sentence length
also effects the run time.
</bodyText>
<subsectionHeader confidence="0.979313">
5.5 Results of Grammar Learning
</subsectionHeader>
<bodyText confidence="0.9999626">
The sentiment grammar plays a central role in the statistical sentiment parsing frame-
work. It is obvious that the accuracy of s.parser relies on the quality of the automatically
learned sentiment grammar. The quality can be implicitly evaluated by the accuracy of
sentiment classification results, as we have shown in previous sections. However, there
is no straightforward way to explicitly evaluate the quality of the learned grammar.
In this section, we provide several case studies of the learned dictionary rules and
combination rules to further illustrate the results of the sentiment grammar learning
process as detailed in Section 4.2.
To start with, we report the total number of dictionary rules and combination rules
learned from the data sets. As shown in Table 5, the results indicate that we can learn
more dictionary rules and combination rules from the larger data sets. Although we
learn more dictionary rules from RT-C than from IMDB-U, the number of combination
rules learned from RT-C is less than from IMDB-U. It indicates that the language usage
of RT-C is more diverse than that of IMDB-U. For SST, more rules are learned because
of its constituent-level annotations.
</bodyText>
<page confidence="0.996543">
299
</page>
<note confidence="0.545552">
Computational Linguistics Volume 41, Number 2
</note>
<tableCaption confidence="0.996393">
Table 5
</tableCaption>
<table confidence="0.9623953">
Number of rules learned from different data sets. τf represents minimum fragment frequency,
|GD |represents total number of dictionary rules, and |GC |is the total number of combination
rules.
Data Set τf |GD ||GC|
RT-C 4 758,723 952
PL05-C 2 44,101 139
SST 4 336,695 751
RT-U 4 831,893 2,003
IMDB-U 4 249,718 1,014
MPQA 2 6,146 21
</table>
<bodyText confidence="0.999863357142857">
Furthermore, we explore how the minimum fragment frequency τf affects the
number of dictionary rules, and present the distribution of dictionary rule length. As
illustrated in Figure 12a, we find that the relation between total number of dictionary
rules |GD |and minimum fragment frequency τf obeys the power law, that is, the
log10(|GD|) − log2(τf ) graph takes a linear form. It indicates that most of the fragments
appear few times, and only some of them appear frequently. Notably, all the syntacti-
cally plausible phrases of SST are annotated, so its distribution is different from the other
sentence-level data sets. Figure 12b shows the cumulative distribution of dictionary rule
length l. It presents most dictionary rules as short ones. For all data sets except SST,
more than 80% of dictionary rules are shorter than five words. The length distributions
of data sets RT-C and IMDB-U are similar, whereas we obtain more high order n-grams
from RT-U and SST.
We further investigate the effect of context for dictionary rule learning. Table 6
shows some dictionary rules with polarity probabilities learned by our method and
</bodyText>
<figure confidence="0.999487878787879">
(a) Effect of minimum fragment frequency
log2(τf )
(b) Cumulative distribution of dictionary rule
length l
7
RT-C
PL05-C
SST
RT-U
IMDB-U
MPQA
1 2 3 4 5
log2(τf)
6
5
4
3
2
1
P(s 1) 1.0
0.8
0.6
0.4
0.2
0.0
RT-C
PL05-C
SST
RT-U
IMDB-U
MPQA
1 2 3 4 5 6 7
Length d
</figure>
<figureCaption confidence="0.965269">
Figure 12
</figureCaption>
<listItem confidence="0.93280575">
(a) We choose τf = 2, 4, 8,16, 32, and plot log10(|GD|)–log2(τf ) graph to show the effects of τf for
total number of dictionary rules |GD|. The results (except SST) follow a power law distribution.
(b) The cumulative distribution of dictionary rule length l indicates that most dictionary rules
are short ones.
</listItem>
<page confidence="0.996865">
300
</page>
<note confidence="0.997144">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<tableCaption confidence="0.980309">
Table 6
</tableCaption>
<bodyText confidence="0.8155975">
Comparing our dictionary rule learning method with naive counting. The dictionary rules that
are assigned different polarities by these two methods are presented. N represents negative, and
P represents positive. The polarity probabilities of fragments are shown in this table, and they
demonstrate that our method learns more intuitive results than counting directly.
</bodyText>
<figure confidence="0.756793333333333">
Fragment
are fun
a very good movie
looks gorgeous
to enjoy the movies
is corny
’ s flawed
a difficult film to
disappoint
</figure>
<table confidence="0.9778487">
Naive Count s.parser
N P Polarity N P Polarity
0.54 0.46 N 0.11 0.89 P
0.61 0.39 N 0.19 0.81 P
0.56 0.44 N 0.17 0.83 P
0.53 0.47 N 0.14 0.86 P
0.43 0.57 P 0.83 0.17 N
0.32 0.68 P 0.63 0.37 N
0.43 0.57 P 0.67 0.33 N
0.39 0.61 P 0.77 0.23 N
</table>
<bodyText confidence="0.99979175">
naive counting on RT-C. We notice that if we count the fragment occurrence number
directly, some polarities of fragments are learned incorrectly. This is caused by the effect
of context as described in Section 4.2.1. By taking the context into consideration, we
obtain more reasonable polarity probabilities of dictionary rules. Our dictionary rule
learning method takes compositionality into consideration, namely, we skip the count
if there exist some negation indicators outside the phrase. This constraint tries to ensure
that the polarity of fragments is the same as the whole sentence. As shown in the results,
the polarity probabilities learned by our method are more reasonable and meet people’s
intuitions. However, there are also some negative examples caused by “false subjective.”
For instance, the neutral phrase to pay it tends to appear in negative sentences, and it is
learned as a negative phrase. This makes sense for the data distribution, but it may lead
to the mismatch for the combination rules.
In Figure 13, we show the polarity model of some combination rules learned from
the data set RT-C. The first two examples are negation rules. We find that both switch
negation and shift negation exist in data, instead of using only one negation type in
previous work (Choi and Cardie 2008; Saur´ı 2008; Taboada et al. 2011). For the rule
“N → i do not P,” we find that it is a switch negation rule. This rule reverses the polarity
and the corresponding polarity strength. For instance, the i do not like it very much is
more negative than the i do not like it. As shown in Figure 13b, the “N → is not P” is
a shift negation that reduces a fixed polarity strength to reverse the original polarity.
Specifically, the is not good is more negative than the is not great, as described in Sec-
tion 3.4. We have a similar conclusion for the next two weaken rules. As illustrated
in Figure 13c, the “P → P actress” describes one aspect of a movie, hence it is more
likely to decrease the polarity intensity. We find that this rule is a fixed intensification
rule that reduces the polarity probability by a fixed value. The “N → a bit of N” is
a percentage intensification rule, which scales polarity intensity by a percentage. It
reduces more strength for stronger polarity. The last two rules in Figure 13e and Fig-
ure 13f are strengthen rules. Both “P → lot of P” and “N → N terribly” increase the
polarity strength of the sub-fragments. These cases indicate that it is necessary to learn
how the context performs compositionality from data. In order to capture the composi-
tionality for different rules, we define the polarity model and learn parameters for each
rule. This also agrees with the models of Socher et al. (2012) and Dong et al. (2014),
</bodyText>
<page confidence="0.99771">
301
</page>
<figureCaption confidence="0.883482">
Figure 13
</figureCaption>
<bodyText confidence="0.9394515">
Illustration of the polarity model for combination rules: (a)(b) Negation rule. (c)(d) Weaken rule.
(e)(f) Strengthen rule. The labels of axes represent the corresponding polarity labels, the red
points are the training instances, and the blue lines are the regression results for the polarity
model.
</bodyText>
<figure confidence="0.998355655737705">
N
P
P
Computational Linguistics Volume 41, Number 2
0.50.5 0.6 0.7 0.8 0.9 1.0
P
(a) N → i do not P
0.50.5 0.6 0.7 0.8 0.9 1.0
P
(b) N → is not P.
0.9
N → i do not P
1.0
0.9
N → is not P .
1.0
0.6
0.50.5 0.6 0.7 0.8 0.9 1.0
P
(c) P → P actress
0.6
0.50.5 0.6 0.7 0.8 0.9 1.0
N
(d) N → a bit of N
P → P actress
1.0
N → a bit of N
1.0
N
N
N
0.50.5 0.6 0.7 0.8 0.9 1.0
P
(e) P → lot of P
0.50.5 0.6 0.7 0.8 0.9 1.0
N
(f) N → N terribly
P → lot of P
1.0
N → N terribly
1.0
0.8
0.7
0.6
0.8
0.7
0.6
0.9
0.8
0.7
0.9
0.8
0.7
0.9
0.8
0.9
0.8
0.7
0.6
0.7
0.6
</figure>
<page confidence="0.996182">
302
</page>
<note confidence="0.9733">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<bodyText confidence="0.9985025">
which use multiple composition matrices to make compositions specific and is an
improvement over the recursive neural network that employs one composition matrix.
</bodyText>
<sectionHeader confidence="0.977326" genericHeader="conclusions">
6. Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989888888889">
In this article, we propose a statistical parsing framework for sentence-level sentiment
classification that provides a novel approach to designing sentiment classifiers from a
new perspective. It directly analyzes the sentiment structure of a sentence other than
relying on syntactic parsing results, as in existing literature. We show that complicated
phenomena in sentiment analysis, such as negation, intensification, and contrast, can
be handled in a similar manner to simple and straightforward sentiment expressions in
a unified and probabilistic way. We provide a formal model to represent the sentiment
grammar built upon Context-Free Grammars. The framework consists of: (1) a parsing
model to analyze the sentiment structure of a sentence; (2) a polarity model to calculate
sentiment strength and polarity for each text span in the parsing process; and (3) a
ranking model to select the best parsing result from a list of candidate sentiment parse
trees. We show that the sentiment parser can be trained from the examples of sentences
annotated only with sentiment polarity labels but without using any syntactic or senti-
ment annotations within sentences. We evaluate the proposed framework on standard
sentiment classification data sets. The experimental results show that the statistical sen-
timent parsing notably outperforms the baseline sentiment classification approaches.
We believe the work on statistical sentiment parsing can be advanced from many
different perspectives. First, statistical parsing has been a well-established research field,
in which many different grammars and parsing algorithms have been proposed in pre-
viously published literature. It will be an interesting direction to apply and adjust more
advanced models and algorithms from the syntactic parsing and the semantic parsing
to our framework. We leave it as a line of future work. Second, we can incorporate target
and aspect information in the statistical sentiment parsing framework to facilitate the
target-dependent and aspect-based sentiment analysis. Intuitively, this can be done by
introducing semantic tags of targets and aspects as new non-terminals in the sentiment
grammar and revising grammar rules accordingly. However, acquiring training data
will be an even more challenging task, as we need more fine-grained information.
Third, as the statistical sentiment parsing produces more fine-grained information (e.g.,
the basic sentiment expressions from the dictionary rules as well as the sentiment
structure trees), we will have more opportunities to generate better opinion summaries.
Moreover, we are interested in jointly learning parameters of the polarity model and the
parsing model from data. Last but not the least, we are interested in investigating the
domain adaptation, which is a very important and challenging problem in sentiment
analysis. Generally, we may need to learn domain-specific dictionary rules for different
domains, whereas we believe combination rules are mostly generic across different
domains. This is also worth consideration for further study.
</bodyText>
<sectionHeader confidence="0.99832" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998740285714286">
This research was partly supported by NSFC
(grant no. 61421003) and the fund of the
State Key Lab of Software Development
Environment (grant no. SKLSDE-2015ZX-05).
We gratefully acknowledge helpful
discussions with Dr. Nan Yang and the
anonymous reviewers.
</bodyText>
<sectionHeader confidence="0.999091" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.947552571428571">
Agarwal, Apoorv, Boyi Xie, Ilia Vovsha,
Owen Rambow, and Rebecca Passonneau.
2011. Sentiment analysis of twitter
data. In Proceedings of the Workshop
on Languages in Social Media,
LSM ’11, pages 30–38,
Stroudsburg, PA.
</reference>
<page confidence="0.996682">
303
</page>
<note confidence="0.619148">
Computational Linguistics Volume 41, Number 2
</note>
<reference confidence="0.998764991525424">
Agrawal, Rakesh and Ramakrishnan Srikant.
1994. Fast algorithms for mining
association rules in large databases.
In Proceedings of the 20th International
Conference on Very Large Data Bases,
VLDB ’94, pages 487–499,
San Francisco, CA.
Artzi, Yoav and Luke Zettlemoyer. 2013.
Weakly supervised learning of semantic
parsers for mapping instructions to
actions. Transactions of the Association
for Computational Linguistics,
1(1):49–62.
Bao, Junwei, Nan Duan, Ming Zhou, and
Tiejun Zhao. 2014. Knowledge-based
question answering as machine
translation. In Proceedings of the
52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1:
Long Papers), pages 967–976,
Baltimore, MD.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. In Proceedings of the Fourteenth
National Conference on Artificial Intelligence
and Ninth Conference on Innovative
Applications of Artificial Intelligence,
AAAI’97/IAAI’97, pages 598–603,
Providence, RI.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent
discriminative reranking. In Proceedings
of the 43rd Annual Meeting of the Association
for Computational Linguistics, ACL ’05,
pages 173–180, Stroudsburg, PA.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201–228.
Choi, Yejin and Claire Cardie. 2008. Learning
with compositional semantics as structural
inference for subsentential sentiment
analysis. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 793–801,
Stroudsburg, PA.
Choi, Yejin and Claire Cardie. 2009a.
Adapting a polarity lexicon using integer
linear programming for domain-specific
sentiment classification. In Proceedings of
the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 –
Volume 2, EMNLP ’09, pages 590–598,
Stroudsburg, PA.
Choi, Yejin and Claire Cardie. 2009b.
Adapting a polarity lexicon using integer
linear programming for domain-specific
sentiment classification. In Proceedings
of the 2009 Conference on Empirical
Methods in Natural Language Processing:
Volume 2 – Volume 2, pages 590–598,
Singapore.
Choi, Yejin and Claire Cardie. 2010.
Hierarchical sequential learning for
extracting opinions and their attributes.
In Proceedings of the ACL 2010 Conference
Short Papers, ACLShort ’10, pages 269–274,
Stroudsburg, PA.
Chomsky, Noam. 1956. Three models
for the description of language. IRE
Transactions on Information Theory,
2(3):113–124.
Clarke, James, Dan Goldwasser,
Ming-Wei Chang, and Dan Roth. 2010.
Driving semantic parsing from the world’s
response. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning, CoNLL ’10,
pages 18–27, Stroudsburg, PA.
Cocke, John. 1969. Programming Languages
and Their Compilers: Preliminary Notes.
Courant Institute of Mathematical
Sciences, New York University.
Councill, Isaac G., Ryan McDonald, and
Leonid Velikovich. 2010. What’s great and
what’s not: Learning to classify the scope
of negation for improved sentiment
analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural
Language Processing, NeSp-NLP ’10,
pages 51–59, Stroudsburg, PA.
Cui, Hang, Vibhu Mittal, and Mayur Datar.
2006. Comparative experiments on
sentiment classification for online product
reviews. In Proceedings of the 21st National
Conference on Artificial Intelligence -
Volume 2, AAAI’06, pages 1,265–1,270,
Boston, MA.
Davidov, Dmitry, Oren Tsur, and Ari
Rappoport. 2010. Enhanced sentiment
learning using twitter hashtags and
smileys. In Proceedings of the 23rd
International Conference on Computational
Linguistics: Posters, COLING ’10,
pages 241–249, Stroudsburg, PA.
de Marneffe, Marie-Catherine,
Christopher D. Manning, and
Christopher Potts. 2010. “was it good? it
was provocative.” Learning the meaning
of scalar adjectives. In Proceedings of the
48th Annual Meeting of the Association for
Computational Linguistics, ACL ’10,
pages 167–176, Stroudsburg, PA.
Dong, Li, Furu Wei, Ming Zhou, and Ke Xu.
2014. Adaptive multi-compositionality for
recursive neural models with applications
to sentiment analysis. In AAAI Conference
on Artificial Intelligence, pages 1,537–1,543,
Quebec.
</reference>
<page confidence="0.998755">
304
</page>
<note confidence="0.996957">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<reference confidence="0.993138737288135">
Duchi, John, Elad Hazan, and Yoram Singer.
2011. Adaptive subgradient methods
for online learning and stochastic
optimization. Journal of Machine Learning
Research, 12:2121–2159.
Fan, Rong-En, Kai-Wei Chang,
Cho-Jui Hsieh, Xiang-Rui Wang,
and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Good, Irving John. 1953. The population
frequencies of species and the estimation
of population parameters. Biometrika,
40(3-4):237–264.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573–605.
Hall, David, Greg Durrett, and Dan Klein.
2014. Less grammar, more features. In
Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics
(Volume 1: Long Papers), pages 228–237,
Baltimore, MD.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
the 35th Annual Meeting of the Association
for Computational Linguistics and Eighth
Conference of the European Chapter of the
Association for Computational Linguistics,
ACL ’98, pages 174–181, Stroudsburg, PA.
Jia, Lifeng, Clement Yu, and Weiyi Meng.
2009. The effect of negation on sentiment
analysis and retrieval effectiveness. In
Conference on Information and Knowledge
Management, pages 1,827–1,830,
Hong Kong.
Kaji, Nobuhiro and Masaru Kitsuregawa.
2007. Building lexicon for sentiment
analysis from massive collection of HTML
documents. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning
(EMNLP-CoNLL), pages 1,075–1,083,
Prague.
Kamps, Jaap, Robert J. Mokken,
Maarten Marx, and Maarten de Rijke.
2004. Using WordNet to measure semantic
orientation of adjectives. In Proceedings of
the 4th International Conference on Language
Resources and Evaluation (LREC 2004),
volume IV, pages 1,115–1,118, Paris.
Kanayama, Hiroshi and Tetsuya Nasukawa.
2006. Fully automatic lexicon expansion
for domain-oriented sentiment analysis. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, EMNLP ’06, pages 355–363,
Stroudsburg, PA.
Kasami, Tadao. 1965. An efficient recognition
and syntax-analysis algorithm for
context-free languages. Technical report
AFCRL-65-758, Air Force Cambridge
Research Lab, Bedford, MA.
Kate, Rohit J. and Raymond J. Mooney.
2006. Using string-kernels for learning
semantic parsers. In ACL 2006: Proceedings
of the 21st International Conference on
Computational Linguistics and the 44th
Annual Meeting of the ACL, pages 913–920,
Morristown, NJ.
Kennedy, Alistair and Diana Inkpen. 2006.
Sentiment classification of movie reviews
using contextual valence shifters.
Computational Intelligence, 22:110–125.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics -
Volume 1, ACL ’03, pages 423–430,
Stroudsburg, PA.
Klenner, Manfred, Stefanos Petrakis, and
Angela Fahrni. 2009. Robust compositional
polarity classification. In Proceedings of the
International Conference RANLP-2009,
pages 180–184, Borovets.
Krestel, Ralf and Stefan Siersdorfer. 2013.
Generating contextualized sentiment
lexica based on latent topics and user
ratings. In Proceedings of the 24th ACM
Conference on Hypertext and Social Media,
HT ’13, pages 129–138, New York, NY.
Krishnamurthy, Jayant and Tom M. Mitchell.
2012. Weakly supervised training of
semantic parsers. In Proceedings of the 2012
Joint Conference on Empirical Methods in
Natural Language Processing and
Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 754–765,
Stroudsburg, PA.
K¨ubler, Sandra, Ryan McDonald, and Joakim
Nivre. 2009. Dependency parsing.
Synthesis Lectures on Human Language
Technologies, 1(1):1–127.
Li, Peng, Yang Liu, and Maosong Sun. 2013.
An extended ghkm algorithm for inducing
Lambda-SCFG. In Conference on Artificial
Intelligence, pages 605–611, Bellevue, WA.
Liang, Percy, Michael I. Jordan, and Dan
Klein. 2013. Learning dependency-based
compositional semantics. Computational
Linguistics, 39(2):389–446.
Liu, Bing. 2012. Sentiment Analysis and
Opinion Mining. Synthesis Lectures on
Human Language Technologies. Morgan
&amp; Claypool Publishers.
Liu, Jingjing and Stephanie Seneff. 2009.
Review sentiment scoring via a
</reference>
<page confidence="0.974254">
305
</page>
<reference confidence="0.994734445378152">
Computational Linguistics Volume 41, Number 2
parse-and-paraphrase paradigm. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing: Volume 1 - Volume 1,
EMNLP ’09, pages 161–169,
Stroudsburg, PA.
Liu, Shizhu, Gady Agam, and
David A. Grossman. 2012. Generalized
sentiment-bearing expression features for
sentiment analysis. In International
Conference on Computational Linguistics,
pages 733–744, Mumbai.
Lu, Yue, Mal´u Castellanos, Umeshwar Dayal,
and ChengXiang Zhai. 2011. Automatic
construction of a context-aware sentiment
lexicon: an optimization approach. In
International World Wide Web Conference,
pages 347–356, Hyderabad.
Maas, Andrew L., Raymond E. Daly,
Peter T. Pham, Dan Huang, Andrew Y. Ng,
and Christopher Potts. 2011. Learning
word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics:
Human Language Technologies - Volume 1,
HLT ’11, pages 142–150,
Stroudsburg, PA.
Manning, Christopher D., Prabhakar
Raghavan, and Hinrich Sch¨utze. 2008.
Introduction to Information Retrieval.
Cambridge University Press, New York.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
Matsumoto, Shotaro, Hiroya Takamura, and
Manabu Okumura. 2005. Sentiment
classification using word sub-sequences
and dependency sub-trees. In Proceedings
of the 9th Pacific-Asia Conference on Advances
in Knowledge Discovery and Data Mining,
PAKDD’05, pages 301–311, Hanoi.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting on Association for Computational
Linguistics, ACL ’05, pages 91–98,
Stroudsburg, PA.
McDonald, Ryan, Kerry Hannan, Tyler
Neylon, Mike Wells, and Jeff Reynar. 2007.
Structured models for fine-to-coarse
sentiment analysis. In Proceedings of the
Association for Computational Linguistics
(ACL), pages 432–439, Prague.
Moilanen, Karo and Stephen Pulman. 2007.
Sentiment composition. In Proceedings of
Recent Advances in Natural Language
Processing (RANLP 2007), pages 378–382,
Borovets.
Moilanen, Karo, Stephen Pulman, and Yue
Zhang. 2010. Packed feelings and ordered
sentiments: Sentiment parsing with
quasi-compositional polarity sequencing
and compression. In Proceedings of the 1st
Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis (WASSA
2010) at the 19th European Conference on
Artificial Intelligence (ECAI2010),
pages 36–43, Lisbon.
Mudinas, Andrius, Dell Zhang, and Mark
Levene. 2012. Combining lexicon and
learning based approaches for
concept-level sentiment analysis. In
Proceedings of the First International
Workshop on Issues of Sentiment Discovery
and Opinion Mining, WISDOM ’12,
pages 5:1–5:8, New York, NY.
Nakagawa, Tetsuji, Kentaro Inui, and Sadao
Kurohashi. 2010. Dependency tree-based
sentiment classification using CRFS with
hidden variables. In Human Language
Technologies: The 2010 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics, HLT ’10,
pages 786–794, Stroudsburg, PA.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational
Linguistics (ACL’04), Main Volume,
pages 271–278, Barcelona.
Pang, Bo and Lillian Lee. 2005. Seeing stars:
exploiting class relationships for sentiment
categorization with respect to rating scales.
In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics,
ACL ’05, pages 115–124, Stroudsburg, PA.
Pang, Bo and Lillian Lee. 2008. Opinion
mining and sentiment analysis.
Foundations and Trends in Information
Retrieval, 2(1-2):1–135.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?:
Sentiment classification using machine
learning techniques. In Proceedings of
the ACL-02 Conference on Empirical
Methods in Natural Language Processing -
Volume 10, EMNLP ’02, pages 79–86,
Stroudsburg, PA.
Polanyi, Livia and Annie Zaenen. 2006.
Contextual valence shifters. In J. G.
Shanahan, Y. Qu, and J. Wiebe, editors,
Computing Attitude and Affect in Text:
Theory and Applications. Springer
Netherlands, pages 1–10.
</reference>
<page confidence="0.998751">
306
</page>
<note confidence="0.983384">
Dong et al. A Statistical Parsing Framework for Sentiment Classification
</note>
<reference confidence="0.999903">
Quirk, R. 1985. A Comprehensive Grammar of
the English Language. Longman.
Raymond, Ruifang Ge and J. Mooney. 2006.
Discriminative reranking for semantic
parsing. In Proceedings of the COLING/ACL
on Main Conference Poster Sessions,
COLING-ACL ’06, pages 263–270,
Stroudsburg, PA.
Robbins, H. and S. Monro. 1951. A stochastic
approximation method. Annals of
Mathematical Statistics, 22:400–407.
Saur´ı, Roser. 2008. A Factuality Profiler for
Eventualities in Text. Ph.D. thesis, Brandeis
University.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
Journal of Logic Programming, 24(1–2):3–36.
Socher, Richard, Brody Huval,
Christopher D. Manning, and Andrew Y.
Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, EMNLP-CoNLL ’12,
pages 1,201–1,211, Stroudsburg, PA.
Socher, Richard, Jeffrey Pennington, Eric H.
Huang, Andrew Y. Ng, and Christopher D.
Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment
distributions. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11,
pages 151–161, Stroudsburg, PA.
Socher, Richard, Alex Perelygin, Jean Y. Wu,
Jason Chuang, Christopher D. Manning,
Andrew Y. Ng, and Christopher Potts.
2013. Recursive deep models for semantic
compositionality over a sentiment
treebank. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 1,631–1,642,
Seattle, WA.
Stolcke, Andreas. 2002. SRILM: An extensible
language modeling toolkit. In Proceedings
of the 7th International Conference on Spoken
Language Processing (ICSLP 2002,
pages 901–904, Denver, CO.
Taboada, Maite, Julian Brooke, Milan
Tofiloski, Kimberly Voll, and Manfred
Stede. 2011. Lexicon-based methods for
sentiment analysis. Computational
Linguistics, 37(2):267–307.
T¨ackstr¨om, Oscar and Ryan McDonald.
2011a. Discovering fine-grained sentiment
with latent variable structured prediction
models. In Proceedings of the 33rd European
Conference on Advances in Information
Retrieval, ECIR’11, pages 368–374,
Berlin.
T¨ackstr¨om, Oscar and Ryan McDonald.
2011b. Semi-supervised latent variable
models for sentence-level sentiment
analysis. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies:
Short Papers - Volume 2, HLT ’11,
pages 569–574, Stroudsburg, PA.
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2005. Extracting
semantic orientations of words using spin
model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, ACL ’05, pages 133–140,
Stroudsburg, PA.
Tu, Zhaopeng, Yifan He, Jennifer Foster,
Josef van Genabith, Qun Liu, and Shouxun
Lin. 2012. Identifying high-impact
sub-structures for convolution kernels in
document-level sentiment classification. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics:
Short Papers - Volume 2, ACL ’12,
pages 338–343, Stroudsburg, PA.
Turney, Peter D. 2002. Thumbs up or thumbs
down?: Semantic orientation applied to
unsupervised classification of reviews. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
ACL ’02, pages 417–424, Stroudsburg, PA.
Velikovich, Leonid, Sasha Blair-Goldensohn,
Kerry Hannan, and Ryan McDonald. 2010.
The viability of Web-derived polarity
lexicons. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, HLT ’10,
pages 777–785, Stroudsburg, PA.
Wainwright, Martin J. and Michael I. Jordan.
2008. Graphical models, exponential
families, and variational inference.
Foundations and Trends in Machine Learning,
1(1-2):1–305.
Wang, Sida and Christopher Manning. 2012.
Baselines and bigrams: Simple, good
sentiment and topic classification. In
Proceedings of the 50th Annual Meeting
of the Association for Computational
Linguistics (ACL 2012), pages 90–94,
Jeju Island.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2-3):165–210.
Williams, Gbolahan K. and Sarabjot Singh
Anand. 2009. Predicting the polarity
</reference>
<page confidence="0.959254">
307
</page>
<reference confidence="0.98898349122807">
Computational Linguistics Volume 41, Number 2
strength of adjectives using Wordnet. In
ICWSM, pages 346–349, San Jose, CA.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffmann. 2009. Recognizing contextual
polarity: An exploration of features for
phrase-level sentiment analysis.
Computational Linguistics, 35:399–433.
Yessenalina, Ainur, Yisong Yue, and Claire
Cardie. 2010. Multi-level structured
models for document-level sentiment
classification. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,046–1,056,
Cambridge, MA.
Younger, Daniel H. 1967. Recognition
and parsing of context-free languages
in time n3. Information and Control,
10(2):189–208.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing, EMNLP ’03,
pages 129–136, Stroudsburg, PA.
Zelle, John M. and Raymond J. Mooney.
1996. Learning to parse database queries
using inductive logic programming.
In AAAI/IAAI, pages 1,050–1,055,
Portland, OR.
Zettlemoyer, Luke S. and Michael Collins.
2007. Online learning of relaxed CCG
grammars for parsing to logical form. In
Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL-2007),
pages 678–687, Prague.
Zettlemoyer, Luke S. and Michael Collins.
2009. Learning context-dependent
mappings from sentences to logical form.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on
Natural Language Processing of the AFNLP:
Volume 2 - Volume 2, ACL ’09, pages
976–984, Stroudsburg, PA.
Zhao, Jichang, Li Dong, Junjie Wu, and
Ke Xu. 2012. Moodlens: An
emoticon-based sentiment analysis system
for Chinese tweets. In Proceedings of the
18th ACM SIGKDD International Conference
on Knowledge Discovery and Data
Mining, KDD ’12, pages 1,528–1,531,
New York, NY.
</reference>
<page confidence="0.998567">
308
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.414392">
<title confidence="0.9791855">A Statistical Parsing Framework for Sentiment Classification</title>
<affiliation confidence="0.9988184">Beihang University Microsoft Research Microsoft Research Microsoft Research Beihang University</affiliation>
<abstract confidence="0.9993114">We present a statistical parsing framework for sentence-level sentiment classification in this article. Unlike previous works that use syntactic parsing results for sentiment analysis, we develop a statistical parser to directly analyze the sentiment structure of a sentence. We show that complicated phenomena in sentiment analysis (e.g., negation, intensification, and contrast) can be handled the same way as simple and straightforward sentiment expressions in a unified and probabilistic way. We formulate the sentiment grammar upon Context-Free Grammars (CFGs), and provide a formal description of the sentiment parsing framework. We develop the parsing model to obtain possible sentiment parse trees for a sentence, from which the polarity model is proposed to derive the sentiment strength and polarity, and the ranking model is dedicated to selecting the best sentiment tree. We train the parser directly from examples of sentences annotated only with sentiment polarity labels but without any syntactic annotations or polarity annotations of constituents within sentences. Therefore we can obtain training data easily. In particular, we train a sentiment parser, s.parser, from a large amount of review sentences with users’ ratings as rough sentiment polarity labels. Extensive experiments on existing benchmark data sets show significant improvements over baseline sentiment classification approaches.</abstract>
<keyword confidence="0.877912">Key Laboratory of Software Development Environment, Beihang University, XueYuan Road No.37, District, Beijing, P.R. China 100191. E-mail: kexu®nlsde.buaa.edu.cn.</keyword>
<note confidence="0.942454777777778">during internship at Microsoft Research. Language Computing Group, Microsoft Research Asia, Building 2, No. 5 Danling Street, Haidian Beijing, P.R. China 100080. E-mail: shujliu, author. Submission received: 10 December 2013; revised version received: 26 July 2014; accepted for publication: 28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media, LSM ’11,</booktitle>
<pages>30--38</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="17799" citStr="Agarwal et al. (2011)" startWordPosition="2578" endWordPosition="2581">al structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level prediction. T¨ackstr¨om and McDonald (2011a) presented a latent variable model for only using document-level annotations to learn sentence-level sentiment labels, and T¨ackstr¨om and McDonald (2011b) improved it by using a semi-supervised latent variable model to utilize manually crafted sentence labels. Agarwal et al. (2011) and Tu et al. (2012) explored part-of-speech tag features and tree-kernel. Wang and Manning (2012) used SVM built over Naive Bayes log-count ratios as feature values to classify polarity. They showed that SVM was better at full-length reviews, and Multinomial Naive Bayes was better at short-length reviews. Liu, Agam, and Grossman (2012) proposed a set of heuristic rules based on dependency structure to detect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to the</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Agarwal, Apoorv, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, LSM ’11, pages 30–38, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Ramakrishnan Srikant</author>
</authors>
<title>Fast algorithms for mining association rules in large databases.</title>
<date>1994</date>
<booktitle>In Proceedings of the 20th International Conference on Very Large Data Bases, VLDB ’94,</booktitle>
<pages>487--499</pages>
<contexts>
<context position="62981" citStr="Agrawal and Srikant 1994" startWordPosition="10329" endWordPosition="10332">entence. It should be noted that we still increase the count for #(not good,N ), because there is no negation rule covering the fragment not good. As shown in Algorithm 3, we learn the dictionary rules and their polarity probabilities by counting the frequencies in negative and positive classes. Only the fragments whose occurrence numbers are larger than threshold rf are kept. Moreover, we take the combination rules into consideration to acquire more reasonable GD. Notably, a subsequence of a frequent fragment must also be frequent. This is similar to the key insight in the Apriori algorithm (Agrawal and Srikant 1994). When we learn the dictionary rules, we can count the sentiment fragments from short to long, and prune the infrequent fragments in the early stages if any subsequence is not frequent. This pruning method accelerates the dictionary rule learning process and makes the procedure fit in memory. Algorithm 3 Dictionary Rule Learning Input: D: Data set, GC: Combination rules, rf: Frequency threshold Output: GD: Dictionary rules 1: function MINEDICTIONARYRULES(D, GC) 2: GD&apos; ← {} 3: for (s, Ls) in D do &gt; s : w0w1 · · · w|s|−1, Ls: Polarity label of s &gt; wj 4: for all i, j s.t. 0 ≤ i &lt; j ≤ |s |do i : w</context>
</contexts>
<marker>Agrawal, Srikant, 1994</marker>
<rawString>Agrawal, Rakesh and Ramakrishnan Srikant. 1994. Fast algorithms for mining association rules in large databases. In Proceedings of the 20th International Conference on Very Large Data Bases, VLDB ’94, pages 487–499,</rawString>
</citation>
<citation valid="false">
<location>San Francisco, CA.</location>
<marker></marker>
<rawString>San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="24322" citStr="Artzi and Zettlemoyer (2013)" startWordPosition="3558" endWordPosition="3561">al of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the </context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Artzi, Yoav and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junwei Bao</author>
<author>Nan Duan</author>
<author>Ming Zhou</author>
<author>Tiejun Zhao</author>
</authors>
<title>Knowledge-based question answering as machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>967--976</pages>
<contexts>
<context position="24486" citStr="Bao et al. (2014)" startWordPosition="3582" endWordPosition="3585">h iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the CYK algorithm that parses sentences in a bottom–up fashion. We use the log-linear model to score candidates generated by beam search. Instead of using question-answ</context>
</contexts>
<marker>Bao, Duan, Zhou, Zhao, 2014</marker>
<rawString>Bao, Junwei, Nan Duan, Ming Zhou, and Tiejun Zhao. 2014. Knowledge-based question answering as machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 967–976,</rawString>
</citation>
<citation valid="false">
<authors>
<author>Baltimore</author>
</authors>
<marker>Baltimore, </marker>
<rawString>Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence, AAAI’97/IAAI’97,</booktitle>
<pages>598--603</pages>
<location>Providence, RI.</location>
<contexts>
<context position="21966" citStr="Charniak 1997" startWordPosition="3204" endWordPosition="3205">r Sentiment Classification compositions by defining sentiment grammar and borrowing some techniques in the parsing research field. Moreover, our method uses symbolic representations instead of vector spaces. 2.2 Syntactic Parsing and Semantic Parsing The work presented in this article is close to traditional statistical parsing, as we borrow some algorithms to build the sentiment parser. Syntactic parsers are learned from the Treebank corpora, and find the most likely parse tree with the largest probability. In this article, we borrow some well-known techniques from syntactic parsing methods (Charniak 1997; Charniak and Johnson 2005; McDonald, Crammer, and Pereira 2005; K¨ubler, McDonald, and Nivre 2009), such as the CYK algorithm and Context-Free Grammar. These techniques are used to build the sentiment grammar and parsing model. They provide a natural way of defining the structure of sentiment trees and parse sentences to trees. The key difference lies in that our task is to calculate the polarity label of a sentence, instead of obtaining the parse tree. We only have sentencepolarity pairs as our training instances instead of annotated tree structures. Moreover, in the decoding process, our g</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, Eugene. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence, AAAI’97/IAAI’97, pages 598–603, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>173--180</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="21993" citStr="Charniak and Johnson 2005" startWordPosition="3206" endWordPosition="3209">ssification compositions by defining sentiment grammar and borrowing some techniques in the parsing research field. Moreover, our method uses symbolic representations instead of vector spaces. 2.2 Syntactic Parsing and Semantic Parsing The work presented in this article is close to traditional statistical parsing, as we borrow some algorithms to build the sentiment parser. Syntactic parsers are learned from the Treebank corpora, and find the most likely parse tree with the largest probability. In this article, we borrow some well-known techniques from syntactic parsing methods (Charniak 1997; Charniak and Johnson 2005; McDonald, Crammer, and Pereira 2005; K¨ubler, McDonald, and Nivre 2009), such as the CYK algorithm and Context-Free Grammar. These techniques are used to build the sentiment grammar and parsing model. They provide a natural way of defining the structure of sentiment trees and parse sentences to trees. The key difference lies in that our task is to calculate the polarity label of a sentence, instead of obtaining the parse tree. We only have sentencepolarity pairs as our training instances instead of annotated tree structures. Moreover, in the decoding process, our goal is to compute correct p</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05, pages 173–180, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="34225" citStr="Chiang (2007)" startWordPosition="5361" endWordPosition="5362"> we represent the form of an inference rule as: (r) H1 ... HK (8) [i,X,j] where, if all the terms r and Hk are true, then we can infer [i, X, j] as true. Here, r denotes a sentiment rule, and Hk denotes an item. When we refer to both rules and items, we employ the word terms. Theoretically, we can convert the sentiment rules to CNF versions, and then use the CYK algorithm to conduct parsing. Because the maximum number of non-terminal symbols in a rule is already restricted to two, we formulate the statistical sentiment parsing based on a customized CYK algorithm that is similar to the work of Chiang (2007). Let X, X1, X2 represent the non-terminals N or P; the inference rules for the statistical sentiment parsing are summarized in Figure 3. 3.3 Ranking Model The parsing model generates many candidate parse trees T(s) for a sentence s. The goal of the ranking model is to score and rank these parse trees. The sentiment tree with the highest score is treated as the best representation for sentence s. We extract a feature vector 4)(s, t) E Rd for the specific sentence-tree pair (s, t), where t E T(s) is the parse tree. Let * E Rd be the parameter vector for the features. We use the log-linear model</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>Chiang, David. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>793--801</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="5629" citStr="Choi and Cardie 2008" startWordPosition="805" endWordPosition="808"> contrast] The negation expressions, intensification modifiers, and the contrastive conjunction can change the polarity (Examples (1), (3), (4), (5)), strength (Examples (2), (3), (5)), or both (Examples (3), (5)) of the sentiment of the sentences. We do not need any detailed explanations here as they can be commonly found and easily understood in people’s 1 http://twitter.com. 2 http://www.imdb.com. 266 Dong et al. A Statistical Parsing Framework for Sentiment Classification daily lives. Existing works to address these issues usually rely on syntactic parsing results either used as features (Choi and Cardie 2008; Moilanen, Pulman, and Zhang 2010) in learning-based methods or hand-crafted rules (Moilanen and Pulman 2007; Jia, Yu, and Meng 2009; Klenner, Petrakis, and Fahrni 2009; Liu and Seneff 2009) in lexiconbased methods. However, even with the difficulty and feasibility of deriving the sentiment structure from syntactic parsing results put aside, it is an even more challenging task to generate stable and reliable parsing results for text that is ungrammatical in nature and has a high ratio of out-of-vocabulary words. The accuracy of the linguistic parsers trained on standard data sets (e.g., the P</context>
<context position="39968" citStr="Choi and Cardie 2008" startWordPosition="6432" endWordPosition="6435">n as 00 + �K �0kP(Xk|wjkik) (15) P(X |wj i) = h k=1 = 1 1 + exp &lt; − (00 + Ek=1 0kP(Xk |wik)) } where h(x) = 1+exp {−x} is the logistic function, K is the number of non-terminals in 1 a rule, and 00, ... , 0K are the parameters that are learned from data. As a concrete example, if the span wji can match N -+ not P and P � wji+1, the inference rule with the polarity model is defined as [i, N, j] N -+ not P [i + 1, P, j]Φ1 �(16) P(N|wji) = h(00 + 01P(P|wji+1)) P(P|wji) = 1 − P(N|wji) where polarity probability is calculated by P(N|wji) = h(00 + 01P(P|wji+1)). To tackle negation, switch negation (Choi and Cardie 2008; Saur´ı 2008) simply reverses the sentiment polarity and corresponding sentiment strength. However, consider not great and not good; flipping polarity directly makes not good more positive than not great, which is unreasonable. Another potential problem of switch negation is that negative polarity items interact with intensifiers in undesirable ways (Kennedy and Inkpen 2006). For example, not very good turns out to be even more negative than not good, given the fact that very good is more positive than good. Therefore, Taboada et al. (2011) argue that shift negation is a better way to handle </context>
<context position="101808" citStr="Choi and Cardie 2008" startWordPosition="16745" endWordPosition="16748">and meet people’s intuitions. However, there are also some negative examples caused by “false subjective.” For instance, the neutral phrase to pay it tends to appear in negative sentences, and it is learned as a negative phrase. This makes sense for the data distribution, but it may lead to the mismatch for the combination rules. In Figure 13, we show the polarity model of some combination rules learned from the data set RT-C. The first two examples are negation rules. We find that both switch negation and shift negation exist in data, instead of using only one negation type in previous work (Choi and Cardie 2008; Saur´ı 2008; Taboada et al. 2011). For the rule “N → i do not P,” we find that it is a switch negation rule. This rule reverses the polarity and the corresponding polarity strength. For instance, the i do not like it very much is more negative than the i do not like it. As shown in Figure 13b, the “N → is not P” is a shift negation that reduces a fixed polarity strength to reverse the original polarity. Specifically, the is not good is more negative than the is not great, as described in Section 3.4. We have a similar conclusion for the next two weaken rules. As illustrated in Figure 13c, th</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Choi, Yejin and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 793–801, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>590--598</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="14474" citStr="Choi and Cardie (2009" startWordPosition="2102" endWordPosition="2105">ity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2-based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Kres</context>
<context position="81736" citStr="Choi and Cardie (2009" startWordPosition="13462" endWordPosition="13465">ted by P(s) = flis|0 1 P (wi|wo 1), where wi−1 0 denotes the word sequence w0 ... wi−1. We use Good-Turing smoothing (Good 1953) to overcome sparsity when estimating the probability of high-order n-gram. We train language models on negative and positive sentences separately. For a sentence, its polarity is determined by comparing the probabilities calculated from the positive and negative language models. The unknown-word token is treated as a regular word (denoted by &lt;UNK&gt;). The SRI Language Modeling Toolkit (Stolcke 2002) is used in our experiment. Voting-w/Rev: This approach is proposed by Choi and Cardie (2009b), and is used as a baseline in Nakagawa, Inui, and Kurohashi (2010). The polarity of a subjective sentence is decided by the voting of each phrase’s prior polarity. The polarity of phrases that have odd numbers of negation phrases in their ancestors is reversed. The results are reported by Nakagawa, Inui, and Kurohashi (2010). HardRule: This baseline method is compared by Nakagawa, Inui, and Kurohashi (2010). The polarity of a subjective sentence is deterministically decided based on rules, by considering the sentiment polarity of dependency subtrees. The polarity of a modifier is reversed i</context>
</contexts>
<marker>Choi, Cardie, 2009</marker>
<rawString>Choi, Yejin and Claire Cardie. 2009a. Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 – Volume 2, EMNLP ’09, pages 590–598, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>590--598</pages>
<contexts>
<context position="14474" citStr="Choi and Cardie (2009" startWordPosition="2102" endWordPosition="2105">ity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2-based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Kres</context>
<context position="81736" citStr="Choi and Cardie (2009" startWordPosition="13462" endWordPosition="13465">ted by P(s) = flis|0 1 P (wi|wo 1), where wi−1 0 denotes the word sequence w0 ... wi−1. We use Good-Turing smoothing (Good 1953) to overcome sparsity when estimating the probability of high-order n-gram. We train language models on negative and positive sentences separately. For a sentence, its polarity is determined by comparing the probabilities calculated from the positive and negative language models. The unknown-word token is treated as a regular word (denoted by &lt;UNK&gt;). The SRI Language Modeling Toolkit (Stolcke 2002) is used in our experiment. Voting-w/Rev: This approach is proposed by Choi and Cardie (2009b), and is used as a baseline in Nakagawa, Inui, and Kurohashi (2010). The polarity of a subjective sentence is decided by the voting of each phrase’s prior polarity. The polarity of phrases that have odd numbers of negation phrases in their ancestors is reversed. The results are reported by Nakagawa, Inui, and Kurohashi (2010). HardRule: This baseline method is compared by Nakagawa, Inui, and Kurohashi (2010). The polarity of a subjective sentence is deterministically decided based on rules, by considering the sentiment polarity of dependency subtrees. The polarity of a modifier is reversed i</context>
</contexts>
<marker>Choi, Cardie, 2009</marker>
<rawString>Choi, Yejin and Claire Cardie. 2009b. Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 – Volume 2, pages 590–598, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Hierarchical sequential learning for extracting opinions and their attributes.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>269--274</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="18828" citStr="Choi and Cardie 2010" startWordPosition="2734" endWordPosition="2737">tect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to these models, we derive polarity labels from tree structures parsed by the sentiment grammar. There have been several attempts to assume that the problem of sentiment analysis is compositional. Sentiment classification can be solved by deriving the sentiment of a complex constituent (sentence) from the sentiment of small units (words and phrases) (Moilanen and Pulman 2007; Klenner, Petrakis, and Fahrni 2009; Choi and Cardie 2010; Nakagawa, Inui, and Kurohashi 2010). Moilanen and Pulman (2007) proposed using delicate written linguistic patterns as heuristic decision rules when computing the sentiment from individual words to phrases and finally to the sentence. The manually compiled rules were powerful enough to discriminate between the different sentiments in effective remedies (positive) / effective torture (negative), and in too colorful (negative) and too sad (negative). Nakagawa, Inui, and Kurohashi (2010) leveraged a conditional random field model to calculate the sentiment of all the parsed elements in the depe</context>
</contexts>
<marker>Choi, Cardie, 2010</marker>
<rawString>Choi, Yejin and Claire Cardie. 2010. Hierarchical sequential learning for extracting opinions and their attributes. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 269–274, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Three models for the description of language.</title>
<date>1956</date>
<journal>IRE Transactions on Information Theory,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="29105" citStr="Chomsky 1956" startWordPosition="4353" endWordPosition="4354">ure 2 Sentiment structure for the sentence The movie is not very good, but i still like it. The rules used in the derivation process include {P -+ the movie is; P -+ good; P -+ i still like it; P -+ very P; N -+ not P; N -+ PN; N -+ NE; E -+ ,; P -+ N but P; S - +P}. In the following sections, we first provide a formal description of the sentiment grammar in Section 3.1. We then present the details of the parsing model in Section 3.2, the ranking model in Section 3.3, and the polarity model in Section 3.4. 3.1 Sentiment Grammar We develop the sentiment grammar upon CFG (Context-Free Grammar) (Chomsky 1956). Let 9 =&lt; V, E, S, R &gt; denote a CFG, where V is a finite set of non-terminals, E is a finite set of terminals (disjointed from V), S E V is the start symbol, and R is a set of rewrite rules (or production rules) of the form A -+ c where A E V and c E (V U E)∗. We use 9s =&lt; Vs, Es, S, Rs &gt; to denote the sentiment grammar in this article. Table 1 Parsing process for the sentence The movie is not very good, but i still like it. [i, Y, j] represents the text spanning from i to j is derived to symbol Y. N and P are non-terminals in the sentiment grammar, and N and P represent polarities of sentime</context>
</contexts>
<marker>Chomsky, 1956</marker>
<rawString>Chomsky, Noam. 1956. Three models for the description of language. IRE Transactions on Information Theory, 2(3):113–124.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Clarke</author>
</authors>
<institution>Dan Goldwasser,</institution>
<marker>Clarke, </marker>
<rawString>Clarke, James, Dan Goldwasser,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10,</booktitle>
<pages>18--27</pages>
<location>Stroudsburg, PA.</location>
<marker>Chang, Roth, 2010</marker>
<rawString>Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10, pages 18–27, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cocke</author>
</authors>
<title>Programming Languages and Their Compilers: Preliminary Notes.</title>
<date>1969</date>
<booktitle>Courant Institute of Mathematical Sciences,</booktitle>
<location>New York University.</location>
<contexts>
<context position="8509" citStr="Cocke 1969" startWordPosition="1249" endWordPosition="1250">ve opinion. We can then use these two fragments and the overall negative opinion of the sentence to deduce sentiment rules automatically from data. These sentiment fragments and rules can be used to analyze the sentiment structure for new sentences. In this article, we propose a statistical parsing framework to directly analyze the structure of a sentence from the perspective of sentiment analysis. Specifically, we formulate a Context-Free Grammar (CFG)–based sentiment grammar. We then develop a statistical parser to derive the sentiment structure of a sentence. We leverage the CYK algorithm (Cocke 1969; Younger 1967; Kasami 1965) to conduct bottom–up parsing, and use dynamic programming to accelerate computation. Meanwhile, we propose using the polarity model to derive sentiment strength and polarity of a sentiment parse tree, and the ranking model to select the best one from the sentiment parsing results. We train the parser directly from examples of sentences annotated with sentiment polarity labels instead of syntactic annotations and polarity annotations of constituents within sentences. Therefore we can obtain training data easily. In particular, we train a sentiment parser, named s.pa</context>
</contexts>
<marker>Cocke, 1969</marker>
<rawString>Cocke, John. 1969. Programming Languages and Their Compilers: Preliminary Notes. Courant Institute of Mathematical Sciences, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac G Councill</author>
<author>Ryan McDonald</author>
<author>Leonid Velikovich</author>
</authors>
<title>What’s great and what’s not: Learning to classify the scope of negation for improved sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, NeSp-NLP ’10,</booktitle>
<pages>51--59</pages>
<location>Stroudsburg, PA.</location>
<marker>Councill, McDonald, Velikovich, 2010</marker>
<rawString>Councill, Isaac G., Ryan McDonald, and Leonid Velikovich. 2010. What’s great and what’s not: Learning to classify the scope of negation for improved sentiment analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, NeSp-NLP ’10, pages 51–59, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Vibhu Mittal</author>
<author>Mayur Datar</author>
</authors>
<title>Comparative experiments on sentiment classification for online product reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence -Volume 2, AAAI’06,</booktitle>
<pages>1--265</pages>
<location>Boston, MA.</location>
<marker>Cui, Mittal, Datar, 2006</marker>
<rawString>Cui, Hang, Vibhu Mittal, and Mayur Datar. 2006. Comparative experiments on sentiment classification for online product reviews. In Proceedings of the 21st National Conference on Artificial Intelligence -Volume 2, AAAI’06, pages 1,265–1,270, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Davidov, Dmitry, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marie-Catherine Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<tech>de</tech>
<pages>241--249</pages>
<location>Stroudsburg, PA.</location>
<marker>Marneffe, Manning, </marker>
<rawString>pages 241–249, Stroudsburg, PA. de Marneffe, Marie-Catherine, Christopher D. Manning, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Potts</author>
</authors>
<title>was it good? it was provocative.” Learning the meaning of scalar adjectives.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>167--176</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="14789" citStr="Potts (2010)" startWordPosition="2152" endWordPosition="2153">n of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2-based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Krestel and Siersdorfer (2013) generated topicdependent lexicons from review articles by incorporating topic and rating probabilities and defined the polarity strength based on the results. In this article, the lexical relations defined in WordNet are not used because of its coverage. Furthermore, most of these method</context>
</contexts>
<marker>Potts, 2010</marker>
<rawString>Christopher Potts. 2010. “was it good? it was provocative.” Learning the meaning of scalar adjectives. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 167–176, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Dong</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
<author>Ke Xu</author>
</authors>
<title>Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis.</title>
<date>2014</date>
<booktitle>In AAAI Conference on Artificial Intelligence,</booktitle>
<pages>1--537</pages>
<location>Quebec.</location>
<contexts>
<context position="21010" citStr="Dong et al. (2014)" startWordPosition="3064" endWordPosition="3067">he inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most previous methods are either rigid in terms of handcrafted rules, or sensitive to the performance of existing syntactic parsers they use. This article addresses sentiment 270 Dong et al. A Statistical Parsing Framework for Sentiment Classification compositions by defining sentiment grammar and borrowing some techniques in the parsing research field. Moreover, our method uses symbolic representations instead of vector spaces. 2.2 Syntactic Parsing and Semantic Parsing The wo</context>
<context position="103260" citStr="Dong et al. (2014)" startWordPosition="17011" endWordPosition="17014">it of N” is a percentage intensification rule, which scales polarity intensity by a percentage. It reduces more strength for stronger polarity. The last two rules in Figure 13e and Figure 13f are strengthen rules. Both “P → lot of P” and “N → N terribly” increase the polarity strength of the sub-fragments. These cases indicate that it is necessary to learn how the context performs compositionality from data. In order to capture the compositionality for different rules, we define the polarity model and learn parameters for each rule. This also agrees with the models of Socher et al. (2012) and Dong et al. (2014), 301 Figure 13 Illustration of the polarity model for combination rules: (a)(b) Negation rule. (c)(d) Weaken rule. (e)(f) Strengthen rule. The labels of axes represent the corresponding polarity labels, the red points are the training instances, and the blue lines are the regression results for the polarity model. N P P Computational Linguistics Volume 41, Number 2 0.50.5 0.6 0.7 0.8 0.9 1.0 P (a) N → i do not P 0.50.5 0.6 0.7 0.8 0.9 1.0 P (b) N → is not P. 0.9 N → i do not P 1.0 0.9 N → is not P . 1.0 0.6 0.50.5 0.6 0.7 0.8 0.9 1.0 P (c) P → P actress 0.6 0.50.5 0.6 0.7 0.8 0.9 1.0 N (d) N </context>
</contexts>
<marker>Dong, Wei, Zhou, Xu, 2014</marker>
<rawString>Dong, Li, Furu Wei, Ming Zhou, and Ke Xu. 2014. Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis. In AAAI Conference on Artificial Intelligence, pages 1,537–1,543, Quebec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>Duchi, John, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rong-En Fan</author>
</authors>
<location>Kai-Wei Chang,</location>
<marker>Fan, </marker>
<rawString>Fan, Rong-En, Kai-Wei Chang,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<marker>Hsieh, Wang, Lin, 2008</marker>
<rawString>Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irving John Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<pages>40--3</pages>
<contexts>
<context position="81243" citStr="Good 1953" startWordPosition="13391" endWordPosition="13392">mial Naive Bayes (MNB) often outperforms SVM for sentence-level sentiment classification. We utilize Laplace smoothing (Manning, Raghavan, and Sch¨utze 2008) to tackle the zero probability problem. High order n-gram (1 &lt; n &lt; m) features are considered in the experiments. LM-m: Language Model (LM) is a generative model calculating the probability of word sequences. It is used for sentiment analysis in Cui, Mittal, and Datar (2006). Probability of generating sentence s is calculated by P(s) = flis|0 1 P (wi|wo 1), where wi−1 0 denotes the word sequence w0 ... wi−1. We use Good-Turing smoothing (Good 1953) to overcome sparsity when estimating the probability of high-order n-gram. We train language models on negative and positive sentences separately. For a sentence, its polarity is determined by comparing the probabilities calculated from the positive and negative language models. The unknown-word token is treated as a regular word (denoted by &lt;UNK&gt;). The SRI Language Modeling Toolkit (Stolcke 2002) is used in our experiment. Voting-w/Rev: This approach is proposed by Choi and Cardie (2009b), and is used as a baseline in Nakagawa, Inui, and Kurohashi (2010). The polarity of a subjective sentenc</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, Irving John. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3-4):237–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="32760" citStr="Goodman 1999" startWordPosition="5074" endWordPosition="5075">In order to tackle the OOV problem, we treat a text span that consists of OOV words as empty text span, and derive them to E. The OOV text spans are combined with other text spans without considering their sentiment information. Finally, each sentence is derived to the symbol S using the start rules that are the beginnings of derivations. We can use the sentiment grammar to compactly describe the derivation process of a sentence. 3.2 Parsing Model We present the formal description of the statistical sentiment parsing model following deductive proof systems (Shieber, Schabes, and Pereira 1995; Goodman 1999) as used in traditional syntactic parsing. For a concrete example, (6) [i, A, j] (A → BC) [i, B, k] [k, C, j] 274 Dong et al. A Statistical Parsing Framework for Sentiment Classification which represents if we have the rule A -+ BC and B ∗ wki and C ∗ wjk (∗ is used to represent the reflexive and transitive closure of immediate derivation), then we can obtain A ∗ wji . By adding a unary rule (7) [i, A, j] with the binary rule in Equation (6), we can express the standard CYK algorithm for CFG in Chomsky Normal Form (CNF). And the goal is [0, S, n], in which S is the start symbol and n is the le</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Goodman, Joshua. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Less grammar, more features.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>228--237</pages>
<location>Baltimore, MD.</location>
<marker>Hall, Durrett, Klein, 2014</marker>
<rawString>Hall, David, Greg Durrett, and Dan Klein. 2014. Less grammar, more features. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 228–237, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, ACL ’98,</booktitle>
<pages>174--181</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="13460" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="1962" endWordPosition="1965">they suffer from coverage and domain adaption problems. Moreover, lexicons are often built and used without considering the context (Wilson, Wiebe, and Hoffmann 2009). Also, hand-crafted rules are often matched heuristically. The sentiment dictionaries used for lexicon-based sentiment analysis can be created manually, or automatically using seed words to expand the list of words. Kamps et al. (2004) and Williams and Anand (2009) used various lexical relations (such as synonym and antonym relations) in WordNet to expend a set of seed words. Some other methods learn lexicons from data directly. Hatzivassiloglou and McKeown (1997) used a log-linear regression model with conjunction constraints to predict whether conjoined adjectives have similar or different polarities. Combining conjunction constraints across many adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polar</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Hatzivassiloglou, Vasileios and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, ACL ’98, pages 174–181, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lifeng Jia</author>
<author>Clement Yu</author>
<author>Weiyi Meng</author>
</authors>
<title>The effect of negation on sentiment analysis and retrieval effectiveness.</title>
<date>2009</date>
<booktitle>In Conference on Information and Knowledge Management,</booktitle>
<pages>1--827</pages>
<marker>Jia, Yu, Meng, 2009</marker>
<rawString>Jia, Lifeng, Clement Yu, and Weiyi Meng. 2009. The effect of negation on sentiment analysis and retrieval effectiveness. In Conference on Information and Knowledge Management, pages 1,827–1,830,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building lexicon for sentiment analysis from massive collection of HTML documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1--075</pages>
<location>Prague.</location>
<contexts>
<context position="14635" citStr="Kaji and Kitsuregawa (2007)" startWordPosition="2127" endWordPosition="2130">akamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2-based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Krestel and Siersdorfer (2013) generated topicdependent lexicons from review articles by incorporating topic and rating probabilities and defined the polarity streng</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>Hong Kong. Kaji, Nobuhiro and Masaru Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of HTML documents. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1,075–1,083, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Robert J Mokken</author>
<author>Maarten Marx</author>
<author>Maarten de Rijke</author>
</authors>
<title>Using WordNet to measure semantic orientation of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), volume IV,</booktitle>
<pages>1--115</pages>
<location>Paris.</location>
<marker>Kamps, Mokken, Marx, de Rijke, 2004</marker>
<rawString>Kamps, Jaap, Robert J. Mokken, Maarten Marx, and Maarten de Rijke. 2004. Using WordNet to measure semantic orientation of adjectives. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), volume IV, pages 1,115–1,118, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Tetsuya Nasukawa</author>
</authors>
<title>Fully automatic lexicon expansion for domain-oriented sentiment analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>355--363</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="14277" citStr="Kanayama and Nasukawa (2006)" startWordPosition="2074" endWordPosition="2077">adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2-based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas,</context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>Kanayama, Hiroshi and Tetsuya Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 355–363, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadao Kasami</author>
</authors>
<title>An efficient recognition and syntax-analysis algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical report AFCRL-65-758,</tech>
<institution>Air Force Cambridge Research Lab,</institution>
<location>Bedford, MA.</location>
<contexts>
<context position="8537" citStr="Kasami 1965" startWordPosition="1253" endWordPosition="1254">e these two fragments and the overall negative opinion of the sentence to deduce sentiment rules automatically from data. These sentiment fragments and rules can be used to analyze the sentiment structure for new sentences. In this article, we propose a statistical parsing framework to directly analyze the structure of a sentence from the perspective of sentiment analysis. Specifically, we formulate a Context-Free Grammar (CFG)–based sentiment grammar. We then develop a statistical parser to derive the sentiment structure of a sentence. We leverage the CYK algorithm (Cocke 1969; Younger 1967; Kasami 1965) to conduct bottom–up parsing, and use dynamic programming to accelerate computation. Meanwhile, we propose using the polarity model to derive sentiment strength and polarity of a sentiment parse tree, and the ranking model to select the best one from the sentiment parsing results. We train the parser directly from examples of sentences annotated with sentiment polarity labels instead of syntactic annotations and polarity annotations of constituents within sentences. Therefore we can obtain training data easily. In particular, we train a sentiment parser, named s.parser, from a large number of</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Kasami, Tadao. 1965. An efficient recognition and syntax-analysis algorithm for context-free languages. Technical report AFCRL-65-758, Air Force Cambridge Research Lab, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In ACL 2006: Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the ACL,</booktitle>
<pages>913--920</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="23256" citStr="Kate and Mooney 2006" startWordPosition="3398" endWordPosition="3401"> latent sentiment trees. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) pr</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Kate, Rohit J. and Raymond J. Mooney. 2006. Using string-kernels for learning semantic parsers. In ACL 2006: Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the ACL, pages 913–920, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment classification of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<pages>22--110</pages>
<contexts>
<context position="40346" citStr="Kennedy and Inkpen 2006" startWordPosition="6486" endWordPosition="6489">s defined as [i, N, j] N -+ not P [i + 1, P, j]Φ1 �(16) P(N|wji) = h(00 + 01P(P|wji+1)) P(P|wji) = 1 − P(N|wji) where polarity probability is calculated by P(N|wji) = h(00 + 01P(P|wji+1)). To tackle negation, switch negation (Choi and Cardie 2008; Saur´ı 2008) simply reverses the sentiment polarity and corresponding sentiment strength. However, consider not great and not good; flipping polarity directly makes not good more positive than not great, which is unreasonable. Another potential problem of switch negation is that negative polarity items interact with intensifiers in undesirable ways (Kennedy and Inkpen 2006). For example, not very good turns out to be even more negative than not good, given the fact that very good is more positive than good. Therefore, Taboada et al. (2011) argue that shift negation is a better way to handle polarity negation. Instead of reversing polarity strength, shift negation shifts it toward the opposite polarity by 278 Dong et al. A Statistical Parsing Framework for Sentiment Classification a fixed amount. This method can partially avoid the aforementioned two problems. However, they set the parameters manually, which might not be reliable and extensible enough to a new da</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Kennedy, Alistair and Diana Inkpen. 2006. Sentiment classification of movie reviews using contextual valence shifters. Computational Intelligence, 22:110–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="20646" citStr="Klein and Manning 2003" startWordPosition="3008" endWordPosition="3011">ey regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most previous methods are either rigid in terms of handcrafted rules, or sensitive to the performance of existing synt</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -Volume 1, ACL ’03, pages 423–430, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Klenner</author>
<author>Stefanos Petrakis</author>
<author>Angela Fahrni</author>
</authors>
<title>Robust compositional polarity classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP-2009,</booktitle>
<pages>180--184</pages>
<location>Borovets.</location>
<marker>Klenner, Petrakis, Fahrni, 2009</marker>
<rawString>Klenner, Manfred, Stefanos Petrakis, and Angela Fahrni. 2009. Robust compositional polarity classification. In Proceedings of the International Conference RANLP-2009, pages 180–184, Borovets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Krestel</author>
<author>Stefan Siersdorfer</author>
</authors>
<title>Generating contextualized sentiment lexica based on latent topics and user ratings.</title>
<date>2013</date>
<booktitle>In Proceedings of the 24th ACM Conference on Hypertext and Social Media, HT ’13,</booktitle>
<pages>129--138</pages>
<location>New York, NY.</location>
<contexts>
<context position="15100" citStr="Krestel and Siersdorfer (2013)" startWordPosition="2198" endWordPosition="2201">2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2-based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Krestel and Siersdorfer (2013) generated topicdependent lexicons from review articles by incorporating topic and rating probabilities and defined the polarity strength based on the results. In this article, the lexical relations defined in WordNet are not used because of its coverage. Furthermore, most of these methods define different criteria to propagate polarity information of seeds, or use optimization algorithms and sentence-level sentiment labels to learn polarity strength values. Their goal is to balance the precision and recall of learned lexicons. We also learn the polarity strength values of phrases from data. H</context>
</contexts>
<marker>Krestel, Siersdorfer, 2013</marker>
<rawString>Krestel, Ralf and Stefan Siersdorfer. 2013. Generating contextualized sentiment lexica based on latent topics and user ratings. In Proceedings of the 24th ACM Conference on Hypertext and Social Media, HT ’13, pages 129–138, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>754--765</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="24144" citStr="Krishnamurthy and Mitchell (2012)" startWordPosition="3532" endWordPosition="3535">ount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a n</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Krishnamurthy, Jayant and Tom M. Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 754–765, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<date>2009</date>
<booktitle>Dependency parsing. Synthesis Lectures on Human Language Technologies,</booktitle>
<volume>1</volume>
<issue>1</issue>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>K¨ubler, Sandra, Ryan McDonald, and Joakim Nivre. 2009. Dependency parsing. Synthesis Lectures on Human Language Technologies, 1(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>An extended ghkm algorithm for inducing Lambda-SCFG.</title>
<date>2013</date>
<booktitle>In Conference on Artificial Intelligence,</booktitle>
<pages>605--611</pages>
<location>Bellevue, WA.</location>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Li, Peng, Yang Liu, and Maosong Sun. 2013. An extended ghkm algorithm for inducing Lambda-SCFG. In Conference on Artificial Intelligence, pages 605–611, Bellevue, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<marker>Liang, Jordan, Klein, 2013</marker>
<rawString>Liang, Percy, Michael I. Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="2401" citStr="Liu 2012" startWordPosition="325" endWordPosition="326">l.com; kexu®nlsde.buaa.edu.cn. ** Contribution during internship at Microsoft Research. † Natural Language Computing Group, Microsoft Research Asia, Building 2, No. 5 Danling Street, Haidian District, Beijing, P.R. China 100080. E-mail: {fuwei, shujliu, mingzhou}@microsoft.com. ‡ Corresponding author. Submission received: 10 December 2013; revised version received: 26 July 2014; accepted for publication: 28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 20</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Liu, Bing. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingjing Liu</author>
<author>Stephanie Seneff</author>
</authors>
<title>Review sentiment scoring via a Computational Linguistics Volume 41, Number 2 parse-and-paraphrase paradigm.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>161--169</pages>
<contexts>
<context position="5820" citStr="Liu and Seneff 2009" startWordPosition="834" endWordPosition="837">th (Examples (3), (5)) of the sentiment of the sentences. We do not need any detailed explanations here as they can be commonly found and easily understood in people’s 1 http://twitter.com. 2 http://www.imdb.com. 266 Dong et al. A Statistical Parsing Framework for Sentiment Classification daily lives. Existing works to address these issues usually rely on syntactic parsing results either used as features (Choi and Cardie 2008; Moilanen, Pulman, and Zhang 2010) in learning-based methods or hand-crafted rules (Moilanen and Pulman 2007; Jia, Yu, and Meng 2009; Klenner, Petrakis, and Fahrni 2009; Liu and Seneff 2009) in lexiconbased methods. However, even with the difficulty and feasibility of deriving the sentiment structure from syntactic parsing results put aside, it is an even more challenging task to generate stable and reliable parsing results for text that is ungrammatical in nature and has a high ratio of out-of-vocabulary words. The accuracy of the linguistic parsers trained on standard data sets (e.g., the Penn Treebank [Marcus, Marcinkiewicz, and Santorini 1993]) drops dramatically on user-generated-content (reviews, tweets, etc.), which is actually the prime focus of sentiment analysis algorit</context>
</contexts>
<marker>Liu, Seneff, 2009</marker>
<rawString>Liu, Jingjing and Stephanie Seneff. 2009. Review sentiment scoring via a Computational Linguistics Volume 41, Number 2 parse-and-paraphrase paradigm. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 161–169,</rawString>
</citation>
<citation valid="false">
<authors>
<author>Shizhu Liu</author>
</authors>
<location>Gady Agam, and</location>
<marker>Liu, </marker>
<rawString>Stroudsburg, PA. Liu, Shizhu, Gady Agam, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Grossman</author>
</authors>
<title>Generalized sentiment-bearing expression features for sentiment analysis.</title>
<date>2012</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>733--744</pages>
<location>Mumbai.</location>
<contexts>
<context position="18138" citStr="Grossman (2012)" startWordPosition="2632" endWordPosition="2633">ted a latent variable model for only using document-level annotations to learn sentence-level sentiment labels, and T¨ackstr¨om and McDonald (2011b) improved it by using a semi-supervised latent variable model to utilize manually crafted sentence labels. Agarwal et al. (2011) and Tu et al. (2012) explored part-of-speech tag features and tree-kernel. Wang and Manning (2012) used SVM built over Naive Bayes log-count ratios as feature values to classify polarity. They showed that SVM was better at full-length reviews, and Multinomial Naive Bayes was better at short-length reviews. Liu, Agam, and Grossman (2012) proposed a set of heuristic rules based on dependency structure to detect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to these models, we derive polarity labels from tree structures parsed by the sentiment grammar. There have been several attempts to assume that the problem of sentiment analysis is compositional. Sentiment classification can be solved by deriving the sentiment of a complex constituent (sentence) from the sentiment of small units (words and ph</context>
</contexts>
<marker>Grossman, 2012</marker>
<rawString>David A. Grossman. 2012. Generalized sentiment-bearing expression features for sentiment analysis. In International Conference on Computational Linguistics, pages 733–744, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Lu</author>
<author>Mal´u Castellanos</author>
<author>Umeshwar Dayal</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Automatic construction of a context-aware sentiment lexicon: an optimization approach.</title>
<date>2011</date>
<booktitle>In International World Wide Web Conference,</booktitle>
<pages>347--356</pages>
<location>Hyderabad.</location>
<contexts>
<context position="14497" citStr="Lu et al. (2011)" startWordPosition="2107" endWordPosition="2110">imilarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2-based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Krestel and Siersdorfer (20</context>
</contexts>
<marker>Lu, Castellanos, Dayal, Zhai, 2011</marker>
<rawString>Lu, Yue, Mal´u Castellanos, Umeshwar Dayal, and ChengXiang Zhai. 2011. Automatic construction of a context-aware sentiment lexicon: an optimization approach. In International World Wide Web Conference, pages 347–356, Hyderabad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>142--150</pages>
<contexts>
<context position="4045" citStr="Maas et al. 2011" startWordPosition="563" endWordPosition="566">en widely recognized that sentiment expressions are colloquial and evolve over time very frequently. Taking tweets from Twitter1 and movie reviews on IMDb2 as examples, people use very casual language as well as informal and new vocabulary to comment on general topics and movies. In practice, it is not feasible to create and maintain sentiment lexicons to capture sentiment expressions with high coverage. On the other hand, the learning-based approach relies on large annotated samples to overcome the vocabulary coverage and deals with variations of words in sentences. Human ratings in reviews (Maas et al. 2011) and emoticons in tweets (Davidov, Tsur, and Rappoport 2010; Zhao et al. 2012) are extensively used to collect a large number of training corpora to train the sentiment classifier. However, it is usually not easy to design effective features to build the classifier. Among others, unigrams have been reported as the most effective features (Pang, Lee, and Vaithyanathan 2002) in sentiment classification. Handling complicated expressions delivering people’s opinions is one of the most challenging problems in sentiment analysis. Compositionalities such as negation, intensification, contrast, and th</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Maas, Andrew L., Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 142–150,</rawString>
</citation>
<citation valid="false">
<location>Stroudsburg, PA.</location>
<marker></marker>
<rawString>Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Marcus, Mitchell P., Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shotaro Matsumoto</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentiment classification using word sub-sequences and dependency sub-trees.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD’05,</booktitle>
<pages>301--311</pages>
<marker>Matsumoto, Takamura, Okumura, 2005</marker>
<rawString>Matsumoto, Shotaro, Hiroya Takamura, and Manabu Okumura. 2005. Sentiment classification using word sub-sequences and dependency sub-trees. In Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD’05, pages 301–311, Hanoi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>91--98</pages>
<location>Stroudsburg, PA.</location>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, Ryan, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 91–98, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured models for fine-to-coarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>432--439</pages>
<location>Prague.</location>
<contexts>
<context position="17158" citStr="McDonald et al. (2007)" startWordPosition="2491" endWordPosition="2494">ated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level prediction. T¨ackstr¨om and McDonald (2011a) presented a latent variable model for only using document-level annotations to learn sentence-level sentiment labels, and T¨ackstr¨om and McDonald (2011b) improved it by using a semi-supervised latent variable model to utilize manually craft</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>McDonald, Ryan, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured models for fine-to-coarse sentiment analysis. In Proceedings of the Association for Computational Linguistics (ACL), pages 432–439, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment composition.</title>
<date>2007</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP</booktitle>
<pages>378--382</pages>
<location>Borovets.</location>
<contexts>
<context position="5738" citStr="Moilanen and Pulman 2007" startWordPosition="820" endWordPosition="823">e the polarity (Examples (1), (3), (4), (5)), strength (Examples (2), (3), (5)), or both (Examples (3), (5)) of the sentiment of the sentences. We do not need any detailed explanations here as they can be commonly found and easily understood in people’s 1 http://twitter.com. 2 http://www.imdb.com. 266 Dong et al. A Statistical Parsing Framework for Sentiment Classification daily lives. Existing works to address these issues usually rely on syntactic parsing results either used as features (Choi and Cardie 2008; Moilanen, Pulman, and Zhang 2010) in learning-based methods or hand-crafted rules (Moilanen and Pulman 2007; Jia, Yu, and Meng 2009; Klenner, Petrakis, and Fahrni 2009; Liu and Seneff 2009) in lexiconbased methods. However, even with the difficulty and feasibility of deriving the sentiment structure from syntactic parsing results put aside, it is an even more challenging task to generate stable and reliable parsing results for text that is ungrammatical in nature and has a high ratio of out-of-vocabulary words. The accuracy of the linguistic parsers trained on standard data sets (e.g., the Penn Treebank [Marcus, Marcinkiewicz, and Santorini 1993]) drops dramatically on user-generated-content (revie</context>
<context position="18770" citStr="Moilanen and Pulman 2007" startWordPosition="2725" endWordPosition="2728">d a set of heuristic rules based on dependency structure to detect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to these models, we derive polarity labels from tree structures parsed by the sentiment grammar. There have been several attempts to assume that the problem of sentiment analysis is compositional. Sentiment classification can be solved by deriving the sentiment of a complex constituent (sentence) from the sentiment of small units (words and phrases) (Moilanen and Pulman 2007; Klenner, Petrakis, and Fahrni 2009; Choi and Cardie 2010; Nakagawa, Inui, and Kurohashi 2010). Moilanen and Pulman (2007) proposed using delicate written linguistic patterns as heuristic decision rules when computing the sentiment from individual words to phrases and finally to the sentence. The manually compiled rules were powerful enough to discriminate between the different sentiments in effective remedies (positive) / effective torture (negative), and in too colorful (negative) and too sad (negative). Nakagawa, Inui, and Kurohashi (2010) leveraged a conditional random field model to calc</context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Moilanen, Karo and Stephen Pulman. 2007. Sentiment composition. In Proceedings of Recent Advances in Natural Language Processing (RANLP 2007), pages 378–382, Borovets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
<author>Yue Zhang</author>
</authors>
<title>Packed feelings and ordered sentiments: Sentiment parsing with quasi-compositional polarity sequencing and compression.</title>
<date>2010</date>
<booktitle>In Proceedings of the 1st Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2010) at the 19th European Conference on Artificial Intelligence (ECAI2010),</booktitle>
<pages>36--43</pages>
<location>Lisbon.</location>
<marker>Moilanen, Pulman, Zhang, 2010</marker>
<rawString>Moilanen, Karo, Stephen Pulman, and Yue Zhang. 2010. Packed feelings and ordered sentiments: Sentiment parsing with quasi-compositional polarity sequencing and compression. In Proceedings of the 1st Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2010) at the 19th European Conference on Artificial Intelligence (ECAI2010), pages 36–43, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrius Mudinas</author>
<author>Dell Zhang</author>
<author>Mark Levene</author>
</authors>
<title>Combining lexicon and learning based approaches for concept-level sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining, WISDOM ’12,</booktitle>
<pages>5--1</pages>
<location>New York, NY.</location>
<marker>Mudinas, Zhang, Levene, 2012</marker>
<rawString>Mudinas, Andrius, Dell Zhang, and Mark Levene. 2012. Combining lexicon and learning based approaches for concept-level sentiment analysis. In Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining, WISDOM ’12, pages 5:1–5:8, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFS with hidden variables.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>786--794</pages>
<location>Stroudsburg, PA.</location>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Nakagawa, Tetsuji, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using CRFS with hidden variables. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 786–794, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>271--278</pages>
<location>Barcelona.</location>
<contexts>
<context position="16786" citStr="Pang and Lee (2004)" startWordPosition="2438" endWordPosition="2441">er of classes in sentiment classification is smaller than that in topic-based text classification (Pang and Lee 2008). Pang, Lee, and Vaithyanathan (2002) investigated three machine learning methods to produce automated classifiers to generate class labels for movie reviews. They tested them on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM), and evaluated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, </context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Pang, Bo and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 271–278, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>115--124</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="73641" citStr="Pang and Lee 2005" startWordPosition="12184" endWordPosition="12187">over, the Stanford Sentiment Treebank5 contains polarity labels of all syntactically plausible phrases. In addition, we use the MPQA6 data set for the phrase-level task. We describe these data sets as follows. RT-C: 436,000 critic reviews from Rotten Tomatoes. It consists of 218,000 negative and 218,000 positive critic reviews. The average review length is 23.2 words. Critic reviews from Rotten Tomatoes contain a label (Rotten: Negative, Fresh: Positive) to indicate the polarity, which we use directly as the polarity label of corresponding reviews. PL05-C: The sentence polarity data set v1.0 (Pang and Lee 2005) contains 5,331 positive and 5,331 negative snippets written by critics from Rotten Tomatoes. This data set is widely used as the benchmark data set in the sentence-level polarity classification task. The data source is the same as RT-C. SST: The Stanford Sentiment Treebank (Socher et al. 2013) is built upon PL05-C. The sentences are parsed to parse trees. Then, 215,154 syntactically plausible phrases are extracted and annotated by workers from Amazon Mechanical Turk. The experimental settings of positive/negative classification for sentences are the same as in Socher et al. (2013). RT-U: 737,</context>
<context position="77133" citStr="Pang and Lee (2005)" startWordPosition="12734" endWordPosition="12737"> sentences in the data set, and |V |is the vocabulary size. Data Set Size #Negative #Positive lavg IVI RT-C 436,000 218,000 218,000 23.2 136,006 PL05-C 10,662 5,331 5,331 21.0 20,263 SST 98,796 42,608 56,188 7.5 16,372 RT-U 737,806 368,903 368,903 15.4 138,815 IMDB-U 600,000 300,000 300,000 6.6 83,615 MPQA 10,624 7,308 3,316 3.1 5,992 Table 3 shows the summary of these data sets, and all of them are publicly available athttp://goo.gl/WxTdPf. 5.1.2 Settings. To compare with other published results for PL05-C and MPQA, the training and testing regime (10-fold cross-validation) is the same as in Pang and Lee (2005), Nakagawa, Inui, and Kurohashi (2010) and Socher et al. (2011). For SST, the regime is the same as in Socher et al. (2013). We use C-TEST as the testing data for RT-C, and U-TEST as the testing data for RT-U and IMDB-U. There are a number of settings that have trade-offs in performance, computation, and the generalization power of our model. The best settings are chosen by a portion of training split data that serves as the validation set. We provide the performance comparisons using different experimental settings in Section 5.4. Number of training examples: The size of training data has bee</context>
<context position="80099" citStr="Pang and Lee 2005" startWordPosition="13200" endWordPosition="13203">to capture polarity information. According to the experimental results, as the maximum fragment length increases, the accuracy of sentiment classification increases. The maximum fragment length is set to 7 words in our experiments. 5.1.3 Sentiment Classification Methods for Comparison. We evaluate the proposed statistical sentiment parsing framework on the different data sets, and compare the results with some baselines and state-of-the-art sentiment classification methods described as follows. SVM-m: Support Vector Machine (SVM) achieves good performance in the sentiment classification task (Pang and Lee 2005). Though unigrams and bigrams are reported as the most effective features in existing work (Pang and Lee 2005), we use high-order n-gram (1 &lt; n &lt; m) features to conduct fair comparisons. Hereafter, m has the same meaning. We use LIBLINEAR (Fan et al. 2008) in our experiments because it can handle well the high feature dimension and a large number of training examples. We try different hyper-parameters C E {10−2,10−1, 1, 5,10, 20} for SVM, and select C on the validation set. MNB-m: As indicated in Wang and Manning (2012), Multinomial Naive Bayes (MNB) often outperforms SVM for sentence-level se</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Pang, Bo and Lillian Lee. 2005. Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 115–124, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="2390" citStr="Pang and Lee 2008" startWordPosition="321" endWordPosition="324">mail: donglixp®gmail.com; kexu®nlsde.buaa.edu.cn. ** Contribution during internship at Microsoft Research. † Natural Language Computing Group, Microsoft Research Asia, Building 2, No. 5 Danling Street, Haidian District, Beijing, P.R. China 100080. E-mail: {fuwei, shujliu, mingzhou}@microsoft.com. ‡ Corresponding author. Submission received: 10 December 2013; revised version received: 26 July 2014; accepted for publication: 28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaith</context>
<context position="16284" citStr="Pang and Lee 2008" startWordPosition="2370" endWordPosition="2373">gth values of phrases from data. However, our primary objective is to obtain correct sentence-level polarity labels, and use them to form the sentiment grammar. Learning-based sentiment analysis uses machine learning methods to classify sentences or documents into two (negative and positive) or three (negative, positive, and neutral) classes. Previous research has shown that sentiment classification is more difficult than traditional topic-based text classification, despite the fact that the number of classes in sentiment classification is smaller than that in topic-based text classification (Pang and Lee 2008). Pang, Lee, and Vaithyanathan (2002) investigated three machine learning methods to produce automated classifiers to generate class labels for movie reviews. They tested them on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM), and evaluated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve bet</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Pang, Bo and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing -Volume 10, EMNLP ’02,</booktitle>
<pages>79--86</pages>
<location>Stroudsburg, PA.</location>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine learning techniques. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing -Volume 10, EMNLP ’02, pages 79–86, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Annie Zaenen</author>
</authors>
<title>Contextual valence shifters.</title>
<date>2006</date>
<booktitle>Computing Attitude and Affect in Text: Theory and Applications.</booktitle>
<pages>1--10</pages>
<editor>In J. G. Shanahan, Y. Qu, and J. Wiebe, editors,</editor>
<publisher>Springer Netherlands,</publisher>
<contexts>
<context position="41427" citStr="Polanyi and Zaenen (2006)" startWordPosition="6658" endWordPosition="6661"> avoid the aforementioned two problems. However, they set the parameters manually, which might not be reliable and extensible enough to a new data set. Using the regression model, switch negation is captured by the negative scale item 0k (k &gt; 0), and shift negation is expressed by the shift item 00. The intensifiers are adjectives or adverbs that strengthen (amplifier) or decrease (downtoner) the semantic intensity of its neighboring item (Quirk 1985). For example, extremely good should obtain higher strength of positive polarity than good, because it is modified by the amplifier (extremely). Polanyi and Zaenen (2006) and Kennedy and Inkpen (2006) handle intensifiers by polarity addition and subtraction. This method, termed fixed intensification, increases a fixed amount of polarity for amplifiers and decreases for downtoners. Taboada et al. (2011) propose a method, called percentage intensification, to associate each intensification word with a percentage scale, which is larger than one for amplifiers, and less than one for downtoners. The regression model can capture these two methods to handle the intensification. The shift item 00 represents the polarity addition and subtraction directly, and the scale</context>
</contexts>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>Polanyi, Livia and Annie Zaenen. 2006. Contextual valence shifters. In J. G. Shanahan, Y. Qu, and J. Wiebe, editors, Computing Attitude and Affect in Text: Theory and Applications. Springer Netherlands, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language. Longman.</journal>
<contexts>
<context position="41257" citStr="Quirk 1985" startWordPosition="6635" endWordPosition="6636">ts it toward the opposite polarity by 278 Dong et al. A Statistical Parsing Framework for Sentiment Classification a fixed amount. This method can partially avoid the aforementioned two problems. However, they set the parameters manually, which might not be reliable and extensible enough to a new data set. Using the regression model, switch negation is captured by the negative scale item 0k (k &gt; 0), and shift negation is expressed by the shift item 00. The intensifiers are adjectives or adverbs that strengthen (amplifier) or decrease (downtoner) the semantic intensity of its neighboring item (Quirk 1985). For example, extremely good should obtain higher strength of positive polarity than good, because it is modified by the amplifier (extremely). Polanyi and Zaenen (2006) and Kennedy and Inkpen (2006) handle intensifiers by polarity addition and subtraction. This method, termed fixed intensification, increases a fixed amount of polarity for amplifiers and decreases for downtoners. Taboada et al. (2011) propose a method, called percentage intensification, to associate each intensification word with a percentage scale, which is larger than one for amplifiers, and less than one for downtoners. Th</context>
</contexts>
<marker>Quirk, 1985</marker>
<rawString>Quirk, R. 1985. A Comprehensive Grammar of the English Language. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge Raymond</author>
<author>J Mooney</author>
</authors>
<title>Discriminative reranking for semantic parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main Conference Poster Sessions, COLING-ACL ’06,</booktitle>
<pages>263--270</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="23281" citStr="Raymond and Mooney 2006" startWordPosition="3402" endWordPosition="3405">s. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration,</context>
</contexts>
<marker>Raymond, Mooney, 2006</marker>
<rawString>Raymond, Ruifang Ge and J. Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of the COLING/ACL on Main Conference Poster Sessions, COLING-ACL ’06, pages 263–270, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Robbins</author>
<author>S Monro</author>
</authors>
<title>A stochastic approximation method.</title>
<date>1951</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>22--400</pages>
<contexts>
<context position="54431" citStr="Robbins and Monro 1951" startWordPosition="8916" endWordPosition="8919"> kψk22 (22) O(ψ, T) = (s,Ls)ED TLs (s)00 To learn the parameters ψ, we use a gradient-based optimization method to maximize the objective function O(ψ, T). According to Wainwright and Jordan (2008), the derivative of the log-partition function is the expected feature vector ∂O(ψ,T) = E (Ep(t|s;TLs,ψ)[φ(s,t)] − Ep(t|s;T,ψ)[φ(s, t)]) − λψ (23) ∂ψ (s,Ls)ED TLs (s)00 where Ep(x)[f (x)] = Ex p(x)f(x) for discrete x. 4.1.3 Parameter Estimation. The objective function O(ψ, T) is not concave (nor convex), hence the optimization potentially results in a local optimum. Stochastic Gradient Descent (SGD; Robbins and Monro 1951) is a widely used optimization method. The SGD algorithm picks up a training instance randomly, and updates the parameter vector ψ according to ψj(t+1) = ψj(t) + α ( ∂ 0 ) |ψ=ψ(t)) (24) i where α is the learning rate, and ∂O(ψ) ∂ψj is the gradient of the objective function with respect to parameter ψj. The SGD is sensitive to α, and the learning rate is the same for all dimensions. As described in Section 4.1.1, we mix sparse features together with dense features. We want the learning rate to be different for each dimension. We use AdaGrad (Duchi, Hazan, and Singer 2011) to update the paramete</context>
</contexts>
<marker>Robbins, Monro, 1951</marker>
<rawString>Robbins, H. and S. Monro. 1951. A stochastic approximation method. Annals of Mathematical Statistics, 22:400–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
</authors>
<title>A Factuality Profiler for Eventualities in Text.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Brandeis University.</institution>
<marker>Saur´ı, 2008</marker>
<rawString>Saur´ı, Roser. 2008. A Factuality Profiler for Eventualities in Text. Ph.D. thesis, Brandeis University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Shieber, Stuart M., Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1–2):3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1--201</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="20177" citStr="Socher et al. (2012)" startWordPosition="2934" endWordPosition="2937"> in that it did not explicitly denote any sentiment designation to words or phrases in parse trees. Instead, it modeled their sentiment polarity as latent variables with a certain probability of being positive or negative. Councill, McDonald, and Velikovich (2010) used a conditional random field model informed by a dependency parser to detect the scope of negation for sentiment analysis. Some other methods model sentiment compositionality in the vector space. They regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained r</context>
<context position="83463" citStr="Socher et al. (2012)" startWordPosition="13731" endWordPosition="13734">ity of the whole sentence. The experimental results are reported by Nakagawa, Inui, and Kurohashi (2010). 294 Dong et al. A Statistical Parsing Framework for Sentiment Classification RAE-pretrain: Socher et al. (2011) introduce a framework based on recursive autoencoders to learn vector space representations for multi-word phrases and predict sentiment distributions for sentences. We use the results with pre-trained word vectors learned on Wikipedia, which leads to better results compared with randomized word vectors. We directly compare the results with those in Socher et al. (2011). MV-RNN: Socher et al. (2012) try to capture the compositional meaning of long phrases through matrix-vector recursive neural networks. This model assigns a vector and a matrix to every node in the parse tree. Matrices are regarded as operators, and vectors capture the meaning of phrases. The results are reported by Socher et al. (2012, 2013). s.parser-LongMatch: The longest matching rules are utilized in the decoding process. In other words, the derivations that contain the fewest rules are used for all text spans. In addition, the dictionary rules are preferred to the combination rules if both of them match the same tex</context>
<context position="103237" citStr="Socher et al. (2012)" startWordPosition="17006" endWordPosition="17009">fixed value. The “N → a bit of N” is a percentage intensification rule, which scales polarity intensity by a percentage. It reduces more strength for stronger polarity. The last two rules in Figure 13e and Figure 13f are strengthen rules. Both “P → lot of P” and “N → N terribly” increase the polarity strength of the sub-fragments. These cases indicate that it is necessary to learn how the context performs compositionality from data. In order to capture the compositionality for different rules, we define the polarity model and learn parameters for each rule. This also agrees with the models of Socher et al. (2012) and Dong et al. (2014), 301 Figure 13 Illustration of the polarity model for combination rules: (a)(b) Negation rule. (c)(d) Weaken rule. (e)(f) Strengthen rule. The labels of axes represent the corresponding polarity labels, the red points are the training instances, and the blue lines are the regression results for the polarity model. N P P Computational Linguistics Volume 41, Number 2 0.50.5 0.6 0.7 0.8 0.9 1.0 P (a) N → i do not P 0.50.5 0.6 0.7 0.8 0.9 1.0 P (b) N → is not P. 0.9 N → i do not P 1.0 0.9 N → is not P . 1.0 0.6 0.50.5 0.6 0.7 0.8 0.9 1.0 P (c) P → P actress 0.6 0.50.5 0.6 0</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Socher, Richard, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1,201–1,211, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>151--161</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="77196" citStr="Socher et al. (2011)" startWordPosition="12744" endWordPosition="12747">ta Set Size #Negative #Positive lavg IVI RT-C 436,000 218,000 218,000 23.2 136,006 PL05-C 10,662 5,331 5,331 21.0 20,263 SST 98,796 42,608 56,188 7.5 16,372 RT-U 737,806 368,903 368,903 15.4 138,815 IMDB-U 600,000 300,000 300,000 6.6 83,615 MPQA 10,624 7,308 3,316 3.1 5,992 Table 3 shows the summary of these data sets, and all of them are publicly available athttp://goo.gl/WxTdPf. 5.1.2 Settings. To compare with other published results for PL05-C and MPQA, the training and testing regime (10-fold cross-validation) is the same as in Pang and Lee (2005), Nakagawa, Inui, and Kurohashi (2010) and Socher et al. (2011). For SST, the regime is the same as in Socher et al. (2013). We use C-TEST as the testing data for RT-C, and U-TEST as the testing data for RT-U and IMDB-U. There are a number of settings that have trade-offs in performance, computation, and the generalization power of our model. The best settings are chosen by a portion of training split data that serves as the validation set. We provide the performance comparisons using different experimental settings in Section 5.4. Number of training examples: The size of training data has been widely recognized as one of the most important factors in mac</context>
<context position="83060" citStr="Socher et al. (2011)" startWordPosition="13671" endWordPosition="13674">de in a dependency tree. We use the results reported by Nakagawa, Inui, and Kurohashi (2010). Tree-CRF: Nakagawa, Inui, and Kurohashi (2010) present a dependency tree-based method using conditional random fields with hidden variables. In this model, the polarity of each dependency subtree is represented by a hidden variable. The value of the hidden variable of the root node is identified as the polarity of the whole sentence. The experimental results are reported by Nakagawa, Inui, and Kurohashi (2010). 294 Dong et al. A Statistical Parsing Framework for Sentiment Classification RAE-pretrain: Socher et al. (2011) introduce a framework based on recursive autoencoders to learn vector space representations for multi-word phrases and predict sentiment distributions for sentences. We use the results with pre-trained word vectors learned on Wikipedia, which leads to better results compared with randomized word vectors. We directly compare the results with those in Socher et al. (2011). MV-RNN: Socher et al. (2012) try to capture the compositional meaning of long phrases through matrix-vector recursive neural networks. This model assigns a vector and a matrix to every node in the parse tree. Matrices are reg</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Socher, Richard, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 151–161, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1--631</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="20536" citStr="Socher et al. (2013)" startWordPosition="2991" endWordPosition="2994">egation for sentiment analysis. Some other methods model sentiment compositionality in the vector space. They regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most pre</context>
<context position="73936" citStr="Socher et al. 2013" startWordPosition="12231" endWordPosition="12234">nd 218,000 positive critic reviews. The average review length is 23.2 words. Critic reviews from Rotten Tomatoes contain a label (Rotten: Negative, Fresh: Positive) to indicate the polarity, which we use directly as the polarity label of corresponding reviews. PL05-C: The sentence polarity data set v1.0 (Pang and Lee 2005) contains 5,331 positive and 5,331 negative snippets written by critics from Rotten Tomatoes. This data set is widely used as the benchmark data set in the sentence-level polarity classification task. The data source is the same as RT-C. SST: The Stanford Sentiment Treebank (Socher et al. 2013) is built upon PL05-C. The sentences are parsed to parse trees. Then, 215,154 syntactically plausible phrases are extracted and annotated by workers from Amazon Mechanical Turk. The experimental settings of positive/negative classification for sentences are the same as in Socher et al. (2013). RT-U: 737,806 user reviews from Rotten Tomatoes. Because we focus on sentencelevel sentiment classification, we filter out user reviews that are longer than 200 characters. The average length of these short user reviews from Rotten Tomatoes is 15.4 words. Following previous work on polarity classificatio</context>
<context position="77256" citStr="Socher et al. (2013)" startWordPosition="12757" endWordPosition="12760">0 218,000 23.2 136,006 PL05-C 10,662 5,331 5,331 21.0 20,263 SST 98,796 42,608 56,188 7.5 16,372 RT-U 737,806 368,903 368,903 15.4 138,815 IMDB-U 600,000 300,000 300,000 6.6 83,615 MPQA 10,624 7,308 3,316 3.1 5,992 Table 3 shows the summary of these data sets, and all of them are publicly available athttp://goo.gl/WxTdPf. 5.1.2 Settings. To compare with other published results for PL05-C and MPQA, the training and testing regime (10-fold cross-validation) is the same as in Pang and Lee (2005), Nakagawa, Inui, and Kurohashi (2010) and Socher et al. (2011). For SST, the regime is the same as in Socher et al. (2013). We use C-TEST as the testing data for RT-C, and U-TEST as the testing data for RT-U and IMDB-U. There are a number of settings that have trade-offs in performance, computation, and the generalization power of our model. The best settings are chosen by a portion of training split data that serves as the validation set. We provide the performance comparisons using different experimental settings in Section 5.4. Number of training examples: The size of training data has been widely recognized as one of the most important factors in machine learning-based methods. Generally, using more data lead</context>
<context position="85852" citStr="Socher et al. 2013" startWordPosition="14104" endWordPosition="14107">, as rule-based methods lack a probabilistic way to model the compositionality of context. Furthermore, s.parser achieves an accuracy improvement rate of 2.2 percentage points, 1.8 percentage points, and 0.5 percentage points over Tree-CRF, RAE-pretrain, and MV-RNN, respectively. On SST, s.parser outperforms SVM, MNB, and LM by 3.4 percentage points, 1.4 percentage points, and 3.8 percentage points, respectively. The performance is better than MV-RNN with an improvement rate of 1.8 percentage points. Moreover, the result is comparable to the 85.4% obtained by recursive neural tensor networks (Socher et al. 2013) without depending on syntactic parsing results. On the user review data sets RT-U and IMDB-U, our method also achieves the best results. More specifically, on the data set RT-U, s.parser outperforms the best results of SVM, MNB, and LM by 1.7 percentage points, 2.9 percentage points, and 1.5 percentage points, respectively. On the data set IMDB-U, our method brings an improved accuracy rate of 2.1 percentage points, 3.7 percentage points, and 2.2 percentage points over SVM, MNB, and LM, respectively. We find that MNB performs better than SVM and LM on the critics review data sets RT-C and PL0</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Socher, Richard, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1,631–1,642, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM: An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP 2002,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="81644" citStr="Stolcke 2002" startWordPosition="13449" endWordPosition="13450">is in Cui, Mittal, and Datar (2006). Probability of generating sentence s is calculated by P(s) = flis|0 1 P (wi|wo 1), where wi−1 0 denotes the word sequence w0 ... wi−1. We use Good-Turing smoothing (Good 1953) to overcome sparsity when estimating the probability of high-order n-gram. We train language models on negative and positive sentences separately. For a sentence, its polarity is determined by comparing the probabilities calculated from the positive and negative language models. The unknown-word token is treated as a regular word (denoted by &lt;UNK&gt;). The SRI Language Modeling Toolkit (Stolcke 2002) is used in our experiment. Voting-w/Rev: This approach is proposed by Choi and Cardie (2009b), and is used as a baseline in Nakagawa, Inui, and Kurohashi (2010). The polarity of a subjective sentence is decided by the voting of each phrase’s prior polarity. The polarity of phrases that have odd numbers of negation phrases in their ancestors is reversed. The results are reported by Nakagawa, Inui, and Kurohashi (2010). HardRule: This baseline method is compared by Nakagawa, Inui, and Kurohashi (2010). The polarity of a subjective sentence is deterministically decided based on rules, by conside</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM: An extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP 2002, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="2820" citStr="Taboada et al. 2011" startWordPosition="380" endWordPosition="383">28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created manually by experts. I</context>
<context position="12436" citStr="Taboada et al. (2011)" startWordPosition="1814" endWordPosition="1817">uses dictionary matching on a predefined sentiment lexicon to derive sentiment polarity. These methods often use a set of manually defined rules to deal with the negation of polarity. Turney (2002) proposed using the average sentiment orientation of phrases, which contains adjectives or adverbs, in a review to predict its sentiment orientation. Yu and Hatzivassiloglou (2003) calculated a modified log-likelihood ratio for every word by the co-occurrences with positive and negative seed words. To determine the polarity of a sentence, they compare the average log-likelihood value with threshold. Taboada et al. (2011) presented a lexicon-based approach for extracting sentiment from text. They used dictionaries of words with annotated sentiment orientation (polarity and strength) while incorporating intensification and negation. The lexicon-based methods often achieve high precisions and do not need 268 Dong et al. A Statistical Parsing Framework for Sentiment Classification any labeled samples. But they suffer from coverage and domain adaption problems. Moreover, lexicons are often built and used without considering the context (Wilson, Wiebe, and Hoffmann 2009). Also, hand-crafted rules are often matched </context>
<context position="40515" citStr="Taboada et al. (2011)" startWordPosition="6517" endWordPosition="6520">+ 01P(P|wji+1)). To tackle negation, switch negation (Choi and Cardie 2008; Saur´ı 2008) simply reverses the sentiment polarity and corresponding sentiment strength. However, consider not great and not good; flipping polarity directly makes not good more positive than not great, which is unreasonable. Another potential problem of switch negation is that negative polarity items interact with intensifiers in undesirable ways (Kennedy and Inkpen 2006). For example, not very good turns out to be even more negative than not good, given the fact that very good is more positive than good. Therefore, Taboada et al. (2011) argue that shift negation is a better way to handle polarity negation. Instead of reversing polarity strength, shift negation shifts it toward the opposite polarity by 278 Dong et al. A Statistical Parsing Framework for Sentiment Classification a fixed amount. This method can partially avoid the aforementioned two problems. However, they set the parameters manually, which might not be reliable and extensible enough to a new data set. Using the regression model, switch negation is captured by the negative scale item 0k (k &gt; 0), and shift negation is expressed by the shift item 00. The intensif</context>
<context position="42394" citStr="Taboada et al. 2011" startWordPosition="6803" endWordPosition="6806">cale, which is larger than one for amplifiers, and less than one for downtoners. The regression model can capture these two methods to handle the intensification. The shift item 00 represents the polarity addition and subtraction directly, and the scale item 0k (k &gt; 0) can scale the polarity by a percentage. Table 2 illustrates how the regression based polarity model represents different negation and intensification methods. For a specific rule, the parameters and the compositional method are automatically learned from data (Section 4.2.3) instead of setting them manually as in previous work (Taboada et al. 2011). In a similar way, this method can handle the contrast. For example, the inference rule for N → P but N is: (N → P but N) [i1, P, j1]Φ1 [i2, N,j2]Φ2 (17) rP(N|wji) = h(00 + 01P(P*1) + 02P(N |wi2)) [i, N,j] P(P|wji) = 1 − P(N|wji) where the polarity probability of the rule N → P but Nis computed by P(N|wji) = h(00 + 01P(P|wj1 i1 ) + 02P(N|wj2i2)). It can express the contrast relation by specific parameters 00, 01, and 02. It should be noted that a linear regression model could turn out to be problematic, as it may produce unreasonable results. For example, if we do not add any constraint, we m</context>
<context position="101843" citStr="Taboada et al. 2011" startWordPosition="16751" endWordPosition="16754">er, there are also some negative examples caused by “false subjective.” For instance, the neutral phrase to pay it tends to appear in negative sentences, and it is learned as a negative phrase. This makes sense for the data distribution, but it may lead to the mismatch for the combination rules. In Figure 13, we show the polarity model of some combination rules learned from the data set RT-C. The first two examples are negation rules. We find that both switch negation and shift negation exist in data, instead of using only one negation type in previous work (Choi and Cardie 2008; Saur´ı 2008; Taboada et al. 2011). For the rule “N → i do not P,” we find that it is a switch negation rule. This rule reverses the polarity and the corresponding polarity strength. For instance, the i do not like it very much is more negative than the i do not like it. As shown in Figure 13b, the “N → is not P” is a shift negation that reduces a fixed polarity strength to reverse the original polarity. Specifically, the is not good is more negative than the is not great, as described in Section 3.4. We have a similar conclusion for the next two weaken rules. As illustrated in Figure 13c, the “P → P actress” describes one asp</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Taboada, Maite, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexicon-based methods for sentiment analysis. Computational Linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
</authors>
<title>Discovering fine-grained sentiment with latent variable structured prediction models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR’11,</booktitle>
<pages>368--374</pages>
<location>Berlin.</location>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>T¨ackstr¨om, Oscar and Ryan McDonald. 2011a. Discovering fine-grained sentiment with latent variable structured prediction models. In Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR’11, pages 368–374, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
</authors>
<title>Semi-supervised latent variable models for sentence-level sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>569--574</pages>
<location>Stroudsburg, PA.</location>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>T¨ackstr¨om, Oscar and Ryan McDonald. 2011b. Semi-supervised latent variable models for sentence-level sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 569–574, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>133--140</pages>
<location>Stroudsburg, PA.</location>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Takamura, Hiroya, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05, pages 133–140, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaopeng Tu</author>
<author>Yifan He</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Identifying high-impact sub-structures for convolution kernels in document-level sentiment classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>338--343</pages>
<location>Stroudsburg, PA.</location>
<marker>Tu, He, Foster, van Genabith, Liu, Lin, 2012</marker>
<rawString>Tu, Zhaopeng, Yifan He, Jennifer Foster, Josef van Genabith, Qun Liu, and Shouxun Lin. 2012. Identifying high-impact sub-structures for convolution kernels in document-level sentiment classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, pages 338–343, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>417--424</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="2798" citStr="Turney 2002" startWordPosition="378" endWordPosition="379">publication: 28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created </context>
<context position="12012" citStr="Turney (2002)" startWordPosition="1755" endWordPosition="1756"> existing works can be divided into phrase-level, sentence-level, or document-level sentiment classification. We focus on sentence-level sentiment classification in this article. Regardless of what granularity the task is performed on, existing approaches deriving sentiment polarity from text fall into two major categories, namely, lexicon-based and learning-based approaches. The lexicon-based sentiment analysis uses dictionary matching on a predefined sentiment lexicon to derive sentiment polarity. These methods often use a set of manually defined rules to deal with the negation of polarity. Turney (2002) proposed using the average sentiment orientation of phrases, which contains adjectives or adverbs, in a review to predict its sentiment orientation. Yu and Hatzivassiloglou (2003) calculated a modified log-likelihood ratio for every word by the co-occurrences with positive and negative seed words. To determine the polarity of a sentence, they compare the average log-likelihood value with threshold. Taboada et al. (2011) presented a lexicon-based approach for extracting sentiment from text. They used dictionaries of words with annotated sentiment orientation (polarity and strength) while incor</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, Peter D. 2002. Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL ’02, pages 417–424, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of Web-derived polarity lexicons.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>777--785</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="13826" citStr="Velikovich et al. (2010)" startWordPosition="2009" endWordPosition="2012">ords. Kamps et al. (2004) and Williams and Anand (2009) used various lexical relations (such as synonym and antonym relations) in WordNet to expend a set of seed words. Some other methods learn lexicons from data directly. Hatzivassiloglou and McKeown (1997) used a log-linear regression model with conjunction constraints to predict whether conjoined adjectives have similar or different polarities. Combining conjunction constraints across many adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Velikovich, Leonid, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of Web-derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 777–785, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Wainwright</author>
<author>Michael I Jordan</author>
</authors>
<title>Graphical models, exponential families, and variational inference. Foundations and Trends</title>
<date>2008</date>
<booktitle>in Machine Learning,</booktitle>
<pages>1--1</pages>
<contexts>
<context position="54005" citStr="Wainwright and Jordan (2008)" startWordPosition="8851" endWordPosition="8854">iction labels are Ls, and A(ψ; s, T) (Equation (10)) is the log-partition function with respect to T(s). Based on the marginal log-likelihood function, the objective function O(ψ, T) consists of two terms. The first term is the sum of marginal log-likelihood over training instances that can obtain the correct polarity labels. The second term is a L2-norm regularization term on the parameters ψ. Formally, � logp(Ls|s; T, ψ) − λ2 kψk22 (22) O(ψ, T) = (s,Ls)ED TLs (s)00 To learn the parameters ψ, we use a gradient-based optimization method to maximize the objective function O(ψ, T). According to Wainwright and Jordan (2008), the derivative of the log-partition function is the expected feature vector ∂O(ψ,T) = E (Ep(t|s;TLs,ψ)[φ(s,t)] − Ep(t|s;T,ψ)[φ(s, t)]) − λψ (23) ∂ψ (s,Ls)ED TLs (s)00 where Ep(x)[f (x)] = Ex p(x)f(x) for discrete x. 4.1.3 Parameter Estimation. The objective function O(ψ, T) is not concave (nor convex), hence the optimization potentially results in a local optimum. Stochastic Gradient Descent (SGD; Robbins and Monro 1951) is a widely used optimization method. The SGD algorithm picks up a training instance randomly, and updates the parameter vector ψ according to ψj(t+1) = ψj(t) + α ( ∂ 0 ) |ψ</context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>Wainwright, Martin J. and Michael I. Jordan. 2008. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012),</booktitle>
<pages>90--94</pages>
<location>Jeju Island.</location>
<contexts>
<context position="17898" citStr="Wang and Manning (2012)" startWordPosition="2593" endWordPosition="2596">el allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level prediction. T¨ackstr¨om and McDonald (2011a) presented a latent variable model for only using document-level annotations to learn sentence-level sentiment labels, and T¨ackstr¨om and McDonald (2011b) improved it by using a semi-supervised latent variable model to utilize manually crafted sentence labels. Agarwal et al. (2011) and Tu et al. (2012) explored part-of-speech tag features and tree-kernel. Wang and Manning (2012) used SVM built over Naive Bayes log-count ratios as feature values to classify polarity. They showed that SVM was better at full-length reviews, and Multinomial Naive Bayes was better at short-length reviews. Liu, Agam, and Grossman (2012) proposed a set of heuristic rules based on dependency structure to detect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to these models, we derive polarity labels from tree structures parsed by the sentiment grammar. There ha</context>
<context position="80624" citStr="Wang and Manning (2012)" startWordPosition="13291" endWordPosition="13294">r Machine (SVM) achieves good performance in the sentiment classification task (Pang and Lee 2005). Though unigrams and bigrams are reported as the most effective features in existing work (Pang and Lee 2005), we use high-order n-gram (1 &lt; n &lt; m) features to conduct fair comparisons. Hereafter, m has the same meaning. We use LIBLINEAR (Fan et al. 2008) in our experiments because it can handle well the high feature dimension and a large number of training examples. We try different hyper-parameters C E {10−2,10−1, 1, 5,10, 20} for SVM, and select C on the validation set. MNB-m: As indicated in Wang and Manning (2012), Multinomial Naive Bayes (MNB) often outperforms SVM for sentence-level sentiment classification. We utilize Laplace smoothing (Manning, Raghavan, and Sch¨utze 2008) to tackle the zero probability problem. High order n-gram (1 &lt; n &lt; m) features are considered in the experiments. LM-m: Language Model (LM) is a generative model calculating the probability of word sequences. It is used for sentiment analysis in Cui, Mittal, and Datar (2006). Probability of generating sentence s is calculated by P(s) = flis|0 1 P (wi|wo 1), where wi−1 0 denotes the word sequence w0 ... wi−1. We use Good-Turing sm</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Wang, Sida and Christopher Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 90–94, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Wiebe, Janyce, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gbolahan K Williams</author>
<author>Sarabjot Singh Anand</author>
</authors>
<title>Predicting the polarity Computational Linguistics Volume 41, Number 2 strength of adjectives using Wordnet. In</title>
<date>2009</date>
<booktitle>ICWSM,</booktitle>
<pages>346--349</pages>
<location>San Jose, CA.</location>
<contexts>
<context position="13257" citStr="Williams and Anand (2009)" startWordPosition="1931" endWordPosition="1934">tion and negation. The lexicon-based methods often achieve high precisions and do not need 268 Dong et al. A Statistical Parsing Framework for Sentiment Classification any labeled samples. But they suffer from coverage and domain adaption problems. Moreover, lexicons are often built and used without considering the context (Wilson, Wiebe, and Hoffmann 2009). Also, hand-crafted rules are often matched heuristically. The sentiment dictionaries used for lexicon-based sentiment analysis can be created manually, or automatically using seed words to expand the list of words. Kamps et al. (2004) and Williams and Anand (2009) used various lexical relations (such as synonym and antonym relations) in WordNet to expend a set of seed words. Some other methods learn lexicons from data directly. Hatzivassiloglou and McKeown (1997) used a log-linear regression model with conjunction constraints to predict whether conjoined adjectives have similar or different polarities. Combining conjunction constraints across many adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity </context>
</contexts>
<marker>Williams, Anand, 2009</marker>
<rawString>Williams, Gbolahan K. and Sarabjot Singh Anand. 2009. Predicting the polarity Computational Linguistics Volume 41, Number 2 strength of adjectives using Wordnet. In ICWSM, pages 346–349, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<pages>35--399</pages>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35:399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Yisong Yue</author>
<author>Claire Cardie</author>
</authors>
<title>Multi-level structured models for document-level sentiment classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--046</pages>
<location>Cambridge, MA.</location>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>Yessenalina, Ainur, Yisong Yue, and Claire Cardie. 2010. Multi-level structured models for document-level sentiment classification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1,046–1,056, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="8523" citStr="Younger 1967" startWordPosition="1251" endWordPosition="1252">We can then use these two fragments and the overall negative opinion of the sentence to deduce sentiment rules automatically from data. These sentiment fragments and rules can be used to analyze the sentiment structure for new sentences. In this article, we propose a statistical parsing framework to directly analyze the structure of a sentence from the perspective of sentiment analysis. Specifically, we formulate a Context-Free Grammar (CFG)–based sentiment grammar. We then develop a statistical parser to derive the sentiment structure of a sentence. We leverage the CYK algorithm (Cocke 1969; Younger 1967; Kasami 1965) to conduct bottom–up parsing, and use dynamic programming to accelerate computation. Meanwhile, we propose using the polarity model to derive sentiment strength and polarity of a sentiment parse tree, and the ranking model to select the best one from the sentiment parsing results. We train the parser directly from examples of sentences annotated with sentiment polarity labels instead of syntactic annotations and polarity annotations of constituents within sentences. Therefore we can obtain training data easily. In particular, we train a sentiment parser, named s.parser, from a l</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Younger, Daniel H. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP ’03,</booktitle>
<pages>129--136</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="12192" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="1778" endWordPosition="1781">n this article. Regardless of what granularity the task is performed on, existing approaches deriving sentiment polarity from text fall into two major categories, namely, lexicon-based and learning-based approaches. The lexicon-based sentiment analysis uses dictionary matching on a predefined sentiment lexicon to derive sentiment polarity. These methods often use a set of manually defined rules to deal with the negation of polarity. Turney (2002) proposed using the average sentiment orientation of phrases, which contains adjectives or adverbs, in a review to predict its sentiment orientation. Yu and Hatzivassiloglou (2003) calculated a modified log-likelihood ratio for every word by the co-occurrences with positive and negative seed words. To determine the polarity of a sentence, they compare the average log-likelihood value with threshold. Taboada et al. (2011) presented a lexicon-based approach for extracting sentiment from text. They used dictionaries of words with annotated sentiment orientation (polarity and strength) while incorporating intensification and negation. The lexicon-based methods often achieve high precisions and do not need 268 Dong et al. A Statistical Parsing Framework for Sentiment Classif</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP ’03, pages 129–136, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>1--050</pages>
<location>Portland, OR.</location>
<contexts>
<context position="23234" citStr="Zelle and Mooney 1996" startWordPosition="3394" endWordPosition="3397">presenting sentences as latent sentiment trees. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linea</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>Zelle, John M. and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In AAAI/IAAI, pages 1,050–1,055, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007),</booktitle>
<pages>678--687</pages>
<location>Prague.</location>
<contexts>
<context position="23311" citStr="Zettlemoyer and Collins 2007" startWordPosition="3406" endWordPosition="3409">t, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Zettlemoyer, Luke S. and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007), pages 678–687, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>976--984</pages>
<location>Stroudsburg, PA.</location>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Zettlemoyer, Luke S. and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages 976–984, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jichang Zhao</author>
<author>Li Dong</author>
<author>Junjie Wu</author>
<author>Ke Xu</author>
</authors>
<title>Moodlens: An emoticon-based sentiment analysis system for Chinese tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’12,</booktitle>
<pages>1--528</pages>
<contexts>
<context position="4123" citStr="Zhao et al. 2012" startWordPosition="576" endWordPosition="579"> time very frequently. Taking tweets from Twitter1 and movie reviews on IMDb2 as examples, people use very casual language as well as informal and new vocabulary to comment on general topics and movies. In practice, it is not feasible to create and maintain sentiment lexicons to capture sentiment expressions with high coverage. On the other hand, the learning-based approach relies on large annotated samples to overcome the vocabulary coverage and deals with variations of words in sentences. Human ratings in reviews (Maas et al. 2011) and emoticons in tweets (Davidov, Tsur, and Rappoport 2010; Zhao et al. 2012) are extensively used to collect a large number of training corpora to train the sentiment classifier. However, it is usually not easy to design effective features to build the classifier. Among others, unigrams have been reported as the most effective features (Pang, Lee, and Vaithyanathan 2002) in sentiment classification. Handling complicated expressions delivering people’s opinions is one of the most challenging problems in sentiment analysis. Compositionalities such as negation, intensification, contrast, and their combinations are typical cases. We show some concrete examples here: (1) T</context>
</contexts>
<marker>Zhao, Dong, Wu, Xu, 2012</marker>
<rawString>Zhao, Jichang, Li Dong, Junjie Wu, and Ke Xu. 2012. Moodlens: An emoticon-based sentiment analysis system for Chinese tweets. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’12, pages 1,528–1,531,</rawString>
</citation>
<citation valid="false">
<location>New York, NY.</location>
<marker></marker>
<rawString>New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>