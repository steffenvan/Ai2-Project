<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005716">
<title confidence="0.9985445">
Word Sense Disambiguation Using OntoNotes:
An Empirical Study
</title>
<author confidence="0.999021">
Zhi Zhong and Hwee Tou Ng and Yee Seng Chan
</author>
<affiliation confidence="0.999918">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.917677">
Law Link, Singapore 117590
</address>
<email confidence="0.992233">
{zhongzhi, nght, chanys}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.997343" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977092">
The accuracy of current word sense disam-
biguation (WSD) systems is affected by the
fine-grained sense inventory of WordNet as
well as a lack of training examples. Using the
WSD examples provided through OntoNotes,
we conduct the first large-scale WSD evalua-
tion involving hundreds of word types and tens
of thousands of sense-tagged examples, while
adopting a coarse-grained sense inventory. We
show that though WSD systems trained with a
large number of examples can obtain a high
level of accuracy, they nevertheless suffer a
substantial drop in accuracy when applied to
a different domain. To address this issue, we
propose combining a domain adaptation tech-
nique using feature augmentation with active
learning. Our results show that this approach
is effective in reducing the annotation effort
required to adapt a WSD system to a new do-
main. Finally, we propose that one can maxi-
mize the dual benefits of reducing the annota-
tion effort while ensuring an increase in WSD
accuracy, by only performing active learning
on the set of most frequently occurring word
types.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931565217391">
In language, many words have multiple meanings.
The process of identifying the correct meaning, or
sense of a word in context, is known as word sense
disambiguation (WSD). WSD is one of the funda-
mental problems in natural language processing and
is important for applications such as machine trans-
lation (MT) (Chan et al., 2007a; Carpuat and Wu,
2007), information retrieval (IR), etc.
WSD is typically viewed as a classification prob-
lem where each ambiguous word is assigned a sense
label (from a pre-defined sense inventory) during the
disambiguation process. In current WSD research,
WordNet (Miller, 1990) is usually used as the sense
inventory. WordNet, however, adopts a very fine
level of sense granularity, thus restricting the accu-
racy of WSD systems. Also, current state-of-the-art
WSD systems are based on supervised learning and
face a general lack of training data.
To provide a standardized test-bed for evalua-
tion of WSD systems, a series of evaluation exer-
cises called SENSEVAL were held. In the English
all-words task of SENSEVAL-2 and SENSEVAL-
3 (Palmer et al., 2001; Snyder and Palmer, 2004),
no training data was provided and systems must tag
all the content words (noun, verb, adjective, and
adverb) in running English texts with their correct
WordNet senses. In SENSEVAL-2, the best per-
forming system (Mihalcea and Moldovan, 2001) in
the English all-words task achieved an accuracy of
69.0%, while in SENSEVAL-3, the best perform-
ing system (Decadt et al., 2004) achieved an accu-
racy of 65.2%. In SemEval-2007, which was the
most recent SENSEVAL evaluation, a similar En-
glish all-words task was held, where systems had to
provide the correct WordNet sense tag for all the
verbs and head words of their arguments in run-
ning English texts. For this task, the best perform-
ing system (Tratz et al., 2007) achieved an accuracy
of 59.1%. Results of these evaluations showed that
state-of-the-art English all-words WSD systems per-
formed with an accuracy of 60%–70%, using the
fine-grained sense inventory of WordNet.
The low level of performance by these state-of-
the-art WSD systems is a cause for concern, since
WSD is supposed to be an enabling technology
to be incorporated as a module into applications
</bodyText>
<page confidence="0.969055">
1002
</page>
<note confidence="0.881316">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1002–1010,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999942916666667">
such as MT and IR. As mentioned earlier, one of
the major reasons for the low performance is that
these evaluation exercises adopted WordNet as the
reference sense inventory, which is often too fine-
grained. As an indication of this, inter-annotator
agreement (ITA) reported for manual sense-tagging
on these SENSEVAL English all-words datasets is
typically in the mid-70s. To address this issue, a
coarse-grained English all-words task (Navigli et al.,
2007) was conducted during SemEval-2007. This
task used a coarse-grained version of WordNet and
reported an ITA of around 90%. We note that the
best performing system (Chan et al., 2007b) of this
task achieved a relatively high accuracy of 82.5%,
highlighting the importance of having an appropri-
ate level of sense granularity.
Another issue faced by current WSD systems is
the lack of training data. We note that the top per-
forming systems mentioned in the previous para-
graphs are all based on supervised learning. With
this approach, however, one would need to obtain
a corpus where each ambiguous word occurrence is
manually annotated with the correct sense, to serve
as training data. Since it is time consuming to per-
form sense annotation of word occurrences, only a
handful of sense-tagged corpora are publicly avail-
able. Among the existing sense-tagged corpora, the
SEMCOR corpus (Miller et al., 1994) is one of the
most widely used. In SEMCOR, content words have
been manually tagged with WordNet senses. Cur-
rent supervised WSD systems (which include all
the top-performing systems in the English all-words
task) usually rely on this relatively small manually
annotated corpus for training examples, and this has
inevitably affected the accuracy and scalability of
current WSD systems.
Related to the problem of a lack of training data
for WSD, there is also a lack of test data. Having
a large amount of test data for evaluation is impor-
tant to ensure the robustness and scalability of WSD
systems. Due to the expensive process of manual
sense-tagging, the SENSEVAL English all-words
task evaluations were conducted on relatively small
sets of evaluation data. For instance, the evaluation
data of SENSEVAL-2 and SENSEVAL-3 English
all-words task consists of 2,473 and 2,041 test exam-
ples respectively. In SemEval-2007, the fine-grained
English all-words task consists of only 465 test ex-
amples, while the SemEval-2007 coarse-grained En-
glish all-words task consists of 2,269 test examples.
Hence, it is necessary to address the issues of
sense granularity, and the lack of both training and
test data. To this end, a recent large-scale anno-
tation effort called the OntoNotes project (Hovy et
al., 2006) was started. Building on the annotations
from the Wall Street Journal (WSJ) portion of the
Penn Treebank (Marcus et al., 1993), the project
added several new layers of semantic annotations,
such as coreference information, word senses, etc.
In its first release (LDC2007T21) through the Lin-
guistic Data Consortium (LDC), the project man-
ually sense-tagged more than 40,000 examples be-
longing to hundreds of noun and verb types with an
ITA of 90%, based on a coarse-grained sense inven-
tory, where each word has an average of only 3.2
senses. Thus, besides providing WSD examples that
were sense-tagged with a high ITA, the project also
addressed the previously discussed issues of a lack
of training and test data.
In this paper, we use the sense-tagged data pro-
vided by the OntoNotes project to investigate the
accuracy achievable by current WSD systems when
adopting a coarse-grained sense inventory. Through
our experiments, we then highlight that domain
adaptation for WSD is an important issue as it sub-
stantially affects the performance of a state-of-the-
art WSD system which is trained on SEMCOR but
evaluated on sense-tagged examples in OntoNotes.
To address this issue, we then show that by com-
bining a domain adaptation technique using feature
augmentation with active learning, one only needs
to annotate a small amount of in-domain examples
to obtain a substantial improvement in the accuracy
of the WSD system which is previously trained on
out-of-domain examples.
The contributions of this paper are as follows.
To our knowledge, this is the first large-scale WSD
evaluation conducted that involves hundreds of word
types and tens of thousands of sense-tagged exam-
ples, and that is based on a coarse-grained sense in-
ventory. The present study also highlights the practi-
cal significance of domain adaptation in word sense
disambiguation in the context of a large-scale empir-
ical evaluation, and proposes an effective method to
address the domain adaptation problem.
In the next section, we give a brief description of
</bodyText>
<page confidence="0.974977">
1003
</page>
<bodyText confidence="0.999870846153846">
our WSD system. In Section 3, we describe exper-
iments where we conduct both training and evalu-
ation using data from OntoNotes. In Section 4, we
investigate the WSD performance when we train our
system on examples that are gathered from a differ-
ent domain as compared to the OntoNotes evalua-
tion data. In Section 5, we perform domain adapta-
tion experiments using a recently introduced feature
augmentation technique. In Section 6, we investi-
gate the use of active learning to reduce the annota-
tion effort required to adapt our WSD system to the
domain of the OntoNotes data, before concluding in
Section 7.
</bodyText>
<sectionHeader confidence="0.966475" genericHeader="introduction">
2 The WSD System
</sectionHeader>
<bodyText confidence="0.999921083333334">
For the experiments reported in this paper, we fol-
low the supervised learning approach of (Lee and
Ng, 2002), by training an individual classifier for
each word using the knowledge sources of local col-
locations, parts-of-speech (POS), and surrounding
words.
For local collocations, we use 11 features:
C−1,−1, C1,1, C−2,−2, C2,2, C−2,−1, C−1,1, C1,2,
C−3,−1, C−2,1, C−1,2, and C1,3, where Ci,j refers to
the ordered sequence of tokens in the local context
of an ambiguous word w. Offsets i and j denote the
starting and ending position (relative to w) of the se-
quence, where a negative (positive) offset refers to a
token to its left (right). For parts-of-speech, we use
7 features: P−3, P−2, P−1, P0, P1, P2, P3, where
P0 is the POS of w, and P−i (Pi) is the POS of the
ith token to the left (right) of w. For surrounding
words, we consider all unigrams (single words) in
the surrounding context of w. These words can be in
a different sentence from w. For our experiments re-
ported in this paper, we use support vector machines
(SVM) as our learning algorithm, which was shown
to achieve good WSD performance in (Lee and Ng,
2002; Chan et al., 2007b).
</bodyText>
<sectionHeader confidence="0.976453" genericHeader="method">
3 Training and Evaluating on OntoNotes
</sectionHeader>
<bodyText confidence="0.999861333333333">
The annotated data of OntoNotes is drawn from the
Wall Street Journal (WSJ) portion of the Penn Tree-
bank corpus, divided into sections 00-24. These
WSJ documents have been widely used in various
NLP tasks such as syntactic parsing (Collins, 1999)
and semantic role labeling (SRL) (Carreras and Mar-
</bodyText>
<table confidence="0.989876333333333">
Section No. of No. of word tokens
word types
Individual Cumulative
02 248 425 425
03 79 107 532
04 186 389 921
05 287 625 1546
06 224 446 1992
07 270 549 2541
08 177 301 2842
09 308 677 3519
10 648 3048 6567
11 724 4071 10638
12 740 4296 14934
13 749 4577 19511
14 710 3900 23411
15 748 4768 28179
16 306 576 28755
17 219 398 29153
18 266 566 29719
19 219 389 30108
20 288 536 30644
21 262 470 31114
23 685 3755 -
</table>
<tableCaption confidence="0.9777625">
Table 1: Size of the sense-tagged data in the various WSJ
sections.
</tableCaption>
<bodyText confidence="0.996441916666667">
quez, 2005). In these tasks, the practice is to use
documents from WSJ sections 02-21 as training data
and WSJ section 23 as test data. Hence for our ex-
periments reported in this paper, we follow this con-
vention and use the annotated instances from WSJ
sections 02-21 as our training data, and instances in
WSJ section 23 as our test data.
As mentioned in Section 1, the OntoNotes data
provided WSD examples for a large number of
nouns and verbs, which are sense-tagged accord-
ing to a coarse-grained sense inventory. In Table 1,
we show the amount of sense-tagged data available
from OntoNotes, across the various WSJ sections.1
In the table, for each WSJ section, we list the num-
ber of word types, the number of sense-tagged ex-
amples, and the cumulative count on the number of
1We removed erroneous examples which were simply
tagged with ‘XXX’ as sense-tag, or tagged with senses that were
not found in the sense-inventory provided. Also, since we will
be comparing against training on SEMCOR later (which was
tagged using WordNet senses), we removed examples tagged
with OntoNotes senses which were not mapped to WordNet
senses. On the whole, about 7% of the original OntoNotes ex-
amples were removed as a result.
</bodyText>
<page confidence="0.982389">
1004
</page>
<bodyText confidence="0.999938971428571">
sense-tagged examples. From the table, we see that
sections 02-21, which will be used as training data
in our experiments, contain a total of slightly over
31,000 sense-tagged examples.
Using examples from sections 02-21 as training
data, we trained our WSD system and evaluated on
the examples from section 23. In our experiments,
if a word type in section 23 has no training exam-
ples from sections 02-21, we randomly select an
OntoNotes sense as the answer. Using these ex-
perimental settings, our WSD system achieved an
accuracy of 89.1%. We note that this accuracy is
much higher than the 60%–70% accuracies achieved
by state-of-the-art English all-words WSD systems
which are trained using the fine-grained sense inven-
tory of WordNet. Hence, this highlights the impor-
tance of having an appropriate level of sense granu-
larity.
Besides training on the entire set of examples
from sections 02-21, we also investigated the per-
formance achievable from training on various sub-
sections of the data and show these results as “ON”
in Figure 1. From the figure, we see that WSD accu-
racy increases as we add more training examples.
The fact that current state-of-the-art WSD sys-
tems are able to achieve a high level of perfor-
mance is important, as this means that WSD systems
will potentially be more usable for inclusion in end-
applications. For instance, the high level of perfor-
mance by syntactic parsers allows it to be used as an
enabling technology in various NLP tasks. Here, we
note that the 89.1% WSD accuracy we obtained is
comparable to state-of-the-art syntactic parsing ac-
curacies, such as the 91.0% performance by the sta-
tistical parser of Charniak and Johnson (2005).
</bodyText>
<sectionHeader confidence="0.990612" genericHeader="method">
4 Building WSD Systems with
Out-of-Domain Data
</sectionHeader>
<bodyText confidence="0.999984947368421">
Although our WSD system had achieved a high
accuracy of 89.1%, this was achieved by train-
ing on a large amount (about 31,000) of manually
sense annotated examples from sections 02-21 of the
OntoNotes data. Further, all these training data and
test data are gathered from the same domain of WSJ.
In reality, however, since manual sense annotation is
time consuming, it is not feasible to collect such a
large amount of manually sense-tagged data for ev-
ery domain of interest. Hence, in this section, we in-
vestigate the performance of our WSD system when
it is trained on out-of-domain data.
In the English all-words task of the previous SEN-
SEVAL evaluations (SENSEVAL-2, SENSEVAL-
3, SemEval-2007), the best performing English
all-words task systems with the highest WSD ac-
curacy were trained on SEMCOR (Mihalcea and
Moldovan, 2001; Decadt et al., 2004; Chan et al.,
2007b). Hence, we similarly trained our WSD sys-
tem on SEMCOR and evaluated on section 23 of the
OntoNotes corpus. For those word types in section
23 which do not have training examples from SEM-
COR, we randomly chose an OntoNotes sense as
the answer. In training on SEMCOR, we have also
ensured that there is a domain difference between
our training and test data. This is because while
the OntoNotes data was gathered from WSJ, which
contains mainly business related news, the SEMCOR
corpus is the sense-tagged portion of the Brown Cor-
pus (BC), which is a mixture of several genres such
as scientific texts, fictions, etc.
Evaluating on the section 23 test data, our WSD
system achieved only 76.2% accuracy. Compared to
the 89.1% accuracy achievable when we had trained
on examples from sections 02-21, this is a substan-
tially lower and disappointing drop of performance
and motivates the need for domain adaptation.
The need for domain adaptation is a general and
important issue for many NLP tasks (Daume III and
Marcu, 2006). For instance, SRL systems are usu-
ally trained and evaluated on data drawn from the
WSJ. In the CoNLL-2005 shared task on SRL (Car-
reras and Marquez, 2005), however, a task of train-
ing and evaluating systems on different domains was
included. For that task, systems that were trained on
the PropBank corpus (Palmer et al., 2005) (which
was gathered from the WSJ), suffered a 10% drop
in accuracy when evaluated on test data drawn from
BC, as compared to the performance achievable
when evaluated on data drawn from WSJ. More re-
cently, CoNLL-2007 included a shared task on de-
pendency parsing (Nivre et al., 2007). In this task,
systems that were trained on Penn Treebank (drawn
from WSJ), but evaluated on data drawn from a
different domain (such as chemical abstracts and
parent-child dialogues) showed a similar drop in per-
formance. For research involving training and eval-
</bodyText>
<page confidence="0.932437">
1005
</page>
<figure confidence="0.770654">
WSD Accuracies on Section 23
Section number
</figure>
<figureCaption confidence="0.979325333333333">
Figure 1: WSD accuracies evaluated on section 23, using SEMCOR and different OntoNotes sections as training
data. ON: only OntoNotes as training data. SC+ON: SEMCOR and OntoNotes as training data, SC+ON Augment:
Combining SEMCOR and OntoNotes via the Augment domain adaptation technique.
</figureCaption>
<figure confidence="0.99090318">
WSD accuracy (%)
100
95
90
85
80
75
70
65
60
55
ON
SC+ON
SC+ON Augment
59.2
76.8
77.5
60.5
77.1
77.5
64.4
77.1
77.6
73.3
78.9
80.3
76.8
79.3
80.9
80.2
79.9
82.1
80.5
80.5
82.6
81.6
80.8
83.1
85.8
83.3
85.6
87.5
86.1
87.6
88.3
87.2
88.7
89.1
87.9
88.9
</figure>
<bodyText confidence="0.990151928571429">
uating WSD systems on data drawn from different
domains, several prior research efforts (Escudero et
al., 2000; Martinez and Agirre, 2000) observed a
similar drop in performance of about 10% when a
WSD system that was trained on the BC part of the
DSO corpus was evaluated on the WSJ part of the
corpus, and vice versa.
In the rest of this paper, we perform domain adap-
tation experiments for WSD, focusing on domain
adaptation methods that use in-domain annotated
data. In particular, we use a feature augmentation
technique recently introduced by Daume III (2007),
and active learning (Lewis and Gale, 1994) to per-
form domain adaptation of WSD systems.
</bodyText>
<sectionHeader confidence="0.997083" genericHeader="method">
5 Combining In-Domain and
Out-of-Domain Data for Training
</sectionHeader>
<bodyText confidence="0.99971125">
In this section, we will first introduce the AUGMENT
technique of Daume III (2007), before showing the
performance of our WSD system with and without
using this technique.
</bodyText>
<sectionHeader confidence="0.6718375" genericHeader="method">
5.1 The AUGMENT technique for Domain
Adaptation
</sectionHeader>
<bodyText confidence="0.9064537">
The AUGMENT technique introduced by Daume III
(2007) is a simple yet very effective approach to per-
forming domain adaptation. This technique is appli-
cable when one has access to training data from the
source domain and a small amount of training data
from the target domain.
The technique essentially augments the feature
space of an instance. Assuming x is an instance and
its original feature vector is Φ(x), the augmented
feature vector for instance x is
�
x _ &lt; Φ(x), Φ(x), 0 &gt; if x E D3
′( ) ,
&lt; Φ(x), 0, Φ(x) &gt; if x E Dt
where 0 is a zero vector of size |Φ(x)|, D3 and
Dt are the sets of instances from the source and
target domains respectively. We see that the tech-
nique essentially treats the first part of the aug-
mented feature space as holding general features that
are not meant to be differentiated between different
</bodyText>
<page confidence="0.855382">
1006
</page>
<bodyText confidence="0.960239551282051">
domains. Then, different parts of the augmented fea- DS +— the set of SEMCOR training examples
ture space are reserved for holding source domain DA +— the set of OntoNotes sections 02-21 examples
specific, or target domain specific features. Despite DT +— empty
its relative simplicity, this AUGMENT technique has while DA # 0
been shown to outperform other domain adaptation pmin +— 00
techniques on various tasks such as named entity P +— WSD system trained on DS and DT using AUGMENT
recognition, part-of-speech tagging, etc. technique
5.2 Experimental Results for each d G DA do
As mentioned in Section 4, training our WSD sys- s +— word sense prediction for d using P
tem on SEMCOR examples gave a relatively low ac- p +— confidence of prediction s
curacy of 76.2%, as compared to the 89.1% accuracy ifp &lt; pmin then
obtained from training on the OntoNotes section 02- pmin + —p, dmin +— d
21 examples. Assuming we have access to some in- end
domain training data, then a simple method to poten- end
tially obtain better accuracies is to train on both the DA +— DA — {dmin}
out-of-domain and in-domain examples. To investi- provide correct sense s for dmin and add dmin to DT
gate this, we combined the SEMCOR examples with end
various amounts of OntoNotes examples to train our
WSD system and show the resulting “SC+ON” ac-
curacies obtained in Figure 1. We also performed
another set of experiments, where instead of simply
combining the SEMCOR and OntoNotes examples,
we applied the AUGMENT technique when combin-
ing these examples, treating SEMCOR examples as
out-of-domain (source domain) data and OntoNotes
examples as in-domain (target domain) data. We
similarly show the resulting accuracies as “SC+ON
Augment” in Figure 1.
Comparing the “SC+ON” and “SC+ON Aug-
ment” accuracies in Figure 1, we see that the AUG-
MENT technique always helps to improve the ac-
curacy of our WSD system. Further, notice from
the first few sets of results in the figure that when
we have access to limited in-domain training exam-
ples from OntoNotes, incorporating additional out-
of-domain training data from SEMCOR (either using
the strategies “SC+ON” or “SC+ON Augment”)
achieves better accuracies than “ON”. Significance
tests using one-tailed paired t-test reveal that these
accuracy improvements are statistically significant
at the level of significance 0.01 (all significance tests
in the rest of this paper use the same level of signif-
icance 0.01). These results validate the contribution
of the SemCor examples. This trend continues till
the result for sections 02-06.
The right half of Figure 1 shows the accuracy
trend of the various strategies, in the unlikely event
1007
Figure 2: The active learning algorithm.
that we have access to a large amount of in-domain
training examples. Although we observe that in
this scenario, “ON” performs better than “SC+ON”,
“SC+ON Augment” continues to perform better
than “ON” (where the improvement is statistically
significant) till the result for sections 02-09. Beyond
that, as we add more OntoNotes examples, signif-
icance testing reveals that the “SC+ON Augment”
and “ON” strategies give comparable performance.
This means that the “SC+ON Augment” strategy,
besides giving good performance when one has few
in-domain examples, does continue to perform well
even when one has a large number of in-domain ex-
amples.
6 Active Learning with AUGMENT
Technique
So far in this paper, we have seen that when we have
access to some in-domain examples, a good strategy
is to combine the out-of-domain and in-domain ex-
amples via the AUGMENT technique. This suggests
that when one wishes to apply a WSD system to a
new domain of interest, it is worth the effort to an-
notate a small number of examples gathered from
the new domain. However, instead of randomly se-
lecting in-domain examples to annotate, we could
use active learning (Lewis and Gale, 1994) to help
select in-domain examples to annotate. By doing
so, we could minimize the manual annotation effort
needed.
</bodyText>
<figure confidence="0.965839214285714">
WSD Accuracies on Section 23
WSD Accuracy (%)
90
88
86
84
82
80
78
76
50 100 150 200
300 400 500 all
SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34
Iteration Number
</figure>
<figureCaption confidence="0.999466666666667">
Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types. Each
curve represents the adaptation process of applying active learning on a certain number of most frequently occurring
word types.
</figureCaption>
<bodyText confidence="0.999945625">
In WSD, several prior research efforts have suc-
cessfully used active learning to reduce the annota-
tion effort required (Zhu and Hovy, 2007; Chan and
Ng, 2007; Chen et al., 2006; Fujii et al., 1998). With
the exception of (Chan and Ng, 2007) which tried
to adapt a WSD system trained on the BC part of
the DSO corpus to the WSJ part of the DSO corpus,
the other researchers simply applied active learning
to reduce the annotation effort required and did not
deal with the issue of adapting a WSD system to a
new domain. Also, these prior research efforts only
experimented with a few word types. In contrast, we
perform active learning experiments on the hundreds
of word types in the OntoNotes data, with the aim of
adapting our WSD system trained on SEMCOR to
the WSJ domain represented by the OntoNotes data.
For our active learning experiments, we use the
uncertainty sampling strategy (Lewis and Gale,
1994), as shown in Figure 2. For our experiments,
the SEMCOR examples will be our initial set of
training examples, while the OntoNotes examples
from sections 02-21 will be used as our pool of
adaptation examples, from which we will select ex-
amples to annotate via active learning. Also, since
we have found that the AUGMENT technique is use-
ful in increasing WSD accuracy, we will apply the
AUGMENT technique during each iteration of active
learning to combine the SEMCOR examples and the
selected adaptation examples.
As shown in Figure 2, we train an initial WSD
system using only the set DS of SEMCOR exam-
ples. We then apply our WSD system on the set DA
of OntoNotes adaptation examples. The example in
DA which is predicted with the lowest confidence
will be removed from DA and added to the set DT
of in-domain examples that have been selected via
active learning thus far. We then use the AUGMENT
technique to combine the set of examples in DS and
DT to train a new WSD system, which is then ap-
plied again on the set DA of remaining adaptation
examples, and this active learning process continues
until we have used up all the adaptation examples.
Note that because we are using OntoNotes sections
02-21 (which have already been sense-tagged be-
forehand) as our adaptation data, the annotation of
the selected example during each active learning it-
eration is simply simulated by referring to its tagged
sense.
</bodyText>
<subsectionHeader confidence="0.968839">
6.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9987795">
As mentioned earlier, we use the examples in
OntoNotes sections 02-21 as our adaptation exam-
</bodyText>
<page confidence="0.978505">
1008
</page>
<bodyText confidence="0.999933363636364">
ples during active learning. Hence, we perform
active learning experiments on all the word types
that have sense-tagged examples from OntoNotes
sections 02-21, and show the evaluation results on
OntoNotes section 23 as the topmost “all” curve in
Figure 3. Since our aim is to reduce the human an-
notation effort required in adapting a WSD system
to a new domain, we may not want to perform active
learning on all the word types in practice. Instead,
we can maximize the benefits by performing active
learning only on the more frequently occurring word
types. Hence, in Figure 3, we also show via var-
ious curves the results of applying active learning
only to various sets of word types, according to their
frequency, or number of sense-tagged examples in
OntoNotes sections 02-21. Note that the various ac-
curacy curves in Figure 3 are plotted in terms of
evaluation accuracies over all the test examples in
OntoNotes section 23, hence they are directly com-
parable to the results reported thus far in this pa-
per. Also, since the accuracies for the various curves
stabilize after 35 active learning iterations, we only
show the results of the first 35 iterations.
From Figure 3, we note that by performing ac-
tive learning on the set of 150 most frequently oc-
curring word types, we are able to achieve a WSD
accuracy of 82.6% after 10 active learning iterations.
Note that in Section 4, we mentioned that training
only on the out-of-domain SEMCOR examples gave
an accuracy of 76.2%. Hence, we have gained an
accuracy improvement of 6.4% (82.6% − 76.2%)
by just using 1,500 in-domain OntoNotes examples.
Compared with the 12.9% (89.1% − 76.2%) im-
provement in accuracy achieved by using all 31,114
OntoNotes sections 02-21 examples, we have ob-
tained half of this maximum increase in accuracy, by
requiring only about 5% (1,500/31,114) of the total
number of sense-tagged examples. Based on these
results, we propose that when there is a need to apply
a previously trained WSD system to a different do-
main, one can apply the AUGMENT technique with
active learning on the most frequent word types, to
greatly reduce the annotation effort required while
obtaining a substantial improvement in accuracy.
</bodyText>
<sectionHeader confidence="0.99844" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999960846153846">
Using the WSD examples made available through
OntoNotes, which are sense-tagged according to a
coarse-grained sense inventory, we show that our
WSD system is able to achieve a high accuracy
of 89.1% when we train and evaluate on these ex-
amples. However, when we apply a WSD system
that is trained on SEMCOR, we suffer a substan-
tial drop in accuracy, highlighting the need to per-
form domain adaptation. We show that by com-
bining the AUGMENT domain adaptation technique
with active learning, we are able to effectively re-
duce the amount of annotation effort required for do-
main adaptation.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918735294118">
M. Carpuat and D. Wu. 2007. Improving Statistical Ma-
chine Translation Using Word Sense Disambiguation.
In Proc. ofEMNLP-CoNLL07, pages 61–72.
X. Carreras and L. Marquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2005, pages 152–164.
Y. S. Chan and H. T. Ng. 2007. Domain Adaptation with
Active Learning for Word Sense Disambiguation. In
Proc. ofACL07, pages 49–56.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007a. Word Sense
Disambiguation Improves Statistical Machine Transla-
tion. In Proc. ofACL07, pages 33–40.
Y. S. Chan, H. T. Ng, and Z. Zhong. 2007b. NUS-PT: Ex-
ploiting Parallel Texts for Word Sense Disambiguation
in the English All-Words Tasks. In Proc. of SemEval-
2007, pages 253–256.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proc. ofACL05, pages 173–180.
J. Y. Chen, A. Schein, L. Ungar, and M. Palmer. 2006.
An Empirical Study of the Behavior of Active Learn-
ing for Word Sense Disambiguation. In Proc. of
HLT/NAACL06, pages 120–127.
M. Collins. 1999. Head-Driven Statistical Model for
Natural Language Parsing. PhD dissertation, Univer-
sity of Pennsylvania.
H. Daume III and D. Marcu. 2006. Domain Adaptation
for Statistical Classifiers. Journal of Artificial Intelli-
gence Research, 26:101–126.
H. Daume III. 2007. Frustratingly Easy Domain Adap-
tation. In Proc. ofACL07, pages 256–263.
B. Decadt, V. Hoste, and W. Daelemans. 2004. GAMBL,
Genetic Algorithm Optimization of Memory-Based
WSD. In Proc. of SENSEVAL-3, pages 108–112.
</reference>
<page confidence="0.843304">
1009
</page>
<reference confidence="0.999841947368422">
G. Escudero, L. Marquez, and G. Riagu. 2000. An
Empirical Study of the Domain Dependence of Super-
vised Word Sense Disambiguation Systems. In Proc.
of EMNLP/VLC00, pages 172–180.
A. Fujii, K. Inui, T. Tokunaga, and H. Tanaka. 1998. Se-
lective Sampling for Example-based Word Sense Dis-
ambiguation. Computational Linguistics, 24(4).
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% solution.
In Proc. ofHLT-NAACL06, pages 57–60.
Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. In Proc. of EMNLP02,
pages 41–48.
D. D. Lewis and W. A. Gale. 1994. A Sequential Al-
gorithm for Training Text Classifiers. In Proc. of SI-
GIR94.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330.
D. Martinez and E. Agirre. 2000. One Sense per
Collocation and Genre/Topic Variations. In Proc. of
EMNLP/VLC00, pages 207–215.
R. Mihalcea and D. Moldovan. 2001. Pattern Learning
and Active Feature Selection for Word Sense Disam-
biguation. In Proc. of SENSEVAL-2, pages 127–130.
G. A. Miller, M. Chodorow, S. Landes, C. Leacock, and
R. G. Thomas. 1994. Using a Semantic Concordance
for Sense Identification. In Proc. of ARPA Human
Language Technology Workshop, pages 240–243.
G. A. Miller. 1990. WordNet: An On-line Lexi-
cal Database. International Journal of Lexicography,
3(4):235–312.
R. Navigli, K. C. Litkowski, and O. Hargraves. 2007.
SemEval-2007 Task 07: Coarse-Grained English All-
Words Task. In Proc. of SemEval-2007, pages 30–35.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proc. of
EMNLP-CoNLL07, pages 915–932.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H. T.
Dang. 2001. English Tasks: All-Words and Verb Lex-
ical Sample. In Proc. of SENSEVAL-2, pages 21–24.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71–105.
B. Snyder and M. Palmer. 2004. The English All-Words
Task. In Proc. of SENSEVAL-3, pages 41–43.
S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell,
C. Posse, and P. Whitney. 2007. PNNL: A Supervised
Maximum Entropy Approach to Word Sense Disam-
biguation. In Proc. of SemEval-2007, pages 264–267.
J. B. Zhu and E. Hovy. 2007. Active Learning for Word
Sense Disambiguation with Methods for Addressing
the Class Imbalance Problem. In Proc. of EMNLP-
CoNLL07, pages 783–790.
</reference>
<page confidence="0.990154">
1010
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.519164">
<title confidence="0.9691125">Word Sense Disambiguation Using An Empirical Study</title>
<author confidence="0.997746">Zhong Tou Ng Seng</author>
<affiliation confidence="0.999404">Department of Computer National University of</affiliation>
<address confidence="0.584568">Law Link, Singapore</address>
<email confidence="0.990854">nght,</email>
<abstract confidence="0.998342384615384">The accuracy of current word sense disambiguation (WSD) systems is affected by the fine-grained sense inventory of WordNet as well as a lack of training examples. Using the WSD examples provided through OntoNotes, we conduct the first large-scale WSD evaluation involving hundreds of word types and tens of thousands of sense-tagged examples, while adopting a coarse-grained sense inventory. We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop in accuracy when applied to a different domain. To address this issue, we propose combining a domain adaptation technique using feature augmentation with active learning. Our results show that this approach is effective in reducing the annotation effort required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving Statistical Machine Translation Using Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. ofEMNLP-CoNLL07,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="1685" citStr="Carpuat and Wu, 2007" startWordPosition="265" endWordPosition="268">a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types. 1 Introduction In language, many words have multiple meanings. The process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD). WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation (MT) (Chan et al., 2007a; Carpuat and Wu, 2007), information retrieval (IR), etc. WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process. In current WSD research, WordNet (Miller, 1990) is usually used as the sense inventory. WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems. Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data. To provide a standardized test-bed for evaluation of WSD systems, a seri</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving Statistical Machine Translation Using Word Sense Disambiguation. In Proc. ofEMNLP-CoNLL07, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Marquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL-2005,</booktitle>
<pages>152--164</pages>
<contexts>
<context position="16017" citStr="Carreras and Marquez, 2005" startWordPosition="2661" endWordPosition="2665"> mixture of several genres such as scientific texts, fictions, etc. Evaluating on the section 23 test data, our WSD system achieved only 76.2% accuracy. Compared to the 89.1% accuracy achievable when we had trained on examples from sections 02-21, this is a substantially lower and disappointing drop of performance and motivates the need for domain adaptation. The need for domain adaptation is a general and important issue for many NLP tasks (Daume III and Marcu, 2006). For instance, SRL systems are usually trained and evaluated on data drawn from the WSJ. In the CoNLL-2005 shared task on SRL (Carreras and Marquez, 2005), however, a task of training and evaluating systems on different domains was included. For that task, systems that were trained on the PropBank corpus (Palmer et al., 2005) (which was gathered from the WSJ), suffered a 10% drop in accuracy when evaluated on test data drawn from BC, as compared to the performance achievable when evaluated on data drawn from WSJ. More recently, CoNLL-2007 included a shared task on dependency parsing (Nivre et al., 2007). In this task, systems that were trained on Penn Treebank (drawn from WSJ), but evaluated on data drawn from a different domain (such as chemic</context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>X. Carreras and L. Marquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In Proc. of CoNLL-2005, pages 152–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
</authors>
<title>Domain Adaptation with Active Learning for Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. ofACL07,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="23655" citStr="Chan and Ng, 2007" startWordPosition="3961" endWordPosition="3964">minimize the manual annotation effort needed. WSD Accuracies on Section 23 WSD Accuracy (%) 90 88 86 84 82 80 78 76 50 100 150 200 300 400 500 all SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 Iteration Number Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types. Each curve represents the adaptation process of applying active learning on a certain number of most frequently occurring word types. In WSD, several prior research efforts have successfully used active learning to reduce the annotation effort required (Zhu and Hovy, 2007; Chan and Ng, 2007; Chen et al., 2006; Fujii et al., 1998). With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. Also, these prior research efforts only experimented with a few word types. In contrast, we perform active learning experiments on the hundreds of word types in the OntoNotes data, with the aim of adapting our WSD system trained on SEMCOR</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Y. S. Chan and H. T. Ng. 2007. Domain Adaptation with Active Learning for Word Sense Disambiguation. In Proc. ofACL07, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
<author>D Chiang</author>
</authors>
<title>Word Sense Disambiguation Improves Statistical Machine Translation. In</title>
<date>2007</date>
<booktitle>Proc. ofACL07,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1661" citStr="Chan et al., 2007" startWordPosition="261" endWordPosition="264">t required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types. 1 Introduction In language, many words have multiple meanings. The process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD). WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation (MT) (Chan et al., 2007a; Carpuat and Wu, 2007), information retrieval (IR), etc. WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process. In current WSD research, WordNet (Miller, 1990) is usually used as the sense inventory. WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems. Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data. To provide a standardized test-bed for evaluatio</context>
<context position="4375" citStr="Chan et al., 2007" startWordPosition="692" endWordPosition="695"> mentioned earlier, one of the major reasons for the low performance is that these evaluation exercises adopted WordNet as the reference sense inventory, which is often too finegrained. As an indication of this, inter-annotator agreement (ITA) reported for manual sense-tagging on these SENSEVAL English all-words datasets is typically in the mid-70s. To address this issue, a coarse-grained English all-words task (Navigli et al., 2007) was conducted during SemEval-2007. This task used a coarse-grained version of WordNet and reported an ITA of around 90%. We note that the best performing system (Chan et al., 2007b) of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity. Another issue faced by current WSD systems is the lack of training data. We note that the top performing systems mentioned in the previous paragraphs are all based on supervised learning. With this approach, however, one would need to obtain a corpus where each ambiguous word occurrence is manually annotated with the correct sense, to serve as training data. Since it is time consuming to perform sense annotation of word occurrences, only a handful of se</context>
<context position="10207" citStr="Chan et al., 2007" startWordPosition="1656" endWordPosition="1659">ion (relative to w) of the sequence, where a negative (positive) offset refers to a token to its left (right). For parts-of-speech, we use 7 features: P−3, P−2, P−1, P0, P1, P2, P3, where P0 is the POS of w, and P−i (Pi) is the POS of the ith token to the left (right) of w. For surrounding words, we consider all unigrams (single words) in the surrounding context of w. These words can be in a different sentence from w. For our experiments reported in this paper, we use support vector machines (SVM) as our learning algorithm, which was shown to achieve good WSD performance in (Lee and Ng, 2002; Chan et al., 2007b). 3 Training and Evaluating on OntoNotes The annotated data of OntoNotes is drawn from the Wall Street Journal (WSJ) portion of the Penn Treebank corpus, divided into sections 00-24. These WSJ documents have been widely used in various NLP tasks such as syntactic parsing (Collins, 1999) and semantic role labeling (SRL) (Carreras and MarSection No. of No. of word tokens word types Individual Cumulative 02 248 425 425 03 79 107 532 04 186 389 921 05 287 625 1546 06 224 446 1992 07 270 549 2541 08 177 301 2842 09 308 677 3519 10 648 3048 6567 11 724 4071 10638 12 740 4296 14934 13 749 4577 1951</context>
<context position="14839" citStr="Chan et al., 2007" startWordPosition="2460" endWordPosition="2463">a are gathered from the same domain of WSJ. In reality, however, since manual sense annotation is time consuming, it is not feasible to collect such a large amount of manually sense-tagged data for every domain of interest. Hence, in this section, we investigate the performance of our WSD system when it is trained on out-of-domain data. In the English all-words task of the previous SENSEVAL evaluations (SENSEVAL-2, SENSEVAL3, SemEval-2007), the best performing English all-words task systems with the highest WSD accuracy were trained on SEMCOR (Mihalcea and Moldovan, 2001; Decadt et al., 2004; Chan et al., 2007b). Hence, we similarly trained our WSD system on SEMCOR and evaluated on section 23 of the OntoNotes corpus. For those word types in section 23 which do not have training examples from SEMCOR, we randomly chose an OntoNotes sense as the answer. In training on SEMCOR, we have also ensured that there is a domain difference between our training and test data. This is because while the OntoNotes data was gathered from WSJ, which contains mainly business related news, the SEMCOR corpus is the sense-tagged portion of the Brown Corpus (BC), which is a mixture of several genres such as scientific tex</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Y. S. Chan, H. T. Ng, and D. Chiang. 2007a. Word Sense Disambiguation Improves Statistical Machine Translation. In Proc. ofACL07, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
<author>Z Zhong</author>
</authors>
<title>NUS-PT: Exploiting Parallel Texts for Word Sense Disambiguation in the English All-Words Tasks.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval2007,</booktitle>
<pages>253--256</pages>
<contexts>
<context position="1661" citStr="Chan et al., 2007" startWordPosition="261" endWordPosition="264">t required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types. 1 Introduction In language, many words have multiple meanings. The process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD). WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation (MT) (Chan et al., 2007a; Carpuat and Wu, 2007), information retrieval (IR), etc. WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process. In current WSD research, WordNet (Miller, 1990) is usually used as the sense inventory. WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems. Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data. To provide a standardized test-bed for evaluatio</context>
<context position="4375" citStr="Chan et al., 2007" startWordPosition="692" endWordPosition="695"> mentioned earlier, one of the major reasons for the low performance is that these evaluation exercises adopted WordNet as the reference sense inventory, which is often too finegrained. As an indication of this, inter-annotator agreement (ITA) reported for manual sense-tagging on these SENSEVAL English all-words datasets is typically in the mid-70s. To address this issue, a coarse-grained English all-words task (Navigli et al., 2007) was conducted during SemEval-2007. This task used a coarse-grained version of WordNet and reported an ITA of around 90%. We note that the best performing system (Chan et al., 2007b) of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity. Another issue faced by current WSD systems is the lack of training data. We note that the top performing systems mentioned in the previous paragraphs are all based on supervised learning. With this approach, however, one would need to obtain a corpus where each ambiguous word occurrence is manually annotated with the correct sense, to serve as training data. Since it is time consuming to perform sense annotation of word occurrences, only a handful of se</context>
<context position="10207" citStr="Chan et al., 2007" startWordPosition="1656" endWordPosition="1659">ion (relative to w) of the sequence, where a negative (positive) offset refers to a token to its left (right). For parts-of-speech, we use 7 features: P−3, P−2, P−1, P0, P1, P2, P3, where P0 is the POS of w, and P−i (Pi) is the POS of the ith token to the left (right) of w. For surrounding words, we consider all unigrams (single words) in the surrounding context of w. These words can be in a different sentence from w. For our experiments reported in this paper, we use support vector machines (SVM) as our learning algorithm, which was shown to achieve good WSD performance in (Lee and Ng, 2002; Chan et al., 2007b). 3 Training and Evaluating on OntoNotes The annotated data of OntoNotes is drawn from the Wall Street Journal (WSJ) portion of the Penn Treebank corpus, divided into sections 00-24. These WSJ documents have been widely used in various NLP tasks such as syntactic parsing (Collins, 1999) and semantic role labeling (SRL) (Carreras and MarSection No. of No. of word tokens word types Individual Cumulative 02 248 425 425 03 79 107 532 04 186 389 921 05 287 625 1546 06 224 446 1992 07 270 549 2541 08 177 301 2842 09 308 677 3519 10 648 3048 6567 11 724 4071 10638 12 740 4296 14934 13 749 4577 1951</context>
<context position="14839" citStr="Chan et al., 2007" startWordPosition="2460" endWordPosition="2463">a are gathered from the same domain of WSJ. In reality, however, since manual sense annotation is time consuming, it is not feasible to collect such a large amount of manually sense-tagged data for every domain of interest. Hence, in this section, we investigate the performance of our WSD system when it is trained on out-of-domain data. In the English all-words task of the previous SENSEVAL evaluations (SENSEVAL-2, SENSEVAL3, SemEval-2007), the best performing English all-words task systems with the highest WSD accuracy were trained on SEMCOR (Mihalcea and Moldovan, 2001; Decadt et al., 2004; Chan et al., 2007b). Hence, we similarly trained our WSD system on SEMCOR and evaluated on section 23 of the OntoNotes corpus. For those word types in section 23 which do not have training examples from SEMCOR, we randomly chose an OntoNotes sense as the answer. In training on SEMCOR, we have also ensured that there is a domain difference between our training and test data. This is because while the OntoNotes data was gathered from WSJ, which contains mainly business related news, the SEMCOR corpus is the sense-tagged portion of the Brown Corpus (BC), which is a mixture of several genres such as scientific tex</context>
</contexts>
<marker>Chan, Ng, Zhong, 2007</marker>
<rawString>Y. S. Chan, H. T. Ng, and Z. Zhong. 2007b. NUS-PT: Exploiting Parallel Texts for Word Sense Disambiguation in the English All-Words Tasks. In Proc. of SemEval2007, pages 253–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proc. ofACL05,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="13922" citStr="Charniak and Johnson (2005)" startWordPosition="2308" endWordPosition="2311">e figure, we see that WSD accuracy increases as we add more training examples. The fact that current state-of-the-art WSD systems are able to achieve a high level of performance is important, as this means that WSD systems will potentially be more usable for inclusion in endapplications. For instance, the high level of performance by syntactic parsers allows it to be used as an enabling technology in various NLP tasks. Here, we note that the 89.1% WSD accuracy we obtained is comparable to state-of-the-art syntactic parsing accuracies, such as the 91.0% performance by the statistical parser of Charniak and Johnson (2005). 4 Building WSD Systems with Out-of-Domain Data Although our WSD system had achieved a high accuracy of 89.1%, this was achieved by training on a large amount (about 31,000) of manually sense annotated examples from sections 02-21 of the OntoNotes data. Further, all these training data and test data are gathered from the same domain of WSJ. In reality, however, since manual sense annotation is time consuming, it is not feasible to collect such a large amount of manually sense-tagged data for every domain of interest. Hence, in this section, we investigate the performance of our WSD system whe</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking. In Proc. ofACL05, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Y Chen</author>
<author>A Schein</author>
<author>L Ungar</author>
<author>M Palmer</author>
</authors>
<title>An Empirical Study of the Behavior of Active Learning for Word Sense Disambiguation.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL06,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="23674" citStr="Chen et al., 2006" startWordPosition="3965" endWordPosition="3968"> annotation effort needed. WSD Accuracies on Section 23 WSD Accuracy (%) 90 88 86 84 82 80 78 76 50 100 150 200 300 400 500 all SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 Iteration Number Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types. Each curve represents the adaptation process of applying active learning on a certain number of most frequently occurring word types. In WSD, several prior research efforts have successfully used active learning to reduce the annotation effort required (Zhu and Hovy, 2007; Chan and Ng, 2007; Chen et al., 2006; Fujii et al., 1998). With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. Also, these prior research efforts only experimented with a few word types. In contrast, we perform active learning experiments on the hundreds of word types in the OntoNotes data, with the aim of adapting our WSD system trained on SEMCOR to the WSJ domain </context>
</contexts>
<marker>Chen, Schein, Ungar, Palmer, 2006</marker>
<rawString>J. Y. Chen, A. Schein, L. Ungar, and M. Palmer. 2006. An Empirical Study of the Behavior of Active Learning for Word Sense Disambiguation. In Proc. of HLT/NAACL06, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Model for Natural Language Parsing.</title>
<date>1999</date>
<institution>University of Pennsylvania.</institution>
<note>PhD dissertation,</note>
<contexts>
<context position="10496" citStr="Collins, 1999" startWordPosition="1705" endWordPosition="1706">ords, we consider all unigrams (single words) in the surrounding context of w. These words can be in a different sentence from w. For our experiments reported in this paper, we use support vector machines (SVM) as our learning algorithm, which was shown to achieve good WSD performance in (Lee and Ng, 2002; Chan et al., 2007b). 3 Training and Evaluating on OntoNotes The annotated data of OntoNotes is drawn from the Wall Street Journal (WSJ) portion of the Penn Treebank corpus, divided into sections 00-24. These WSJ documents have been widely used in various NLP tasks such as syntactic parsing (Collins, 1999) and semantic role labeling (SRL) (Carreras and MarSection No. of No. of word tokens word types Individual Cumulative 02 248 425 425 03 79 107 532 04 186 389 921 05 287 625 1546 06 224 446 1992 07 270 549 2541 08 177 301 2842 09 308 677 3519 10 648 3048 6567 11 724 4071 10638 12 740 4296 14934 13 749 4577 19511 14 710 3900 23411 15 748 4768 28179 16 306 576 28755 17 219 398 29153 18 266 566 29719 19 219 389 30108 20 288 536 30644 21 262 470 31114 23 685 3755 - Table 1: Size of the sense-tagged data in the various WSJ sections. quez, 2005). In these tasks, the practice is to use documents from </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Model for Natural Language Parsing. PhD dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
<author>D Marcu</author>
</authors>
<title>Domain Adaptation for Statistical Classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>26--101</pages>
<marker>Daume, Marcu, 2006</marker>
<rawString>H. Daume III and D. Marcu. 2006. Domain Adaptation for Statistical Classifiers. Journal of Artificial Intelligence Research, 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
</authors>
<title>Frustratingly Easy Domain Adaptation.</title>
<date>2007</date>
<booktitle>In Proc. ofACL07,</booktitle>
<pages>256--263</pages>
<marker>Daume, 2007</marker>
<rawString>H. Daume III. 2007. Frustratingly Easy Domain Adaptation. In Proc. ofACL07, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Decadt</author>
<author>V Hoste</author>
<author>W Daelemans</author>
</authors>
<title>GAMBL, Genetic Algorithm Optimization of Memory-Based WSD.</title>
<date>2004</date>
<booktitle>In Proc. of SENSEVAL-3,</booktitle>
<pages>108--112</pages>
<contexts>
<context position="2816" citStr="Decadt et al., 2004" startWordPosition="445" endWordPosition="448">f training data. To provide a standardized test-bed for evaluation of WSD systems, a series of evaluation exercises called SENSEVAL were held. In the English all-words task of SENSEVAL-2 and SENSEVAL3 (Palmer et al., 2001; Snyder and Palmer, 2004), no training data was provided and systems must tag all the content words (noun, verb, adjective, and adverb) in running English texts with their correct WordNet senses. In SENSEVAL-2, the best performing system (Mihalcea and Moldovan, 2001) in the English all-words task achieved an accuracy of 69.0%, while in SENSEVAL-3, the best performing system (Decadt et al., 2004) achieved an accuracy of 65.2%. In SemEval-2007, which was the most recent SENSEVAL evaluation, a similar English all-words task was held, where systems had to provide the correct WordNet sense tag for all the verbs and head words of their arguments in running English texts. For this task, the best performing system (Tratz et al., 2007) achieved an accuracy of 59.1%. Results of these evaluations showed that state-of-the-art English all-words WSD systems performed with an accuracy of 60%–70%, using the fine-grained sense inventory of WordNet. The low level of performance by these state-ofthe-ar</context>
<context position="14820" citStr="Decadt et al., 2004" startWordPosition="2456" endWordPosition="2459">ing data and test data are gathered from the same domain of WSJ. In reality, however, since manual sense annotation is time consuming, it is not feasible to collect such a large amount of manually sense-tagged data for every domain of interest. Hence, in this section, we investigate the performance of our WSD system when it is trained on out-of-domain data. In the English all-words task of the previous SENSEVAL evaluations (SENSEVAL-2, SENSEVAL3, SemEval-2007), the best performing English all-words task systems with the highest WSD accuracy were trained on SEMCOR (Mihalcea and Moldovan, 2001; Decadt et al., 2004; Chan et al., 2007b). Hence, we similarly trained our WSD system on SEMCOR and evaluated on section 23 of the OntoNotes corpus. For those word types in section 23 which do not have training examples from SEMCOR, we randomly chose an OntoNotes sense as the answer. In training on SEMCOR, we have also ensured that there is a domain difference between our training and test data. This is because while the OntoNotes data was gathered from WSJ, which contains mainly business related news, the SEMCOR corpus is the sense-tagged portion of the Brown Corpus (BC), which is a mixture of several genres suc</context>
</contexts>
<marker>Decadt, Hoste, Daelemans, 2004</marker>
<rawString>B. Decadt, V. Hoste, and W. Daelemans. 2004. GAMBL, Genetic Algorithm Optimization of Memory-Based WSD. In Proc. of SENSEVAL-3, pages 108–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Escudero</author>
<author>L Marquez</author>
<author>G Riagu</author>
</authors>
<title>An Empirical Study of the Domain Dependence of Supervised Word Sense Disambiguation Systems.</title>
<date>2000</date>
<booktitle>In Proc. of EMNLP/VLC00,</booktitle>
<pages>172--180</pages>
<contexts>
<context position="17434" citStr="Escudero et al., 2000" startWordPosition="2902" endWordPosition="2905">luated on section 23, using SEMCOR and different OntoNotes sections as training data. ON: only OntoNotes as training data. SC+ON: SEMCOR and OntoNotes as training data, SC+ON Augment: Combining SEMCOR and OntoNotes via the Augment domain adaptation technique. WSD accuracy (%) 100 95 90 85 80 75 70 65 60 55 ON SC+ON SC+ON Augment 59.2 76.8 77.5 60.5 77.1 77.5 64.4 77.1 77.6 73.3 78.9 80.3 76.8 79.3 80.9 80.2 79.9 82.1 80.5 80.5 82.6 81.6 80.8 83.1 85.8 83.3 85.6 87.5 86.1 87.6 88.3 87.2 88.7 89.1 87.9 88.9 uating WSD systems on data drawn from different domains, several prior research efforts (Escudero et al., 2000; Martinez and Agirre, 2000) observed a similar drop in performance of about 10% when a WSD system that was trained on the BC part of the DSO corpus was evaluated on the WSJ part of the corpus, and vice versa. In the rest of this paper, we perform domain adaptation experiments for WSD, focusing on domain adaptation methods that use in-domain annotated data. In particular, we use a feature augmentation technique recently introduced by Daume III (2007), and active learning (Lewis and Gale, 1994) to perform domain adaptation of WSD systems. 5 Combining In-Domain and Out-of-Domain Data for Trainin</context>
</contexts>
<marker>Escudero, Marquez, Riagu, 2000</marker>
<rawString>G. Escudero, L. Marquez, and G. Riagu. 2000. An Empirical Study of the Domain Dependence of Supervised Word Sense Disambiguation Systems. In Proc. of EMNLP/VLC00, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fujii</author>
<author>K Inui</author>
<author>T Tokunaga</author>
<author>H Tanaka</author>
</authors>
<title>Selective Sampling for Example-based Word Sense Disambiguation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="23695" citStr="Fujii et al., 1998" startWordPosition="3969" endWordPosition="3972">needed. WSD Accuracies on Section 23 WSD Accuracy (%) 90 88 86 84 82 80 78 76 50 100 150 200 300 400 500 all SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 Iteration Number Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types. Each curve represents the adaptation process of applying active learning on a certain number of most frequently occurring word types. In WSD, several prior research efforts have successfully used active learning to reduce the annotation effort required (Zhu and Hovy, 2007; Chan and Ng, 2007; Chen et al., 2006; Fujii et al., 1998). With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. Also, these prior research efforts only experimented with a few word types. In contrast, we perform active learning experiments on the hundreds of word types in the OntoNotes data, with the aim of adapting our WSD system trained on SEMCOR to the WSJ domain represented by the On</context>
</contexts>
<marker>Fujii, Inui, Tokunaga, Tanaka, 1998</marker>
<rawString>A. Fujii, K. Inui, T. Tokunaga, and H. Tanaka. 1998. Selective Sampling for Example-based Word Sense Disambiguation. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>OntoNotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proc. ofHLT-NAACL06,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="6401" citStr="Hovy et al., 2006" startWordPosition="1018" endWordPosition="1021">task evaluations were conducted on relatively small sets of evaluation data. For instance, the evaluation data of SENSEVAL-2 and SENSEVAL-3 English all-words task consists of 2,473 and 2,041 test examples respectively. In SemEval-2007, the fine-grained English all-words task consists of only 465 test examples, while the SemEval-2007 coarse-grained English all-words task consists of 2,269 test examples. Hence, it is necessary to address the issues of sense granularity, and the lack of both training and test data. To this end, a recent large-scale annotation effort called the OntoNotes project (Hovy et al., 2006) was started. Building on the annotations from the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993), the project added several new layers of semantic annotations, such as coreference information, word senses, etc. In its first release (LDC2007T21) through the Linguistic Data Consortium (LDC), the project manually sense-tagged more than 40,000 examples belonging to hundreds of noun and verb types with an ITA of 90%, based on a coarse-grained sense inventory, where each word has an average of only 3.2 senses. Thus, besides providing WSD examples that were sense-tagged</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% solution. In Proc. ofHLT-NAACL06, pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y K Lee</author>
<author>H T Ng</author>
</authors>
<title>An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP02,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="9166" citStr="Lee and Ng, 2002" startWordPosition="1472" endWordPosition="1475">m OntoNotes. In Section 4, we investigate the WSD performance when we train our system on examples that are gathered from a different domain as compared to the OntoNotes evaluation data. In Section 5, we perform domain adaptation experiments using a recently introduced feature augmentation technique. In Section 6, we investigate the use of active learning to reduce the annotation effort required to adapt our WSD system to the domain of the OntoNotes data, before concluding in Section 7. 2 The WSD System For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words. For local collocations, we use 11 features: C−1,−1, C1,1, C−2,−2, C2,2, C−2,−1, C−1,1, C1,2, C−3,−1, C−2,1, C−1,2, and C1,3, where Ci,j refers to the ordered sequence of tokens in the local context of an ambiguous word w. Offsets i and j denote the starting and ending position (relative to w) of the sequence, where a negative (positive) offset refers to a token to its left (right). For parts-of-speech, we use 7 features: P−3, P−2, P−1, P0, P1, P2</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation. In Proc. of EMNLP02, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>W A Gale</author>
</authors>
<title>A Sequential Algorithm for Training Text Classifiers.</title>
<date>1994</date>
<booktitle>In Proc. of SIGIR94.</booktitle>
<contexts>
<context position="17932" citStr="Lewis and Gale, 1994" startWordPosition="2987" endWordPosition="2990">.1 87.9 88.9 uating WSD systems on data drawn from different domains, several prior research efforts (Escudero et al., 2000; Martinez and Agirre, 2000) observed a similar drop in performance of about 10% when a WSD system that was trained on the BC part of the DSO corpus was evaluated on the WSJ part of the corpus, and vice versa. In the rest of this paper, we perform domain adaptation experiments for WSD, focusing on domain adaptation methods that use in-domain annotated data. In particular, we use a feature augmentation technique recently introduced by Daume III (2007), and active learning (Lewis and Gale, 1994) to perform domain adaptation of WSD systems. 5 Combining In-Domain and Out-of-Domain Data for Training In this section, we will first introduce the AUGMENT technique of Daume III (2007), before showing the performance of our WSD system with and without using this technique. 5.1 The AUGMENT technique for Domain Adaptation The AUGMENT technique introduced by Daume III (2007) is a simple yet very effective approach to performing domain adaptation. This technique is applicable when one has access to training data from the source domain and a small amount of training data from the target domain. T</context>
<context position="22968" citStr="Lewis and Gale, 1994" startWordPosition="3835" endWordPosition="3838">amples, does continue to perform well even when one has a large number of in-domain examples. 6 Active Learning with AUGMENT Technique So far in this paper, we have seen that when we have access to some in-domain examples, a good strategy is to combine the out-of-domain and in-domain examples via the AUGMENT technique. This suggests that when one wishes to apply a WSD system to a new domain of interest, it is worth the effort to annotate a small number of examples gathered from the new domain. However, instead of randomly selecting in-domain examples to annotate, we could use active learning (Lewis and Gale, 1994) to help select in-domain examples to annotate. By doing so, we could minimize the manual annotation effort needed. WSD Accuracies on Section 23 WSD Accuracy (%) 90 88 86 84 82 80 78 76 50 100 150 200 300 400 500 all SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 Iteration Number Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types. Each curve represents the adaptation process of applying active learning on a certain number of most frequently occurring word types. In WSD, several prior research efforts have successfully used active l</context>
<context position="24409" citStr="Lewis and Gale, 1994" startWordPosition="4094" endWordPosition="4097">BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. Also, these prior research efforts only experimented with a few word types. In contrast, we perform active learning experiments on the hundreds of word types in the OntoNotes data, with the aim of adapting our WSD system trained on SEMCOR to the WSJ domain represented by the OntoNotes data. For our active learning experiments, we use the uncertainty sampling strategy (Lewis and Gale, 1994), as shown in Figure 2. For our experiments, the SEMCOR examples will be our initial set of training examples, while the OntoNotes examples from sections 02-21 will be used as our pool of adaptation examples, from which we will select examples to annotate via active learning. Also, since we have found that the AUGMENT technique is useful in increasing WSD accuracy, we will apply the AUGMENT technique during each iteration of active learning to combine the SEMCOR examples and the selected adaptation examples. As shown in Figure 2, we train an initial WSD system using only the set DS of SEMCOR e</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>D. D. Lewis and W. A. Gale. 1994. A Sequential Algorithm for Training Text Classifiers. In Proc. of SIGIR94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="6528" citStr="Marcus et al., 1993" startWordPosition="1039" endWordPosition="1042">and SENSEVAL-3 English all-words task consists of 2,473 and 2,041 test examples respectively. In SemEval-2007, the fine-grained English all-words task consists of only 465 test examples, while the SemEval-2007 coarse-grained English all-words task consists of 2,269 test examples. Hence, it is necessary to address the issues of sense granularity, and the lack of both training and test data. To this end, a recent large-scale annotation effort called the OntoNotes project (Hovy et al., 2006) was started. Building on the annotations from the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993), the project added several new layers of semantic annotations, such as coreference information, word senses, etc. In its first release (LDC2007T21) through the Linguistic Data Consortium (LDC), the project manually sense-tagged more than 40,000 examples belonging to hundreds of noun and verb types with an ITA of 90%, based on a coarse-grained sense inventory, where each word has an average of only 3.2 senses. Thus, besides providing WSD examples that were sense-tagged with a high ITA, the project also addressed the previously discussed issues of a lack of training and test data. In this paper</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Martinez</author>
<author>E Agirre</author>
</authors>
<title>One Sense per Collocation and Genre/Topic Variations.</title>
<date>2000</date>
<booktitle>In Proc. of EMNLP/VLC00,</booktitle>
<pages>207--215</pages>
<contexts>
<context position="17462" citStr="Martinez and Agirre, 2000" startWordPosition="2906" endWordPosition="2909">sing SEMCOR and different OntoNotes sections as training data. ON: only OntoNotes as training data. SC+ON: SEMCOR and OntoNotes as training data, SC+ON Augment: Combining SEMCOR and OntoNotes via the Augment domain adaptation technique. WSD accuracy (%) 100 95 90 85 80 75 70 65 60 55 ON SC+ON SC+ON Augment 59.2 76.8 77.5 60.5 77.1 77.5 64.4 77.1 77.6 73.3 78.9 80.3 76.8 79.3 80.9 80.2 79.9 82.1 80.5 80.5 82.6 81.6 80.8 83.1 85.8 83.3 85.6 87.5 86.1 87.6 88.3 87.2 88.7 89.1 87.9 88.9 uating WSD systems on data drawn from different domains, several prior research efforts (Escudero et al., 2000; Martinez and Agirre, 2000) observed a similar drop in performance of about 10% when a WSD system that was trained on the BC part of the DSO corpus was evaluated on the WSJ part of the corpus, and vice versa. In the rest of this paper, we perform domain adaptation experiments for WSD, focusing on domain adaptation methods that use in-domain annotated data. In particular, we use a feature augmentation technique recently introduced by Daume III (2007), and active learning (Lewis and Gale, 1994) to perform domain adaptation of WSD systems. 5 Combining In-Domain and Out-of-Domain Data for Training In this section, we will f</context>
</contexts>
<marker>Martinez, Agirre, 2000</marker>
<rawString>D. Martinez and E. Agirre. 2000. One Sense per Collocation and Genre/Topic Variations. In Proc. of EMNLP/VLC00, pages 207–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>D Moldovan</author>
</authors>
<title>Pattern Learning and Active Feature Selection for Word Sense Disambiguation.</title>
<date>2001</date>
<booktitle>In Proc. of SENSEVAL-2,</booktitle>
<pages>127--130</pages>
<contexts>
<context position="2685" citStr="Mihalcea and Moldovan, 2001" startWordPosition="423" endWordPosition="426">tricting the accuracy of WSD systems. Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data. To provide a standardized test-bed for evaluation of WSD systems, a series of evaluation exercises called SENSEVAL were held. In the English all-words task of SENSEVAL-2 and SENSEVAL3 (Palmer et al., 2001; Snyder and Palmer, 2004), no training data was provided and systems must tag all the content words (noun, verb, adjective, and adverb) in running English texts with their correct WordNet senses. In SENSEVAL-2, the best performing system (Mihalcea and Moldovan, 2001) in the English all-words task achieved an accuracy of 69.0%, while in SENSEVAL-3, the best performing system (Decadt et al., 2004) achieved an accuracy of 65.2%. In SemEval-2007, which was the most recent SENSEVAL evaluation, a similar English all-words task was held, where systems had to provide the correct WordNet sense tag for all the verbs and head words of their arguments in running English texts. For this task, the best performing system (Tratz et al., 2007) achieved an accuracy of 59.1%. Results of these evaluations showed that state-of-the-art English all-words WSD systems performed w</context>
<context position="14799" citStr="Mihalcea and Moldovan, 2001" startWordPosition="2452" endWordPosition="2455">ata. Further, all these training data and test data are gathered from the same domain of WSJ. In reality, however, since manual sense annotation is time consuming, it is not feasible to collect such a large amount of manually sense-tagged data for every domain of interest. Hence, in this section, we investigate the performance of our WSD system when it is trained on out-of-domain data. In the English all-words task of the previous SENSEVAL evaluations (SENSEVAL-2, SENSEVAL3, SemEval-2007), the best performing English all-words task systems with the highest WSD accuracy were trained on SEMCOR (Mihalcea and Moldovan, 2001; Decadt et al., 2004; Chan et al., 2007b). Hence, we similarly trained our WSD system on SEMCOR and evaluated on section 23 of the OntoNotes corpus. For those word types in section 23 which do not have training examples from SEMCOR, we randomly chose an OntoNotes sense as the answer. In training on SEMCOR, we have also ensured that there is a domain difference between our training and test data. This is because while the OntoNotes data was gathered from WSJ, which contains mainly business related news, the SEMCOR corpus is the sense-tagged portion of the Brown Corpus (BC), which is a mixture </context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>R. Mihalcea and D. Moldovan. 2001. Pattern Learning and Active Feature Selection for Word Sense Disambiguation. In Proc. of SENSEVAL-2, pages 127–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>M Chodorow</author>
<author>S Landes</author>
<author>C Leacock</author>
<author>R G Thomas</author>
</authors>
<title>Using a Semantic Concordance for Sense Identification.</title>
<date>1994</date>
<booktitle>In Proc. of ARPA Human Language Technology Workshop,</booktitle>
<pages>240--243</pages>
<contexts>
<context position="5098" citStr="Miller et al., 1994" startWordPosition="810" endWordPosition="813">appropriate level of sense granularity. Another issue faced by current WSD systems is the lack of training data. We note that the top performing systems mentioned in the previous paragraphs are all based on supervised learning. With this approach, however, one would need to obtain a corpus where each ambiguous word occurrence is manually annotated with the correct sense, to serve as training data. Since it is time consuming to perform sense annotation of word occurrences, only a handful of sense-tagged corpora are publicly available. Among the existing sense-tagged corpora, the SEMCOR corpus (Miller et al., 1994) is one of the most widely used. In SEMCOR, content words have been manually tagged with WordNet senses. Current supervised WSD systems (which include all the top-performing systems in the English all-words task) usually rely on this relatively small manually annotated corpus for training examples, and this has inevitably affected the accuracy and scalability of current WSD systems. Related to the problem of a lack of training data for WSD, there is also a lack of test data. Having a large amount of test data for evaluation is important to ensure the robustness and scalability of WSD systems. </context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>G. A. Miller, M. Chodorow, S. Landes, C. Leacock, and R. G. Thomas. 1994. Using a Semantic Concordance for Sense Identification. In Proc. of ARPA Human Language Technology Workshop, pages 240–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1943" citStr="Miller, 1990" startWordPosition="305" endWordPosition="306">ion In language, many words have multiple meanings. The process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD). WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation (MT) (Chan et al., 2007a; Carpuat and Wu, 2007), information retrieval (IR), etc. WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process. In current WSD research, WordNet (Miller, 1990) is usually used as the sense inventory. WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems. Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data. To provide a standardized test-bed for evaluation of WSD systems, a series of evaluation exercises called SENSEVAL were held. In the English all-words task of SENSEVAL-2 and SENSEVAL3 (Palmer et al., 2001; Snyder and Palmer, 2004), no training data was provided and systems must tag all the content words (noun, verb, adjective, a</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. A. Miller. 1990. WordNet: An On-line Lexical Database. International Journal of Lexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>K C Litkowski</author>
<author>O Hargraves</author>
</authors>
<date>2007</date>
<booktitle>SemEval-2007 Task 07: Coarse-Grained English AllWords Task. In Proc. of SemEval-2007,</booktitle>
<pages>30--35</pages>
<contexts>
<context position="4195" citStr="Navigli et al., 2007" startWordPosition="662" endWordPosition="665">the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1002–1010, Honolulu, October 2008. c�2008 Association for Computational Linguistics such as MT and IR. As mentioned earlier, one of the major reasons for the low performance is that these evaluation exercises adopted WordNet as the reference sense inventory, which is often too finegrained. As an indication of this, inter-annotator agreement (ITA) reported for manual sense-tagging on these SENSEVAL English all-words datasets is typically in the mid-70s. To address this issue, a coarse-grained English all-words task (Navigli et al., 2007) was conducted during SemEval-2007. This task used a coarse-grained version of WordNet and reported an ITA of around 90%. We note that the best performing system (Chan et al., 2007b) of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity. Another issue faced by current WSD systems is the lack of training data. We note that the top performing systems mentioned in the previous paragraphs are all based on supervised learning. With this approach, however, one would need to obtain a corpus where each ambiguous word </context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>R. Navigli, K. C. Litkowski, and O. Hargraves. 2007. SemEval-2007 Task 07: Coarse-Grained English AllWords Task. In Proc. of SemEval-2007, pages 30–35.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
</authors>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, </marker>
<rawString>J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL07,</booktitle>
<pages>915--932</pages>
<marker>Riedel, Yuret, 2007</marker>
<rawString>S. Riedel, and D. Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In Proc. of EMNLP-CoNLL07, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>C Fellbaum</author>
<author>S Cotton</author>
<author>L Delfs</author>
<author>H T Dang</author>
</authors>
<title>English Tasks: All-Words and Verb Lexical Sample.</title>
<date>2001</date>
<booktitle>In Proc. of SENSEVAL-2,</booktitle>
<pages>21--24</pages>
<contexts>
<context position="2417" citStr="Palmer et al., 2001" startWordPosition="381" endWordPosition="384">is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process. In current WSD research, WordNet (Miller, 1990) is usually used as the sense inventory. WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems. Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data. To provide a standardized test-bed for evaluation of WSD systems, a series of evaluation exercises called SENSEVAL were held. In the English all-words task of SENSEVAL-2 and SENSEVAL3 (Palmer et al., 2001; Snyder and Palmer, 2004), no training data was provided and systems must tag all the content words (noun, verb, adjective, and adverb) in running English texts with their correct WordNet senses. In SENSEVAL-2, the best performing system (Mihalcea and Moldovan, 2001) in the English all-words task achieved an accuracy of 69.0%, while in SENSEVAL-3, the best performing system (Decadt et al., 2004) achieved an accuracy of 65.2%. In SemEval-2007, which was the most recent SENSEVAL evaluation, a similar English all-words task was held, where systems had to provide the correct WordNet sense tag for</context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, Dang, 2001</marker>
<rawString>M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H. T. Dang. 2001. English Tasks: All-Words and Verb Lexical Sample. In Proc. of SENSEVAL-2, pages 21–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="16190" citStr="Palmer et al., 2005" startWordPosition="2691" endWordPosition="2694">acy achievable when we had trained on examples from sections 02-21, this is a substantially lower and disappointing drop of performance and motivates the need for domain adaptation. The need for domain adaptation is a general and important issue for many NLP tasks (Daume III and Marcu, 2006). For instance, SRL systems are usually trained and evaluated on data drawn from the WSJ. In the CoNLL-2005 shared task on SRL (Carreras and Marquez, 2005), however, a task of training and evaluating systems on different domains was included. For that task, systems that were trained on the PropBank corpus (Palmer et al., 2005) (which was gathered from the WSJ), suffered a 10% drop in accuracy when evaluated on test data drawn from BC, as compared to the performance achievable when evaluated on data drawn from WSJ. More recently, CoNLL-2007 included a shared task on dependency parsing (Nivre et al., 2007). In this task, systems that were trained on Penn Treebank (drawn from WSJ), but evaluated on data drawn from a different domain (such as chemical abstracts and parent-child dialogues) showed a similar drop in performance. For research involving training and eval1005 WSD Accuracies on Section 23 Section number Figur</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>M Palmer</author>
</authors>
<title>The English All-Words Task.</title>
<date>2004</date>
<booktitle>In Proc. of SENSEVAL-3,</booktitle>
<pages>41--43</pages>
<contexts>
<context position="2443" citStr="Snyder and Palmer, 2004" startWordPosition="385" endWordPosition="388">abel (from a pre-defined sense inventory) during the disambiguation process. In current WSD research, WordNet (Miller, 1990) is usually used as the sense inventory. WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems. Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data. To provide a standardized test-bed for evaluation of WSD systems, a series of evaluation exercises called SENSEVAL were held. In the English all-words task of SENSEVAL-2 and SENSEVAL3 (Palmer et al., 2001; Snyder and Palmer, 2004), no training data was provided and systems must tag all the content words (noun, verb, adjective, and adverb) in running English texts with their correct WordNet senses. In SENSEVAL-2, the best performing system (Mihalcea and Moldovan, 2001) in the English all-words task achieved an accuracy of 69.0%, while in SENSEVAL-3, the best performing system (Decadt et al., 2004) achieved an accuracy of 65.2%. In SemEval-2007, which was the most recent SENSEVAL evaluation, a similar English all-words task was held, where systems had to provide the correct WordNet sense tag for all the verbs and head wo</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>B. Snyder and M. Palmer. 2004. The English All-Words Task. In Proc. of SENSEVAL-3, pages 41–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tratz</author>
<author>A Sanfilippo</author>
<author>M Gregory</author>
<author>A Chappell</author>
<author>C Posse</author>
<author>P Whitney</author>
</authors>
<title>PNNL: A Supervised Maximum Entropy Approach to Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval-2007,</booktitle>
<pages>264--267</pages>
<contexts>
<context position="3154" citStr="Tratz et al., 2007" startWordPosition="505" endWordPosition="508">ctive, and adverb) in running English texts with their correct WordNet senses. In SENSEVAL-2, the best performing system (Mihalcea and Moldovan, 2001) in the English all-words task achieved an accuracy of 69.0%, while in SENSEVAL-3, the best performing system (Decadt et al., 2004) achieved an accuracy of 65.2%. In SemEval-2007, which was the most recent SENSEVAL evaluation, a similar English all-words task was held, where systems had to provide the correct WordNet sense tag for all the verbs and head words of their arguments in running English texts. For this task, the best performing system (Tratz et al., 2007) achieved an accuracy of 59.1%. Results of these evaluations showed that state-of-the-art English all-words WSD systems performed with an accuracy of 60%–70%, using the fine-grained sense inventory of WordNet. The low level of performance by these state-ofthe-art WSD systems is a cause for concern, since WSD is supposed to be an enabling technology to be incorporated as a module into applications 1002 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1002–1010, Honolulu, October 2008. c�2008 Association for Computational Linguistics such as MT and IR</context>
</contexts>
<marker>Tratz, Sanfilippo, Gregory, Chappell, Posse, Whitney, 2007</marker>
<rawString>S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell, C. Posse, and P. Whitney. 2007. PNNL: A Supervised Maximum Entropy Approach to Word Sense Disambiguation. In Proc. of SemEval-2007, pages 264–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Zhu</author>
<author>E Hovy</author>
</authors>
<title>Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL07,</booktitle>
<pages>783--790</pages>
<contexts>
<context position="23636" citStr="Zhu and Hovy, 2007" startWordPosition="3957" endWordPosition="3960"> doing so, we could minimize the manual annotation effort needed. WSD Accuracies on Section 23 WSD Accuracy (%) 90 88 86 84 82 80 78 76 50 100 150 200 300 400 500 all SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 Iteration Number Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types. Each curve represents the adaptation process of applying active learning on a certain number of most frequently occurring word types. In WSD, several prior research efforts have successfully used active learning to reduce the annotation effort required (Zhu and Hovy, 2007; Chan and Ng, 2007; Chen et al., 2006; Fujii et al., 1998). With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. Also, these prior research efforts only experimented with a few word types. In contrast, we perform active learning experiments on the hundreds of word types in the OntoNotes data, with the aim of adapting our WSD syste</context>
</contexts>
<marker>Zhu, Hovy, 2007</marker>
<rawString>J. B. Zhu and E. Hovy. 2007. Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem. In Proc. of EMNLPCoNLL07, pages 783–790.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>