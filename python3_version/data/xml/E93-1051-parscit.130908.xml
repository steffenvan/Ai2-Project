<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000141">
<title confidence="0.998713">
Lexical Disambiguation Using
Constraint Handling In Prolog (CHIP) *
</title>
<author confidence="0.984838">
George C. Demetriou
</author>
<affiliation confidence="0.8207145">
Centre for Computer Analysis of Language And Speech (CCALAS)
Artificial Intelligence Division, School of Computer Studies, University of Leeds
</affiliation>
<address confidence="0.602421">
Leeds, LS2 9JT, United Kingdom
</address>
<sectionHeader confidence="0.997821" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994364">
Automatic sense disambiguation has been recognised
by the research community as very important for
a number of natural language processing applica-
tions like information retrieval, machine translation,
or speech recognition. This paper describes exper-
iments with an algorithm for lexical sense disam-
biguation, that is, predicting which of many possible
senses of a word is intended in a given sentence. The
definitions of senses of a given word are those used in
LDOCE, the Longman Dictionary of Contemporary
English [Procter et at., 1978]. The algorithm first as-
signs a set of meanings or senses drawn from LDOCE
to each word in the given sentence, and then chooses
the combination of word-senses (one for each word in
the sentence), yielding the maximum semantic over-
lap. The metric of semantic overlap is based on the
fact that LDOCE sense definitions are made in terms
of the Longman Defining Vocabulary, effectively a
(large) set of semantic primitives. Since the prob-
lem of finding the word-sense-chain with maximum
overlap can be viewed as a specialised example of
the class of constraint-based optimisation problems
for which Constraint Handling In Prolog (CHIP) was
designed, we have chosen to implement our algorithm
in CHIP.
</bodyText>
<sectionHeader confidence="0.9964285" genericHeader="keywords">
2 Background: LDOCE, Word Sense
Disambiguation and related work
</sectionHeader>
<bodyText confidence="0.999066323943662">
LDOCE&apos;s important feature is that its definitions
(and examples) are written in a controlled vocab-
ulary of 2187 words. A definition is therefore al-
ways written in simpler terms than the word it de-
scribes. These 2187 words effectively constitute se-
mantic primitives, and any particular word-sense is
defined by a set of these primitives.
Several researchers have been experimented with
lexical disambiguation using MRDs, including [Lesk,
1986; Wilks et al., 1989; McDonald et al., 1990;
Veronis and Ide, 1990; Guthrie et al., 1991; Guthrie
et at., 1992]. Lesk&apos;s technique decides the cor-
rect sense of a word by counting the overlap be-
tween a dictionary sense definition (of the word to
be disambiguated) and the definitions of the nearby
words in the phrase. Performance (using brief ex-
perimentation) was reported 50-70% and the results
This work was supported by the Greek Employment
Manpower Organisation (OAED), Ministry of Labour, as
part of an 1991-93 scholarship scheme.
were roughly comparable between Webster&apos;s 7th
Collegiate, Collins English Dictionary and Oxford
Advanced Learner&apos;s Dictionary of Current English.
Methods based on co-occurence statistics have been
used by [Wilks et cd., 1989; McDonald et al., 1990;
Guthrie et at., 1991]. By co-occurence is meant the
preference two words appear together in the same
context. [Wilks et at., 1989] computed lexical neigh-
bourhoods for all the words of the controlled vocab-
ulary of LDOCE. This neighbourhood information
is used for partitioning the words according to the
senses they correspond to in order to make a clas-
sification of the senses. Their results for using oc-
curences of the word bank were about 53% for the
classification of each instance into one of the thirteen
sense definitions of LDOCE and 85-90% into one of
the more general coarse meanings. Neighbourhoods
were used by [McDonald et al., 1990] for expanding
the word sense definitions. The union of neighbour-
hoods is then intersected with the local context and
the largest overlap gives the most likely sense. A sim-
ilar technique is used by [Guthrie et al., 1991] except
that they define neighbourhoods according to sub-
ject categories (i.e engineering, economic etc.) based
on the subject code markings of the on-line version
of LDOCE.
Closer to the work we describe in this paper is
[Guthrie et al., 19921&apos;s. They try to deal with large-
scale text data disambiguation problems. Their
method is based on the idea that the correct mean-
ing of a complete phrase should be extracted by con-
current evaluation of sets of senses for the words to
be disambiguated. They count the overlap between
sense definitions of the words of the sentence as they
appear in the on-line version of LDOCE. The prob-
lem is that the number of sense combinations in-
creases rapidly if the sentence contains ambiguous
words having a considerable number of sense defini-
tions in LDOCE (say that word A has X different
senses in LDOCE, B has Y and C has Z, then the
number of possible sense combinations of the phrase
ABC is X*Y*Z, e.g if X=Y=Z=10 sense definitions
for each word then we have 1000 possible sense com-
binations). Simulated annealing is used by [Guthrie
et al., 1992] to reduce the search space and find an
optimal (or near-optimal) solution without generat-
ing and evaluating all possible solutions, or pruning
the search space and testing a well-defined subspace
of reasonable candidate solutions. The success of
their algorithm is reported 47% at sense level and
72% at homograph level using 50 example sentences
</bodyText>
<page confidence="0.998041">
431
</page>
<bodyText confidence="0.748193">
from LDOCE.
</bodyText>
<sectionHeader confidence="0.878955" genericHeader="method">
3 CHIP: Constraint Handling In
Prolog
</sectionHeader>
<bodyText confidence="0.999286692307693">
We decided it was worthwhile investigating the use
of a constraint handling language so that we could
exhaustively search the space by applying CHIP&apos;s op-
timisation procedures. A CHIP compiler is available
from International Computers Limited (ICL) as part
of its DecisionPower prolog-based toolkit 1. CHIP
extends usual Prolog-like logic programming by in-
troducing three new computation domains of finite
restricted terms, boolean terms and linear rational
terms. Another feature offered by CHIP is the demon
constructs used for user-defined constraints to imple-
ment the local propagation. For each of them CHIP
uses specialised constraint solving techniques: con-
sistency techniques for finite domains, equation solv-
ing in Boolean algebra, and a symbolic simplex-like
algorithm. CHIP&apos;s declarations are used to define
the domain of variables or to choose one of the spe-
cialised unification algorithms; they can be: (1) finite
domains (i.e. variables range over finite domains and
terms are constructed from natural numbers, domain
variables over natural numbers and operators); (2)
boolean declarations or (3) demon declarations (for
specifying a data-driven behaviour; they consist of a
set of rules which describe how a constraint can be
satisfied). In addition, classes of built-in predicates
over finite domain variables exist for: (1) arithmetic
and symbolic constraints (basic constraints for do-
main variables), (2) choice predicates (help making
choices), (3) higher order predicates (providing opti-
misation methods for combinatorial problems using
depth-first and branch and bound strategies) and
(4) extra-logical predicates (for help in debugging
processes). Forward checking and looking ahead in-
ference rules are introduced for the control mecha-
nism in the computation of constraints using finite
domains. Auxiliary predicates to monitor or control
the resolution process in the CHIP environment also
exist.
In our case we were particularly interested in
transforming the general structure of our algorithm
into a form usable by CHIP&apos;s choice and higher or-
der built-in predicates. Choice predicates are used
for the automatic generation of word-sense combina-
tions and higher order predicates facilitate the pro-
cess of finding the most likely combination according
to the &apos;score&apos; of overlap. To get an idea of this kind
of implementation the main core of the optimisation
part of our program looks like this:
optimize(Words,Choice,Cost):-
minimize((makeChoice(Choice),
findCost(Choice,Cost)), Cost).
&apos;DecisionPower donated by ICL under the University
Funding Council&apos;s Knowledge and Constraint Manage-
ment (KCM) Initiative.
Minimize is one of CHIP&apos;s optimisation built-in
predicates. Words represents the list of am-
biguous words submitted to the program and
Choice a list of domain variables for the selec-
tion of sense definitions. Cost is a domain vari-
able whose domain is constrained to an arithmetic
term. For our purposes, Cost was Max-Overlap
where Max (a maximum possible score) is large
enough so that Overlap (score of overlap in a
sense definition) can never exceed it. Any answer
substitution that causes (makeChoice (Choice) ,
f indCost (Choice , Cost ) ) to be ground also causes
Cost to be ground. The search then back-
tracks to the last choice point and continuous
along another branch. The cost of any other
solution found in the sub-tree must be neces-
sarily lower (i.e Overlap must be higher) than
the last one found, because Cost is constrained
to that bound. This process of backtracking
for better solutions and imposing constraints on
Cost continues until the space has been searched
implicitly. At the end, (mak eCho ic e (Choic e) ,
f indCost (Choice , Cost ) is bound to the last solu-
tion found which is the optimal one.
</bodyText>
<sectionHeader confidence="0.996944" genericHeader="method">
4 Algorithm
</sectionHeader>
<bodyText confidence="0.999205625">
Our method is based on the overlap between sense
definitions of the words to be disambiguated. This
is similar to [Guthrie et al., 1992] although there are
distinct differences on the scoring method and the
implementation. To illustrate our method we use
the following example and describe each phase:
The bank arranged for an overdraft on my
account.
</bodyText>
<subsectionHeader confidence="0.948443">
4.1 Step 1
</subsectionHeader>
<bodyText confidence="0.999994590909091">
All the common function words (particles) belonging
to our &apos;stop list&apos; (a set of 38 very common words)
e.g. for our example the set of words (the, for, an,
on, my) should be removed. Function words tend to
appear very often both in context and in sense def-
initions for syntactic and style reasons rather than
pure semantics. Since our algorithm is intended to
maximise overlap the participation of function words
in a definition chain could lead to false interpreta-
tion for the correct sense combination. Moreover,
function words are usually much more ambiguous
than content words (for example, there are 21 listed
senses of the word the and 35 of for in LDOCE).
Thus, the searching process could be significantly
increased without any obvious benefit to the reso-
lution of ambiguity of context words as explained
above. Words of the &apos;stop list&apos; have also been re-
moved from the sense definitions and the remaining
words are stemmed so that only their roots appear in
the definition. With this way, derived (or inflected)
forms of the same word can be matched together.
For this reason, the program also uses the primitive
</bodyText>
<page confidence="0.993383">
432
</page>
<bodyText confidence="0.984632186046512">
or root forms of the input words. After function-
word-deletion the program is given the following set
of words:
bank arrange overdraft account
These are processed according to their stemmed
sense definitions in LDOCE, represented as Prolog
database structures such as:
bank( [
[bank, land, along, side, river,
lake],
[bank, earth, heap, field, garden,
make, border, division],
[bank, mass, snow, cloud, mud],
[bank, slope, make, bend, road, race,
track, safer, car, go, round],
[bank, sandbank],
[bank, car, aircraft, move, side,
higher, other, make, turn],
[bank, row, oar, ancient, boat, key,
typewriter],
[bank, place, money, keep, pay,
demand, relate, activity, go],
[bank, place, something, hold, ready,
use, organic, product, human,
origin, medical, use],
[bank, person, keep, supply, money,
piece, payment, use, game, chance],
[bank, win, money, game, chance],
[bank, put, keep, money, bank],
[keep, money, state, bank]]).
The conventions we use are: a) Each word to be
disambiguated is the functor of a predicate, contain-
ing a list with stemmed sense definitions (in lists).
b) We do not put a subject code in each sense defi-
nition (as [Guthrie et al., 19921 do). Instead we put
the word to be disambiguated as a member of the
list of each sense definition. The rationale behind
this is that although a word put in its sense defini-
tion cannot help with the disambiguation of itself, it
can provide help in the disambiguation of the other
words if it appears in their sense definitions. c) Com-
pound words of the form &apos;race-track&apos; were used as
two words &apos;race&apos; and &apos;track&apos;.
</bodyText>
<subsectionHeader confidence="0.938884">
4.2 Step 2
</subsectionHeader>
<bodyText confidence="0.997320666666667">
The algorithm generates sense combinations by go-
ing through the sense definitions for each word one
by one. For example, a sense combination can be
called by taking the 8th sense of bank (call it b8,
see above), the first sense of arrange (al4arrange,
set, good, please, order)), the definition of over-
draft (o1_â€”[overdraft, sum, lend, person, bank, more,
money, have, bank]), and the seventh of account
(c7=1-account, sum, money, keep, bank, add, take.)).
The scoring process for this sense combination is
given by taking the definitions pairwise and count-
ing the overlap of words between them. Before the
program proceeds to counting, redundancy of words
is eliminated in each sense definition in order to pre-
vent each word from being counted more than once.
The algorithm checks for word overlap in advance
and in case this constraint is not satisfied, the com-
bination is discarded and a new one generated so
that only overlapping combinations are considered.
For each combination the total score is the sum of
all the overlaps pairwise. This means that for n am-
biguous words in the sentence the program counts
the overlap for all n.f/(2!(n-2)!) pair combinations
and add them together. For the above example,
</bodyText>
<equation confidence="0.596439285714286">
score(b8a1o1c7)= overlap(b8a1)
+overlap(b8o1)
+overlap(b8c7)
+overlap(a1o1)
+overlap(alc7)
+overlap(o1c7)
=0+2+3+0+0+3= 8
</equation>
<bodyText confidence="0.999859">
This scoring method is quite different to the one
used by [Lesk, 1986]. Lesk simply counted overlaps
by comparing each sense definition of a word with
all the sense definitions of the other words. [Guthrie
et al., 1992] use a similar method. It is different
in that if there is a subject (pragmatic) code for a
sense definition they put this subject code as a single
word in the definition list. Then they go through
each list of the words, put the word in an array and
begin a counter at 0. If the word is already in the
list they increment the counter. So if, for example,
three definitions have the same word they count it 2,
while with our method this counts 3 and it seems that
our method generally overestimates. Although no
evidence of the best scoring scheme can be obtained
without results we think that our method may work
better in chains where all definitions share a common
word (and this overestimation goes higher compared
to [Guthrie et al., 19921) which may indicate a strong
preference for that combination.
</bodyText>
<subsectionHeader confidence="0.997868">
4.3 Step 3
</subsectionHeader>
<bodyText confidence="0.982933333333333">
If a new generated combination has a higher score,
it is considered as a better solution. This new (tem-
porary maximum) score acts as a constraint (a lower
minimum) to new generated combinations. At the
end, the most likely sense combination is the one
with the highest score. Implementation in CHIP
guarantees to give one and only solution (or no so-
lution if no overlapping combination exists). The
way choices are generated is by taking at the be-
ginning the first sense definition for each word in
the sentence. This is because the most common or
most typical meanings of a word are shown first in
LDOCE. Following choices replace the definitions of
the words one by one according to the order these
words are submitted to the program. An example
sentence and its output is illustrated next [Procter
et al., 19781:
Sentence: a tight feeling in the chest.
</bodyText>
<page confidence="0.998293">
433
</page>
<note confidence="0.606889">
Total number of sense combinations: 392
Optimal solution found:
</note>
<table confidence="0.2925215">
tight = [tight, have, produce,
uncomfortable, feeling, closeness,
part, body]
feeling = [feeling, consciousness,
something, feel, mind, body]
chest = [chest, upper, front, part, body,
enclose, heart, lung]
Its Score is: s
</table>
<sectionHeader confidence="0.999771" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99892414">
Evaluation of a dictionary-based lexical disambigua-
tion routine is difficult since the preselection of the
correct senses is in practice very difficult and time-
consuming. The most obvious technique would seem
to be to start by creating a benchmark of sentences,
disambiguating these manually using intuitive lin-
guistic and lexicographical expertise to assign the
best sense-number to each word. However, distinc-
tions between senses are often delicate and fine-
grained in a dictionary, and it is often hard to fit
a particular case into one and only one category. It
is typical in work of this kind that researchers use
human choices for the words or sentences to disam-
biguate and the senses they will attempt to recognise
[Guthrie, 1993]. In most of the cases [Hearst, 1991;
McDonald et al., 1990; Guthrie et al., 1991; Guthrie
et al., 19921, the number of test sentences is rather
small (less than 50) so that no exact comparison be-
tween different methods can be done. Our tests in-
cluded a set of 20 sentences, from sentences cited
in an NLP textbook [Harris, 1985] (used to illus-
trate non-MRD-based semantic disambiguation tech-
niques) example sentences cited in [Guthrie et al.,
1992; Lesk, 1986; Hearst, 1991] (for comparison be-
tween different lexical disambiguation routines) and
examples taken from LDOCE (to assess the algo-
rithm&apos;s performance with example sentences of par-
ticular senses in the dictionary-this might also be
a way of testing the consistency of the relationship
between different senses and their corresponding ex-
amples of a word in LDOCE). A sense chosen by our
algorithm is compared with the &apos;intuitive&apos; sense; but
if there is not an exact match, we need to look further
to judge how &apos;plausible&apos; the predicted sense remains.
After pruning of function words, length varied
from 2 to 6 content words to be disambiguated, with
an average of 3.1 ambiguous words per sentence. The
number of different sense combinations ranged from
15 to 126000.
Of the 62 ambiguous words, 36 were assigned
senses exactly matching our prior intuitions, giving
an overall success rate of 58%. Although accuracy of
the results is far from 100%, the method confirms the
potential contribution of the use of dictionary defini-
tions to the problem of lexical sense disambiguation.
Ambiguous words had between 2 and 44 different
senses. Investigating the success at disambiguating
a particular word depended on the number of alter-
native senses given in the dictionary we had the fol-
lowing results:
</bodyText>
<table confidence="0.967939">
No. senses No. words Disambiguated Success
per word per range correctly
2-5 23 16 70
6-10 19 11 58
11-15 11 5 45
16-20 3 2 67
21-44 6 2 33
</table>
<bodyText confidence="0.945132">
It might be expected that if the algorithm has to
choose between a very large number of alternative
senses it would be much likelier to fail; but in fact
the algorithm held up well against the odds, showing
graceful degradation in success rate with increasing
ambiguity. Furthermore, success rate showed little
variation with increased number of ambiguous words
per sentence:
No. amb. words No. sentences Success
per sentence per range
</bodyText>
<table confidence="0.54606625">
2 7 64
3 8 58
4 2 63
5-6 3 50
</table>
<bodyText confidence="0.999468">
This presumably indicates a balanced trade-off be-
tween competing factors. One might expect that
each extra word brings with it more information
to help disambiguate other words, improving overall
success rate; on the other hand, it also brings with
it spurious senses with primitives which may act as
&apos;red herrings&apos; favouring alternative senses for other
words.
The average overlap score per sentence for the best
analysis rose in line with sentence length, or rather,
number of ambiguous words in the sentence:
</bodyText>
<figure confidence="0.360363666666667">
No. ambiguous words Average overlap for
per sentence best disambiguation
2 2.2
3 3 . 1
4 5 . 0
5-6 .7
</figure>
<page confidence="0.998702">
434
</page>
<bodyText confidence="0.999987395348837">
We noticed a trend towards choosing longer sense-
definitions over shorter ones (i.e senses defined by a
larger set of semantic primitives tended to be pre-
ferred); 41 out of the 62 solutions given by the pro-
gram (66%) were longer definitions than average.
This is to be expected in an algorithm maximising
overlap, as there are more primitives to overlap with
in a larger definition. However, this tendency did
NOT appear to imply wrong long sense were being
preferred to correct short sense leading to a wors-
ening overall success rate: of the 41 cases, 27 were
correct, i.e 66% compared to 58% overall. A better
interpretation of this result might be that longer def-
initions are more detailed and accurate, thus making
a better &apos;target&apos;.
Of the 26 &apos;failures&apos;, 5 were assigned senses which
were in fact incompatible with the syntactic word-
class in the given sentence. This indicates that if
the algorithm was combined with a word-tagger such
as CLAWS [Atwell, 1983; Leech, 1983], and lexical
senses were constrained to those allowed by the word-
tags predicted by CLAWS, the success rate could rise
to 66%. This may also be necessary in cases where
LDOCE&apos;s definitions are not accurate enough. For
example, trying to disambiguate the words show, in-
terest and music in the sentence &apos;He&apos;s showing an
interest in music&apos; [Procter et al., 19781. the pro-
gram chose the eighth noun sense of show and the
second verb sense of interest. This was because the
occurence of the word &apos;do&apos; in both definitions re-
sulted in a maximum overlap for that combination.
However, the &apos;do&apos;s sense is completely different in
each case. For the show &apos;do&apos; was related to &apos;well
done&apos; and for interest to &apos;do something&apos;.
Optimisation with CHIP performed well in finding
the optimal solution. In all cases no other sense com-
bination had a better score than the one found. This
was confirmed by testing our algorithm in a separate
implementation without any of CHIP&apos;s optimisation
procedures but using a conventional method for ex-
ploring the search space for the best solution. Opti-
misation with CHIP was found to be from 120% to
600% faster than the conventional approach.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="conclusions">
6 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999991022222222">
It is difficult to make a straightforward comparison
with other methods for lexical disambiguation, par-
ticularly [Guthrie et at, 19921&apos;s and [Lesk, 19861&apos;s, as
there is no standard evaluation benchmark; but this
approach seems to work reasonably well for small
and medium scale disambiguation problems with a
broadly similar success rate. We could try produc-
ing a much larger testbed for further comparative
evaluations; but it is not clear how large this would
have to be to become authoritative as an application-
independent metric. Future enhancements to the ap-
proach incorporating the automatic use of the on-line
subject codes and cross reference and subcategorisa-
tion systems of LDOCE can provide better results.
Concerning CHIP, it provides a platform from
which we can build in order to deal with large scale
disambiguation; this could be used as an alternative
to numerical optimisation techniques. The approach
will involve the modelling of the problem in a com-
binatorial form so that constraint satisfaction logic
programming [Van Hentenryck, 1989] can apply. For
each sense of a word we can specify a set of con-
straints such as its subject code(s), or part-of-speech
information or both. Forward checkable (or looka-
head) rules can be introduced to decrease the num-
ber of possible senses of other words in advance (say,
for example, that the &apos;economic&apos; sense for the word
&apos;bank&apos; has been chosen, then only the &apos;economic&apos; or
&apos;neutral&apos; senses of the &apos;arrange&apos;, &apos;overdraft&apos; and &apos;ac-
count&apos; will be taken into account). This suggests a
dramatic reduction on the search space; CHIP offers
all the necessary arithmetic and symbolic facilities
for the implementation.
Our experiments will be based on the use the ma-
chine version of LDOCE to verify the utility of this
dictionary for the specific kind of applications we
have in mind: the development methods and tech-
niques that can assist large scale speech and hand-
writing recognition systems using semantic knowl-
edge from already available resources (MRDs and
corpora) [Atwell et al., 1992]. But the problem here
is somewhat different: semantic constraints must be
used for the correct choice between different candi-
date Ascii interpretations of a spoken or handwritten
word.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999718">
This paper summarizes research reported in
[Demetriou, 1992].
I am grateful to Mr Eric Atwell, director of
CCALAS and my supervisor, for the motivating in-
fluence and background material he provided me for
this project.
I would also like to express my appreciation to
Dr Gyuri Lajos for the organisational support and
advice on CHIP programming and Mr Clive Souter
for his useful recommendations.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999505615384615">
[Atwell, 19831 Eric Atwell. Constituent-Likelihood
Grammar. In ICAME Journal of the International
Computer Archive of Modern English no.7, pages
34-67, 1983.
[Atwell et al., 1992] Eric Atwell, David Hogg and
Robert Pocock. Speech-Oriented Probabilistic
Parser Project: Progress Reports 1&amp;2. Techni-
cal Reports, School of Computer Studies, Leeds
University, 1992.
[Demetriou, 1992] George C. Demetriou. Lexical
Disambiguation Using Constraint Handling In
Prolog (CHIP). MSc Dissertation, School of Com-
puter Studies, University or Leeds, 1992.
</reference>
<page confidence="0.98959">
435
</page>
<reference confidence="0.999154866666667">
[Guthrie et at., 1991] Joe Guthrie, Louise Guthrie,
Yorick Wilks and H. Aidinejad. Subject-
dependent Co-occurence and Word Sense Disam-
biguation. In Proceedings of the 29th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 146-152, 1991.
[Guthrie et al., 1992] Joe Guthrie, Louise Guthrie
and Jim Cowie. Lexical Disambiguation Us-
ing Simulated Annealing. In Proceedings of the
14th Conference on Computational Linguistics,
COLING-92, pages 359-364, 1992.
[Guthrie, 1993] Louise Guthrie. A Note on Lexical
Disambiguation. In C. Souter and E.Atwell (eds),
Corpus-based Computational Linguistics, Rodopi
Press, Amsterdam, 1993.
[Harris, 1985] Mary D. Harris. An Introduction to
Natural Language Processing. Reston Publishing
Company, 1985.
[Hearst, 19911 Marti Hearst. Toward Noun Homo-
graph Disambiguation Using Local Context in
Large Text Corpora. In Proceedings of the 7th
Annual Conference of the UW Centre for the New
OED and TEXT Research, Using Corpora, pages
1-22, 1991.
[Leech, 19831 Geoffrey Leech, Roger Garside and
Eric Atwell. The Automatic Grammatical Tagging
of the LOB Corpus. In ICAME Journal of the In-
ternational Computer Archive of Modern English
no.7, pages 13-33, 1983.
[Lesk, 1986] Michael Lesk. Automatic Sense Disam-
biguation Using Machine Readable Dictionaries:
how to tell a pine cone from an ice cream cone.
In Proceedings of the ACM SIG-DOC Conference,
Ontario, Canada, 1986.
[McDonald et at., 19901 James E. McDonald, Tony
Plate and R. Schvaneveldt. Using Pathfinder
to Extract Semantic Information from Text. In
Pathfinder associative networks: studies in knowl-
edge organisation, R. Schvaneveldt (ed), Norwood,
NJ:Ablex, 1990.
[Procter et al., 19781 Paul Procter et al. The Long-
man Dictionary of Contemporary English. Long-
man, 1978.
[Van Hentenryck, 1989]
Pascal Van Hentenryck. Constraint Satisfaction
in Logic Programming. MIT Press, Cambridge,
Massachusetts, 1989.
[Veronis and Ide, 1990] Jean Veronis and Nancy M.
Ide. Word Sense Disambiguation with Very Large
Neural Networks Extracted from Machine Read-
able Dictionaries. In Proceedings of the 13th Con-
ference on Computational Linguistics, COLING-
90, Helsinki, Finland, 2, pages 389-394, 1990.
[Wilks et al., 19891 Yorick Wilks, Dan Fass, Cheng-
Ming Guo, James McDonald, Tony Plate and
Brian Slator. A Tractable Machine Dictionary as
a Resource for Computational Semantics. In Com-
putational Lexicography for Natural Language Pro-
cessing, B. Boguraev and T. Briscoe (eds), Long-
man, 1989.
</reference>
<page confidence="0.999221">
436
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.569043">
<title confidence="0.7852965">Lexical Disambiguation Using Constraint Handling In Prolog (CHIP) *</title>
<author confidence="0.999176">George C Demetriou</author>
<affiliation confidence="0.9999045">Centre for Computer Analysis of Language And Speech (CCALAS) Artificial Intelligence Division, School of Computer Studies, University of Leeds</affiliation>
<address confidence="0.995827">Leeds, LS2 9JT, United Kingdom</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Eric Atwell. Constituent-Likelihood Grammar. In</title>
<date>1983</date>
<journal>ICAME Journal of the International Computer Archive of Modern English</journal>
<volume>7</volume>
<pages>34--67</pages>
<marker>1983</marker>
<rawString>[Atwell, 19831 Eric Atwell. Constituent-Likelihood Grammar. In ICAME Journal of the International Computer Archive of Modern English no.7, pages 34-67, 1983.</rawString>
</citation>
<citation valid="true">
<title>Eric Atwell, David Hogg and Robert Pocock. Speech-Oriented Probabilistic Parser Project: Progress Reports 1&amp;2. Technical Reports,</title>
<date>1992</date>
<institution>School of Computer Studies, Leeds University,</institution>
<marker>1992</marker>
<rawString>[Atwell et al., 1992] Eric Atwell, David Hogg and Robert Pocock. Speech-Oriented Probabilistic Parser Project: Progress Reports 1&amp;2. Technical Reports, School of Computer Studies, Leeds University, 1992.</rawString>
</citation>
<citation valid="true">
<title>Lexical Disambiguation Using Constraint Handling</title>
<date>1992</date>
<booktitle>In Prolog (CHIP). MSc Dissertation,</booktitle>
<institution>School of Computer Studies, University or Leeds,</institution>
<marker>1992</marker>
<rawString>[Demetriou, 1992] George C. Demetriou. Lexical Disambiguation Using Constraint Handling In Prolog (CHIP). MSc Dissertation, School of Computer Studies, University or Leeds, 1992.</rawString>
</citation>
<citation valid="true">
<title>Subjectdependent Co-occurence and Word Sense Disambiguation.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>146--152</pages>
<marker>1991</marker>
<rawString>[Guthrie et at., 1991] Joe Guthrie, Louise Guthrie, Yorick Wilks and H. Aidinejad. Subjectdependent Co-occurence and Word Sense Disambiguation. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 146-152, 1991.</rawString>
</citation>
<citation valid="true">
<title>Lexical Disambiguation Using Simulated Annealing.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th Conference on Computational Linguistics, COLING-92,</booktitle>
<pages>359--364</pages>
<marker>1992</marker>
<rawString>[Guthrie et al., 1992] Joe Guthrie, Louise Guthrie and Jim Cowie. Lexical Disambiguation Using Simulated Annealing. In Proceedings of the 14th Conference on Computational Linguistics, COLING-92, pages 359-364, 1992.</rawString>
</citation>
<citation valid="true">
<title>Louise Guthrie. A Note on Lexical Disambiguation.</title>
<date>1993</date>
<booktitle>In C. Souter and E.Atwell (eds), Corpus-based Computational Linguistics, Rodopi</booktitle>
<publisher>Press,</publisher>
<location>Amsterdam,</location>
<marker>1993</marker>
<rawString>[Guthrie, 1993] Louise Guthrie. A Note on Lexical Disambiguation. In C. Souter and E.Atwell (eds), Corpus-based Computational Linguistics, Rodopi Press, Amsterdam, 1993.</rawString>
</citation>
<citation valid="true">
<title>An Introduction to Natural Language Processing.</title>
<date>1985</date>
<publisher>Reston Publishing Company,</publisher>
<marker>1985</marker>
<rawString>[Harris, 1985] Mary D. Harris. An Introduction to Natural Language Processing. Reston Publishing Company, 1985.</rawString>
</citation>
<citation valid="true">
<title>Toward Noun Homograph Disambiguation Using Local Context in Large Text Corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 7th Annual Conference of the UW Centre for the New OED and TEXT Research, Using Corpora,</booktitle>
<pages>1--22</pages>
<marker>1991</marker>
<rawString>[Hearst, 19911 Marti Hearst. Toward Noun Homograph Disambiguation Using Local Context in Large Text Corpora. In Proceedings of the 7th Annual Conference of the UW Centre for the New OED and TEXT Research, Using Corpora, pages 1-22, 1991.</rawString>
</citation>
<citation valid="true">
<title>Roger Garside and Eric Atwell. The Automatic Grammatical Tagging of the LOB Corpus.</title>
<date>1983</date>
<booktitle>In ICAME Journal of the International Computer Archive of Modern English no.7,</booktitle>
<pages>13--33</pages>
<marker>1983</marker>
<rawString>[Leech, 19831 Geoffrey Leech, Roger Garside and Eric Atwell. The Automatic Grammatical Tagging of the LOB Corpus. In ICAME Journal of the International Computer Archive of Modern English no.7, pages 13-33, 1983.</rawString>
</citation>
<citation valid="true">
<title>Michael Lesk. Automatic Sense Disambiguation Using Machine Readable Dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the ACM SIG-DOC Conference,</booktitle>
<location>Ontario, Canada,</location>
<marker>1986</marker>
<rawString>[Lesk, 1986] Michael Lesk. Automatic Sense Disambiguation Using Machine Readable Dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the ACM SIG-DOC Conference, Ontario, Canada, 1986.</rawString>
</citation>
<citation valid="true">
<title>Using Pathfinder to Extract Semantic Information from Text.</title>
<date>1990</date>
<booktitle>In Pathfinder associative networks: studies in knowledge organisation, R. Schvaneveldt (ed),</booktitle>
<location>Norwood, NJ:Ablex,</location>
<marker>1990</marker>
<rawString>[McDonald et at., 19901 James E. McDonald, Tony Plate and R. Schvaneveldt. Using Pathfinder to Extract Semantic Information from Text. In Pathfinder associative networks: studies in knowledge organisation, R. Schvaneveldt (ed), Norwood, NJ:Ablex, 1990.</rawString>
</citation>
<citation valid="true">
<title>19781 Paul Procter et al. The Longman Dictionary of Contemporary English.</title>
<date>1978</date>
<publisher>Longman,</publisher>
<marker>1978</marker>
<rawString>[Procter et al., 19781 Paul Procter et al. The Longman Dictionary of Contemporary English. Longman, 1978.</rawString>
</citation>
<citation valid="true">
<title>Pascal Van Hentenryck. Constraint Satisfaction in Logic Programming.</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<marker>1989</marker>
<rawString>[Van Hentenryck, 1989] Pascal Van Hentenryck. Constraint Satisfaction in Logic Programming. MIT Press, Cambridge, Massachusetts, 1989.</rawString>
</citation>
<citation valid="true">
<title>Word Sense Disambiguation with Very Large Neural Networks Extracted from Machine Readable Dictionaries.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th Conference on Computational Linguistics, COLING90,</booktitle>
<volume>2</volume>
<pages>389--394</pages>
<location>Helsinki,</location>
<marker>1990</marker>
<rawString>[Veronis and Ide, 1990] Jean Veronis and Nancy M. Ide. Word Sense Disambiguation with Very Large Neural Networks Extracted from Machine Readable Dictionaries. In Proceedings of the 13th Conference on Computational Linguistics, COLING90, Helsinki, Finland, 2, pages 389-394, 1990.</rawString>
</citation>
<citation valid="true">
<title>A Tractable Machine Dictionary as a Resource for Computational Semantics.</title>
<date>1989</date>
<booktitle>In Computational Lexicography for Natural Language Processing, B. Boguraev</booktitle>
<marker>1989</marker>
<rawString>[Wilks et al., 19891 Yorick Wilks, Dan Fass, ChengMing Guo, James McDonald, Tony Plate and Brian Slator. A Tractable Machine Dictionary as a Resource for Computational Semantics. In Computational Lexicography for Natural Language Processing, B. Boguraev and T. Briscoe (eds), Longman, 1989.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>