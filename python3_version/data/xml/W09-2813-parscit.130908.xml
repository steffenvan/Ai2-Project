<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.088634">
<title confidence="0.998355">
Non-textual Event Summarization by Applying Machine Learning to
Template-based Language Generation
</title>
<author confidence="0.975439">
Mohit Kumar and Dipanjan Das and Sachin Agarwal and Alexander I. Rudnicky
</author>
<affiliation confidence="0.984703">
Language Technologies Institute
Carnegie Mellon University, Pittsburgh, USA
</affiliation>
<email confidence="0.9987">
mohitkum,dipanjan,sachina,air@cs.cmu.edu
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998724">
We describe a learning-based system that
creates draft reports based on observation
of people preparing such reports in a tar-
get domain (conference replanning). The
reports (or briefings) are based on a mix
of text and event data. The latter consist
of task creation and completion actions,
collected from a wide variety of sources
within the target environment. The report
drafting system is part of a larger learning-
based cognitive assistant system that im-
proves the quality of its assistance based
on an opportunity to learn from observa-
tion. The system can learn to accurately
predict the briefing assembly behavior and
shows significant performance improve-
ments relative to a non-learning system,
demonstrating that it’s possible to create
meaningful verbal descriptions of activity
from event streams.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989372093023">
We describe a system for recommending items for
a briefing created after a session with a crisis man-
agement system in a conference replanning do-
main. The briefing system is learning-based, in
that it initially observes how one set of users cre-
ates such briefings then generates draft reports for
another set of users. This system, the Briefing
Assistant(BA), is part of a set of learning-based
cognitive assistants each of which observes users
and learns to assist users in performing their tasks
faster and more accurately.
The difference between this work from
most previous efforts, primarily based on text-
extraction approaches is the emphasis on learning
to summarize event patterns. This work also
differs in its emphasis on learning from user
behavior in the context of a task.
Report generation from non-textual sources has
been previously explored in the Natural Language
Generation (NLG) community in a variety of do-
mains, based on, for example, a database of events.
However, a purely generative approach is not suit-
able in our circumstances, as we want to summa-
rize a variety of tasks that the user is performing
and present a summary tailored to a target audi-
ence, a desirable characteristic of good briefings
(Radev and McKeown, 1998). Thus we approach
the problem by applying learning techniques com-
bined with a template-based generation system to
instantiate the briefing-worthy report items. The
task of instantiating the briefing-worthy items is
similar to the task of Content Selection (Duboue,
2004) in the Generation pipeline however our ap-
proach minimizes linguistic involvement. Our
choice of a template-based generative system was
motivated by recent discussions in the NLG com-
munity (van Deemter et al., 2005) about the prac-
ticality and effectiveness of this approach.
The plan of the paper is as follows. We describe
relevant work from existing literature in the next
section. Then, we provide brief system description
followed by experiments and results. We conclude
with a summary of the work.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999721538461538">
Event based summarization has been studied in the
summarization community. (Daniel et al., 2003)
described identification of sub-events in multiple
documents. (Filatova and Hatzivassiloglou, 2004)
mentioned the use of event-based features in ex-
tractive summarization and (Wu, 2006; Li et al.,
2006) describe similar work based on events oc-
curring in text. However, unlike the case at hand,
all the work on event-based summarization used
text as source material.
Non-textual summarization has also been ex-
plored in the Natural Language Generation (NLG)
community within the broad task of generating
</bodyText>
<page confidence="0.998782">
67
</page>
<note confidence="0.985256">
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999958705882353">
reports based on database of events in specific
domains such as medical (Portet et al., 2009),
weather (Belz, 2007), sports (Oh and Shrobe,
2008) etc. However, in our case we want to sum-
marize a variety of tasks that the user is perform-
ing and present a summary to an intended audi-
ence (as defined by a report request).
Recent advances in NLG research use statis-
tical approaches at various stages of processing
in the generation pipeline like content selection
(Duboue and McKeown, 2003; Barzilay and Lee,
2004), probabilistic generation rules (Belz, 2007).
Our proposed approach differs from these in that
we apply machine learning after generation of all
the templates, as a post-processing step, to rank
them for inclusion in the final briefing. We could
have used a general purpose template-based gen-
eration framework like TG/2 (Busemann, 2005),
but since the number of templates and their corre-
sponding aggregators is limited, we chose an ap-
proach based on string manipulation.
We found in our work that an approach based
on modeling individual users and then combining
the outputs of such models using a voting scheme
gives the best results, although our approach is
distinguishable from collaborative filtering tech-
niques used for driving recommendation systems
(Hofmann, 2004). We believe this is due to the
fact that the individual sessions from which rank-
ing models are learned, although they range over
the same collection of component tasks, can lead
to very different (human-generated) reports. That
is, the particular history of a session will affect
what is considered to be briefing-worthy.
</bodyText>
<sectionHeader confidence="0.948192" genericHeader="method">
3 System Overview
</sectionHeader>
<figureCaption confidence="0.997958">
Figure 1: Briefing Assistant Data Flow.
</figureCaption>
<bodyText confidence="0.998966666666667">
The Briefing Assistant Model: We treat the
task of briefing generation in the current domain1
as non-textual event-based summarization. The
</bodyText>
<footnote confidence="0.867555333333333">
1More details about the domain and the interaction of BA
with the larger system are mentioned in a longer version of
the paper (Kumar et al., 2009)
</footnote>
<figureCaption confidence="0.9010915">
Figure 2: The category tree showing the informa-
tion types that we expect in a briefing.
</figureCaption>
<bodyText confidence="0.999982875">
events are the task creation and task completion
actions logged by various cognitive assistants in
the system (so-called specialists). As part of the
design phase for the template-based generation
component, we identified a set of templates, based
on the actual briefings written by users in a sepa-
rate experiment. Ideally, we would like to adopt
a corpus-based approach to automatically extract
the templates in the domain, like (Kumar et al.,
2008), but since the sample briefings available to
us were very few, the application of such corpus-
based techniques was not necessary. Based on
this set of templates we identified the patterns that
needed to be extracted from the event logs in order
to populate the templates. A ranking model was
also designed for ordering instantiations of this set
of templates and to recommend the top 4 most rel-
evant ones for a given session.
The overall data flow for BA during a session
(runtime) is shown in Figure 1. The various spe-
cialist modules generate task related events that
are logged in a database. The aggregators operate
over this database and emails to extract relevant
patterns. These patterns in turn are used to popu-
late templates which constitute candidate briefing
items. The candidate briefing items are then or-
dered by the ranking module and presented to the
user.
Template Design and Aggregators: The set
of templates used in the current instantiation of
the BA was derived from a corpus of human-
generated briefings collected in a previous exper-
iment using the same crisis management system.
The set of templates was designed to cover the
range of items that users in that experiment chose
to include in their reports corresponding to nine
categories shown in Figure 2. We found that in-
formation can be conveyed at different levels of
granularity (for example, qualitatively or quantita-
tively). The appropriate choice of granularity for
</bodyText>
<page confidence="0.996917">
68
</page>
<bodyText confidence="0.987395566666667">
a particular session is a factor that the system can
learn2.
Ranking Model, Classifiers and Features: The
ranking module orders candidate templates so
that the four most relevant ones appear in the
briefing draft. The ranking system consists of
a consensus-based classifier, based on individual
classifier models for each user in the training set.
The prediction from each classifier are combined
(averaged) to produce a final rank of each tem-
plate.
We used the Minorthird package (Cohen, 2004)
for modeling. Specifically we allowed the sys-
tem to experiment with eleven different learning
schemes and select the best one based on cross-
validation within the training corpus. The schemes
were Naive Bayes, Voted Perceptron, Support
Vector Machines, Ranking Perceptron, K Nearest
Neighbor, Decision Tree, AdaBoost, Passive Ag-
gressive learner, Maximum Entropy learner, Bal-
anced Winnow and Boosted Ranking learner.
The features3 used in the system are static or
dynamic. Static features reflect the properties of
the templates irrespective of the user’s activity
whereas the dynamic features are based on the
actual events that took place. We used the In-
formation Gain (IG) metric for feature selection,
experimenting with seven different cut-off values
All, 20,15,10, 7, 5, 4 for the total number of se-
lected features.
</bodyText>
<sectionHeader confidence="0.998258" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999579307692308">
Experimental Setup: Two experimental condi-
tions were used to differentiate performance based
on knowledge engineering, designated MinusL
and performance based on learning, designated
PlusL.4
Email Trigger: In the simulated conference
replanning crisis, the briefing was triggered by
an email containing explicit information requests,
not known beforehand. To customize the brief-
ing according to the request, a natural language
processing module identified the categories of in-
formation requested. The details of the module
are beyond the scope of the current paper as it
</bodyText>
<footnote confidence="0.982981625">
2The details of template design process including sample
templates, categories of templates and details of aggregators
are presented in (Kumar et al., 2009)
3Detailed description of the features are mentioned in
(Kumar et al., 2009)
4The details of the experimental setup as part of the larger
cognitive assistant system are presented in (Kumar et al.,
2009).
</footnote>
<bodyText confidence="0.999891918367347">
is external to our system; it took into account
the template categories we earlier identified. Fig-
ure 4 shows a sample briefing email stimulus.
The mapping from the sample email in the figure
to the categories is as follows: “expected atten-
dance” - Property-Session; “how many sessions
have been rescheduled”, “how many still need to
be rescheduled”, “any problems you see as you
try to reschedule” - Session-Reschedule; “status
of food service (I am worried about the keynote
lunch)” - Catering Vendors.
Training: Eleven expert users5 were asked to
provide training by using the system then generat-
ing the end of session briefing using the BA GUI.
For this training phase, no item ranking was per-
formed by the system, i.e. all the templates were
populated by the aggregators and recommenda-
tions were random. The expert user was asked
to select the best possible four items and was fur-
ther asked to judge the usefulness of the remaining
items. The resulting training data consists of the
activity log, extracted features and the user-labeled
items. The trigger message for the training users
did not contain any specific information request.
Test: Subjects were recruited to use the crisis
management system in MinusL and PlusL condi-
tion, although they were not aware of the condition
of the system and they were not involved with the
project. There were 54 test runs in the MinusL
condition and 47 in the PlusL condition. Out of
these runs, 29 subjects in MinusL and 43 subjects
in PlusL wrote a briefing using the BA. We report
the evaluation scores for this latter set.
Evaluation: The base performance metric is
Recall, defined in terms of the briefing templates
recommended by the system compared to the tem-
plates ultimately selected by the user. We justify
this by noting that Recall can be directly linked to
the expected time savings for the users. We cal-
culate two variants of Recall: Category-based—
calculated by matching the categories of the BA
recommended templates and user selected ones
ignoring the granularity and Template-based—
calculated by matching the exact templates. The
first metric indicates whether the right category of
information was selected and the latter indicates
whether the information was presented at the ap-
propriate level of detail.
We also performed subjective human evaluation
</bodyText>
<footnote confidence="0.859753333333333">
5Members of the project from other groups who were
aware of the scenario and various system functionalities but
not the ML methods
</footnote>
<page confidence="0.999335">
69
</page>
<bodyText confidence="0.975598379310345">
using a panel of three judges. The judges assigned
scores (0-4) to each of the bullets based on the
coverage of the crisis, clarity and conciseness, ac-
curacy and the correct level of granularity. They
were advised about certain briefing-specific char-
acteristics (e.g. negative bullet items are useful
and hence should be rated favorably). They were
also asked to provide a global assessment of report
quality, and evaluate the coverage of the requests
in the briefing stimulus email message. This pro-
cedure was very similar to the one used as the basis
for template selection.
Experiment: The automatic evaluation met-
ric used for the trained system configuration is
the Template-based recall measure. To obtain
the final system configuration, we automatically
evaluate the system under the various combina-
tions of parameter settings with eleven different
learning schemes and seven different feature se-
lection threshold (as mentioned in previous sec-
tions). Thus a total of 77 different configurations
are tested. For each configuration, we do a eleven-
fold cross-validation between the 11 training users
i.e. we leave one user as the test user and consider
the remaining ten users as training users. We av-
erage the performance across the 11 test cases and
obtain the final score for the configuration. We
choose the configuration with the highest score as
the final trained system configuration. The learned
system configuration in the current test includes
Balanced Winnow (Littlestone, 1988) and top 7
features.
Results: We noticed that four users in PlusL
condition took more than 8 minutes to complete
the briefing when the median time taken by the
users in PlusL condition was 55 seconds, so we
did not include these users in our analysis in order
to maintain the homogeneity of the dataset. These
four data points were identified as extreme outliers
using a procedure suggested by (NIST, 2008)6.
There were no extreme outliers in MinusL condi-
tion.
Figure 3a shows the Recall values for the Mi-
nusL and PlusL conditions. The learning delta
i.e. the difference between the recall values of
PlusL and MinusL is 33% for Template-based re-
call and 21% for Category-based recall. These
differences are significant at the p &lt; 0.001 level.
6Extreme outliers are defined as data points that are out-
side the range [Q1−3*IQ, Q3+3*IQ] in a box plot. Q1 is
lower quartile, Q3 is upper quartile and IQ is the difference
(Q3 − Q1) is the interquartile range.
The statistical significance for the Template-based
metric, which was the metric used for select-
ing system parameters during the training phase,
shows that learning is effective in this case. Since
the email stimulus processing module extracts the
briefing categories from the email the Category-
based and Template-based recall is expected to be
high for the baseline MinusL case. In our test, the
email stimuli had 3 category requests and so the
Category-based recall of 0.77 and Template-based
recall of 0.67 in MinusL is not unexpected.
Figure 3b shows the Judges’ panel scores for
the briefings in MinusL and PlusL condition. The
learning delta in this case is 3.6% which is also
statistically significant, at p &lt; 0.05. The statistical
significance of the learning delta validates that the
briefings generated during PlusL conditions are
better than MinusL condition. The absolute differ-
ence in the qualitative briefing scores between the
two conditions is small because MinusL users can
select from all candidates, while the recommenda-
tions they receive are random. Consequently they
need to spend more time in finding the right items.
The average time taken for a briefing in MinusL
condition is about 83 seconds and 62 seconds in
PlusL (see Figure 3c). While the time difference
is high (34%) it is not statistically significant due
to high variance.
Four of the top 10 most frequently selected fea-
tures across users for this system are dynamic fea-
tures. This indicates that the learning model is
capturing the user’s world state and the recom-
mendations are related to the underlying events.
We believe this validates the process we used to
generate briefing reports from non-textual events.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999715076923077">
The Briefing Assistant is not designed to learn
the generic attributes of good reports; rather it’s
meant to rapidly learn the attributes of good re-
ports within a particular domain and to accom-
modate specific information needs on a report-by-
report basis. We found that learned customiza-
tion produces reports that are judged to be of bet-
ter quality. We also found that a consensus-based
modeling approach, which incorporates informa-
tion from multiple users, yields the best perfor-
mance. We believe that our approach can be used
to create flexible summarization systems for a va-
riety of applications.
</bodyText>
<page confidence="0.972039">
70
</page>
<figure confidence="0.998247">
(a) (b) (c)
</figure>
<figureCaption confidence="0.9955295">
Figure 3: (a) Recall values for MinusL and PlusL conditions (b) Briefing scores from the judges’ panel
for MinusL and PlusL conditions (c) Briefing time taken for MinusL and PlusL conditions.
Figure 4: Template categories corresponding to
the Briefing request email.
</figureCaption>
<sectionHeader confidence="0.981661" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.953663727272728">
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings ofHLT-NAACL.
Stephan Busemann. 2005. Ten years after: An update
on TG/2 (and friends). In Proceedings of European
Natural Language Generation Workshop.
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net, 10th Jun 2009.
Naomi Daniel, Dragomir Radev, and Timothy Allison.
2003. Sub-event based multi-document summariza-
tion. In Proceedings ofHLT-NAACL.
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of
EMNLP.
Pablo A. Duboue. 2004. Indirect supervised learning
of content selection logic. In Proceedings ofINLG.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In Text Sum-
marization Branches Out: Proceedings of the ACL-
04 Workshop.
Thomas Hofmann. 2004. Latent semantic models for
collaborative filtering. ACM Transactions on Infor-
mation Systems, 22(1):89–115.
Mohit Kumar, Dipanjan Das, and Alexander I. Rud-
nicky. 2008. Automatic extraction of briefing tem-
plates. In Proceedings ofIJCNLP.
Mohit Kumar, Dipanjan Das, Sachin Agarwal, and
Alexander I. Rudnicky. 2009. Non-textual event
summarization by applying machine learning to
template-based language generation. Technical Re-
port CMU-LTI-09-012, Language Technologies In-
stitute, Carnegie Mellon University.
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu, and Chunfa
Yuan. 2006. Extractive summarization using inter-
and intra- event relevance. In Proceedings ofACL.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. Machine Learning, 2(4):285–318.
NIST. 2008. NIST/SEMATECH e-
handbook of statistical methods.
http://www.itl.nist.gov/div898/handbook/, 10th
Jun 2009.
Alice Oh and Howard Shrobe. 2008. Generating base-
ball summaries from multiple perspectives by re-
ordering content. In Proceedings ofINLG.
Franc¸ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy
Sykes. 2009. Automatic generation of textual sum-
maries from neonatal intensive care data. Artificial
Intelligence, 173(7-8):789–816.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470–500.
Kees van Deemter, Emiel Krahmer, and Mariet The-
une. 2005. Real versus template-based natural lan-
guage generation: A false opposition? Computa-
tional Linguistics, 31(1):15–24.
Mingli Wu. 2006. Investigations on event-based sum-
marization. In Proceedings ofACL.
</reference>
<page confidence="0.999145">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.492336">
<title confidence="0.76981525">Non-textual Event Summarization by Applying Machine Learning Template-based Language Generation Kumar Das Agarwal I. Language Technologies</title>
<affiliation confidence="0.991543">Carnegie Mellon University, Pittsburgh,</affiliation>
<email confidence="0.999922">mohitkum,dipanjan,sachina,air@cs.cmu.edu</email>
<abstract confidence="0.999751">We describe a learning-based system that creates draft reports based on observation of people preparing such reports in a target domain (conference replanning). The reports (or briefings) are based on a mix of text and event data. The latter consist of task creation and completion actions, collected from a wide variety of sources within the target environment. The report drafting system is part of a larger learningbased cognitive assistant system that improves the quality of its assistance based on an opportunity to learn from observation. The system can learn to accurately predict the briefing assembly behavior and shows significant performance improvements relative to a non-learning system, demonstrating that it’s possible to create meaningful verbal descriptions of activity from event streams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="4430" citStr="Barzilay and Lee, 2004" startWordPosition="684" endWordPosition="687">eration and Summarisation, ACL-IJCNLP 2009, pages 67–71, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs of such models using a v</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: probabilistic content models, with applications to generation and summarization. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Probabilistic generation of weather forecast texts.</title>
<date>2007</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="4034" citStr="Belz, 2007" startWordPosition="617" endWordPosition="618">n and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for</context>
</contexts>
<marker>Belz, 2007</marker>
<rawString>Anja Belz. 2007. Probabilistic generation of weather forecast texts. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Busemann</author>
</authors>
<title>Ten years after: An update on TG/2 (and friends).</title>
<date>2005</date>
<booktitle>In Proceedings of European Natural Language Generation Workshop.</booktitle>
<contexts>
<context position="4767" citStr="Busemann, 2005" startWordPosition="737" endWordPosition="738">orming and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs of such models using a voting scheme gives the best results, although our approach is distinguishable from collaborative filtering techniques used for driving recommendation systems (Hofmann, 2004). We believe this is due to the fact that the individual sessions from which ranking models are learned, although they range over the same collection of component t</context>
</contexts>
<marker>Busemann, 2005</marker>
<rawString>Stephan Busemann. 2005. Ten years after: An update on TG/2 (and friends). In Proceedings of European Natural Language Generation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Minorthird: Methods for identifying names and ontological relations in text using heuristics for inducing regularities from data. http://minorthird.sourceforge.net,</title>
<date>2004</date>
<pages>10</pages>
<contexts>
<context position="8351" citStr="Cohen, 2004" startWordPosition="1320" endWordPosition="1321">nt levels of granularity (for example, qualitatively or quantitatively). The appropriate choice of granularity for 68 a particular session is a factor that the system can learn2. Ranking Model, Classifiers and Features: The ranking module orders candidate templates so that the four most relevant ones appear in the briefing draft. The ranking system consists of a consensus-based classifier, based on individual classifier models for each user in the training set. The prediction from each classifier are combined (averaged) to produce a final rank of each template. We used the Minorthird package (Cohen, 2004) for modeling. Specifically we allowed the system to experiment with eleven different learning schemes and select the best one based on crossvalidation within the training corpus. The schemes were Naive Bayes, Voted Perceptron, Support Vector Machines, Ranking Perceptron, K Nearest Neighbor, Decision Tree, AdaBoost, Passive Aggressive learner, Maximum Entropy learner, Balanced Winnow and Boosted Ranking learner. The features3 used in the system are static or dynamic. Static features reflect the properties of the templates irrespective of the user’s activity whereas the dynamic features are bas</context>
</contexts>
<marker>Cohen, 2004</marker>
<rawString>William W. Cohen. 2004. Minorthird: Methods for identifying names and ontological relations in text using heuristics for inducing regularities from data. http://minorthird.sourceforge.net, 10th Jun 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Daniel</author>
<author>Dragomir Radev</author>
<author>Timothy Allison</author>
</authors>
<title>Sub-event based multi-document summarization.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="3254" citStr="Daniel et al., 2003" startWordPosition="497" endWordPosition="500"> 2004) in the Generation pipeline however our approach minimizes linguistic involvement. Our choice of a template-based generative system was motivated by recent discussions in the NLG community (van Deemter et al., 2005) about the practicality and effectiveness of this approach. The plan of the paper is as follows. We describe relevant work from existing literature in the next section. Then, we provide brief system description followed by experiments and results. We conclude with a summary of the work. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pag</context>
</contexts>
<marker>Daniel, Radev, Allison, 2003</marker>
<rawString>Naomi Daniel, Dragomir Radev, and Timothy Allison. 2003. Sub-event based multi-document summarization. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Statistical acquisition of content selection rules for natural language generation.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4405" citStr="Duboue and McKeown, 2003" startWordPosition="680" endWordPosition="683">9 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs</context>
</contexts>
<marker>Duboue, McKeown, 2003</marker>
<rawString>Pablo A. Duboue and Kathleen R. McKeown. 2003. Statistical acquisition of content selection rules for natural language generation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
</authors>
<title>Indirect supervised learning of content selection logic.</title>
<date>2004</date>
<booktitle>In Proceedings ofINLG.</booktitle>
<contexts>
<context position="2640" citStr="Duboue, 2004" startWordPosition="402" endWordPosition="403">y in a variety of domains, based on, for example, a database of events. However, a purely generative approach is not suitable in our circumstances, as we want to summarize a variety of tasks that the user is performing and present a summary tailored to a target audience, a desirable characteristic of good briefings (Radev and McKeown, 1998). Thus we approach the problem by applying learning techniques combined with a template-based generation system to instantiate the briefing-worthy report items. The task of instantiating the briefing-worthy items is similar to the task of Content Selection (Duboue, 2004) in the Generation pipeline however our approach minimizes linguistic involvement. Our choice of a template-based generative system was motivated by recent discussions in the NLG community (van Deemter et al., 2005) about the practicality and effectiveness of this approach. The plan of the paper is as follows. We describe relevant work from existing literature in the next section. Then, we provide brief system description followed by experiments and results. We conclude with a summary of the work. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel</context>
</contexts>
<marker>Duboue, 2004</marker>
<rawString>Pablo A. Duboue. 2004. Indirect supervised learning of content selection logic. In Proceedings ofINLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL04 Workshop.</booktitle>
<contexts>
<context position="3354" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="508" endWordPosition="511">ent. Our choice of a template-based generative system was motivated by recent discussions in the NLG community (van Deemter et al., 2005) about the practicality and effectiveness of this approach. The plan of the paper is as follows. We describe relevant work from existing literature in the next section. Then, we provide brief system description followed by experiments and results. We conclude with a summary of the work. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP reports based on database of events</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. Event-based extractive summarization. In Text Summarization Branches Out: Proceedings of the ACL04 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Latent semantic models for collaborative filtering.</title>
<date>2004</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="5203" citStr="Hofmann, 2004" startWordPosition="805" endWordPosition="806">, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs of such models using a voting scheme gives the best results, although our approach is distinguishable from collaborative filtering techniques used for driving recommendation systems (Hofmann, 2004). We believe this is due to the fact that the individual sessions from which ranking models are learned, although they range over the same collection of component tasks, can lead to very different (human-generated) reports. That is, the particular history of a session will affect what is considered to be briefing-worthy. 3 System Overview Figure 1: Briefing Assistant Data Flow. The Briefing Assistant Model: We treat the task of briefing generation in the current domain1 as non-textual event-based summarization. The 1More details about the domain and the interaction of BA with the larger system</context>
</contexts>
<marker>Hofmann, 2004</marker>
<rawString>Thomas Hofmann. 2004. Latent semantic models for collaborative filtering. ACM Transactions on Information Systems, 22(1):89–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Kumar</author>
<author>Dipanjan Das</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Automatic extraction of briefing templates.</title>
<date>2008</date>
<booktitle>In Proceedings ofIJCNLP.</booktitle>
<contexts>
<context position="6410" citStr="Kumar et al., 2008" startWordPosition="998" endWordPosition="1001">arger system are mentioned in a longer version of the paper (Kumar et al., 2009) Figure 2: The category tree showing the information types that we expect in a briefing. events are the task creation and task completion actions logged by various cognitive assistants in the system (so-called specialists). As part of the design phase for the template-based generation component, we identified a set of templates, based on the actual briefings written by users in a separate experiment. Ideally, we would like to adopt a corpus-based approach to automatically extract the templates in the domain, like (Kumar et al., 2008), but since the sample briefings available to us were very few, the application of such corpusbased techniques was not necessary. Based on this set of templates we identified the patterns that needed to be extracted from the event logs in order to populate the templates. A ranking model was also designed for ordering instantiations of this set of templates and to recommend the top 4 most relevant ones for a given session. The overall data flow for BA during a session (runtime) is shown in Figure 1. The various specialist modules generate task related events that are logged in a database. The a</context>
</contexts>
<marker>Kumar, Das, Rudnicky, 2008</marker>
<rawString>Mohit Kumar, Dipanjan Das, and Alexander I. Rudnicky. 2008. Automatic extraction of briefing templates. In Proceedings ofIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Kumar</author>
<author>Dipanjan Das</author>
<author>Sachin Agarwal</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Non-textual event summarization by applying machine learning to template-based language generation.</title>
<date>2009</date>
<tech>Technical Report CMU-LTI-09-012,</tech>
<institution>Language Technologies Institute, Carnegie Mellon University.</institution>
<contexts>
<context position="5871" citStr="Kumar et al., 2009" startWordPosition="911" endWordPosition="914">ividual sessions from which ranking models are learned, although they range over the same collection of component tasks, can lead to very different (human-generated) reports. That is, the particular history of a session will affect what is considered to be briefing-worthy. 3 System Overview Figure 1: Briefing Assistant Data Flow. The Briefing Assistant Model: We treat the task of briefing generation in the current domain1 as non-textual event-based summarization. The 1More details about the domain and the interaction of BA with the larger system are mentioned in a longer version of the paper (Kumar et al., 2009) Figure 2: The category tree showing the information types that we expect in a briefing. events are the task creation and task completion actions logged by various cognitive assistants in the system (so-called specialists). As part of the design phase for the template-based generation component, we identified a set of templates, based on the actual briefings written by users in a separate experiment. Ideally, we would like to adopt a corpus-based approach to automatically extract the templates in the domain, like (Kumar et al., 2008), but since the sample briefings available to us were very fe</context>
<context position="9925" citStr="Kumar et al., 2009" startWordPosition="1553" endWordPosition="1556">gineering, designated MinusL and performance based on learning, designated PlusL.4 Email Trigger: In the simulated conference replanning crisis, the briefing was triggered by an email containing explicit information requests, not known beforehand. To customize the briefing according to the request, a natural language processing module identified the categories of information requested. The details of the module are beyond the scope of the current paper as it 2The details of template design process including sample templates, categories of templates and details of aggregators are presented in (Kumar et al., 2009) 3Detailed description of the features are mentioned in (Kumar et al., 2009) 4The details of the experimental setup as part of the larger cognitive assistant system are presented in (Kumar et al., 2009). is external to our system; it took into account the template categories we earlier identified. Figure 4 shows a sample briefing email stimulus. The mapping from the sample email in the figure to the categories is as follows: “expected attendance” - Property-Session; “how many sessions have been rescheduled”, “how many still need to be rescheduled”, “any problems you see as you try to reschedul</context>
</contexts>
<marker>Kumar, Das, Agarwal, Rudnicky, 2009</marker>
<rawString>Mohit Kumar, Dipanjan Das, Sachin Agarwal, and Alexander I. Rudnicky. 2009. Non-textual event summarization by applying machine learning to template-based language generation. Technical Report CMU-LTI-09-012, Language Technologies Institute, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenjie Li</author>
<author>Mingli Wu</author>
<author>Qin Lu</author>
<author>Wei Xu</author>
<author>Chunfa Yuan</author>
</authors>
<title>Extractive summarization using interand intra- event relevance.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="3456" citStr="Li et al., 2006" startWordPosition="525" endWordPosition="528">r et al., 2005) about the practicality and effectiveness of this approach. The plan of the paper is as follows. We describe relevant work from existing literature in the next section. Then, we provide brief system description followed by experiments and results. We conclude with a summary of the work. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrob</context>
</contexts>
<marker>Li, Wu, Lu, Xu, Yuan, 2006</marker>
<rawString>Wenjie Li, Mingli Wu, Qin Lu, Wei Xu, and Chunfa Yuan. 2006. Extractive summarization using interand intra- event relevance. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Littlestone</author>
</authors>
<title>Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="14074" citStr="Littlestone, 1988" startWordPosition="2226" endWordPosition="2227">d seven different feature selection threshold (as mentioned in previous sections). Thus a total of 77 different configurations are tested. For each configuration, we do a elevenfold cross-validation between the 11 training users i.e. we leave one user as the test user and consider the remaining ten users as training users. We average the performance across the 11 test cases and obtain the final score for the configuration. We choose the configuration with the highest score as the final trained system configuration. The learned system configuration in the current test includes Balanced Winnow (Littlestone, 1988) and top 7 features. Results: We noticed that four users in PlusL condition took more than 8 minutes to complete the briefing when the median time taken by the users in PlusL condition was 55 seconds, so we did not include these users in our analysis in order to maintain the homogeneity of the dataset. These four data points were identified as extreme outliers using a procedure suggested by (NIST, 2008)6. There were no extreme outliers in MinusL condition. Figure 3a shows the Recall values for the MinusL and PlusL conditions. The learning delta i.e. the difference between the recall values of </context>
</contexts>
<marker>Littlestone, 1988</marker>
<rawString>Nick Littlestone. 1988. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2(4):285–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>NIST/SEMATECH ehandbook of statistical methods.</title>
<date>2008</date>
<booktitle>http://www.itl.nist.gov/div898/handbook/, 10th</booktitle>
<contexts>
<context position="14480" citStr="NIST, 2008" startWordPosition="2297" endWordPosition="2298">onfiguration. We choose the configuration with the highest score as the final trained system configuration. The learned system configuration in the current test includes Balanced Winnow (Littlestone, 1988) and top 7 features. Results: We noticed that four users in PlusL condition took more than 8 minutes to complete the briefing when the median time taken by the users in PlusL condition was 55 seconds, so we did not include these users in our analysis in order to maintain the homogeneity of the dataset. These four data points were identified as extreme outliers using a procedure suggested by (NIST, 2008)6. There were no extreme outliers in MinusL condition. Figure 3a shows the Recall values for the MinusL and PlusL conditions. The learning delta i.e. the difference between the recall values of PlusL and MinusL is 33% for Template-based recall and 21% for Category-based recall. These differences are significant at the p &lt; 0.001 level. 6Extreme outliers are defined as data points that are outside the range [Q1−3*IQ, Q3+3*IQ] in a box plot. Q1 is lower quartile, Q3 is upper quartile and IQ is the difference (Q3 − Q1) is the interquartile range. The statistical significance for the Template-based</context>
</contexts>
<marker>NIST, 2008</marker>
<rawString>NIST. 2008. NIST/SEMATECH ehandbook of statistical methods. http://www.itl.nist.gov/div898/handbook/, 10th Jun 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Oh</author>
<author>Howard Shrobe</author>
</authors>
<title>Generating baseball summaries from multiple perspectives by reordering content.</title>
<date>2008</date>
<booktitle>In Proceedings ofINLG.</booktitle>
<contexts>
<context position="4064" citStr="Oh and Shrobe, 2008" startWordPosition="620" endWordPosition="623">t al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefi</context>
</contexts>
<marker>Oh, Shrobe, 2008</marker>
<rawString>Alice Oh and Howard Shrobe. 2008. Generating baseball summaries from multiple perspectives by reordering content. In Proceedings ofINLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Portet</author>
<author>Ehud Reiter</author>
<author>Albert Gatt</author>
<author>Jim Hunter</author>
<author>Somayajulu Sripada</author>
<author>Yvonne Freer</author>
<author>Cindy Sykes</author>
</authors>
<title>Automatic generation of textual summaries from neonatal intensive care data.</title>
<date>2009</date>
<journal>Artificial Intelligence,</journal>
<pages>173--7</pages>
<contexts>
<context position="4012" citStr="Portet et al., 2009" startWordPosition="612" endWordPosition="615">ures in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing </context>
</contexts>
<marker>Portet, Reiter, Gatt, Hunter, Sripada, Freer, Sykes, 2009</marker>
<rawString>Franc¸ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter, Somayajulu Sripada, Yvonne Freer, and Cindy Sykes. 2009. Automatic generation of textual summaries from neonatal intensive care data. Artificial Intelligence, 173(7-8):789–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="2369" citStr="Radev and McKeown, 1998" startWordPosition="361" endWordPosition="364">roaches is the emphasis on learning to summarize event patterns. This work also differs in its emphasis on learning from user behavior in the context of a task. Report generation from non-textual sources has been previously explored in the Natural Language Generation (NLG) community in a variety of domains, based on, for example, a database of events. However, a purely generative approach is not suitable in our circumstances, as we want to summarize a variety of tasks that the user is performing and present a summary tailored to a target audience, a desirable characteristic of good briefings (Radev and McKeown, 1998). Thus we approach the problem by applying learning techniques combined with a template-based generation system to instantiate the briefing-worthy report items. The task of instantiating the briefing-worthy items is similar to the task of Content Selection (Duboue, 2004) in the Generation pipeline however our approach minimizes linguistic involvement. Our choice of a template-based generative system was motivated by recent discussions in the NLG community (van Deemter et al., 2005) about the practicality and effectiveness of this approach. The plan of the paper is as follows. We describe relev</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Dragomir R. Radev and Kathleen R. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):470–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Emiel Krahmer</author>
<author>Mariet Theune</author>
</authors>
<title>Real versus template-based natural language generation: A false opposition?</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>van Deemter, Krahmer, Theune, 2005</marker>
<rawString>Kees van Deemter, Emiel Krahmer, and Mariet Theune. 2005. Real versus template-based natural language generation: A false opposition? Computational Linguistics, 31(1):15–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingli Wu</author>
</authors>
<title>Investigations on event-based summarization.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="3438" citStr="Wu, 2006" startWordPosition="523" endWordPosition="524">van Deemter et al., 2005) about the practicality and effectiveness of this approach. The plan of the paper is as follows. We describe relevant work from existing literature in the next section. Then, we provide brief system description followed by experiments and results. We conclude with a summary of the work. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sp</context>
</contexts>
<marker>Wu, 2006</marker>
<rawString>Mingli Wu. 2006. Investigations on event-based summarization. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>