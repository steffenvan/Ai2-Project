<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.079502">
<title confidence="0.984333">
Modeling Local Context for Pitch Accent Prediction
</title>
<author confidence="0.998386">
Shimei Pan
</author>
<affiliation confidence="0.996237">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.986022">
New York, NY, 10027, USA
</address>
<email confidence="0.991178">
pan@cs.columbia.edu
</email>
<author confidence="0.800015">
Julia Hirschberg
</author>
<affiliation confidence="0.68272">
AT&amp;T Labs-Research
</affiliation>
<address confidence="0.803553">
Florham Park, NJ, 07932-0971, USA
</address>
<email confidence="0.996208">
julia@research.att.com
</email>
<sectionHeader confidence="0.995585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996725">
Pitch accent placement is a major
topic in intonational phonology re-
search and its application to speech
synthesis. What factors influence
whether or not a word is made
intonationally prominent or not is
an open question. In this paper,
we investigate how one aspect of a
word&apos;s local context its colloca-
tion with neighboring words influ-
ences whether it is accented or not.
Results of experiments on two tran-
scribed speech corpora in a medical
domain show that such collocation
information is a useful predictor of
pitch accent placement.
</bodyText>
<sectionHeader confidence="0.998524" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998901040322581">
In English, speakers make some words more
intonationally prominent than others. These
words are said to be accented or to bear
pitch accents. Accented words are typically
louder and longer than their unaccented coun-
terparts, and their stressable syllable is usu-
ally aligned with an excursion in the funda-
mental frequency. This excursion will differ
in shape according to the type of pitch ac-
cent. Pitch accent type, in turn, influences
listeners&apos; interpretation of the accented word
or its larger syntactic constituent. Previous
research has associated pitch accent with vari-
ation in various types of information status,
including the given/new distinction, focus, and
contrastiveness, inter alia. Assigning pitch ac-
cent in speech generation systems which em-
ploy speech synthesizers for output is thus crit-
ical to system performance: not only must one
convey meaning naturally, as humans would,
but one must avoid conveying mis-information
which reliance on the synthesizers&apos; defaults
may result in.
The speech generation work discussed here
is part of a larger effort in developing an intel-
ligent multimedia presentation generation sys-
tem called MAGIC (Medical Abstract Gen-
eration for Intensive Care) (Dalal et al.,
1996). In MAGIC, given a patient&apos;s medical
record stored at Columbia Presbyterian Medi-
cal Center (CPMC)&apos;s on-line database system,
the system automatically generates a post-
operative status report for a patient who has
just undergone bypass surgery. There are two
media-specific generators in MAGIC: a graph-
ics generator which automatically produces
graphical presentations from database entities,
and a spoken language generator which auto-
matically produces coherent spoken language
presentations from these entities. The graph-
ical and the speech generators communicate
with each other on the fly to ensure that the
final multimedia output is synchronized.
In order to produce natural and coherent
speech output, MAGIC&apos;s spoken language gen-
erator models a collection of speech features,
such as accenting and intonational phrasing,
which are critical to the naturalness and intel-
ligibility of output speech. In order to assign
these features accurately, the system needs to
identify useful correlates of accent and phrase
boundary location to use as predictors. This
work represents part of our efforts in identi-
fying useful predictors for pitch accent place-
ment.
Pitch accent placement has long been a re-
search focus for scientists working on phonol-
ogy, speech analysis and synthesis (Bolinger,
1989; Ladd, 1996). In general, syntactic fea-
tures are the most widely used features in
pitch accent predication. For example, part-
of-speech is traditionally the most useful sin-
gle pitch accent predictor (Hirschberg, 1993).
Function words, such as prepositions and ar-
ticles, are less likely to be accented, while
content words, such as nouns and adjectives,
are more likely to be accented. Other lin-
guistic features, such as inferred given/new
status (Hirschberg, 1993; Brown, 1983), con-
trastiveness (Bolinger, 1961), and discourse
structure (Nakatani, 1998), have also been ex-
amined to explain accent assignment in large
speech corpora. In a previous study (Pan and
McKeown, 1998; Pan and McKeown, 1999), we
investigated how features such as deep syntac-
tic/semantic structure and word informative-
ness correlate with accent placement. In this
paper, we focus on how local context influences
accent patterns. More specifically, we investi-
gate how word collocation influences whether
nouns are accented or not.
Determining which nouns are accented and
which are not is challenging, since part-of-
speech information cannot help here. So, other
accent predictors must be found. There are
some advantages in looking only at one word
class. We eliminate the interaction between
part-of-speech and collocation, so that the in-
fluence of collocation is easier to identify. It
also seems likely that collocation may have a
greater impact on content words, like nouns,
than on function words, like prepositions.
Previous researchers have speculated that
word collocation affects stress assignment of
noun phrases in English. For example, James
Marchand (1993) notes how familiar colloca-
tions change their stress, witness the American
pronunciation of `Little House&apos; [in the televi-
sion series Little House on the Prairie], where
stress used to be on HOUSE, but now, since the
series is so familiar, is placed on the LITTLE.
That is, for collocated words, stress shifts to
the left element of the compound. However,
there are numerous counter-examples: con-
sider apple PIE, which retains a right stress
pattern, despite the collocation. So, the ex-
tent to which collocational status affects ac-
cent patterns is still unclear.
Despite some preliminary investigation
(Liberman and Sproat, 1992), word colloca-
tion information has not, to our knowledge,
been successfully used to model pitch accent
assignment; nor has it been incorporated into
any existing speech synthesis systems. In this
paper, we empirically verify the usefulness of
word collocation for accent prediction. In Sec-
tion 2, we describe our annotated speech cor-
pora. In Section 3, we present a description of
the collocation measures we investigated. Sec-
tion 4 to 7 describe our analyses and machine
learning experiments in which we attempt to
predict accent location. In Section 8 we sum
up our results and discuss plans for further re-
search.
</bodyText>
<sectionHeader confidence="0.973035" genericHeader="introduction">
2 Speech Corpora
</sectionHeader>
<bodyText confidence="0.9999666">
From the medical domain described in Section
1, we collected two speech corpora and one text
corpus for pitch accent modeling. The speech
corpora consist of one multi-speaker sponta-
neous corpus, containing twenty segments and
totaling fifty minutes, and one read corpus of
five segments, read by a single speaker and to-
taling eleven minutes of speech. The text cor-
pus consists of 3.5 million words from 7375 dis-
charge summaries of patients who had under-
gone surgery. The speech corpora only cover
cardiac patients, while the text corpus covers
a larger group of patients and the majority of
them have also undergone cardiac surgery.
The speech corpora were first transcribed or-
thographically and then intonationally, using
the ToBI convention for prosodic labeling of
standard American English (Silverman et al.,
1992). For this study, we used only binary ac-
cented/deaccented decisions derived from the
ToBI tonal tier, in which location and type
of pitch accent is marked. After ToBI label-
ing, each word in the corpora was tagged with
part-of-speech, from a nine-element set: noun,
verb, adjective, adverb, article, conjunction,
pronoun, cardinal, and preposition. The spon-
taneous corpus was tagged by hand and the
read tagged automatically. As noted above,
we focus here on predicting whether nouns are
accented or not.
</bodyText>
<sectionHeader confidence="0.977174" genericHeader="method">
3 Collocation Measures
</sectionHeader>
<bodyText confidence="0.999883583333333">
We used three measures of word collocation to
examine the relationship between collocation
and accent placement: WORD BIGRAM PRE-
DICTABILITY, MUTUAL INFORMATION, and the
DICE COEFFICIENT. While word predictabil-
ity is not typically used to measure collocation,
there is some correlation between word collo-
cation and predictability. For example, if two
words are collocated, then it will be easy to
predict the second word from the first. Sim-
ilarly, if one word is highly predictable given
another word, then there is a higher possibility
that these two words are collocated. Mutual
information (Fano, 1961) and the Dice coeffi-
cient (Dice, 1945) are two standard measures
of collocation. In general, mutual information
measures uncertainty reduction or departure
from independence. The Dice coefficient is a
collocation measure widely used in information
retrieval. In the following, we will give a more
detailed definitions of each.
Statistically, bigram word predictability is
defined as the log conditional probability of
word wi, given the previous word wi_1:
</bodyText>
<equation confidence="0.997504">
Pred(wi) = log(Prob(wiJwi_1))
</equation>
<bodyText confidence="0.998706235294118">
Bigram predictability directly measures the
likelihood of seeing one word, given the
occurrence of the previous word. Bi-
gram predictability has two forms: abso-
lute and relative. Absolute predictability is
the value directly computed from the for-
mula. For example, given four adjacent
words wi_1, wi, wi+1 and wi+2, if we assume
Prob(wilwi_1) = 0.0001, Prob(wi+1Jwi) =
0.001, and Prob(wi+2Jwi+1) = 0.01, the abso-
lute bigram predictability will be -4, -3 and
-2 for wi, wi+1 and wi+2. The relative pre-
dictability is defined as the rank of absolute
predictability among words in a constituent.
In the same example, the relative predictabil-
ity will be 1, 2 and 3 for wi, wi+1 and wi+2,
where 1 is associated with the word with the
lowest absolute predictability. In general, the
higher the rank, the higher the absolute pre-
dictability. Except in Section 7, all the pre-
dictability measures mentioned in this paper
use the absolute form.
We used our text corpus to compute bigram
word predictability for our domain. When cal-
culating the word bigram predictability, we
first filtered uncommon words (words occur-
ring 5 times or fewer in the corpus) then used
the Good-Turing discount strategy to smooth
the bigram. Finally we calculated the log con-
ditional probability of each word as the mea-
sure of its bigram predictability.
Two measures of mutual information were
used for word collocation: POINTWISE MU-
TUAL INFORMATION, which is defined as :
</bodyText>
<equation confidence="0.999618333333333">
I1 (wi-1; wi) = log
Pr(wi-1, wi)
Pr(wi_1)Pr(wi)
</equation>
<bodyText confidence="0.995808285714286">
and AVERAGE MUTUAL INFORMATION, which
is defined as:
The same text corpus was used to compute
both mutual information measures. Only word
pairs with bigram frequency greater than five
were retained.
The Dice coefficient is defined as:
</bodyText>
<equation confidence="0.999422333333333">
2 x Pr(wi-1, wi)
Dice(wi_1, wi) =
Pr(wi-1) + Pr(wi)
</equation>
<bodyText confidence="0.999943473684211">
Here, we also use a cut off threshold of five to
filter uncommon bigrams.
Although all these measures are correlated,
one measure can score word pairs quite differ-
ently from another. Table 1 shows the top ten
collocations for each metric.
In the predictability top ten list, we have
pairs like scarlet fever where fever is very pre-
dictable from scarlet (in our corpus, scarlet is
always followed by fever), thus, it ranks high-
est in the predictability list. Since scarlet can
be difficult to predict from fever, these types
of pairs will not receive a very high score us-
ing mutual information (in the top 5% in I1
sorted list and in the top 20% in I2 list) and
Dice coefficient (top 22%). From this table, it
is also quite clear that I1 tends to rank un-
common words high. All the words in the top
ten I1 list have a frequency less than or equal
</bodyText>
<equation confidence="0.946597375">
I2(wi-1; wi) =
Pr(wi-1, wi) log Pr(wi_1)Pr(wi)
Pr(wi—1, wi)
Pr(wi-1, wi)
+Pr (wi-1 , wi) log Pr(wi_1)Pr(wi)
Pr(wi-1, wi)
+Pr(wi-1, wi) log Pr(wi_1)Pr(wi)
Pr(wi-1, wi)
</equation>
<table confidence="0.969721583333333">
+Pr(wi-1, wi) log Pr(wi_1)Pr(wi)
Pred I� Iz Dice
chief complaint polymyalgia rheumatica The patient greenfield filter
cerebrospinal fluid hemiside stepper present illness Guillain Barre
folic acid Pepto Bismol hospital course Viet Nam
periprocedural complications Glen Cove p o Neo Synephrine
normoactive bowel hydrogen peroxide physical exam polymyalgia rheumatica
uric acid Viet Nam i d hemiside stepper
postpericardiotomy syndrome Neo Synephrine coronary artery Pepto Bismol
Staten Island otitis media postoperative day Glen Cove
scarlet fever Lo Gerfo saphenous vein present illness
pericardiotomy syndrome Chlor Trimeton medical history chief complaint
</table>
<tableCaption confidence="0.999932">
Table 1: Top Ten Most Collocated Words for Each Measure
</tableCaption>
<bodyText confidence="0.99976684">
to seven (we filter all the pairs occurring fewer
than six times).
Of the different metrics, only bigram pre-
dictability is a unidirectional measure. It cap-
tures how the appearance of one word affects
the appearance of the following word. In con-
trast, the other measures are all bidirectional
measures, making no distinction between the
relative position of elements of a pair of col-
located items. Among the bidirectional mea-
sures, point-wise mutual information is sensi-
tive to marginal probabilities Pr(wordi_1) and
Pr(wordi). It tends to give higher values as
these probabilities decrease, independently of
the distribution of their co-occurrence. The
Dice coefficient, however, is not sensitive to
marginal probability. It computes conditional
probabilities which are equally weighted in
both directions.
Average mutual information measures the
reduction in the uncertainty, of one word,
given another, and is totally symmetric. Since
I2(wordi_1; wordi)=I2(wordi;wordi_1), the
uncertainty reduction of the first word, given
the second word, is equal to the uncer-
tainty reduction of the second word, given the
first word. Further more, because I2(wordi;
wordi_1) = I2(wordi;wordi_1), the uncer-
tainty reduction of one word, given another,
is also equal to the uncertainty reduction of
failing to see one word, having failed to see
the other.
Since there is considerable evidence that
prior discourse context, such as previous men-
tion of a word, affects pitch accent decisions,
it is possible that symmetric measures, such
as mutual information and the Dice coeffi-
cient, may not model accent placement as
well as asymmetric measures, such as bigram
predictability. Also, the bias of point-wise
mutual information toward uncommon words
can affect its ability to model accent assign-
ment, since, in general, uncommon words are
more likely to be accented (Pan and McKe-
own, 1999). Since this metric disproportion-
ately raises the mutual information for un-
common words, making them more predictable
than their appearance in the corpus warrants,
it may predict that uncommon words are more
likely to be deaccented than they really are.
</bodyText>
<sectionHeader confidence="0.995643" genericHeader="method">
4 Statistical Analyses
</sectionHeader>
<bodyText confidence="0.9997082">
In order to determine whether word collo-
cation is useful for pitch accent prediction,
we first employed Spearman&apos;s rank correlation
test (Conover, 1980).
In this experiment, we employed a unigram
predictability-based baseline model. The un-
igram predictability of a word is defined as
the log probability of a word in the text cor-
pus. The maximum likelihood estimation of
this measure is:
</bodyText>
<equation confidence="0.983356333333333">
Freq(wi)
log
T,i Freq(wi)
</equation>
<bodyText confidence="0.999956142857143">
The reason for choosing this as the baseline
model is not only because it is context inde-
pendent, but also because it is effective. In
a previous study (Pan and McKeown, 1999),
we showed that when this feature is used, it
is as powerful a predictor as part-of-speech.
When jointly used with part-of-speech infor-
mation, the combined model can perform sig-
nificantly better than each individual model.
When tested on a similar medical corpus, this
combined model also outperforms a compre-
hensive pitch accent model employed by the
Bell Labs&apos; TTS system (Sproat et al., 1992;
Hirschberg, 1993; Sproat, 1998), where dis-
course information, such as given/new, syntac-
tic information, such as POS, and surface in-
formation, such as word distance, are incorpo-
rated. Since unigram predictability is context
independent. By comparing other predictors
to this baseline model, we can demonstrate the
impact of context, measured by word colloca-
tion, on pitch accent assignment.
Table 2 shows that for our read speech
corpus, unigram predictability, bigram pre-
dictability and mutual information are all sig-
nificantly correlated (p &lt; 0.001) with pitch ac-
cent decision.&apos; However, the Dice coefficient
shows only a trend toward correlation (p &lt;
0.07). In addition, both bigram predictabil-
ity and (pointwise) mutual information show a
slightly stronger correlation with pitch accent
than the baseline. When we conducted a sim-
ilar test on the spontaneous corpus, we found
that all but the baseline model are significantly
correlated with pitch accent placement. Since
all three models incorporate a context word
while the baseline model does not, these re-
sults suggest the usefulness of context in ac-
cent prediction. Overall, for all the different
measures of collocation, bigram predictability
explains the largest amount of variation in ac-
cent status for both corpora. We conducted a
similar test using trigram predictability, where
two context words, instead of one, were used
to predict the current word. The results are
slightly worse than bigram predictability (for
the read corpus r = —0.167, p &lt; 0.0001; for
the spontaneous r = —0.355, p &lt; 0.0001).
The failure of the trigram model to improve
over the bigram model may be due to sparse
data. Thus, in the following analysis, we focus
on bigram predictability. In order to further
verify the effectiveness of word predictability
in accent prediction, we will show some exam-
ples in our speech corpora first. Then we will
describe how machine learning helps to derive
pitch accent prediction models using this fea-
ture. Finally, we show that both absolute pre-
dictability and relative predictability are use-
ful for pitch accent prediction.
&apos;Since pointwise mutual information performed con-
sistently better than average mutual information in our
experiment, we present results only for the former.
</bodyText>
<sectionHeader confidence="0.799459" genericHeader="method">
5 Word Predictability and Accent
</sectionHeader>
<bodyText confidence="0.999857642857143">
In general, nouns, especially head nouns, are
very likely to be accented. However, cer-
tain nouns consistently do not get accented.
For example, Table 3 shows some collocations
containing the word cell in our speech cor-
pus. For each context, we list the collocated
pair, its most frequent accent pattern in our
corpus (upper case indicates that the word
was accented and lower case indicates that
it was deaccented), its bigram predictability
(the larger the number is, the more predictable
the word is), and the frequency of this ac-
cent pattern, as well as the total occurrence
of the bigram in the corpus. In the first ex-
</bodyText>
<table confidence="0.984839">
Word Pair Pred(cell) Freq
[of] CELL -3.11 7/7
[RED] CELL -1.119 2/2
[PACKED] cell -0.5759 4/6
[BLOOD] cell -0.067 2/2
</table>
<tableCaption confidence="0.999297">
Table 3: cell Collocations
</tableCaption>
<bodyText confidence="0.99987215">
ample, cell in [of] CELL is very unpredictable
from the occurrence of of and always receives a
pitch accent. In [RED] CELL, [PACKED] cell,
and [BLOOD] cell, cell has the same semantic
meaning, but different accent patterns: cell in
[PACKED] cell and [BLOOD] cell is more pre-
dictable and deaccented, while in [RED] CELL
it is less predictable and is accented. These
examples show the influence of context and
its usefulness for bigram predictability. Other
predictable nouns, such as saver in CELL
saver usually are not accented even when they
function as head nouns. Saver is deaccented in
ten of the eleven instances in our speech cor-
pus. Its bigram score is -1.5517, which is much
higher than that of CELL (-4.6394{3.1083 de-
pending upon context). Without collocation
information, a typical accent prediction sys-
tem is likely to accent saver, which would be
inappropriate in this domain.
</bodyText>
<sectionHeader confidence="0.9927" genericHeader="method">
6 Accent Prediction Models
</sectionHeader>
<bodyText confidence="0.999908">
Both the correlation test results and direct ob-
servations provide some evidence on the useful-
ness of word predictability. But we still need to
demonstrate that we can successfully use this
feature in automatic accent prediction. In or-
der to achieve this, we used machine learning
</bodyText>
<table confidence="0.996433166666666">
Corpus Read Spontaneous
r p-value r p-value
Baseline (Unigram) r = —0.166 p = 0.0002 r = —0.02 p = 0.39
Bigram Predictability r = —0.236 p &lt; 0.0001 r = —0.36 p &lt; 0.0001
Pointwise Mutual Information r = —0.185 p &lt; 0.0001 r = —0.177 p &lt; 0.0001
Dice Coefficient r = —0.079 p = 0.066 r = —0.094 p &lt; 0.0001
</table>
<tableCaption confidence="0.999669">
Table 2: Correlation of Different Collocation Measures with Accent Decision
</tableCaption>
<bodyText confidence="0.997809677419355">
techniques to automatically build accent pre-
diction models using bigram word predictabil-
ity scores.
We used RIPPER (Cohen, 1995b) to ex-
plore the relations between predictability and
accent placement. RIPPER is a classification-
based rule induction system. From annotated
examples, it derives a set of ordered if-then
rules, describing how input features can be
used to predict an output feature. In order
to avoid overfitting, we use 5-fold cross valida-
tion. The training data include all the nouns in
the speech corpora. The independent variables
used to predict accent status are the unigram
and bigram predictability measures, and the
dependent variable is pitch accent status. We
used a majority-based predictability model as
our baseline (i.e. predict accented).
In the combined model, both unigram and
bigram predictability are used together for ac-
cent prediction. From the results in Table 4,
we see that the bigram model consistently out-
performs the unigram model, and the com-
bined model achieves the best performance.
To evaluate the significance of the improve-
ments achieved by incorporating a context
word, we use the standard error produced by
RIPPER. Two results are statistically signif-
icant when the results plus or minus twice
the standard error do not overlap (Cohen,
1995a). As shown in Table 4, for the read
corpus, except for the unigram model, all the
models with bigram predictability performed
significantly better than the baseline model.
However, the bigram model and the combined
model failed to improve significantly over the
unigram model. This may result from too
small a corpus. For the spontaneous corpus,
the unigram, bigram and the combined model
all achieved significant improvement over the
baseline. The bigram also performed signifi-
cantly better than the unigram model. The
combined model had the best performance. It
also achieved significant improvement over the
unigram model.
The improvement of the combined model
over both unigram and bigram models may
be due to the fact that some accent patterns
that are not captured by one are indeed cap-
tured by the other. For example, accent pat-
terns for street names have been extensively
discussed in the literature (Ladd, 1996). For
example, street in phrases like (e.g. FIFTH
street) is typically deaccented while avenue
(e.g. Fifth AVENUE) is accented. While it
seems likely that the conditional probability
of Pr(Street Fifth) is no higher than that of
Pr(AvenueIFifth), the unigram probability of
Pr(street) is probably higher than that of av-
enue Pr(avenue).2. So, incorporating both
predictability measures may tease apart these
and similar cases.
</bodyText>
<sectionHeader confidence="0.987316" genericHeader="method">
7 Relative Predictability
</sectionHeader>
<bodyText confidence="0.999951272727273">
In the our previous analysis, we showed the ef-
fectiveness of absolute word predictability. We
now consider whether relative predictability is
correlated with a larger constituent&apos;s accent
pattern. The following analysis focuses on ac-
cent patterns of non-trivial base NPs.3 For
this study we labeled base NPs by hand for
the corpora described in Section 2. For each
base NP, we calculate which word is the most
predictable and which is the least. We want
to see, when comparing with its neighboring
</bodyText>
<footnote confidence="0.981833642857143">
2For example, in a 7.5M word general news corpus
(from CNN and Reuters), street occurs 2115 times and
avenue just 194. Therefore, the unigram predictabil-
ity of street is higher than that of avenue. The most
common bigram with street is Wall Street which occurs
116 times and the most common bigram with avenue is
Pennsylvania Avenue which occurs 97. In this domain,
the bigram predictability for street in Fifth Street is ex-
tremely low because this combination never occurred,
while that for avenue in Fifth Avenue is -3.0995 which
is the third most predictable bigrams with avenue as
the second word.
3Non-recursive noun phrases containing at least two
elements.
</footnote>
<table confidence="0.998721555555556">
Corpus Predictability Model Performance Standard Error
Read baseline model 81.98%
unigram model 82.86% f 0.93
bigram predictability model 84.41% f 1.10
unigram+bigram model 85.03% f 1.04
Spontaneous baseline model 70.03%
unigram model 72.22% f 0.62
bigram model 74.46% f 0.30
unigram+bigram model 77.43% f 0.51
</table>
<tableCaption confidence="0.983651">
Table 4: Ripper Results for Accent Status Prediction
</tableCaption>
<table confidence="0.9998148">
Model Predictability Total Accented Word Not Accented Accentability
unigram Least Predictable 1206 877 329 72.72%
Most Predictable 1198 485 713 40.48%
bigram Least Predictable 1205 965 240 80.08%
Most Predictable 1194 488 706 40.87%
</table>
<tableCaption confidence="0.99953">
Table 5: Relative Predictability and Accent Status
</tableCaption>
<bodyText confidence="0.999971">
words, whether the most predictable word is
more likely to be deaccented. As shown in Ta-
ble 5, the &amp;quot;total&amp;quot; column represents the total
number of most (or least) predictable words
in all baseNPs4. The next two columns indi-
cate how many of them are accented and deac-
cented. The last column is the percentage of
words that are accented. Table 5 shows that
the probability of accenting a most predictable
word is between 40.48% and 45.96% and that
of a least predictable word is between 72.72%
and 80.08%. This result indicates that rela-
tive predictability is also a useful predictor for
a word&apos;s accentability.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999970375">
It is difficult to directly compare our results
with previous accent prediction studies, to
determine the general utility of bigram pre-
dictability in accent assignment, due to dif-
ferences in domain and the scope of our task.
For example, Hirschberg (1993) built a com-
prehensive accent prediction model using ma-
chine learning techniques for predicting ac-
cent status for all word classes for a text-to-
speech system, employing part-of-speech, var-
ious types of information status inferred from
the text, and a number of distance metrics,
as well as a complex nominal predictor devel-
oped by Sproat (1992). An algorithm making
use of these features achieved 76.5%-80% ac-
cent prediction accuracy for a broadcast news
</bodyText>
<footnote confidence="0.8297985">
4The total number of most predictable words is not
equal to that of least predictable words due to ties.
</footnote>
<bodyText confidence="0.999935227272727">
corpus, 85% for sentences from the ATIS cor-
pus of spontaneous elicited speech, and 98.3%
success on a corpus of laboratory read sen-
tences. Liberman and Sproat&apos;s (1992) success
in predicting accent patterns for complex nom-
inals alone, using rules combining a number
of features, achieved considerably higher suc-
cess rates (91% correct, 5.4% acceptable, 3.6%
unacceptable when rated by human subjects)
for 500 complex nominals of 2 or more ele-
ments chosen from the AP Newswire. Our re-
sults, using bigram predictability alone, 77%
for the spontaneous corpus and 85% for the
read corpus, and using a different success es-
timate, while not as impressive as (Liberman
and Sproat, 1992)&apos;s, nonetheless demonstrate
the utility of a relatively untested feature for
this task.
In this paper, we have investigated several
collocation-based measures for pitch accent
prediction. Our initial hypothesis was that
word collocation affects pitch accent place-
ment, and that the more predictable a word
is in terms of its local lexical context, the
more likely it is to be deaccented. In order
to verify this claim, we estimated three col-
location measures: word predictability, mu-
tual information and the Dice coefficient. We
then used statistical techniques to analyze the
correlation between our different word collo-
cation metrics and pitch accent assignment
for nouns. Our results show that, of all the
collocation measures we investigated, bigram
word predictability has the strongest correla-
tion with pitch accent assignment. Based on
this finding, we built several pitch accent mod-
els, assessing the usefulness of unigram and
bigram word predictability —as well as a com-
bined model— in accent predication. Our re-
sults show that the bigram model performs
consistently better than the unigram model,
which does not incorporate local context in-
formation. However, our combined model per-
forms best of all, suggesting that both con-
textual and non-contextual features of a word
are important in determining whether or not
it should be accented.
These results are particularly important for
the development of future accent assignment
algorithms for text-to-speech. For our contin-
uing research, we will focus on two directions.
The first is to combine our word predictability
feature with other pitch accent predictors that
have been previously used for automatic accent
prediction. Features such as information sta-
tus, grammatical function, and part-of-speech,
have also been shown to be important deter-
minants of accent assignment. So, our final
pitch accent model should include many other
features. Second, we hope to test whether the
utility of bigram predictability can be gener-
alized across different domains. For this pur-
pose, we have collected an annotated AP news
speech corpus and an AP news text corpus,
and we will carry out a similar experiment in
this domain.
</bodyText>
<sectionHeader confidence="0.997983" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999226666666666">
Thanks for C. Jin, K. McKeown, R. Barzi-
lay, J. Shaw, N. Elhadad, M. Kan, D. Jor-
dan, and anonymous reviewers for the help on
data preparation and useful comments. This
research is supported in part by the NSF Grant
IRI 9528998, the NLM Grant R01 LM06593-01
and the Columbia University Center for Ad-
vanced Technology in High Performance Com-
puting and Communications in Healthcare.
</bodyText>
<sectionHeader confidence="0.998941" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997868428571429">
D. Bolinger. 1961. Contrastive accent and con-
trastive stress. language, 37:83{96.
D. Bolinger. 1989. Intonation and Its Uses. Stan-
ford University Press.
G. Brown. 1983. Prosodic structure and the
given/new distinction. In A. Cutler and D.R.
Ladd, ed., Prosody: Models and Measurements,
pages 67{78. Springer-Verlag, Berlin.
P. Cohen. 1995a. Empirical methods for artificial
intelligence. MIT press, Cambridge, MA.
W. Cohen. 1995b. Fast effective rule induction.
In Proc. of the 12th International Conference on
Machine Learning.
W. J. Conover. 1980. Practical Nonparametric
Statistics. Wiley, New York, 2nd edition.
M. Dalal, S. Feiner, K. McKeown, S. Pan, M. Zhou,
T. Hoellerer, J. Shaw, Y. Feng, and J. Fromer.
1996. Negotiation for automated generation of
temporal multimedia presentations. In Proc. of
ACM Multimedia 96, pages 55{64.
Lee R. Dice. 1945. Measures of the amount of
ecologic association between species. Journal of
Ecology, 26:297{302.
Robert M. Fano. 1961. Transmission of Informa-
tion: A Statistical Theory of Communications.
MIT Press, Cambridge, MA.
J. Hirschberg. 1993. Pitch accent in context: pre-
dicting intonational prominence from text. Ar-
tificial Intelligence, 63:305{340.
D. Robert Ladd. 1996. Intonational Phonology.
Cambridge University Press, Cambridge.
M. Liberman and R. Sproat. 1992. The stress and
structure of modified noun phrases in English.
In I. Sag, ed., Lexical Matters, pages 131{182.
University of Chicago Press.
J. Marchand. 1993. Message posted on HUMAN-
IST mailing list, April.
C. Nakatani. 1998. Constituent-based accent pre-
diction. In Proc. of COLING/ACL&apos;98, pages
939{945, Montreal, Canada.
S. Pan and K. McKeown. 1998. Learning intona-
tion rules for concept to speech generation. In
Proc. of COLING/ACL&apos;98, Montreal, Canada.
S. Pan and K. McKeown. 1999. Word informa-
tiveness and automatic pitch accent modeling.
In Proc. of the Joint SIGDAT Conference on
EMNLP and VLC, pages 148{157.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert,
and J. Hirschberg. 1992. ToBI: a standard for
labeling English prosody. In Proc. of ICSLP92.
R. Sproat, J. Hirschberg, and D. Yarowsky. 1992.
A corpus-based synthesizer. In Proc. of IC-
SLP92, pages 563{566, Banff.
R. Sproat, ed. 1998. Multilingual Text-to-Speech
Synthesis: The Bell Labs Approach. Kluwer.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983030">
<title confidence="0.999984">Modeling Local Context for Pitch Accent Prediction</title>
<author confidence="0.999861">Shimei Pan</author>
<affiliation confidence="0.9998895">Department of Computer Science Columbia University</affiliation>
<address confidence="0.999828">New York, NY, 10027, USA</address>
<email confidence="0.997906">pan@cs.columbia.edu</email>
<author confidence="0.999573">Julia Hirschberg</author>
<affiliation confidence="0.999937">AT&amp;T Labs-Research</affiliation>
<address confidence="0.999473">Florham Park, NJ, 07932-0971, USA</address>
<email confidence="0.999897">julia@research.att.com</email>
<abstract confidence="0.999167823529412">Pitch accent placement is a major in intonational phonology search and its application to speech synthesis. What factors influence whether or not a word is made intonationally prominent or not is an open question. In this paper, we investigate how one aspect of a local context its collocawith neighboring words influences whether it is accented or not. Results of experiments on two transcribed speech corpora in a medical domain show that such collocation information is a useful predictor of pitch accent placement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bolinger</author>
</authors>
<title>Contrastive accent and contrastive stress.</title>
<date>1961</date>
<pages>37--83</pages>
<contexts>
<context position="3842" citStr="Bolinger, 1961" startWordPosition="582" endWordPosition="583">been a research focus for scientists working on phonology, speech analysis and synthesis (Bolinger, 1989; Ladd, 1996). In general, syntactic features are the most widely used features in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in large speech corpora. In a previous study (Pan and McKeown, 1998; Pan and McKeown, 1999), we investigated how features such as deep syntactic/semantic structure and word informativeness correlate with accent placement. In this paper, we focus on how local context influences accent patterns. More specifically, we investigate how word collocation influences whether nouns are accented or not. Determining which nouns are accented and which are not is challenging, since part-ofspeech information cann</context>
</contexts>
<marker>Bolinger, 1961</marker>
<rawString>D. Bolinger. 1961. Contrastive accent and contrastive stress. language, 37:83{96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bolinger</author>
</authors>
<title>Intonation and Its Uses.</title>
<date>1989</date>
<publisher>Stanford University Press.</publisher>
<contexts>
<context position="3331" citStr="Bolinger, 1989" startWordPosition="504" endWordPosition="505">nd coherent speech output, MAGIC&apos;s spoken language generator models a collection of speech features, such as accenting and intonational phrasing, which are critical to the naturalness and intelligibility of output speech. In order to assign these features accurately, the system needs to identify useful correlates of accent and phrase boundary location to use as predictors. This work represents part of our efforts in identifying useful predictors for pitch accent placement. Pitch accent placement has long been a research focus for scientists working on phonology, speech analysis and synthesis (Bolinger, 1989; Ladd, 1996). In general, syntactic features are the most widely used features in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent ass</context>
</contexts>
<marker>Bolinger, 1989</marker>
<rawString>D. Bolinger. 1989. Intonation and Its Uses. Stanford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Brown</author>
</authors>
<title>Prosodic structure and the given/new distinction.</title>
<date>1983</date>
<booktitle>Prosody: Models and Measurements,</booktitle>
<pages>67--78</pages>
<editor>In A. Cutler and D.R. Ladd, ed.,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="3808" citStr="Brown, 1983" startWordPosition="578" endWordPosition="579">itch accent placement has long been a research focus for scientists working on phonology, speech analysis and synthesis (Bolinger, 1989; Ladd, 1996). In general, syntactic features are the most widely used features in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in large speech corpora. In a previous study (Pan and McKeown, 1998; Pan and McKeown, 1999), we investigated how features such as deep syntactic/semantic structure and word informativeness correlate with accent placement. In this paper, we focus on how local context influences accent patterns. More specifically, we investigate how word collocation influences whether nouns are accented or not. Determining which nouns are accented and which are not is challenging, si</context>
</contexts>
<marker>Brown, 1983</marker>
<rawString>G. Brown. 1983. Prosodic structure and the given/new distinction. In A. Cutler and D.R. Ladd, ed., Prosody: Models and Measurements, pages 67{78. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cohen</author>
</authors>
<title>Empirical methods for artificial intelligence.</title>
<date>1995</date>
<publisher>MIT press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="20062" citStr="Cohen, 1995" startWordPosition="3184" endWordPosition="3185">fully use this feature in automatic accent prediction. In order to achieve this, we used machine learning Corpus Read Spontaneous r p-value r p-value Baseline (Unigram) r = —0.166 p = 0.0002 r = —0.02 p = 0.39 Bigram Predictability r = —0.236 p &lt; 0.0001 r = —0.36 p &lt; 0.0001 Pointwise Mutual Information r = —0.185 p &lt; 0.0001 r = —0.177 p &lt; 0.0001 Dice Coefficient r = —0.079 p = 0.066 r = —0.094 p &lt; 0.0001 Table 2: Correlation of Different Collocation Measures with Accent Decision techniques to automatically build accent prediction models using bigram word predictability scores. We used RIPPER (Cohen, 1995b) to explore the relations between predictability and accent placement. RIPPER is a classificationbased rule induction system. From annotated examples, it derives a set of ordered if-then rules, describing how input features can be used to predict an output feature. In order to avoid overfitting, we use 5-fold cross validation. The training data include all the nouns in the speech corpora. The independent variables used to predict accent status are the unigram and bigram predictability measures, and the dependent variable is pitch accent status. We used a majority-based predictability model a</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>P. Cohen. 1995a. Empirical methods for artificial intelligence. MIT press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proc. of the 12th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="20062" citStr="Cohen, 1995" startWordPosition="3184" endWordPosition="3185">fully use this feature in automatic accent prediction. In order to achieve this, we used machine learning Corpus Read Spontaneous r p-value r p-value Baseline (Unigram) r = —0.166 p = 0.0002 r = —0.02 p = 0.39 Bigram Predictability r = —0.236 p &lt; 0.0001 r = —0.36 p &lt; 0.0001 Pointwise Mutual Information r = —0.185 p &lt; 0.0001 r = —0.177 p &lt; 0.0001 Dice Coefficient r = —0.079 p = 0.066 r = —0.094 p &lt; 0.0001 Table 2: Correlation of Different Collocation Measures with Accent Decision techniques to automatically build accent prediction models using bigram word predictability scores. We used RIPPER (Cohen, 1995b) to explore the relations between predictability and accent placement. RIPPER is a classificationbased rule induction system. From annotated examples, it derives a set of ordered if-then rules, describing how input features can be used to predict an output feature. In order to avoid overfitting, we use 5-fold cross validation. The training data include all the nouns in the speech corpora. The independent variables used to predict accent status are the unigram and bigram predictability measures, and the dependent variable is pitch accent status. We used a majority-based predictability model a</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. Cohen. 1995b. Fast effective rule induction. In Proc. of the 12th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Conover</author>
</authors>
<title>Practical Nonparametric Statistics.</title>
<date>1980</date>
<publisher>Wiley,</publisher>
<location>New York,</location>
<note>2nd edition.</note>
<contexts>
<context position="14471" citStr="Conover, 1980" startWordPosition="2263" endWordPosition="2264">wise mutual information toward uncommon words can affect its ability to model accent assignment, since, in general, uncommon words are more likely to be accented (Pan and McKeown, 1999). Since this metric disproportionately raises the mutual information for uncommon words, making them more predictable than their appearance in the corpus warrants, it may predict that uncommon words are more likely to be deaccented than they really are. 4 Statistical Analyses In order to determine whether word collocation is useful for pitch accent prediction, we first employed Spearman&apos;s rank correlation test (Conover, 1980). In this experiment, we employed a unigram predictability-based baseline model. The unigram predictability of a word is defined as the log probability of a word in the text corpus. The maximum likelihood estimation of this measure is: Freq(wi) log T,i Freq(wi) The reason for choosing this as the baseline model is not only because it is context independent, but also because it is effective. In a previous study (Pan and McKeown, 1999), we showed that when this feature is used, it is as powerful a predictor as part-of-speech. When jointly used with part-of-speech information, the combined model </context>
</contexts>
<marker>Conover, 1980</marker>
<rawString>W. J. Conover. 1980. Practical Nonparametric Statistics. Wiley, New York, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dalal</author>
<author>S Feiner</author>
<author>K McKeown</author>
<author>S Pan</author>
<author>M Zhou</author>
<author>T Hoellerer</author>
<author>J Shaw</author>
<author>Y Feng</author>
<author>J Fromer</author>
</authors>
<title>Negotiation for automated generation of temporal multimedia presentations.</title>
<date>1996</date>
<booktitle>In Proc. of ACM Multimedia 96,</booktitle>
<pages>55--64</pages>
<contexts>
<context position="2036" citStr="Dalal et al., 1996" startWordPosition="307" endWordPosition="310">formation status, including the given/new distinction, focus, and contrastiveness, inter alia. Assigning pitch accent in speech generation systems which employ speech synthesizers for output is thus critical to system performance: not only must one convey meaning naturally, as humans would, but one must avoid conveying mis-information which reliance on the synthesizers&apos; defaults may result in. The speech generation work discussed here is part of a larger effort in developing an intelligent multimedia presentation generation system called MAGIC (Medical Abstract Generation for Intensive Care) (Dalal et al., 1996). In MAGIC, given a patient&apos;s medical record stored at Columbia Presbyterian Medical Center (CPMC)&apos;s on-line database system, the system automatically generates a postoperative status report for a patient who has just undergone bypass surgery. There are two media-specific generators in MAGIC: a graphics generator which automatically produces graphical presentations from database entities, and a spoken language generator which automatically produces coherent spoken language presentations from these entities. The graphical and the speech generators communicate with each other on the fly to ensur</context>
</contexts>
<marker>Dalal, Feiner, McKeown, Pan, Zhou, Hoellerer, Shaw, Feng, Fromer, 1996</marker>
<rawString>M. Dalal, S. Feiner, K. McKeown, S. Pan, M. Zhou, T. Hoellerer, J. Shaw, Y. Feng, and J. Fromer. 1996. Negotiation for automated generation of temporal multimedia presentations. In Proc. of ACM Multimedia 96, pages 55{64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee R Dice</author>
</authors>
<title>Measures of the amount of ecologic association between species.</title>
<date>1945</date>
<journal>Journal of Ecology,</journal>
<pages>26--297</pages>
<contexts>
<context position="8221" citStr="Dice, 1945" startWordPosition="1273" endWordPosition="1274">collocation to examine the relationship between collocation and accent placement: WORD BIGRAM PREDICTABILITY, MUTUAL INFORMATION, and the DICE COEFFICIENT. While word predictability is not typically used to measure collocation, there is some correlation between word collocation and predictability. For example, if two words are collocated, then it will be easy to predict the second word from the first. Similarly, if one word is highly predictable given another word, then there is a higher possibility that these two words are collocated. Mutual information (Fano, 1961) and the Dice coefficient (Dice, 1945) are two standard measures of collocation. In general, mutual information measures uncertainty reduction or departure from independence. The Dice coefficient is a collocation measure widely used in information retrieval. In the following, we will give a more detailed definitions of each. Statistically, bigram word predictability is defined as the log conditional probability of word wi, given the previous word wi_1: Pred(wi) = log(Prob(wiJwi_1)) Bigram predictability directly measures the likelihood of seeing one word, given the occurrence of the previous word. Bigram predictability has two for</context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>Lee R. Dice. 1945. Measures of the amount of ecologic association between species. Journal of Ecology, 26:297{302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Fano</author>
</authors>
<title>Transmission of Information: A Statistical Theory of Communications.</title>
<date>1961</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8183" citStr="Fano, 1961" startWordPosition="1266" endWordPosition="1267">asures We used three measures of word collocation to examine the relationship between collocation and accent placement: WORD BIGRAM PREDICTABILITY, MUTUAL INFORMATION, and the DICE COEFFICIENT. While word predictability is not typically used to measure collocation, there is some correlation between word collocation and predictability. For example, if two words are collocated, then it will be easy to predict the second word from the first. Similarly, if one word is highly predictable given another word, then there is a higher possibility that these two words are collocated. Mutual information (Fano, 1961) and the Dice coefficient (Dice, 1945) are two standard measures of collocation. In general, mutual information measures uncertainty reduction or departure from independence. The Dice coefficient is a collocation measure widely used in information retrieval. In the following, we will give a more detailed definitions of each. Statistically, bigram word predictability is defined as the log conditional probability of word wi, given the previous word wi_1: Pred(wi) = log(Prob(wiJwi_1)) Bigram predictability directly measures the likelihood of seeing one word, given the occurrence of the previous w</context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Robert M. Fano. 1961. Transmission of Information: A Statistical Theory of Communications. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
</authors>
<title>Pitch accent in context: predicting intonational prominence from text.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--305</pages>
<contexts>
<context position="3548" citStr="Hirschberg, 1993" startWordPosition="537" endWordPosition="538">speech. In order to assign these features accurately, the system needs to identify useful correlates of accent and phrase boundary location to use as predictors. This work represents part of our efforts in identifying useful predictors for pitch accent placement. Pitch accent placement has long been a research focus for scientists working on phonology, speech analysis and synthesis (Bolinger, 1989; Ladd, 1996). In general, syntactic features are the most widely used features in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in large speech corpora. In a previous study (Pan and McKeown, 1998; Pan and McKeown, 1999), we investigated how features such as deep syntactic/semantic structure and word informativeness correlate with accen</context>
<context position="15321" citStr="Hirschberg, 1993" startWordPosition="2404" endWordPosition="2405">: Freq(wi) log T,i Freq(wi) The reason for choosing this as the baseline model is not only because it is context independent, but also because it is effective. In a previous study (Pan and McKeown, 1999), we showed that when this feature is used, it is as powerful a predictor as part-of-speech. When jointly used with part-of-speech information, the combined model can perform significantly better than each individual model. When tested on a similar medical corpus, this combined model also outperforms a comprehensive pitch accent model employed by the Bell Labs&apos; TTS system (Sproat et al., 1992; Hirschberg, 1993; Sproat, 1998), where discourse information, such as given/new, syntactic information, such as POS, and surface information, such as word distance, are incorporated. Since unigram predictability is context independent. By comparing other predictors to this baseline model, we can demonstrate the impact of context, measured by word collocation, on pitch accent assignment. Table 2 shows that for our read speech corpus, unigram predictability, bigram predictability and mutual information are all significantly correlated (p &lt; 0.001) with pitch accent decision.&apos; However, the Dice coefficient shows </context>
<context position="25283" citStr="Hirschberg (1993)" startWordPosition="4016" endWordPosition="4017"> deaccented. The last column is the percentage of words that are accented. Table 5 shows that the probability of accenting a most predictable word is between 40.48% and 45.96% and that of a least predictable word is between 72.72% and 80.08%. This result indicates that relative predictability is also a useful predictor for a word&apos;s accentability. 8 Discussion It is difficult to directly compare our results with previous accent prediction studies, to determine the general utility of bigram predictability in accent assignment, due to differences in domain and the scope of our task. For example, Hirschberg (1993) built a comprehensive accent prediction model using machine learning techniques for predicting accent status for all word classes for a text-tospeech system, employing part-of-speech, various types of information status inferred from the text, and a number of distance metrics, as well as a complex nominal predictor developed by Sproat (1992). An algorithm making use of these features achieved 76.5%-80% accent prediction accuracy for a broadcast news 4The total number of most predictable words is not equal to that of least predictable words due to ties. corpus, 85% for sentences from the ATIS </context>
</contexts>
<marker>Hirschberg, 1993</marker>
<rawString>J. Hirschberg. 1993. Pitch accent in context: predicting intonational prominence from text. Artificial Intelligence, 63:305{340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Robert Ladd</author>
</authors>
<title>Intonational Phonology.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="3344" citStr="Ladd, 1996" startWordPosition="506" endWordPosition="507">ch output, MAGIC&apos;s spoken language generator models a collection of speech features, such as accenting and intonational phrasing, which are critical to the naturalness and intelligibility of output speech. In order to assign these features accurately, the system needs to identify useful correlates of accent and phrase boundary location to use as predictors. This work represents part of our efforts in identifying useful predictors for pitch accent placement. Pitch accent placement has long been a research focus for scientists working on phonology, speech analysis and synthesis (Bolinger, 1989; Ladd, 1996). In general, syntactic features are the most widely used features in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in la</context>
<context position="22145" citStr="Ladd, 1996" startWordPosition="3516" endWordPosition="3517"> corpus. For the spontaneous corpus, the unigram, bigram and the combined model all achieved significant improvement over the baseline. The bigram also performed significantly better than the unigram model. The combined model had the best performance. It also achieved significant improvement over the unigram model. The improvement of the combined model over both unigram and bigram models may be due to the fact that some accent patterns that are not captured by one are indeed captured by the other. For example, accent patterns for street names have been extensively discussed in the literature (Ladd, 1996). For example, street in phrases like (e.g. FIFTH street) is typically deaccented while avenue (e.g. Fifth AVENUE) is accented. While it seems likely that the conditional probability of Pr(Street Fifth) is no higher than that of Pr(AvenueIFifth), the unigram probability of Pr(street) is probably higher than that of avenue Pr(avenue).2. So, incorporating both predictability measures may tease apart these and similar cases. 7 Relative Predictability In the our previous analysis, we showed the effectiveness of absolute word predictability. We now consider whether relative predictability is correl</context>
</contexts>
<marker>Ladd, 1996</marker>
<rawString>D. Robert Ladd. 1996. Intonational Phonology. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liberman</author>
<author>R Sproat</author>
</authors>
<title>The stress and structure of modified noun phrases in English. In</title>
<date>1992</date>
<booktitle>Lexical Matters,</booktitle>
<pages>131--182</pages>
<editor>I. Sag, ed.,</editor>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="5601" citStr="Liberman and Sproat, 1992" startWordPosition="852" endWordPosition="855">otes how familiar collocations change their stress, witness the American pronunciation of `Little House&apos; [in the television series Little House on the Prairie], where stress used to be on HOUSE, but now, since the series is so familiar, is placed on the LITTLE. That is, for collocated words, stress shifts to the left element of the compound. However, there are numerous counter-examples: consider apple PIE, which retains a right stress pattern, despite the collocation. So, the extent to which collocational status affects accent patterns is still unclear. Despite some preliminary investigation (Liberman and Sproat, 1992), word collocation information has not, to our knowledge, been successfully used to model pitch accent assignment; nor has it been incorporated into any existing speech synthesis systems. In this paper, we empirically verify the usefulness of word collocation for accent prediction. In Section 2, we describe our annotated speech corpora. In Section 3, we present a description of the collocation measures we investigated. Section 4 to 7 describe our analyses and machine learning experiments in which we attempt to predict accent location. In Section 8 we sum up our results and discuss plans for fu</context>
<context position="26521" citStr="Liberman and Sproat, 1992" startWordPosition="4217" endWordPosition="4220">ontaneous elicited speech, and 98.3% success on a corpus of laboratory read sentences. Liberman and Sproat&apos;s (1992) success in predicting accent patterns for complex nominals alone, using rules combining a number of features, achieved considerably higher success rates (91% correct, 5.4% acceptable, 3.6% unacceptable when rated by human subjects) for 500 complex nominals of 2 or more elements chosen from the AP Newswire. Our results, using bigram predictability alone, 77% for the spontaneous corpus and 85% for the read corpus, and using a different success estimate, while not as impressive as (Liberman and Sproat, 1992)&apos;s, nonetheless demonstrate the utility of a relatively untested feature for this task. In this paper, we have investigated several collocation-based measures for pitch accent prediction. Our initial hypothesis was that word collocation affects pitch accent placement, and that the more predictable a word is in terms of its local lexical context, the more likely it is to be deaccented. In order to verify this claim, we estimated three collocation measures: word predictability, mutual information and the Dice coefficient. We then used statistical techniques to analyze the correlation between our</context>
</contexts>
<marker>Liberman, Sproat, 1992</marker>
<rawString>M. Liberman and R. Sproat. 1992. The stress and structure of modified noun phrases in English. In I. Sag, ed., Lexical Matters, pages 131{182. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Marchand</author>
</authors>
<date>1993</date>
<note>Message posted on HUMANIST mailing list,</note>
<contexts>
<context position="4973" citStr="Marchand (1993)" startWordPosition="754" endWordPosition="755"> are accented and which are not is challenging, since part-ofspeech information cannot help here. So, other accent predictors must be found. There are some advantages in looking only at one word class. We eliminate the interaction between part-of-speech and collocation, so that the influence of collocation is easier to identify. It also seems likely that collocation may have a greater impact on content words, like nouns, than on function words, like prepositions. Previous researchers have speculated that word collocation affects stress assignment of noun phrases in English. For example, James Marchand (1993) notes how familiar collocations change their stress, witness the American pronunciation of `Little House&apos; [in the television series Little House on the Prairie], where stress used to be on HOUSE, but now, since the series is so familiar, is placed on the LITTLE. That is, for collocated words, stress shifts to the left element of the compound. However, there are numerous counter-examples: consider apple PIE, which retains a right stress pattern, despite the collocation. So, the extent to which collocational status affects accent patterns is still unclear. Despite some preliminary investigation</context>
</contexts>
<marker>Marchand, 1993</marker>
<rawString>J. Marchand. 1993. Message posted on HUMANIST mailing list, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nakatani</author>
</authors>
<title>Constituent-based accent prediction.</title>
<date>1998</date>
<booktitle>In Proc. of COLING/ACL&apos;98,</booktitle>
<pages>939--945</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3884" citStr="Nakatani, 1998" startWordPosition="587" endWordPosition="588">ng on phonology, speech analysis and synthesis (Bolinger, 1989; Ladd, 1996). In general, syntactic features are the most widely used features in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in large speech corpora. In a previous study (Pan and McKeown, 1998; Pan and McKeown, 1999), we investigated how features such as deep syntactic/semantic structure and word informativeness correlate with accent placement. In this paper, we focus on how local context influences accent patterns. More specifically, we investigate how word collocation influences whether nouns are accented or not. Determining which nouns are accented and which are not is challenging, since part-ofspeech information cannot help here. So, other accent predictors </context>
</contexts>
<marker>Nakatani, 1998</marker>
<rawString>C. Nakatani. 1998. Constituent-based accent prediction. In Proc. of COLING/ACL&apos;98, pages 939{945, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pan</author>
<author>K McKeown</author>
</authors>
<title>Learning intonation rules for concept to speech generation.</title>
<date>1998</date>
<booktitle>In Proc. of COLING/ACL&apos;98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="4006" citStr="Pan and McKeown, 1998" startWordPosition="606" endWordPosition="609">most widely used features in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in large speech corpora. In a previous study (Pan and McKeown, 1998; Pan and McKeown, 1999), we investigated how features such as deep syntactic/semantic structure and word informativeness correlate with accent placement. In this paper, we focus on how local context influences accent patterns. More specifically, we investigate how word collocation influences whether nouns are accented or not. Determining which nouns are accented and which are not is challenging, since part-ofspeech information cannot help here. So, other accent predictors must be found. There are some advantages in looking only at one word class. We eliminate the interaction between part-of-s</context>
</contexts>
<marker>Pan, McKeown, 1998</marker>
<rawString>S. Pan and K. McKeown. 1998. Learning intonation rules for concept to speech generation. In Proc. of COLING/ACL&apos;98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pan</author>
<author>K McKeown</author>
</authors>
<title>Word informativeness and automatic pitch accent modeling.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conference on EMNLP and VLC,</booktitle>
<pages>148--157</pages>
<contexts>
<context position="4030" citStr="Pan and McKeown, 1999" startWordPosition="610" endWordPosition="613">es in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in large speech corpora. In a previous study (Pan and McKeown, 1998; Pan and McKeown, 1999), we investigated how features such as deep syntactic/semantic structure and word informativeness correlate with accent placement. In this paper, we focus on how local context influences accent patterns. More specifically, we investigate how word collocation influences whether nouns are accented or not. Determining which nouns are accented and which are not is challenging, since part-ofspeech information cannot help here. So, other accent predictors must be found. There are some advantages in looking only at one word class. We eliminate the interaction between part-of-speech and collocation, s</context>
<context position="14042" citStr="Pan and McKeown, 1999" startWordPosition="2194" endWordPosition="2198">l to the uncertainty reduction of failing to see one word, having failed to see the other. Since there is considerable evidence that prior discourse context, such as previous mention of a word, affects pitch accent decisions, it is possible that symmetric measures, such as mutual information and the Dice coefficient, may not model accent placement as well as asymmetric measures, such as bigram predictability. Also, the bias of point-wise mutual information toward uncommon words can affect its ability to model accent assignment, since, in general, uncommon words are more likely to be accented (Pan and McKeown, 1999). Since this metric disproportionately raises the mutual information for uncommon words, making them more predictable than their appearance in the corpus warrants, it may predict that uncommon words are more likely to be deaccented than they really are. 4 Statistical Analyses In order to determine whether word collocation is useful for pitch accent prediction, we first employed Spearman&apos;s rank correlation test (Conover, 1980). In this experiment, we employed a unigram predictability-based baseline model. The unigram predictability of a word is defined as the log probability of a word in the te</context>
</contexts>
<marker>Pan, McKeown, 1999</marker>
<rawString>S. Pan and K. McKeown. 1999. Word informativeness and automatic pitch accent modeling. In Proc. of the Joint SIGDAT Conference on EMNLP and VLC, pages 148{157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Silverman</author>
<author>M Beckman</author>
<author>J Pitrelli</author>
<author>M Ostendorf</author>
<author>C Wightman</author>
<author>P Price</author>
<author>J Pierrehumbert</author>
<author>J Hirschberg</author>
</authors>
<title>ToBI: a standard for labeling English prosody.</title>
<date>1992</date>
<booktitle>In Proc. of ICSLP92.</booktitle>
<contexts>
<context position="7052" citStr="Silverman et al., 1992" startWordPosition="1087" endWordPosition="1090">containing twenty segments and totaling fifty minutes, and one read corpus of five segments, read by a single speaker and totaling eleven minutes of speech. The text corpus consists of 3.5 million words from 7375 discharge summaries of patients who had undergone surgery. The speech corpora only cover cardiac patients, while the text corpus covers a larger group of patients and the majority of them have also undergone cardiac surgery. The speech corpora were first transcribed orthographically and then intonationally, using the ToBI convention for prosodic labeling of standard American English (Silverman et al., 1992). For this study, we used only binary accented/deaccented decisions derived from the ToBI tonal tier, in which location and type of pitch accent is marked. After ToBI labeling, each word in the corpora was tagged with part-of-speech, from a nine-element set: noun, verb, adjective, adverb, article, conjunction, pronoun, cardinal, and preposition. The spontaneous corpus was tagged by hand and the read tagged automatically. As noted above, we focus here on predicting whether nouns are accented or not. 3 Collocation Measures We used three measures of word collocation to examine the relationship be</context>
</contexts>
<marker>Silverman, Beckman, Pitrelli, Ostendorf, Wightman, Price, Pierrehumbert, Hirschberg, 1992</marker>
<rawString>K. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf, C. Wightman, P. Price, J. Pierrehumbert, and J. Hirschberg. 1992. ToBI: a standard for labeling English prosody. In Proc. of ICSLP92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>J Hirschberg</author>
<author>D Yarowsky</author>
</authors>
<title>A corpus-based synthesizer.</title>
<date>1992</date>
<booktitle>In Proc. of ICSLP92,</booktitle>
<pages>563--566</pages>
<location>Banff.</location>
<contexts>
<context position="15303" citStr="Sproat et al., 1992" startWordPosition="2400" endWordPosition="2403">on of this measure is: Freq(wi) log T,i Freq(wi) The reason for choosing this as the baseline model is not only because it is context independent, but also because it is effective. In a previous study (Pan and McKeown, 1999), we showed that when this feature is used, it is as powerful a predictor as part-of-speech. When jointly used with part-of-speech information, the combined model can perform significantly better than each individual model. When tested on a similar medical corpus, this combined model also outperforms a comprehensive pitch accent model employed by the Bell Labs&apos; TTS system (Sproat et al., 1992; Hirschberg, 1993; Sproat, 1998), where discourse information, such as given/new, syntactic information, such as POS, and surface information, such as word distance, are incorporated. Since unigram predictability is context independent. By comparing other predictors to this baseline model, we can demonstrate the impact of context, measured by word collocation, on pitch accent assignment. Table 2 shows that for our read speech corpus, unigram predictability, bigram predictability and mutual information are all significantly correlated (p &lt; 0.001) with pitch accent decision.&apos; However, the Dice </context>
</contexts>
<marker>Sproat, Hirschberg, Yarowsky, 1992</marker>
<rawString>R. Sproat, J. Hirschberg, and D. Yarowsky. 1992. A corpus-based synthesizer. In Proc. of ICSLP92, pages 563{566, Banff.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>ed</author>
</authors>
<title>Multilingual Text-to-Speech Synthesis: The Bell Labs Approach.</title>
<date>1998</date>
<publisher>Kluwer.</publisher>
<marker>Sproat, ed, 1998</marker>
<rawString>R. Sproat, ed. 1998. Multilingual Text-to-Speech Synthesis: The Bell Labs Approach. Kluwer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>