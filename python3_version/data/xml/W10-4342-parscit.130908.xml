<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030546">
<title confidence="0.9990955">
Collaborating on Utterances with a Spoken Dialogue System
Using an ISU-based Approach to Incremental Dialogue Management
</title>
<author confidence="0.998997">
Okko Bull, Timo Baumann, David Schlangen
</author>
<affiliation confidence="0.998966">
Department of Linguistics
University of Potsdam, Germany
</affiliation>
<email confidence="0.997988">
{okko|timo|das}@ling.uni-potsdam.de
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935941176471">
When dialogue systems, through the
use of incremental processing, are
not bounded anymore by strict, non-
overlapping turn-taking, a whole range of
additional interactional devices becomes
available. We explore the use of one such
device, trial intonation. We elaborate
our approach to dialogue management
in incremental systems, based on the
Information-State-Update approach, and
discuss an implementation in a micro-
domain that lends itself to the use of
immediate feedback, trial intonations and
expansions. In an overhearer evaluation,
the incremental system was judged as sig-
nificantly more human-like and reactive
than a non-incremental version.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999476846153846">
In human–human dialogue, most utterances have
only one speaker.1 However, the shape that an
utterance ultimately takes on is often determined
not just by the one speaker, but also by her ad-
dressees. A speaker intending to refer to some-
thing may start with a description, monitor while
they go on whether the description appears to be
understood sufficiently well, and if not, possibly
extend it, rather than finishing the utterance in the
form that was initially planned. This monitoring
within the utterance is sometimes even made very
explicit, as in the following example from (Clark,
1996):
</bodyText>
<listItem confidence="0.888711">
(1) A: A man called Annegra? -
</listItem>
<note confidence="0.6494265">
B: yeah, Allegra
A: Allegra, uh, replied and, uh, .. .
</note>
<bodyText confidence="0.998058333333333">
In this example, A makes use of what Sacks and
Schegloff (1979) called a try marker, a “question-
ing upward intonational contour, followed by a
</bodyText>
<footnote confidence="0.981063">
1Though by far not all; see (Clark, 1996; Purver et al.,
2009; Poesio and Rieser, 2010).
</footnote>
<bodyText confidence="0.998987958333333">
brief pause”. As discussed by Clark (1996), this
device is an efficient solution to the problem posed
by uncertainty on the side of the speaker whether
a reference is going to be understood, as it checks
for understanding in situ, and lets the conversation
partners collaborate on the utterance that is in pro-
duction.
Spoken dialogue systems (SDS) typically can-
not achieve the close coupling between produc-
tion and interpretation that is needed for this to
work, as normally the smallest unit on which they
operate is the full utterance (or, more precisely,
the turn). (For a discussion see e.g. (Skantze and
Schlangen, 2009).) We present here an approach
to managing dialogue in an incremental SDS that
can handle this phenomenon, explaining how it is
implemented in system (Section 4) that works in
a micro-domain (which is described in Section 3).
As we will discuss in the next section, this goes be-
yond earlier work on incremental SDS, combining
the production of multimodal feedback (as in (Aist
et al., 2007)) with fast interaction in a semantically
more complex domain (compared to (Skantze and
Schlangen, 2009)).
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999882875">
Collaboration on utterances has not often been
modelled in SDS, as it presupposes fully incre-
mental processing, which itself is still something
of a rarity in such systems. (There is work on
collaborative reference (DeVault et al., 2005; Hee-
man and Hirst, 1995), but that focuses on written
input, and on collaboration over several utterances
and not within utterances.) There are two systems
that are directly relevant here.
The system described in (Aist et al., 2007) is
able to produce some of the phenomena that we
are interested in here. The set-up is a simple
reference game (as we will see, the domain we
have chosen is very similar), where users can re-
fer to objects shown on the screen, and the SDS
gives continuous feedback about its understand-
</bodyText>
<subsubsectionHeader confidence="0.677394">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 233–236,
</subsubsectionHeader>
<affiliation confidence="0.890989">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999611">
233
</page>
<bodyText confidence="0.999971487804878">
ing by performing on-screen actions. While we
do produce similar non-linguistic behaviour in our
system, we also go beyond this by producing
verbal feedback that responds to the certainty of
the speaker (expressed by the use of trial intona-
tion). Unfortunately, very little technical details
are given in that paper, so that we cannot compare
the approaches more fully.
Even more closely related is some of our own
previous work, (Skantze and Schlangen, 2009),
where we modeled fast system reactions to deliv-
ery of information in installments in a number se-
quence dictation domain. In a small corpus study,
we found a very pronounced use of trial or in-
stallment intonations, with the first installments of
numbers being bounded by rising intonation, and
the final installment of a sequence by falling into-
nation. We made use of this fact by letting the sys-
tem distinguish these situations based on prosody,
and giving it different reaction possibilities (back-
channel feedback vs. explicit confirmation).
The work reported here is a direct scaling up of
that work. For number sequences, the notion of
utterance is somewhat vague, as there are no syn-
tactic constraints that help demarcate its bound-
aries. Moreover, there is no semantics (beyond
the individual number) that could pose problems
– the main problem for the speaker in that do-
main is ensuring that the signal is correctly identi-
fied (as in, the string could be written down), and
the trial intonation is meant to provide opportuni-
ties for grounding whether that is the fact. Here,
we want to go beyond that and look at utterances
where it is the intended meaning whose recogni-
tion the speaker is unsure about (grounding at level
3 rather than (just) at level 2 in terms of (Clark,
1996).) This difference leads to differences in the
follow up potential: where in the numbers domain,
typical repair follow-ups were repetitions, in se-
mantically more complex domains we can expect
expansions or reformulations.
</bodyText>
<sectionHeader confidence="0.96323" genericHeader="method">
3 The Puzzle Micro-Domain
</sectionHeader>
<bodyText confidence="0.999413166666667">
To investigate these issues in a controlled set-
ting, we chose a domain that makes complex and
possibly underspecified references likely, and that
also allows a combination of linguistic and non-
linguistic feedback. In this domain, the user’s goal
is to instruct the system to pick up and manipu-
late Tetris-like puzzle pieces, which are shown on
the screen. We recorded human–human as well
as human–(simulated) machine interactions in this
domain, and indeed found frequent use of “pack-
aging” of instructions, and immediate feedback, as
in (2) (arrow indicating intonation).
</bodyText>
<figure confidence="0.5465032">
(2) IG-1: The cross in the corner/ ...
IF-2: erm
IG-3: the red one .. yeah
IF-4: [moves cursor]
IG-5: take that.
</figure>
<bodyText confidence="0.99977525">
We chose these as our target phenomena for the
implementation: intra-utterance hesitations, possi-
bly with trial intonation (as in line 2);2 immediate
execution of actions (line 4), and their grounding
role as display of understanding (“yeah” in line 3).
The system controls the mouse cursor, e.g. moving
it over pieces once it has a good hypothesis about
a reference; other actions are visualised similarly.
</bodyText>
<sectionHeader confidence="0.999409" genericHeader="method">
4 Implementation
</sectionHeader>
<subsectionHeader confidence="0.768326">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999974857142857">
Our system is realised as a collection of incre-
mental processing modules in the InproToolKit
(Schlangen et al., 2010), a middle-ware pack-
age that implements some of the features of the
model of incremental processing of (Schlangen
and Skantze, 2009). The modules used in the im-
plementation will be described briefly below.
</bodyText>
<subsectionHeader confidence="0.913855">
4.2 ASR, Prosody, Floor Tracker &amp; NLU
</subsectionHeader>
<bodyText confidence="0.999992882352941">
For speech recognition, we use Sphinx-4 (Walker
et al., 2004), with our own extensions for incre-
mental speech recognition (Baumann et al., 2009),
and our own domain-specific acoustic model. For
the experiments described here, we used a recog-
nition grammar.
Another module performs online prosodic anal-
ysis, based on pitch change, which is measured in
semi-tone per second over the turn-final word, us-
ing a modified YIN (de Cheveign´e and Kawahara,
2002). Based on the slope of the fo curve, we clas-
sify pitch as rising or falling.
This information is used by the floor track-
ing module, which notifies the dialogue manager
(DM) about changes in floor status. These sta-
tus changes are classified by simple rules: silence
following rising pitch leads to a timeout signal
</bodyText>
<footnote confidence="0.99329">
2Although we chose to label this “intra-utterance” here,
it doesn’t matter much for our approach whether one consid-
ers this example to consist of one or several utterances; what
matters is that differences in intonation and pragmatic com-
pleteness have an effect.
</footnote>
<page confidence="0.995585">
234
</page>
<bodyText confidence="0.999769111111111">
sent to the DM faster (200ms) than silence after
falling pitch (500ms). (Comparable to the rules in
(Skantze and Schlangen, 2009).)
Natural language understanding finally is per-
formed by a unification-based semantic composer,
which builds simple semantic representations out
of the lexical entries for the recognised words; and
a resolver, which matches these representations
against knowledge of the objects in the domain.
</bodyText>
<subsectionHeader confidence="0.999491">
4.3 Dialogue Manager and Action Manager
</subsectionHeader>
<bodyText confidence="0.99959515625">
The DM reacts to input from three sides: semantic
material coming from the NLU, floor state signals
from the floor tracker, and notifications about exe-
cution of actions from the action manager.
The central element of the information state
used in the dialogue manager is what we call the
iQUD (for incremental Question under Discus-
sion, as it’s a variant of the QUD of (Ginzburg,
1996)). Figure 1 gives an example. The iQUD
collects all relevant sub-questions into one struc-
ture, which also records what the relevant non-
linguistic actions are (RNLAs; more on this in a
second, but see also (Buß and Schlangen, 2010),
where we’ve sketched this approach before), and
what the grounding status is of that sub-question.
Let’s go through example (2). The iQUD in
Figure 1 represents the state after the system has
asked “what shall I do now?”. The system an-
ticipates two alternative replies, a take request, or
a delete request; this is what the specification of
the slot value in 1 and 10 in the iQUD indicates.
Now the user starts to speak and produces what is
shown in line 1 in the example. The floor tracker
reacts to the rising pitch and to the silence of ap-
propriate length, and notifies the dialogue man-
ager. In the meantime, the DM has received up-
dates from the NLU module, has checked for each
update whether it is relevant to a sub-question on
the iQUD, and if so, whether it resolves it. In this
situation, the material was relevant to both 4 and
13, but did not resolve it. This is a precondition for
the continuer-questioning rule, which is triggered
by the signal from the floor tracker. The system
then back-channels as in the example, indicating
acoustic understanding (Clark’s level 2), but fail-
ure to operate on the understanding (level 3). (As
an aside, we found that it is far from trivial to find
the right wording for this prompt. We settled on
an “erm” with level pitch.)
The user then indeed produces more material,
which together with the previously given informa-
tion resolves the question. This is where the RN-
LAs come in: when a sub-question is resolved, the
DM looks into the field for RNLAs, and if there
are any, puts them up for execution to the action
manager. In our case, slots 4 and 13 are both
applicable, but as they have compatible RNLAs,
this does not cause a conflict. When the action
has been performed, a new question is accommo-
dated (not shown here), which can be paraphrased
as “was the understanding displayed through this
action correct?”. This is what allows the user reply
in line 3 to be integrated, which otherwise would
need to be ignored, or even worse, would confuse
a dialogue system. A relevant continuation, on the
other hand, would also have resolved the question.
We consider this modelling of grounding effects
of actions an important feature of our approach.
Similar rules handle other floor tracker events;
not elaborated here for reasons of space. In
our current prototype the rules are hard-coded,
but we are preparing a version where rules and
information-states can be specified externally and
are read in by a rule-engine.
</bodyText>
<subsectionHeader confidence="0.996321">
4.4 Overhearer Evaluation
</subsectionHeader>
<bodyText confidence="0.999979">
Evaluating the contribution of one of the many
modules in an SDS is notoriously difficult (Walker
et al., 1998). To be able to focus on evaluation of
the incremental dialogue strategies and avoid in-
terference from ASR problems (and more techni-
cal problems; our system is still somewhat frag-
ile), we opted for an overhearer evaluation. (Such
a setting was also used for the test of the incremen-
tal system of (Aist et al., 2007).)
We implemented a non-incremental version of
the system that does not give non-linguistic feed-
back during user utterances and has only one,
fixed, timeout of 800ms (comparable to typical
settings in commercial dialogue systems). Two
of the authors then recorded 30 minutes of inter-
actions with the two versions of the system.We
then identified and discarded “outlier” interac-
tions, i.e. those with technical problems, or where
</bodyText>
<figure confidence="0.7306895">
{&lt; a ( 1 action=A=take; 2 prepare(A) ; 3 U),
( 4 tile=T ; 5 highlight(T) ; 6 U),
( 7 ; 8 execute(A,T) ; 9 U) &gt;
&lt; b (10 action=A=del ;11 prepare(A) ;12 U),
(13 tile=T ;14 highlight(T) ;15 U),
(16 ;17 execute(A,T) ;18 U) &gt;}
</figure>
<figureCaption confidence="0.995711">
Figure 1: Example iQUD
</figureCaption>
<page confidence="0.993718">
235
</page>
<bodyText confidence="0.99999432">
recognition problems were so severe that a non-
understanding state was entered repeatedly. These
criteria were meant to be fair to both versions
of the system, and indeed we excluded similar
numbers of failed interactions from both versions
(around 10 % of interactions in total).
We measured the length of interactions in the
two sets, and found that the interactions in the in-
cremental setting were significantly shorter (t-test,
p &lt; 0.005). This was to be expected, of course,
as the incremental strategies allow faster reactions
(execution time can be folded into the user utter-
ance); other outcomes would have been possible,
though, if the incremental version had systemati-
cally more understanding problems.
We then had 8 subjects (university students,
not involved in the research) watch and directly
judge (questionnaire, Likert-scale replies to ques-
tions about human-likeness, helpfulness, and re-
activity) 34 randomly selected interactions from
either condition. Human-likeness and reactivity
were judged significantly higher for the incremen-
tal version (Wilcoxon rank-sum test; p &lt; 0.05 and
p &lt; 0.005, respectively), while there was no effect
for helpfulness (p = 0.06).
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.979799">
We described our incremental micro-domain dia-
logue system, which is capable of reacting to sub-
tle signals from the user about expected feedback,
and is able to produce overlapping non-linguistic
actions, modelling their effect as displays of un-
derstanding. Interactions with the system were
judged by overhearers to be more human-like and
reactive than with a non-incremental variant. We
are currently working on extending and generalis-
ing our approach to incremental dialogue manage-
ment, porting it to other domains.
Acknowledgments Funded by an ENP grant from DFG.
</bodyText>
<sectionHeader confidence="0.99781" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999750828571428">
Gregory Aist, James Allen, Ellen Campana, Car-
los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K. Tanenhaus. 2007. Incremental under-
standing in human-computer dialogue and experi-
mental evidence for advantages over nonincremen-
tal methods. In Proceedings of Decalog (Semdial
2007), Trento, Italy.
Timo Baumann, Michaela Atterer, and David
Schlangen. 2009. Assessing and Improving the
Performance of Speech Recognition for Incremental
Systems. In Proceedings of NAACL-HLT 2009,
Boulder, USA.
Okko Buß and David Schlangen. 2010. Modelling
sub-utterance phenomena in spoken dialogue sys-
tems. In Proceedings of Semdial 2010 (“Pozdial”),
pages 33–41, Poznan, Poland, June.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Alain de Cheveign´e and Hideki Kawahara. 2002. YIN,
a fundamental frequency estimator for speech and
music. Journal of the Acoustical Society ofAmerica,
111(4):1917–1930.
David DeVault, Natalia Kariaeva, Anubha Kothari, Iris
Oved, and Matthew Stone. 2005. An information-
state approach to collaborative reference. In Short
Papers, ACL 2005, Michigan, USA, June.
Jonathan Ginzburg. 1996. Interrogatives: Ques-
tions, facts and dialogue. In Shalom Lappin, editor,
The Handbook of Contemporary Semantic Theory.
Blackwell, Oxford.
Peter A. Heeman and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21(3):351–382.
Massimo Poesio and Hannes Rieser. 2010. Comple-
tions, coordination, and alignment in dialogue. Dia-
logue and Discourse, 1(1):1–89.
Matthew Purver, Christine Howes, Eleni Gre-
goromichelaki, and Patrick Healey. 2009. Split
utterances in dialogue: a corpus study. In Proceed-
ings of the SIGDIAL 2009, pages 262–271, London,
UK, September.
Harvey Sacks and Emanuel A. Schegloff. 1979. Two
preferences in the organization of reference to per-
sons in conversation and their interaction. In George
Psathas, editor, Everyday Language: Studies in Eth-
nomethodology, pages 15–21. Irvington Publishers,
Inc., New York, NY, USA.
David Schlangen and Gabriel Skantze. 2009. A gen-
eral, abstract model of incremental dialogue pro-
cessing. In Proceedings of EACL 2009, pages 710–
718, Athens, Greece, March.
David Schlangen, Timo Baumann, Hendrik
Buschmeier, Okko Buß, Stefan Kopp, Gabriel
Skantze, and Ramin Yaghoubzadeh. 2010. Middle-
ware for incremental processing in conversational
agents. In Proceedings of SIGDIAL 2010, Tokyo,
Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of EACL 2009, pages 745–753, Athens,
Greece, March.
Marilyn A. Walker, Diane J. Litman, Candace A.
Kamm, and Alicia Abella. 1998. Evaluating spoken
dialogue agents with PARADISE: Two case studies.
Computer Speech and Language, 12(3).
Willie Walker, Paul Lamere, Philip Kwok, Bhiksha
Raj, Rita Singh, Evandro Gouvea, Peter Wolf, and
Joe Woelfel. 2004. Sphinx-4: A flexible open
source framework for speech recognition. Techni-
cal report, Sun Microsystems Inc.
</reference>
<page confidence="0.998527">
236
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987071">
<title confidence="0.999329">Collaborating on Utterances with a Spoken Dialogue Using an ISU-based Approach to Incremental Dialogue Management</title>
<author confidence="0.994384">Okko Bull</author>
<author confidence="0.994384">Timo Baumann</author>
<author confidence="0.994384">David</author>
<affiliation confidence="0.9996155">Department of University of Potsdam,</affiliation>
<abstract confidence="0.999692277777778">When dialogue systems, through the use of incremental processing, are not bounded anymore by strict, nonoverlapping turn-taking, a whole range of additional interactional devices becomes available. We explore the use of one such device, trial intonation. We elaborate our approach to dialogue management in incremental systems, based on the Information-State-Update approach, and discuss an implementation in a microdomain that lends itself to the use of immediate feedback, trial intonations and expansions. In an overhearer evaluation, the incremental system was judged as significantly more human-like and reactive than a non-incremental version.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gregory Aist</author>
<author>James Allen</author>
<author>Ellen Campana</author>
<author>Carlos Gomez Gallo</author>
<author>Scott Stoness</author>
<author>Mary Swift</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>Incremental understanding in human-computer dialogue and experimental evidence for advantages over nonincremental methods.</title>
<date>2007</date>
<booktitle>In Proceedings of Decalog (Semdial</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="2857" citStr="Aist et al., 2007" startWordPosition="448" endWordPosition="451">en production and interpretation that is needed for this to work, as normally the smallest unit on which they operate is the full utterance (or, more precisely, the turn). (For a discussion see e.g. (Skantze and Schlangen, 2009).) We present here an approach to managing dialogue in an incremental SDS that can handle this phenomenon, explaining how it is implemented in system (Section 4) that works in a micro-domain (which is described in Section 3). As we will discuss in the next section, this goes beyond earlier work on incremental SDS, combining the production of multimodal feedback (as in (Aist et al., 2007)) with fast interaction in a semantically more complex domain (compared to (Skantze and Schlangen, 2009)). 2 Related Work Collaboration on utterances has not often been modelled in SDS, as it presupposes fully incremental processing, which itself is still something of a rarity in such systems. (There is work on collaborative reference (DeVault et al., 2005; Heeman and Hirst, 1995), but that focuses on written input, and on collaboration over several utterances and not within utterances.) There are two systems that are directly relevant here. The system described in (Aist et al., 2007) is able </context>
<context position="12474" citStr="Aist et al., 2007" startWordPosition="2049" endWordPosition="2052">t prototype the rules are hard-coded, but we are preparing a version where rules and information-states can be specified externally and are read in by a rule-engine. 4.4 Overhearer Evaluation Evaluating the contribution of one of the many modules in an SDS is notoriously difficult (Walker et al., 1998). To be able to focus on evaluation of the incremental dialogue strategies and avoid interference from ASR problems (and more technical problems; our system is still somewhat fragile), we opted for an overhearer evaluation. (Such a setting was also used for the test of the incremental system of (Aist et al., 2007).) We implemented a non-incremental version of the system that does not give non-linguistic feedback during user utterances and has only one, fixed, timeout of 800ms (comparable to typical settings in commercial dialogue systems). Two of the authors then recorded 30 minutes of interactions with the two versions of the system.We then identified and discarded “outlier” interactions, i.e. those with technical problems, or where {&lt; a ( 1 action=A=take; 2 prepare(A) ; 3 U), ( 4 tile=T ; 5 highlight(T) ; 6 U), ( 7 ; 8 execute(A,T) ; 9 U) &gt; &lt; b (10 action=A=del ;11 prepare(A) ;12 U), (13 tile=T ;14 h</context>
</contexts>
<marker>Aist, Allen, Campana, Gallo, Stoness, Swift, Tanenhaus, 2007</marker>
<rawString>Gregory Aist, James Allen, Ellen Campana, Carlos Gomez Gallo, Scott Stoness, Mary Swift, and Michael K. Tanenhaus. 2007. Incremental understanding in human-computer dialogue and experimental evidence for advantages over nonincremental methods. In Proceedings of Decalog (Semdial 2007), Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Baumann</author>
<author>Michaela Atterer</author>
<author>David Schlangen</author>
</authors>
<title>Assessing and Improving the Performance of Speech Recognition for Incremental Systems.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT 2009,</booktitle>
<location>Boulder, USA.</location>
<contexts>
<context position="7582" citStr="Baumann et al., 2009" startWordPosition="1220" endWordPosition="1223">e it has a good hypothesis about a reference; other actions are visualised similarly. 4 Implementation 4.1 Overview Our system is realised as a collection of incremental processing modules in the InproToolKit (Schlangen et al., 2010), a middle-ware package that implements some of the features of the model of incremental processing of (Schlangen and Skantze, 2009). The modules used in the implementation will be described briefly below. 4.2 ASR, Prosody, Floor Tracker &amp; NLU For speech recognition, we use Sphinx-4 (Walker et al., 2004), with our own extensions for incremental speech recognition (Baumann et al., 2009), and our own domain-specific acoustic model. For the experiments described here, we used a recognition grammar. Another module performs online prosodic analysis, based on pitch change, which is measured in semi-tone per second over the turn-final word, using a modified YIN (de Cheveign´e and Kawahara, 2002). Based on the slope of the fo curve, we classify pitch as rising or falling. This information is used by the floor tracking module, which notifies the dialogue manager (DM) about changes in floor status. These status changes are classified by simple rules: silence following rising pitch le</context>
</contexts>
<marker>Baumann, Atterer, Schlangen, 2009</marker>
<rawString>Timo Baumann, Michaela Atterer, and David Schlangen. 2009. Assessing and Improving the Performance of Speech Recognition for Incremental Systems. In Proceedings of NAACL-HLT 2009, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okko Buß</author>
<author>David Schlangen</author>
</authors>
<title>Modelling sub-utterance phenomena in spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In Proceedings of Semdial 2010 (“Pozdial”),</booktitle>
<pages>33--41</pages>
<location>Poznan, Poland,</location>
<contexts>
<context position="9552" citStr="Buß and Schlangen, 2010" startWordPosition="1540" endWordPosition="1543"> Manager The DM reacts to input from three sides: semantic material coming from the NLU, floor state signals from the floor tracker, and notifications about execution of actions from the action manager. The central element of the information state used in the dialogue manager is what we call the iQUD (for incremental Question under Discussion, as it’s a variant of the QUD of (Ginzburg, 1996)). Figure 1 gives an example. The iQUD collects all relevant sub-questions into one structure, which also records what the relevant nonlinguistic actions are (RNLAs; more on this in a second, but see also (Buß and Schlangen, 2010), where we’ve sketched this approach before), and what the grounding status is of that sub-question. Let’s go through example (2). The iQUD in Figure 1 represents the state after the system has asked “what shall I do now?”. The system anticipates two alternative replies, a take request, or a delete request; this is what the specification of the slot value in 1 and 10 in the iQUD indicates. Now the user starts to speak and produces what is shown in line 1 in the example. The floor tracker reacts to the rising pitch and to the silence of appropriate length, and notifies the dialogue manager. In </context>
</contexts>
<marker>Buß, Schlangen, 2010</marker>
<rawString>Okko Buß and David Schlangen. 2010. Modelling sub-utterance phenomena in spoken dialogue systems. In Proceedings of Semdial 2010 (“Pozdial”), pages 33–41, Poznan, Poland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1522" citStr="Clark, 1996" startWordPosition="221" endWordPosition="222">on. 1 Introduction In human–human dialogue, most utterances have only one speaker.1 However, the shape that an utterance ultimately takes on is often determined not just by the one speaker, but also by her addressees. A speaker intending to refer to something may start with a description, monitor while they go on whether the description appears to be understood sufficiently well, and if not, possibly extend it, rather than finishing the utterance in the form that was initially planned. This monitoring within the utterance is sometimes even made very explicit, as in the following example from (Clark, 1996): (1) A: A man called Annegra? - B: yeah, Allegra A: Allegra, uh, replied and, uh, .. . In this example, A makes use of what Sacks and Schegloff (1979) called a try marker, a “questioning upward intonational contour, followed by a 1Though by far not all; see (Clark, 1996; Purver et al., 2009; Poesio and Rieser, 2010). brief pause”. As discussed by Clark (1996), this device is an efficient solution to the problem posed by uncertainty on the side of the speaker whether a reference is going to be understood, as it checks for understanding in situ, and lets the conversation partners collaborate on</context>
<context position="5705" citStr="Clark, 1996" startWordPosition="924" endWordPosition="925">ere are no syntactic constraints that help demarcate its boundaries. Moreover, there is no semantics (beyond the individual number) that could pose problems – the main problem for the speaker in that domain is ensuring that the signal is correctly identified (as in, the string could be written down), and the trial intonation is meant to provide opportunities for grounding whether that is the fact. Here, we want to go beyond that and look at utterances where it is the intended meaning whose recognition the speaker is unsure about (grounding at level 3 rather than (just) at level 2 in terms of (Clark, 1996).) This difference leads to differences in the follow up potential: where in the numbers domain, typical repair follow-ups were repetitions, in semantically more complex domains we can expect expansions or reformulations. 3 The Puzzle Micro-Domain To investigate these issues in a controlled setting, we chose a domain that makes complex and possibly underspecified references likely, and that also allows a combination of linguistic and nonlinguistic feedback. In this domain, the user’s goal is to instruct the system to pick up and manipulate Tetris-like puzzle pieces, which are shown on the scre</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alain de Cheveign´e</author>
<author>Hideki Kawahara</author>
</authors>
<title>YIN, a fundamental frequency estimator for speech and music.</title>
<date>2002</date>
<journal>Journal of the Acoustical Society ofAmerica,</journal>
<volume>111</volume>
<issue>4</issue>
<marker>de Cheveign´e, Kawahara, 2002</marker>
<rawString>Alain de Cheveign´e and Hideki Kawahara. 2002. YIN, a fundamental frequency estimator for speech and music. Journal of the Acoustical Society ofAmerica, 111(4):1917–1930.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Natalia Kariaeva</author>
<author>Anubha Kothari</author>
<author>Iris Oved</author>
<author>Matthew Stone</author>
</authors>
<title>An informationstate approach to collaborative reference.</title>
<date>2005</date>
<booktitle>In Short Papers, ACL 2005,</booktitle>
<location>Michigan, USA,</location>
<contexts>
<context position="3215" citStr="DeVault et al., 2005" startWordPosition="504" endWordPosition="507">implemented in system (Section 4) that works in a micro-domain (which is described in Section 3). As we will discuss in the next section, this goes beyond earlier work on incremental SDS, combining the production of multimodal feedback (as in (Aist et al., 2007)) with fast interaction in a semantically more complex domain (compared to (Skantze and Schlangen, 2009)). 2 Related Work Collaboration on utterances has not often been modelled in SDS, as it presupposes fully incremental processing, which itself is still something of a rarity in such systems. (There is work on collaborative reference (DeVault et al., 2005; Heeman and Hirst, 1995), but that focuses on written input, and on collaboration over several utterances and not within utterances.) There are two systems that are directly relevant here. The system described in (Aist et al., 2007) is able to produce some of the phenomena that we are interested in here. The set-up is a simple reference game (as we will see, the domain we have chosen is very similar), where users can refer to objects shown on the screen, and the SDS gives continuous feedback about its understandProceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group</context>
</contexts>
<marker>DeVault, Kariaeva, Kothari, Oved, Stone, 2005</marker>
<rawString>David DeVault, Natalia Kariaeva, Anubha Kothari, Iris Oved, and Matthew Stone. 2005. An informationstate approach to collaborative reference. In Short Papers, ACL 2005, Michigan, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Ginzburg</author>
</authors>
<title>Interrogatives: Questions, facts and dialogue.</title>
<date>1996</date>
<booktitle>The Handbook of Contemporary Semantic Theory.</booktitle>
<editor>In Shalom Lappin, editor,</editor>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="9322" citStr="Ginzburg, 1996" startWordPosition="1503" endWordPosition="1504">ilds simple semantic representations out of the lexical entries for the recognised words; and a resolver, which matches these representations against knowledge of the objects in the domain. 4.3 Dialogue Manager and Action Manager The DM reacts to input from three sides: semantic material coming from the NLU, floor state signals from the floor tracker, and notifications about execution of actions from the action manager. The central element of the information state used in the dialogue manager is what we call the iQUD (for incremental Question under Discussion, as it’s a variant of the QUD of (Ginzburg, 1996)). Figure 1 gives an example. The iQUD collects all relevant sub-questions into one structure, which also records what the relevant nonlinguistic actions are (RNLAs; more on this in a second, but see also (Buß and Schlangen, 2010), where we’ve sketched this approach before), and what the grounding status is of that sub-question. Let’s go through example (2). The iQUD in Figure 1 represents the state after the system has asked “what shall I do now?”. The system anticipates two alternative replies, a take request, or a delete request; this is what the specification of the slot value in 1 and 10 </context>
</contexts>
<marker>Ginzburg, 1996</marker>
<rawString>Jonathan Ginzburg. 1996. Interrogatives: Questions, facts and dialogue. In Shalom Lappin, editor, The Handbook of Contemporary Semantic Theory. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>Graeme Hirst</author>
</authors>
<title>Collaborating on referring expressions.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="3240" citStr="Heeman and Hirst, 1995" startWordPosition="508" endWordPosition="512">(Section 4) that works in a micro-domain (which is described in Section 3). As we will discuss in the next section, this goes beyond earlier work on incremental SDS, combining the production of multimodal feedback (as in (Aist et al., 2007)) with fast interaction in a semantically more complex domain (compared to (Skantze and Schlangen, 2009)). 2 Related Work Collaboration on utterances has not often been modelled in SDS, as it presupposes fully incremental processing, which itself is still something of a rarity in such systems. (There is work on collaborative reference (DeVault et al., 2005; Heeman and Hirst, 1995), but that focuses on written input, and on collaboration over several utterances and not within utterances.) There are two systems that are directly relevant here. The system described in (Aist et al., 2007) is able to produce some of the phenomena that we are interested in here. The set-up is a simple reference game (as we will see, the domain we have chosen is very similar), where users can refer to objects shown on the screen, and the SDS gives continuous feedback about its understandProceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogu</context>
</contexts>
<marker>Heeman, Hirst, 1995</marker>
<rawString>Peter A. Heeman and Graeme Hirst. 1995. Collaborating on referring expressions. Computational Linguistics, 21(3):351–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Hannes Rieser</author>
</authors>
<title>Completions, coordination, and alignment in dialogue.</title>
<date>2010</date>
<booktitle>Dialogue and Discourse,</booktitle>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1840" citStr="Poesio and Rieser, 2010" startWordPosition="278" endWordPosition="281">ey go on whether the description appears to be understood sufficiently well, and if not, possibly extend it, rather than finishing the utterance in the form that was initially planned. This monitoring within the utterance is sometimes even made very explicit, as in the following example from (Clark, 1996): (1) A: A man called Annegra? - B: yeah, Allegra A: Allegra, uh, replied and, uh, .. . In this example, A makes use of what Sacks and Schegloff (1979) called a try marker, a “questioning upward intonational contour, followed by a 1Though by far not all; see (Clark, 1996; Purver et al., 2009; Poesio and Rieser, 2010). brief pause”. As discussed by Clark (1996), this device is an efficient solution to the problem posed by uncertainty on the side of the speaker whether a reference is going to be understood, as it checks for understanding in situ, and lets the conversation partners collaborate on the utterance that is in production. Spoken dialogue systems (SDS) typically cannot achieve the close coupling between production and interpretation that is needed for this to work, as normally the smallest unit on which they operate is the full utterance (or, more precisely, the turn). (For a discussion see e.g. (S</context>
</contexts>
<marker>Poesio, Rieser, 2010</marker>
<rawString>Massimo Poesio and Hannes Rieser. 2010. Completions, coordination, and alignment in dialogue. Dialogue and Discourse, 1(1):1–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Christine Howes</author>
<author>Eleni Gregoromichelaki</author>
<author>Patrick Healey</author>
</authors>
<title>Split utterances in dialogue: a corpus study.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009,</booktitle>
<pages>262--271</pages>
<location>London, UK,</location>
<contexts>
<context position="1814" citStr="Purver et al., 2009" startWordPosition="274" endWordPosition="277">ion, monitor while they go on whether the description appears to be understood sufficiently well, and if not, possibly extend it, rather than finishing the utterance in the form that was initially planned. This monitoring within the utterance is sometimes even made very explicit, as in the following example from (Clark, 1996): (1) A: A man called Annegra? - B: yeah, Allegra A: Allegra, uh, replied and, uh, .. . In this example, A makes use of what Sacks and Schegloff (1979) called a try marker, a “questioning upward intonational contour, followed by a 1Though by far not all; see (Clark, 1996; Purver et al., 2009; Poesio and Rieser, 2010). brief pause”. As discussed by Clark (1996), this device is an efficient solution to the problem posed by uncertainty on the side of the speaker whether a reference is going to be understood, as it checks for understanding in situ, and lets the conversation partners collaborate on the utterance that is in production. Spoken dialogue systems (SDS) typically cannot achieve the close coupling between production and interpretation that is needed for this to work, as normally the smallest unit on which they operate is the full utterance (or, more precisely, the turn). (Fo</context>
</contexts>
<marker>Purver, Howes, Gregoromichelaki, Healey, 2009</marker>
<rawString>Matthew Purver, Christine Howes, Eleni Gregoromichelaki, and Patrick Healey. 2009. Split utterances in dialogue: a corpus study. In Proceedings of the SIGDIAL 2009, pages 262–271, London, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
<author>Emanuel A Schegloff</author>
</authors>
<title>Two preferences in the organization of reference to persons in conversation and their interaction.</title>
<date>1979</date>
<booktitle>Everyday Language: Studies in Ethnomethodology,</booktitle>
<pages>15--21</pages>
<editor>In George Psathas, editor,</editor>
<publisher>Irvington Publishers, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1673" citStr="Sacks and Schegloff (1979)" startWordPosition="249" endWordPosition="252"> on is often determined not just by the one speaker, but also by her addressees. A speaker intending to refer to something may start with a description, monitor while they go on whether the description appears to be understood sufficiently well, and if not, possibly extend it, rather than finishing the utterance in the form that was initially planned. This monitoring within the utterance is sometimes even made very explicit, as in the following example from (Clark, 1996): (1) A: A man called Annegra? - B: yeah, Allegra A: Allegra, uh, replied and, uh, .. . In this example, A makes use of what Sacks and Schegloff (1979) called a try marker, a “questioning upward intonational contour, followed by a 1Though by far not all; see (Clark, 1996; Purver et al., 2009; Poesio and Rieser, 2010). brief pause”. As discussed by Clark (1996), this device is an efficient solution to the problem posed by uncertainty on the side of the speaker whether a reference is going to be understood, as it checks for understanding in situ, and lets the conversation partners collaborate on the utterance that is in production. Spoken dialogue systems (SDS) typically cannot achieve the close coupling between production and interpretation t</context>
</contexts>
<marker>Sacks, Schegloff, 1979</marker>
<rawString>Harvey Sacks and Emanuel A. Schegloff. 1979. Two preferences in the organization of reference to persons in conversation and their interaction. In George Psathas, editor, Everyday Language: Studies in Ethnomethodology, pages 15–21. Irvington Publishers, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Gabriel Skantze</author>
</authors>
<title>A general, abstract model of incremental dialogue processing.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL 2009,</booktitle>
<pages>710--718</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="7326" citStr="Schlangen and Skantze, 2009" startWordPosition="1178" endWordPosition="1181">on: intra-utterance hesitations, possibly with trial intonation (as in line 2);2 immediate execution of actions (line 4), and their grounding role as display of understanding (“yeah” in line 3). The system controls the mouse cursor, e.g. moving it over pieces once it has a good hypothesis about a reference; other actions are visualised similarly. 4 Implementation 4.1 Overview Our system is realised as a collection of incremental processing modules in the InproToolKit (Schlangen et al., 2010), a middle-ware package that implements some of the features of the model of incremental processing of (Schlangen and Skantze, 2009). The modules used in the implementation will be described briefly below. 4.2 ASR, Prosody, Floor Tracker &amp; NLU For speech recognition, we use Sphinx-4 (Walker et al., 2004), with our own extensions for incremental speech recognition (Baumann et al., 2009), and our own domain-specific acoustic model. For the experiments described here, we used a recognition grammar. Another module performs online prosodic analysis, based on pitch change, which is measured in semi-tone per second over the turn-final word, using a modified YIN (de Cheveign´e and Kawahara, 2002). Based on the slope of the fo curv</context>
</contexts>
<marker>Schlangen, Skantze, 2009</marker>
<rawString>David Schlangen and Gabriel Skantze. 2009. A general, abstract model of incremental dialogue processing. In Proceedings of EACL 2009, pages 710– 718, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Timo Baumann</author>
<author>Hendrik Buschmeier</author>
<author>Okko Buß</author>
<author>Stefan Kopp</author>
<author>Gabriel Skantze</author>
<author>Ramin Yaghoubzadeh</author>
</authors>
<title>Middleware for incremental processing in conversational agents.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGDIAL 2010,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="7194" citStr="Schlangen et al., 2010" startWordPosition="1157" endWordPosition="1160">erm IG-3: the red one .. yeah IF-4: [moves cursor] IG-5: take that. We chose these as our target phenomena for the implementation: intra-utterance hesitations, possibly with trial intonation (as in line 2);2 immediate execution of actions (line 4), and their grounding role as display of understanding (“yeah” in line 3). The system controls the mouse cursor, e.g. moving it over pieces once it has a good hypothesis about a reference; other actions are visualised similarly. 4 Implementation 4.1 Overview Our system is realised as a collection of incremental processing modules in the InproToolKit (Schlangen et al., 2010), a middle-ware package that implements some of the features of the model of incremental processing of (Schlangen and Skantze, 2009). The modules used in the implementation will be described briefly below. 4.2 ASR, Prosody, Floor Tracker &amp; NLU For speech recognition, we use Sphinx-4 (Walker et al., 2004), with our own extensions for incremental speech recognition (Baumann et al., 2009), and our own domain-specific acoustic model. For the experiments described here, we used a recognition grammar. Another module performs online prosodic analysis, based on pitch change, which is measured in semi-</context>
</contexts>
<marker>Schlangen, Baumann, Buschmeier, Buß, Kopp, Skantze, Yaghoubzadeh, 2010</marker>
<rawString>David Schlangen, Timo Baumann, Hendrik Buschmeier, Okko Buß, Stefan Kopp, Gabriel Skantze, and Ramin Yaghoubzadeh. 2010. Middleware for incremental processing in conversational agents. In Proceedings of SIGDIAL 2010, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>David Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL 2009,</booktitle>
<pages>745--753</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="2467" citStr="Skantze and Schlangen, 2009" startWordPosition="382" endWordPosition="385">). brief pause”. As discussed by Clark (1996), this device is an efficient solution to the problem posed by uncertainty on the side of the speaker whether a reference is going to be understood, as it checks for understanding in situ, and lets the conversation partners collaborate on the utterance that is in production. Spoken dialogue systems (SDS) typically cannot achieve the close coupling between production and interpretation that is needed for this to work, as normally the smallest unit on which they operate is the full utterance (or, more precisely, the turn). (For a discussion see e.g. (Skantze and Schlangen, 2009).) We present here an approach to managing dialogue in an incremental SDS that can handle this phenomenon, explaining how it is implemented in system (Section 4) that works in a micro-domain (which is described in Section 3). As we will discuss in the next section, this goes beyond earlier work on incremental SDS, combining the production of multimodal feedback (as in (Aist et al., 2007)) with fast interaction in a semantically more complex domain (compared to (Skantze and Schlangen, 2009)). 2 Related Work Collaboration on utterances has not often been modelled in SDS, as it presupposes fully </context>
<context position="4418" citStr="Skantze and Schlangen, 2009" startWordPosition="701" endWordPosition="704">he Special Interest Group on Discourse and Dialogue, pages 233–236, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 233 ing by performing on-screen actions. While we do produce similar non-linguistic behaviour in our system, we also go beyond this by producing verbal feedback that responds to the certainty of the speaker (expressed by the use of trial intonation). Unfortunately, very little technical details are given in that paper, so that we cannot compare the approaches more fully. Even more closely related is some of our own previous work, (Skantze and Schlangen, 2009), where we modeled fast system reactions to delivery of information in installments in a number sequence dictation domain. In a small corpus study, we found a very pronounced use of trial or installment intonations, with the first installments of numbers being bounded by rising intonation, and the final installment of a sequence by falling intonation. We made use of this fact by letting the system distinguish these situations based on prosody, and giving it different reaction possibilities (backchannel feedback vs. explicit confirmation). The work reported here is a direct scaling up of that w</context>
<context position="8602" citStr="Skantze and Schlangen, 2009" startWordPosition="1388" endWordPosition="1391">information is used by the floor tracking module, which notifies the dialogue manager (DM) about changes in floor status. These status changes are classified by simple rules: silence following rising pitch leads to a timeout signal 2Although we chose to label this “intra-utterance” here, it doesn’t matter much for our approach whether one considers this example to consist of one or several utterances; what matters is that differences in intonation and pragmatic completeness have an effect. 234 sent to the DM faster (200ms) than silence after falling pitch (500ms). (Comparable to the rules in (Skantze and Schlangen, 2009).) Natural language understanding finally is performed by a unification-based semantic composer, which builds simple semantic representations out of the lexical entries for the recognised words; and a resolver, which matches these representations against knowledge of the objects in the domain. 4.3 Dialogue Manager and Action Manager The DM reacts to input from three sides: semantic material coming from the NLU, floor state signals from the floor tracker, and notifications about execution of actions from the action manager. The central element of the information state used in the dialogue manag</context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>Gabriel Skantze and David Schlangen. 2009. Incremental dialogue processing in a micro-domain. In Proceedings of EACL 2009, pages 745–753, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>Evaluating spoken dialogue agents with PARADISE: Two case studies.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="12159" citStr="Walker et al., 1998" startWordPosition="1993" endWordPosition="1996">would confuse a dialogue system. A relevant continuation, on the other hand, would also have resolved the question. We consider this modelling of grounding effects of actions an important feature of our approach. Similar rules handle other floor tracker events; not elaborated here for reasons of space. In our current prototype the rules are hard-coded, but we are preparing a version where rules and information-states can be specified externally and are read in by a rule-engine. 4.4 Overhearer Evaluation Evaluating the contribution of one of the many modules in an SDS is notoriously difficult (Walker et al., 1998). To be able to focus on evaluation of the incremental dialogue strategies and avoid interference from ASR problems (and more technical problems; our system is still somewhat fragile), we opted for an overhearer evaluation. (Such a setting was also used for the test of the incremental system of (Aist et al., 2007).) We implemented a non-incremental version of the system that does not give non-linguistic feedback during user utterances and has only one, fixed, timeout of 800ms (comparable to typical settings in commercial dialogue systems). Two of the authors then recorded 30 minutes of interac</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1998</marker>
<rawString>Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1998. Evaluating spoken dialogue agents with PARADISE: Two case studies. Computer Speech and Language, 12(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willie Walker</author>
<author>Paul Lamere</author>
<author>Philip Kwok</author>
<author>Bhiksha Raj</author>
<author>Rita Singh</author>
<author>Evandro Gouvea</author>
<author>Peter Wolf</author>
<author>Joe Woelfel</author>
</authors>
<title>Sphinx-4: A flexible open source framework for speech recognition.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>Sun Microsystems Inc.</institution>
<contexts>
<context position="7499" citStr="Walker et al., 2004" startWordPosition="1207" endWordPosition="1210">” in line 3). The system controls the mouse cursor, e.g. moving it over pieces once it has a good hypothesis about a reference; other actions are visualised similarly. 4 Implementation 4.1 Overview Our system is realised as a collection of incremental processing modules in the InproToolKit (Schlangen et al., 2010), a middle-ware package that implements some of the features of the model of incremental processing of (Schlangen and Skantze, 2009). The modules used in the implementation will be described briefly below. 4.2 ASR, Prosody, Floor Tracker &amp; NLU For speech recognition, we use Sphinx-4 (Walker et al., 2004), with our own extensions for incremental speech recognition (Baumann et al., 2009), and our own domain-specific acoustic model. For the experiments described here, we used a recognition grammar. Another module performs online prosodic analysis, based on pitch change, which is measured in semi-tone per second over the turn-final word, using a modified YIN (de Cheveign´e and Kawahara, 2002). Based on the slope of the fo curve, we classify pitch as rising or falling. This information is used by the floor tracking module, which notifies the dialogue manager (DM) about changes in floor status. The</context>
</contexts>
<marker>Walker, Lamere, Kwok, Raj, Singh, Gouvea, Wolf, Woelfel, 2004</marker>
<rawString>Willie Walker, Paul Lamere, Philip Kwok, Bhiksha Raj, Rita Singh, Evandro Gouvea, Peter Wolf, and Joe Woelfel. 2004. Sphinx-4: A flexible open source framework for speech recognition. Technical report, Sun Microsystems Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>