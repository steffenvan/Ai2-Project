<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.996622">
Implications for Generating Clarification Requests in Task-oriented
Dialogues
</title>
<author confidence="0.996723">
Verena Rieser
</author>
<affiliation confidence="0.9955905">
Department of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.858658">
Saarbr¨ucken, D-66041
</address>
<email confidence="0.997101">
vrieser@coli.uni-sb.de
</email>
<author confidence="0.992349">
Johanna D. Moore
</author>
<affiliation confidence="0.997677">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.960531">
Edinburgh, EH8 9LW, GB
</address>
<email confidence="0.999423">
J.Moore@ed.ac.uk
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903647058824">
Clarification requests (CRs) in conversa-
tion ensure and maintain mutual under-
standing and thus play a crucial role in
robust dialogue interaction. In this pa-
per, we describe a corpus study of CRs
in task-oriented dialogue and compare our
findings to those reported in two prior
studies. We find that CR behavior in
task-oriented dialogue differs significantly
from that in everyday conversation in a
number of ways. Moreover, the dialogue
type, the modality and the channel qual-
ity all influence the decision of when to
clarify and at which level of the ground-
ing process. Finally we identify form-
function correlations which can inform the
generation of CRs.
</bodyText>
<sectionHeader confidence="0.999126" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.966429416666667">
Clarification requests in conversation ensure and
maintain mutual understanding and thus play a sig-
nificant role in robust and efficient dialogue interac-
tion. From a theoretical perspective, the model of
grounding explains how mutual understanding is es-
tablished. According to Clark (1996), speakers and
listeners ground mutual understanding on four lev-
els of coordination in an action ladder, as shown in
Table 1.
Several current research dialogue systems can de-
tect errors on different levels of grounding (Paek
and Horvitz, 2000; Larsson, 2002; Purver, 2004;
</bodyText>
<table confidence="0.99040125">
Level Speaker S Listener L
Convers. S is proposing activity L is considering pro-
α posal α
Intention S is signalling that p L is recognizing that p
Signal S is presenting signal v L is identifying signal
v
Channel S is executing behavior L is attending to behav-
,Q ior ,Q
</table>
<tableCaption confidence="0.999849">
Table 1: Four levels of grounding
</tableCaption>
<bodyText confidence="0.998583208333333">
Schlangen, 2004). However, only the work of
Purver (2004) addresses the question of how the
source of the error affects the form the CR takes.
In this paper, we investigate the use of form-
function mappings derived from human-human di-
alogues to inform the generation of CRs. We iden-
tify the factors that determine which function a CR
should take and identify function-form correlations
that can be used to guide the automatic generation
of CRs.
In Section 2, we discuss the classification
schemes used in two recent corpus studies of CRs
in human-human dialogue, and assess their applica-
bility to the problem of generating CRs. Section 3
describes the results we obtained by applying the
classification scheme of Rodriguez and Schlangen
(2004) to the Communicator Corpus (Bennett and
Rudnicky, 2002). Section 4 draws general conclu-
sions for generating CRs by comparing our results
to those of (Purver et al., 2003) and (Rodriguez and
Schlangen, 2004). Section 5 describes the correla-
tions between function and form features that are
present in the corpus and their implications for gen-
erating CRs.
</bodyText>
<page confidence="0.978027">
239
</page>
<note confidence="0.9931395">
Proceedings of the 43rd Annual Meeting of the ACL, pages 239–246,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<table confidence="0.993386066666667">
Attr. Value Category Example
form non Non-Reprise “What did you say?”
wot Conventional “Sorry?”
frg Reprise Fragment “Edinburgh?”
lit Literal Reprise “You want a flight to Edinburgh?”
slu Reprise Sluice “Where?”
sub Wh-substituted Reprise “You want a flight where?”
gap Gap “You want a flight to...?”
fil Gap Filler “...Edinburgh?”
other Other x
readings cla Clausal “Are you asking/asserting that X?”
con Constituent “What do you mean by X?”
lex Lexical “Did you utter X?”
corr Correction “Did you intend to utterX instead?”
other Other x
</table>
<tableCaption confidence="0.997175">
Table 2: CR classification scheme by PGH
</tableCaption>
<sectionHeader confidence="0.977978" genericHeader="method">
2 CR Classification Schemes
</sectionHeader>
<bodyText confidence="0.999902">
We now discuss two recently proposed classifica-
tion schemes for CRs, and assess their usefulness for
generating CRs in a spoken dialogue system (SDS).
</bodyText>
<subsectionHeader confidence="0.998805">
2.1 Purver, Ginzburg and Healey (PGH)
</subsectionHeader>
<bodyText confidence="0.998456">
Purver, Ginzburg and Healey (2003) investigated
CRs in the British National Corpus (BNC) (Burnard,
2000). In their annotation scheme, a CR can take
seven distinct surface forms and four readings, as
shown in Table 2. The examples for the form feature
are possible CRs following the statement “I want a
flight to Edinburgh”. The focus of this classification
scheme is to map semantic readings to syntactic sur-
face forms. The form feature is defined by its rela-
tion to the problematic utterance, i.e., whether a CR
reprises the antecedent utterance and to what extent.
CRs may take the three different readings as defined
by Ginzburg and Cooper (2001), as well as a fourth
reading which indicates a correction.
Although PGH report good coverage of the
scheme on their subcorpus of the BNC (99%), we
found their classification scheme to to be too coarse-
grained to prescribe the form that a CR should take.
As shown in example 1, Reprise Fragments (RFs),
which make up one third of the BNC, are ambigu-
ous in their readings and may also take several sur-
face forms.
</bodyText>
<listItem confidence="0.919606272727273">
(1) I would like to book a flight on Monday.
(a) Monday?
frg, con/cla
(b) Which Monday?
frg, con
(c) Monday the first?
frg, con
(d) The first of May?
frg, con
(e) Monday the first or Monday the eighth?
frg, (exclusive) con
</listItem>
<bodyText confidence="0.999643454545455">
RFs endorse literal repetitions of part of the prob-
lematic utterance (1.a); repetitions with an addi-
tional question word (1.b); repetition with further
specification (1.c); reformulations (1.d); and alter-
native questions (1.e)1.
In addition to being too general to describe such
differences, the classification scheme also fails to
describe similarities. As noted by (Rodriguez and
Schlangen, 2004), PGH provide no feature to de-
scribe the extent to which an RF repeats the prob-
lematic utterance.
Finally, some phenomena cannot be described at
all by the four readings. For example, the readings
do not account for non-understanding on the prag-
matic level. Furthermore the readings may have sev-
eral problem sources: the clausal reading may be
appropriate where the CR initiator failed to recog-
nise the word acoustically as well as when he failed
to resolve the reference. Since we are interested in
generating CRs that indicate the source of the error,
we need a classification scheme that represents such
information.
</bodyText>
<subsectionHeader confidence="0.999238">
2.2 Rodriguez and Schlangen (R&amp;S)
</subsectionHeader>
<bodyText confidence="0.9955595">
Rodriguez and Schlangen (2004) devised a multi-
dimensional classification scheme where form and
</bodyText>
<footnote confidence="0.9871295">
1Alternative questions would be interpreted as asking apolar
question with an exclusive reading.
</footnote>
<page confidence="0.996792">
240
</page>
<bodyText confidence="0.999960347826087">
function are meta-features taking sub-features as at-
tributes. The function feature breaks down into
the sub-features source, severity, extent, reply and
satisfaction. The sources that might have caused
the problem map to the levels as defined by Clark
(1996). These sources can also be of different
severity. The severity can be interpreted as de-
scribing the set of possible referents: asking for
repetition indicates that no interpretation is avail-
able (cont-rep); asking for confirmation means
that the CR initiator has some kind of hypothesis
(cont-conf). The extent of a problem describes
whether the CR points out a problematic element in
the problem utterance. The reply represents the an-
swer the addressee gives to the CR. The satisfaction
of the CR-initiator is indicated by whether he renews
the request for clarification or not.
The meta-feature form describes how the CR is
lingustically realised. It describes the sentence’s
mood, whether it is grammatically complete, the re-
lation to the antecedent, and the boundary tone. Ac-
cording to R&amp;S’s classification scheme our illustra-
tive example would be annotated as follows2:
</bodyText>
<listItem confidence="0.6325988">
(2) I would like to book a flight on Monday.
(a) Monday?
mood: decl
completeness: partial
rel-antecedent: repet
source: acous/np-ref
severity: cont-repet
extent: yes
(b) Which Monday?
mood: wh-question
completeness: partial
rel-antecedent: addition
source: np-ref
severity: cont-repet
extent: yes
(c) Monday the first?
mood: decl
completeness: partial
rel-antecedent: addition
source: np-ref
severity: cont-conf
extent: yes
(d) The first of May?
mood: decl
completeness: partial
2The source features answer and satisfaction are ignored as
they depend on how the dialogue continues. The interpretation
of the source is dependent on the reply to the CR. Therefore all
possible interpretations are listed.
rel-antecedent: reformul
source: np-ref
severity: cont-conf
extent: yes
(d) Monday the first or Monday the eighth?
mood: alt-q
completeness: partial
rel-antecedent: addition
source: np-ref
severity: cont-repet
extent: yes
</listItem>
<bodyText confidence="0.998823">
In R&amp;S’s classification scheme, ambiguities
about CRs having different sources cannot be re-
solved entirely as example (2.a) shows. However,
in contrast to PGH, the overall approach is a differ-
ent one: instead of explaining causes of CRs within
a theoretic-semantic model (as the three different
readings of Ginzburg and Cooper (2001) do), they
infer the interpretation of the CR from the context.
Ambiguities get resolved by the reply of the ad-
dressee and the satisfaction of the CR initiator in-
dicates the “mutually agreed interpretation” .
R&amp;S’s multi-dimensional CR description allows
the fine-grained distinctions needed to generate nat-
ural CRs to be made. For example, PGH’s general
category of RFs can be made more specific via the
values for the feature relation to antecedent. In ad-
dition, the form feature is not restricted to syntax; it
includes features such as intonation and coherence,
which are useful for generating the surface form of
CRs. Furthermore, the multi-dimensional function
feature allows us to describe information relevant to
generating CRs that is typically available in dialogue
systems, such as the level of confidence in the hy-
pothesis and the problem source.
</bodyText>
<sectionHeader confidence="0.727521" genericHeader="method">
3 CRs in the Communicator Corpus
</sectionHeader>
<subsectionHeader confidence="0.994785">
3.1 Material and Method
</subsectionHeader>
<bodyText confidence="0.999296">
Material: We annotated the human-human travel
reservation dialogues available as part of the
Carnegie Mellon Communicator Corpus (Bennett
and Rudnicky, 2002) because we were interested
in studying naturally occurring CRs in task-oriented
dialogue. In these dialogues, an experienced travel
agent is making reservations for trips that people in
the Carnegie Mellon Speech Group were taking in
the upcoming months. The corpus comprises 31 di-
alogues of transcribed telephone speech, with 2098
dialogue turns and 19395 words.
</bodyText>
<page confidence="0.876619">
241
</page>
<equation confidence="0.959372631578947">
⎡ ⎡ ⎤
distance-src:{1  |2  |3  |4  |5  |more� ⎤
⎢ mood:{ none  |decl  |polar-q  |wh-q  |alt-q  |imp  |other� ⎥ ⎥
⎢ ⎥ ⎥
⎢ ⎥ ⎥
form: ⎢form:lnone  |particle  |partial  |complete ⎥ ⎥
⎢ ⎣relation-antecedent:Inone  |add  |repet  |repet-add  |reformul  |indep� ⎥ ⎥
⎦ ⎥ ⎥
boundary-tone:�none  |rising  |falling  |no-appl� ⎥ ⎥
⎡source:nnone  |acous  |lex  |parsing  |np-ref  |deitic-ref  |act-ref
int+eval  |relevance  |belief  |ambiguity  |scr-several
⎢ ⎥ ⎥
⎢ ⎥
⎢ extent:�none  |fragment  |whole ⎥ ⎥ ⎥
⎢ ⎥
function: ⎢ severity:�none  |cont-conf  |cont-rep  |cont-disamb  |no-reactI ⎥ ⎥ ⎥
⎢ answer: nnone  |ans-repet  |ans-y/n  |ans-reformul  |ans-elab |o ⎦
ans-w-defin  |no-react ⎦
satisfaction:�none |happy-yes  |happy-no  |happy-ambigI
</equation>
<figureCaption confidence="0.996945">
Figure 1: CR classification scheme
</figureCaption>
<bodyText confidence="0.999562866666667">
Annotation Scheme: Our annotation scheme,
shown in Figure 1, is an extention of the R&amp;S
scheme described in the previous section. R&amp;S’s
scheme was devised for and tested on the Bielefeld
Corpus of German task-oriented dialogues about
joint problem solving.3 To annotate the Commu-
nicator Corpus we extended the scheme in the fol-
lowing ways. First, we found the need to distin-
guish CRs that consist only of newly added infor-
mation, as in example 3, from those that add in-
formation while also repeating part of the utterance
to be clarified, as in 4. We augmented the scheme
to allow two distinct values for the form feature
relation-antecedent, add for cases like 3
and repet-add for cases like 4.
</bodyText>
<listItem confidence="0.97460425">
(3) Cust: What is the last flight I could come back on?
Agent: On the 29th of March?
(4) Cust: I’ll be returning on Thursday the fifth.
Agent: The fifth of February?
</listItem>
<bodyText confidence="0.590763666666667">
To the function feature source we added the val-
ues belief to cover CRs like 5 and ambiguity
refinement to cover CRs like 6.
</bodyText>
<listItem confidence="0.993910166666667">
(5) Agent: You need a visa.
Cust: I do need one?
Agent: Yes you do.
(6) Agent: Okay I have two options ... with Hertz ... if not
they do have a lower rate with Budget and that is
fifty one dollars.
</listItem>
<bodyText confidence="0.74034275">
Cust: Per day?
Agent: Per day um mm.
Finally, following Gabsdil (2003) we introduced
an additional value for severity, cont-disamb, to
</bodyText>
<footnote confidence="0.806024">
3http://sfb360.uni-bielefeld.de
</footnote>
<bodyText confidence="0.991775153846154">
cover CRs that request disambiguation when more
than one interpretation is available.
Method: We first identified turns containing CRs,
and then annotated them with form and function fea-
tures. It is not always possible to identify CRs from
the utterance alone. Frequently, context (e.g., the
reaction of the addressee) or intonation is required
to distinguish a CR from other feedback strategies,
such as positive feedback. See (Rieser, 2004) for a
detailed discussion. The annotation was only per-
formed once. The coding scheme is a slight varia-
tion of R&amp;S, which has been shown relaiable with
Kappa of 0.7 for identifying source.
</bodyText>
<subsectionHeader confidence="0.9770435">
3.2 Forms and Functions of CRs in the
Communicator Corpus
</subsectionHeader>
<bodyText confidence="0.999988266666667">
The human-human dialogues in the Communica-
tor Corpus contain 98 CRs in 2098 dialogue turns
(4.6%).
Forms: The frequencies for the values of the
individual form features are shown in Table 3.
The most frequent type of CRs were partial
declarative questions, which combine the mood
value declarative and the completeness value
partial.4 These account for 53.1% of the CRs
in the corpus. Moreover, four of the five most
frequent surface forms of CRs in the Communi-
cator Corpus differ only in the value for the fea-
ture relation-antecedent. They are partial
declaratives with rising boundary tone, that either re-
formulate (7.1%) the problematic utterance, repeat
</bodyText>
<footnote confidence="0.976899666666667">
4Declarative questions cover “all cases of non-interrogative
word-order, i.e., both declarative sentences and fragments” (Ro-
driguez and Schlangen, 2004).
</footnote>
<page confidence="0.958125">
242
</page>
<table confidence="0.999579294117647">
Feature Value Freq. (%)
Mood declarative 65
polar 21
wh-question 7
other 7
Completeness partial 58
complete 38
other 4
Relation antecedent rep-add 27
independent 21
reformulation 19
repetition 18
addition 10
other 5
Boundary tone rising 74
falling 22
other 4
</table>
<tableCaption confidence="0.976655">
Table 3: Distribution of values for the form features
</tableCaption>
<table confidence="0.998719263157895">
Feature Value Freq.(%)
Source np-reference 40
acoustic 31
intention 8
belief 6
ambiguity 4
contact 4
others 3
relevance 2
several 2
Extent yes 80
no 20
Severity confirmation 73
repetition 20
other 7
Answer y/n answer 64
other 15
elaboration 13
no reaction 6
</table>
<tableCaption confidence="0.9327255">
Table 4: Distribution of values for the function fea-
tures
</tableCaption>
<bodyText confidence="0.999918578947368">
the problematic constituent (11.2%), add only new
information (7.1%), or repeat the problematic con-
stituent and add new information (10.2%). The fifth
most frequent type is conventional CRs (10.2%).5
Functions: The distributions of the function fea-
tures are given in Figure 4. The most frequent source
of problems was np-reference. Next most frequent
were acoustic problems, possibly due to the poor
channel quality. Third were CRs that enquire about
intention. As indicated by the feature extent, al-
most 80% of CRs point out a specific element of
the problematic utterance. The features severity and
answer illustrate that most of the time CRs request
confirmation of an hypothesis (73.5%) with a yes-
no-answer (64.3%). The majority of the provided
answers were satisfying, which means that the ad-
dressee tends to interpret the CR correctly and an-
swers collaboratively. Only 6.1% of CRs failed to
elicit a response.
</bodyText>
<sectionHeader confidence="0.979818" genericHeader="method">
4 CRs in Task-oriented Dialogue
</sectionHeader>
<subsectionHeader confidence="0.946975">
4.1 Comparison
</subsectionHeader>
<bodyText confidence="0.967099666666667">
In order to determine whether there are differences
as regards CRs between task-oriented dialogues and
everyday conversations, we compared our results to
those of PGH’s study on the BNC and those of R&amp;S
5Conventional forms are “Excuse me?”, “Pardon?”, etc.
on the Bielefeld Corpus. The BNC contains a 10
million word sub-corpus of English dialogue tran-
scriptions about topics of general interest. PGH
analysed a portion consisting of ca. 10,600 turns,
ca. 150,000 words. R&amp;S annotated 22 dialogues
from the Bielefeld Corpus, consisting of ca. 3962
turns, ca. 36,000 words.
The major differences in the feature distributions
are listed in Table 5. We found that there are no
significant differences between the feature distri-
butions for the Communicator and Bielefeld cor-
pora, but that the differences between Communica-
tor and BNC, and Bielefeld and BNC are significant
at the levels indicated in Table 5 using Pearson’s
x2. The differences between dialogues of differ-
ent types suggest that there is a different grounding
strategy. In task-oriented dialogues we see a trade-
off between avoiding misunderstanding and keeping
the conversation as efficient as possible. The hy-
pothesis that grounding in task-oriented dialogues is
more cautious is supported by the following facts (as
shown by the figures in Table 5):
</bodyText>
<listItem confidence="0.98622575">
• CRs are more frequent in task-oriented dia-
logues.
• The overwhelming majority of CRs directly
follow the problematic utterance.
</listItem>
<page confidence="0.994336">
243
</page>
<table confidence="0.998750090909091">
Corpus
Feature Communicator Bielefeld BNC
CRs 98 230 418
frequency 4.6% 5.8%*** 3.9%
distance-src=1 92.8%* 94.8%*** 84.4%
no-react 6.1%* 8.7%** 17.0%
cont-conf 73.5%*** 61.7%*** 46.6%
partial 58.2%** 76.5%*** 42.4%
independent 21.4%*** 9.6%*** 44.2%
cont-rep 19.8%*** 14.8%*** 39.5%
y/n-answer 64.3% 44.8% n/a
</table>
<tableCaption confidence="0.991059">
Table 5: Comparison of CR forms in everyday vs. task-
oriented corpora (* denotes P &lt; .05, ** is P &lt; .01, *** is
</tableCaption>
<table confidence="0.983600142857143">
P &lt; .005.)
Feature Corpus
Communicator Bielefeld Significance
contact 4.1% 0 inst n/a
acoustic 30.6% 11.7% ***
lexical 1 inst 1 inst n/a
parsing 1 inst 0 inst n/a
np-ref 39.8% 24.4% **
deict-ref 1 inst 27.4% ***
ambiguity 4.1% not eval. n/a
belief 6.1% not eval. n/a
relevance 2.1% not eval. n/a
intention 8.2% 22.2% **
several 2.0% 14.3% ***
</table>
<tableCaption confidence="0.984742">
Table 6: Comparison of CR problem sources in task-oriented
corpora
</tableCaption>
<listItem confidence="0.9774126">
• CRs in everyday conversation fail to elicit a re-
sponse nearly three times as often.6
• Even though dialogue participants seem to
have strong hypotheses, they frequently con-
firm them.
</listItem>
<bodyText confidence="0.908296">
Although grounding is more cautious in task-
oriented dialogues, the dialogue participants try to
keep the dialogue as efficient as possible:
</bodyText>
<listItem confidence="0.9970415">
• Most CRs are partial in form.
• Most of the CRs point out one specific element
</listItem>
<bodyText confidence="0.625039">
(with only a minority being independent as
shown in Table 5). Therefore, in task-oriented
dialogues, CRs locate the understanding prob-
lem directly and give partial credit for what was
understood.
</bodyText>
<listItem confidence="0.993485833333333">
• In task-oriented dialogues, the CR-initiator
asks to confirm an hypothesis about what he
understood rather than asking the other dia-
logue participant to repeat her utterance.
• The addressee prefers to give a short y/n answer
in most cases.
</listItem>
<bodyText confidence="0.999114">
Comparing error sources in the two task-oriented
corpora, we found a number of differences as shown
in Table 6. In particular:
</bodyText>
<footnote confidence="0.997587166666667">
6Another factor that might account for these differences is
that the BNC contains multi-party conversations, and questions
in multi-party conversations may be less likely to receive re-
sponses. Furthermore, due to the poor recording quality of the
BNC, many utterances are marked as “not interpretable”, which
could also lower the response rate.
</footnote>
<listItem confidence="0.998697222222222">
• Dialogue type: Belief and ambiguity refine-
ment do not seem to be a source of problems
in joint problem solving dialogues, as R&amp;S did
not include them in their annotation scheme.
For CRs in information seeking these features
need to be added to explain quite frequent phe-
nomena. As shown in Table 6, 10.2% of CRs
were in one of these two classes.
• Modality: Deictic reference resolution causes
</listItem>
<bodyText confidence="0.775533916666666">
many more understanding difficulties in dia-
logues where people have a shared point of
view than in telephone communication (Biele-
feld: most frequent problem source; Communi-
cator: one instance detected). Furthermore, in
the Bielefeld Corpus, people tend to formulate
more fragmentary sentences. In environments
where people have a shared point of view, com-
plete sentences can be avoided by using non-
verbal communication channels. Finally, we
see that establishing contact is more of a prob-
lem when speech is the only modality available.
</bodyText>
<listItem confidence="0.8328545">
• Channel quality: Acoustic problems are much
more likely in the Communicator Corpus.
</listItem>
<bodyText confidence="0.9995438">
These results indicate that the decision process for
grounding needs to consider the modality, the do-
main, and the communication channel. Similar ex-
tensions to the grounding model are suggested by
(Traum, 1999).
</bodyText>
<page confidence="0.994144">
244
</page>
<subsectionHeader confidence="0.525158">
4.2 Consequences for Generation
</subsectionHeader>
<bodyText confidence="0.999976954545455">
The similarities and differences detected can be
used to give recommendations for generating CRs.
In terms of when to initiate a CR, we can state
that clarification should not be postponed, and im-
mediate, local management of uncertainty is criti-
cal. This view is also supported by observations of
how non-native speakers handle non-understanding
(Paek, 2003).
Furthermore, for task-oriented dialogues the sys-
tem should present an hypothesis to be confirmed,
rather than ask for repetition. Our data suggests that,
when they are confronted with uncertainty, humans
tend to build up hypotheses from the dialogue his-
tory and from their world knowledge. For example,
when the customer specified a date without a month,
the travel agent would propose the most reasonable
hypothesis instead of asking a wh-question. It is in-
teresting to note that Skantze (2003) found that users
are more satisfied if the system “hides” its recog-
nition problem by asking a task-related question to
help to confirm the hypothesis, rather than explicitly
indicating non-understanding.
</bodyText>
<sectionHeader confidence="0.7691555" genericHeader="method">
5 Correlations between Function and
Form: How to say it?
</sectionHeader>
<bodyText confidence="0.999981711864407">
Once the dialogue system has decided on the func-
tion features, it must find a corresponding surface
form to be generated. Many forms are indeed re-
lated to the function as shown in Table 7, where we
present a significance analysis using Pearson’s x2
(with Yates correction).
Source: We found that the relation to the an-
tecedent seems to distinguish fairly reliably be-
tween CRs clarifying reference and those clarify-
ing acoustic understanding. In the Communicator
Corpus, for acoustic problems the CR-initiator tends
to repeat the problematic part literally, while refer-
ence problems trigger a reformulation or a repeti-
tion with addition. For both problem sources, par-
tial declarative questions are preferred. These find-
ings are also supported by R&amp;S. For the first level
of non-understanding, the inability to establish con-
tact, complete polar questions with no relation to the
antecedent are formulated, e.g., ”Are you there?”.
Severity: The severity indicates how much was
understood, i.e., whether the CR initiator asks to
confirm an hypothesis or to repeat the antecedent
utterance. The severity of an error strongly cor-
relates with the sentence mood. Declarative and
polar questions, which take up material from the
problematic utterance, ask to confirm an hypothe-
sis. Wh-questions, which are independent, refor-
mulations or repetitions with additions (e.g., wh-
substituted reprises) of the problematic utterance
usually prompt for repetition, as do imperatives. Al-
ternative questions prompt the addressee to disam-
biguate the hypothesis.
Answer: By definition, certain types of question
prompt for certain answers. Therefore, the feature
answer is closely linked to the sentence mood of
the CR. As polar questions and declarative ques-
tions generally enquire about a proposition, i.e., an
hypothesis or belief, they tend to receive yes/no
answers, but repetitions are also possible. Wh-
questions, alternative questions and imperatives tend
to get answers providing additional information (i.e.,
reformulations and elaborations).
Extent: The function feature extent is logically in-
dependent from the form feature completeness, al-
though they are strongly correlated. Extent is a bi-
nary feature indicating whether the CR points out
a specific element or concerns the whole utterance.
Most fragmentary declarative questions and frag-
mentary polar questions point out a specific element,
especially when they are not independent but stand
in some relation to the antecedent utterance. In-
dependent complete imperatives address the whole
previous utterance.
The correlations found in the Communicator Cor-
pus are fairly consistent with those found in the
Bielefeld Corpus, and thus we believe that the guide-
lines for generating CRs in task-oriented dialogues
may be language independent, at least for German
and English.
</bodyText>
<sectionHeader confidence="0.996743" genericHeader="conclusions">
6 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.996948">
In this paper we presented the results of a corpus
study of naturally occurring CRs in task-oriented di-
alogue. Comparing our results to two other stud-
ies, one of a task-oriented corpus and one of a cor-
</bodyText>
<page confidence="0.993847">
245
</page>
<bodyText confidence="0.8364212">
Form Function
source severity extent answer
mood x2(24) = 112.20 x2(5) = 30.34 x2(5) = 24.25 x2(5) = 25.19
p &lt; 0.001 p &lt; 0.001 df = p &lt; 0.005 p &lt; 0.001
bound-tone indep. indep. indep. indep.
x2(24) = 108.23 rel-antec x2(4) = 42.58 indep.
p &lt; 0.001 x2(4) = 11.69 p &lt; 0.001
p &lt; 0.005
complete x2(7) = 27.39 indep. x2(1) = 27.39 indep.
p &lt; 0.005 p &lt; 0.001
</bodyText>
<tableCaption confidence="0.9758">
Table 7: Significance analysis for form/function correlations.
</tableCaption>
<bodyText confidence="0.999944941176471">
pus of everyday conversation, we found no signif-
icant differences in frequency of CRs and distribu-
tion of forms in the two task-oriented corpora, but
many significant differences between CRs in task-
oriented dialogue and everyday conversation. Our
findings suggest that in task-oriented dialogues, hu-
mans use a cautious, but efficient strategy for clar-
ification, preferring to present an hypothesis rather
than ask the user to repeat or rephrase the problem-
atic utterance. We also identified correlations be-
tween function and form features that can serve as
a basis for generating more natural sounding CRs,
which indicate a specific problem with understand-
ing. In current work, we are studying data collected
in a wizard-of-oz study in a multi-modal setting, in
order to study clarification behavior in multi-modal
dialogue.
</bodyText>
<sectionHeader confidence="0.99645" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.86323">
The authors would like thank Kepa Rodriguez, Oliver Lemon,
and David Reitter for help and discussion.
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999769269230769">
Christina L. Bennett and Alexander I. Rudnicky. 2002.
The Carnegie Mellon Communicator Corpus. In Pro-
ceedings of the International Conference of Spoken
Language Processing (ICSLP02).
Lou Burnard. 2000. The British National Corpus Users
Reference Guide. Technical report, Oxford Universiry
Computing Services.
Herbert Clark. 1996. Using Language. Cambridge Uni-
versity Press.
Malte Gabsdil. 2003. Clarification in Spoken Dialogue
Systems. Proceedings of the 2003 AAAI Spring Sym-
posium. Workshop on Natural Language Generation in
Spoken and Written Dialogue.
Jonathan Ginzburg and Robin Cooper. 2001. Resolving
Ellipsis in Clarification. In Proceedings of the 39th
meeting of the Association for Computational Linguis-
tics.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, Goteborg University.
Tim Paek and Eric Horvitz. 2000. Conversation as Ac-
tion Under Uncertainty. In Proceedings of the Six-
teenth Conference on Uncertainty in Artificial Intelli-
gence.
Tim Paek. 2003. Toward a Taxonomy of Communica-
tion Errors. In ISCA Tutorial and Research Workshop
on Error Handling in Spoken Dialogue Systems.
Matthew Purver, Jonathan Ginzburg, and Patrick Healey.
2003. On the Means for Clarification in Dialogue. In
R. Smith and J. van Kuppevelt, editors, Current and
New Directions in Discourse and Dialogue.
Matthew Purver. 2004. CLARIE: The Clarification En-
gine. In Proceedings of the Eighth Workshop on For-
mal Semantics and Dialogue.
Verena Rieser. 2004. Fragmentary Clarifications on Sev-
eral Levels for Robust Dialogue Systems. Master’s
thesis, School of Informatics, University of Edinburgh.
Kepa J. Rodriguez and David Schlangen. 2004. Form,
Intonation and Function of Clarification Requests in
German Task-orientaded Spoken Dialogues. In Pro-
ceedings of the Eighth Workshop on Formal Semantics
and Dialogue.
David Schlangen. 2004. Causes and Strategies for Re-
question Clarification in Dialogue. Proceedings of the
5th SIGdial Workshop on Discourse and Dialogue.
Gabriel Skantze. 2003. Exploring Human Error Han-
dling Strategies: Implications for Spoken Dialogue
Systems. In ISCA Tutorial and Research Workshop
on Error Handling in Spoken Dialogue Systems.
David R. Traum. 1999. Computational Models of
Grounding in Collaborative Systems. In Proceedings
of the AAAI Fall Symposium on Psychological Models
of Communication.
</reference>
<page confidence="0.998723">
246
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947554">
<title confidence="0.9979815">Implications for Generating Clarification Requests in Task-oriented Dialogues</title>
<author confidence="0.999973">Verena Rieser</author>
<affiliation confidence="0.9999705">Department of Computational Linguistics Saarland University</affiliation>
<address confidence="0.99367">Saarbr¨ucken, D-66041</address>
<email confidence="0.974791">vrieser@coli.uni-sb.de</email>
<author confidence="0.998357">Johanna D Moore</author>
<affiliation confidence="0.9999535">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.99758">Edinburgh, EH8 9LW, GB</address>
<email confidence="0.996244">J.Moore@ed.ac.uk</email>
<abstract confidence="0.999381777777778">Clarification requests (CRs) in conversation ensure and maintain mutual understanding and thus play a crucial role in robust dialogue interaction. In this paper, we describe a corpus study of CRs in task-oriented dialogue and compare our findings to those reported in two prior studies. We find that CR behavior in task-oriented dialogue differs significantly from that in everyday conversation in a number of ways. Moreover, the dialogue type, the modality and the channel quality all influence the decision of when to clarify and at which level of the grounding process. Finally we identify formfunction correlations which can inform the generation of CRs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christina L Bennett</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>The Carnegie Mellon Communicator Corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference of Spoken Language Processing (ICSLP02).</booktitle>
<contexts>
<context position="2647" citStr="Bennett and Rudnicky, 2002" startWordPosition="413" endWordPosition="416">e the use of formfunction mappings derived from human-human dialogues to inform the generation of CRs. We identify the factors that determine which function a CR should take and identify function-form correlations that can be used to guide the automatic generation of CRs. In Section 2, we discuss the classification schemes used in two recent corpus studies of CRs in human-human dialogue, and assess their applicability to the problem of generating CRs. Section 3 describes the results we obtained by applying the classification scheme of Rodriguez and Schlangen (2004) to the Communicator Corpus (Bennett and Rudnicky, 2002). Section 4 draws general conclusions for generating CRs by comparing our results to those of (Purver et al., 2003) and (Rodriguez and Schlangen, 2004). Section 5 describes the correlations between function and form features that are present in the corpus and their implications for generating CRs. 239 Proceedings of the 43rd Annual Meeting of the ACL, pages 239–246, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Attr. Value Category Example form non Non-Reprise “What did you say?” wot Conventional “Sorry?” frg Reprise Fragment “Edinburgh?” lit Literal Reprise “You want </context>
<context position="9881" citStr="Bennett and Rudnicky, 2002" startWordPosition="1552" endWordPosition="1555">dent. In addition, the form feature is not restricted to syntax; it includes features such as intonation and coherence, which are useful for generating the surface form of CRs. Furthermore, the multi-dimensional function feature allows us to describe information relevant to generating CRs that is typically available in dialogue systems, such as the level of confidence in the hypothesis and the problem source. 3 CRs in the Communicator Corpus 3.1 Material and Method Material: We annotated the human-human travel reservation dialogues available as part of the Carnegie Mellon Communicator Corpus (Bennett and Rudnicky, 2002) because we were interested in studying naturally occurring CRs in task-oriented dialogue. In these dialogues, an experienced travel agent is making reservations for trips that people in the Carnegie Mellon Speech Group were taking in the upcoming months. The corpus comprises 31 dialogues of transcribed telephone speech, with 2098 dialogue turns and 19395 words. 241 ⎡ ⎡ ⎤ distance-src:{1 |2 |3 |4 |5 |more� ⎤ ⎢ mood:{ none |decl |polar-q |wh-q |alt-q |imp |other� ⎥ ⎥ ⎢ ⎥ ⎥ ⎢ ⎥ ⎥ form: ⎢form:lnone |particle |partial |complete ⎥ ⎥ ⎢ ⎣relation-antecedent:Inone |add |repet |repet-add |reformul |ind</context>
</contexts>
<marker>Bennett, Rudnicky, 2002</marker>
<rawString>Christina L. Bennett and Alexander I. Rudnicky. 2002. The Carnegie Mellon Communicator Corpus. In Proceedings of the International Conference of Spoken Language Processing (ICSLP02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>The British National Corpus Users Reference Guide.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Oxford Universiry Computing Services.</institution>
<contexts>
<context position="3989" citStr="Burnard, 2000" startWordPosition="627" endWordPosition="628">ight to...?” fil Gap Filler “...Edinburgh?” other Other x readings cla Clausal “Are you asking/asserting that X?” con Constituent “What do you mean by X?” lex Lexical “Did you utter X?” corr Correction “Did you intend to utterX instead?” other Other x Table 2: CR classification scheme by PGH 2 CR Classification Schemes We now discuss two recently proposed classification schemes for CRs, and assess their usefulness for generating CRs in a spoken dialogue system (SDS). 2.1 Purver, Ginzburg and Healey (PGH) Purver, Ginzburg and Healey (2003) investigated CRs in the British National Corpus (BNC) (Burnard, 2000). In their annotation scheme, a CR can take seven distinct surface forms and four readings, as shown in Table 2. The examples for the form feature are possible CRs following the statement “I want a flight to Edinburgh”. The focus of this classification scheme is to map semantic readings to syntactic surface forms. The form feature is defined by its relation to the problematic utterance, i.e., whether a CR reprises the antecedent utterance and to what extent. CRs may take the three different readings as defined by Ginzburg and Cooper (2001), as well as a fourth reading which indicates a correct</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. The British National Corpus Users Reference Guide. Technical report, Oxford Universiry Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1272" citStr="Clark (1996)" startWordPosition="184" endWordPosition="185">ficantly from that in everyday conversation in a number of ways. Moreover, the dialogue type, the modality and the channel quality all influence the decision of when to clarify and at which level of the grounding process. Finally we identify formfunction correlations which can inform the generation of CRs. 1 Introduction Clarification requests in conversation ensure and maintain mutual understanding and thus play a significant role in robust and efficient dialogue interaction. From a theoretical perspective, the model of grounding explains how mutual understanding is established. According to Clark (1996), speakers and listeners ground mutual understanding on four levels of coordination in an action ladder, as shown in Table 1. Several current research dialogue systems can detect errors on different levels of grounding (Paek and Horvitz, 2000; Larsson, 2002; Purver, 2004; Level Speaker S Listener L Convers. S is proposing activity L is considering proα posal α Intention S is signalling that p L is recognizing that p Signal S is presenting signal v L is identifying signal v Channel S is executing behavior L is attending to behav,Q ior ,Q Table 1: Four levels of grounding Schlangen, 2004). Howev</context>
<context position="6674" citStr="Clark (1996)" startWordPosition="1069" endWordPosition="1070"> in generating CRs that indicate the source of the error, we need a classification scheme that represents such information. 2.2 Rodriguez and Schlangen (R&amp;S) Rodriguez and Schlangen (2004) devised a multidimensional classification scheme where form and 1Alternative questions would be interpreted as asking apolar question with an exclusive reading. 240 function are meta-features taking sub-features as attributes. The function feature breaks down into the sub-features source, severity, extent, reply and satisfaction. The sources that might have caused the problem map to the levels as defined by Clark (1996). These sources can also be of different severity. The severity can be interpreted as describing the set of possible referents: asking for repetition indicates that no interpretation is available (cont-rep); asking for confirmation means that the CR initiator has some kind of hypothesis (cont-conf). The extent of a problem describes whether the CR points out a problematic element in the problem utterance. The reply represents the answer the addressee gives to the CR. The satisfaction of the CR-initiator is indicated by whether he renews the request for clarification or not. The meta-feature fo</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert Clark. 1996. Using Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Gabsdil</author>
</authors>
<title>Clarification in Spoken Dialogue Systems.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 AAAI Spring Symposium. Workshop on Natural Language Generation in Spoken and Written Dialogue.</booktitle>
<contexts>
<context position="12224" citStr="Gabsdil (2003)" startWordPosition="1949" endWordPosition="1950">t, add for cases like 3 and repet-add for cases like 4. (3) Cust: What is the last flight I could come back on? Agent: On the 29th of March? (4) Cust: I’ll be returning on Thursday the fifth. Agent: The fifth of February? To the function feature source we added the values belief to cover CRs like 5 and ambiguity refinement to cover CRs like 6. (5) Agent: You need a visa. Cust: I do need one? Agent: Yes you do. (6) Agent: Okay I have two options ... with Hertz ... if not they do have a lower rate with Budget and that is fifty one dollars. Cust: Per day? Agent: Per day um mm. Finally, following Gabsdil (2003) we introduced an additional value for severity, cont-disamb, to 3http://sfb360.uni-bielefeld.de cover CRs that request disambiguation when more than one interpretation is available. Method: We first identified turns containing CRs, and then annotated them with form and function features. It is not always possible to identify CRs from the utterance alone. Frequently, context (e.g., the reaction of the addressee) or intonation is required to distinguish a CR from other feedback strategies, such as positive feedback. See (Rieser, 2004) for a detailed discussion. The annotation was only performed</context>
</contexts>
<marker>Gabsdil, 2003</marker>
<rawString>Malte Gabsdil. 2003. Clarification in Spoken Dialogue Systems. Proceedings of the 2003 AAAI Spring Symposium. Workshop on Natural Language Generation in Spoken and Written Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Ginzburg</author>
<author>Robin Cooper</author>
</authors>
<title>Resolving Ellipsis in Clarification.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4534" citStr="Ginzburg and Cooper (2001)" startWordPosition="718" endWordPosition="721">aley (2003) investigated CRs in the British National Corpus (BNC) (Burnard, 2000). In their annotation scheme, a CR can take seven distinct surface forms and four readings, as shown in Table 2. The examples for the form feature are possible CRs following the statement “I want a flight to Edinburgh”. The focus of this classification scheme is to map semantic readings to syntactic surface forms. The form feature is defined by its relation to the problematic utterance, i.e., whether a CR reprises the antecedent utterance and to what extent. CRs may take the three different readings as defined by Ginzburg and Cooper (2001), as well as a fourth reading which indicates a correction. Although PGH report good coverage of the scheme on their subcorpus of the BNC (99%), we found their classification scheme to to be too coarsegrained to prescribe the form that a CR should take. As shown in example 1, Reprise Fragments (RFs), which make up one third of the BNC, are ambiguous in their readings and may also take several surface forms. (1) I would like to book a flight on Monday. (a) Monday? frg, con/cla (b) Which Monday? frg, con (c) Monday the first? frg, con (d) The first of May? frg, con (e) Monday the first or Monday</context>
<context position="8808" citStr="Ginzburg and Cooper (2001)" startWordPosition="1384" endWordPosition="1387">he reply to the CR. Therefore all possible interpretations are listed. rel-antecedent: reformul source: np-ref severity: cont-conf extent: yes (d) Monday the first or Monday the eighth? mood: alt-q completeness: partial rel-antecedent: addition source: np-ref severity: cont-repet extent: yes In R&amp;S’s classification scheme, ambiguities about CRs having different sources cannot be resolved entirely as example (2.a) shows. However, in contrast to PGH, the overall approach is a different one: instead of explaining causes of CRs within a theoretic-semantic model (as the three different readings of Ginzburg and Cooper (2001) do), they infer the interpretation of the CR from the context. Ambiguities get resolved by the reply of the addressee and the satisfaction of the CR initiator indicates the “mutually agreed interpretation” . R&amp;S’s multi-dimensional CR description allows the fine-grained distinctions needed to generate natural CRs to be made. For example, PGH’s general category of RFs can be made more specific via the values for the feature relation to antecedent. In addition, the form feature is not restricted to syntax; it includes features such as intonation and coherence, which are useful for generating th</context>
</contexts>
<marker>Ginzburg, Cooper, 2001</marker>
<rawString>Jonathan Ginzburg and Robin Cooper. 2001. Resolving Ellipsis in Clarification. In Proceedings of the 39th meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
</authors>
<title>Issue-based Dialogue Management.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Goteborg University.</institution>
<contexts>
<context position="1529" citStr="Larsson, 2002" startWordPosition="225" endWordPosition="226">orrelations which can inform the generation of CRs. 1 Introduction Clarification requests in conversation ensure and maintain mutual understanding and thus play a significant role in robust and efficient dialogue interaction. From a theoretical perspective, the model of grounding explains how mutual understanding is established. According to Clark (1996), speakers and listeners ground mutual understanding on four levels of coordination in an action ladder, as shown in Table 1. Several current research dialogue systems can detect errors on different levels of grounding (Paek and Horvitz, 2000; Larsson, 2002; Purver, 2004; Level Speaker S Listener L Convers. S is proposing activity L is considering proα posal α Intention S is signalling that p L is recognizing that p Signal S is presenting signal v L is identifying signal v Channel S is executing behavior L is attending to behav,Q ior ,Q Table 1: Four levels of grounding Schlangen, 2004). However, only the work of Purver (2004) addresses the question of how the source of the error affects the form the CR takes. In this paper, we investigate the use of formfunction mappings derived from human-human dialogues to inform the generation of CRs. We ide</context>
</contexts>
<marker>Larsson, 2002</marker>
<rawString>Staffan Larsson. 2002. Issue-based Dialogue Management. Ph.D. thesis, Goteborg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Paek</author>
<author>Eric Horvitz</author>
</authors>
<title>Conversation as Action Under Uncertainty.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="1514" citStr="Paek and Horvitz, 2000" startWordPosition="221" endWordPosition="224"> identify formfunction correlations which can inform the generation of CRs. 1 Introduction Clarification requests in conversation ensure and maintain mutual understanding and thus play a significant role in robust and efficient dialogue interaction. From a theoretical perspective, the model of grounding explains how mutual understanding is established. According to Clark (1996), speakers and listeners ground mutual understanding on four levels of coordination in an action ladder, as shown in Table 1. Several current research dialogue systems can detect errors on different levels of grounding (Paek and Horvitz, 2000; Larsson, 2002; Purver, 2004; Level Speaker S Listener L Convers. S is proposing activity L is considering proα posal α Intention S is signalling that p L is recognizing that p Signal S is presenting signal v L is identifying signal v Channel S is executing behavior L is attending to behav,Q ior ,Q Table 1: Four levels of grounding Schlangen, 2004). However, only the work of Purver (2004) addresses the question of how the source of the error affects the form the CR takes. In this paper, we investigate the use of formfunction mappings derived from human-human dialogues to inform the generation</context>
</contexts>
<marker>Paek, Horvitz, 2000</marker>
<rawString>Tim Paek and Eric Horvitz. 2000. Conversation as Action Under Uncertainty. In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Paek</author>
</authors>
<title>Toward a Taxonomy of Communication Errors.</title>
<date>2003</date>
<booktitle>In ISCA Tutorial and Research Workshop on Error Handling in Spoken Dialogue Systems.</booktitle>
<contexts>
<context position="20637" citStr="Paek, 2003" startWordPosition="3291" endWordPosition="3292">pus. These results indicate that the decision process for grounding needs to consider the modality, the domain, and the communication channel. Similar extensions to the grounding model are suggested by (Traum, 1999). 244 4.2 Consequences for Generation The similarities and differences detected can be used to give recommendations for generating CRs. In terms of when to initiate a CR, we can state that clarification should not be postponed, and immediate, local management of uncertainty is critical. This view is also supported by observations of how non-native speakers handle non-understanding (Paek, 2003). Furthermore, for task-oriented dialogues the system should present an hypothesis to be confirmed, rather than ask for repetition. Our data suggests that, when they are confronted with uncertainty, humans tend to build up hypotheses from the dialogue history and from their world knowledge. For example, when the customer specified a date without a month, the travel agent would propose the most reasonable hypothesis instead of asking a wh-question. It is interesting to note that Skantze (2003) found that users are more satisfied if the system “hides” its recognition problem by asking a task-rel</context>
</contexts>
<marker>Paek, 2003</marker>
<rawString>Tim Paek. 2003. Toward a Taxonomy of Communication Errors. In ISCA Tutorial and Research Workshop on Error Handling in Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Jonathan Ginzburg</author>
<author>Patrick Healey</author>
</authors>
<title>On the Means for Clarification in Dialogue. In</title>
<date>2003</date>
<booktitle>Current and New Directions in Discourse and Dialogue.</booktitle>
<editor>R. Smith and J. van Kuppevelt, editors,</editor>
<contexts>
<context position="2762" citStr="Purver et al., 2003" startWordPosition="433" endWordPosition="436">tors that determine which function a CR should take and identify function-form correlations that can be used to guide the automatic generation of CRs. In Section 2, we discuss the classification schemes used in two recent corpus studies of CRs in human-human dialogue, and assess their applicability to the problem of generating CRs. Section 3 describes the results we obtained by applying the classification scheme of Rodriguez and Schlangen (2004) to the Communicator Corpus (Bennett and Rudnicky, 2002). Section 4 draws general conclusions for generating CRs by comparing our results to those of (Purver et al., 2003) and (Rodriguez and Schlangen, 2004). Section 5 describes the correlations between function and form features that are present in the corpus and their implications for generating CRs. 239 Proceedings of the 43rd Annual Meeting of the ACL, pages 239–246, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Attr. Value Category Example form non Non-Reprise “What did you say?” wot Conventional “Sorry?” frg Reprise Fragment “Edinburgh?” lit Literal Reprise “You want a flight to Edinburgh?” slu Reprise Sluice “Where?” sub Wh-substituted Reprise “You want a flight where?” gap Gap “</context>
</contexts>
<marker>Purver, Ginzburg, Healey, 2003</marker>
<rawString>Matthew Purver, Jonathan Ginzburg, and Patrick Healey. 2003. On the Means for Clarification in Dialogue. In R. Smith and J. van Kuppevelt, editors, Current and New Directions in Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
</authors>
<title>CLARIE: The Clarification Engine.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Workshop on Formal Semantics and Dialogue.</booktitle>
<contexts>
<context position="1543" citStr="Purver, 2004" startWordPosition="227" endWordPosition="228">ch can inform the generation of CRs. 1 Introduction Clarification requests in conversation ensure and maintain mutual understanding and thus play a significant role in robust and efficient dialogue interaction. From a theoretical perspective, the model of grounding explains how mutual understanding is established. According to Clark (1996), speakers and listeners ground mutual understanding on four levels of coordination in an action ladder, as shown in Table 1. Several current research dialogue systems can detect errors on different levels of grounding (Paek and Horvitz, 2000; Larsson, 2002; Purver, 2004; Level Speaker S Listener L Convers. S is proposing activity L is considering proα posal α Intention S is signalling that p L is recognizing that p Signal S is presenting signal v L is identifying signal v Channel S is executing behavior L is attending to behav,Q ior ,Q Table 1: Four levels of grounding Schlangen, 2004). However, only the work of Purver (2004) addresses the question of how the source of the error affects the form the CR takes. In this paper, we investigate the use of formfunction mappings derived from human-human dialogues to inform the generation of CRs. We identify the fact</context>
</contexts>
<marker>Purver, 2004</marker>
<rawString>Matthew Purver. 2004. CLARIE: The Clarification Engine. In Proceedings of the Eighth Workshop on Formal Semantics and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
</authors>
<title>Fragmentary Clarifications on Several Levels for Robust Dialogue Systems. Master’s thesis,</title>
<date>2004</date>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="12763" citStr="Rieser, 2004" startWordPosition="2027" endWordPosition="2028">rs. Cust: Per day? Agent: Per day um mm. Finally, following Gabsdil (2003) we introduced an additional value for severity, cont-disamb, to 3http://sfb360.uni-bielefeld.de cover CRs that request disambiguation when more than one interpretation is available. Method: We first identified turns containing CRs, and then annotated them with form and function features. It is not always possible to identify CRs from the utterance alone. Frequently, context (e.g., the reaction of the addressee) or intonation is required to distinguish a CR from other feedback strategies, such as positive feedback. See (Rieser, 2004) for a detailed discussion. The annotation was only performed once. The coding scheme is a slight variation of R&amp;S, which has been shown relaiable with Kappa of 0.7 for identifying source. 3.2 Forms and Functions of CRs in the Communicator Corpus The human-human dialogues in the Communicator Corpus contain 98 CRs in 2098 dialogue turns (4.6%). Forms: The frequencies for the values of the individual form features are shown in Table 3. The most frequent type of CRs were partial declarative questions, which combine the mood value declarative and the completeness value partial.4 These account for </context>
</contexts>
<marker>Rieser, 2004</marker>
<rawString>Verena Rieser. 2004. Fragmentary Clarifications on Several Levels for Robust Dialogue Systems. Master’s thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kepa J Rodriguez</author>
<author>David Schlangen</author>
</authors>
<title>Form, Intonation and Function of Clarification Requests in German Task-orientaded Spoken Dialogues.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Workshop on Formal Semantics and Dialogue.</booktitle>
<contexts>
<context position="2591" citStr="Rodriguez and Schlangen (2004)" startWordPosition="405" endWordPosition="408">affects the form the CR takes. In this paper, we investigate the use of formfunction mappings derived from human-human dialogues to inform the generation of CRs. We identify the factors that determine which function a CR should take and identify function-form correlations that can be used to guide the automatic generation of CRs. In Section 2, we discuss the classification schemes used in two recent corpus studies of CRs in human-human dialogue, and assess their applicability to the problem of generating CRs. Section 3 describes the results we obtained by applying the classification scheme of Rodriguez and Schlangen (2004) to the Communicator Corpus (Bennett and Rudnicky, 2002). Section 4 draws general conclusions for generating CRs by comparing our results to those of (Purver et al., 2003) and (Rodriguez and Schlangen, 2004). Section 5 describes the correlations between function and form features that are present in the corpus and their implications for generating CRs. 239 Proceedings of the 43rd Annual Meeting of the ACL, pages 239–246, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Attr. Value Category Example form non Non-Reprise “What did you say?” wot Conventional “Sorry?” frg Repr</context>
<context position="5566" citStr="Rodriguez and Schlangen, 2004" startWordPosition="893" endWordPosition="896">e forms. (1) I would like to book a flight on Monday. (a) Monday? frg, con/cla (b) Which Monday? frg, con (c) Monday the first? frg, con (d) The first of May? frg, con (e) Monday the first or Monday the eighth? frg, (exclusive) con RFs endorse literal repetitions of part of the problematic utterance (1.a); repetitions with an additional question word (1.b); repetition with further specification (1.c); reformulations (1.d); and alternative questions (1.e)1. In addition to being too general to describe such differences, the classification scheme also fails to describe similarities. As noted by (Rodriguez and Schlangen, 2004), PGH provide no feature to describe the extent to which an RF repeats the problematic utterance. Finally, some phenomena cannot be described at all by the four readings. For example, the readings do not account for non-understanding on the pragmatic level. Furthermore the readings may have several problem sources: the clausal reading may be appropriate where the CR initiator failed to recognise the word acoustically as well as when he failed to resolve the reference. Since we are interested in generating CRs that indicate the source of the error, we need a classification scheme that represent</context>
<context position="13820" citStr="Rodriguez and Schlangen, 2004" startWordPosition="2192" endWordPosition="2196">n Table 3. The most frequent type of CRs were partial declarative questions, which combine the mood value declarative and the completeness value partial.4 These account for 53.1% of the CRs in the corpus. Moreover, four of the five most frequent surface forms of CRs in the Communicator Corpus differ only in the value for the feature relation-antecedent. They are partial declaratives with rising boundary tone, that either reformulate (7.1%) the problematic utterance, repeat 4Declarative questions cover “all cases of non-interrogative word-order, i.e., both declarative sentences and fragments” (Rodriguez and Schlangen, 2004). 242 Feature Value Freq. (%) Mood declarative 65 polar 21 wh-question 7 other 7 Completeness partial 58 complete 38 other 4 Relation antecedent rep-add 27 independent 21 reformulation 19 repetition 18 addition 10 other 5 Boundary tone rising 74 falling 22 other 4 Table 3: Distribution of values for the form features Feature Value Freq.(%) Source np-reference 40 acoustic 31 intention 8 belief 6 ambiguity 4 contact 4 others 3 relevance 2 several 2 Extent yes 80 no 20 Severity confirmation 73 repetition 20 other 7 Answer y/n answer 64 other 15 elaboration 13 no reaction 6 Table 4: Distribution o</context>
</contexts>
<marker>Rodriguez, Schlangen, 2004</marker>
<rawString>Kepa J. Rodriguez and David Schlangen. 2004. Form, Intonation and Function of Clarification Requests in German Task-orientaded Spoken Dialogues. In Proceedings of the Eighth Workshop on Formal Semantics and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
</authors>
<title>Causes and Strategies for Requestion Clarification in Dialogue.</title>
<date>2004</date>
<booktitle>Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="1865" citStr="Schlangen, 2004" startWordPosition="287" endWordPosition="288">cording to Clark (1996), speakers and listeners ground mutual understanding on four levels of coordination in an action ladder, as shown in Table 1. Several current research dialogue systems can detect errors on different levels of grounding (Paek and Horvitz, 2000; Larsson, 2002; Purver, 2004; Level Speaker S Listener L Convers. S is proposing activity L is considering proα posal α Intention S is signalling that p L is recognizing that p Signal S is presenting signal v L is identifying signal v Channel S is executing behavior L is attending to behav,Q ior ,Q Table 1: Four levels of grounding Schlangen, 2004). However, only the work of Purver (2004) addresses the question of how the source of the error affects the form the CR takes. In this paper, we investigate the use of formfunction mappings derived from human-human dialogues to inform the generation of CRs. We identify the factors that determine which function a CR should take and identify function-form correlations that can be used to guide the automatic generation of CRs. In Section 2, we discuss the classification schemes used in two recent corpus studies of CRs in human-human dialogue, and assess their applicability to the problem of gener</context>
<context position="5566" citStr="Schlangen, 2004" startWordPosition="895" endWordPosition="896"> would like to book a flight on Monday. (a) Monday? frg, con/cla (b) Which Monday? frg, con (c) Monday the first? frg, con (d) The first of May? frg, con (e) Monday the first or Monday the eighth? frg, (exclusive) con RFs endorse literal repetitions of part of the problematic utterance (1.a); repetitions with an additional question word (1.b); repetition with further specification (1.c); reformulations (1.d); and alternative questions (1.e)1. In addition to being too general to describe such differences, the classification scheme also fails to describe similarities. As noted by (Rodriguez and Schlangen, 2004), PGH provide no feature to describe the extent to which an RF repeats the problematic utterance. Finally, some phenomena cannot be described at all by the four readings. For example, the readings do not account for non-understanding on the pragmatic level. Furthermore the readings may have several problem sources: the clausal reading may be appropriate where the CR initiator failed to recognise the word acoustically as well as when he failed to resolve the reference. Since we are interested in generating CRs that indicate the source of the error, we need a classification scheme that represent</context>
<context position="13820" citStr="Schlangen, 2004" startWordPosition="2195" endWordPosition="2196"> most frequent type of CRs were partial declarative questions, which combine the mood value declarative and the completeness value partial.4 These account for 53.1% of the CRs in the corpus. Moreover, four of the five most frequent surface forms of CRs in the Communicator Corpus differ only in the value for the feature relation-antecedent. They are partial declaratives with rising boundary tone, that either reformulate (7.1%) the problematic utterance, repeat 4Declarative questions cover “all cases of non-interrogative word-order, i.e., both declarative sentences and fragments” (Rodriguez and Schlangen, 2004). 242 Feature Value Freq. (%) Mood declarative 65 polar 21 wh-question 7 other 7 Completeness partial 58 complete 38 other 4 Relation antecedent rep-add 27 independent 21 reformulation 19 repetition 18 addition 10 other 5 Boundary tone rising 74 falling 22 other 4 Table 3: Distribution of values for the form features Feature Value Freq.(%) Source np-reference 40 acoustic 31 intention 8 belief 6 ambiguity 4 contact 4 others 3 relevance 2 several 2 Extent yes 80 no 20 Severity confirmation 73 repetition 20 other 7 Answer y/n answer 64 other 15 elaboration 13 no reaction 6 Table 4: Distribution o</context>
</contexts>
<marker>Schlangen, 2004</marker>
<rawString>David Schlangen. 2004. Causes and Strategies for Requestion Clarification in Dialogue. Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
</authors>
<title>Exploring Human Error Handling Strategies: Implications for Spoken Dialogue Systems.</title>
<date>2003</date>
<booktitle>In ISCA Tutorial and Research Workshop on Error Handling in Spoken Dialogue Systems.</booktitle>
<contexts>
<context position="21134" citStr="Skantze (2003)" startWordPosition="3370" endWordPosition="3371">ritical. This view is also supported by observations of how non-native speakers handle non-understanding (Paek, 2003). Furthermore, for task-oriented dialogues the system should present an hypothesis to be confirmed, rather than ask for repetition. Our data suggests that, when they are confronted with uncertainty, humans tend to build up hypotheses from the dialogue history and from their world knowledge. For example, when the customer specified a date without a month, the travel agent would propose the most reasonable hypothesis instead of asking a wh-question. It is interesting to note that Skantze (2003) found that users are more satisfied if the system “hides” its recognition problem by asking a task-related question to help to confirm the hypothesis, rather than explicitly indicating non-understanding. 5 Correlations between Function and Form: How to say it? Once the dialogue system has decided on the function features, it must find a corresponding surface form to be generated. Many forms are indeed related to the function as shown in Table 7, where we present a significance analysis using Pearson’s x2 (with Yates correction). Source: We found that the relation to the antecedent seems to di</context>
</contexts>
<marker>Skantze, 2003</marker>
<rawString>Gabriel Skantze. 2003. Exploring Human Error Handling Strategies: Implications for Spoken Dialogue Systems. In ISCA Tutorial and Research Workshop on Error Handling in Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
</authors>
<title>Computational Models of Grounding in Collaborative Systems.</title>
<date>1999</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Psychological Models of Communication.</booktitle>
<contexts>
<context position="20241" citStr="Traum, 1999" startWordPosition="3230" endWordPosition="3231">he Bielefeld Corpus, people tend to formulate more fragmentary sentences. In environments where people have a shared point of view, complete sentences can be avoided by using nonverbal communication channels. Finally, we see that establishing contact is more of a problem when speech is the only modality available. • Channel quality: Acoustic problems are much more likely in the Communicator Corpus. These results indicate that the decision process for grounding needs to consider the modality, the domain, and the communication channel. Similar extensions to the grounding model are suggested by (Traum, 1999). 244 4.2 Consequences for Generation The similarities and differences detected can be used to give recommendations for generating CRs. In terms of when to initiate a CR, we can state that clarification should not be postponed, and immediate, local management of uncertainty is critical. This view is also supported by observations of how non-native speakers handle non-understanding (Paek, 2003). Furthermore, for task-oriented dialogues the system should present an hypothesis to be confirmed, rather than ask for repetition. Our data suggests that, when they are confronted with uncertainty, human</context>
</contexts>
<marker>Traum, 1999</marker>
<rawString>David R. Traum. 1999. Computational Models of Grounding in Collaborative Systems. In Proceedings of the AAAI Fall Symposium on Psychological Models of Communication.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>