<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000802">
<title confidence="0.996457">
Character-based Kernels for Novelistic Plot Structure
</title>
<author confidence="0.99537">
Micha Elsner
</author>
<affiliation confidence="0.998830666666667">
Institute for Language, Cognition and Computation (ILCC)
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.997962">
melsner0@gmail.com
</email>
<sectionHeader confidence="0.99739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99928555">
Better representations of plot structure
could greatly improve computational meth-
ods for summarizing and generating sto-
ries. Current representations lack abstrac-
tion, focusing too closely on events. We
present a kernel for comparing novelistic
plots at a higher level, in terms of the
cast of characters they depict and the so-
cial relationships between them. Our kernel
compares the characters of different nov-
els to one another by measuring their fre-
quency of occurrence over time and the
descriptive and emotional language associ-
ated with them. Given a corpus of 19th-
century novels as training data, our method
can accurately distinguish held-out novels
in their original form from artificially dis-
ordered or reversed surrogates, demonstrat-
ing its ability to robustly represent impor-
tant aspects of plot structure.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925923076923">
Every culture has stories, and storytelling is one
of the key functions of human language. Yet while
we have robust, flexible models for the structure
of informative documents (for instance (Chen et
al., 2009; Abu Jbara and Radev, 2011)), current
approaches have difficulty representing the nar-
rative structure of fictional stories. This causes
problems for any task requiring us to model
fiction, including summarization and generation
of stories; Kazantseva and Szpakowicz (2010)
show that state-of-the-art summarizers perform
extremely poorly on short fictional texts1. A ma-
jor problem with applying models for informative
</bodyText>
<footnote confidence="0.998416">
1Apart from Kazantseva, we know of one other at-
tempt to apply a modern summarizer to fiction, by the
artist Jason Huff, using Microsoft Word 2008’s extrac-
tive summary feature: http://jason-huff.com/
</footnote>
<bodyText confidence="0.969173435897436">
text to fiction is that the most important struc-
ture underlying the narrative—its plot—occurs at
a high level of abstraction, while the actual narra-
tion is of a series of lower-level events.
A short synopsis of Jane Austen’s novel Pride
and Prejudice, for example, is that Elizabeth Ben-
net first thinks Mr. Darcy is arrogant, but later
grows to love him. But this is not stated straight-
forwardly in the text; the reader must infer it from
the behavior of the characters as they participate
in various everyday scenes.
In this paper, we present the plot kernel, a
coarse-grained, but robust representation of nov-
elistic plot structure. The kernel evaluates the
similarity between two novels in terms of the
characters and their relationships, constructing
functional analogies between them. These are in-
tended to correspond to the labelings produced by
human literary critics when they write, for exam-
ple, that Elizabeth Bennet and Emma Woodhouse
are protagonists of their respective novels. By fo-
cusing on which characters and relationships are
important, rather than specifically how they inter-
act, our system can abstract away from events and
focus on more easily-captured notions of what
makes a good story.
The ability to find correspondences between
characters is key to eventually summarizing or
even generating interesting stories. Once we can
effectively model the kinds of people a romance
or an adventure story is usually about, and what
kind of relationships should exist between them,
we can begin trying to analyze new texts by com-
parison with familiar ones. In this work, we eval-
uate our system on the comparatively easy task
projects/autosummarize. Although this cannot be
treated as a scientific experiment, the results are unusably
bad; they consist mostly of short exclamations containing
the names of major characters.
</bodyText>
<page confidence="0.964195">
634
</page>
<note confidence="0.977023">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 634–644,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999814222222222">
of recognizing acceptable novels (section 6), but
recognition is usually a good first step toward
generation—a recognition model can always be
used as part of a generate-and-rank pipeline, and
potentially its underlying representation can be
used in more sophisticated ways. We show a de-
tailed analysis of the character correspondences
discovered by our system, and discuss their po-
tential relevance to summarization, in section 9.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999920546666667">
Some recent work on story understanding has fo-
cused on directly modeling the series of events
that occur in the narrative. McIntyre and Lapata
(2010) create a story generation system that draws
on earlier work on narrative schemas (Chambers
and Jurafsky, 2009). Their system ensures that
generated stories contain plausible event-to-event
transitions and are coherent. Since it focuses only
on events, however, it cannot enforce a global no-
tion of what the characters want or how they relate
to one another.
Our own work draws on representations that
explicitly model emotions rather than events. Alm
and Sproat (2005) were the first to describe sto-
ries in terms of an emotional trajectory. They an-
notate emotional states in 22 Grimms’ fairy tales
and discover an increase in emotion (mostly posi-
tive) toward the ends of stories. They later use this
corpus to construct a reasonably accurate clas-
sifier for emotional states of sentences (Alm et
al., 2005). Volkova et al. (2010) extend the hu-
man annotation approach using a larger number of
emotion categories and applying them to freely-
defined chunks instead of sentences. The largest-
scale emotional analysis is performed by Moham-
mad (2011), using crowd-sourcing to construct a
large emotional lexicon with which he analyzes
adult texts such as plays and novels. In this work,
we adopt the concept of emotional trajectory, but
apply it to particular characters rather than works
as a whole.
In focusing on characters, we follow Elson et
al. (2010), who analyze narratives by examining
their social network relationships. They use an
automatic method based on quoted speech to find
social links between characters in 19th century
novels. Their work, designed for computational
literary criticism, does not extract any temporal
or emotional structure.
A few projects attempt to represent story struc-
ture in terms of both characters and their emo-
tional states. However, they operate at a very de-
tailed level and so can be applied only to short
texts. Scheherazade (Elson and McKeown, 2010)
allows human annotators to mark character goals
and emotional states in a narrative, and indicate
the causal links between them. AESOP (Goyal et
al., 2010) attempts to learn a similar structure au-
tomatically. AESOP’s accuracy, however, is rel-
atively poor even on short fables, indicating that
this fine-grained approach is unlikely to be scal-
able to novel-length texts; our system relies on a
much coarser analysis.
Kazantseva and Szpakowicz (2010) summarize
short stories, although unlike the other projects
we discuss here, they explicitly try to avoid giving
away plot details—their goal is to create “spoiler-
free” summaries focusing on characters, settings
and themes, in order to attract potential readers.
They do find it useful to detect character men-
tions, and also use features based on verb aspect to
automatically exclude plot events while retaining
descriptive passages. They compare their genre-
specific system with a few state-of-the-art meth-
ods for summarizing news, and find it outper-
forms them substantially.
We evaluate our system by comparing real nov-
els to artificially produced surrogates, a procedure
previously used to evaluate models of discourse
coherence (Karamanis et al., 2004; Barzilay and
Lapata, 2005) and models of syntax (Post, 2011).
As in these settings, we anticipate that perfor-
mance on this kind of task will be correlated with
performance in applied settings, so we use it as an
easier preliminary test of our capabilities.
</bodyText>
<sectionHeader confidence="0.998577" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.999277857142857">
We focus on the 19th century novel, partly fol-
lowing Elson et al. (2010) and partly because
these texts are freely available via Project Guten-
berg. Our main dataset is composed of romances
(which we loosely define as novels focusing on a
courtship or love affair). We select 41 texts, tak-
ing 11 as a development set and the remaining
30 as a test set; a complete list is given in Ap-
pendix A. We focus on the novels used in Elson
et al. (2010), but in some cases add additional ro-
mances by an already-included author. We also
selected 10 of the least romantic works as an out-
of-domain set; experiments on these are in section
8.
</bodyText>
<page confidence="0.999387">
635
</page>
<sectionHeader confidence="0.996391" genericHeader="method">
4 Preprocessing
</sectionHeader>
<bodyText confidence="0.9999825">
In order to compare two texts, we must first ex-
tract the characters in each and some features of
their relationships with one another. Our first step
is to split the text into chapters, and each chapter
into paragraphs; if the text contains a running di-
alogue where each line begins with a quotation
mark, we append it to the previous paragraph.
We segment each paragraph with MXTerminator
(Reynar and Ratnaparkhi, 1997) and parse it with
the self-trained Charniak parser (McClosky et al.,
2006). Next, we extract a list of characters, com-
pute dependency tree-based unigram features for
each character, and record character frequencies
and relationships over time.
</bodyText>
<subsectionHeader confidence="0.995979">
4.1 Identifying characters
</subsectionHeader>
<bodyText confidence="0.999711625">
We create a list of possible character references
for each work by extracting all strings of proper
nouns (as detected by the parser), then discarding
those which occur less than 5 times. Grouping
these into a useful character list is a problem of
cross-document coreference.
Although cross-document coreference has been
extensively studied (Bhattacharya and Getoor,
2005) and modern systems can achieve quite high
accuracy on the TAC-KBP task, where the list
of available entities is given in advance (Dredze
et al., 2010), novelistic text poses a significant
challenge for the methods normally used. The
typical 19th-century novel contains many related
characters, often named after one another. There
are complicated social conventions determining
which titles are used for whom—for instance,
the eldest unmarried daughter of a family can be
called “Miss Bennet”, while her younger sister
must be “Miss Elizabeth Bennet”. And characters
often use nicknames, such as “Lizzie”.
Our system uses the multi-stage clustering
approach outlined in Bhattacharya and Getoor
(2005), but with some features specific to 19th
century European names. To begin, we merge all
identical mentions which contain more than two
words (leaving bare first or last names unmerged).
Next, we heuristically assign each mention a gen-
der (masculine, feminine or neuter) using a list of
gendered titles, then a list of male and female first
names2. We then merge mentions where each is
longer than one word, the genders do not clash,
</bodyText>
<footnote confidence="0.773879">
2The most frequent names from the 1990 US census.
</footnote>
<figure confidence="0.5992974">
reply left-of-[name] 17
right-of-[name] feel 14
right-of-[name] look 10
right-of-[name] mind 7
right-of-[name] make 7
</figure>
<tableCaption confidence="0.961585333333333">
Table 1: Top five stemmed unigram dependency fea-
tures for “Miss Elizabeth Bennet”, protagonist of
Pride and Prejudice, and their frequencies.
</tableCaption>
<bodyText confidence="0.9999414375">
and the first and last names are consistent (Char-
niak, 2001). We then merge single-word mentions
with matching multiword mentions if they appear
in the same paragraph, or if not, with the multi-
word mention that occurs in the most paragraphs.
When this process ends, we have resolved each
mention in the novel to some specific character.
As in previous work, we discard very infrequent
characters and their mentions.
For the reasons stated, this method is error-
prone. Our intuition is that the simpler method
described in Elson et al. (2010), which merges
each mention to the most recent possible coref-
erent, must be even more so. However, due to
the expense of annotation, we make no attempt to
compare these methods directly.
</bodyText>
<subsectionHeader confidence="0.992557">
4.2 Unigram character features
</subsectionHeader>
<bodyText confidence="0.999240285714286">
Once we have obtained the character list, we use
the dependency relationships extracted from our
parse trees to compute features for each charac-
ter. Similar feature sets are used in previous work
in word classification, such as (Lin and Pantel,
2001). A few example features are shown in Table
1.
To find the features, we take each mention in
the corpus and count up all the words outside the
mention which depend on the mention head, ex-
cept proper nouns and stop words. We also count
the mention’s own head word, and mark whether
it appears to the right or the left (in general, this
word is a verb and the direction reflects the men-
tion’s role as subject or object). We lemmatize
all feature words with the WordNet (Miller et al.,
1990) stemmer. The resulting distribution over
words is our set of unigram features for the char-
acter. (We do not prune rare features, although
they have proportionally little influence on our
measurement of similarity.)
</bodyText>
<page confidence="0.986228">
636
</page>
<figure confidence="0.998290384615384">
0.6
0.4
0.0
0.8
0.2
1.6
1.4
1.0
1.2
0 10 20 30 40 50
Freq of Miss Elizabeth Bennet
Emotions of Miss Elizabeth Bennet
Cross freq x Mr. Darcy
</figure>
<figureCaption confidence="0.995077666666667">
Figure 1: Normalized frequency and emotions associated with “Miss Elizabeth Bennet”, protagonist of Pride
and Prejudice, and frequency of paragraphs about her and “Mr. Darcy”, smoothed and projected onto 50 basis
points.
</figureCaption>
<subsectionHeader confidence="0.998365">
4.3 Temporal relationships
</subsectionHeader>
<bodyText confidence="0.9584091">
We record two time-varying features for each
character, each taking one value per chapter. The
first is the character’s frequency as a proportion
of all character mentions in the chapter. The sec-
ond is the frequency with which the character is
associated with emotional language—their emo-
tional trajectory (Alm et al., 2005). We use the
strong subjectivity cues from the lexicon of Wil-
son et al. (2005) as a measurement of emotion.
If, in a particular paragraph, only one character
is mentioned, we count all emotional words in
that paragraph and add them to the character’s
total. To render the numbers comparable across
works, each paragraph subtotal is normalized by
the amount of emotional language in the novel as
a whole. Then the chapter score is the average
over paragraphs.
For pairwise character relationships, we count
the number of paragraphs in which only two char-
acters are mentioned, and treat this number (as a
proportion of the total) as a measurement of the
strength of the relationship between that pair3. El-
son et al. (2010) show that their method of find-
ing conversations between characters is more pre-
cise in showing whether a relationship exists, but
the co-occurrence technique is simpler, and we
3We tried also counting emotional language in these para-
graphs, but this did not seem to help in development experi-
ments.
care mostly about the strength of key relationships
rather than the existence of infrequent ones.
Finally, we perform some smoothing, by taking
a weighted moving average of each feature value
with a window of the three values on either side.
Then, in order to make it easy to compare books
with different numbers of chapters, we linearly in-
terpolate each series of points into a curve and
project it onto a fixed basis of 50 evenly spaced
points. An example of the final output is shown in
Figure 1.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="method">
5 Kernels
</sectionHeader>
<bodyText confidence="0.9908194">
Our plot kernel k(x, y) measures the similarity
between two novels x and y in terms of the fea-
tures computed above. It takes the form of a
convolution kernel (Haussler, 1999) where the
“parts” of each novel are its characters u E x,
v E y and c is a kernel over characters:
c(u, v) (1)
We begin by constructing a first-order ker-
nel over characters, c1(u, v), which is defined in
terms of a kernel d over the unigram features and
a kernel a over the single-character temporal fea-
tures. We represent the unigram feature counts as
distributions pu(w) and pv(w), and compute their
similarity as the amount of shared mass, times a
small penalty of .1 for mismatched genders:
</bodyText>
<equation confidence="0.933153875">
�
k(x, y) �
uEx
�
vEy
637
d(pu, pv) = exp(−α(1 − E7„ min(pu(w), pv(w))
x.1 ff{genu = genv}
</equation>
<bodyText confidence="0.9999116">
We compute similarity between a pair of time-
varying curves (which are projected onto 50
evenly spaced points) using standard cosine dis-
tance, which approximates the normalized inte-
gral of their product.
</bodyText>
<equation confidence="0.9726555">
a
e(u,v) = V/Il U11 U0ll vllvll (2)
</equation>
<bodyText confidence="0.999991">
The weights α and β are parameters of the sys-
tem, which scale d and e so that they are compa-
rable to one another, and also determine how fast
the similarity scales up as the feature sets grow
closer; we set them to 5 and 10 respectively.
We sum together the similarities of the char-
acter frequency and emotion curves to measure
overall temporal similarity between the charac-
ters. Thus our first-order character kernel c1 is:
</bodyText>
<equation confidence="0.991287">
c1(u,v) = d(pu,pv)(e(ufreq,vfreq)+e(uemo,vemo)
</equation>
<bodyText confidence="0.999920166666667">
We use c1 and equation 1 to construct a first-
order plot kernel (which we call k1), and also as
an ingredient in a second-order character kernel
c2 which takes into account the curve of pairwise
frequencies u, u0 between two characters u and u0
in the same novel.
</bodyText>
<equation confidence="0.769162">
�c2(u, v) = c1(u, v) � e( u, u0, v, v0)c1(u0, v0)
u�∈Xv�∈y
</equation>
<bodyText confidence="0.9999306">
In other words, u is similar to v if, for some
relationships of u with other characters u0, there
are similar characters v0 who serves the same role
for v. We use c2 and equation 1 to construct our
full plot kernel k2.
</bodyText>
<subsectionHeader confidence="0.983528">
5.1 Sentiment-only baseline
</subsectionHeader>
<bodyText confidence="0.999993357142857">
In addition to our plot kernel systems, we imple-
ment a simple baseline intended to test the effec-
tiveness of tracking the emotional trajectory of
the novel without using character identities. We
give our baseline access to the same subjectiv-
ity lexicon used for our temporal features. We
compute the number of emotional words used in
each chapter (regardless of which characters they
co-occur with), smoothed and normalized as de-
scribed in subsection 4.3. This produces a single
1iime-varying curve for each novel, representing
the average emotional intensity of each chapter.
We use our curve kernel e (equation 2) to mea-
sure similarity between novels.
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99975868">
We evaluate our kernels on their ability to distin-
guish between real novels from our dataset and
artificial surrogate novels of three types. First, we
alter the order of a real novel by permuting its
chapters before computing features. We construct
one uniformally-random permutation for each test
novel. Second, we change the identities of the
characters by reassigning the temporal features
for the different characters uniformally at random
while leaving the unigram features unaltered. (For
example, we might assign the frequency, emotion
and relationship curves for “Mr. Collins” to “Miss
Elizabeth Bennet” instead.) Again, we produce
one test instance of this type for each test novel.
Third, we experiment with a more difficult order-
ing task by taking the chapters in reverse.
) In each case, we use our kernel to perform
a ranking task, deciding whether k(x, y) &gt;
k(x, yperm). Since this is a binary forced-choice
classification, a random baseline would score
50%. We evaluate performance in the case where
we are given only a single training document x,
and for a whole training set X, in which case we
combine the decisions using a weighted nearest
neighbor (WNN) strategy:
</bodyText>
<equation confidence="0.9880465">
� k(x, y) &gt; � k(x, yperm)
X∈X X∈X
</equation>
<bodyText confidence="0.9864985">
In each case, we perform the experiment in
a leave-one-out fashion; we include the 11 de-
velopment documents in X, but not in the test
set. Thus there are 1200 single-document compar-
isons and 30 with WNN. The results of our three
systems (the baseline, the first-order kernel k1 and
the second-order kernel k2) are shown in Table
2. (The sentiment-only baseline has no character-
specific features, and so cannot perform the char-
acter task.)
Using the full dataset and second-order kernel
k2, our system’s performance on these tasks is
quite good; we are correct 90% of the time for
order and character examples, and 67% for the
</bodyText>
<page confidence="0.996953">
638
</page>
<table confidence="0.998651142857143">
sentiment only order character reverse
single doc k1 46.2 - 51.5
single doc k2 59.5 63.7 50.7
61.8 67.7 51.6
WNN sentiment 50 - 53
WNN k1 77 90 63
WNN k2 90 90 67
</table>
<tableCaption confidence="0.996075">
Table 2: Accuracy of kernels ranking 30 real novels
against artificial surrogates (chance accuracy 50%).
</tableCaption>
<bodyText confidence="0.969223536585366">
more difficult reverse cases. Results of this qual-
ity rely heavily on the WNN strategy, which trusts
close neighbors more than distant ones.
In the single training point setup, the system
is much less accurate. In this setting, the sys-
tem is forced to make decisions for all pairs of
texts independently, including pairs it considers
very dissimilar because it has failed to find any
useful correspondences. Performance for these
pairs is close to chance, dragging down overall
scores (52% for reverse) even if the system per-
forms well on pairs where it finds good correspon-
dences, enabling a higher WNN score (67%).
The reverse case is significantly harder than
order. This is because randomly permuting a
novel actually breaks up the temporal continuity
of the text—for instance, a minor character who
appeared in three adjacent chapters might now ap-
pear in three separate places. Reversing the text
does not cause this kind of disruption, so correctly
detecting a reversal requires the system to repre-
sent patterns with a distinct temporal orientation,
for instance an intensification in the main char-
acter’s emotions, or in the number of paragraphs
focusing on pairwise relationships, toward the end
of the text.
The baseline system is ineffective at detecting
either ordering or reversals4. The first-order ker-
nel k1 is as good as k2 in detecting character per-
mutations, but less effective on reorderings and
reversals. As we will show in section 9, k1 places
more emphasis on correspondences between mi-
nor characters and between places, while k2 is
more sensitive to protagonists and their relation-
ships, which carry the richest temporal informa-
4The baseline detects reversals as well as the plot kernels
given only a single point of comparison, but these results do
not transfer to the WNN strategy. This suggests that unlike
the plot kernels, the baseline is no more accurate for docu-
ments it considers similar than for those it judges are distant.
tion.
</bodyText>
<sectionHeader confidence="0.934398" genericHeader="method">
7 Significance testing
</sectionHeader>
<bodyText confidence="0.999989722222222">
In addition to using our kernel as a classifier, we
can directly test its ability to distinguish real from
altered novels via a non-parametric two-sample
significance test, the Maximum Mean Discrep-
ancy (MMD) test (Gretton et al., 2007). Given
samples from a pair of distributions p and q and
a kernel k, this test determines whether the null
hypothesis that p and q are identically distributed
in the kernel’s feature space can be rejected. The
advantage of this test is that, since it takes all
pairwise comparisons (except self-comparisons)
within and across the classes into account, it uses
more information than our classification experi-
ments, and can therefore be more sensitive.
As in Gretton et al. (2007), we find an unbiased
estimate of the test statistic MMD2 for sample
sets x — p, y — q, each with m samples, by pair-
ing the two as z = (xi, yi) and computing:
</bodyText>
<equation confidence="0.998116">
1
MMD2 (x, y) = _ Y- h(zi, zj )
(m) (m 1) i7�j
h(zi, zj) = k(xi, xj)+k(yi, yj)—k(xi, yj)—k(xj, yi)
</equation>
<bodyText confidence="0.999263090909091">
Intuitively, MMD2 approaches 0 if the ker-
nel cannot distinguish x from y and is positive
otherwise. The null distribution is computed by
the bootstrap method; we create null-distributed
samples by randomly swapping xi and yi in ele-
ments of z and computing the test statistic. We
use 10000 test permutations. Using both k1 and
k2, we can reject the null hypothesis that the dis-
tribution of novels is equal to order or characters
with p &lt; .001; for reversals, we cannot reject the
null hypothesis.
</bodyText>
<sectionHeader confidence="0.996837" genericHeader="method">
8 Out-of-domain data
</sectionHeader>
<bodyText confidence="0.9999834">
In our main experiments, we tested our kernel
only on romances; here we investigate its ability
to generalize across genres. We take as our train-
ing set X the same romances as above, but as our
test set Y a disjoint set of novels focusing mainly
on crime, children and the supernatural.
Our results (Table 3) are not appreciably differ-
ent from those of the in-domain experiments (Ta-
ble 2) considering the small size of the dataset.
This shows our system to be robust, but shallow;
</bodyText>
<equation confidence="0.599524">
m
</equation>
<page confidence="0.991766">
639
</page>
<table confidence="0.998786428571429">
sentiment only order character reverse
single doc k1 33.0 - 53.4
single doc k2 59.5 61.7 52.7
63.7 62.0 57.3
WNN sentiment 20 - 70
WNN k1 80 90 80
WNN k2 100 80 70
</table>
<tableCaption confidence="0.998524666666667">
Table 3: Accuracy of kernels ranking 10 non-romance
novels against artificial surrogates, with 41 romances
used for comparison.
</tableCaption>
<bodyText confidence="0.995876">
the patterns it can represent generalize acceptably
across domains, but this suggests it is describing
broad concepts like “main character” rather than
genre-specific ones like “female romantic lead”.
</bodyText>
<sectionHeader confidence="0.846111" genericHeader="conclusions">
9 Character-level analysis
</sectionHeader>
<bodyText confidence="0.999979566666667">
To gain some insight into exactly what kinds of
similarities the system picks up on when compar-
ing two works, we sorted the characters detected
by our system into categories and measured their
contribution to the kernel’s overall scores. We
selected four Jane Austen works from the devel-
opment set5 and hand-categorized each character
detected by our system. (We performed the cate-
gorization based on the most common full name
mention in each cluster. This name is usually a
good identifier for all the mentions in the cluster,
but if our coreference system has made an error, it
may not be.)
Our categorization for characters is intended to
capture the stereotypical plot dynamics of liter-
ary romance, sorting the characters according to
their gender and a simple notion of their plot func-
tion. The genders are female, male, plural (“the
Crawfords”) or not a character (“London”). The
functional classes are protagonist (used for the
female viewpoint character and her eventual hus-
band), marriageable (single men and women
who are seeking to marry within the story) and
other (older characters, children, and characters
married before the story begins).
We evaluate the pairwise kernel similarities
among our four works, and add up the propor-
tional contribution made by character pairs of
each type to the eventual score. (For instance,
the similarity between “Elizabeth Bennet” and
</bodyText>
<footnote confidence="0.7934775">
5Pride and Prejudice, Emma, Mansfield Park and Per-
suasion.
</footnote>
<bodyText confidence="0.995756020408164">
“Emma Woodhouse”, both labeled “female pro-
tagonist”, contributes 26% of the kernel similarity
between the works in which they appear.) We plot
these as Hinton-style diagrams in Figure 2. The
size of each black rectangle indicates the magni-
tude of the contribution. (Since kernel functions
are symmetric, we show only the lower diagonal.)
Under the kernel for unigram features, d
(top), the most common character types—non-
characters (almost always places) and non-
marriageable women—contribute most to the ker-
nel scores; this is especially true for places, since
they often occur with similar descriptive terms.
The diagram also shows the effect of the kernel’s
penalty for gender mismatches, since females pair
more strongly with females and males with males.
Character roles have relatively little impact.
The first-order kernel c1 (middle), which takes
into account frequency and emotion as well as un-
igrams, is much better than d at distinguishing
places from real characters, and assigns somewhat
more weight to protagonists.
Finally, c2 (bottom), which takes into account
second-order relationships, places much more
emphasis on female protagonists and much less
on places. This is presumably because the female
protagonists of Jane Austen’s novels are the view-
point characters, and the novels focus on their re-
lationships, while characters do not tend to have
strong relationships with places. An increased
tendency to match male marriageable characters
with marriageable females, and “other” males
with “other” females, suggests that c2 relies more
on character function and less on unigrams than
c1 when finding correspondences between char-
acters.
As we concluded in the previous section, the
frequent confusion between categories suggests
that the analogies we construct are relatively non-
specific. We might hope to create role-based sum-
mary of novels by finding their nearest neighbors
and then propagating the character categories (for
example, “ is the protagonist of this novel. She
lives at . She eventually marries , her other
suitors are and her older guardian is .”)
but the present system is probably not adequate
for the purpose. We expect that detecting a fine-
grained set of emotions will help to separate char-
acter functions more clearly.
</bodyText>
<page confidence="0.992841">
640
</page>
<figure confidence="0.4632305">
10 Conclusions
Second-order (c2)
</figure>
<figureCaption confidence="0.996349666666667">
Figure 2: Affinity diagrams showing character types
contributing to the kernel similarity between four
works by Jane Austen.
</figureCaption>
<bodyText confidence="0.999959428571429">
This work presents a method for describing nov-
elistic plots at an abstract level. It has three main
contributions: the description of a plot in terms
of analogies between characters, the use of emo-
tional and frequency trajectories for individual
characters rather than whole works, and evalua-
tion using artificially disordered surrogate novels.
In future work, we hope to sharpen the analogies
we construct so that they are useful for summa-
rization, perhaps by finding an external standard
by which we can make the notion of “analogous”
characters precise. We would also like to investi-
gate what gains are possible with a finer-grained
emotional vocabulary.
</bodyText>
<sectionHeader confidence="0.99838" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999757">
Thanks to Sharon Goldwater, Mirella Lapata, Vic-
toria Adams and the ProbModels group for their
comments on preliminary versions of this work,
Kira Mour˜ao for suggesting graph kernels, and
three reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.998171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988649107142857">
Amjad Abu Jbara and Dragomir Radev. 2011. Coher-
ent citation-based summarization of scientific pa-
pers. In Proceedings of ACL 2011, Portland, Ore-
gon.
Cecilia Ovesdotter Alm and Richard Sproat. 2005.
Emotional sequencing and development in fairy
tales. In ACII, pages 668–674.
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: Machine learn-
ing for text-based emotion prediction. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 579–586, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Regina Barzilay and Mirella Lapata. 2005. Model-
ing local coherence: an entity-based approach. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL’05).
Indrajit Bhattacharya and Lise Getoor. 2005. Rela-
tional clustering for multi-type entity resolution. In
Proceedings of the 4th international workshop on
Multi-relational mining, MRDM ’05, pages 3–12,
New York, NY, USA. ACM.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
</reference>
<figure confidence="0.815731">
Character frequency by category
Unigram features (d)
First-order (c1)
</figure>
<page confidence="0.971976">
641
</page>
<reference confidence="0.998605184210527">
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 602–610,
Suntec, Singapore, August. Association for Com-
putational Linguistics.
Eugene Charniak. 2001. Unsupervised learning of
name structure from coreference data. In Second
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (NACL-01).
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of docu-
ment structure using latent permutations. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 371–379, Boulder, Colorado, June.
Association for Computational Linguistics.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambigua-
tion for knowledge base population. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 277–
285, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
David K. Elson and Kathleen R. McKeown. 2010.
Building a bank of semantically encoded narratives.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh con-
ference on International Language Resources and
Evaluation (LREC’10), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).
David Elson, Nicholas Dames, and Kathleen McKe-
own. 2010. Extracting social networks from liter-
ary fiction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 138–147, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daume III. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 77–86, Cambridge, MA, Octo-
ber. Association for Computational Linguistics.
Arthur Gretton, Karsten M. Borgwardt, Malte Rasch,
Bernhard Schlkopf, and Alexander J. Smola. 2007.
A kernel method for the two-sample-problem. In
B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems
19, pages 513–520. MIT Press, Cambridge, MA.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, Computer Science Department, UC Santa Cruz.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence. In ACL, pages 391–
398.
Anna Kazantseva and Stan Szpakowicz. 2010. Sum-
marizing short stories. Computational Linguistics,
pages 71–109.
Dekang Lin and Patrick Pantel. 2001. Induction of
semantic classes from natural language text. In
Proceedings of the seventh ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, KDD ’01, pages 317–322, New York, NY,
USA. ACM.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152–159.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1562–1572, Uppsala, Sweden, July. Association for
Computational Linguistics.
G. Miller, A.R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: an on-
line lexical database. International Journal of Lexi-
cography, 3(4).
Saif Mohammad. 2011. From once upon a time
to happily ever after: Tracking emotions in novels
and fairy tales. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 105–114, Portland, OR, USA, June. Associa-
tion for Computational Linguistics.
Matt Post. 2011. Judging grammaticality with tree
substitution grammar derivations. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 217–222, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997.
A maximum entropy approach to identifying sen-
tence boundaries. In Proceedings of the Fifth Con-
ference on Applied Natural Language Processing,
pages 16–19, Washington D.C.
Ekaterina P. Volkova, Betty Mohler, Detmar Meur-
ers, Dale Gerdemann, and Heinrich H. B¨ulthoff.
2010. Emotional perception of fairy tales: Achiev-
ing agreement in emotion annotation of text. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, pages 98–106, Los Ange-
les, CA, June. Association for Computational Lin-
guistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
</reference>
<page confidence="0.977032">
642
</page>
<reference confidence="0.997873">
Processing, pages 347–354, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.946846">
643
</page>
<reference confidence="0.981907193548387">
Dev set (11 works)
Emma, Mansfield Park, Northanger Bront¨e, Emily
Abbey, Persuasion, Pride and Prej-
udice, Sense and Sensibility
Cecilia (1782) Hardy
The Ambassadors Scott
Test set (30 works)
Aurora Floyd Bront¨e, Anne
Jane Eyre, Villette Bulwer-Lytton
Coningsby, Tancred Edgeworth
Adam Bede, Daniel Deronda, Mid- Gaskell
dlemarch Hardy
In the Year of Jubilee, New Grub Meredith
Street Thackeray
The Wings of the Dove
The Bride of Lammermoor
Doctor Thorne
Wuthering Heights
Tess of the D’Urbervilles
Ivanhoe
The Tenant of Wildfell Hall
Zanoni
The Absentee, Belinda, Helen
Mary Barton, North and South
Far From the Madding Crowd, Jude
the Obscure, Return of the Native,
Under the Greenwood Tree
The Egoist, The Ordeal of Richard
Feverel
History of Henry Esmond, History
of Pendennis, Vanity Fair
</reference>
<figure confidence="0.981010730769231">
A List of texts
Austen
Burney
James
Braddon
Bront¨e, Charlotte
Disraeli
Eliot
Gissing
James
Scott
Trollope
Out-of-domain set (10 works)
Paul Clifford
The Moonstone
Tom Brown’s Schooldays
Dracula
Ainsworth
Dickens
Conan-Doyle
Stevenson
The Lancashire Witches Bulwer-Lytton
Oliver Twist, The Pickwick Papers Collins
A Study in Scarlet, The Sign of the Hughes
Four Stoker
Treasure Island
</figure>
<tableCaption confidence="0.99382">
Table 4: 19th century novels used in our study.
</tableCaption>
<page confidence="0.997482">
644
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937492">
<title confidence="0.994235">Character-based Kernels for Novelistic Plot Structure</title>
<author confidence="0.985641">Micha</author>
<affiliation confidence="0.998855">Institute for Language, Cognition and Computation School of University of</affiliation>
<email confidence="0.994042">melsner0@gmail.com</email>
<abstract confidence="0.998040809523809">Better representations of plot structure could greatly improve computational methods for summarizing and generating stories. Current representations lack abstraction, focusing too closely on events. We present a kernel for comparing novelistic plots at a higher level, in terms of the cast of characters they depict and the social relationships between them. Our kernel compares the characters of different novels to one another by measuring their frequency of occurrence over time and the descriptive and emotional language associated with them. Given a corpus of 19thcentury novels as training data, our method can accurately distinguish held-out novels in their original form from artificially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu Jbara</author>
<author>Dragomir Radev</author>
</authors>
<title>Coherent citation-based summarization of scientific papers.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL 2011,</booktitle>
<location>Portland, Oregon.</location>
<contexts>
<context position="1266" citStr="Jbara and Radev, 2011" startWordPosition="187" endWordPosition="190">r frequency of occurrence over time and the descriptive and emotional language associated with them. Given a corpus of 19thcentury novels as training data, our method can accurately distinguish held-out novels in their original form from artificially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure. 1 Introduction Every culture has stories, and storytelling is one of the key functions of human language. Yet while we have robust, flexible models for the structure of informative documents (for instance (Chen et al., 2009; Abu Jbara and Radev, 2011)), current approaches have difficulty representing the narrative structure of fictional stories. This causes problems for any task requiring us to model fiction, including summarization and generation of stories; Kazantseva and Szpakowicz (2010) show that state-of-the-art summarizers perform extremely poorly on short fictional texts1. A major problem with applying models for informative 1Apart from Kazantseva, we know of one other attempt to apply a modern summarizer to fiction, by the artist Jason Huff, using Microsoft Word 2008’s extractive summary feature: http://jason-huff.com/ text to fic</context>
</contexts>
<marker>Jbara, Radev, 2011</marker>
<rawString>Amjad Abu Jbara and Dragomir Radev. 2011. Coherent citation-based summarization of scientific papers. In Proceedings of ACL 2011, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecilia Ovesdotter Alm</author>
<author>Richard Sproat</author>
</authors>
<title>Emotional sequencing and development in fairy tales.</title>
<date>2005</date>
<booktitle>In ACII,</booktitle>
<pages>668--674</pages>
<contexts>
<context position="4972" citStr="Alm and Sproat (2005)" startWordPosition="761" endWordPosition="764">rk Some recent work on story understanding has focused on directly modeling the series of events that occur in the narrative. McIntyre and Lapata (2010) create a story generation system that draws on earlier work on narrative schemas (Chambers and Jurafsky, 2009). Their system ensures that generated stories contain plausible event-to-event transitions and are coherent. Since it focuses only on events, however, it cannot enforce a global notion of what the characters want or how they relate to one another. Our own work draws on representations that explicitly model emotions rather than events. Alm and Sproat (2005) were the first to describe stories in terms of an emotional trajectory. They annotate emotional states in 22 Grimms’ fairy tales and discover an increase in emotion (mostly positive) toward the ends of stories. They later use this corpus to construct a reasonably accurate classifier for emotional states of sentences (Alm et al., 2005). Volkova et al. (2010) extend the human annotation approach using a larger number of emotion categories and applying them to freelydefined chunks instead of sentences. The largestscale emotional analysis is performed by Mohammad (2011), using crowd-sourcing to c</context>
</contexts>
<marker>Alm, Sproat, 2005</marker>
<rawString>Cecilia Ovesdotter Alm and Richard Sproat. 2005. Emotional sequencing and development in fairy tales. In ACII, pages 668–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecilia Ovesdotter Alm</author>
<author>Dan Roth</author>
<author>Richard Sproat</author>
</authors>
<title>Emotions from text: Machine learning for text-based emotion prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>579--586</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="5309" citStr="Alm et al., 2005" startWordPosition="819" endWordPosition="822"> transitions and are coherent. Since it focuses only on events, however, it cannot enforce a global notion of what the characters want or how they relate to one another. Our own work draws on representations that explicitly model emotions rather than events. Alm and Sproat (2005) were the first to describe stories in terms of an emotional trajectory. They annotate emotional states in 22 Grimms’ fairy tales and discover an increase in emotion (mostly positive) toward the ends of stories. They later use this corpus to construct a reasonably accurate classifier for emotional states of sentences (Alm et al., 2005). Volkova et al. (2010) extend the human annotation approach using a larger number of emotion categories and applying them to freelydefined chunks instead of sentences. The largestscale emotional analysis is performed by Mohammad (2011), using crowd-sourcing to construct a large emotional lexicon with which he analyzes adult texts such as plays and novels. In this work, we adopt the concept of emotional trajectory, but apply it to particular characters rather than works as a whole. In focusing on characters, we follow Elson et al. (2010), who analyze narratives by examining their social networ</context>
<context position="13449" citStr="Alm et al., 2005" startWordPosition="2147" endWordPosition="2150">Miss Elizabeth Bennet Cross freq x Mr. Darcy Figure 1: Normalized frequency and emotions associated with “Miss Elizabeth Bennet”, protagonist of Pride and Prejudice, and frequency of paragraphs about her and “Mr. Darcy”, smoothed and projected onto 50 basis points. 4.3 Temporal relationships We record two time-varying features for each character, each taking one value per chapter. The first is the character’s frequency as a proportion of all character mentions in the chapter. The second is the frequency with which the character is associated with emotional language—their emotional trajectory (Alm et al., 2005). We use the strong subjectivity cues from the lexicon of Wilson et al. (2005) as a measurement of emotion. If, in a particular paragraph, only one character is mentioned, we count all emotional words in that paragraph and add them to the character’s total. To render the numbers comparable across works, each paragraph subtotal is normalized by the amount of emotional language in the novel as a whole. Then the chapter score is the average over paragraphs. For pairwise character relationships, we count the number of paragraphs in which only two characters are mentioned, and treat this number (as</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat. 2005. Emotions from text: Machine learning for text-based emotion prediction. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 579–586, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</booktitle>
<contexts>
<context position="7623" citStr="Barzilay and Lapata, 2005" startWordPosition="1182" endWordPosition="1185">spoilerfree” summaries focusing on characters, settings and themes, in order to attract potential readers. They do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages. They compare their genrespecific system with a few state-of-the-art methods for summarizing news, and find it outperforms them substantially. We evaluate our system by comparing real novels to artificially produced surrogates, a procedure previously used to evaluate models of discourse coherence (Karamanis et al., 2004; Barzilay and Lapata, 2005) and models of syntax (Post, 2011). As in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities. 3 Dataset We focus on the 19th century novel, partly following Elson et al. (2010) and partly because these texts are freely available via Project Gutenberg. Our main dataset is composed of romances (which we loosely define as novels focusing on a courtship or love affair). We select 41 texts, taking 11 as a development set and the remaining 30 as a test set; a compl</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Indrajit Bhattacharya</author>
<author>Lise Getoor</author>
</authors>
<title>Relational clustering for multi-type entity resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th international workshop on Multi-relational mining, MRDM ’05,</booktitle>
<pages>3--12</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9576" citStr="Bhattacharya and Getoor, 2005" startWordPosition="1512" endWordPosition="1515">with the self-trained Charniak parser (McClosky et al., 2006). Next, we extract a list of characters, compute dependency tree-based unigram features for each character, and record character frequencies and relationships over time. 4.1 Identifying characters We create a list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times. Grouping these into a useful character list is a problem of cross-document coreference. Although cross-document coreference has been extensively studied (Bhattacharya and Getoor, 2005) and modern systems can achieve quite high accuracy on the TAC-KBP task, where the list of available entities is given in advance (Dredze et al., 2010), novelistic text poses a significant challenge for the methods normally used. The typical 19th-century novel contains many related characters, often named after one another. There are complicated social conventions determining which titles are used for whom—for instance, the eldest unmarried daughter of a family can be called “Miss Bennet”, while her younger sister must be “Miss Elizabeth Bennet”. And characters often use nicknames, such as “Li</context>
</contexts>
<marker>Bhattacharya, Getoor, 2005</marker>
<rawString>Indrajit Bhattacharya and Lise Getoor. 2005. Relational clustering for multi-type entity resolution. In Proceedings of the 4th international workshop on Multi-relational mining, MRDM ’05, pages 3–12, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>602--610</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="4614" citStr="Chambers and Jurafsky, 2009" startWordPosition="705" endWordPosition="708"> first step toward generation—a recognition model can always be used as part of a generate-and-rank pipeline, and potentially its underlying representation can be used in more sophisticated ways. We show a detailed analysis of the character correspondences discovered by our system, and discuss their potential relevance to summarization, in section 9. 2 Related work Some recent work on story understanding has focused on directly modeling the series of events that occur in the narrative. McIntyre and Lapata (2010) create a story generation system that draws on earlier work on narrative schemas (Chambers and Jurafsky, 2009). Their system ensures that generated stories contain plausible event-to-event transitions and are coherent. Since it focuses only on events, however, it cannot enforce a global notion of what the characters want or how they relate to one another. Our own work draws on representations that explicitly model emotions rather than events. Alm and Sproat (2005) were the first to describe stories in terms of an emotional trajectory. They annotate emotional states in 22 Grimms’ fairy tales and discover an increase in emotion (mostly positive) toward the ends of stories. They later use this corpus to </context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 602–610, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Unsupervised learning of name structure from coreference data.</title>
<date>2001</date>
<booktitle>In Second Meeting of the North American Chapter of the Association for Computational Linguistics (NACL-01).</booktitle>
<contexts>
<context position="11081" citStr="Charniak, 2001" startWordPosition="1748" endWordPosition="1750">xt, we heuristically assign each mention a gender (masculine, feminine or neuter) using a list of gendered titles, then a list of male and female first names2. We then merge mentions where each is longer than one word, the genders do not clash, 2The most frequent names from the 1990 US census. reply left-of-[name] 17 right-of-[name] feel 14 right-of-[name] look 10 right-of-[name] mind 7 right-of-[name] make 7 Table 1: Top five stemmed unigram dependency features for “Miss Elizabeth Bennet”, protagonist of Pride and Prejudice, and their frequencies. and the first and last names are consistent (Charniak, 2001). We then merge single-word mentions with matching multiword mentions if they appear in the same paragraph, or if not, with the multiword mention that occurs in the most paragraphs. When this process ends, we have resolved each mention in the novel to some specific character. As in previous work, we discard very infrequent characters and their mentions. For the reasons stated, this method is errorprone. Our intuition is that the simpler method described in Elson et al. (2010), which merges each mention to the most recent possible coreferent, must be even more so. However, due to the expense of</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Unsupervised learning of name structure from coreference data. In Second Meeting of the North American Chapter of the Association for Computational Linguistics (NACL-01).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harr Chen</author>
<author>S R K Branavan</author>
<author>Regina Barzilay</author>
<author>David R Karger</author>
</authors>
<title>Global models of document structure using latent permutations.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>371--379</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1238" citStr="Chen et al., 2009" startWordPosition="182" endWordPosition="185">other by measuring their frequency of occurrence over time and the descriptive and emotional language associated with them. Given a corpus of 19thcentury novels as training data, our method can accurately distinguish held-out novels in their original form from artificially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure. 1 Introduction Every culture has stories, and storytelling is one of the key functions of human language. Yet while we have robust, flexible models for the structure of informative documents (for instance (Chen et al., 2009; Abu Jbara and Radev, 2011)), current approaches have difficulty representing the narrative structure of fictional stories. This causes problems for any task requiring us to model fiction, including summarization and generation of stories; Kazantseva and Szpakowicz (2010) show that state-of-the-art summarizers perform extremely poorly on short fictional texts1. A major problem with applying models for informative 1Apart from Kazantseva, we know of one other attempt to apply a modern summarizer to fiction, by the artist Jason Huff, using Microsoft Word 2008’s extractive summary feature: http:/</context>
</contexts>
<marker>Chen, Branavan, Barzilay, Karger, 2009</marker>
<rawString>Harr Chen, S.R.K. Branavan, Regina Barzilay, and David R. Karger. 2009. Global models of document structure using latent permutations. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 371–379, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Paul McNamee</author>
<author>Delip Rao</author>
<author>Adam Gerber</author>
<author>Tim Finin</author>
</authors>
<title>Entity disambiguation for knowledge base population.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>277--285</pages>
<location>Beijing, China,</location>
<contexts>
<context position="9727" citStr="Dredze et al., 2010" startWordPosition="1538" endWordPosition="1541">racter, and record character frequencies and relationships over time. 4.1 Identifying characters We create a list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times. Grouping these into a useful character list is a problem of cross-document coreference. Although cross-document coreference has been extensively studied (Bhattacharya and Getoor, 2005) and modern systems can achieve quite high accuracy on the TAC-KBP task, where the list of available entities is given in advance (Dredze et al., 2010), novelistic text poses a significant challenge for the methods normally used. The typical 19th-century novel contains many related characters, often named after one another. There are complicated social conventions determining which titles are used for whom—for instance, the eldest unmarried daughter of a family can be called “Miss Bennet”, while her younger sister must be “Miss Elizabeth Bennet”. And characters often use nicknames, such as “Lizzie”. Our system uses the multi-stage clustering approach outlined in Bhattacharya and Getoor (2005), but with some features specific to 19th century </context>
</contexts>
<marker>Dredze, McNamee, Rao, Gerber, Finin, 2010</marker>
<rawString>Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin. 2010. Entity disambiguation for knowledge base population. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277– 285, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David K Elson</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Building a bank of semantically encoded narratives.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="6391" citStr="Elson and McKeown, 2010" startWordPosition="993" endWordPosition="996">ather than works as a whole. In focusing on characters, we follow Elson et al. (2010), who analyze narratives by examining their social network relationships. They use an automatic method based on quoted speech to find social links between characters in 19th century novels. Their work, designed for computational literary criticism, does not extract any temporal or emotional structure. A few projects attempt to represent story structure in terms of both characters and their emotional states. However, they operate at a very detailed level and so can be applied only to short texts. Scheherazade (Elson and McKeown, 2010) allows human annotators to mark character goals and emotional states in a narrative, and indicate the causal links between them. AESOP (Goyal et al., 2010) attempts to learn a similar structure automatically. AESOP’s accuracy, however, is relatively poor even on short fables, indicating that this fine-grained approach is unlikely to be scalable to novel-length texts; our system relies on a much coarser analysis. Kazantseva and Szpakowicz (2010) summarize short stories, although unlike the other projects we discuss here, they explicitly try to avoid giving away plot details—their goal is to cr</context>
</contexts>
<marker>Elson, McKeown, 2010</marker>
<rawString>David K. Elson and Kathleen R. McKeown. 2010. Building a bank of semantically encoded narratives. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elson</author>
<author>Nicholas Dames</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting social networks from literary fiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>138--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5852" citStr="Elson et al. (2010)" startWordPosition="908" endWordPosition="911">bly accurate classifier for emotional states of sentences (Alm et al., 2005). Volkova et al. (2010) extend the human annotation approach using a larger number of emotion categories and applying them to freelydefined chunks instead of sentences. The largestscale emotional analysis is performed by Mohammad (2011), using crowd-sourcing to construct a large emotional lexicon with which he analyzes adult texts such as plays and novels. In this work, we adopt the concept of emotional trajectory, but apply it to particular characters rather than works as a whole. In focusing on characters, we follow Elson et al. (2010), who analyze narratives by examining their social network relationships. They use an automatic method based on quoted speech to find social links between characters in 19th century novels. Their work, designed for computational literary criticism, does not extract any temporal or emotional structure. A few projects attempt to represent story structure in terms of both characters and their emotional states. However, they operate at a very detailed level and so can be applied only to short texts. Scheherazade (Elson and McKeown, 2010) allows human annotators to mark character goals and emotiona</context>
<context position="7936" citStr="Elson et al. (2010)" startWordPosition="1238" endWordPosition="1241">th a few state-of-the-art methods for summarizing news, and find it outperforms them substantially. We evaluate our system by comparing real novels to artificially produced surrogates, a procedure previously used to evaluate models of discourse coherence (Karamanis et al., 2004; Barzilay and Lapata, 2005) and models of syntax (Post, 2011). As in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities. 3 Dataset We focus on the 19th century novel, partly following Elson et al. (2010) and partly because these texts are freely available via Project Gutenberg. Our main dataset is composed of romances (which we loosely define as novels focusing on a courtship or love affair). We select 41 texts, taking 11 as a development set and the remaining 30 as a test set; a complete list is given in Appendix A. We focus on the novels used in Elson et al. (2010), but in some cases add additional romances by an already-included author. We also selected 10 of the least romantic works as an outof-domain set; experiments on these are in section 8. 635 4 Preprocessing In order to compare two </context>
<context position="11561" citStr="Elson et al. (2010)" startWordPosition="1827" endWordPosition="1830"> Elizabeth Bennet”, protagonist of Pride and Prejudice, and their frequencies. and the first and last names are consistent (Charniak, 2001). We then merge single-word mentions with matching multiword mentions if they appear in the same paragraph, or if not, with the multiword mention that occurs in the most paragraphs. When this process ends, we have resolved each mention in the novel to some specific character. As in previous work, we discard very infrequent characters and their mentions. For the reasons stated, this method is errorprone. Our intuition is that the simpler method described in Elson et al. (2010), which merges each mention to the most recent possible coreferent, must be even more so. However, due to the expense of annotation, we make no attempt to compare these methods directly. 4.2 Unigram character features Once we have obtained the character list, we use the dependency relationships extracted from our parse trees to compute features for each character. Similar feature sets are used in previous work in word classification, such as (Lin and Pantel, 2001). A few example features are shown in Table 1. To find the features, we take each mention in the corpus and count up all the words o</context>
<context position="14169" citStr="Elson et al. (2010)" startWordPosition="2269" endWordPosition="2273">otion. If, in a particular paragraph, only one character is mentioned, we count all emotional words in that paragraph and add them to the character’s total. To render the numbers comparable across works, each paragraph subtotal is normalized by the amount of emotional language in the novel as a whole. Then the chapter score is the average over paragraphs. For pairwise character relationships, we count the number of paragraphs in which only two characters are mentioned, and treat this number (as a proportion of the total) as a measurement of the strength of the relationship between that pair3. Elson et al. (2010) show that their method of finding conversations between characters is more precise in showing whether a relationship exists, but the co-occurrence technique is simpler, and we 3We tried also counting emotional language in these paragraphs, but this did not seem to help in development experiments. care mostly about the strength of key relationships rather than the existence of infrequent ones. Finally, we perform some smoothing, by taking a weighted moving average of each feature value with a window of the three values on either side. Then, in order to make it easy to compare books with differ</context>
</contexts>
<marker>Elson, Dames, McKeown, 2010</marker>
<rawString>David Elson, Nicholas Dames, and Kathleen McKeown. 2010. Extracting social networks from literary fiction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 138–147, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Ellen Riloff</author>
<author>Hal Daume</author>
</authors>
<title>Automatically producing plot unit representations for narrative text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>77--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="6547" citStr="Goyal et al., 2010" startWordPosition="1018" endWordPosition="1021">ey use an automatic method based on quoted speech to find social links between characters in 19th century novels. Their work, designed for computational literary criticism, does not extract any temporal or emotional structure. A few projects attempt to represent story structure in terms of both characters and their emotional states. However, they operate at a very detailed level and so can be applied only to short texts. Scheherazade (Elson and McKeown, 2010) allows human annotators to mark character goals and emotional states in a narrative, and indicate the causal links between them. AESOP (Goyal et al., 2010) attempts to learn a similar structure automatically. AESOP’s accuracy, however, is relatively poor even on short fables, indicating that this fine-grained approach is unlikely to be scalable to novel-length texts; our system relies on a much coarser analysis. Kazantseva and Szpakowicz (2010) summarize short stories, although unlike the other projects we discuss here, they explicitly try to avoid giving away plot details—their goal is to create “spoilerfree” summaries focusing on characters, settings and themes, in order to attract potential readers. They do find it useful to detect character </context>
</contexts>
<marker>Goyal, Riloff, Daume, 2010</marker>
<rawString>Amit Goyal, Ellen Riloff, and Hal Daume III. 2010. Automatically producing plot unit representations for narrative text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 77–86, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Gretton</author>
<author>Karsten M Borgwardt</author>
<author>Malte Rasch</author>
<author>Bernhard Schlkopf</author>
<author>Alexander J Smola</author>
</authors>
<title>A kernel method for the two-sample-problem. In</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>513--520</pages>
<editor>B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="22024" citStr="Gretton et al., 2007" startWordPosition="3610" endWordPosition="3613">tionships, which carry the richest temporal informa4The baseline detects reversals as well as the plot kernels given only a single point of comparison, but these results do not transfer to the WNN strategy. This suggests that unlike the plot kernels, the baseline is no more accurate for documents it considers similar than for those it judges are distant. tion. 7 Significance testing In addition to using our kernel as a classifier, we can directly test its ability to distinguish real from altered novels via a non-parametric two-sample significance test, the Maximum Mean Discrepancy (MMD) test (Gretton et al., 2007). Given samples from a pair of distributions p and q and a kernel k, this test determines whether the null hypothesis that p and q are identically distributed in the kernel’s feature space can be rejected. The advantage of this test is that, since it takes all pairwise comparisons (except self-comparisons) within and across the classes into account, it uses more information than our classification experiments, and can therefore be more sensitive. As in Gretton et al. (2007), we find an unbiased estimate of the test statistic MMD2 for sample sets x — p, y — q, each with m samples, by pairing th</context>
</contexts>
<marker>Gretton, Borgwardt, Rasch, Schlkopf, Smola, 2007</marker>
<rawString>Arthur Gretton, Karsten M. Borgwardt, Malte Rasch, Bernhard Schlkopf, and Alexander J. Smola. 2007. A kernel method for the two-sample-problem. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 513–520. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical Report UCSC-CRL-99-10,</tech>
<institution>Computer Science Department, UC Santa Cruz.</institution>
<contexts>
<context position="15152" citStr="Haussler, 1999" startWordPosition="2444" endWordPosition="2445">xistence of infrequent ones. Finally, we perform some smoothing, by taking a weighted moving average of each feature value with a window of the three values on either side. Then, in order to make it easy to compare books with different numbers of chapters, we linearly interpolate each series of points into a curve and project it onto a fixed basis of 50 evenly spaced points. An example of the final output is shown in Figure 1. 5 Kernels Our plot kernel k(x, y) measures the similarity between two novels x and y in terms of the features computed above. It takes the form of a convolution kernel (Haussler, 1999) where the “parts” of each novel are its characters u E x, v E y and c is a kernel over characters: c(u, v) (1) We begin by constructing a first-order kernel over characters, c1(u, v), which is defined in terms of a kernel d over the unigram features and a kernel a over the single-character temporal features. We represent the unigram feature counts as distributions pu(w) and pv(w), and compute their similarity as the amount of shared mass, times a small penalty of .1 for mismatched genders: � k(x, y) � uEx � vEy 637 d(pu, pv) = exp(−α(1 − E7„ min(pu(w), pv(w)) x.1 ff{genu = genv} We compute si</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10, Computer Science Department, UC Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
<author>Massimo Poesio</author>
<author>Chris Mellish</author>
<author>Jon Oberlander</author>
</authors>
<title>Evaluating centeringbased metrics of coherence.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>391--398</pages>
<contexts>
<context position="7595" citStr="Karamanis et al., 2004" startWordPosition="1178" endWordPosition="1181">heir goal is to create “spoilerfree” summaries focusing on characters, settings and themes, in order to attract potential readers. They do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages. They compare their genrespecific system with a few state-of-the-art methods for summarizing news, and find it outperforms them substantially. We evaluate our system by comparing real novels to artificially produced surrogates, a procedure previously used to evaluate models of discourse coherence (Karamanis et al., 2004; Barzilay and Lapata, 2005) and models of syntax (Post, 2011). As in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities. 3 Dataset We focus on the 19th century novel, partly following Elson et al. (2010) and partly because these texts are freely available via Project Gutenberg. Our main dataset is composed of romances (which we loosely define as novels focusing on a courtship or love affair). We select 41 texts, taking 11 as a development set and the remaini</context>
</contexts>
<marker>Karamanis, Poesio, Mellish, Oberlander, 2004</marker>
<rawString>Nikiforos Karamanis, Massimo Poesio, Chris Mellish, and Jon Oberlander. 2004. Evaluating centeringbased metrics of coherence. In ACL, pages 391– 398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Kazantseva</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Summarizing short stories. Computational Linguistics,</title>
<date>2010</date>
<pages>71--109</pages>
<contexts>
<context position="1511" citStr="Kazantseva and Szpakowicz (2010)" startWordPosition="220" endWordPosition="223">om artificially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure. 1 Introduction Every culture has stories, and storytelling is one of the key functions of human language. Yet while we have robust, flexible models for the structure of informative documents (for instance (Chen et al., 2009; Abu Jbara and Radev, 2011)), current approaches have difficulty representing the narrative structure of fictional stories. This causes problems for any task requiring us to model fiction, including summarization and generation of stories; Kazantseva and Szpakowicz (2010) show that state-of-the-art summarizers perform extremely poorly on short fictional texts1. A major problem with applying models for informative 1Apart from Kazantseva, we know of one other attempt to apply a modern summarizer to fiction, by the artist Jason Huff, using Microsoft Word 2008’s extractive summary feature: http://jason-huff.com/ text to fiction is that the most important structure underlying the narrative—its plot—occurs at a high level of abstraction, while the actual narration is of a series of lower-level events. A short synopsis of Jane Austen’s novel Pride and Prejudice, for </context>
<context position="6840" citStr="Kazantseva and Szpakowicz (2010)" startWordPosition="1063" endWordPosition="1066">n terms of both characters and their emotional states. However, they operate at a very detailed level and so can be applied only to short texts. Scheherazade (Elson and McKeown, 2010) allows human annotators to mark character goals and emotional states in a narrative, and indicate the causal links between them. AESOP (Goyal et al., 2010) attempts to learn a similar structure automatically. AESOP’s accuracy, however, is relatively poor even on short fables, indicating that this fine-grained approach is unlikely to be scalable to novel-length texts; our system relies on a much coarser analysis. Kazantseva and Szpakowicz (2010) summarize short stories, although unlike the other projects we discuss here, they explicitly try to avoid giving away plot details—their goal is to create “spoilerfree” summaries focusing on characters, settings and themes, in order to attract potential readers. They do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages. They compare their genrespecific system with a few state-of-the-art methods for summarizing news, and find it outperforms them substantially. We evaluate our system </context>
</contexts>
<marker>Kazantseva, Szpakowicz, 2010</marker>
<rawString>Anna Kazantseva and Stan Szpakowicz. 2010. Summarizing short stories. Computational Linguistics, pages 71–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Induction of semantic classes from natural language text.</title>
<date>2001</date>
<booktitle>In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’01,</booktitle>
<pages>317--322</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="12029" citStr="Lin and Pantel, 2001" startWordPosition="1904" endWordPosition="1907">aracters and their mentions. For the reasons stated, this method is errorprone. Our intuition is that the simpler method described in Elson et al. (2010), which merges each mention to the most recent possible coreferent, must be even more so. However, due to the expense of annotation, we make no attempt to compare these methods directly. 4.2 Unigram character features Once we have obtained the character list, we use the dependency relationships extracted from our parse trees to compute features for each character. Similar feature sets are used in previous work in word classification, such as (Lin and Pantel, 2001). A few example features are shown in Table 1. To find the features, we take each mention in the corpus and count up all the words outside the mention which depend on the mention head, except proper nouns and stop words. We also count the mention’s own head word, and mark whether it appears to the right or the left (in general, this word is a verb and the direction reflects the mention’s role as subject or object). We lemmatize all feature words with the WordNet (Miller et al., 1990) stemmer. The resulting distribution over words is our set of unigram features for the character. (We do not pru</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Induction of semantic classes from natural language text. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’01, pages 317–322, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="9007" citStr="McClosky et al., 2006" startWordPosition="1429" endWordPosition="1432">lso selected 10 of the least romantic works as an outof-domain set; experiments on these are in section 8. 635 4 Preprocessing In order to compare two texts, we must first extract the characters in each and some features of their relationships with one another. Our first step is to split the text into chapters, and each chapter into paragraphs; if the text contains a running dialogue where each line begins with a quotation mark, we append it to the previous paragraph. We segment each paragraph with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse it with the self-trained Charniak parser (McClosky et al., 2006). Next, we extract a list of characters, compute dependency tree-based unigram features for each character, and record character frequencies and relationships over time. 4.1 Identifying characters We create a list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times. Grouping these into a useful character list is a problem of cross-document coreference. Although cross-document coreference has been extensively studied (Bhattacharya and Getoor, 2005) and modern systems can achieve</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil McIntyre</author>
<author>Mirella Lapata</author>
</authors>
<title>Plot induction and evolutionary search for story generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1562--1572</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="4503" citStr="McIntyre and Lapata (2010)" startWordPosition="688" endWordPosition="691">for Computational Linguistics of recognizing acceptable novels (section 6), but recognition is usually a good first step toward generation—a recognition model can always be used as part of a generate-and-rank pipeline, and potentially its underlying representation can be used in more sophisticated ways. We show a detailed analysis of the character correspondences discovered by our system, and discuss their potential relevance to summarization, in section 9. 2 Related work Some recent work on story understanding has focused on directly modeling the series of events that occur in the narrative. McIntyre and Lapata (2010) create a story generation system that draws on earlier work on narrative schemas (Chambers and Jurafsky, 2009). Their system ensures that generated stories contain plausible event-to-event transitions and are coherent. Since it focuses only on events, however, it cannot enforce a global notion of what the characters want or how they relate to one another. Our own work draws on representations that explicitly model emotions rather than events. Alm and Sproat (2005) were the first to describe stories in terms of an emotional trajectory. They annotate emotional states in 22 Grimms’ fairy tales a</context>
</contexts>
<marker>McIntyre, Lapata, 2010</marker>
<rawString>Neil McIntyre and Mirella Lapata. 2010. Plot induction and evolutionary search for story generation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1562–1572, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>A R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: an online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="12517" citStr="Miller et al., 1990" startWordPosition="1996" endWordPosition="1999">e features for each character. Similar feature sets are used in previous work in word classification, such as (Lin and Pantel, 2001). A few example features are shown in Table 1. To find the features, we take each mention in the corpus and count up all the words outside the mention which depend on the mention head, except proper nouns and stop words. We also count the mention’s own head word, and mark whether it appears to the right or the left (in general, this word is a verb and the direction reflects the mention’s role as subject or object). We lemmatize all feature words with the WordNet (Miller et al., 1990) stemmer. The resulting distribution over words is our set of unigram features for the character. (We do not prune rare features, although they have proportionally little influence on our measurement of similarity.) 636 0.6 0.4 0.0 0.8 0.2 1.6 1.4 1.0 1.2 0 10 20 30 40 50 Freq of Miss Elizabeth Bennet Emotions of Miss Elizabeth Bennet Cross freq x Mr. Darcy Figure 1: Normalized frequency and emotions associated with “Miss Elizabeth Bennet”, protagonist of Pride and Prejudice, and frequency of paragraphs about her and “Mr. Darcy”, smoothed and projected onto 50 basis points. 4.3 Temporal relati</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G. Miller, A.R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Introduction to WordNet: an online lexical database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
</authors>
<title>From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th ACLHLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities,</booktitle>
<pages>105--114</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, OR, USA,</location>
<contexts>
<context position="5545" citStr="Mohammad (2011)" startWordPosition="858" endWordPosition="860"> rather than events. Alm and Sproat (2005) were the first to describe stories in terms of an emotional trajectory. They annotate emotional states in 22 Grimms’ fairy tales and discover an increase in emotion (mostly positive) toward the ends of stories. They later use this corpus to construct a reasonably accurate classifier for emotional states of sentences (Alm et al., 2005). Volkova et al. (2010) extend the human annotation approach using a larger number of emotion categories and applying them to freelydefined chunks instead of sentences. The largestscale emotional analysis is performed by Mohammad (2011), using crowd-sourcing to construct a large emotional lexicon with which he analyzes adult texts such as plays and novels. In this work, we adopt the concept of emotional trajectory, but apply it to particular characters rather than works as a whole. In focusing on characters, we follow Elson et al. (2010), who analyze narratives by examining their social network relationships. They use an automatic method based on quoted speech to find social links between characters in 19th century novels. Their work, designed for computational literary criticism, does not extract any temporal or emotional s</context>
</contexts>
<marker>Mohammad, 2011</marker>
<rawString>Saif Mohammad. 2011. From once upon a time to happily ever after: Tracking emotions in novels and fairy tales. In Proceedings of the 5th ACLHLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 105–114, Portland, OR, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
</authors>
<title>Judging grammaticality with tree substitution grammar derivations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>217--222</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="7657" citStr="Post, 2011" startWordPosition="1190" endWordPosition="1191">ttings and themes, in order to attract potential readers. They do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages. They compare their genrespecific system with a few state-of-the-art methods for summarizing news, and find it outperforms them substantially. We evaluate our system by comparing real novels to artificially produced surrogates, a procedure previously used to evaluate models of discourse coherence (Karamanis et al., 2004; Barzilay and Lapata, 2005) and models of syntax (Post, 2011). As in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities. 3 Dataset We focus on the 19th century novel, partly following Elson et al. (2010) and partly because these texts are freely available via Project Gutenberg. Our main dataset is composed of romances (which we loosely define as novels focusing on a courtship or love affair). We select 41 texts, taking 11 as a development set and the remaining 30 as a test set; a complete list is given in Appendix A. W</context>
</contexts>
<marker>Post, 2011</marker>
<rawString>Matt Post. 2011. Judging grammaticality with tree substitution grammar derivations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 217–222, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<location>Washington D.C.</location>
<contexts>
<context position="8932" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="1417" endWordPosition="1420">10), but in some cases add additional romances by an already-included author. We also selected 10 of the least romantic works as an outof-domain set; experiments on these are in section 8. 635 4 Preprocessing In order to compare two texts, we must first extract the characters in each and some features of their relationships with one another. Our first step is to split the text into chapters, and each chapter into paragraphs; if the text contains a running dialogue where each line begins with a quotation mark, we append it to the previous paragraph. We segment each paragraph with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse it with the self-trained Charniak parser (McClosky et al., 2006). Next, we extract a list of characters, compute dependency tree-based unigram features for each character, and record character frequencies and relationships over time. 4.1 Identifying characters We create a list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times. Grouping these into a useful character list is a problem of cross-document coreference. Although cross-document coreference has been extensi</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 16–19, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina P Volkova</author>
<author>Betty Mohler</author>
<author>Detmar Meurers</author>
<author>Dale Gerdemann</author>
<author>Heinrich H B¨ulthoff</author>
</authors>
<title>Emotional perception of fairy tales: Achieving agreement in emotion annotation of text.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>98--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, CA,</location>
<marker>Volkova, Mohler, Meurers, Gerdemann, B¨ulthoff, 2010</marker>
<rawString>Ekaterina P. Volkova, Betty Mohler, Detmar Meurers, Dale Gerdemann, and Heinrich H. B¨ulthoff. 2010. Emotional perception of fairy tales: Achieving agreement in emotion annotation of text. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 98–106, Los Angeles, CA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="13527" citStr="Wilson et al. (2005)" startWordPosition="2161" endWordPosition="2165"> and emotions associated with “Miss Elizabeth Bennet”, protagonist of Pride and Prejudice, and frequency of paragraphs about her and “Mr. Darcy”, smoothed and projected onto 50 basis points. 4.3 Temporal relationships We record two time-varying features for each character, each taking one value per chapter. The first is the character’s frequency as a proportion of all character mentions in the chapter. The second is the frequency with which the character is associated with emotional language—their emotional trajectory (Alm et al., 2005). We use the strong subjectivity cues from the lexicon of Wilson et al. (2005) as a measurement of emotion. If, in a particular paragraph, only one character is mentioned, we count all emotional words in that paragraph and add them to the character’s total. To render the numbers comparable across works, each paragraph subtotal is normalized by the amount of emotional language in the novel as a whole. Then the chapter score is the average over paragraphs. For pairwise character relationships, we count the number of paragraphs in which only two characters are mentioned, and treat this number (as a proportion of the total) as a measurement of the strength of the relationsh</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dev set Emma</author>
<author>Mansfield Park</author>
<author>Northanger Bront¨e</author>
<author>Emily Abbey</author>
<author>Persuasion</author>
</authors>
<date>1782</date>
<booktitle>Pride and Prej- udice, Sense and Sensibility Cecilia</booktitle>
<marker>Emma, Park, Bront¨e, Abbey, Persuasion, 1782</marker>
<rawString>Dev set (11 works) Emma, Mansfield Park, Northanger Bront¨e, Emily Abbey, Persuasion, Pride and Prej- udice, Sense and Sensibility Cecilia (1782)</rawString>
</citation>
<citation valid="false">
<note>Test set (30 works)</note>
<marker></marker>
<rawString>Test set (30 works)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aurora Floyd Jane Eyre</author>
</authors>
<title>Villette Coningsby, Tancred Bront¨e, Anne Bulwer-Lytton Edgeworth Adam Bede, Daniel Deronda, Mid- dlemarch</title>
<booktitle>In the Year of Jubilee, New Grub Street The Wings of the Dove The Bride of Lammermoor Doctor Thorne Gaskell Hardy</booktitle>
<location>Meredith Thackeray</location>
<marker>Eyre, </marker>
<rawString>Aurora Floyd Jane Eyre, Villette Coningsby, Tancred Bront¨e, Anne Bulwer-Lytton Edgeworth Adam Bede, Daniel Deronda, Mid- dlemarch In the Year of Jubilee, New Grub Street The Wings of the Dove The Bride of Lammermoor Doctor Thorne Gaskell Hardy Meredith Thackeray</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>