<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.978195">
Examining the Content Load of Part of Speech Blocks for Information
Retrieval
</title>
<author confidence="0.99761">
Christina Lioma
</author>
<affiliation confidence="0.9978275">
Department of Computing Science
University of Glasgow
</affiliation>
<address confidence="0.938074">
17 Lilybank Gardens
Scotland, U.K.
</address>
<email confidence="0.998895">
xristina@dcs.gla.ac.uk
</email>
<author confidence="0.97661">
Iadh Ounis
</author>
<affiliation confidence="0.9976175">
Department of Computing Science
University of Glasgow
</affiliation>
<address confidence="0.938061">
17 Lilybank Gardens
Scotland, U.K.
</address>
<email confidence="0.999124">
ounis@dcs.gla.ac.uk
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937741935484">
We investigate the connection between
part of speech (POS) distribution and con-
tent in language. We define POS blocks
to be groups of parts of speech. We hypo-
thesise that there exists a directly propor-
tional relation between the frequency of
POS blocks and their content salience. We
also hypothesise that the class membership
of the parts of speech within such blocks
reflects the content load of the blocks, on
the basis that open class parts of speech
are more content-bearing than closed class
parts of speech. We test these hypothe-
ses in the context of Information Retrieval,
by syntactically representing queries, and
removing from them content-poor blocks,
in line with the aforementioned hypothe-
ses. For our first hypothesis, we induce
POS distribution information from a cor-
pus, and approximate the probability of
occurrence of POS blocks as per two sta-
tistical estimators separately. For our se-
cond hypothesis, we use simple heuristics
to estimate the content load within POS
blocks. We use the Text REtrieval Con-
ference (TREC) queries of 1999 and 2000
to retrieve documents from the WT2G and
WT10G test collections, with five differ-
ent retrieval strategies. Experimental out-
comes confirm that our hypotheses hold in
the context of Information Retrieval.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913414634146">
The task of an Information Retrieval (IR) system
is to retrieve documents from a collection, in re-
sponse to a user need, which is expressed in the
form of a query. Very often, this task is realised
by indexing the documents in the collection with
keyword descriptors. Retrieval consists in match-
ing the query against the descriptors of the do-
cuments, and returning the ones that appear clo-
sest, in ranked lists of relevance (van Rijsbergen,
1979). Usually, the keywords that constitute the
document descriptors are associated with indivi-
dual weights, which capture the importance of the
keywords to the content of the document. Such
weights, commonly referred to as term weights,
can be computed using various term weighting
schemes. Not all words can be used as keyword
descriptors. In fact, a relatively small number of
words accounts for most of a document’s content
(van Rijsbergen, 1979). Function words make
‘noisy’ index terms, and are usually ignored du-
ring the retrieval process. This is practically re-
alised with the use of stopword lists, which are
lists of words to be exempted when indexing the
collection and the queries.
The use of stopword lists in IR is a mani-
festation of a well-known bifurcation in lingui-
stics between open and closed classes of words
(Lyons, 1977). In brief, open class words are
more content-bearing than closed class words. Ge-
nerally, the open class contains parts of speech
that are morphologically and semantically flexi-
ble, while the closed class contains words that pri-
marily perform linguistic well-formedness func-
tions. The membership of the closed class is
mostly fixed and largely restricted to function
words, which are not prone to semantic or mor-
phological alterations.
We define a block of parts of speech (POS
block) as a block of fixed length , where is set
empirically. We define POS block tokens as in-
dividual instances of POS blocks, and POS block
</bodyText>
<page confidence="0.968502">
531
</page>
<note confidence="0.7235865">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 531–538,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99953912">
types as distinct POS blocks in a corpus. The pur-
pose of this paper is to test two hypotheses.
The intuition behind both of these hypotheses is
that, just as individual words can be content-rich
or content-poor, the same can hold for blocks of
parts of speech. According to our first hypothe-
sis, POS blocks can be categorized as content-rich
or content-poor, on the basis of their distribution
within a corpus. Specifically, we hypothesise that
the more frequently a POS block occurs in lan-
guage, the more content it is likely to bear. Ac-
cording to our second hypothesis, POS blocks can
be categorized as content-rich or content-poor, on
the basis of the part of speech class membership of
their individual components. Specifically, we hy-
pothesise that the more closed class components
found in a POS block, the less content the block is
likely to bear.
Both aforementioned hypotheses are evaluated
in the context of IR as follows. We observe the
distribution of POS blocks in a corpus. We create
a list of POS block types with their respective pro-
babilities of occurrence. As a first step, to test our
first hypothesis, we remove the POS blocks with a
low probability of occurrence from each query, on
the assumption that these blocks are content-poor.
The decision regarding the threshold of low
probability of occurrence is realised empirically.
As a second step, we further remove from each
query POS blocks that contain less open class than
closed class components, in order to test the va-
lidity of our second hypothesis, as an extension of
the first hypothesis. We retrieve documents from
two standard IR English test collections, namely
WT2G and WT10G. Both of these collections are
commonly used for retrieval effectiveness evalu-
ations in the Text REtrieval Conference (TREC),
and come with sets of queries and query relevance
assessments&apos;. Query relevance assessments are
lists of relevant documents, given a query. We
retrieve relevant documents using firstly the ori-
ginal queries, secondly the queries produced after
step 1, and thirdly the queries produced after step
2. We use five statistically different term weight-
ing schemes to match the query terms to the docu-
ment keywords, in order to assess our hypotheses
across a range of retrieval techniques. We asso-
ciate improvement of retrieval performance with
successful noise reduction in the queries. We as-
sume noise reduction to reflect the correct iden-
</bodyText>
<footnote confidence="0.940882">
1http://trec.nist.gov/
</footnote>
<bodyText confidence="0.999838285714286">
tification of content-poor blocks, in line with our
hypotheses.
Section 2 presents related studies in this field.
Section 3 introduces our methodology. Section 4
presents the experimental settings used to test our
hypotheses, and their evaluation outcomes. Sec-
tion 5 provides our conclusions and remarks.
</bodyText>
<sectionHeader confidence="0.994245" genericHeader="introduction">
2 Related Studies
</sectionHeader>
<bodyText confidence="0.999926347826087">
We examine the distribution of POS blocks in lan-
guage. This is but one type of language distribu-
tion analysis that can be realised. One can also
examine the distribution of character or word n-
grams, e.g. Language Modeling (Croft and Laf-
ferty, 2003), phrases (Church and Hanks, 1990;
Lewis, 1992), and so on. In class-based n-gram
modeling (Brown et al., 1992) for example, class-
based n-grams are used to determine the probabi-
lity of occurrence of a POS class, given its pre-
ceding classes, and the probability of a particular
word, given its own POS class. Unlike the class-
based n-gram model, we do not use POS blocks to
make predictions. We estimate their probability of
occurrence as blocks, not the individual probabi-
lities of their components, motivated by the intu-
ition that the more frequently a POS block occurs,
the more content it bears. In the context of IR,
efforts have been made to use syntactic informa-
tion to enhance retrieval (Smeaton, 1999; Strza-
lkowski, 1996; Zukerman and Raskutti, 2002), but
not by using POS block-based distribution repre-
sentations.
</bodyText>
<sectionHeader confidence="0.998488" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999945692307692">
We present the steps realised in order to assess
our hypotheses in the context of IR. Firstly, POS
blocks with their respective frequencies are ex-
tracted from a corpus. The probability of occur-
rence of each POS block is statistically estimated.
In order to test our first hypothesis, we remove
from the query all but POS blocks of high probabi-
lity of occurrence, on the assumption that the latter
are content-rich. In order to test our second hypo-
thesis, POS blocks that contain more closed class
than open class tags are removed from the queries,
on the assumption that these blocks are content-
poor.
</bodyText>
<subsectionHeader confidence="0.994184">
3.1 Inducing POS blocks from a corpus
</subsectionHeader>
<bodyText confidence="0.9989765">
We extract POS blocks from a corpus and estimate
their probability of occurrence, as follows.
</bodyText>
<page confidence="0.982906">
532
</page>
<bodyText confidence="0.998996111111111">
The corpus is POS tagged. All lexical word
forms are eliminated. Thus, sentences are consti-
tuted solely by sequences of POS tags. The fol-
lowing example illustrates this point.
[Original sentence] Many of the propos-
als for directives and action programmes
planned by the Commission have for
some obscure reason never seen the light
of day.
</bodyText>
<listItem confidence="0.346706916666667">
[Tagged sentence] Many/JJ of/IN
the/DT proposals/NNS for/IN di-
rectives/NNS and/CC action/NN
programmes/NNS planned/VVN by/IN
the/DT Commission/NP have/VHP
for/IN some/DT obscure/JJ reason/NN
never/RB seen/VVN the/DT light/NN
of/IN day/NN
[Tags-only sentence] JJ IN DT NNS IN
NNS CC NN NNS VVN IN DT NP
VHP IN DT JJ NN RB VVN DT NN
IN NN
</listItem>
<bodyText confidence="0.999766666666667">
For each sentence in the corpus, all possible POS
blocks are extracted. Thus, for a given sentence
ABCDEFGH, where POS tags are denoted by sin-
gle letters, and where POS block length = 4, the
POS blocks extracted are ABCD, BCDE, CDEF,
and so on. The extracted POS blocks overlap. The
order in which the POS blocks occur in the sen-
tence is disregarded.
We statistically infer the probability of occur-
rence of each POS block, on the basis of the indi-
vidual POS block frequencies counted in the cor-
pus. Maximum Likelihood inference is eschewed,
as it assigns the maximum possible likelihood to
the POS blocks observed in the corpus, and no pro-
bability to unseen POS blocks. Instead, we employ
statistical estimation that accounts for unseen POS
blocks, namely Laplace and Good-Turing (Man-
ning and Schutze, 1999).
</bodyText>
<subsectionHeader confidence="0.999344">
3.2 Removing POS blocks from the queries
</subsectionHeader>
<bodyText confidence="0.915590377777778">
In order to test our first hypothesis, POS blocks of
low probability of occurrence are removed from
the queries. Specifically, we POS tag the queries,
and remove the POS blocks that have a probability
of occurrence below an empirical threshold . The
following example illustrates this point.
[Original query] A relevant document
will focus on the causes of the lack of
integration in a significant way; that is,
the mere mention of immigration diffi-
culties is not relevant. Documents that
discuss immigration problems unrelated
to Germany are also not relevant.
[Tags-only query] DT JJ NN MD VV IN
DT NNS IN DT NN IN NN IN DT JJ
NN; WDT VBZ DT JJ NN IN NN NNS
VBZ RB JJ. NNS WDT VVP NN NNS
JJ TO NP VBP RB RB JJ
[Query with high-probability POS
blocks] DT NNS IN DT NN IN NN IN
NN IN NN NNS
[Resulting query] the causes of the lack
of integration in mention of immigration
difficulties
Some of the low-probability POS blocks, which
are removed from the query in the above exam-
ple, are DT JJ NN MD, JJ NN MD VV, NN MD
VV IN, and so on. The resulting query contains
fragments of the original query, assumed to be
content-rich. In the context of the bag-of-words
approach to IR investigated here, the grammatical
well-formedness of the query is thus not an issue
to be considered.
In order to test the second hypothesis, we re-
move from the queries POS blocks that contain
less open class than closed class components. We
propose a simple heuristic Content Load algo-
rithm, to ‘count’ the presence of content within
a POS block, on the premise that open class tags
bear more content than closed class tags. The or-
der of tags within a POS block is ignored. Figure
1 displays our Content Load algorithm.
After the POS block component has been
‘counted’, if the Content Load is zero or more,
we consider the POS block content-rich. If the
</bodyText>
<figureCaption confidence="0.999661">
Figure 1: The Content Load algorithm
</figureCaption>
<bodyText confidence="0.98491">
function CONTENT-LOAD(POSblock)
returns ContentLoad
INITIALISE-FOR-EACH-POSBLOCK(query)
for pos from 1 to POSblock-size do
</bodyText>
<equation confidence="0.9179235">
if(current-tag = = OpenClass)
(ContentLoad)+ +
elseif(current-tag = = ClosedClass)
(ContentLoad)- -
end
return(ContentLoad)
</equation>
<page confidence="0.977492">
533
</page>
<bodyText confidence="0.997906125">
Content Load is strictly less than zero, we con-
sider the POS block content-poor. We assume an
underlying equivalence of content in all open class
parts of speech, which albeit being linguistically
counter-intuitive, is shown to be effective when
applied to IR (Section 4). The following example
illustrates this point. In this example, POS block
length = 4.
</bodyText>
<tableCaption confidence="0.924688">
Table 1: Correspondence between the TreeBank
(TB) and Reduced TreeBank (RTB) tags.
</tableCaption>
<table confidence="0.853425434782609">
TB TBR
JJ, JJR, JJS
RB,RBR,RBS
CD, LS
CC
DT, WDT, PDT
JJ
RB
CD
CC
DT
FW
MD
NN
PP
IN
PO
RP
SY
UH
VB
FW
[Original query] A relevant document
</table>
<bodyText confidence="0.869872833333333">
will focus on the causes of the lack of
integration in a significant way; that is,
the mere mention of immigration diffi-
culties is not relevant. Documents that
discuss immigration problems unrelated
to Germany are also not relevant.
</bodyText>
<listItem confidence="0.531480285714286">
[Tags-only query] DT JJ NN MD VV IN
DT NNS IN DT NN IN NN IN DT JJ
NN; WDT VBZ DT JJ NN IN NN NNS
VBZ RB JJ. NNS WDT VVP NN NNS
JJ TO NP VBP RB RB JJ
[Query with high-probability POS
blocks] DT NNS IN DT NN IN NN IN
NN IN NN NNS
[Content Load of POS blocks]
DT NNS IN DT (-2), NN IN NN IN (0),
NN IN NN NNS (+2)
[Query with high-probability POS
blocks of zero or positive Content Load]
NN IN NN IN NN IN NN NNS
</listItem>
<bodyText confidence="0.632769">
[Resulting query] lack of integration in
mention of immigration difficulties
</bodyText>
<sectionHeader confidence="0.998594" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9998365">
We present the experiments realised to test the two
hypotheses formulated in Section 1. Section 4.1
presents our experimental settings, and Section 4.2
our evaluation results.
</bodyText>
<subsectionHeader confidence="0.970228">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9999178">
We induce POS blocks from the English language
component of the second release of the parallel
Europarl corpus(75MB)2. We POS tag the cor-
pus using the TreeTagger3, which is a probabilis-
tic POS tagger that uses the Penn TreeBank tagset
</bodyText>
<footnote confidence="0.981208666666667">
2http://people.csail.mit.edu/koehn/publications/europarl/
3http://www.ims.uni-stuttgart.de/projekte/corplex/
TreeTagger/
</footnote>
<table confidence="0.622861909090909">
MD, VB, VBD, VBG, VBN,
VBP, VBZ, VH, VHD,
VHG, VHN, VHP, VHZ
NN, NNS, NP, NPS
PP, WP, PP$, WP$, EX, WRB
IN, TO
POS
RP
SYM
UH
VV, VVD, VVG, VVN, VVP, VVZ
</table>
<bodyText confidence="0.998558586206897">
(Marcus et al., 1993). Since we are solely inter-
ested in a POS analysis, we introduce a stage of
tagset simplification, during which, any informa-
tion on top of surface POS classification is lost
(Table 1). Practically, this leads to 48 original
TreeBank (TB) tag classes being narrowed down
to 15 Reduced TreeBank (RTB) tag classes. Ad-
ditionally, tag names are shortened into two-letter
names, for reasons of computational efficiency.
We consider the TBR tags JJ, FW, NN, and VB as
open-class, and the remaining tags as closed class
(Lyons, 1977). We extract 214,398,227 POS block
tokens and 19,343 POS block types from the cor-
pus.
We retrieve relevant documents from two stan-
dard TREC test collections, namely WT2G (2GB)
and WT10G (10GB), from the 1999 and 2000
TREC Web tracks, respectively. We use the
queries 401-450 from the ad-hoc task of the 1999
Web track, for the WT2G test collection, and
the queries 451-500 from the ad-hoc task of the
2000 Web track, for the WT10G test collection,
with their respective relevance assessments. Each
query contains three fields, namely title, descri-
ption, and narrative. The title contains keywords
describing the information need. The description
expands briefly on the information need. The nar-
rative part consists of sentences denoting key con-
cepts to be considered or ignored. We use all three
</bodyText>
<page confidence="0.995809">
534
</page>
<bodyText confidence="0.978420714285714">
query fields to match query terms to document
keyword descriptors, but extract POS blocks only
from the narrative field of the queries. This choice
is motivated by the two following reasons. Firstly,
the narrative includes the longest sentences in the
whole query. For our experiments, longer sen-
tences provide better grounds upon which we can
test our hypotheses, since the longer a sentence,
the more POS blocks we can match within it. Sec-
ondly, the narrative field contains the most noise
in the whole query. Especially when using bag-of-
words term weighting, such as in our evaluation,
information on what is not relevant to the query
only introduces noise. Thus, we select the most
noisy field of the query to test whether the appli-
cation of our hypotheses indeed results in the re-
duction of noise.
During indexing, we remove stopwords, and
stem the collections and the queries, using
Porter’s4 stemming algorithm. We use the Terrier5
IR platform, and apply five different weighting
schemes to match query terms to document de-
scriptors. In IR, term weighting schemes estimate
the relevance of a document for a query
, as: , where is
a term in , is the query term weight, and
is the weight of document for term .
For example, we use the classical TF IDF weight-
ing scheme (Sparck-Jones, 1972; Robertson et
al., 1995): , where
is the normalised term frequency in a document:
; is the frequency of
a term in a document; , and are parameters;
and are the document length and the ave-
rage document length in the collection, respec-
tively; is the number of documents in the collec-
tion; and is the number of documents contain-
ing the term . For all weighting schemes we use,
, where is the query term fre-
quency, and is the maximum among
all query terms. We also use the well-established
probabilistic BM25 weighting scheme (Robertson
et al., 1995), and three distinct weighting schemes
from the more recent Divergence From Random-
ness (DFR) framework (Amati, 2003), namely
BB2, PL2, and DLH. Note that, even though we
use three weighting schemes from the DFR frame-
work, the said schemes are statistically different to
one another. Also, DLH is the only parameter-free
</bodyText>
<footnote confidence="0.999742">
4http://snowball.tartarus.org/
5http://ir.dcs.gla.ac.uk/terrier/
</footnote>
<bodyText confidence="0.999844074074074">
weighting scheme we use, as it computes all of the
variables automatically from the collection
statistics.
We use the default values of all parameters,
namely, for the TF IDF and BM25 weighting
schemes (Robertson et al., 1995),
schemes (Amati, 2003), for the WT2G
test collection, and for the WT10G test
collection. We use default values, instead of tun-
ing the term weighting parameters, because our fo-
cus lies in testing our hypotheses, and not in opti-
mising retrieval performance. If the said param-
eters are optimised, retrieval performance may be
further improved. We measure the retrieval perfor-
mance using the Mean Average Precision (MAP)
measure (van Rijsbergen, 1979).
Throughout all experiments, we set POS block
length at = 4. We employ Good-Turing and
Laplace smoothing, and set the threshold of high
probability of occurrence empirically at = 0.01.
We present all evaluation results in tables, the for-
mat of which is as follows: GT and LA indicate
Good-Turing and Laplace respectively, and
denotes the % difference in MAP from the base-
line. Statistically significant scores, as per the
Wilcoxon test (❝), appear in boldface,
while highest percentages appear in italics.
</bodyText>
<subsectionHeader confidence="0.996938">
4.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999971583333333">
Our retrieval baseline consists in testing the per-
formance of each term weighting scheme, with
each of the two test collections, using the original
queries. We introduce two retrieval combinations
on top of the baseline, which we call POS and
POSC. The POS retrieval experiments, which re-
late to our first hypothesis, and the POSC retrieval
experiments, which relate to our second hypothe-
sis, are described in Section 4.2.1. Section 4.2.2
presents the assessment of our hypotheses using a
performance-boosting retrieval technique, namely
query expansion.
</bodyText>
<subsubsectionHeader confidence="0.613281">
4.2.1 POS and POSC Retrieval Experiments
</subsubsectionHeader>
<bodyText confidence="0.9952581">
The aim of the POS and POSC experiments is to
test our first and second hypotheses, respectively.
Firstly, to test the first hypothesis, namely that
there is a direct connection between the removal
of low-frequency POS blocks from the queries and
noise reduction in the queries, we remove all low-
frequency POS blocks from the narrative field of
,
, and for both test collec-
tions; while for the PL2 and BB2 term weighting
</bodyText>
<page confidence="0.96755">
535
</page>
<bodyText confidence="0.999962931034483">
the queries. Secondly, to test our second hypo-
thesis as an extension of our first hypothesis, we
refilter the queries used in the POS experiments
by removing from them POS blocks that contain
more closed class than open class tags. The pro-
cesses involved in both hypotheses take place prior
to the removal of stop words and stemming of the
queries. Table 2 displays the relevant evaluation
results.
Overall, the removal of low-probability POS
blocks from the queries (Hypothesis 1 section in
Table 2) is associated with an improvement in
retrieval performance over the baseline in most
cases, which sometimes is statistically significant.
This improvement is quite similar across the two
statistical estimators. Moreover, two interest-
ing patterns emerge. Firstly, the DFR weighting
schemes seem to be divided, performance-wise,
between the parametric BB2 and PL2, which are
associated with the highest improvement in re-
trieval performance, and the non-parametric DLH,
which is associated with the lowest improvement,
or even deterioration in retrieval performance.
This may indicate that the parameter used in BB2
and PL2 is not optimal, which would explain a low
baseline, and thus a very high improvement over
it. Secondly, when comparing the improvement in
performance related to the WT2G and the WT10G
test collections, we observe a more marked im-
provement in retrieval performance with WT2G
than with WT10G.
The combination of our two hypotheses (Hy-
potheses 1+2 section in Table 2) is associated
with an improvement in retrieval performance
over the baseline in most cases, which sometimes
is statistically significant. This improvement is
very similar across the two statistical estimators,
namely Good-Turing and Laplace. When com-
bining hypotheses 1+2, retrieval performance im-
proves more than it did for hypothesis 1 only,
for the WT2G test collection, which indicates
that our second hypothesis might further reduce
the amount of noise in the queries successfully.
For the WT10G collection, we object similar re-
sults, with the exception of DLH. Generally, the
improvement in performance associated to the
WT2G test collection is more marked than the im-
provement associated to WT10G.
To recapitulate on the evaluation outcomes of
our two hypotheses, we report an improvement in
retrieval performance over the baseline for most,
but not all cases, which is sometimes statistically
significant. This may be indicative of successful
noise reduction in the queries, as per our hypothe-
ses. Also, the difference in the improvement in re-
trieval performance across the two test collections
may suggest that data sparseness affects retrieval
performance.
</bodyText>
<sectionHeader confidence="0.466461" genericHeader="method">
4.2.2 POS and POSC Retrieval Experiments
with Query Expansion
</sectionHeader>
<bodyText confidence="0.9999815">
Query expansion (QE) is a performance-
boosting technique often used in IR, which con-
sists in extracting the most relevant terms from
the top retrieved documents, and in using these
terms to expand the initial query. The expanded
query is then used to retrieve documents anew.
Query expansion has the distinct property of im-
proving retrieval performance when queries do not
contain noise, but harming retrieval performance
when queries contain noise, furnishing us with a
strong baseline, against which we can measure our
hypotheses. We repeat the experiments described
in Section 4.2.1 with query expansion.
We use the Bo1 query expansion scheme from
the DFR framework (Amati, 2003). We optimise
the query expansion settings, so as to maximise
its performance. This provides us with an even
stronger baseline, against which we can compare
our proposed technique, which we tune empiri-
cally too through the tuning of the threshold . We
optimise query expansion on the basis of the cor-
responding relevance assessments available for the
queries and collections employed, by selecting the
most relevant terms from the top retrieved docu-
ments. For the WT2G test collection, the relevant
terms / top retrieved documents ratio we use is (i)
20/5 with TF IDF, BM25, and DLH; (ii) 30/5 with
PL2; and (iii) 10/5 with BB2. For the WT10G col-
lection, the said ratio is (i) 10/5 for TF IDF; (ii)
20/5 for BM25 and DLH; and (iii) 5/5 for PL2 and
BB2.
We repeat our POS and POSC retrieval experi-
ments with query expansion. Table 3 displays the
relevant evaluation results.
Query expansion has overall improved retrieval
performance (compare Tables 2 and 3), for both
test collections, with two exceptions, where query
expansion has made no difference at all, namely
for BB2 and PL2, with the WT10G collection.
The removal of low-probability POS blocks from
the queries, as per our first hypothesis, combined
with query expansion, is associated with an im-
</bodyText>
<page confidence="0.998658">
536
</page>
<tableCaption confidence="0.996781">
Table 2: Mean Average Precision (MAP) scores of the POS and POSC experiments.
</tableCaption>
<table confidence="0.9794928125">
WT2G collection
Hypothesis 1 Hypotheses 1+2
w(t,d) base POSGT % POSLA % POSCGT % POSCLA %
TFIDF 0.276 0.295 +6.8 0.293 +6.1 0.298 +8.0 0.294 +6.4
BM25 0.280 0.294 +4.8 0.292 +4.1 0.297 +5.9 0.293 +4.5
BB2 0.237 0.291 +22.8 0.287 +21.0 0.295 +24.2 0.288 +21.5
PL2 0.268 0.298 +11.2 0.297 +10.9 0.306 +14.1 0.302 +12.8
DLH 0.237 0.239 +0.7 0.238 +0.4 0.243 +2.3 0.241 +1.6
WT10G collection
Hypothesis 1 Hypotheses 1+2
w(t,d) base POSGT % POSLA % POSCGT % POSCLA %
TFIDF 0.231 0.234 +1.2 0.238 +2.8 0.233 +0.7 0.237 +2.6
BM25 0.234 0.234 none 0.238 +1.5 0.233 -0.4 0.237 +1.2
BB2 0.206 0.213 +3.5 0.214 +4.0 0.216 +5.0 0.220 +6.7
PL2 0.237 0.253 +6.8 0.253 +7.0 0.251 +6.1 0.256 +8.2
DLH 0.232 0.231 -0.7 0.233 +0.5 0.230 -1.0 0.234 +0.9
</table>
<tableCaption confidence="0.9656125">
Table 3: Mean Average Precision (MAP) scores of the POS and POSC experiments with Query Expan-
sion.
</tableCaption>
<table confidence="0.9702366875">
WT2G collection
Hypothesis 1 Hypotheses 1+2
w(t,d) base POSGT % POSLA % POSCGT % POSCLA %
TFIDF 0.299 0.323 +8.0 0.329 +10.0 0.322 +7.7 0.325 +8.7
BM25 0.302 0.320 +5.7 0.326 +7.9 0.319 +5.6 0.322 +6.6
BB2 0.239 0.291 +21.7 0.288 +20.5 0.291 +21.7 0.287 +20.1
PL2 0.285 0.312 +9.5 0.315 +10.5 0.315 +10.5 0.316 +10.9
DLH 0.267 0.283 +6.0 0.283 +6.0 0.284 +6.4 0.283 +6.0
WT10G collection
Hypothesis 1 Hypotheses 1+2
w(t,d) base POSGTQE % POSLAQE % POSCGT % POSCLA %
TFIDF 0.233 0.241 +3.4 0.249 +6.9 0.240 +3.0 0.250 +7.3
BM25 0.240 0.248 +3.3 0.250 +4.2 0.244 +1.7 0.249 +3.7
BB2 0.206 0.213 +3.4 0.214 +3.9 0.216 +4.8 0.220 +6.8
PL2 0.237 0.253 +6.7 0.253 +6.7 0.251 +5.9 0.256 +8.0
DLH 0.236 0.250 +5.9 0.246 +4.2 0.250 +5.9 0.253 +7.2
</table>
<page confidence="0.995668">
537
</page>
<bodyText confidence="0.999975724137931">
provement in retrieval performance over the new
baseline at all times, which is sometimes stati-
stically significant. This may indicate that noise
has been further reduced in the queries. Also, the
two statistical estimators lead to similar improve-
ments in retrieval performance. When we com-
pare these results to the ones reported with identi-
cal settings but without query expansion (Table 2),
we observe the following. Firstly, the previously
reported division in the DFR weighting schemes,
where BB2 and PL2 improved the most from our
hypothesised noise reduction in the queries, while
DLH improved the least, is no longer valid. The
improvement in retrieval performance now associ-
ated to DLH is similar to the improvement associ-
ated with the other weighting schemes. Secondly,
the difference in the retrieval improvement previ-
ously observed between the two test collections is
now smaller.
To recapitulate on the evaluation outcomes of
our two hypotheses combined with query expan-
sion, we report an improvement in retrieval per-
formance over the baseline at all times, which is
sometimes statistically significant. It appears that
the combination of our hypotheses with query ex-
pansion tones down previously reported sharp dif-
ferences in retrieval improvements over the base-
line (Table 2), which may be indicative of further
noise reduction.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999962772727273">
We described a block-based part of speech (POS)
modeling of language distribution, induced from
a corpus, and statistically smoothened using two
different estimators. We hypothesised that high-
frequency POS blocks bear more content than low-
frequency POS blocks. Also, we hypothesised that
the more closed class components a POS block
contains, the less content it bears. We evalu-
ated both hypotheses in the context of Informa-
tion Retrieval, across two standard test collec-
tions, and five statistically different term weight-
ing schemes. Our hypotheses led to a general
improvement in retrieval performance. This im-
provement was overall higher for the smaller of
the two collections, indicating that data sparseness
may have an effect on retrieval. The use of query
expansion worked well with our hypotheses, by
helping weaker weighting schemes to benefit more
from the reduction of noise in the queries.
In the future, we wish to investigate varying the
size of POS blocks, as well as testing our hypo-
theses on shorter queries.
</bodyText>
<sectionHeader confidence="0.996385" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999081886363636">
Alan F. Smeaton. 1999. Using NLP or NLP resources
for information retrieval tasks. Natural language in-
formation retrieval. Kluwer Academic Publishers
Dordrecht, NL.
Bruce Croft and John Lafferty. 2003. Language Mod-
elingfor Information Retrieval. Springer.
Christopher D. Manning and Hinrich Schutze. 1999.
Foundations of Statistical Language Processing.
The MIT Press, London.
David D. Lewis. 1992. An Evaluation of Phrasal and
Clustered Representations on a Text Categorization
Task. ACMSIGIR 1992, 37–50.
Gianni Amati. 2003. Probabilistic Models for In-
formation Retrieval based on Divergence from Ran-
domness. Ph.D. Thesis, University of Glasgow.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
Query Paraphrasing for Document Retrieval. COL-
ING 2002, 1177–1183.
John Lyons. 1977. Semantics: Volume 2. CUP, Cam-
bridge.
Karen Sparck-Jones. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal ofDocumentation, 28:11–21.
‘Keith’ (C. J.) van Rijsbergen. 1979. Information Re-
trieval. Butterworths, London.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22–29.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313–330.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jennifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479.
Stephen Robertson, Steve Walker, Micheline Beaulieu,
Mike Gatford, and A. Payne. 1995. Okapi at TREC-
4. NIST Special Publication 500-236: TREC-4, 73–
96.
Tomek Strzalkowski. 1996. Robust Natural Language
Processing and user-guided concept discovery for
Information retrieval, extraction and summarization.
Tipster Text Phase III Kickoff Workshop.
</reference>
<page confidence="0.996644">
538
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.352723">
<title confidence="0.984544">Examining the Content Load of Part of Speech Blocks for Information Retrieval</title>
<author confidence="0.999809">Christina Lioma</author>
<affiliation confidence="0.999976">Department of Computing Science University of Glasgow</affiliation>
<address confidence="0.9101625">17 Lilybank Gardens Scotland, U.K.</address>
<email confidence="0.995924">xristina@dcs.gla.ac.uk</email>
<author confidence="0.724578">Iadh Ounis</author>
<affiliation confidence="0.9999455">Department of Computing Science University of Glasgow</affiliation>
<address confidence="0.91751">17 Lilybank Gardens Scotland, U.K.</address>
<email confidence="0.998698">ounis@dcs.gla.ac.uk</email>
<abstract confidence="0.9859654375">We investigate the connection between part of speech (POS) distribution and content in language. We define POS blocks to be groups of parts of speech. We hypothesise that there exists a directly proportional relation between the frequency of POS blocks and their content salience. We also hypothesise that the class membership of the parts of speech within such blocks reflects the content load of the blocks, on the basis that open class parts of speech are more content-bearing than closed class parts of speech. We test these hypotheses in the context of Information Retrieval, by syntactically representing queries, and removing from them content-poor blocks, in line with the aforementioned hypotheses. For our first hypothesis, we induce POS distribution information from a corpus, and approximate the probability of occurrence of POS blocks as per two statistical estimators separately. For our second hypothesis, we use simple heuristics to estimate the content load within POS blocks. We use the Text REtrieval Conference (TREC) queries of 1999 and 2000 to retrieve documents from the WT2G and WT10G test collections, with five different retrieval strategies. Experimental outcomes confirm that our hypotheses hold in the context of Information Retrieval.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan F Smeaton</author>
</authors>
<title>Using NLP or NLP resources for information retrieval tasks. Natural language information retrieval.</title>
<date>1999</date>
<publisher>Kluwer Academic Publishers</publisher>
<location>Dordrecht, NL.</location>
<contexts>
<context position="7393" citStr="Smeaton, 1999" startWordPosition="1198" endWordPosition="1199">own et al., 1992) for example, classbased n-grams are used to determine the probability of occurrence of a POS class, given its preceding classes, and the probability of a particular word, given its own POS class. Unlike the classbased n-gram model, we do not use POS blocks to make predictions. We estimate their probability of occurrence as blocks, not the individual probabilities of their components, motivated by the intuition that the more frequently a POS block occurs, the more content it bears. In the context of IR, efforts have been made to use syntactic information to enhance retrieval (Smeaton, 1999; Strzalkowski, 1996; Zukerman and Raskutti, 2002), but not by using POS block-based distribution representations. 3 Methodology We present the steps realised in order to assess our hypotheses in the context of IR. Firstly, POS blocks with their respective frequencies are extracted from a corpus. The probability of occurrence of each POS block is statistically estimated. In order to test our first hypothesis, we remove from the query all but POS blocks of high probability of occurrence, on the assumption that the latter are content-rich. In order to test our second hypothesis, POS blocks that </context>
</contexts>
<marker>Smeaton, 1999</marker>
<rawString>Alan F. Smeaton. 1999. Using NLP or NLP resources for information retrieval tasks. Natural language information retrieval. Kluwer Academic Publishers Dordrecht, NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Croft</author>
<author>John Lafferty</author>
</authors>
<title>Language Modelingfor Information Retrieval.</title>
<date>2003</date>
<publisher>Springer.</publisher>
<contexts>
<context position="6686" citStr="Croft and Lafferty, 2003" startWordPosition="1074" endWordPosition="1078">ion to reflect the correct iden1http://trec.nist.gov/ tification of content-poor blocks, in line with our hypotheses. Section 2 presents related studies in this field. Section 3 introduces our methodology. Section 4 presents the experimental settings used to test our hypotheses, and their evaluation outcomes. Section 5 provides our conclusions and remarks. 2 Related Studies We examine the distribution of POS blocks in language. This is but one type of language distribution analysis that can be realised. One can also examine the distribution of character or word ngrams, e.g. Language Modeling (Croft and Lafferty, 2003), phrases (Church and Hanks, 1990; Lewis, 1992), and so on. In class-based n-gram modeling (Brown et al., 1992) for example, classbased n-grams are used to determine the probability of occurrence of a POS class, given its preceding classes, and the probability of a particular word, given its own POS class. Unlike the classbased n-gram model, we do not use POS blocks to make predictions. We estimate their probability of occurrence as blocks, not the individual probabilities of their components, motivated by the intuition that the more frequently a POS block occurs, the more content it bears. In</context>
</contexts>
<marker>Croft, Lafferty, 2003</marker>
<rawString>Bruce Croft and John Lafferty. 2003. Language Modelingfor Information Retrieval. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schutze</author>
</authors>
<title>Foundations of Statistical Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>London.</location>
<contexts>
<context position="9742" citStr="Manning and Schutze, 1999" startWordPosition="1588" endWordPosition="1592"> = 4, the POS blocks extracted are ABCD, BCDE, CDEF, and so on. The extracted POS blocks overlap. The order in which the POS blocks occur in the sentence is disregarded. We statistically infer the probability of occurrence of each POS block, on the basis of the individual POS block frequencies counted in the corpus. Maximum Likelihood inference is eschewed, as it assigns the maximum possible likelihood to the POS blocks observed in the corpus, and no probability to unseen POS blocks. Instead, we employ statistical estimation that accounts for unseen POS blocks, namely Laplace and Good-Turing (Manning and Schutze, 1999). 3.2 Removing POS blocks from the queries In order to test our first hypothesis, POS blocks of low probability of occurrence are removed from the queries. Specifically, we POS tag the queries, and remove the POS blocks that have a probability of occurrence below an empirical threshold . The following example illustrates this point. [Original query] A relevant document will focus on the causes of the lack of integration in a significant way; that is, the mere mention of immigration difficulties is not relevant. Documents that discuss immigration problems unrelated to Germany are also not relev</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Schutze. 1999. Foundations of Statistical Language Processing. The MIT Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>An Evaluation of Phrasal and Clustered Representations on a Text Categorization Task.</title>
<date>1992</date>
<pages>37--50</pages>
<publisher>ACMSIGIR</publisher>
<contexts>
<context position="6733" citStr="Lewis, 1992" startWordPosition="1084" endWordPosition="1085">tion of content-poor blocks, in line with our hypotheses. Section 2 presents related studies in this field. Section 3 introduces our methodology. Section 4 presents the experimental settings used to test our hypotheses, and their evaluation outcomes. Section 5 provides our conclusions and remarks. 2 Related Studies We examine the distribution of POS blocks in language. This is but one type of language distribution analysis that can be realised. One can also examine the distribution of character or word ngrams, e.g. Language Modeling (Croft and Lafferty, 2003), phrases (Church and Hanks, 1990; Lewis, 1992), and so on. In class-based n-gram modeling (Brown et al., 1992) for example, classbased n-grams are used to determine the probability of occurrence of a POS class, given its preceding classes, and the probability of a particular word, given its own POS class. Unlike the classbased n-gram model, we do not use POS blocks to make predictions. We estimate their probability of occurrence as blocks, not the individual probabilities of their components, motivated by the intuition that the more frequently a POS block occurs, the more content it bears. In the context of IR, efforts have been made to u</context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>David D. Lewis. 1992. An Evaluation of Phrasal and Clustered Representations on a Text Categorization Task. ACMSIGIR 1992, 37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gianni Amati</author>
</authors>
<title>Probabilistic Models for Information Retrieval based on Divergence from Randomness.</title>
<date>2003</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Glasgow.</institution>
<contexts>
<context position="17231" citStr="Amati, 2003" startWordPosition="2882" endWordPosition="2883">d term frequency in a document: ; is the frequency of a term in a document; , and are parameters; and are the document length and the average document length in the collection, respectively; is the number of documents in the collection; and is the number of documents containing the term . For all weighting schemes we use, , where is the query term frequency, and is the maximum among all query terms. We also use the well-established probabilistic BM25 weighting scheme (Robertson et al., 1995), and three distinct weighting schemes from the more recent Divergence From Randomness (DFR) framework (Amati, 2003), namely BB2, PL2, and DLH. Note that, even though we use three weighting schemes from the DFR framework, the said schemes are statistically different to one another. Also, DLH is the only parameter-free 4http://snowball.tartarus.org/ 5http://ir.dcs.gla.ac.uk/terrier/ weighting scheme we use, as it computes all of the variables automatically from the collection statistics. We use the default values of all parameters, namely, for the TF IDF and BM25 weighting schemes (Robertson et al., 1995), schemes (Amati, 2003), for the WT2G test collection, and for the WT10G test collection. We use default </context>
<context position="23122" citStr="Amati, 2003" startWordPosition="3806" endWordPosition="3807"> IR, which consists in extracting the most relevant terms from the top retrieved documents, and in using these terms to expand the initial query. The expanded query is then used to retrieve documents anew. Query expansion has the distinct property of improving retrieval performance when queries do not contain noise, but harming retrieval performance when queries contain noise, furnishing us with a strong baseline, against which we can measure our hypotheses. We repeat the experiments described in Section 4.2.1 with query expansion. We use the Bo1 query expansion scheme from the DFR framework (Amati, 2003). We optimise the query expansion settings, so as to maximise its performance. This provides us with an even stronger baseline, against which we can compare our proposed technique, which we tune empirically too through the tuning of the threshold . We optimise query expansion on the basis of the corresponding relevance assessments available for the queries and collections employed, by selecting the most relevant terms from the top retrieved documents. For the WT2G test collection, the relevant terms / top retrieved documents ratio we use is (i) 20/5 with TF IDF, BM25, and DLH; (ii) 30/5 with P</context>
</contexts>
<marker>Amati, 2003</marker>
<rawString>Gianni Amati. 2003. Probabilistic Models for Information Retrieval based on Divergence from Randomness. Ph.D. Thesis, University of Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Bhavani Raskutti</author>
</authors>
<title>Lexical Query Paraphrasing for Document Retrieval. COLING</title>
<date>2002</date>
<pages>1177--1183</pages>
<contexts>
<context position="7443" citStr="Zukerman and Raskutti, 2002" startWordPosition="1203" endWordPosition="1206">sbased n-grams are used to determine the probability of occurrence of a POS class, given its preceding classes, and the probability of a particular word, given its own POS class. Unlike the classbased n-gram model, we do not use POS blocks to make predictions. We estimate their probability of occurrence as blocks, not the individual probabilities of their components, motivated by the intuition that the more frequently a POS block occurs, the more content it bears. In the context of IR, efforts have been made to use syntactic information to enhance retrieval (Smeaton, 1999; Strzalkowski, 1996; Zukerman and Raskutti, 2002), but not by using POS block-based distribution representations. 3 Methodology We present the steps realised in order to assess our hypotheses in the context of IR. Firstly, POS blocks with their respective frequencies are extracted from a corpus. The probability of occurrence of each POS block is statistically estimated. In order to test our first hypothesis, we remove from the query all but POS blocks of high probability of occurrence, on the assumption that the latter are content-rich. In order to test our second hypothesis, POS blocks that contain more closed class than open class tags are</context>
</contexts>
<marker>Zukerman, Raskutti, 2002</marker>
<rawString>Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical Query Paraphrasing for Document Retrieval. COLING 2002, 1177–1183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lyons</author>
</authors>
<date>1977</date>
<volume>2</volume>
<publisher>CUP,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2899" citStr="Lyons, 1977" startWordPosition="462" endWordPosition="463">eights, can be computed using various term weighting schemes. Not all words can be used as keyword descriptors. In fact, a relatively small number of words accounts for most of a document’s content (van Rijsbergen, 1979). Function words make ‘noisy’ index terms, and are usually ignored during the retrieval process. This is practically realised with the use of stopword lists, which are lists of words to be exempted when indexing the collection and the queries. The use of stopword lists in IR is a manifestation of a well-known bifurcation in linguistics between open and closed classes of words (Lyons, 1977). In brief, open class words are more content-bearing than closed class words. Generally, the open class contains parts of speech that are morphologically and semantically flexible, while the closed class contains words that primarily perform linguistic well-formedness functions. The membership of the closed class is mostly fixed and largely restricted to function words, which are not prone to semantic or morphological alterations. We define a block of parts of speech (POS block) as a block of fixed length , where is set empirically. We define POS block tokens as individual instances of POS bl</context>
<context position="14475" citStr="Lyons, 1977" startWordPosition="2413" endWordPosition="2414">P$, WP$, EX, WRB IN, TO POS RP SYM UH VV, VVD, VVG, VVN, VVP, VVZ (Marcus et al., 1993). Since we are solely interested in a POS analysis, we introduce a stage of tagset simplification, during which, any information on top of surface POS classification is lost (Table 1). Practically, this leads to 48 original TreeBank (TB) tag classes being narrowed down to 15 Reduced TreeBank (RTB) tag classes. Additionally, tag names are shortened into two-letter names, for reasons of computational efficiency. We consider the TBR tags JJ, FW, NN, and VB as open-class, and the remaining tags as closed class (Lyons, 1977). We extract 214,398,227 POS block tokens and 19,343 POS block types from the corpus. We retrieve relevant documents from two standard TREC test collections, namely WT2G (2GB) and WT10G (10GB), from the 1999 and 2000 TREC Web tracks, respectively. We use the queries 401-450 from the ad-hoc task of the 1999 Web track, for the WT2G test collection, and the queries 451-500 from the ad-hoc task of the 2000 Web track, for the WT10G test collection, with their respective relevance assessments. Each query contains three fields, namely title, description, and narrative. The title contains keywords des</context>
</contexts>
<marker>Lyons, 1977</marker>
<rawString>John Lyons. 1977. Semantics: Volume 2. CUP, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck-Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal ofDocumentation,</journal>
<pages>28--11</pages>
<contexts>
<context position="16568" citStr="Sparck-Jones, 1972" startWordPosition="2766" endWordPosition="2767">t the most noisy field of the query to test whether the application of our hypotheses indeed results in the reduction of noise. During indexing, we remove stopwords, and stem the collections and the queries, using Porter’s4 stemming algorithm. We use the Terrier5 IR platform, and apply five different weighting schemes to match query terms to document descriptors. In IR, term weighting schemes estimate the relevance of a document for a query , as: , where is a term in , is the query term weight, and is the weight of document for term . For example, we use the classical TF IDF weighting scheme (Sparck-Jones, 1972; Robertson et al., 1995): , where is the normalised term frequency in a document: ; is the frequency of a term in a document; , and are parameters; and are the document length and the average document length in the collection, respectively; is the number of documents in the collection; and is the number of documents containing the term . For all weighting schemes we use, , where is the query term frequency, and is the maximum among all query terms. We also use the well-established probabilistic BM25 weighting scheme (Robertson et al., 1995), and three distinct weighting schemes from the more </context>
</contexts>
<marker>Sparck-Jones, 1972</marker>
<rawString>Karen Sparck-Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal ofDocumentation, 28:11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>‘Keith’ van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>‘Keith’ (C. J.) van Rijsbergen. 1979. Information Retrieval. Butterworths, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="6719" citStr="Church and Hanks, 1990" startWordPosition="1080" endWordPosition="1083">://trec.nist.gov/ tification of content-poor blocks, in line with our hypotheses. Section 2 presents related studies in this field. Section 3 introduces our methodology. Section 4 presents the experimental settings used to test our hypotheses, and their evaluation outcomes. Section 5 provides our conclusions and remarks. 2 Related Studies We examine the distribution of POS blocks in language. This is but one type of language distribution analysis that can be realised. One can also examine the distribution of character or word ngrams, e.g. Language Modeling (Croft and Lafferty, 2003), phrases (Church and Hanks, 1990; Lewis, 1992), and so on. In class-based n-gram modeling (Brown et al., 1992) for example, classbased n-grams are used to determine the probability of occurrence of a POS class, given its preceding classes, and the probability of a particular word, given its own POS class. Unlike the classbased n-gram model, we do not use POS blocks to make predictions. We estimate their probability of occurrence as blocks, not the individual probabilities of their components, motivated by the intuition that the more frequently a POS block occurs, the more content it bears. In the context of IR, efforts have </context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="13950" citStr="Marcus et al., 1993" startWordPosition="2324" endWordPosition="2327"> experimental settings, and Section 4.2 our evaluation results. 4.1 Experimental Settings We induce POS blocks from the English language component of the second release of the parallel Europarl corpus(75MB)2. We POS tag the corpus using the TreeTagger3, which is a probabilistic POS tagger that uses the Penn TreeBank tagset 2http://people.csail.mit.edu/koehn/publications/europarl/ 3http://www.ims.uni-stuttgart.de/projekte/corplex/ TreeTagger/ MD, VB, VBD, VBG, VBN, VBP, VBZ, VH, VHD, VHG, VHN, VHP, VHZ NN, NNS, NP, NPS PP, WP, PP$, WP$, EX, WRB IN, TO POS RP SYM UH VV, VVD, VVG, VVN, VVP, VVZ (Marcus et al., 1993). Since we are solely interested in a POS analysis, we introduce a stage of tagset simplification, during which, any information on top of surface POS classification is lost (Table 1). Practically, this leads to 48 original TreeBank (TB) tag classes being narrowed down to 15 Reduced TreeBank (RTB) tag classes. Additionally, tag names are shortened into two-letter names, for reasons of computational efficiency. We consider the TBR tags JJ, FW, NN, and VB as open-class, and the remaining tags as closed class (Lyons, 1977). We extract 214,398,227 POS block tokens and 19,343 POS block types from t</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="6797" citStr="Brown et al., 1992" startWordPosition="1093" endWordPosition="1096"> Section 2 presents related studies in this field. Section 3 introduces our methodology. Section 4 presents the experimental settings used to test our hypotheses, and their evaluation outcomes. Section 5 provides our conclusions and remarks. 2 Related Studies We examine the distribution of POS blocks in language. This is but one type of language distribution analysis that can be realised. One can also examine the distribution of character or word ngrams, e.g. Language Modeling (Croft and Lafferty, 2003), phrases (Church and Hanks, 1990; Lewis, 1992), and so on. In class-based n-gram modeling (Brown et al., 1992) for example, classbased n-grams are used to determine the probability of occurrence of a POS class, given its preceding classes, and the probability of a particular word, given its own POS class. Unlike the classbased n-gram model, we do not use POS blocks to make predictions. We estimate their probability of occurrence as blocks, not the individual probabilities of their components, motivated by the intuition that the more frequently a POS block occurs, the more content it bears. In the context of IR, efforts have been made to use syntactic information to enhance retrieval (Smeaton, 1999; St</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
<author>Steve Walker</author>
<author>Micheline Beaulieu</author>
<author>Mike Gatford</author>
<author>A Payne</author>
</authors>
<date>1995</date>
<note>Okapi at TREC4. NIST Special Publication 500-236: TREC-4, 73– 96.</note>
<contexts>
<context position="16593" citStr="Robertson et al., 1995" startWordPosition="2768" endWordPosition="2771">ld of the query to test whether the application of our hypotheses indeed results in the reduction of noise. During indexing, we remove stopwords, and stem the collections and the queries, using Porter’s4 stemming algorithm. We use the Terrier5 IR platform, and apply five different weighting schemes to match query terms to document descriptors. In IR, term weighting schemes estimate the relevance of a document for a query , as: , where is a term in , is the query term weight, and is the weight of document for term . For example, we use the classical TF IDF weighting scheme (Sparck-Jones, 1972; Robertson et al., 1995): , where is the normalised term frequency in a document: ; is the frequency of a term in a document; , and are parameters; and are the document length and the average document length in the collection, respectively; is the number of documents in the collection; and is the number of documents containing the term . For all weighting schemes we use, , where is the query term frequency, and is the maximum among all query terms. We also use the well-established probabilistic BM25 weighting scheme (Robertson et al., 1995), and three distinct weighting schemes from the more recent Divergence From Ra</context>
</contexts>
<marker>Robertson, Walker, Beaulieu, Gatford, Payne, 1995</marker>
<rawString>Stephen Robertson, Steve Walker, Micheline Beaulieu, Mike Gatford, and A. Payne. 1995. Okapi at TREC4. NIST Special Publication 500-236: TREC-4, 73– 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
</authors>
<title>Robust Natural Language Processing and user-guided concept discovery for Information retrieval, extraction and summarization. Tipster Text Phase III Kickoff Workshop.</title>
<date>1996</date>
<contexts>
<context position="7413" citStr="Strzalkowski, 1996" startWordPosition="1200" endWordPosition="1202">2) for example, classbased n-grams are used to determine the probability of occurrence of a POS class, given its preceding classes, and the probability of a particular word, given its own POS class. Unlike the classbased n-gram model, we do not use POS blocks to make predictions. We estimate their probability of occurrence as blocks, not the individual probabilities of their components, motivated by the intuition that the more frequently a POS block occurs, the more content it bears. In the context of IR, efforts have been made to use syntactic information to enhance retrieval (Smeaton, 1999; Strzalkowski, 1996; Zukerman and Raskutti, 2002), but not by using POS block-based distribution representations. 3 Methodology We present the steps realised in order to assess our hypotheses in the context of IR. Firstly, POS blocks with their respective frequencies are extracted from a corpus. The probability of occurrence of each POS block is statistically estimated. In order to test our first hypothesis, we remove from the query all but POS blocks of high probability of occurrence, on the assumption that the latter are content-rich. In order to test our second hypothesis, POS blocks that contain more closed </context>
</contexts>
<marker>Strzalkowski, 1996</marker>
<rawString>Tomek Strzalkowski. 1996. Robust Natural Language Processing and user-guided concept discovery for Information retrieval, extraction and summarization. Tipster Text Phase III Kickoff Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>