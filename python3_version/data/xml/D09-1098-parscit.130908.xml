<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.985102">
Web-Scale Distributional Similarity and Entity Set Expansion
</title>
<author confidence="0.987113">
Patrick Pantel†, Eric Crestan†, Arkady Borkovsky‡, Ana-Maria Popescu†, Vishnu Vyas†
</author>
<affiliation confidence="0.836627">
†Yahoo! Labs ‡Yandex Labs
</affiliation>
<address confidence="0.685846">
Sunnyvale, CA 94089 Burlingame, CA 94010
</address>
<email confidence="0.533804">
{ppantel,ecrestan}@yahoo-inc.com arkady@yandex-team.ru
{amp,vishnu}@yahoo-inc.com
</email>
<sectionHeader confidence="0.996409" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99978605">
Computing the pairwise semantic similarity
between all words on the Web is a compu-
tationally challenging task. Parallelization
and optimizations are necessary. We pro-
pose a highly scalable implementation
based on distributional similarity, imple-
mented in the MapReduce framework and
deployed over a 200 billion word crawl of
the Web. The pairwise similarity between
500 million terms is computed in 50 hours
using 200 quad-core nodes. We apply the
learned similarity matrix to the task of au-
tomatic set expansion and present a large
empirical study to quantify the effect on
expansion performance of corpus size, cor-
pus quality, seed composition and seed
size. We make public an experimental
testbed for set expansion analysis that in-
cludes a large collection of diverse entity
sets extracted from Wikipedia.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999822357142857">
Computing the semantic similarity between terms
has many applications in NLP including word clas-
sification (Turney and Littman 2003), word sense
disambiguation (Yuret and Yatbaz 2009), context-
spelling correction (Jones and Martin 1997), fact
extraction (Paşca et al. 2006), semantic role labe-
ling (Erk 2007), and applications in IR such as
query expansion (Cao et al. 2008) and textual ad-
vertising (Chang et al. 2009).
For commercial engines such as Yahoo! and
Google, creating lists of named entities found on
the Web is critical for query analysis, document
categorization, and ad matching. Computing term
similarity is typically done by comparing co-
occurrence vectors between all pairs of terms
(Sarmento et al. 2007). Scaling this task to the
Web requires parallelization and optimizations.
In this paper, we propose a large-scale term si-
milarity algorithm, based on distributional similari-
ty, implemented in the MapReduce framework and
deployed over a 200 billion word crawl of the
Web. The resulting similarity matrix between 500
million terms is applied to the task of expanding
lists of named entities (automatic set expansion).
We provide a detailed empirical analysis of the
discovered named entities and quantify the effect
on expansion accuracy of corpus size, corpus
quality, seed composition, and seed set size.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998525">
Below we review relevant work in optimizing si-
milarity computations and automatic set expansion.
</bodyText>
<subsectionHeader confidence="0.997881">
2.1 Computing Term Similarities
</subsectionHeader>
<bodyText confidence="0.999959">
The distributional hypothesis (Harris 1954), which
links the meaning of words to their contexts, has
inspired many algorithms for computing term simi-
larities (Lund and Burgess 1996; Lin 1998; Lee
1999; Erk and Padó 2008; Agirre et al. 2009).
Brute force similarity computation compares all
the contexts for each pair of terms, with complexi-
ty O(n2m) where n is the number of terms and m is
the number of possible contexts. More efficient
strategies are of three kinds:
</bodyText>
<page confidence="0.962506">
938
</page>
<note confidence="0.99663">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.997005055555555">
Smoothing: Techniques such as Latent Semantic
Analysis reduce the context space by applying
truncated Singular Value Decomposition (SVD)
(Deerwester et al. 1990). Computing the matrix
decomposition however does not scale well to
web-size term-context matrices. Other currently
unscalable smoothing techniques include Probabil-
istic Latent Semantic Analysis (Hofmann 1999),
Iterative Scaling (Ando 2000), and Latent Dirichlet
Allocation (Blei et al. 2003).
Randomized Algorithms: Randomized tech-
niques for approximating various similarity meas-
ures have been successfully applied to term simi-
larity (Ravichandran et al. 2005; Gorman and Cur-
ran 2006). Common techniques include Random
Indexing based on Sparse Distributed Memory
(Kanerva 1993) and Locality Sensitive Hashing
(Broder 1997).
</bodyText>
<subsubsectionHeader confidence="0.582538">
Optimizations and Distributed Processing:
</subsubsectionHeader>
<bodyText confidence="0.999556909090909">
Bayardo et al. (2007) present a sparse matrix opti-
mization strategy capable of efficiently computing
the similarity between terms which’s similarity
exceeds a given threshold. Rychlý and Kilgarriff
(2007), Elsayed et al. (2008) and Agirre et al.
(2009) use reverse indexing and the MapReduce
framework to distribute the similarity computa-
tions across several machines. Our proposed ap-
proach combines these two strategies and efficient-
ly computes the exact similarity (cosine, Jaccard,
Dice, and Overlap) between all pairs.
</bodyText>
<subsectionHeader confidence="0.995203">
2.2 Entity extraction and classification
</subsectionHeader>
<bodyText confidence="0.999938128205128">
Building entity lexicons is a task of great interest
for which structured, semi-structured and unstruc-
tured data have all been explored (GoogleSets;
Sarmento et al. 2007; Wang and Cohen 2007; Bu-
nescu and Mooney 2004; Etzioni et al. 2005; Paşca
et al. 2006). Our own work focuses on set expan-
sion from unstructured Web text. Apart from the
choice of a data source, state-of-the-art entity ex-
traction methods differ in their use of numerous,
few or no labeled examples, the open or targeted
nature of the extraction as well as the types of fea-
tures employed. Supervised approaches (McCal-
lum and Li 2003, Bunescu and Mooney 2004) rely
on large sets of labeled examples, perform targeted
extraction and employ a variety of sentence- and
corpus-level features. While very precise, these
methods are typically used for coarse grained enti-
ty classes (People, Organizations, Companies) for
which large training data sets are available. Unsu-
pervised approaches rely on no labeled data and
use either bootstrapped class-specific extraction
patterns (Etzioni et al. 2005) to find new elements
of a given class (for targeted extraction) or corpus-
based term similarity (Pantel and Lin 2002) to find
term clusters (in an open extraction framework).
Finally, semi-supervised methods have shown
great promise for identifying and labeling entities
(Riloff and Shepherd 1997; Riloff and Jones 1999;
Banko et al. 2007; Downey et al. 2007; Paşca et al.
2006; Paşca 2007a; Paşca 2007b; Paşca and Durme
2008). Starting with a set of seed entities, semi-
supervised extraction methods use either class-
specific patterns to populate an entity class or dis-
tributional similarity to find terms similar to the
seed set (Paşca’s work also examines the advan-
tages of combining these approaches). Semi-
supervised methods (including ours) are useful for
extending finer grain entity classes, for which large
unlabeled data sets are available.
</bodyText>
<subsectionHeader confidence="0.9998">
2.3 Impact of corpus on system performance
</subsectionHeader>
<bodyText confidence="0.999996421052632">
Previous work has examined the effect of using
large, sometimes Web-size corpora, on system per-
formance in the case of familiar NLP tasks. Banko
and Brill (2001) show that Web-scale data helps
with confusion set disambiguation while Lapata
and Keller (2005) find that the Web is a good
source of n-gram counts for unsupervised models.
Atterer and Schutze (2006) examine the influence
of corpus size on combining a supervised approach
with an unsupervised one for relative clause and
PP-attachment. Etzioni et al. (2005) and Pantel et
al. (2004) show the advantages of using large
quantities of generic Web text over smaller corpora
for extracting relations and named entities. Overall,
corpus size and quality are both found to be impor-
tant for extraction. Our paper adds to this body of
work by focusing on the task of similarity-based
set expansion and providing a large empirical
study quantify the relative corpus effects.
</bodyText>
<subsectionHeader confidence="0.999857">
2.4 Impact of seeds on extraction performance
</subsectionHeader>
<bodyText confidence="0.999127333333333">
Previous extraction systems report on the size and
quality of the training data or, if semi-supervised,
the size and quality of entity or pattern seed sets.
Narrowing the focus to closely related work, Paşca
(2007a; 2007b) and Paşca and Durme (2008) show
the impact of varying the number of instances rep-
resentative of a given class and the size of the
attribute seed set on the precision of class attribute
extraction. An example observation is that good
</bodyText>
<page confidence="0.999561">
939
</page>
<tableCaption confidence="0.9463105">
Table 1. Definitions for f0, f1, f2, and f3 for commonly used
similarity scores.
</tableCaption>
<equation confidence="0.9642945">
METRIC f0(x,y,z) f1(x,y) f ( x � ) f ( � x)
2 = 3
Overlap x 1 0
x min (x ,y ) ∑xi
y + −
z x i
y
*weighted generalization
</equation>
<bodyText confidence="0.999834333333333">
quality class attributes can still be extracted using
20 or even 10 instances to represent an entity class.
Among others, Etzioni et al. (2005) shows that a
small pattern set can help bootstrap useful entity
seed sets and reports on the impact of seed set
noise on final performance. Unlike previous work,
empirically quantifying the influence of seed set
size and quality on extraction performance of ran-
dom entity types is a key objective of this paper.
</bodyText>
<sectionHeader confidence="0.990938" genericHeader="method">
3 Large-Scale Similarity Model
</sectionHeader>
<bodyText confidence="0.999966576923077">
Term semantic models normally invoke the distri-
butional hypothesis (Harris 1985), which links the
meaning of terms to their contexts. Models are
built by recording the surrounding contexts for
each term in a large collection of unstructured text
and storing them in a term-context matrix. Me-
thods differ in their definition of a context (e.g.,
text window or syntactic relations), or by a means
to weigh contexts (e.g., frequency, tf-idf, pointwise
mutual information), or ultimately in measuring
the similarity between two context vectors (e.g.,
using Euclidean distance, Cosine, Dice).
In this paper, we adopt the following methodol-
ogy for computing term similarity. Our various
web crawls, described in Section 6.1, are POS-
tagged using Brill’s tagger (1995) and chunked
using a variant of the Abney chunker (Abney
1991). Terms are NP chunks with some modifiers
removed; their contexts (i.e., features) are defined
as their rightmost and leftmost stemmed chunks.
We weigh each context f using pointwise mutual
information (Church and Hanks 1989). Let PMI(w)
denote a pointwise mutual information vector, con-
structed for each term as follows: PMI(w) = (pmiw1,
pmiw2, ..., pmiwm), where pmiwf is the pointwise
mutual information between term w and feature f:
</bodyText>
<equation confidence="0.9929372">
cwf×N
n m
∑ c × ∑ c
if wj
i=1 j=1
</equation>
<bodyText confidence="0.9998955">
where cwf is the frequency of feature f occurring for
term w, n is the number of unique terms and N is
the total number of features for all terms.
Term similarities are computed by comparing
these pmi context vectors using measures such as
cosine, Jaccard, and Dice.
</bodyText>
<subsectionHeader confidence="0.994498">
3.1 Large-Scale Implementation
</subsectionHeader>
<bodyText confidence="0.999963416666667">
Computing the similarity between terms on a large
Web crawl is a non-trivial problem, with a worst
case cubic running time – O(n2m) where n is the
number of terms and m is the dimensionality of the
feature space. Section 2.1 introduces several opti-
mization techniques; below we propose an algo-
rithm for large-scale term similarity computation
which calculates exact scores for all pairs of terms,
generalizes to several different metrics, and is scal-
able to a large crawl of the Web.
Our optimization strategy follows a generalized
sparse-matrix multiplication approach (Sarawagi
and Kirpal 2004), which is based on the well-
known observation that a scalar product of two
vectors depends only on the coordinates for which
both vectors have non-zero values. Further, we
observe that most commonly used similarity scores
for feature vectors x� and y�, such as cosine and
Dice, can be decomposed into three values: one
depending only on features of x�, another depend-
ing only on features of y�, and the third depending
on the features shared both by x� and y�. More for-
mally, commonly used similarity scores F(x�, y)
can be expressed as:
</bodyText>
<equation confidence="0.3629305">
FU ,A(xi,yi),f2(4f3(A⎠
i
</equation>
<tableCaption confidence="0.672921">
Table 1 defines
</tableCaption>
<bodyText confidence="0.961906388888889">
and f3 for some common
similarity functions. For each of these scores, f2 =
f3. In our work, we compute all of these scores, but
report our results using only the cosine function.
Let A and B be two matrices of PMI feature vec-
tors. Our task is to compute the similarity between
all vectors in A and all vectors in B. In computing
the similarity between all pairs of terms, A = B.
Figure 1 outlines our algorithm for computing
the similarity between all elements of A and B. Ef-
ficient computation of the similarity matrix can be
achieved by leveraging the fact that
is de-
termined solely by the features shared by
and
(i.e.,
=
= 0 for any x) an
</bodyText>
<equation confidence="0.980475333333333">
f0,f1,f2,
F(x�,y�)
x�
y�
f1(0,x)
f1(x,0)
</equation>
<bodyText confidence="0.950881">
d that most of
</bodyText>
<figure confidence="0.807181764705882">
2 x
y
+ z
x
Jaccard*
Dice*
Cosine
×
z
x ×y ∑ ix2
i
x ×y ∑i
2
xi
=
log
pmiwf
</figure>
<page confidence="0.967195">
940
</page>
<bodyText confidence="0.999913">
the feature vectors are very sparse (i.e., most poss-
ible contexts never occur for a given term). In this
case, calculating f1(x, y) is only required when both
feature vectors have a shared non-zero feature, sig-
nificantly reducing the cost of computation. De-
termining which vectors share a non-zero feature
can easily be achieved by first building an inverted
index for the features. The computational cost of
this algorithm is ∑ 2
Ni , where Ni is the number of
vectors that have a non-zero ith coordinate. Its
worst case time complexity is O(ncv) where n is
the number of terms to be compared, c is the max-
imum number of non-zero coordinates of any vec-
tor, and v is the number of vectors that have a non-
zero ith coordinate where i is the coordinate which
is non-zero for the most vectors. In other words,
the algorithm is efficient only when the density of
the coordinates is low. On our datasets, we ob-
served near linear running time in the corpus size.
Bayardo et al. (2007) described a strategy that
potentially reduces the cost even further by omit-
ting the coordinates with the highest number of
non-zero value. However, their algorithm gives a
significant advantage only when we are interested
in finding solely the similarity between highly sim-
ilar terms. In our experiments, we compute the ex-
act similarity between all pairs of terms.
</bodyText>
<subsectionHeader confidence="0.608525">
Distributed Implementation
</subsectionHeader>
<bodyText confidence="0.99994285">
The pseudo-code in Figure 1 assumes that A can fit
into memory, which for large A may be impossible.
Also, as each element of B is processed indepen-
dently, running parallel processes for non-
intersecting subsets of B makes the processing
faster. In this section, we outline our MapReduce
implementation of Figure 1 deployed using Ha-
doop1, the open-source software package imple-
menting the MapReduce framework and distri-
buted file system. Hadoop has been shown to scale
to several thousands of machines, allowing users to
write simple “map” and “reduce” code, and to
seamlessly manage the sophisticated parallel ex-
ecution of the code. A good primer on MapReduce
programming is in (Dean and Ghemawat 2008).
Our implementation employs the MapReduce
model by using the Map step to start M×N Map
tasks in parallel, each caching 1/Mth part of A as
an inverted index and streaming 1/Nth part of B
through it. The actual inputs are read by the tasks
</bodyText>
<footnote confidence="0.883497">
1 Hadoop, http://lucene.apache.org/hadoop/
</footnote>
<bodyText confidence="0.761719">
Input: Two matrices A and B of feature vectors.
## Build an inverted index for A (optimiza-
## tion for data sparseness)
</bodyText>
<equation confidence="0.982913333333333">
AA = an empty hash-table
for i in (1..n):
F2[i] = f2(A[i]) ## cache values of f2(x)
</equation>
<bodyText confidence="0.7644362">
for k in non-zero features of A[i]:
if k not in AA: AA[k] = empty-set
## append &lt;vector-id, feature-value&gt;
## pairs to the set of non-zero
## values for feature k
</bodyText>
<equation confidence="0.837129285714286">
AA[k].append( (i,A[i,k]) )
## Process the elements of B
for b in B:
F1 = {} ## the set of Ai that have non-
zero similarity with b
for k in non-zero features of b:
for i in AA[k]:
if i not in sim: sim[i] = 0
F1[i] += f1( AA[k][i], b[k])
F3 = f3(b)
for i in sim:
print i, b, f0( F1[i], F2[i], F3)
Output: A matrix containing the similarity between
all elements in A and in B.
</equation>
<figureCaption confidence="0.998422">
Figure 1. Similarity computation algorithm.
</figureCaption>
<bodyText confidence="0.999403545454545">
directly from HDFS (Hadoop Distributed File Sys-
tem). Each part of A is processed N times, and each
part of B is processed M times. M is determined by
the amount of memory dedicated for the inverted
index, and N should be determined by trading off
the fact that as N increases, more parallelism can
be obtained at the increased cost of building the
same inverse index N times.
The similarity algorithm from Figure 1 is run in
each task of the Map step of a MapReduce job.
The Reduce step is used to group the output by bi.
</bodyText>
<sectionHeader confidence="0.978936" genericHeader="method">
4 Application to Set Expansion
</sectionHeader>
<bodyText confidence="0.999806">
Creating lists of named entities is a critical prob-
lem at commercial engines such as Yahoo! and
Google. The types of entities to be expanded are
often not known a priori, leaving supervised clas-
sifiers undesirable. Additionally, list creators typi-
cally need the ability to expand sets of varying
granularity. Semi-supervised approaches are pre-
dominantly adopted since they allow targeted ex-
pansions while requiring only small sets of seed
entities. State-of-the-art techniques first compute
term-term similarities for all available terms and
then select candidates for set expansion from
amongst the terms most similar to the seeds (Sar-
mento et al. 2007).
</bodyText>
<page confidence="0.992429">
941
</page>
<bodyText confidence="0.994879818181818">
Formally, we define our expansion task as:
Task Definition: Given a set of seed entities S =
{s1, s2, ..., sk} of a class C = {s1, s2, ..., sk, ...,, sn} and
an unlabeled textual corpus T, find all members of
the class C.
For example, consider the class of Bottled Water
Brands. Given the set of seeds S = {Volvic, San
Pellegrino, Gerolsteiner Brunnen, Bling H2O}, our
task is to find all other members of this class, such
as {Agua Vida, Apenta, Culligan, Dasani, Ethos
Water, Iceland Pure Spring Water, Imsdal, ...}
</bodyText>
<subsectionHeader confidence="0.997485">
4.1 Set Expansion Algorithm
</subsectionHeader>
<bodyText confidence="0.997488333333333">
Our goal is not to propose a new set expansion al-
gorithm, but instead to test the effect of using our
Web-scale term similarity matrix (enabled by the
algorithm proposed in Section 3) on a state-of-the-
art distributional set expansion algorithm, namely
(Sarmento et al. 2007).
We consider S as a set of prototypical examples
of the underlying entity set. A representation for
the meaning of S is computed by building a feature
vector consisting of a weighted average of the fea-
tures of its seed elements s1, s2, ..., sk, a centroid. For
example, given the seed elements {Volvic, San Pel-
legrino, Gerolsteiner Brunnen, Bling H2O}, the
resulting centroid consists of (details of the feature
extraction protocol are in Section 6.1):
brand, mineral water, monitor,
lake, water, take over, ...
Centroids are represented in the same space as
terms allowing us to compute the similarity be-
tween centroids and all terms in our corpus. A
scored and ranked set for expansion is ultimately
generated by sorting all terms according to their
similarity to the seed set centroid, and applying a
cutoff on either the similarity score or on the total
number of retrieved terms. In our reported experi-
ments, we expanded over 22,000 seed sets using
our Web similarity model from Section 3.
</bodyText>
<sectionHeader confidence="0.998216" genericHeader="method">
5 Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.9989035">
In this section, we describe our methodology for
evaluating Web-scale set expansion.
</bodyText>
<subsectionHeader confidence="0.977028">
5.1 Gold Standard Entity Sets
</subsectionHeader>
<bodyText confidence="0.996359382352942">
Estimating the quality of a set expansion algorithm
requires a random sample from the universe of all
entity sets that may ever be expanded, where a set
represents some concept such as Stage Actors. An
approximation of this universe can be extracted
from the “List of” pages in Wikipedia2.
Upon inspection of a random sample of the “List
of” pages, we found that several lists were compo-
sitions or joins of concepts, for example “List of
World War II aces from Denmark” and “List of
people who claimed to be God”. We addressed this
issue by constructing a quasi-random sample as
follows. We randomly sorted the list of every noun
occurring in Wikipedia2. Then, for each noun we
verified whether or not it existed in a Wikipedia
list, and if so we extracted this list. If a noun be-
longed to multiple lists, the authors chose the list
that seemed most appropriate. Although this does
not generate a perfect random sample, diversity is
ensured by the random selection of nouns and rele-
vancy is ensured by the author adjudication.
The final gold standard consists of 50 sets, in-
cluding: classical pianists, Spanish provinces,
Texas counties, male tennis players, first ladies,
cocktails, bottled water brands, and Archbishops of
Canterbury. For each set, we then manually
scraped every instance from Wikipedia keeping
track also of the listed variants names.
The gold standard is available for download at:
http://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl?type=data&amp;id=sse-
gold/wikipedia.20071218.goldsets.tgz
The 50 sets consist on average of 208 instances
(with a minimum of 11 and a maximum of 1,116)
for a total of 10,377 instances.
</bodyText>
<subsectionHeader confidence="0.982162">
5.2 Trials
</subsectionHeader>
<bodyText confidence="0.999982333333333">
In order to analyze the corpus and seed effects on
performance, we created 30 copies of each of the
50 sets and randomly sorted each copy. Then, for
each of the 1500 copies, we created a trial for each
of the following 23 seed sizes: 1, 2, 5, 10, 20, 30,
40, ..., 200. Each trial of seed size s was created by
taking the first s entries in each of the 1500 random
copies. For sets that contained fewer than 200
items, we only generated trials for seed sizes
</bodyText>
<footnote confidence="0.673425">
2 In this paper, extractions from Wikipedia are taken
from a snapshot of the resource in December 2008.
</footnote>
<page confidence="0.996095">
942
</page>
<tableCaption confidence="0.999917">
Table 2. Corpora used to build our expansion models.
</tableCaption>
<table confidence="0.956885375">
CORPORA UNIQUE TOKENS UNIQUE
SENTENCES (MILLIONS) WORDS
(MILLIONS) (MILLIONS)
Web100 5,201 217,940 542
Web020† 1040 43,588 108
Web004† 208 8,717 22
Wikipedia6 30 721 34
†Estimated from Web100 statistics.
</table>
<bodyText confidence="0.941103">
smaller than the set size. The resulting trial dataset
consists of 20,220 trials3.
</bodyText>
<subsectionHeader confidence="0.982988">
5.3 Judgments
</subsectionHeader>
<bodyText confidence="0.999984785714286">
Set expansion systems consist of an expansion al-
gorithm (such as the one described in Section 4.1)
as well as a corpus (such as Wikipedia, a news
corpus, or a web crawl). For a given system, each
of the 20,220 trials described in the previous sec-
tion are expanded. In our work, we limited the total
number of system expansions, per trial, to 1000.
Before judgment of an expanded set, we first
collapse each instance that is a variant of another
(determined using the variants in our gold stan-
dard) into one single instance (keeping the highest
system score)4. Then, each expanded instance is
judged as correct or incorrect automatically
against the gold standard described in Section 5.1.
</bodyText>
<subsectionHeader confidence="0.99428">
5.4 Analysis Metrics
</subsectionHeader>
<bodyText confidence="0.999384666666667">
Our experiments in Section 6 consist of precision
vs. recall or precision vs. rank curves, where:
a) precision is defined as the percentage of correct
instances in the expansion of a seed set; and
b) recall is defined as the percentage of non-seed
gold standard instances retrieved by the system.
Since the gold standard sets vary significantly in
size, we also provide the R-precision metric to
normalize for set size:
</bodyText>
<listItem confidence="0.820876">
c) R-precision is defined as the average precision
of all trials where precision is taken at rank R =
{size of trial’s associated gold standard set},
thereby normalizing for set size.
</listItem>
<footnote confidence="0.90711675">
3 Available for download at http://www.patrickpantel.com/cgi-
bin/Web/Tools/getfile.pl?type=data&amp;id=sse-gold/wikipedia.20071218.trials.tgz.
4 Note also that we do not allow seed instances nor their
variants to appear in an expansion set.
</footnote>
<bodyText confidence="0.999367666666667">
For the above metrics, 95% confidence bounds are
computed using the randomly generated samples
described in Section 5.2.
</bodyText>
<sectionHeader confidence="0.996559" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999828">
Our goal is to study the performance gains on set
expansion using our Web-scale term similarity al-
gorithm from Section 3. We present a large empir-
ical study quantifying the importance of corpus
and seeds on expansion accuracy.
</bodyText>
<subsectionHeader confidence="0.989088">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999714210526316">
We extracted statistics to build our model from
Section 3 using four different corpora, outlined in
Table 2. The Wikipedia corpus consists of a snap-
shot of the English articles in December 20085.
The Web100 corpus consists of an extraction from
a large crawl of the Web, from Yahoo!, of over
600 million English webpages. For each crawled
document, we removed paragraphs containing
fewer than 50 tokens (as a rough approximation of
the narrative part of a webpage) and then removed
all duplicate sentences. The resulting corpus con-
sists of over 200 billion words. The Web020 cor-
pus is a random sample of 1/5th of the sentences in
Web100 whereas Web004 is a random sample of
1/25th of Web100.
For each corpus, we tagged and chunked each
sentence as described in Section 3. We then com-
puted the similarity between all noun phrase
chunks using the model of Section 3.1.
</bodyText>
<subsectionHeader confidence="0.999424">
6.2 Quantitative Analysis
</subsectionHeader>
<bodyText confidence="0.9900251875">
Our proposed optimization for term similarity
computation produces exact scores (unlike rando-
mized techniques) for all pairs of terms on a large
Web crawl. For our largest corpus, Web100, we
computed the pairwise similarity between over 500
million words in 50 hours using 200 four-core ma-
chines. Web004 is of similar scale to the largest
reported randomized technique (Ravichandran et
al. 2005). On this scale, we compute the exact si-
milarity matrix in a little over two hours whereas
Ravichandran et al. (2005) compute an approxima-
tion in 570 hours. On average they only find 73%
5 To avoid biasing our Wikipedia corpus with the test
sets, Wikipedia “List of” pages were omitted from our
statistics as were any page linked to gold standard list
members from “List of” pages.
</bodyText>
<page confidence="0.999498">
943
</page>
<tableCaption confidence="0.9888645">
Table 3. Corpora analysis: R-precision and Precision at var-
ious ranks. 95% confidence bounds are all below 0.005†.
</tableCaption>
<table confidence="0.953653125">
RPRA RE RE RE RE
CORPORA R-PREC PREC@25 PREC@50 PREC@100
Web100 0.404 0.407 0.347 0.278
Web020 0.356 0.377 0.319 0.250
Web004 0.264 0.353 0.298 0.239
Wikipedia 0.315 0.372 0.314 0.253
95% cofid d d db i 5
†95% confidence bounds are computed over all trials described in Section 5.2.
</table>
<bodyText confidence="0.999804625">
of the top-1000 similar terms of a random term
whereas we find all of them.
For set expansion, experiments have been run on
corpora as large as Web004 and Wikipedia (Sar-
mento et al. 2007), a corpora 300 times smaller
than our Web crawl. Below, we compare the ex-
pansion accuracy of Sarmento et al. (2007) on Wi-
kipedia and our Web crawls.
Figure 2 illustrates the precision and recall tra-
deoff for our four corpora, with 95% confidence
intervals computed over all 20,220 trials described
in Section 4.2. Table 3 lists the resulting R-
precision along with the system precisions at ranks
25, 50, and 100 (see Figure 2 for detailed precision
analysis). Why are the precision scores so low?
Compared with previous work that manually select
entity types for expansion, such as countries and
companies, our work is the first to evaluate over a
large set of randomly selected entity types. On just
the countries class, our R-Precision was 0.816 us-
ing Web100.
The following sections analyze the effects of
various expansion variables: corpus size, corpus
quality, seed size, and seed quality.
</bodyText>
<subsectionHeader confidence="0.986317">
6.2.1 Corpus Size and Corpus Quality Effect
</subsectionHeader>
<bodyText confidence="0.991711307692308">
Not surprisingly, corpus size and quality have a
significant impact on expansion performance. Fig-
ure 2 and Table 3 quantify this expectation. On our
Web crawl corpora, we observe that the full 200+
billion token crawl (Web100) has an average R-
precision 13% higher than 1/5th of the crawl
(Web020) and 53% higher than 1/25th of the crawl.
Figure 2 also illustrates that throughout the full
precision/recall curve, Web100 significantly out-
performs Web020, which in turn significantly out-
performs Web004.
The higher text quality Wikipedia corpus, which
consists of roughly 60 times fewer tokens than
</bodyText>
<figureCaption confidence="0.998544">
Figure 2. Corpus size and quality improve performance.
</figureCaption>
<bodyText confidence="0.977722714285714">
Web020, performs nearly as well as Web020 (see
Figure 2). We omitted statistics from Wikipedia
“List of” pages in order to not bias our evaluation
to the test set described in Section 5.1. Inspection
of the precision vs. rank graph (omitted for lack of
space) revealed that from rank 1 thru 550, Wikipe-
dia had the same precision as Web020. From rank
550 to 1000, however, Wikipedia’s precision
dropped off significantly compared with Web020,
accounting for the fact that the Web corpus con-
tains a higher recall of gold standard instances. The
R-precision reported in Table 3 shows that this
precision drop-off results in a significantly lower
R-precision for Wikipedia compared with Web020.
</bodyText>
<subsectionHeader confidence="0.708931">
6.2.2 The Effect of Seed Selection
</subsectionHeader>
<bodyText confidence="0.999968111111111">
Intuitively, some seeds are better than others. We
study the impact of seed selection effect by in-
specting the system performance for several ran-
domly selected seed sets of fixed size and we find
that seed set composition greatly affects perfor-
mance. Figure 3 illustrates the precision vs. recall
tradeoff on our best performing corpus Web100 for
30 random seed sets of size 10 for each of our 50
gold standard sets (i.e., 1500 trials were tested.)
Each of the trials performed better than the average
system performance (the double-lined curve lowest
in Figure 3). Distinguishing between the various
data series is not important, however important to
notice is the very large gap between the preci-
sion/recall curves of the best and worst performing
random seed sets. On average, the best performing
seed sets had 42% higher precision and 39% higher
recall than the worst performing seed set. Similar
</bodyText>
<figure confidence="0.766131581395349">
Recall
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6
Precision
Corpora Analysis
(Precision vs. Recall)
Web100
Web020
Web004
Wikipedia
944
Recall
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
Precision
Web100: Seed Selection Effect
Precision vs. Recall
Web100 s010
s010.t01 s010.t02
s010.t03 s010.t04
s010.t05 s010.t06
s010.t07 s010.t08
s010.t09 s010.t10
s010.t11 s010.t12
s010.t13 s010.t14
s010.t15 s010.t16
s010.t17 s010.t18
s010.t19 s010.t20
s010.t21 s010.t22
s010.t23 s010.t24
s010.t25 s010.t26
s010.t27 s010.t28
s010.t29 s010.t30
</figure>
<figureCaption confidence="0.995092">
Figure 3. Seed set composition greatly affects system performance (with 30 different seed samples of size 10).
</figureCaption>
<figure confidence="0.999000636363636">
3
Rate of New Correct Expansions
vs. Seed Size
0
2.5
2
1.5
1
Rate of New Correct
0.5
0 20 40 60 80 100 120 140 160 180 200
Seed Size
Seed Size vs. % of Errors
0.4
1
% of Error
0.8
0.6
0.2
0
0 20 40 60 80 100 120 140 160 180 200
Seed Size
</figure>
<figureCaption confidence="0.9972435">
Figure 4. Few new instances are discovered with more
than 5-20 seeds on Web100 (with 95% confidence).
</figureCaption>
<bodyText confidence="0.997674166666667">
curves were observed for inspected seed sets of
size 5, 20, 30, and 40.
Although outside of the scope of this paper, we
are currently investigating ways to automatically
detect which seed elements are better than others in
order to reduce the impact of seed selection effect.
</bodyText>
<subsectionHeader confidence="0.546432">
6.2.3 The Effect of Seed Size
</subsectionHeader>
<bodyText confidence="0.999925">
Here we aim to confirm, with a large empirical
study, the anecdotal claims in (Paşca and Durme
2008) that few seeds are necessary. We found that
a) very small seed sets of size 1 or 2 are not suffi-
cient for representing the intended entity set; b) 5-
20 seeds yield on average best performance; and c)
surprisingly, increasing the seed set size beyond
20 or 30 on average does not find any new correct
instances.
We inspected the effect of seed size on R-
precision over the four corpora. Each seed size
curve is computed by averaging the system per-
formance over the 30 random trials of all 50 sets.
For each corpus, R-precision increased sharply
from seed size 1 to 10 and the curve flattened out
</bodyText>
<figureCaption confidence="0.994896">
Figure 5. Percentage of errors does not increase as
seed size increases on Web100 (with 95% confidence).
</figureCaption>
<bodyText confidence="0.999233304347826">
for seed sizes larger than 20 (figure omitted for
lack of space). Error analysis on the Web100 cor-
pus shows that once our model has seen 10-20
seeds, the distributional similarity model seems to
have enough statistics to discover as many new
correct instances as it could ever find. Some enti-
ties could never be found by the distributional si-
milarity model since they either do not occur or
infrequently occur in the corpus or they occur in
contexts that vary a great deal from other set ele-
ments. Figure 4 illustrates this behavior by plotting
for each seed set size the rate of increase in discov-
ery of new correct instances (i.e., not found in
smaller seed set sizes).
We see that most gold standard instances are
discovered with the first 5-10 seeds. After the 30th
seed is introduced, no new correct instances are
found. An important finding is that the error rate
does not increase with increased seed set size (see
Figure 5). This study shows that only few seeds
(10-20) yield best performance and that adding
more seeds beyond this does not on average affect
performance in a positive or negative way.
</bodyText>
<page confidence="0.997972">
945
</page>
<sectionHeader confidence="0.999106" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999716">
We proposed a highly scalable term similarity al-
gorithm, implemented in the MapReduce frame-
work, and deployed over a 200 billion word crawl
of the Web. The pairwise similarity between 500
million terms was computed in 50 hours using 200
quad-core nodes. We evaluated the impact of the
large similarity matrix on a set expansion task and
found that the Web similarity matrix gave a large
performance boost over a state-of-the-art expan-
sion algorithm using Wikipedia. Finally, we re-
lease to the community a testbed for experimental-
ly analyzing automatic set expansion, which in-
cludes a large collection of nearly random entity
sets extracted from Wikipedia and over 22,000
randomly sampled seed expansion trials.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99639772368421">
Abney, S. Parsing by Chunks. In: Robert Berwick, Ste-
ven Abney and Carol Tenny (eds.), Principle-Based
Parsing. Kluwer Academic Publishers, Dordrecht.
1991.
Agirre, E.; Alfonseca, E.; Hall, K.; Kravalova, J.; Pasca,
M.; and Soroa, A.. 2009. A Study on Similarity and
Relatedness Using Distributional and WordNet-based
Approaches. In Proceedings of NAACL HLT 09.
Ando, R. K. 2000. Latent semantic space: Iterative scal-
ing improves precision of interdocument similarity
measurement. In Proceedings of SIGIR-00. pp. 216–
223.
Atterer, M. and Schutze, H., 2006. The Effect of Corpus
Size when Combining Supervised and Unsupervised
Training for Disambiguation. In Proceedings of ACL-
Banko, M. and Brill, E. 2001. Mitigating the paucity of
data problem. In Proceedings of HLT-2001. San Di-
ego, CA.
Banko, M.; Cafarella, M.; Soderland, S.; Broadhead, M.;
Etzioni, O. 2007. Open Information Extraction from
the Web. In Proceedings of IJCAI.
Bayardo, R. J.; Ma, Y.; Srikant, R. 2007. Scaling Up
All-Pairs Similarity Search. In Proceedings of WWW-
7. pp. 131-140. Banff, Canada.
Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent
Dirichlet Allocation. Journal of Machine Learning
Research, 3:993–1022.
Brill, E. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part of Speech Tagging. Computational
Linguistics.
Broder, A. 1997. On the resemblance and containment
of documents. In Compression and Complexity of
Sequences. pp. 21-29.
Bunescu, R. and Mooney, R. 2004 Collective Informa-
tion Extraction with Relational Markov Networks. In
Proceedings of ACL-04, pp. 438-445.
Cao, H.; Jiang, D.; Pei, J.; He, Q.; Liao, Z.; Chen, E.;
and Li, H. 2008. Context-aware query suggestion by
mining click-through and session data. In Proceed-
ings of KDD-08. pp. 875–883.
Chang, W.; Pantel, P.; Popescu, A.-M.; and Gabrilovich,
E. 2009. Towards intent-driven bidterm suggestion.
In Proceedings of WWW-09 (Short Paper), Madrid,
Spain.
Church, K. and Hanks, P. 1989. Word association
norms, mutual information, and lexicography. In
Proceedings of ACL89. pp. 76–83.
Dean, J. and Ghemawat, S. 2008. MapReduce: Simpli-
fied Data Processing on Large Clusters. Communica-
tions of the ACM, 51(1):107-113.
Deerwester, S. C.; Dumais, S. T.; Landauer, T. K.; Fur-
nas, G. W.; and Harshman, R. A. 1990. Indexing by
latent semantic analysis. Journal of the American So-
ciety for Information Science, 41(6):391–407.
Downey, D.; Broadhead, M; Etzioni, O. 2007. Locating
Complex Named Entities in Web Text. In Proceed-
ings of IJCAI-07.
Elsayed, T.; Lin, J.; Oard, D. 2008. Pairwise Document
Similarity in Large Collections with MapReduce. In
Proceedings of ACL-08: HLT, Short Papers (Com-
panion Volume). pp. 265–268. Columbus, OH.
Erk, K. 2007. A simple, similarity-based model for se-
lectional preferences. In Proceedings of ACL-07. pp.
216–223. Prague, Czech Republic.
Erk, K. and Padó, S. 2008. A structured vector space
model for word meaning in context. In Proceedings
of EMNLP-08. Honolulu, HI.
Etzioni, O.; Cafarella, M.; Downey. D.; Popescu, A.;
Shaked, T; Soderland, S.; Weld, D.; Yates, A. 2005.
Unsupervised named-entity extraction from the Web:
An Experimental Study. In Artificial Intelligence,
165(1):91-134.
Gorman, J. and Curran, J. R. 2006. Scaling distribution-
al similarity to large corpora. In Proceedings of ACL-
06. pp. 361-368.
</reference>
<page confidence="0.987149">
946
</page>
<reference confidence="0.999908046511628">
Harris, Z. 1985. Distributional Structure. In: Katz, J. J.
(ed.), The Philosophy of Linguistics. New York: Ox-
ford University Press. pp. 26-47.
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp.
268–275. Pittsburgh, PA.
Hofmann, T. 1999. Probabilistic Latent Semantic Index-
ing. In Proceedings of SIGIR-99. pp. 50–57, Berke-
ley, California.
Kanerva, P. 1993. Sparse distributed memory and re-
lated models. pp. 50-76.
Lapata, M. and Keller, F., 2005. Web-based Models for
Natural Language Processing, In ACM Transactions
on Speech and Language Processing (TSLP), 2(1).
Lee, Lillian. 1999. Measures of Distributional Similarity.
In Proceedings of ACL-93. pp. 25-32. College Park,
MD.
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL-98.
pp. 768–774. Montreal, Canada.
Lund, K., and Burgess, C. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28(2):203–208.
McCallum, A. and Li, W. Early Results for Named
Entity Recognition with Conditional Random Fields,
Feature Induction and Enhanced Lexicons. In Pro-
ceedings of CoNLL-03.
McQueen, J. 1967. Some methods for classification and
analysis of multivariate observations. In Proceedings
of 5th Berkeley Symposium on Mathematics, Statistics
and Probability, 1:281–298.
Paşca, M. 2007a. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of CIKM-07. pp. 683-690.
Paşca, M. 2007b. Organizing and Searching the World
Wide Web of Facts – Step Two: Harnessing the Wis-
dom of the Crowds. In Proceedings of WWW-07.
Paşca, M. and Durme, B.J. 2008. Weakly-supervised
Acquisition of Open-Domain Classes and Class
Attributes from Web Documents and Query Logs. In
Proceedings of ACL-08.
Paşca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A.
2006. Names and Similarities on the Web: Fast Ex-
traction in the Fast Lane. In Proceedings of ACL-
2006. pp. 113-120.
Pantel, P. and Lin, D. 2002. Discovering Word Senses
from Text. In Proceedings of KDD-02. pp. 613-619.
Edmonton, Canada.
Pantel, P.; Ravichandran, D; Hovy, E.H. 2004. Towards
terascale knowledge acquisition. In proceedings of
COLING-04. pp 771-777.
Ravichandran, D.; Pantel, P.; and Hovy, E. 2005. Ran-
domized algorithms and NLP: Using locality sensi-
tive hash function for high speed noun clustering. In
Proceedings of ACL-05. pp. 622-629.
Riloff, E. and Jones, R. 1999 Learning Dictionaries for
Information Extraction by Multi-Level Boostrapping.
In Proceedings of AAAI/IAAAI-99.
Riloff, E. and Shepherd, J. 1997. A corpus-based ap-
proach for building semantic lexicons. In Proceed-
ings of EMNLP-97.
Rychlý, P. and Kilgarriff, A. 2007. An efficient algo-
rithm for building a distributional thesaurus (and oth-
er Sketch Engine developments). In Proceedings of
ACL-07, demo sessions. Prague, Czech Republic.
Sarawagi, S. and Kirpal, A. 2004. Efficient set joins on
similarity predicates. In Proceedings of SIGMOD &apos;04.
pp. 74 –754. New York, NY.
Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E.
2007. “More like these”: growing entity classes from
seeds. In Proceedings of CIKM-07. pp. 959-962. Lis-
bon, Portugal.
Turney, P. D., and Littman, M. L. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4).
Wang, R.C. and Cohen, W.W. 2008. Iterative Set Ex-
pansion of Named Entities using the Web. In Pro-
ceedings of ICDM 2008. Pisa, Italy.
Wang. R.C. and Cohen, W.W. 2007 Language-
Independent Set Expansion of Named Entities Using
the Web. In Proceedings of ICDM-07.
Yuret, D., and Yatbaz, M. A. 2009. The noisy channel
model for unsupervised word sense disambiguation.
Computational Linguistics. Under review.
</reference>
<page confidence="0.998111">
947
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.400756">
<title confidence="0.999811">Web-Scale Distributional Similarity and Entity Set Expansion</title>
<author confidence="0.737024">Eric Arkady Ana-Maria Vishnu Labs Labs</author>
<address confidence="0.990272">Sunnyvale, CA 94089 Burlingame, CA 94010</address>
<email confidence="0.9677695">ppantel@yahoo-inc.com</email>
<email confidence="0.9677695">ecrestan}@yahoo-inc.comarkady@yandex-team.ru{amp@yahoo-inc.com</email>
<email confidence="0.9677695">vishnu@yahoo-inc.com</email>
<abstract confidence="0.99540680952381">Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task. Parallelization and optimizations are necessary. We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web. The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes. We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by Chunks. In:</title>
<date>1991</date>
<editor>Robert Berwick, Steven Abney and Carol Tenny (eds.), Principle-Based Parsing.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="9509" citStr="Abney 1991" startWordPosition="1480" endWordPosition="1481">ge collection of unstructured text and storing them in a term-context matrix. Methods differ in their definition of a context (e.g., text window or syntactic relations), or by a means to weigh contexts (e.g., frequency, tf-idf, pointwise mutual information), or ultimately in measuring the similarity between two context vectors (e.g., using Euclidean distance, Cosine, Dice). In this paper, we adopt the following methodology for computing term similarity. Our various web crawls, described in Section 6.1, are POStagged using Brill’s tagger (1995) and chunked using a variant of the Abney chunker (Abney 1991). Terms are NP chunks with some modifiers removed; their contexts (i.e., features) are defined as their rightmost and leftmost stemmed chunks. We weigh each context f using pointwise mutual information (Church and Hanks 1989). Let PMI(w) denote a pointwise mutual information vector, constructed for each term as follows: PMI(w) = (pmiw1, pmiw2, ..., pmiwm), where pmiwf is the pointwise mutual information between term w and feature f: cwf×N n m ∑ c × ∑ c if wj i=1 j=1 where cwf is the frequency of feature f occurring for term w, n is the number of unique terms and N is the total number of featur</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Abney, S. Parsing by Chunks. In: Robert Berwick, Steven Abney and Carol Tenny (eds.), Principle-Based Parsing. Kluwer Academic Publishers, Dordrecht. 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pasca</author>
<author>A Soroa</author>
</authors>
<title>A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL HLT 09.</booktitle>
<contexts>
<context position="2837" citStr="Agirre et al. 2009" startWordPosition="421" endWordPosition="424"> of named entities (automatic set expansion). We provide a detailed empirical analysis of the discovered named entities and quantify the effect on expansion accuracy of corpus size, corpus quality, seed composition, and seed set size. 2 Related Work Below we review relevant work in optimizing similarity computations and automatic set expansion. 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not </context>
<context position="4302" citStr="Agirre et al. (2009)" startWordPosition="631" endWordPosition="634">mized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between all pairs. 2.2 Entity extraction and classification Building entity lexicons is a task of great interest for which structured, semi-structured and unstructured data have all been explored (GoogleSets; Sarmento et al. 2007; Wang and Cohen 2007; Bunescu and Mooney 2004; Etzioni et al. 2005; Paşca et al. 2006). Our own work focuses on set ex</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Agirre, E.; Alfonseca, E.; Hall, K.; Kravalova, J.; Pasca, M.; and Soroa, A.. 2009. A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches. In Proceedings of NAACL HLT 09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R K Ando</author>
</authors>
<title>Latent semantic space: Iterative scaling improves precision of interdocument similarity measurement.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR-00.</booktitle>
<pages>216--223</pages>
<contexts>
<context position="3623" citStr="Ando 2000" startWordPosition="537" endWordPosition="538">texts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshol</context>
</contexts>
<marker>Ando, 2000</marker>
<rawString>Ando, R. K. 2000. Latent semantic space: Iterative scaling improves precision of interdocument similarity measurement. In Proceedings of SIGIR-00. pp. 216– 223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Atterer</author>
<author>H Schutze</author>
</authors>
<title>The Effect of Corpus Size when Combining Supervised and Unsupervised Training for Disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-</booktitle>
<contexts>
<context position="6926" citStr="Atterer and Schutze (2006)" startWordPosition="1045" endWordPosition="1048">et (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web-size corpora, on system performance in the case of familiar NLP tasks. Banko and Brill (2001) show that Web-scale data helps with confusion set disambiguation while Lapata and Keller (2005) find that the Web is a good source of n-gram counts for unsupervised models. Atterer and Schutze (2006) examine the influence of corpus size on combining a supervised approach with an unsupervised one for relative clause and PP-attachment. Etzioni et al. (2005) and Pantel et al. (2004) show the advantages of using large quantities of generic Web text over smaller corpora for extracting relations and named entities. Overall, corpus size and quality are both found to be important for extraction. Our paper adds to this body of work by focusing on the task of similarity-based set expansion and providing a large empirical study quantify the relative corpus effects. 2.4 Impact of seeds on extraction </context>
</contexts>
<marker>Atterer, Schutze, 2006</marker>
<rawString>Atterer, M. and Schutze, H., 2006. The Effect of Corpus Size when Combining Supervised and Unsupervised Training for Disambiguation. In Proceedings of ACL-</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Mitigating the paucity of data problem.</title>
<date>2001</date>
<booktitle>In Proceedings of HLT-2001.</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="6726" citStr="Banko and Brill (2001)" startWordPosition="1013" endWordPosition="1016">arting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web-size corpora, on system performance in the case of familiar NLP tasks. Banko and Brill (2001) show that Web-scale data helps with confusion set disambiguation while Lapata and Keller (2005) find that the Web is a good source of n-gram counts for unsupervised models. Atterer and Schutze (2006) examine the influence of corpus size on combining a supervised approach with an unsupervised one for relative clause and PP-attachment. Etzioni et al. (2005) and Pantel et al. (2004) show the advantages of using large quantities of generic Web text over smaller corpora for extracting relations and named entities. Overall, corpus size and quality are both found to be important for extraction. Our </context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Banko, M. and Brill, E. 2001. Mitigating the paucity of data problem. In Proceedings of HLT-2001. San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open Information Extraction from the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of IJCAI.</booktitle>
<contexts>
<context position="6012" citStr="Banko et al. 2007" startWordPosition="898" endWordPosition="901"> methods are typically used for coarse grained entity classes (People, Organizations, Companies) for which large training data sets are available. Unsupervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using </context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Banko, M.; Cafarella, M.; Soderland, S.; Broadhead, M.; Etzioni, O. 2007. Open Information Extraction from the Web. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Bayardo</author>
<author>Y Ma</author>
<author>R Srikant</author>
</authors>
<title>Scaling Up All-Pairs Similarity Search.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW7.</booktitle>
<pages>131--140</pages>
<location>Banff, Canada.</location>
<contexts>
<context position="4071" citStr="Bayardo et al. (2007)" startWordPosition="597" endWordPosition="600"> web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between all pairs. 2.2 Entity extraction and classification Building entity lexicons is a task of great interest for </context>
<context position="13209" citStr="Bayardo et al. (2007)" startWordPosition="2139" endWordPosition="2142">ed index for the features. The computational cost of this algorithm is ∑ 2 Ni , where Ni is the number of vectors that have a non-zero ith coordinate. Its worst case time complexity is O(ncv) where n is the number of terms to be compared, c is the maximum number of non-zero coordinates of any vector, and v is the number of vectors that have a nonzero ith coordinate where i is the coordinate which is non-zero for the most vectors. In other words, the algorithm is efficient only when the density of the coordinates is low. On our datasets, we observed near linear running time in the corpus size. Bayardo et al. (2007) described a strategy that potentially reduces the cost even further by omitting the coordinates with the highest number of non-zero value. However, their algorithm gives a significant advantage only when we are interested in finding solely the similarity between highly similar terms. In our experiments, we compute the exact similarity between all pairs of terms. Distributed Implementation The pseudo-code in Figure 1 assumes that A can fit into memory, which for large A may be impossible. Also, as each element of B is processed independently, running parallel processes for nonintersecting subs</context>
</contexts>
<marker>Bayardo, Ma, Srikant, 2007</marker>
<rawString>Bayardo, R. J.; Ma, Y.; Srikant, R. 2007. Scaling Up All-Pairs Similarity Search. In Proceedings of WWW7. pp. 131-140. Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="3675" citStr="Blei et al. 2003" startWordPosition="543" endWordPosition="546"> kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (200</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging. Computational Linguistics.</title>
<date>1995</date>
<marker>Brill, 1995</marker>
<rawString>Brill, E. 1995. Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Broder</author>
</authors>
<title>On the resemblance and containment of documents.</title>
<date>1997</date>
<booktitle>In Compression and Complexity of Sequences.</booktitle>
<pages>21--29</pages>
<contexts>
<context position="4006" citStr="Broder 1997" startWordPosition="591" endWordPosition="592"> the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between all pairs. 2.2 Entity extraction and classif</context>
</contexts>
<marker>Broder, 1997</marker>
<rawString>Broder, A. 1997. On the resemblance and containment of documents. In Compression and Complexity of Sequences. pp. 21-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>Collective Information Extraction with Relational Markov Networks.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04,</booktitle>
<pages>438--445</pages>
<contexts>
<context position="4829" citStr="Bunescu and Mooney 2004" startWordPosition="709" endWordPosition="713"> a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between all pairs. 2.2 Entity extraction and classification Building entity lexicons is a task of great interest for which structured, semi-structured and unstructured data have all been explored (GoogleSets; Sarmento et al. 2007; Wang and Cohen 2007; Bunescu and Mooney 2004; Etzioni et al. 2005; Paşca et al. 2006). Our own work focuses on set expansion from unstructured Web text. Apart from the choice of a data source, state-of-the-art entity extraction methods differ in their use of numerous, few or no labeled examples, the open or targeted nature of the extraction as well as the types of features employed. Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence- and corpus-level features. While very precise, these methods are typically used for co</context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>Bunescu, R. and Mooney, R. 2004 Collective Information Extraction with Relational Markov Networks. In Proceedings of ACL-04, pp. 438-445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cao</author>
<author>D Jiang</author>
<author>J Pei</author>
<author>Q He</author>
<author>Z Liao</author>
<author>E Chen</author>
<author>H Li</author>
</authors>
<title>Context-aware query suggestion by mining click-through and session data.</title>
<date>2008</date>
<booktitle>In Proceedings of KDD-08.</booktitle>
<pages>875--883</pages>
<contexts>
<context position="1500" citStr="Cao et al. 2008" startWordPosition="213" endWordPosition="216">ion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. 1 Introduction Computing the semantic similarity between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). For commercial engines such as Yahoo! and Google, creating lists of named entities found on the Web is critical for query analysis, document categorization, and ad matching. Computing term similarity is typically done by comparing cooccurrence vectors between all pairs of terms (Sarmento et al. 2007). Scaling this task to the Web requires parallelization and optimizations. In this paper, we propose a large-scale term similarity algorithm, based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word</context>
</contexts>
<marker>Cao, Jiang, Pei, He, Liao, Chen, Li, 2008</marker>
<rawString>Cao, H.; Jiang, D.; Pei, J.; He, Q.; Liao, Z.; Chen, E.; and Li, H. 2008. Context-aware query suggestion by mining click-through and session data. In Proceedings of KDD-08. pp. 875–883.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chang</author>
<author>P Pantel</author>
<author>A-M Popescu</author>
<author>E Gabrilovich</author>
</authors>
<title>Towards intent-driven bidterm suggestion.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW-09 (Short Paper),</booktitle>
<location>Madrid,</location>
<contexts>
<context position="1544" citStr="Chang et al. 2009" startWordPosition="221" endWordPosition="224">lity, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. 1 Introduction Computing the semantic similarity between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). For commercial engines such as Yahoo! and Google, creating lists of named entities found on the Web is critical for query analysis, document categorization, and ad matching. Computing term similarity is typically done by comparing cooccurrence vectors between all pairs of terms (Sarmento et al. 2007). Scaling this task to the Web requires parallelization and optimizations. In this paper, we propose a large-scale term similarity algorithm, based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web. The resulting similarity </context>
</contexts>
<marker>Chang, Pantel, Popescu, Gabrilovich, 2009</marker>
<rawString>Chang, W.; Pantel, P.; Popescu, A.-M.; and Gabrilovich, E. 2009. Towards intent-driven bidterm suggestion. In Proceedings of WWW-09 (Short Paper), Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of ACL89.</booktitle>
<pages>76--83</pages>
<contexts>
<context position="9734" citStr="Church and Hanks 1989" startWordPosition="1512" endWordPosition="1515">ency, tf-idf, pointwise mutual information), or ultimately in measuring the similarity between two context vectors (e.g., using Euclidean distance, Cosine, Dice). In this paper, we adopt the following methodology for computing term similarity. Our various web crawls, described in Section 6.1, are POStagged using Brill’s tagger (1995) and chunked using a variant of the Abney chunker (Abney 1991). Terms are NP chunks with some modifiers removed; their contexts (i.e., features) are defined as their rightmost and leftmost stemmed chunks. We weigh each context f using pointwise mutual information (Church and Hanks 1989). Let PMI(w) denote a pointwise mutual information vector, constructed for each term as follows: PMI(w) = (pmiw1, pmiw2, ..., pmiwm), where pmiwf is the pointwise mutual information between term w and feature f: cwf×N n m ∑ c × ∑ c if wj i=1 j=1 where cwf is the frequency of feature f occurring for term w, n is the number of unique terms and N is the total number of features for all terms. Term similarities are computed by comparing these pmi context vectors using measures such as cosine, Jaccard, and Dice. 3.1 Large-Scale Implementation Computing the similarity between terms on a large Web cr</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Church, K. and Hanks, P. 1989. Word association norms, mutual information, and lexicography. In Proceedings of ACL89. pp. 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dean</author>
<author>S Ghemawat</author>
</authors>
<title>MapReduce: Simplified Data Processing on Large Clusters.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<pages>51--1</pages>
<contexts>
<context position="14304" citStr="Dean and Ghemawat 2008" startWordPosition="2314" endWordPosition="2317"> A may be impossible. Also, as each element of B is processed independently, running parallel processes for nonintersecting subsets of B makes the processing faster. In this section, we outline our MapReduce implementation of Figure 1 deployed using Hadoop1, the open-source software package implementing the MapReduce framework and distributed file system. Hadoop has been shown to scale to several thousands of machines, allowing users to write simple “map” and “reduce” code, and to seamlessly manage the sophisticated parallel execution of the code. A good primer on MapReduce programming is in (Dean and Ghemawat 2008). Our implementation employs the MapReduce model by using the Map step to start M×N Map tasks in parallel, each caching 1/Mth part of A as an inverted index and streaming 1/Nth part of B through it. The actual inputs are read by the tasks 1 Hadoop, http://lucene.apache.org/hadoop/ Input: Two matrices A and B of feature vectors. ## Build an inverted index for A (optimiza## tion for data sparseness) AA = an empty hash-table for i in (1..n): F2[i] = f2(A[i]) ## cache values of f2(x) for k in non-zero features of A[i]: if k not in AA: AA[k] = empty-set ## append &lt;vector-id, feature-value&gt; ## pairs</context>
</contexts>
<marker>Dean, Ghemawat, 2008</marker>
<rawString>Dean, J. and Ghemawat, S. 2008. MapReduce: Simplified Data Processing on Large Clusters. Communications of the ACM, 51(1):107-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="3383" citStr="Deerwester et al. 1990" startWordPosition="505" endWordPosition="508"> and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitiv</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Deerwester, S. C.; Dumais, S. T.; Landauer, T. K.; Furnas, G. W.; and Harshman, R. A. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Locating Complex Named Entities in Web Text.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI-07.</booktitle>
<contexts>
<context position="6032" citStr="Downey et al. 2007" startWordPosition="902" endWordPosition="905">lly used for coarse grained entity classes (People, Organizations, Companies) for which large training data sets are available. Unsupervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web</context>
</contexts>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>Downey, D.; Broadhead, M; Etzioni, O. 2007. Locating Complex Named Entities in Web Text. In Proceedings of IJCAI-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Elsayed</author>
<author>J Lin</author>
<author>D Oard</author>
</authors>
<title>Pairwise Document Similarity in Large Collections with MapReduce.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers (Companion Volume).</booktitle>
<pages>265--268</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="4277" citStr="Elsayed et al. (2008)" startWordPosition="626" endWordPosition="629"> (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between all pairs. 2.2 Entity extraction and classification Building entity lexicons is a task of great interest for which structured, semi-structured and unstructured data have all been explored (GoogleSets; Sarmento et al. 2007; Wang and Cohen 2007; Bunescu and Mooney 2004; Etzioni et al. 2005; Paşca et al. 2006). Our o</context>
</contexts>
<marker>Elsayed, Lin, Oard, 2008</marker>
<rawString>Elsayed, T.; Lin, J.; Oard, D. 2008. Pairwise Document Similarity in Large Collections with MapReduce. In Proceedings of ACL-08: HLT, Short Papers (Companion Volume). pp. 265–268. Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07.</booktitle>
<pages>216--223</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1434" citStr="Erk 2007" startWordPosition="203" endWordPosition="204">nt a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. 1 Introduction Computing the semantic similarity between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). For commercial engines such as Yahoo! and Google, creating lists of named entities found on the Web is critical for query analysis, document categorization, and ad matching. Computing term similarity is typically done by comparing cooccurrence vectors between all pairs of terms (Sarmento et al. 2007). Scaling this task to the Web requires parallelization and optimizations. In this paper, we propose a large-scale term similarity algorithm, based on distributional similarity, implement</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Erk, K. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of ACL-07. pp. 216–223. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Padó</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-08.</booktitle>
<location>Honolulu, HI.</location>
<contexts>
<context position="2816" citStr="Erk and Padó 2008" startWordPosition="417" endWordPosition="420"> of expanding lists of named entities (automatic set expansion). We provide a detailed empirical analysis of the discovered named entities and quantify the effect on expansion accuracy of corpus size, corpus quality, seed composition, and seed set size. 2 Related Work Below we review relevant work in optimizing similarity computations and automatic set expansion. 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposit</context>
</contexts>
<marker>Erk, Padó, 2008</marker>
<rawString>Erk, K. and Padó, S. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP-08. Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Popescu</author>
<author>A Shaked</author>
<author>T Soderland</author>
<author>S Weld</author>
<author>D Yates</author>
<author>A</author>
</authors>
<title>Unsupervised named-entity extraction from the Web: An Experimental Study.</title>
<date>2005</date>
<journal>In Artificial Intelligence,</journal>
<pages>165--1</pages>
<marker>Popescu, Shaked, Soderland, Weld, Yates, A, 2005</marker>
<rawString>Etzioni, O.; Cafarella, M.; Downey. D.; Popescu, A.; Shaked, T; Soderland, S.; Weld, D.; Yates, A. 2005. Unsupervised named-entity extraction from the Web: An Experimental Study. In Artificial Intelligence, 165(1):91-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gorman</author>
<author>J R Curran</author>
</authors>
<title>Scaling distributional similarity to large corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL06.</booktitle>
<pages>361--368</pages>
<contexts>
<context position="3868" citStr="Gorman and Curran 2006" startWordPosition="570" endWordPosition="574">uch as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategi</context>
</contexts>
<marker>Gorman, Curran, 2006</marker>
<rawString>Gorman, J. and Curran, J. R. 2006. Scaling distributional similarity to large corpora. In Proceedings of ACL06. pp. 361-368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional Structure. In:</title>
<date>1985</date>
<booktitle>The Philosophy of Linguistics.</booktitle>
<pages>26--47</pages>
<editor>Katz, J. J. (ed.),</editor>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="8767" citStr="Harris 1985" startWordPosition="1363" endWordPosition="1364">) ∑xi y + − z x i y *weighted generalization quality class attributes can still be extracted using 20 or even 10 instances to represent an entity class. Among others, Etzioni et al. (2005) shows that a small pattern set can help bootstrap useful entity seed sets and reports on the impact of seed set noise on final performance. Unlike previous work, empirically quantifying the influence of seed set size and quality on extraction performance of random entity types is a key objective of this paper. 3 Large-Scale Similarity Model Term semantic models normally invoke the distributional hypothesis (Harris 1985), which links the meaning of terms to their contexts. Models are built by recording the surrounding contexts for each term in a large collection of unstructured text and storing them in a term-context matrix. Methods differ in their definition of a context (e.g., text window or syntactic relations), or by a means to weigh contexts (e.g., frequency, tf-idf, pointwise mutual information), or ultimately in measuring the similarity between two context vectors (e.g., using Euclidean distance, Cosine, Dice). In this paper, we adopt the following methodology for computing term similarity. Our various</context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Harris, Z. 1985. Distributional Structure. In: Katz, J. J. (ed.), The Philosophy of Linguistics. New York: Oxford University Press. pp. 26-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90.</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, PA.</location>
<marker>Hindle, 1990</marker>
<rawString>Hindle, D. 1990. Noun classification from predicateargument structures. In Proceedings of ACL-90. pp. 268–275. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR-99.</booktitle>
<pages>50--57</pages>
<location>Berkeley, California.</location>
<contexts>
<context position="3592" citStr="Hofmann 1999" startWordPosition="533" endWordPosition="534">nd m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s simi</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, T. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of SIGIR-99. pp. 50–57, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kanerva</author>
</authors>
<title>Sparse distributed memory and related models.</title>
<date>1993</date>
<pages>50--76</pages>
<contexts>
<context position="3961" citStr="Kanerva 1993" startWordPosition="585" endWordPosition="586">tion (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between</context>
</contexts>
<marker>Kanerva, 1993</marker>
<rawString>Kanerva, P. 1993. Sparse distributed memory and related models. pp. 50-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>Web-based Models for Natural Language Processing,</title>
<date>2005</date>
<booktitle>In ACM Transactions on Speech and Language Processing (TSLP),</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="6822" citStr="Lapata and Keller (2005)" startWordPosition="1027" endWordPosition="1030"> patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web-size corpora, on system performance in the case of familiar NLP tasks. Banko and Brill (2001) show that Web-scale data helps with confusion set disambiguation while Lapata and Keller (2005) find that the Web is a good source of n-gram counts for unsupervised models. Atterer and Schutze (2006) examine the influence of corpus size on combining a supervised approach with an unsupervised one for relative clause and PP-attachment. Etzioni et al. (2005) and Pantel et al. (2004) show the advantages of using large quantities of generic Web text over smaller corpora for extracting relations and named entities. Overall, corpus size and quality are both found to be important for extraction. Our paper adds to this body of work by focusing on the task of similarity-based set expansion and pr</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Lapata, M. and Keller, F., 2005. Web-based Models for Natural Language Processing, In ACM Transactions on Speech and Language Processing (TSLP), 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of Distributional Similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL-93.</booktitle>
<pages>25--32</pages>
<location>College Park, MD.</location>
<contexts>
<context position="2797" citStr="Lee 1999" startWordPosition="415" endWordPosition="416">o the task of expanding lists of named entities (automatic set expansion). We provide a detailed empirical analysis of the discovered named entities and quantify the effect on expansion accuracy of corpus size, corpus quality, seed composition, and seed set size. 2 Related Work Below we review relevant work in optimizing similarity computations and automatic set expansion. 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing th</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lee, Lillian. 1999. Measures of Distributional Similarity. In Proceedings of ACL-93. pp. 25-32. College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-98.</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="2787" citStr="Lin 1998" startWordPosition="413" endWordPosition="414"> applied to the task of expanding lists of named entities (automatic set expansion). We provide a detailed empirical analysis of the discovered named entities and quantify the effect on expansion accuracy of corpus size, corpus quality, seed composition, and seed set size. 2 Related Work Below we review relevant work in optimizing similarity computations and automatic set expansion. 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Co</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL-98. pp. 768–774. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing highdimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="2777" citStr="Lund and Burgess 1996" startWordPosition="409" endWordPosition="412">en 500 million terms is applied to the task of expanding lists of named entities (automatic set expansion). We provide a detailed empirical analysis of the discovered named entities and quantify the effect on expansion accuracy of corpus size, corpus quality, seed composition, and seed set size. 2 Related Work Below we review relevant work in optimizing similarity computations and automatic set expansion. 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al.</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Lund, K., and Burgess, C. 1996. Producing highdimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A McCallum</author>
<author>W Li</author>
</authors>
<title>Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Enhanced Lexicons.</title>
<booktitle>In Proceedings of CoNLL-03.</booktitle>
<marker>McCallum, Li, </marker>
<rawString>McCallum, A. and Li, W. Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Enhanced Lexicons. In Proceedings of CoNLL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McQueen</author>
</authors>
<title>Some methods for classification and analysis of multivariate observations.</title>
<date>1967</date>
<booktitle>In Proceedings of 5th Berkeley Symposium on Mathematics, Statistics and Probability,</booktitle>
<pages>1--281</pages>
<marker>McQueen, 1967</marker>
<rawString>McQueen, J. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of 5th Berkeley Symposium on Mathematics, Statistics and Probability, 1:281–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paşca</author>
</authors>
<title>Weakly-supervised discovery of named entities using web search queries.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM-07.</booktitle>
<pages>683--690</pages>
<contexts>
<context position="6063" citStr="Paşca 2007" startWordPosition="910" endWordPosition="911">ses (People, Organizations, Companies) for which large training data sets are available. Unsupervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web-size corpora, on system perfor</context>
<context position="7751" citStr="Paşca (2007" startWordPosition="1180" endWordPosition="1181">arge quantities of generic Web text over smaller corpora for extracting relations and named entities. Overall, corpus size and quality are both found to be important for extraction. Our paper adds to this body of work by focusing on the task of similarity-based set expansion and providing a large empirical study quantify the relative corpus effects. 2.4 Impact of seeds on extraction performance Previous extraction systems report on the size and quality of the training data or, if semi-supervised, the size and quality of entity or pattern seed sets. Narrowing the focus to closely related work, Paşca (2007a; 2007b) and Paşca and Durme (2008) show the impact of varying the number of instances representative of a given class and the size of the attribute seed set on the precision of class attribute extraction. An example observation is that good 939 Table 1. Definitions for f0, f1, f2, and f3 for commonly used similarity scores. METRIC f0(x,y,z) f1(x,y) f ( x � ) f ( � x) 2 = 3 Overlap x 1 0 x min (x ,y ) ∑xi y + − z x i y *weighted generalization quality class attributes can still be extracted using 20 or even 10 instances to represent an entity class. Among others, Etzioni et al. (2005) shows t</context>
</contexts>
<marker>Paşca, 2007</marker>
<rawString>Paşca, M. 2007a. Weakly-supervised discovery of named entities using web search queries. In Proceedings of CIKM-07. pp. 683-690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paşca</author>
</authors>
<title>Organizing and Searching the World Wide Web of Facts – Step Two: Harnessing the Wisdom of the Crowds.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW-07.</booktitle>
<contexts>
<context position="6063" citStr="Paşca 2007" startWordPosition="910" endWordPosition="911">ses (People, Organizations, Companies) for which large training data sets are available. Unsupervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web-size corpora, on system perfor</context>
<context position="7751" citStr="Paşca (2007" startWordPosition="1180" endWordPosition="1181">arge quantities of generic Web text over smaller corpora for extracting relations and named entities. Overall, corpus size and quality are both found to be important for extraction. Our paper adds to this body of work by focusing on the task of similarity-based set expansion and providing a large empirical study quantify the relative corpus effects. 2.4 Impact of seeds on extraction performance Previous extraction systems report on the size and quality of the training data or, if semi-supervised, the size and quality of entity or pattern seed sets. Narrowing the focus to closely related work, Paşca (2007a; 2007b) and Paşca and Durme (2008) show the impact of varying the number of instances representative of a given class and the size of the attribute seed set on the precision of class attribute extraction. An example observation is that good 939 Table 1. Definitions for f0, f1, f2, and f3 for commonly used similarity scores. METRIC f0(x,y,z) f1(x,y) f ( x � ) f ( � x) 2 = 3 Overlap x 1 0 x min (x ,y ) ∑xi y + − z x i y *weighted generalization quality class attributes can still be extracted using 20 or even 10 instances to represent an entity class. Among others, Etzioni et al. (2005) shows t</context>
</contexts>
<marker>Paşca, 2007</marker>
<rawString>Paşca, M. 2007b. Organizing and Searching the World Wide Web of Facts – Step Two: Harnessing the Wisdom of the Crowds. In Proceedings of WWW-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paşca</author>
<author>B J Durme</author>
</authors>
<title>Weakly-supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08.</booktitle>
<contexts>
<context position="6100" citStr="Paşca and Durme 2008" startWordPosition="914" endWordPosition="917">, Companies) for which large training data sets are available. Unsupervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web-size corpora, on system performance in the case of familiar NLP tas</context>
<context position="7787" citStr="Paşca and Durme (2008)" startWordPosition="1184" endWordPosition="1187">c Web text over smaller corpora for extracting relations and named entities. Overall, corpus size and quality are both found to be important for extraction. Our paper adds to this body of work by focusing on the task of similarity-based set expansion and providing a large empirical study quantify the relative corpus effects. 2.4 Impact of seeds on extraction performance Previous extraction systems report on the size and quality of the training data or, if semi-supervised, the size and quality of entity or pattern seed sets. Narrowing the focus to closely related work, Paşca (2007a; 2007b) and Paşca and Durme (2008) show the impact of varying the number of instances representative of a given class and the size of the attribute seed set on the precision of class attribute extraction. An example observation is that good 939 Table 1. Definitions for f0, f1, f2, and f3 for commonly used similarity scores. METRIC f0(x,y,z) f1(x,y) f ( x � ) f ( � x) 2 = 3 Overlap x 1 0 x min (x ,y ) ∑xi y + − z x i y *weighted generalization quality class attributes can still be extracted using 20 or even 10 instances to represent an entity class. Among others, Etzioni et al. (2005) shows that a small pattern set can help boo</context>
<context position="29919" citStr="Paşca and Durme 2008" startWordPosition="4946" endWordPosition="4949">ed Size Seed Size vs. % of Errors 0.4 1 % of Error 0.8 0.6 0.2 0 0 20 40 60 80 100 120 140 160 180 200 Seed Size Figure 4. Few new instances are discovered with more than 5-20 seeds on Web100 (with 95% confidence). curves were observed for inspected seed sets of size 5, 20, 30, and 40. Although outside of the scope of this paper, we are currently investigating ways to automatically detect which seed elements are better than others in order to reduce the impact of seed selection effect. 6.2.3 The Effect of Seed Size Here we aim to confirm, with a large empirical study, the anecdotal claims in (Paşca and Durme 2008) that few seeds are necessary. We found that a) very small seed sets of size 1 or 2 are not sufficient for representing the intended entity set; b) 5- 20 seeds yield on average best performance; and c) surprisingly, increasing the seed set size beyond 20 or 30 on average does not find any new correct instances. We inspected the effect of seed size on Rprecision over the four corpora. Each seed size curve is computed by averaging the system performance over the 30 random trials of all 50 sets. For each corpus, R-precision increased sharply from seed size 1 to 10 and the curve flattened out Figu</context>
</contexts>
<marker>Paşca, Durme, 2008</marker>
<rawString>Paşca, M. and Durme, B.J. 2008. Weakly-supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs. In Proceedings of ACL-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paşca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Names and Similarities on the Web: Fast Extraction in the Fast Lane.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL2006.</booktitle>
<pages>113--120</pages>
<contexts>
<context position="1399" citStr="Paşca et al. 2006" startWordPosition="195" endWordPosition="198">he task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. 1 Introduction Computing the semantic similarity between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). For commercial engines such as Yahoo! and Google, creating lists of named entities found on the Web is critical for query analysis, document categorization, and ad matching. Computing term similarity is typically done by comparing cooccurrence vectors between all pairs of terms (Sarmento et al. 2007). Scaling this task to the Web requires parallelization and optimizations. In this paper, we propose a large-scale term similarity algorithm, based on d</context>
<context position="4870" citStr="Paşca et al. 2006" startWordPosition="718" endWordPosition="721">7), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between all pairs. 2.2 Entity extraction and classification Building entity lexicons is a task of great interest for which structured, semi-structured and unstructured data have all been explored (GoogleSets; Sarmento et al. 2007; Wang and Cohen 2007; Bunescu and Mooney 2004; Etzioni et al. 2005; Paşca et al. 2006). Our own work focuses on set expansion from unstructured Web text. Apart from the choice of a data source, state-of-the-art entity extraction methods differ in their use of numerous, few or no labeled examples, the open or targeted nature of the extraction as well as the types of features employed. Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence- and corpus-level features. While very precise, these methods are typically used for coarse grained entity classes (People, Orga</context>
</contexts>
<marker>Paşca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Paşca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A. 2006. Names and Similarities on the Web: Fast Extraction in the Fast Lane. In Proceedings of ACL2006. pp. 113-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of KDD-02.</booktitle>
<pages>613--619</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5791" citStr="Pantel and Lin 2002" startWordPosition="865" endWordPosition="868">Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence- and corpus-level features. While very precise, these methods are typically used for coarse grained entity classes (People, Organizations, Companies) for which large training data sets are available. Unsupervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervise</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In Proceedings of KDD-02. pp. 613-619. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
<author>E H Hovy</author>
</authors>
<title>Towards terascale knowledge acquisition.</title>
<date>2004</date>
<booktitle>In proceedings of COLING-04.</booktitle>
<pages>771--777</pages>
<contexts>
<context position="7109" citStr="Pantel et al. (2004)" startWordPosition="1074" endWordPosition="1077">labeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web-size corpora, on system performance in the case of familiar NLP tasks. Banko and Brill (2001) show that Web-scale data helps with confusion set disambiguation while Lapata and Keller (2005) find that the Web is a good source of n-gram counts for unsupervised models. Atterer and Schutze (2006) examine the influence of corpus size on combining a supervised approach with an unsupervised one for relative clause and PP-attachment. Etzioni et al. (2005) and Pantel et al. (2004) show the advantages of using large quantities of generic Web text over smaller corpora for extracting relations and named entities. Overall, corpus size and quality are both found to be important for extraction. Our paper adds to this body of work by focusing on the task of similarity-based set expansion and providing a large empirical study quantify the relative corpus effects. 2.4 Impact of seeds on extraction performance Previous extraction systems report on the size and quality of the training data or, if semi-supervised, the size and quality of entity or pattern seed sets. Narrowing the </context>
</contexts>
<marker>Pantel, Ravichandran, Hovy, 2004</marker>
<rawString>Pantel, P.; Ravichandran, D; Hovy, E.H. 2004. Towards terascale knowledge acquisition. In proceedings of COLING-04. pp 771-777.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>P Pantel</author>
<author>E Hovy</author>
</authors>
<title>Randomized algorithms and NLP: Using locality sensitive hash function for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05.</booktitle>
<pages>622--629</pages>
<contexts>
<context position="3843" citStr="Ravichandran et al. 2005" startWordPosition="566" endWordPosition="569">LP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach co</context>
<context position="24341" citStr="Ravichandran et al. 2005" startWordPosition="4005" endWordPosition="4008"> sample of 1/25th of Web100. For each corpus, we tagged and chunked each sentence as described in Section 3. We then computed the similarity between all noun phrase chunks using the model of Section 3.1. 6.2 Quantitative Analysis Our proposed optimization for term similarity computation produces exact scores (unlike randomized techniques) for all pairs of terms on a large Web crawl. For our largest corpus, Web100, we computed the pairwise similarity between over 500 million words in 50 hours using 200 four-core machines. Web004 is of similar scale to the largest reported randomized technique (Ravichandran et al. 2005). On this scale, we compute the exact similarity matrix in a little over two hours whereas Ravichandran et al. (2005) compute an approximation in 570 hours. On average they only find 73% 5 To avoid biasing our Wikipedia corpus with the test sets, Wikipedia “List of” pages were omitted from our statistics as were any page linked to gold standard list members from “List of” pages. 943 Table 3. Corpora analysis: R-precision and Precision at various ranks. 95% confidence bounds are all below 0.005†. RPRA RE RE RE RE CORPORA R-PREC PREC@25 PREC@50 PREC@100 Web100 0.404 0.407 0.347 0.278 Web020 0.35</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Ravichandran, D.; Pantel, P.; and Hovy, E. 2005. Randomized algorithms and NLP: Using locality sensitive hash function for high speed noun clustering. In Proceedings of ACL-05. pp. 622-629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Boostrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI/IAAAI-99.</booktitle>
<contexts>
<context position="5993" citStr="Riloff and Jones 1999" startWordPosition="894" endWordPosition="897">ile very precise, these methods are typically used for coarse grained entity classes (People, Organizations, Companies) for which large training data sets are available. Unsupervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined t</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Riloff, E. and Jones, R. 1999 Learning Dictionaries for Information Extraction by Multi-Level Boostrapping. In Proceedings of AAAI/IAAAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A corpus-based approach for building semantic lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP-97.</booktitle>
<contexts>
<context position="5970" citStr="Riloff and Shepherd 1997" startWordPosition="890" endWordPosition="893"> corpus-level features. While very precise, these methods are typically used for coarse grained entity classes (People, Organizations, Companies) for which large training data sets are available. Unsupervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previ</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Riloff, E. and Shepherd, J. 1997. A corpus-based approach for building semantic lexicons. In Proceedings of EMNLP-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rychlý</author>
<author>A Kilgarriff</author>
</authors>
<title>An efficient algorithm for building a distributional thesaurus (and other Sketch Engine developments).</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07, demo sessions.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4254" citStr="Rychlý and Kilgarriff (2007)" startWordPosition="622" endWordPosition="625">nd Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between all pairs. 2.2 Entity extraction and classification Building entity lexicons is a task of great interest for which structured, semi-structured and unstructured data have all been explored (GoogleSets; Sarmento et al. 2007; Wang and Cohen 2007; Bunescu and Mooney 2004; Etzioni et al. 2005; Pa</context>
</contexts>
<marker>Rychlý, Kilgarriff, 2007</marker>
<rawString>Rychlý, P. and Kilgarriff, A. 2007. An efficient algorithm for building a distributional thesaurus (and other Sketch Engine developments). In Proceedings of ACL-07, demo sessions. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sarawagi</author>
<author>A Kirpal</author>
</authors>
<title>Efficient set joins on similarity predicates.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGMOD &apos;04.</booktitle>
<pages>74--754</pages>
<location>New York, NY.</location>
<contexts>
<context position="10874" citStr="Sarawagi and Kirpal 2004" startWordPosition="1704" endWordPosition="1707">1 Large-Scale Implementation Computing the similarity between terms on a large Web crawl is a non-trivial problem, with a worst case cubic running time – O(n2m) where n is the number of terms and m is the dimensionality of the feature space. Section 2.1 introduces several optimization techniques; below we propose an algorithm for large-scale term similarity computation which calculates exact scores for all pairs of terms, generalizes to several different metrics, and is scalable to a large crawl of the Web. Our optimization strategy follows a generalized sparse-matrix multiplication approach (Sarawagi and Kirpal 2004), which is based on the wellknown observation that a scalar product of two vectors depends only on the coordinates for which both vectors have non-zero values. Further, we observe that most commonly used similarity scores for feature vectors x� and y�, such as cosine and Dice, can be decomposed into three values: one depending only on features of x�, another depending only on features of y�, and the third depending on the features shared both by x� and y�. More formally, commonly used similarity scores F(x�, y) can be expressed as: FU ,A(xi,yi),f2(4f3(A⎠ i Table 1 defines and f3 for some commo</context>
</contexts>
<marker>Sarawagi, Kirpal, 2004</marker>
<rawString>Sarawagi, S. and Kirpal, A. 2004. Efficient set joins on similarity predicates. In Proceedings of SIGMOD &apos;04. pp. 74 –754. New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sarmento</author>
<author>V Jijkuon</author>
<author>M de Rijke</author>
<author>E Oliveira</author>
</authors>
<title>More like these”: growing entity classes from seeds.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM-07.</booktitle>
<pages>959--962</pages>
<location>Lisbon,</location>
<marker>Sarmento, Jijkuon, de Rijke, Oliveira, 2007</marker>
<rawString>Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E. 2007. “More like these”: growing entity classes from seeds. In Proceedings of CIKM-07. pp. 959-962. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>M L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="1259" citStr="Turney and Littman 2003" startWordPosition="175" endWordPosition="178">. The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes. We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. 1 Introduction Computing the semantic similarity between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). For commercial engines such as Yahoo! and Google, creating lists of named entities found on the Web is critical for query analysis, document categorization, and ad matching. Computing term similarity is typically done by comparing cooccurrence vectors between all pairs of terms (Sarmento et al. 2007). Scaling th</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Turney, P. D., and Littman, M. L. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Wang</author>
<author>W W Cohen</author>
</authors>
<title>Iterative Set Expansion of Named Entities using the Web. In</title>
<date>2008</date>
<booktitle>Proceedings of ICDM</booktitle>
<location>Pisa, Italy.</location>
<marker>Wang, Cohen, 2008</marker>
<rawString>Wang, R.C. and Cohen, W.W. 2008. Iterative Set Expansion of Named Entities using the Web. In Proceedings of ICDM 2008. Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C</author>
<author>W W Cohen</author>
</authors>
<title>LanguageIndependent Set Expansion of Named Entities Using the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of ICDM-07.</booktitle>
<marker>C, Cohen, 2007</marker>
<rawString>Wang. R.C. and Cohen, W.W. 2007 LanguageIndependent Set Expansion of Named Entities Using the Web. In Proceedings of ICDM-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yuret</author>
<author>M A Yatbaz</author>
</authors>
<title>The noisy channel model for unsupervised word sense disambiguation. Computational Linguistics. Under review.</title>
<date>2009</date>
<contexts>
<context position="1310" citStr="Yuret and Yatbaz 2009" startWordPosition="182" endWordPosition="185">s computed in 50 hours using 200 quad-core nodes. We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. 1 Introduction Computing the semantic similarity between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). For commercial engines such as Yahoo! and Google, creating lists of named entities found on the Web is critical for query analysis, document categorization, and ad matching. Computing term similarity is typically done by comparing cooccurrence vectors between all pairs of terms (Sarmento et al. 2007). Scaling this task to the Web requires parallelization and opt</context>
</contexts>
<marker>Yuret, Yatbaz, 2009</marker>
<rawString>Yuret, D., and Yatbaz, M. A. 2009. The noisy channel model for unsupervised word sense disambiguation. Computational Linguistics. Under review.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>