<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000095">
<title confidence="0.945491">
Opinion and Generic Question Answering Systems: a Performance
Analysis
</title>
<note confidence="0.709612">
1
</note>
<author confidence="0.929752">
Alexandra Balahur
</author>
<affiliation confidence="0.943246">
1DLSI, University of Alicante
</affiliation>
<address confidence="0.720613666666667">
Ap. De Correos 99, 03080, Alicante
2IPSC, EC Joint Research Centre
Via E. Fermi, 21027, Ispra
</address>
<email confidence="0.998286">
abalahur@dlsi.ua.es
</email>
<author confidence="0.995942">
Andres Montoyo
</author>
<affiliation confidence="0.997165">
DLSI, University of Alicante
</affiliation>
<author confidence="0.319429">
Ap. De Correos 99, 03080, Alicante
</author>
<email confidence="0.976931">
montoyo@dlsi.ua.es
</email>
<sectionHeader confidence="0.995345" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99932735">
The importance of the new textual genres such
as blogs or forum entries is growing in parallel
with the evolution of the Social Web. This pa-
per presents two corpora of blog posts in Eng-
lish and in Spanish, annotated according to the
EmotiBlog annotation scheme. Furthermore,
we created 20 factual and opinionated ques-
tions for each language and also the Gold
Standard for their answers in the corpus. The
purpose of our work is to study the challenges
involved in a mixed fact and opinion question
answering setting by comparing the perform-
ance of two Question Answering (QA) sys-
tems as far as mixed opinion and factual set-
ting is concerned. The first one is open do-
main, while the second one is opinion-
oriented. We evaluate separately the two sys-
tems in both languages and propose possible
solutions to improve QA systems that have to
process mixed questions.
</bodyText>
<sectionHeader confidence="0.972997" genericHeader="categories and subject descriptors">
Introduction and motivation
</sectionHeader>
<bodyText confidence="0.999673909090909">
In the last few years, the number of blogs has
grown exponentially. Thus, the Web contains
more and more subjective texts. A research from
the Pew Institute shows that 75.000 blogs are
created daily (Pang and Lee, 2008). They ap-
proach a great variety of topics (computer sci-
ence, sociology, political science or economics)
and are written by different types of people, thus
are a relevant resource for large community be-
havior analysis. Due to the high volume of data
contained in blogs, new Natural Language Proc-
</bodyText>
<author confidence="0.690959">
Ester Boldrini
</author>
<affiliation confidence="0.64709">
DLSI, University of Alicante
Ap. De Correos 99, 03080, Alicante
</affiliation>
<email confidence="0.992088">
eboldrini@dlsi.ua.es
</email>
<author confidence="0.980556">
Patricio MartinezBarco
</author>
<affiliation confidence="0.8219235">
DLSI, University of Alicante
Ap. De Correos 99, 03080, Alicante
</affiliation>
<email confidence="0.921761">
patricio@dlsi.ua.es
</email>
<bodyText confidence="0.999937961538461">
essing (NLP) resources, tools and methods are
needed in order to manage their language under-
standing. Our fist contribution consists in carry-
ing out a multilingual research, for English and
Spanish. Secondly, many sources are present in
blogs, as people introduce quotes from newspa-
per articles or other information to support their
arguments and make references to previous posts
in the discussion thread. Thus, when performing
a task such as Question Answering (QA), many
new aspects have to be taken into consideration.
Previous studies in the field (Stoyanov, Cardie
and Wiebe, 2005) showed that certain types of
queries, which are factual in nature, require the
use of Opinion Mining (OM) resources and tech-
niques to retrieve the correct answers. A further
contribution this paper brings is the analysis and
definition of the criteria for the discrimination
among types of factual versus opinionated ques-
tions. Previous researchers mainly concentrated
on newspaper collections. We formulated and
annotated of a set of questions and answers over
a multilingual blog collection. A further contri-
bution is the evaluation and comparison of two
different approaches to QA a fact-oriented one
and another designed for opinion QA scenarios.
</bodyText>
<subsectionHeader confidence="0.650611">
Related work
</subsectionHeader>
<bodyText confidence="0.999424571428571">
Research in building factoid QA systems has a
long history. However, it is only recently that
studies have started to focus also on the creation
and development of QA systems for opinions.
Recent years have seen the growth of interest in
this field, both by the research performed and the
publishing of various studies on the requirements
</bodyText>
<page confidence="0.967041">
157
</page>
<note confidence="0.66839">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 157–160,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.981555166666667">
and peculiarities of opinion QA systems (Stoy-
anov, Cardie and Wiebe, 2005), (Pustejovsky
and Wiebe, 2006), as well as the organization of
international conferences that promote the crea-
tion of effective QA systems both for general and
subjective texts, as, for example, the Text Analy-
sis Conference (TAC)1. Last year&apos;s TAC 2008
Opinion QA track proposed a mixed setting of
factoid (&amp;quot;rigid list&amp;quot;) and opinion questions
(&amp;quot;squishy list&amp;quot;), to which the traditional systems
had to be adapted. The Alyssa system (Shen et
al., 2007), classified the polarity of the question
and of the extracted answer snippet, using a Sup-
port Vector Machines classifier trained on the
MPQA corpus (Wiebe, Wilson and Cardie,
2005), English NTCIR2 data and rules based on
the subjectivity lexicon (Wilson, Wiebe and
Hoffman, 2005). The PolyU (Wenjie et al.,
2008) system determines the sentiment orienta-
tion with two estimated language models for the
positive versus negative categories. The
QUANTA (Li, 2008) system detects the opinion
holder, the object and the polarity of the opinion
using a semantic labeler based on PropBank3 and
some manually defined patterns.
Evaluation
In order to carry out our evaluation, we em-
ployed a corpus of blog posts presented in
(Boldrini et al., 2009). It is a collection of blog
entries in English, Spanish and Italian. However,
for this research we used the first two languages.
We annotated it using EmotiBlog (Balahur et al.,
2009) and we also created a list of 20 questions
for each language. Finally, we produced the Gold
Standard, by labeling the corpus with the correct
answers corresponding to the questions.
</bodyText>
<subsectionHeader confidence="0.986727">
1.1 Questions
</subsectionHeader>
<table confidence="0.544465235294118">
No TYPE QUESTION
1 F F What international organization do people criticize for
its policy on carbon emissions?
jCudl fue uno de los primeros paises que se preocupd
por el problema medioambiental?
2 O F What motivates people&apos;s negative opinions on the
Kyoto Protocol?
jCudl es el pais con mayor responsabilidad de la
contaminacidn mundial segtin la opinidn piiblica?
3 F F What country do people praise for not signing the
Kyoto Protocol?
jQuien piensa que la reduccidn de la contaminacidn se
deberia apoyar en los consejos de los cientificos?
4 F F What is the nation that brings most criticism to the
Kyoto Protocol?
jQue administracidn actua totalmente en contra de la
lucha contra el cambio climdtico?
</table>
<footnote confidence="0.994724666666667">
1 http://www.nist.gov/tac/
2 http://research.nii.ac.jp/ntcir/
3 http://verbs.colorado.edu/—mpalmer/projects/ace.html
</footnote>
<table confidence="0.9925625">
5 O F What are the reasons for the success of the Kyoto
Protocol?
jQue personaje importante estd a favor de la
colaboracidn del estado en la lucha contra el
calentamiento global?
6 O F What arguments do people bring for their criticism of
media as far as the Kyoto Protocol is concerned?
jA que politicos americanos culpa la gente por la
grave situacidn en la que se encuentra el planeta?
7 O F Why do people criticize Richard Branson?
jA quien reprocha la gente el fracaso del Protocolo de
Kyoto?
8 F F What president is criticized worldwide for his reaction
to the Kyoto Protocol?
jQuien acusa a China por provocar el mayor dano al
medio ambiente?
9 F O What American politician is thought to have developed
bad environmental policies?
jCdmo ven los expertos el futuro?
10 F O What American politician has a positive opinion on the
Kyoto protocol?
Cdmo se considera el atentado del 11 de septiembre?
11 O O What negative opinions do people have on Hilary
Benn?
jCudl es la opinidn sobre EEUU?
12 O O Why do Americans praise Al Gore&apos;s attitude towards
the Kyoto protocol and other environmental issues?
jDe ddnde viene la riqueza de EEUU?
13 F O What country disregards the importance of the Kyoto
Protocol?
jPor que la guerra es negativa?
14 F O What country is thought to have rejected the Kyoto
Protocol due to corruption?
jPor que Bush se retird del Protocolo de Kyoto?
15 F/ O What alternative environmental friendly resources do
O people suggest to use instead of gas en the future?
jCudl fue la posicidn de EEUU sobre el Protocolo de
Kyoto?
16 F/ O Is Arnold Schwarzenegger pro or against the reduction
O of CO2 emissions?
jQue piensa Bush sobre el cambio climdtico?
17 F O What American politician supports the reduction of
CO2 emissions?
jQue impresidn da Bush?
18 F/ O What improvements are proposed to the Kyoto Proto-
O col?
jQuepiensa China del calentamiento global?
19 F/ O What is Bush accused of as far as political measures
O are concerned?
jCudl es la opinidn de Rusia sobre el Protocolo de
Kyoto?
20 F/ O What initiative of an international body is thought to be
O a good continuation for the Kyoto Protocol?
jQue cree que es necesario hacer Yvo Boer?
</table>
<tableCaption confidence="0.999907">
Table 1: List of question in English and Spanish
</tableCaption>
<bodyText confidence="0.996563571428571">
As it can be seen in the table above, we created
factoid (F) and opinion (O) queries for English
and for Spanish; however, there are some that
could be defined between factoid and opinion
(F/O) and the system can retrieve multiple an-
swers after having selected, for example, the po-
larity of the sentences in the corpus.
</bodyText>
<sectionHeader confidence="0.396562" genericHeader="general terms">
1. 2 Performance of the two systems
</sectionHeader>
<bodyText confidence="0.9882694">
We evaluated and compared the generic QA sys-
tem of the University of Alicante (Moreda et al.,
2008) and the opinion QA system presented in
(Balahur et al., 2008), in which Named Entity
Recognition with LingPipe4 and FreeLing5 was
</bodyText>
<footnote confidence="0.9932655">
4 http://alias-i.com/lingpipe/
5 http://garraf.epsevg.upc.es/freeling/
</footnote>
<page confidence="0.992285">
158
</page>
<bodyText confidence="0.9999784">
added, in order to boost the scores of answers
containing NEs of the question Expected Answer
Type (EAT). Table 2 presents the results ob-
tained for English and Table 3 for Spanish. We
indicate the id of the question (Q), the question
type (T) and the number of answer of the Gold
Standard (A). We present the number of the re-
trieved questions by the traditional system
(TQA) and by the opinion one (OQA). We take
into account the first 1, 5, 10 and 50 answers.
</bodyText>
<table confidence="0.984351">
Q T A Number of found answers
@1 @5 @10 @ 50
TQA OQA TQA OQA TQA OQA TQA OQA
1 F 5 0 0 0 2 0 3 4 4
2 O 5 0 0 0 1 0 1 0 3
3 F 2 1 1 2 1 2 1 2 1
4 F 10 1 1 2 1 6 2 10 4
5 O 11 0 0 0 0 0 0 0 0
6 O 2 0 0 0 0 0 1 0 2
7 O 5 0 0 0 0 0 1 0 3
8 F 5 1 0 3 1 3 1 5 1
9 F 5 0 1 0 2 0 2 1 3
10 F 2 1 0 1 0 1 1 2 1
11 O 2 0 1 0 1 0 1 0 1
12 O 3 0 0 0 1 0 1 0 1
13 F 1 0 0 0 0 0 0 0 1
14 F 7 1 0 1 1 1 2 1 2
15 F/O 1 0 0 0 0 0 1 0 1
16 F/O 6 0 1 0 4 0 4 0 4
17 F 10 0 1 0 1 4 1 0 2
18 F/O 1 0 0 0 0 0 0 0 0
19 F/O 27 0 1 0 5 0 6 0 18
20 F/O 4 0 0 0 0 0 0 0 0
</table>
<tableCaption confidence="0.671091">
Table 2: Results for English
</tableCaption>
<table confidence="0.999702913043478">
Q T A Number of found answers
@1 @5 @10 @ 50
TQA OQA TQA OQA TQA OQA TQA OQA
1 F 9 1 0 0 1 1 1 1 3
2 F 13 0 1 2 3 0 6 11 7
3 F 2 0 1 0 2 0 2 2 2
4 F 1 0 0 0 0 0 0 1 0
5 F 3 0 0 0 0 0 0 1 0
6 F 2 0 0 0 1 0 1 2 1
7 F 4 0 0 0 0 1 0 4 0
8 F 1 0 0 0 0 0 0 1 0
9 O 5 0 1 0 2 0 2 0 4
10 O 2 0 0 0 0 0 0 0 0
11 O 5 0 0 0 1 0 2 0 3
12 O 2 0 0 0 1 0 1 0 1
13 O 8 0 1 0 2 0 2 0 4
14 O 25 0 1 0 2 0 4 0 8
15 O 36 0 1 0 2 0 6 0 15
16 O 23 0 0 0 0 0 0 0 0
17 O 50 0 1 0 5 0 6 0 10
18 O 10 0 1 0 1 0 2 0 2
19 O 4 0 1 0 1 0 1 0 1
20 O 4 0 1 0 1 0 1 0 1
</table>
<tableCaption confidence="0.995638">
Table 3: Results for Spanish
</tableCaption>
<subsectionHeader confidence="0.907528">
1.3 Results and discussion
</subsectionHeader>
<bodyText confidence="0.999989471698113">
There are many problems involved when trying
to perform mixed fact and opinion QA. The first
can be the ambiguity of the questions e.g. ZDe
donde viene la riqueza de EEUU?. The answer
can be explicitly stated in one of the blog sen-
tences, or a system might have to infer them
from assumptions made by the bloggers and their
comments. Moreover, most of the opinion ques-
tions have longer answers, not just a phrase snip-
pet, but up to 2 or 3 sentences. As we can ob-
serve in Table 2, the questions for which the
TQA system performed better were the pure fac-
tual ones (1, 3, 4, 8, 10 and 14), although in some
cases (question number 14) the OQA system re-
trieved more correct answers. At the same time,
opinion queries, although revolving around NEs,
were not answered by the traditional QA system,
but were satisfactorily answered by the opinion
QA system (2, 5, 6, 7, 11, 12). Questions 18 and
20 were not correctly answered by any of the two
systems. We believe the reason is that question
18 was ambiguous as far as polarity of the opin-
ions expressed in the answer snippets (&amp;quot;im-
provement&amp;quot; does not translate to either &amp;quot;positive&amp;quot;
or &amp;quot;negative&amp;quot;) and question 20 referred to the
title of a project proposal that was not annotated
by any of the tools used. Thus, as part of the fu-
ture work in our OQA system, we must add a
component for the identification of quotes and
titles, as well as explore a wider range of polar-
ity/opinion scales. Furthermore, questions 15, 16,
18, 19 and 20 contain both factual as well as
opinion aspects and the OQA system performed
better than the TQA, although in some cases,
answers were lost due to the artificial boosting of
the queries containing NEs of the EAT (Ex-
pected Answer Type). Therefore, it is obvious
that an extra method for answer ranking should
be used, as Answer Validation techniques using
Textual Entailment. In Table 3, the OQA missed
some of the answers due to erroneous sentence
splitting, either separating text into two sentences
where it was not the case or concatenating two
consecutive sentences; thus missing out on one
of two consecutively annotated answers. Exam-
ples are questions number 16 and 17, where
many blog entries enumerated the different ar-
guments in consecutive sentences. Another
source of problems was the fact that we gave a
high weight to the presence of the NE of the
sought type within the retrieved snippet and in
some cases the name was misspelled in the blog
entries, whereas in other NER performed by
</bodyText>
<page confidence="0.99649">
159
</page>
<bodyText confidence="0.9993654">
FreeLing either attributed the wrong category to
an entity, failed to annotate it or wrongfully an-
notated words as being NEs. Not of less impor-
tance is the question duality aspect in question
17. Bush is commented in more than 600 sen-
tences; therefore, when polarity is not specified,
it is difficult to correctly rank the answers. Fi-
nally, also the problems of temporal expressions
and the coreference need to be taken into ac-
count.
</bodyText>
<subsectionHeader confidence="0.562364">
Conclusions and future work
</subsectionHeader>
<bodyText confidence="0.999924772727273">
In this article, we created a collection of both
factual and opinion queries in Spanish and Eng-
lish. We labeled the Gold Standard of the an-
swers in the corpora and subsequently we em-
ployed two QA systems, one open domain, one
for opinion questions. Our main objective was to
compare the performances of these two systems
and analyze their errors, proposing solutions to
creating an effective QA system for both factoid
an opinionated queries. We saw that, even using
specialized resources, the task of QA is still chal-
lenging. Opinion QA can benefit from a snippet
retrieval at a paragraph level, since in many
cases the answers were not simple parts of sen-
tences, but consisted in two or more consecutive
sentences. On the other hand, we have seen cases
in which each of three different consecutive sen-
tences was a separate answer to a question. Our
future work contemplates the study of the impact
anaphora resolution and temporality on opinion
QA, as well as the possibility to use Answer
Validation techniques for answer re-ranking.
</bodyText>
<sectionHeader confidence="0.997292" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999838666666667">
The authors would like to thank Paloma Moreda,
Hector Llorens, Estela Saquete and Manuel
Palomar for evaluating the questions on their QA
system. This research has been partially funded
by the Spanish Government under the project
TEXT-MESS (TIN 2006-15265-C06-01), by the
European project QALL-ME (FP6 IST 033860)
and by the University of Alicante, through its
doctoral scholarship.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999639868852459">
Alexandra Balahur, Ester Boldrini, Andr6s Montoyo,
and Patricio Martinez-Barco, 2009. Cross-topic
Opinion Mining for Real-time Human-Computer
Interaction. In Proceedings of the 6th Workshop in
Natural Language Processing and Cognitive Sci-
ence, ICEIS 2009 Conference, Milan, Italy.
Alexandra Balahur, Elena Lloret, Oscar Ferrandez,
Andr6s Montoyo, Manuel Palomar, Rafael Munoz.
2008. The DLSIUAES Team&apos;s Participation in the
TAC 2008 Tracks. In Proceedings of the Text
Analysis Conference (TAC 2008).
Ester Boldrini, Alexandra Balahur, Patricio Martinez-
Barco, and Andr6s Montoyo. 2009. EmotiBlog: An
Annotation Scheme for Emotion Detection and
Analysis in Non-Traditional Textual Genres. To
appear in Proceedings of the 5th Conference on
data Mining. Las Vegas, Nevada, USA.
W. Li, Y. Ouyang, Y. Hu, F. Wei. PolyU at TAC
2008. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical
methods in Natural Language Processing
(HLT/EMNLP), Vancouver, BC, Canada, 2008.
Fangtao Li, Zhicheng Zheng, Tang Yang, Fan Bu,
Rong Ge, Xiaoyan Zhu, Xian Zhang, and Minlie
Huang. THU QUANTA at TAC 2008 QA and RTE
track. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical
methods in Natural Language Processing
(HLT/EMNLP), Vancouver, BC, Canada, 2008.
Bo Pang, and Lilian. Lee, Opinion mining and senti-
ment analysis. Foundations and Trends R. In In-
formation Retrieval Vol. 2, Nos. 1-2 (2008) 1-135,
2008.
James Pustejovsky and Janyce. Wiebe. Introduction
to Special Issue on Advances in Question Answer-
ing. In Language Resources and Evaluation (2005)
39: 119-122. Springer, 2006.
Dan Shen, Jochen L. Leidner, Andreas Merkel, Diet-
rich Klakow. The Alyssa system at TREC QA 2007:
Do we need Blog06? In Proceedings of The Six-
teenth Text Retrieval Conference (TREC 2007),
Gaithersburg, MD, USA, 2007
Vaselin, Stoyanov, Claire Cardie, Janyce Wiebe.
Multi-Perspective Question Answering Using the
OpQA Corpus. In Proceedings of HLT/EMNLP.
2005.
Paloma Moreda, Hector Llorens, Estela Saquete,
Manuel Palomar. 2008. Automatic Generalization
of a QA Answer Extraction Module Based on Se-
mantic Roles. In: AAI - IBERAMIA, Lisbon, Portu-
gal, pages 233-242, Springer.
Janyce. Wiebe, Theresa Wilson, and Claire Cardie
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
volume 39, issue 2-3, pp. 165-210, 2005.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
Recognising Contextual Polarity in Phrase-level
sentiment Analysis. In Proceedings of Human lan-
guage Technologies Conference/Conference on
Empirical methods in Natural Language Processing
(HLT/EMNLP), Vancouver, BC, Canada, 2005.
</reference>
<page confidence="0.997485">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.271920">
<title confidence="0.817875">Opinion and Generic Question Answering Systems: a Performance</title>
<note confidence="0.6851135">Analysis 1</note>
<author confidence="0.99642">Alexandra Balahur</author>
<affiliation confidence="0.999759">University of Alicante</affiliation>
<address confidence="0.890093">Ap. De Correos 99, 03080, Alicante</address>
<affiliation confidence="0.977979">EC Joint Research Centre</affiliation>
<address confidence="0.925283">Via E. Fermi, 21027, Ispra</address>
<email confidence="0.813947">abalahur@dlsi.ua.es</email>
<author confidence="0.99683">Andres Montoyo</author>
<affiliation confidence="0.999256">DLSI, University of Alicante</affiliation>
<address confidence="0.927748">Ap. De Correos 99, 03080, Alicante</address>
<email confidence="0.983139">montoyo@dlsi.ua.es</email>
<abstract confidence="0.998791380952381">The importance of the new textual genres such as blogs or forum entries is growing in parallel with the evolution of the Social Web. This paper presents two corpora of blog posts in English and in Spanish, annotated according to the scheme. Furthermore, we created 20 factual and opinionated quesfor each language and also the their answers in the corpus. The purpose of our work is to study the challenges involved in a mixed fact and opinion question answering setting by comparing the performance of two Question Answering (QA) systems as far as mixed opinion and factual setting is concerned. The first one is open domain, while the second one is opinionoriented. We evaluate separately the two systems in both languages and propose possible solutions to improve QA systems that have to process mixed questions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Ester Boldrini</author>
<author>Andr6s Montoyo</author>
<author>Patricio Martinez-Barco</author>
</authors>
<title>Cross-topic Opinion Mining for Real-time Human-Computer Interaction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 6th Workshop in Natural Language Processing and Cognitive Science, ICEIS 2009 Conference,</booktitle>
<location>Milan, Italy.</location>
<contexts>
<context position="5123" citStr="Balahur et al., 2009" startWordPosition="813" endWordPosition="816">t al., 2008) system determines the sentiment orientation with two estimated language models for the positive versus negative categories. The QUANTA (Li, 2008) system detects the opinion holder, the object and the polarity of the opinion using a semantic labeler based on PropBank3 and some manually defined patterns. Evaluation In order to carry out our evaluation, we employed a corpus of blog posts presented in (Boldrini et al., 2009). It is a collection of blog entries in English, Spanish and Italian. However, for this research we used the first two languages. We annotated it using EmotiBlog (Balahur et al., 2009) and we also created a list of 20 questions for each language. Finally, we produced the Gold Standard, by labeling the corpus with the correct answers corresponding to the questions. 1.1 Questions No TYPE QUESTION 1 F F What international organization do people criticize for its policy on carbon emissions? jCudl fue uno de los primeros paises que se preocupd por el problema medioambiental? 2 O F What motivates people&apos;s negative opinions on the Kyoto Protocol? jCudl es el pais con mayor responsabilidad de la contaminacidn mundial segtin la opinidn piiblica? 3 F F What country do people praise f</context>
</contexts>
<marker>Balahur, Boldrini, Montoyo, Martinez-Barco, 2009</marker>
<rawString>Alexandra Balahur, Ester Boldrini, Andr6s Montoyo, and Patricio Martinez-Barco, 2009. Cross-topic Opinion Mining for Real-time Human-Computer Interaction. In Proceedings of the 6th Workshop in Natural Language Processing and Cognitive Science, ICEIS 2009 Conference, Milan, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Elena Lloret</author>
<author>Oscar Ferrandez</author>
<author>Andr6s Montoyo</author>
<author>Manuel Palomar</author>
<author>Rafael Munoz</author>
</authors>
<title>Tracks.</title>
<date>2008</date>
<booktitle>The DLSIUAES Team&apos;s Participation in the TAC</booktitle>
<contexts>
<context position="8878" citStr="Balahur et al., 2008" startWordPosition="1461" endWordPosition="1464">o Protocol? jQue cree que es necesario hacer Yvo Boer? Table 1: List of question in English and Spanish As it can be seen in the table above, we created factoid (F) and opinion (O) queries for English and for Spanish; however, there are some that could be defined between factoid and opinion (F/O) and the system can retrieve multiple answers after having selected, for example, the polarity of the sentences in the corpus. 1. 2 Performance of the two systems We evaluated and compared the generic QA system of the University of Alicante (Moreda et al., 2008) and the opinion QA system presented in (Balahur et al., 2008), in which Named Entity Recognition with LingPipe4 and FreeLing5 was 4 http://alias-i.com/lingpipe/ 5 http://garraf.epsevg.upc.es/freeling/ 158 added, in order to boost the scores of answers containing NEs of the question Expected Answer Type (EAT). Table 2 presents the results obtained for English and Table 3 for Spanish. We indicate the id of the question (Q), the question type (T) and the number of answer of the Gold Standard (A). We present the number of the retrieved questions by the traditional system (TQA) and by the opinion one (OQA). We take into account the first 1, 5, 10 and 50 answ</context>
</contexts>
<marker>Balahur, Lloret, Ferrandez, Montoyo, Palomar, Munoz, 2008</marker>
<rawString>Alexandra Balahur, Elena Lloret, Oscar Ferrandez, Andr6s Montoyo, Manuel Palomar, Rafael Munoz. 2008. The DLSIUAES Team&apos;s Participation in the TAC 2008 Tracks. In Proceedings of the Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ester Boldrini</author>
<author>Alexandra Balahur</author>
<author>Patricio MartinezBarco</author>
<author>Andr6s Montoyo</author>
</authors>
<title>EmotiBlog: An Annotation Scheme for Emotion Detection and Analysis in Non-Traditional Textual Genres.</title>
<date>2009</date>
<booktitle>Proceedings of the 5th Conference on data Mining. Las Vegas,</booktitle>
<location>Nevada, USA.</location>
<note>To appear in</note>
<contexts>
<context position="4939" citStr="Boldrini et al., 2009" startWordPosition="782" endWordPosition="785">ifier trained on the MPQA corpus (Wiebe, Wilson and Cardie, 2005), English NTCIR2 data and rules based on the subjectivity lexicon (Wilson, Wiebe and Hoffman, 2005). The PolyU (Wenjie et al., 2008) system determines the sentiment orientation with two estimated language models for the positive versus negative categories. The QUANTA (Li, 2008) system detects the opinion holder, the object and the polarity of the opinion using a semantic labeler based on PropBank3 and some manually defined patterns. Evaluation In order to carry out our evaluation, we employed a corpus of blog posts presented in (Boldrini et al., 2009). It is a collection of blog entries in English, Spanish and Italian. However, for this research we used the first two languages. We annotated it using EmotiBlog (Balahur et al., 2009) and we also created a list of 20 questions for each language. Finally, we produced the Gold Standard, by labeling the corpus with the correct answers corresponding to the questions. 1.1 Questions No TYPE QUESTION 1 F F What international organization do people criticize for its policy on carbon emissions? jCudl fue uno de los primeros paises que se preocupd por el problema medioambiental? 2 O F What motivates pe</context>
</contexts>
<marker>Boldrini, Balahur, MartinezBarco, Montoyo, 2009</marker>
<rawString>Ester Boldrini, Alexandra Balahur, Patricio MartinezBarco, and Andr6s Montoyo. 2009. EmotiBlog: An Annotation Scheme for Emotion Detection and Analysis in Non-Traditional Textual Genres. To appear in Proceedings of the 5th Conference on data Mining. Las Vegas, Nevada, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>Y Ouyang</author>
<author>Y Hu</author>
<author>F Wei</author>
</authors>
<title>PolyU at TAC</title>
<date>2008</date>
<booktitle>In Proceedings of Human Language Technologies Conference/Conference on Empirical methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<location>Vancouver, BC, Canada,</location>
<marker>Li, Ouyang, Hu, Wei, 2008</marker>
<rawString>W. Li, Y. Ouyang, Y. Hu, F. Wei. PolyU at TAC 2008. In Proceedings of Human Language Technologies Conference/Conference on Empirical methods in Natural Language Processing (HLT/EMNLP), Vancouver, BC, Canada, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Zhicheng Zheng</author>
<author>Tang Yang</author>
<author>Fan Bu</author>
<author>Rong Ge</author>
<author>Xiaoyan Zhu</author>
<author>Xian Zhang</author>
<author>Minlie Huang</author>
</authors>
<date>2008</date>
<booktitle>THU QUANTA at TAC 2008 QA and RTE track. In Proceedings of Human Language Technologies Conference/Conference on Empirical methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<location>Vancouver, BC, Canada,</location>
<marker>Li, Zheng, Yang, Bu, Ge, Zhu, Zhang, Huang, 2008</marker>
<rawString>Fangtao Li, Zhicheng Zheng, Tang Yang, Fan Bu, Rong Ge, Xiaoyan Zhu, Xian Zhang, and Minlie Huang. THU QUANTA at TAC 2008 QA and RTE track. In Proceedings of Human Language Technologies Conference/Conference on Empirical methods in Natural Language Processing (HLT/EMNLP), Vancouver, BC, Canada, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and Trends R.</title>
<date>2008</date>
<journal>In Information Retrieval</journal>
<volume>2</volume>
<pages>1--2</pages>
<contexts>
<context position="1450" citStr="Lee, 2008" startWordPosition="236" endWordPosition="237">answering setting by comparing the performance of two Question Answering (QA) systems as far as mixed opinion and factual setting is concerned. The first one is open domain, while the second one is opinionoriented. We evaluate separately the two systems in both languages and propose possible solutions to improve QA systems that have to process mixed questions. Introduction and motivation In the last few years, the number of blogs has grown exponentially. Thus, the Web contains more and more subjective texts. A research from the Pew Institute shows that 75.000 blogs are created daily (Pang and Lee, 2008). They approach a great variety of topics (computer science, sociology, political science or economics) and are written by different types of people, thus are a relevant resource for large community behavior analysis. Due to the high volume of data contained in blogs, new Natural Language ProcEster Boldrini DLSI, University of Alicante Ap. De Correos 99, 03080, Alicante eboldrini@dlsi.ua.es Patricio MartinezBarco DLSI, University of Alicante Ap. De Correos 99, 03080, Alicante patricio@dlsi.ua.es essing (NLP) resources, tools and methods are needed in order to manage their language understandin</context>
</contexts>
<marker>Lee, 2008</marker>
<rawString>Bo Pang, and Lilian. Lee, Opinion mining and sentiment analysis. Foundations and Trends R. In Information Retrieval Vol. 2, Nos. 1-2 (2008) 1-135, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wiebe</author>
</authors>
<title>Introduction to Special Issue on Advances in Question Answering.</title>
<date>2005</date>
<booktitle>In Language Resources and Evaluation</booktitle>
<volume>39</volume>
<pages>119--122</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="2538" citStr="Wiebe, 2005" startWordPosition="403" endWordPosition="404">ante patricio@dlsi.ua.es essing (NLP) resources, tools and methods are needed in order to manage their language understanding. Our fist contribution consists in carrying out a multilingual research, for English and Spanish. Secondly, many sources are present in blogs, as people introduce quotes from newspaper articles or other information to support their arguments and make references to previous posts in the discussion thread. Thus, when performing a task such as Question Answering (QA), many new aspects have to be taken into consideration. Previous studies in the field (Stoyanov, Cardie and Wiebe, 2005) showed that certain types of queries, which are factual in nature, require the use of Opinion Mining (OM) resources and techniques to retrieve the correct answers. A further contribution this paper brings is the analysis and definition of the criteria for the discrimination among types of factual versus opinionated questions. Previous researchers mainly concentrated on newspaper collections. We formulated and annotated of a set of questions and answers over a multilingual blog collection. A further contribution is the evaluation and comparison of two different approaches to QA a fact-oriented</context>
<context position="3750" citStr="Wiebe, 2005" startWordPosition="594" endWordPosition="595"> one and another designed for opinion QA scenarios. Related work Research in building factoid QA systems has a long history. However, it is only recently that studies have started to focus also on the creation and development of QA systems for opinions. Recent years have seen the growth of interest in this field, both by the research performed and the publishing of various studies on the requirements 157 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 157–160, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP and peculiarities of opinion QA systems (Stoyanov, Cardie and Wiebe, 2005), (Pustejovsky and Wiebe, 2006), as well as the organization of international conferences that promote the creation of effective QA systems both for general and subjective texts, as, for example, the Text Analysis Conference (TAC)1. Last year&apos;s TAC 2008 Opinion QA track proposed a mixed setting of factoid (&amp;quot;rigid list&amp;quot;) and opinion questions (&amp;quot;squishy list&amp;quot;), to which the traditional systems had to be adapted. The Alyssa system (Shen et al., 2007), classified the polarity of the question and of the extracted answer snippet, using a Support Vector Machines classifier trained on the MPQA corpus </context>
</contexts>
<marker>Wiebe, 2005</marker>
<rawString>James Pustejovsky and Janyce. Wiebe. Introduction to Special Issue on Advances in Question Answering. In Language Resources and Evaluation (2005) 39: 119-122. Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jochen L Leidner</author>
<author>Andreas Merkel</author>
<author>Dietrich Klakow</author>
</authors>
<title>The Alyssa system at TREC QA 2007: Do we need Blog06?</title>
<date>2007</date>
<booktitle>In Proceedings of The Sixteenth Text Retrieval Conference (TREC 2007),</booktitle>
<location>Gaithersburg, MD, USA,</location>
<contexts>
<context position="4201" citStr="Shen et al., 2007" startWordPosition="664" endWordPosition="667">nference Short Papers, pages 157–160, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP and peculiarities of opinion QA systems (Stoyanov, Cardie and Wiebe, 2005), (Pustejovsky and Wiebe, 2006), as well as the organization of international conferences that promote the creation of effective QA systems both for general and subjective texts, as, for example, the Text Analysis Conference (TAC)1. Last year&apos;s TAC 2008 Opinion QA track proposed a mixed setting of factoid (&amp;quot;rigid list&amp;quot;) and opinion questions (&amp;quot;squishy list&amp;quot;), to which the traditional systems had to be adapted. The Alyssa system (Shen et al., 2007), classified the polarity of the question and of the extracted answer snippet, using a Support Vector Machines classifier trained on the MPQA corpus (Wiebe, Wilson and Cardie, 2005), English NTCIR2 data and rules based on the subjectivity lexicon (Wilson, Wiebe and Hoffman, 2005). The PolyU (Wenjie et al., 2008) system determines the sentiment orientation with two estimated language models for the positive versus negative categories. The QUANTA (Li, 2008) system detects the opinion holder, the object and the polarity of the opinion using a semantic labeler based on PropBank3 and some manually </context>
</contexts>
<marker>Shen, Leidner, Merkel, Klakow, 2007</marker>
<rawString>Dan Shen, Jochen L. Leidner, Andreas Merkel, Dietrich Klakow. The Alyssa system at TREC QA 2007: Do we need Blog06? In Proceedings of The Sixteenth Text Retrieval Conference (TREC 2007), Gaithersburg, MD, USA, 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stoyanov Vaselin</author>
</authors>
<title>Claire Cardie, Janyce Wiebe. Multi-Perspective Question Answering Using the OpQA Corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<marker>Vaselin, 2005</marker>
<rawString>Vaselin, Stoyanov, Claire Cardie, Janyce Wiebe. Multi-Perspective Question Answering Using the OpQA Corpus. In Proceedings of HLT/EMNLP. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paloma Moreda</author>
<author>Hector Llorens</author>
<author>Estela Saquete</author>
<author>Manuel Palomar</author>
</authors>
<title>Automatic Generalization of a QA Answer Extraction Module Based on Semantic Roles. In: AAI - IBERAMIA,</title>
<date>2008</date>
<pages>233--242</pages>
<publisher>Springer.</publisher>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="8816" citStr="Moreda et al., 2008" startWordPosition="1450" endWordPosition="1453">onal body is thought to be O a good continuation for the Kyoto Protocol? jQue cree que es necesario hacer Yvo Boer? Table 1: List of question in English and Spanish As it can be seen in the table above, we created factoid (F) and opinion (O) queries for English and for Spanish; however, there are some that could be defined between factoid and opinion (F/O) and the system can retrieve multiple answers after having selected, for example, the polarity of the sentences in the corpus. 1. 2 Performance of the two systems We evaluated and compared the generic QA system of the University of Alicante (Moreda et al., 2008) and the opinion QA system presented in (Balahur et al., 2008), in which Named Entity Recognition with LingPipe4 and FreeLing5 was 4 http://alias-i.com/lingpipe/ 5 http://garraf.epsevg.upc.es/freeling/ 158 added, in order to boost the scores of answers containing NEs of the question Expected Answer Type (EAT). Table 2 presents the results obtained for English and Table 3 for Spanish. We indicate the id of the question (Q), the question type (T) and the number of answer of the Gold Standard (A). We present the number of the retrieved questions by the traditional system (TQA) and by the opinion </context>
</contexts>
<marker>Moreda, Llorens, Saquete, Palomar, 2008</marker>
<rawString>Paloma Moreda, Hector Llorens, Estela Saquete, Manuel Palomar. 2008. Automatic Generalization of a QA Answer Extraction Module Based on Semantic Roles. In: AAI - IBERAMIA, Lisbon, Portugal, pages 233-242, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wiebe</author>
</authors>
<title>Theresa Wilson, and Claire Cardie Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<pages>2--3</pages>
<contexts>
<context position="2538" citStr="Wiebe, 2005" startWordPosition="403" endWordPosition="404">ante patricio@dlsi.ua.es essing (NLP) resources, tools and methods are needed in order to manage their language understanding. Our fist contribution consists in carrying out a multilingual research, for English and Spanish. Secondly, many sources are present in blogs, as people introduce quotes from newspaper articles or other information to support their arguments and make references to previous posts in the discussion thread. Thus, when performing a task such as Question Answering (QA), many new aspects have to be taken into consideration. Previous studies in the field (Stoyanov, Cardie and Wiebe, 2005) showed that certain types of queries, which are factual in nature, require the use of Opinion Mining (OM) resources and techniques to retrieve the correct answers. A further contribution this paper brings is the analysis and definition of the criteria for the discrimination among types of factual versus opinionated questions. Previous researchers mainly concentrated on newspaper collections. We formulated and annotated of a set of questions and answers over a multilingual blog collection. A further contribution is the evaluation and comparison of two different approaches to QA a fact-oriented</context>
<context position="3750" citStr="Wiebe, 2005" startWordPosition="594" endWordPosition="595"> one and another designed for opinion QA scenarios. Related work Research in building factoid QA systems has a long history. However, it is only recently that studies have started to focus also on the creation and development of QA systems for opinions. Recent years have seen the growth of interest in this field, both by the research performed and the publishing of various studies on the requirements 157 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 157–160, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP and peculiarities of opinion QA systems (Stoyanov, Cardie and Wiebe, 2005), (Pustejovsky and Wiebe, 2006), as well as the organization of international conferences that promote the creation of effective QA systems both for general and subjective texts, as, for example, the Text Analysis Conference (TAC)1. Last year&apos;s TAC 2008 Opinion QA track proposed a mixed setting of factoid (&amp;quot;rigid list&amp;quot;) and opinion questions (&amp;quot;squishy list&amp;quot;), to which the traditional systems had to be adapted. The Alyssa system (Shen et al., 2007), classified the polarity of the question and of the extracted answer snippet, using a Support Vector Machines classifier trained on the MPQA corpus </context>
</contexts>
<marker>Wiebe, 2005</marker>
<rawString>Janyce. Wiebe, Theresa Wilson, and Claire Cardie Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, volume 39, issue 2-3, pp. 165-210, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognising Contextual Polarity in Phrase-level sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human language Technologies Conference/Conference on Empirical methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<location>Vancouver, BC, Canada,</location>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. Recognising Contextual Polarity in Phrase-level sentiment Analysis. In Proceedings of Human language Technologies Conference/Conference on Empirical methods in Natural Language Processing (HLT/EMNLP), Vancouver, BC, Canada, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>