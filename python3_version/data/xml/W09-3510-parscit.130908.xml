<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011328">
<title confidence="0.981019">
Transliteration by Bidirectional Statistical Machine Translation
</title>
<author confidence="0.874118">
Andrew Finch
</author>
<note confidence="0.69223175">
NICT
2-2-2 Hikaridai
Keihanna Science City
619-0288 JAPAN
</note>
<email confidence="0.95799">
andrew.finch@nict.go.jp
</email>
<sectionHeader confidence="0.867635" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999827863636364">
The system presented in this paper uses
phrase-based statistical machine translation
(SMT) techniques to directly transliterate be-
tween all language pairs in this shared task.
The technique makes no language specific as-
sumptions, uses no dictionaries or explicit
phonetic information. The translation process
transforms sequences of tokens in the source
language directly into to sequences of tokens
in the target. All language pairs were transli-
terated by applying this technique in a single
unified manner. The machine translation sys-
tem used was a system comprised of two
phrase-based SMT decoders. The first gener-
ated from the first token of the target to the
last. The second system generated the target
from last to first. Our results show that if only
one of these decoding strategies is to be cho-
sen, the optimal choice depends on the lan-
guages involved, and that in general a combi-
nation of the two approaches is able to outper-
form either approach.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999761571428571">
It is possible to couch the task of machine trans-
literation as a task of machine translation. Both
processes involve the transformation of se-
quences of tokens in one language into se-
quences of tokens in another language. The
principle differences between the machine trans-
lation and language translation are:
</bodyText>
<listItem confidence="0.848512285714286">
• Transliteration does not normally re-
quire the re-ordering of tokens that are
generated in the target
• The number of types (the vocabulary
size) in both source and target languages
is considerably less for the translitera-
tion task
</listItem>
<bodyText confidence="0.9994822">
We take a statistical machine translation pa-
radigm (Brown at al., 1991) as the basis for our
systems. The work in this paper is related to the
work of (Finch and Sumita, 2008) who also use
SMT directly to transliterate.
</bodyText>
<sectionHeader confidence="0.8586862" genericHeader="introduction">
Eiichiro Sumita
NICT
2-2-2 Hikaridai
Keihanna Science City
619-0288 JAPAN
</sectionHeader>
<email confidence="0.765771">
eiichiro.sumita@nict.go.jp
</email>
<bodyText confidence="0.999920642857143">
We view the task of machine transliteration
as a process of machine translation at the cha-
racter level (Donoual and LePage, 2006). We
use state of the art phrase-based statistical ma-
chine translation systems (Koehn et al., 2003) to
perform the transliteration. By adopting this ap-
proach we were able to build systems for all of
the language pairs in the shared task using pre-
cisely the same procedures. No modeling of the
phonetics of either source or target language
(Knight and Graehl, 1997) was necessary, since
the approach is simply a direct transformation of
sequences of tokens in the source language into
sequences of tokens in the target.
</bodyText>
<sectionHeader confidence="0.991089" genericHeader="method">
2 Overview
</sectionHeader>
<bodyText confidence="0.999954117647059">
Our approach differs from the approach of
(Finch and Sumita, 2008) in that we decode bi-
directional. In a typical statistical machine trans-
lation system the sequence of target tokens is
generated in a left-to-right manner, by left-to-
right here we mean the target sequence is gener-
ated from the first token to its last. During the
generation process the models (in particular the
target language model) are able to refer to only
the target tokens that have already been generat-
ed. In our approach, by using decoders that de-
code in both directions we are able to exploit
context to the left and to the right of target to-
kens being generated. Furthermore, we expect
our system to gain because it is a combination of
two different MT systems that are performing
the same task.
</bodyText>
<sectionHeader confidence="0.998827" genericHeader="method">
3 Experimental Conditions
</sectionHeader>
<bodyText confidence="0.99987475">
In our experiments we used an in-house phrase-
based statistical machine translation decoder
called CleopATRa. This decoder operates on
exactly the same principles as the publicly
available MOSES decoder (Koehn et al., 2003).
Like MOSES we utilize a future cost in our cal-
culations. Our decoder was modified to be able
to run two instances of the decoder at the same
</bodyText>
<page confidence="0.968989">
52
</page>
<note confidence="0.987698">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 52–56,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.7774205">
Word 1 Word 2 Word m
Segment into individual words and decode each word independently
</figure>
<figureCaption confidence="0.999841">
Figure 1: The decoding process for multi-word sequences
</figureCaption>
<figure confidence="0.99424275">
... ... ...
hypothesis n
hypothesis n hypothesis n
Decode
n-best
hypothesis 1
hypothesis 2
Decode
n-best
hypothesis 1
hypothesis 2
Decode
Search for the best path
hypothesis 1
hypothesis 2
n-best
</figure>
<bodyText confidence="0.999161866666667">
time. One instance decoding from left-to-right
the other decoding from right-to-left. The hypo-
theses being combined by linearly interpolating
the scores from both decoders at the end of the
decoding process. In addition, the decoders were
constrained decode in a monotone manner. That
is, they were not allowed to re-order the phrases
during decoding. The decoders were also confi-
gured to produce a list of unique sequences of
tokens in their n-best lists. During SMT decod-
ing it is possible to derive the same sequence of
tokens in multiple ways. Multiply occurring se-
quences of this form were combined into a sin-
gle hypothesis in the n-best list by summing
their scores.
</bodyText>
<subsectionHeader confidence="0.999829">
3.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999993">
In order to reduce data sparseness issues we
took the decision to work with data in only its
lowercase form. The only target language with
case information was Russian. During the para-
meter tuning phase (where output translations
are compared against a set of references) we
restored the case for Russian by simply capita-
lizing the first character of each word.
We chose not to perform any tokenization for
any of the language pairs in the shared task. We
chose this approach for several reasons:
</bodyText>
<listItem confidence="0.991550625">
• It allowed us to have a single unified
approach for all language pairs
• It was in the spirit of the evaluation, as
it did not require specialist knowledge
outside of the supplied corpora
• It enabled us to handle the Chinese
names that occurred in the Japanese
Romaji-Japanese Kanji task
</listItem>
<bodyText confidence="0.999701333333333">
However we believe that a more effective
approach for Japanese-Kanji task may have been
to re-tokenize the alphabetic characters into ka-
na (for example transforming “k a” into the kana
consonant vowel pair “ka”) since these are the
basic building blocks of the Japanese language.
</bodyText>
<subsectionHeader confidence="0.995682">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.9999686875">
For the final submission, all systems were
trained on the union of the training data and de-
velopment data. It was felt that the training set
was sufficiently small that the inclusion of the
development data into the training set would
yield a reasonable boost in performance by in-
creasing the coverage of the language model and
phrase table. The language models and transla-
tion models were therefore built from all the
data, and the log-linear weights used to combine
the models of the systems were tuned using sys-
tems trained only on the training data. The de-
velopment data in this case being held-out. It
was assumed that these parameters would per-
form well in the systems trained on the com-
bined development/training corpora.
</bodyText>
<subsectionHeader confidence="0.995744">
3.3 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.9999172">
The SMT systems were tuned using the mini-
mum error rate training procedure introduced in
(Och, 2003). For convenience, we used BLEU
as a proxy for the various metrics used in the
shared task evaluation. The BLEU score is
</bodyText>
<page confidence="0.99486">
53
</page>
<table confidence="0.999651666666667">
En-Ch En-Ja En-Ko En-Ru Jn-Jk
After tuning 0.908 0.772 0.622 0.914 0.769
Before tuning 0.871 0.635 0.543 0.832 0.737
</table>
<tableCaption confidence="0.99997">
Table 1: The effect on 1-best accuracy by tuning with respect to BLEU score
</tableCaption>
<bodyText confidence="0.999574285714286">
commonly used to evaluate the performance of
machine translation systems and is a function of
the geometric mean of n-gram precision. Table 1
shows the effect of tuning for BLEU on the
ACC (1-best accuracy) scores for several lan-
guages. Improvements in the BLEU score also
gave improvements in ACC. Tuning to maxim-
ize the BLEU score gave improvements for all
language pairs and in all of the evaluation me-
trics used in this shared task. Nonetheless, it is
reasonable to assume that one would be able to
improve the performance in a particular evalua-
tion metric by doing minimum error rate train-
ing specifically for that metric.
</bodyText>
<subsectionHeader confidence="0.711205">
3.3.1 Multi-word sequences
</subsectionHeader>
<bodyText confidence="0.9984145">
The data for some languages (for example Hin-
di) contained some multi-word sequences. These
posed a challenge for our approach, and gave us
the following alternatives:
</bodyText>
<listItem confidence="0.98889225">
• Introduce a &lt;space&gt; token into the se-
quence, and treat it as one long charac-
ter sequence to transliterate; or
• Segment the word sequences into indi-
</listItem>
<bodyText confidence="0.977144807692307">
vidual words and transliterate these in-
dependently, combining the n-best hy-
pothesis lists for all the individual words
in the sequence into a single output se-
quence.
We adopted both approaches for the training
of our systems. For those multi-word sequences
where the number of words in the source and
target matched, the latter approach was taken.
For those where the numbers of source and tar-
get words differed, the former approach was
taken. The decoding process for multi-word se-
quences is shown in Figure 1. This approach
was only used during the parameter tuning on
the development set, and in experiments to eva-
luate the system performance on development
data since no multi-word sequences occurred in
the test data.
During recombination, the score for the target
word sequence was calculated as the product of
the scores of each hypothesis for each word.
Therefore a search over all combinations of hy-
potheses was required. In almost all cases we
were able to perform a full search. For the rare
long word sequences in the data, a beam search
strategy was adopted.
</bodyText>
<subsectionHeader confidence="0.967636">
3.3.2 Bidirectional Decoding
</subsectionHeader>
<bodyText confidence="0.999997580645161">
In SMT it is usual to decode generating the tar-
get sequence in order from the first token to the
last token (we refer to this as left-to-right decod-
ing, as this is the usual term for this, even
though it may be confusing as some languages
are naturally written from right-to-left). Since
the decoding process is symmetrical, it is also
possible to reverse the decoding process, gene-
rating from the end of the target sequence to the
start (we will refer to this as right-to-left decod-
ing). This reverse decoding is counter-intuitive
since language is generated in a left-to-right
manner by humans (by definition), however, in
pilot experiments on language translation, we
found that the best decoding strategy varies de-
pending on the languages involved. The analo-
gue of this observation was observed in our
transliteration results (Table 1). For some lan-
guage pairs, a left-to-right decoding strategy
performed better, and for other language pairs
the right-to-left strategy was preferable.
Our pilot experiments also showed that com-
bining the hypotheses from both decoding
processes almost always gave better results that
the best of either left-to-right or right-to-left de-
coding. We observe a similar effect in the expe-
riments presented here, although our results here
are less consistent. This is possibly due to the
differences in the size of the data sets used for
the experiments. The data used in the experi-
ments here being an order of magnitude smaller.
</bodyText>
<sectionHeader confidence="0.999905" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999973222222222">
The results of our experiments are shown in Ta-
ble 1. These results are from a closed evaluation
on development data. Only the training data
were used to build the system’s models, the de-
velopment data being used to tune the log-linear
weights for the translation engines’ models and
for evaluation. We show results for the case of
equal interpolation weights of the left-to-right
and right-to-left decoders. For the final submis-
</bodyText>
<page confidence="0.996496">
54
</page>
<table confidence="0.999776807692308">
Language Decoding ACC Mean MRR MAP_ref MAP_10 MAP_sys
Strategy F-score
En-Ch ฺ 0.908 0.972 0.908 0.266 0.266 0.908
ู 0.914 0.974 0.914 0.268 0.268 0.914
฻ 0.915 0.974 0.915 0.268 0.268 0.915
En-Hi ฺ 0.788 0.969 0.788 0.231 0.231 0.788
ู 0.785 0.968 0.785 0.230 0.230 0.785
฻ 0.790 0.970 0.790 0.231 0.231 0.790
En-Ja ฺ 0.773 0.950 0.793 0.251 0.251 0.776
ู 0.767 0.948 0.785 0.249 0.249 0.768
฻ 0.769 0.949 0.789 0.250 0.250 0.771
En-Ka ฺ 0.682 0.954 0.684 0.202 0.202 0.683
ู 0.660 0.953 0.661 0.195 0.195 0.660
฻ 0.674 0.955 0.675 0.199 0.199 0.674
En-Ko ฺ 0.622 0.850 0.623 0.183 0.183 0.622
ู 0.620 0.851 0.621 0.182 0.182 0.619
฻ 0.627 0.853 0.628 0.184 0.184 0.626
En-Ru ฺ 0.915 0.982 0.915 0.268 0.268 0.915
ู 0.921 0.983 0.921 0.270 0.270 0.921
฻ 0.922 0.983 0.922 0.270 0.270 0.922
En-Ta ฺ 0.731 0.963 0.732 0.216 0.216 0.731
ู 0.734 0.962 0.735 0.217 0.217 0.735
฻ 0.748 0.965 0.749 0.221 0.221 0.749
Jn-Jk ฺ 0.769 0.869 0.797 0.301 0.301 0.766
ู 0.766 0.862 0.792 0.299 0.299 0.761
฻ 0.772 0.867 0.799 0.300 0.300 0.767
</table>
<tableCaption confidence="0.881670333333333">
Table 2: Results showing the peformance of three decoding strategies with respect to the evaluation
metrics used for the shared task. Here ฺ denotes left-to-right decoding, ู denotes right-to-left de-
coding and ฻ denotes bidirectional decoding.
</tableCaption>
<bodyText confidence="0.996104625">
Key to Language Acronyms: En = English, Ch = Chinese, Hi = Hindi, Ja = Japanese Katakana, Ka =
Kannada, Ko = Korean, Ru = Russian, Ta = Tamil, Jn = Japanese Romaji, Jk = Japanese Kanji.
sion these weights were tuned on the develop-
ment data. The bidirectional performance was
the best strategy for all but En-Ja and En-Ka in
terms of ACC. This varies for other metrics but
in general the bidirectional system most often
gave the highest performance.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999933">
Our results show the performance of state of the
art phrase-based machine translation techniques
on the task of transliteration. We show that it is
reasonable to use the BLEU score to tune the
system, and that bidirectional decoding can im-
prove performance. In future work we would
like to consider more tightly coupling the de-
coders, introducing monotonicity into the
alignment process, and adding contextual fea-
tures into the translation models.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999765666666667">
The results presented in this paper draw on the
following data sets. For Chinese-English, Li et
al., 2004. For Japanese-English, Korean-
English, and Japanese(romaji)-Japanese(kanji),
the reader is referred to the CJK website:
http://www.cjk.org. For Hindi-English, Tamil-
English, Kannada-English and Russian-English
the data sets originated from the work of Kura-
man and Kellner, 2007.
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.877270333333333">
Peter Brown, S. Della Pietra, V. Della Pietra, and R.
Mercer (1991). The mathematics of statistical ma-
chine translation: parameter estimation. Computa-
tional Linguistics, 19(2), 263-311.
Etienne Denoual and Yves Lepage. 2006. The cha-
racter as an appropriate unit of processing for non-
</reference>
<page confidence="0.984816">
55
</page>
<reference confidence="0.99974912">
segmenting languages, Proceedings of the 12th
Annual Meeting of The Association of NLP, pp.
731-734.
Kevin Knight and Jonathan Graehl. 1997. Machine
Transliteration. Proceedings of the Thirty-Fifth
Annual Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of the
European Chapter of the Association for Compu-
tational Linguistics, pp. 128-135, Somerset, New
Jersey.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proceedings of the Human Language Technology
Conference 2003 (HLT-NAACL 2003), Edmonton,
Canada.
Franz Josef Och, “Minimum error rate training for
statistical machine translation,” Proceedings of the
ACL, 2003.
Kumaran A., Kellner T., &amp;quot;A generic framework for
machine transliteration&amp;quot;, Proc. of the 30th SIGIR,
2007
Haizhou Li, Min Zhang, Jian Su, English-Chinese
(EnCh): &amp;quot;A joint source channel model for ma-
chine transliteration&amp;quot;, Proc. of the 42nd ACL,
2004.
</reference>
<page confidence="0.998413">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684058">
<title confidence="0.999354">Transliteration by Bidirectional Statistical Machine Translation</title>
<author confidence="0.961912">Andrew</author>
<pubnum confidence="0.812765">2-2-2</pubnum>
<affiliation confidence="0.978068">Keihanna Science</affiliation>
<address confidence="0.975063">619-0288 JAPAN</address>
<abstract confidence="0.995858782608696">The system presented in this paper uses phrase-based statistical machine translation (SMT) techniques to directly transliterate between all language pairs in this shared task. The technique makes no language specific assumptions, uses no dictionaries or explicit phonetic information. The translation process transforms sequences of tokens in the source language directly into to sequences of tokens in the target. All language pairs were transliterated by applying this technique in a single unified manner. The machine translation system used was a system comprised of two phrase-based SMT decoders. The first generated from the first token of the target to the last. The second system generated the target from last to first. Our results show that if only one of these decoding strategies is to be chosen, the optimal choice depends on the languages involved, and that in general a combination of the two approaches is able to outperform either approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>Peter Brown, S. Della Pietra, V. Della Pietra, and R. Mercer (1991). The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2), 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Etienne Denoual</author>
<author>Yves Lepage</author>
</authors>
<title>The character as an appropriate unit of processing for nonsegmenting languages,</title>
<date>2006</date>
<booktitle>Proceedings of the 12th Annual Meeting of The Association of NLP,</booktitle>
<pages>731--734</pages>
<marker>Denoual, Lepage, 2006</marker>
<rawString>Etienne Denoual and Yves Lepage. 2006. The character as an appropriate unit of processing for nonsegmenting languages, Proceedings of the 12th Annual Meeting of The Association of NLP, pp. 731-734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1997</date>
<booktitle>Machine Transliteration. Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="2498" citStr="Knight and Graehl, 1997" startWordPosition="397" endWordPosition="400">MT directly to transliterate. Eiichiro Sumita NICT 2-2-2 Hikaridai Keihanna Science City 619-0288 JAPAN eiichiro.sumita@nict.go.jp We view the task of machine transliteration as a process of machine translation at the character level (Donoual and LePage, 2006). We use state of the art phrase-based statistical machine translation systems (Koehn et al., 2003) to perform the transliteration. By adopting this approach we were able to build systems for all of the language pairs in the shared task using precisely the same procedures. No modeling of the phonetics of either source or target language (Knight and Graehl, 1997) was necessary, since the approach is simply a direct transformation of sequences of tokens in the source language into sequences of tokens in the target. 2 Overview Our approach differs from the approach of (Finch and Sumita, 2008) in that we decode bidirectional. In a typical statistical machine translation system the sequence of target tokens is generated in a left-to-right manner, by left-toright here we mean the target sequence is generated from the first token to its last. During the generation process the models (in particular the target language model) are able to refer to only the tar</context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1997. Machine Transliteration. Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pp. 128-135, Somerset, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2233" citStr="Koehn et al., 2003" startWordPosition="351" endWordPosition="354">nd target languages is considerably less for the transliteration task We take a statistical machine translation paradigm (Brown at al., 1991) as the basis for our systems. The work in this paper is related to the work of (Finch and Sumita, 2008) who also use SMT directly to transliterate. Eiichiro Sumita NICT 2-2-2 Hikaridai Keihanna Science City 619-0288 JAPAN eiichiro.sumita@nict.go.jp We view the task of machine transliteration as a process of machine translation at the character level (Donoual and LePage, 2006). We use state of the art phrase-based statistical machine translation systems (Koehn et al., 2003) to perform the transliteration. By adopting this approach we were able to build systems for all of the language pairs in the shared task using precisely the same procedures. No modeling of the phonetics of either source or target language (Knight and Graehl, 1997) was necessary, since the approach is simply a direct transformation of sequences of tokens in the source language into sequences of tokens in the target. 2 Overview Our approach differs from the approach of (Finch and Sumita, 2008) in that we decode bidirectional. In a typical statistical machine translation system the sequence of t</context>
<context position="3684" citStr="Koehn et al., 2003" startWordPosition="596" endWordPosition="599">are able to refer to only the target tokens that have already been generated. In our approach, by using decoders that decode in both directions we are able to exploit context to the left and to the right of target tokens being generated. Furthermore, we expect our system to gain because it is a combination of two different MT systems that are performing the same task. 3 Experimental Conditions In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa. This decoder operates on exactly the same principles as the publicly available MOSES decoder (Koehn et al., 2003). Like MOSES we utilize a future cost in our calculations. Our decoder was modified to be able to run two instances of the decoder at the same 52 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 52–56, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP Word 1 Word 2 Word m Segment into individual words and decode each word independently Figure 1: The decoding process for multi-word sequences ... ... ... hypothesis n hypothesis n hypothesis n Decode n-best hypothesis 1 hypothesis 2 Decode n-best hypothesis 1 hypothesis 2 Decode Search for the best path hypothesis 1 hy</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the Human Language Technology Conference 2003 (HLT-NAACL 2003), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation,”</title>
<date>2003</date>
<booktitle>Proceedings of the ACL,</booktitle>
<contexts>
<context position="6925" citStr="Och, 2003" startWordPosition="1145" endWordPosition="1146"> a reasonable boost in performance by increasing the coverage of the language model and phrase table. The language models and translation models were therefore built from all the data, and the log-linear weights used to combine the models of the systems were tuned using systems trained only on the training data. The development data in this case being held-out. It was assumed that these parameters would perform well in the systems trained on the combined development/training corpora. 3.3 Parameter Tuning The SMT systems were tuned using the minimum error rate training procedure introduced in (Och, 2003). For convenience, we used BLEU as a proxy for the various metrics used in the shared task evaluation. The BLEU score is 53 En-Ch En-Ja En-Ko En-Ru Jn-Jk After tuning 0.908 0.772 0.622 0.914 0.769 Before tuning 0.871 0.635 0.543 0.832 0.737 Table 1: The effect on 1-best accuracy by tuning with respect to BLEU score commonly used to evaluate the performance of machine translation systems and is a function of the geometric mean of n-gram precision. Table 1 shows the effect of tuning for BLEU on the ACC (1-best accuracy) scores for several languages. Improvements in the BLEU score also gave impro</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och, “Minimum error rate training for statistical machine translation,” Proceedings of the ACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>T Kellner</author>
</authors>
<title>A generic framework for machine transliteration&amp;quot;,</title>
<date>2007</date>
<booktitle>Proc. of the 30th SIGIR,</booktitle>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>Kumaran A., Kellner T., &amp;quot;A generic framework for machine transliteration&amp;quot;, Proc. of the 30th SIGIR, 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>English-Chinese (EnCh): &amp;quot;A joint source channel model for machine transliteration&amp;quot;,</title>
<date>2004</date>
<booktitle>Proc. of the 42nd ACL,</booktitle>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, Jian Su, English-Chinese (EnCh): &amp;quot;A joint source channel model for machine transliteration&amp;quot;, Proc. of the 42nd ACL, 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>