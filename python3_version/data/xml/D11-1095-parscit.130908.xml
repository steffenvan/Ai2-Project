<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.923738">
Hierarchical Verb Clustering Using Graph Factorization
</title>
<author confidence="0.995933">
Lin Sun and Anna Korhonen
</author>
<affiliation confidence="0.999923">
University of Cambridge, Computer Laboratory
</affiliation>
<address confidence="0.947481">
15 JJ Thomson Avenue, Cambridge CB3 0GD, UK
</address>
<email confidence="0.997104">
ls418,alk23@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.996616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99951735">
Most previous research on verb clustering has
focussed on acquiring flat classifications from
corpus data, although many manually built
classifications are taxonomic in nature. Also
Natural Language Processing (NLP) applica-
tions benefit from taxonomic classifications
because they vary in terms of the granularity
they require from a classification. We intro-
duce a new clustering method called Hierar-
chical Graph Factorization Clustering (HGFC)
and extend it so that it is optimal for the task.
Our results show that HGFC outperforms the
frequently used agglomerative clustering on a
hierarchical test set extracted from VerbNet,
and that it yields state-of-the-art performance
also on a flat test set. We demonstrate how the
method can be used to acquire novel classifi-
cations as well as to extend existing ones on
the basis of some prior knowledge about the
classification.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954844444444">
A variety of verb classifications have been built to
support NLP tasks. These include syntactic and se-
mantic classifications, as well as ones which in-
tegrate aspects of both (Grishman et al., 1994;
Miller, 1995; Baker et al., 1998; Palmer et al., 2005;
Kipper, 2005; Hovy et al., 2006). Classifications
which integrate a wide range of linguistic proper-
ties can be particularly useful for tasks suffering
from data sparseness. One such classification is
the taxonomy of English verbs proposed by Levin
(1993) which is based on shared (morpho-)syntactic
and semantic properties of verbs. Levin’s taxon-
omy or its extended version in VerbNet (Kipper,
2005) has proved helpful for various NLP applica-
tion tasks, including e.g. parsing, word sense disam-
biguation, semantic role labeling, information ex-
traction, question-answering, and machine transla-
tion (Swier and Stevenson, 2004; Dang, 2004; Shi
and Mihalcea, 2005; Zapirain et al., 2008).
Because verbs change their meaning and be-
haviour across domains, it is important to be able
to tune existing classifications as well to build novel
ones in a cost-effective manner, when required. In
recent years, a variety of approaches have been pro-
posed for automatic induction of Levin style classes
from corpus data which could be used for this pur-
pose (Schulte im Walde, 2006; Joanis et al., 2008;
Sun et al., 2008; Li and Brew, 2008; Korhonen
et al., 2008; O´ S´eaghdha and Copestake, 2008; Vla-
chos et al., 2009). The best of such approaches
have yielded promising results. However, they have
mostly focussed on acquiring and evaluating flat
classifications. Levin’s classification is not flat, but
taxonomic in nature, which is practical for NLP pur-
poses since applications differ in terms of the gran-
ularity they require from a classification.
In this paper, we experiment with hierarchical
Levin-style clustering. We adopt as our baseline
method a well-known hierarchical method – ag-
glomerative clustering (AGG) – which has been pre-
viously used to acquire flat Levin-style classifica-
tions (Stevenson and Joanis, 2003) as well as hierar-
chical verb classifications not based on Levin (Fer-
rer, 2004; Schulte im Walde, 2008). The method has
also been popular in the related task of noun clus-
</bodyText>
<page confidence="0.830384">
1023
</page>
<note confidence="0.9578255">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99330135483871">
tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou
and Kotropoulos, 2011).
We introduce then a new method called Hierar-
chical Graph Factorization Clustering (HGFC) (Yu
et al., 2006). This graph-based, probabilistic cluster-
ing algorithm has some clear advantages over AGG
(e.g. it delays the decision on a verb’s cluster mem-
bership at any level until a full graph is available,
minimising the problem of error propagation) and it
has been shown to perform better than several other
hierarchical clustering methods in recent compar-
isons (Yu et al., 2006). The method has been applied
to the identification of social network communities
(Lin et al., 2008), but has not been used (to the best
of our knowledge) in NLP before.
We modify HGFC with a new tree extraction al-
gorithm which ensures a more consistent result, and
we propose two novel extensions to it. The first is a
method for automatically determining the tree struc-
ture (i.e. number of clusters to be produced for each
level of the hierarchy). This avoids the need to pre-
determine the number of clusters manually. The sec-
ond is addition of soft constraints to guide the clus-
tering performance (Vlachos et al., 2009). This is
useful for situations where a partial (e.g. a flat) verb
classification is available and the goal is to extend it.
Adopting a set of lexical and syntactic features
which have performed well in previous works, we
compare the performance of the two methods on test
sets extracted from Levin and VerbNet. When eval-
uated on a flat clustering task, HGFC outperforms
AGG and performs very similarly with the best flat
clustering method reported on the same test set (Sun
and Korhonen, 2009). When evaluated on a hierar-
chical task, HGFC performs considerably better than
AGG at all levels of gold standard classification. The
constrained version of HGFC performs the best, as
expected, demonstrating the usefulness of soft con-
straints for extending partial classifications.
Our qualitative analysis shows that HGFC is ca-
pable of detecting novel information not included in
our gold standards. The unconstrained version can
be used to acquire novel classifications from scratch
while the constrained version can be used to extend
existing ones with additional class members, classes
and levels of hierarchy.
2 Target classification and test sets
The taxonomy of Levin (1993) groups English verbs
(e.g. break, fracture, rip) into classes (e.g. 45.1
Break verbs) on the basis of their shared mean-
ing components and (morpho-)syntactic behaviour,
defined in terms of diathesis alternations (e.g. the
causative/inchoative alternation, where an NP frame
alternates with an intransitive frame: Tony broke the
window ↔ The window broke). It classifies over
3000 verbs in 57 top level classes, some of which
divide further into subclasses. The extended version
of the taxonomy in VerbNet (Kipper, 2005) classifies
5757 verbs. Its 5 level taxonomy includes 101 top
level and 369 subclasses. We used three gold stan-
dards (and corresponding test sets) extracted from
these resources in our experiments:
T1: The first gold standard is a flat gold standard
which includes 13 classes appearing in Levin’s orig-
inal taxonomy (Stevenson and Joanis, 2003). We in-
cluded this small gold standard in our experiments
so that we could compare the flat version of our
method against previously published methods. Fol-
lowing Stevenson and Joanis (2003), we selected 20
verbs from each class which occur at least 100 times
in our corpus. This gave us 260 verbs in total.
T2: The second gold standard is a large, hi-
erarchical gold standard which we extracted from
VerbNet as follows: 1) We removed all the verbs
that have less than 1000 occurrences in our cor-
pus. 2) In order to minimise the problem of pol-
ysemy, we assigned each verb to the class which,
according to VerbNet, corresponds to its predomi-
nant sense in WordNet (Miller, 1995). 3) In order
to minimise the sparse data problem with very fine-
grained classes, we converted the resulting classifi-
cation into a 3-level representation so that the classes
at the 4th and 5th level were combined. For exam-
ple, the sub-classes of Declare verbs (numbered as
29.4.1.1.{1,2,3}) were combined into 29.4.1. 4) The
classes that have fewer than 5 members were dis-
carded. The total number of verb senses in the re-
sulting gold standard is 1750, which is 33.2% of the
verbs in VerbNet. T2 has 51 top level, 117 second
level, and 133 third level classes.
T3: The third gold standard is a subset of T2
where singular classes (top level classes which do
not divide into subclasses) are removed. This gold
</bodyText>
<page confidence="0.951529">
1024
</page>
<bodyText confidence="0.993068673913043">
standard was constructed to enable proper evalua- 3.2 Clustering
tion of the constrained version of HGFC (introduced We introduce the agglomerative clustering (AGG)
in the following section) where we want to com- and Hierarchical Graph Factorization Clustering
pare the impact of constraints across several levels (HGFC) methods in the following two subsec-
of classification. T3 provides classification of 357 tions, respectively. The subsequent two subsections
verbs into 11 top level, 14 second level, and 32 third present our extensions to HGFC: (i) automatically
level classes. determining the cluster structure and (ii) adding soft
For each verb appearing in T1-T3, we extracted constraints to guide clustering performance.
all the occurrences (up to 10,000) from the British 3.2.1 Agglomerative clustering
National Corpus (Leech, 1992) and North American AGG is a method which treats each verb as a
News Text Corpus (Graff, 1995). singleton cluster and then successively merges two
3 Method closest clusters until all the clusters have been
3.1 Features and feature extraction merged into one. We used the SciPy’s imple-
Previous works on Levin style verb classification mentation (Oliphant, 2007) of the algorithm. The
have investigated optimal features for this task cluster distance is measured using linkage criteria.
(Stevenson and Joanis, 2003; Li and Brew, 2008; We experimented with four commonly used link-
Sun and Korhonen, 2009)). We adopt for our exper- age criteria: Single, Average, Complete and Ward’s
iments a set of features which have performed well (Ward Jr., 1963). Ward’s criterion performed the
in recent verb clustering works: best and was used in all the experiments in this pa-
A: Subcategorization frames (SCFs) and their rela- per. It measures the increase in variance after two
tive frequencies with individual verbs. clusters are merged. The output of AGG tends to
B: A with SCFs parameterized for prepositions. have excessive number of levels. Cut-based meth-
C: B with SCFs parameterized for subjects appear- ods (Wu and Leahy, 1993; Shi and Malik, 2000) are
ing in grammatical relations associated with the frequently applied to extract a simplified view. We
verb in parsed data. followed previous verb clustering works and cut the
D: B with SCFs parameterized for objects appear- AGG hierarchy manually.
ing in grammatical relations associated with the AGG suffers from two problems. The first is er-
verb in parsed data. ror propagation. When a verb is misclassified at a
These features are purely syntactic. Although lower level, the error propagates to all the upper lev-
semantic features – verb selectional preferences – els. The second is local pairwise merging, i.e. the
proved the best (when used in combination with syn- fact that only two clusters can be combined at any
tactic features) in the recent work of Sun and Ko- level. For example, in order to group clusters rep-
rhonen (2009), we left such features for future work resenting Levin classes 9.1, 9.2 and 9.3 into a sin-
because we noticed that different levels of classifi- gle cluster representing class 9, the method has to
cation are likely to require semantic features at dif- produce intermediate clusters, e.g. 9.{1,2} and 9.3.
ferent granularities. Such clusters do not always have a semantic inter-
We extracted the syntactic features using the sys- pretation. Although they can be removed using a
tem of Preiss et al. (2007). The system tags, lemma- cut-based method, this requires a pre-defined cut-off
tizes and parses corpus data using the RASP (Robust value which is difficult to set (Stevenson and Joanis,
Accurate Statistical Parsing toolkit (Briscoe et al., 2003). In addition, a significant amount of informa-
2006)), and on the basis of the resulting grammat- tion is lost in pair-wise clustering. In the above ex-
ical relations, assigns each occurrence of a verb as ample, only the clusters 9.{1,2} and 9.3 are consid-
a member of one of the 168 verbal SCFs. We pa- ered, while alternative clusters 9.{1,3} and 9.2 are
rameterized the SCFs as described above using the ignored. Ideally, information about all the possible
information provided by the system. intermediate clusters should be aggregated, but this
1025 is intractable in practice.
</bodyText>
<subsectionHeader confidence="0.975528">
3.2.2 Hierarchical Graph Factorization
Clustering
</subsectionHeader>
<bodyText confidence="0.999684117647059">
Our new method HGFC derives a probabilistic bi-
partite graph from the similarity matrix (Yu et al.,
2006). The local and global clustering structures are
learned via the random walk properties of the graph.
The method does not suffer from the above prob-
lems with AGG. Firstly, there is no error propagation
because the decision on a verb’s membership at any
level is delayed until the full bipartite graph is avail-
able and until a tree structure can be extracted from
it by aggregating probabilistic information from all
the levels. Secondly, the bipartite graph enables the
construction of a hierarchical structure without any
intermediate classes. For example, we can group
classes 9.{1,2,3} directly into class 9.
We use HGFC with the distributional similarity
measure Jensen-Shannon Divergence (djs(v, v0)).
Given a set of verbs, V = {vn}Nn=1, we
compute a similarity matrix W where Wij =
exp(−djs(v1,v2)). W can be encoded by a undi-
rected graph G (Figure 1(a)), where the verbs are
mapped to vertices and the Wij is the edge weight
between vertices i and j.
The graph G and the cluster structure can be rep-
resented by a bipartite graph K(V, U). V are the
vertices on G. U = {up}mp=1 represent the hidden m
clusters. For example, looking at Figure 1(b), V on
G can be grouped into three clusters u1, u2 and u3.
The matrix B denotes the n x m adjacency matrix,
with bip being the connection weight between the
vertex vi and the cluster up. Thus, B represents the
connections between clusters at an upper and lower
level of clustering. A flat clustering algorithm can
be induced by computing B.
The bipartite graph K also induces a similarity
</bodyText>
<equation confidence="0.79011">
bipbjp
(W0) between vi and vj: w0 ij = Pm λp =
p=1
</equation>
<bodyText confidence="0.980492857142857">
(BA−1BT)ij where A = diag(λ1, ..., λm). There-
fore, B can be found by approximating the similarity
matrix W of G using W0 derived from K. Given a
distance function ( between two similarity matrices,
B approximates W by minimizing the cost function
((W, BA−1BT). The coupling between B and A is
removed by setting H = BA−1:
</bodyText>
<equation confidence="0.36744">
min ((W, HAHT ), s.t.
H,Λ
</equation>
<bodyText confidence="0.955747">
We use the divergence distance: ((X, Y ) =
</bodyText>
<equation confidence="0.7750618">
Pij(xij log xij
yij −xij+yij). Yu et al. (2006) showed
that this cost function is non-increasing under the
update rule:
wij X
λphjp s.t. ˜hip = 1 (2)
(HAHT )ij
wij hiphjp s.t. X λp =Xwij (3)
(HAHT )ij
p ij
</equation>
<bodyText confidence="0.978939666666667">
wij can be interpreted as the probability of the di-
rect transition between vi and vj: wij = p(vi, vj),
when Pij wij = 1. bip can be interpreted as:
</bodyText>
<equation confidence="0.969139">
D = diag(d1, ..., dn) where di =
</equation>
<bodyText confidence="0.9391405">
p(up, uq) is the similarity between the clusters. It
takes into account of a weighted average of contri-
butions from all the data. This is different from the
linkage method where only the data from two clus-
ters are considered.
Given the cluster similarity p(up, uq), we can con-
struct a new graph G1 (Figure 1(d)) with the clusters
U as vertices. The cluster algorithm can be applied
again (Figure 1(e)). This process can go on itera-
tively, leading to a hierarchical graph.
Algorithm 1 HGFC algorithm (Yu et al., 2006)
Require: N verbs V , number of clusters ml for L levels
Compute the similarity matrix W0 from V
Build the graph G0 from W0 , and m0 ← n
for l = 1,2 to L do
Factorize Gl−1 to obtain bipartite graph Kl with the
adjacency matrix Bl (eq. 1, 2 and 3)
Build a graph Gl with similarity matrix Wl =
</bodyText>
<equation confidence="0.554576">
BTl D−1
</equation>
<bodyText confidence="0.885721">
l Bl according to equation 4
</bodyText>
<subsectionHeader confidence="0.877596">
end for
</subsectionHeader>
<bodyText confidence="0.9661185">
return BL, BL−1...B1
Additional steps need to be performed in order to
extract a tree from the hierarchical graph. Yu et al.
(2006) performs the extraction via a propagation of
probabilities from the bottom level clusters. For a
verb vi, the probability of assigning it to clusterv(l)
</bodyText>
<equation confidence="0.976672473684211">
p
at level l is given by:
Xn hip = 1 (1)
i=1
X˜hip oc hip
j
i
X˜λp oc λp
j
p(up,uq) = p(up)p(up|uq) =
= (BT D−1B)pq (4)
Xn
i=1
bipbiq
di
bip
m
X
p=0
</equation>
<page confidence="0.931421">
1026
</page>
<figure confidence="0.990524549019608">
V2
V6
V4
V3
V1
V
V9
V7
V8
V,
V2
V3
V4
V
V6
V7
V8
V9
U,
U2
U3
V6
V2
V4
U
,
V3
U3
V,
V
V9
V7
U2
V8
U, U2
U3
v1
v2
v3
v4
v
v6
v7
v8
v9
u1
u2
u3
q1
q2
(a) (b) (c) (d) (e)
</figure>
<figureCaption confidence="0.9947295">
Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters
on G; (c) The induced clusters U; (d) The new graph G1 over clusters U; (e) The new bipartite graph over G1
</figureCaption>
<equation confidence="0.997519857142857">
p(v(l)
p |v(l−1))...p(v(1)|vi)
= (D(−1)
1 B1D−1
2 B2D−1
3 B3...D−1
l Bl)ip (5)
</equation>
<bodyText confidence="0.893912">
This method might not extract a consistent tree
structure, because the cluster membership at the
lower level does not constrain the upper level mem-
bership. This prevented us from extracting a Levin
style hierarchical classification in our initial experi-
ments. For example, where two verbs were grouped
together at a lower level, they could belong to sepa-
rate clusters at an upper level. We therefore propose
a new tree extraction algorithm (Algorithm 2).
The new algorithm starts from the top level bipar-
tite graph, and generates consistent labels for each
level by taking into account of the tree constraints
set at upper levels.
Algorithm 2 Tree extraction algorithm for HGFC
Require: Given N, (Bl, ml) on each level for L levels
On the top level L, collect the labels TL (eq. 5)
Define C to be a (mL−1 x mL) zero matrix, Cij ← 1,
where i, j = arg maxi,j{BL ij}
for l = L − 1 to 1 do
fori= 1 to N do
Compute p(vlp|vi) for each cluster p (eq. 5)
tli = argmaxp{p(vl p|vi)|p = 1...ml, Cptl+1 i=� 0}
</bodyText>
<subsectionHeader confidence="0.314375">
end for
</subsectionHeader>
<bodyText confidence="0.2457">
Redefine C to be a (ml−1 xml) zero matrix, Cij ←
1, where i, j = arg maxi,j{Blij}
</bodyText>
<subsectionHeader confidence="0.237827">
end for
</subsectionHeader>
<bodyText confidence="0.8511270625">
return Tree consistent labels TL, TL−1...T1
3.2.3 Automatically determining the number of
clusters for HGFC
HGFC needs the number of levels and clusters at
each level as input. However, this information is not
always available (e.g. when the goal is to actually
learn this information automatically). We therefore
propose a method for inferring the cluster structure
from data. As shown in figure 1, a similarity ma-
trix W models one-hop transitions that follow the
links from vertices to neighbors. A walker can also
go to other vertices via multi-hop transitions. Ac-
cording to the chain rule of the Markov process, the
multi-hop transitions indicate a decaying similarity
function on the graph (Yu et al., 2006). After t tran-
sitions, the similarity matrix (Wt) becomes:
</bodyText>
<equation confidence="0.997102">
Wt = Wt−1D−1
0 W0
</equation>
<bodyText confidence="0.99962875">
Yu et al. (2006) proved the correspondence be-
tween the HGFC levels (l) and the random walk time:
t = 2l−1. So the vertices at level l induce a sim-
ilarity matrix of verbs after t-hop transitions. The
decaying similarity function captures the different
scales of clustering structure in the data (Azran and
Ghahramani, 2006b). The upper levels would have
a smaller number of clusters which represent a more
global structure. After several levels, all the verbs
are expected to be grouped into one cluster. The
number of levels and clusters at each level can thus
be learned automatically.
We therefore propose a method that uses the de-
caying similarity function to learn the hierarchical
clustering structure. One simple modification to al-
gorithm 1 is to set the number of clusters at level l
</bodyText>
<equation confidence="0.999286">
p(vpll)|vi) = E �...
Vl−1 V1
</equation>
<page confidence="0.876525">
1027
</page>
<bodyText confidence="0.999788625">
(ml) to be ml−1 − 1. m is denoted as the number
of clusters that have at least one member according
to eq. 5. We start by treating each verb as a cluster
at the bottom level. The algorithm stops when all
the data points are merged into one cluster. The in-
creasingly decaying similarity causes many clusters
to have 0 members especially at lower levels, which
are pruned in the tree extraction.
</bodyText>
<subsectionHeader confidence="0.95519">
3.2.4 Adding constraints to HGFC
</subsectionHeader>
<bodyText confidence="0.999983111111111">
The basic version of HGFC makes no prior as-
sumptions about the classification. It is useful
for learning novel verb classifications from scratch.
However, when wishing to extend an existing clas-
sification (e.g. VerbNet) it may be desirable to guide
the clustering performance on the basis of informa-
tion that is already known. We propose a constrained
version of HGFC which makes uses of labels at the
bottom level to learn upper level classifications. We
do this by adding soft constraints to clustering, fol-
lowing Vlachos et al. (2009).
We modify the similarity matrix W as follows: If
two verbs have different labels (li =� lj), the simi-
larity between them is decreased by a factor a, and
a &lt; 1. We set a to 0.5 in the experiments. The re-
sulting tree is generally consistent with the original
classification. The influence of the underlying data
(domain or features) is reduced according to a.
</bodyText>
<sectionHeader confidence="0.997138" genericHeader="introduction">
4 Experimental evaluation
</sectionHeader>
<bodyText confidence="0.99998975">
We applied the clustering methods introduced in
section 3 to the test sets described in section 2 and
evaluated them both quantitatively and qualitatively,
as described in the subsequent sections.
</bodyText>
<subsectionHeader confidence="0.630792">
4.1 Evaluation methods
</subsectionHeader>
<bodyText confidence="0.9995542">
We used class based accuracy (ACC) and adjusted
rand index (Radj) to evaluate the results on the flat
test set T1 (see section 2 for details of T1-T3).
ACC is the proportion of members of dominant
clusters DOM-CLUSTi within all classes ci.
</bodyText>
<equation confidence="0.837334">
PC i=1 verbs in DOM-CLUSTi
ACC =
</equation>
<bodyText confidence="0.7777925">
number of verbs
The formula of Radj is (Hubert and Arabie, 1985):
P �nij� − P �ni· ~P �n·j�/�n �
i,j 2 i 2 j 2
</bodyText>
<equation confidence="0.993475">
Radj = �ni· �n·j
2
2[P � + P �n·j�] − P �ni· ~P �/�n�
1
i 2 j 2 i 2 j 2 2
</equation>
<bodyText confidence="0.999969625">
where nij is the size of the intersection between
class i and cluster j.
We used normalized mutual information (NMI)
and F-Score (F) to evaluate hierarchical clustering
results on T2 and T3. NMI measures the amount of
statistical information shared by two random vari-
ables representing the clustering result and the gold-
standard labels. Given random variables A and B:
</bodyText>
<equation confidence="0.9956155">
I(A; B)
NMI(A, B) = [H(A) + H(B)]/2
|(vk n cj |log N |vk n cj |
N |vk||cj|
</equation>
<bodyText confidence="0.999316818181818">
where |vk n cj |is the number of shared member-
ship between cluster vk and gold-standard class cj.
The normalized variant of mutual information (MI)
enables the comparison of clustering with different
cluster numbers (Manning et al., 2008).
F is the harmonic mean of precision (P) and re-
call (R). P is calculated using modified purity – a
global measure which evaluates the mean precision
of clusters. Each cluster is associated with its preva-
lent class. The number of verbs in a cluster K that
take this class is denoted by nprevalent(K).
</bodyText>
<equation confidence="0.9547606">
nprevalent(ki)&gt;2 nprevalent(ki)
number of verbs
R is calculated using ACC.
F= 2 · mPUR · ACC
mPUR + ACC
</equation>
<bodyText confidence="0.99542725">
F is not suitable for comparing results with dif-
ferent cluster numbers (Rosenberg and Hirschberg,
2007). Therefore, we only report NMI when the
number of classes in clustering and gold-standard is
substantially different.
Finally, we supplemented quantitative evaluation
with qualitative evaluation of clusters produced by
different methods.
</bodyText>
<subsectionHeader confidence="0.996791">
4.2 Quantitative evaluation
</subsectionHeader>
<bodyText confidence="0.999965857142857">
We first evaluated AGG and the basic (uncon-
strained) HGFC on the small flat test set T1. The
main purpose of this evaluation was to compare the
results of our methods against previously published
results on the same test set. The number of clus-
ters (K) and levels (L) were inferred automatically
for HGFC as described in section 3.2.3. However, to
</bodyText>
<equation confidence="0.97692975">
I(A, B) = X X
k j
P
mPUR =
</equation>
<page confidence="0.967772">
1028
</page>
<bodyText confidence="0.992837">
make the results comparable with previously pub-
lished ones, we cut the resulting hierarchy at the
level of closest match (12 clusters) to the K (13) in
the gold-standard. For AGG, we cut the hierarchy at
13 clusters.
</bodyText>
<table confidence="0.99886175">
Method ACC Radj
HGFC 41.2 17.4
AGG (reproduced) 32.7 9.9
AGG (Stevenson and Joanis (2003) 31.0 9.0
</table>
<tableCaption confidence="0.996036">
Table 1: Comparison against Stevenson and Joanis
(2003)’s result on T1 (using similar features).
</tableCaption>
<bodyText confidence="0.997641885714285">
Table 1 shows our results and the results of
Stevenson and Joanis (2003) on T1 when employing
AGG using Ward as the linkage criterion. In this ex-
periment, we used the same feature set as Stevenson
and Joanis (2003) (set B, see section 3.1) and were
therefore able to reproduce their AGG result with a
difference smaller than 2%. When using this simple
feature set, HGFC outperforms the best performing
AGG clearly: 8.5% in ACC and 7.3% in Radj.
We also compared HGFC against the best reported
clustering method on T1 to date – that of spectral
clustering by Sun and Korhonen (2009). We used
the feature sets C and D which are similar to the
features (SCF parameterized by lexical prefences) in
their experiments. HGFC obtains F of 49.93% on T1
which is 5% lower than the result of Sun and Ko-
rhonen (2009). The difference comes from the tree
consistency requirement. When the HGFC is forced
to produce a flat clustering (a one level tree only), it
achieves the F of 52.55% which is very close to the
performance of spectral clustering.
We then evaluated our methods on the hierarchi-
cal test sets T2 and T3. In the first set of experi-
ments, we pre-defined the tree structure for HGFC
by setting L to 3 and K at each level to be the K
in the hierarchical gold standard. The hierarchy pro-
duced by AGG was cut into 3 levels according to Ks
in the gold standard. This enabled direct evaluation
of the results against the 3 level gold standards using
both NMI and F.
The results are reported in tables 2 and 3. In these
tables, Nc is the number of clusters in HGFC cluster-
ing while Nl is the number of classes in the gold
standard (the two do not always correspond per-
fectly because a few clusters have zero members).
</bodyText>
<table confidence="0.999594833333333">
Nc Nl HGFC AGG
unconstrained
NMI F NMI F
130 133 57.31 36.65 54.22 32.62
114 117 54.67 37.96 51.35 32.44
50 51 37.75 40.00 32.61 32.78
</table>
<tableCaption confidence="0.9797835">
Table 2: Performance on T2 using a pre-defined tree
structure.
</tableCaption>
<table confidence="0.999560166666667">
Nc Nl HGFC HGFC AGG
unconstrained constrained
NMI F NMI F NMI F
31 32 51.65 42.01 91.47 92.07 49.70 40.30
15 14 42.75 47.70 82.16 82.80 39.19 43.69
11 11 38.91 51.17 71.69 75.00 34.88 44.80
</table>
<tableCaption confidence="0.987008">
Table 3: Performance on T3 using a pre-defined tree
structure.
</tableCaption>
<bodyText confidence="0.995616592592593">
Table 2 compares the results of the unconstrained
version of HGFC against those of AGG on our largest
test set T2. As with T1, HGFC outperforms AGG
clearly. The benefit can now be seen at 3 different
levels of hierarchy. On average, the HGFC outper-
forms AGG 3.5% in NMI and 4.8% in F. The dif-
ference between the methods becomes clearer when
moving towards the upper levels of the hierarchy.
Table 3 shows the results of both unconstrained
and constrained versions of HGFC and those of
AGG on the test set T3 (where singular classes are
removed to enable proper evaluation of the con-
strained method). The results are generally gener-
ally better on this test set than on T2 – which is to be
expected since T3 is a refined subset of T21.
Recall that the constrained version of HGFC learns
the upper levels of classification on the basis of soft
constraints set at the bottom level, as described ear-
lier in section 3.2.4. As a consequence, NMI and F
are both greater than 90% at the bottom level and
the results at the top level are notably lower because
the impact of the constraints degrades the further
away one moves from the bottom level. Yet, the rela-
tively high result across all levels shows that the con-
strained version of HGFC can be employed a useful
method to extend the hierarchical structure of known
classifications.
</bodyText>
<footnote confidence="0.992942">
1NMI is higher on T2, however, because NMI has a higher
baseline for larger number of clusters (Vinh et al., 2009). NMI
is not ideal for comparing the results of T2 and T3.
</footnote>
<page confidence="0.970956">
1029
</page>
<table confidence="0.998009375">
T2 T3
N, Nl HGFC N, Nl HGFC
148 133 53.26 64 32 54.91
97 117 49.85 35 32 50.83
46 51 33.55 20 14 44.02
19 51 25.80 10 14 34.41
9 51 19.17 6 11 32.27
3 51 13.06
</table>
<tableCaption confidence="0.9918715">
Table 4: NMI of unconstrained HGFC when trees for T2
and T3 are inferred automatically.
</tableCaption>
<bodyText confidence="0.999973052631579">
Finally, Table 4 shows the results for the uncon-
strained HGFC on T2 and and T3 when the tree struc-
ture is not pre-defined but inferred automatically as
described in section 3.2.3. 6 levels are learned for
T2 and 5 for T3. The number of clusters produced
ranges from 3 to 148 for T2 and from 6 to 64 for
T3. We can see that the automatically detected clus-
ter numbers distribute evenly across different levels.
The scale of the clustering structure is more com-
plete here than in the gold standards.
In the table, N, indicates the number of clusters
in the inferred tree, while Ni indicates the closest
match to the number of classes in the gold stan-
dard. This evaluation is not fully reliable because
the match between the gold standard and the cluster-
ing is poor at some levels of hierarchy. However, it
is encouraging to see that the results do not drop dra-
matically until the match between the two is really
poor.
</bodyText>
<subsectionHeader confidence="0.9996">
4.3 Qualitative evaluation
</subsectionHeader>
<bodyText confidence="0.999985276595745">
To gain a better insight into the performance of
HGFC, we conducted further qualitative analysis of
the clusters the two versions of this method pro-
duced for T3. We focussed on the top level of 11
clusters (in the evaluation against the hierarchical
gold standard, see table 3) as the impact of soft con-
straints is the weakest for the constrained method at
this level.
As expected, the constrained HGFC kept many in-
dividual verbs belonging to same Verbnet subclass
together (e.g. verbs enjoy, hate, disdain, regret, love,
despise, detest, dislike, fear for the class 31.2.1) so
that most clusters simply group lower level classes
and their members together. Three nearly clean clus-
ters were produced which only include sub-classes
of the same class (e.g. 31.2.0 and 31.2.1 which both
belong to 31.2 Admire verbs). However, the remain-
ing 8 clusters group together sub-classes (and their
members) belonging to unrelated parent classes. In-
terestingly, 6 of these make both syntactic and se-
mantic sense. For example, several such 37.7 Say
verbs and 29.5 Conjencture verbs are found together
which share the meaning of communication and
which take similar sentential complements.
In contrast, none of the clusters produced by
the unconstrained HGFC represent a single VerbNet
class. The majority represent a high number of
classes and fewer members per class. Yet many of
the clusters make syntactic and semantic sense. A
good example is a cluster which includes member
verbs from 9.7 Spray/Load verbs, 21.2 Carve verbs,
51.3.1 Roll verbs, and 10.4 Wipe verbs. The verbs
included in this cluster share the meaning of specific
type of motion and show similar syntactic behaviour.
Thorough Levin style investigation of especially
the unconstrained method would require looking at
shared diathesis alternations between cluster mem-
bers. We left this for future work. However,
the analysis we conducted confirmed that the con-
strained method could indeed be used for extend-
ing known classifications, while the unconstrained
method is more suitable for acquiring novel classi-
fications from scratch. The errors in clusters pro-
duced by both methods were mostly due to syntactic
idiosyncracy and the lack of semantic information in
clustering. We plan to address the latter problem in
our future work.
</bodyText>
<sectionHeader confidence="0.955877" genericHeader="discussions">
5 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.999961928571429">
We have introduced a new graph-based method –
HGFC – to hierarchical verb clustering which avoids
some of the problems (e.g. error propagation, pair-
wise cluster merging) reported with the frequently
used AGG method. We modified HGFC so that it can
be used to automatically determine the tree struc-
ture for clustering, and proposed two extensions to
it which make it even more suitable for our task. The
first involves automatically determining the number
of clusters to be produced, which is useful when
this is not known in advance. The second involves
adding soft constraints to guide the clustering per-
formance, which is useful when aiming to extend
existing classification.
</bodyText>
<page confidence="0.984002">
1030
</page>
<bodyText confidence="0.99997965934066">
The results reported in the previous section are
promising. On a flat test set (T1), the unconstrained
version of HGFC outperforms AGG and performs
very similarly with the best current flat clustering
method (spectral clustering) evaluated on the same
dataset. On the hierarchical test sets (T2 and T3),
the unconstrained and constrained versions of HGFC
outperform AGG clearly at all levels of classification.
The constrained version of HGFC detects the missing
hierarchy from the existing gold standards with high
accuracy. When the number of clusters and levels
is learned automatically, the unconstrained method
produces a multi-level hierarchy. Our evaluation
against a 3-level gold standard shows that such a hi-
erarchy is fairly accurate. Finally, the results from
our qualitative evaluation show that both constrained
and unconstrained versions of HGFC are capable of
learning valuable novel information not included in
the gold standards.
The previous work on Levin style verb classifica-
tion has mostly focussed on flat classifications us-
ing methods suitable for flat clustering (Schulte im
Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li
and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha
and Copestake, 2008; Vlachos et al., 2009). How-
ever, some works have employed hierarchical clus-
tering as a method to infer flat clustering.
For example, Schulte im Walde and Brew (2002)
employed AGG to initialize the KMeans clustering
for German verbs. This gave better results than
random initialization. Stevenson and Joanis (2003)
used AGG for flat clustering on T1. They cut the hi-
erarchy at the number of classes in the gold standard
and found that it is difficult to automatically deter-
mine a good cut-off. Our evaluation in the previous
section shows that HGFC outperforms their imple-
mentation of AGG.
AGG was also used by Ferrer (2004) who per-
formed hierarchical clustering of 514 Spanish verbs.
The results were evaluated against a hierarchical
gold standard resembling that of Levin’s classifi-
cation in English (V´azquez et al., 2000). Radj of
0.07 was reported for a 15-way classification which
is comparable to the result of Stevenson and Joanis
(2003).
Hierarchical clustering has also been performed
for the related task of semantic verb classification.
For example, Basili et al. (1993) identified the prob-
lems of AGG, and applied a conceptual clustering al-
gorithm (Fisher, 1987) to Italian verbs. They used
semi-automatically acquired semantic roles and the
concept types as features. No quantitative results
were reported. The qualitative evaluation shows that
the resulting clusters are very fine-grained.
Schulte im Walde (2008) performed hierarchical
clustering of German verbs using human verb asso-
ciation as features and AGG as a method. They fo-
cussed on two small collections of 56 and 104 verbs
and evaluated the result against flat gold standard
extracted from GermaNet (Kunze and Lemnitzer,
2002) and German FrameNet (Erk et al., 2003), re-
spectively. They reported F of 62.69% for the 56
verbs, and F of 34.68% for the 104 verbs.
In the future, we plan to extend this research line
in several directions. First, we will try to deter-
mine optimal features for different levels of clus-
tering. For example, the general syntactic features
(e.g. SCF) may perform the best at top levels of a hi-
erarchy while more specific or refined features (e.g.
SCF+pp) may be optimal at lower levels. We also
plan to investigate incorporating semantic features,
like verb selectional preferences, in our feature set.
It is likely that different levels of clustering require
more or less specific selectional preferences. One
way to obtain the latter is hierarchical clustering of
relevant noun data.
In addition, we plan to apply the unconstrained
HGFC to specific domains to investigate its capabil-
ity to learn novel, previously unknown classifica-
tions. As for the constrained version of HGFC, we
will conduct a larger scale experiment on the Verb-
Net data to investigate what kind of upper level hi-
erarchy it can propose for this resource (which cur-
rently has over 100 top level classes).
Finally, we plan to compare HGFC to other hier-
archical clustering methods that are relatively new
to NLP but have proved promising in other fields,
including Bayesian Hierarchical Clustering (Heller
and Ghahramani, 2005; Teh et al., 2008) and the
method of Azran and Ghahramani (2006a) based on
spectral clustering.
</bodyText>
<sectionHeader confidence="0.998248" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.945148333333333">
Our work was funded by the Royal Society Uni-
versity Research Fellowship (AK), the Dorothy
Hodgkin Postgraduate Award (LS), the EPSRC
</bodyText>
<page confidence="0.981875">
1031
</page>
<bodyText confidence="0.998087">
grants EP/F030061/1 and EP/G051070/1 (UK) and
the EU FP7 project ’PANACEA’.
</bodyText>
<sectionHeader confidence="0.982735" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999703525773196">
Arik Azran and Zoubin Ghahramani. A new approach
to data driven clustering. In Proceedings of the 23rd
international conference on Machine learning, ICML
’06, pages 57–64, New York, NY, USA, 2006a. ISBN
1-59593-383-2.
Arik Azran and Zoubin Ghahramani. Spectral methods
for automatic multiscale data clustering. In Proceed-
ings of the 2006 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition-Volume
1, pages 190–197. IEEE Computer Society Washing-
ton, DC, USA, 2006b.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
The berkeley framenet project. In In COLING-ACL,
pages 86–90, 1998.
Roberto. Basili, Maria Teresa Pazienza, and Paola Ve-
lardi. Hierarchical clustering of verbs. In Proceedings
of the Workshop on Acquisition of Lexical Knowledge
from Text, 1993.
Nikoletta Bassiou and Constantine Kotropoulos. Long
distance bigram models applied to word clustering.
Pattern Recogn., 44:145–158, January 2011. ISSN
0031-3203.
Ted Briscoe, John Carroll, and Rebecca Watson. The
second release of the rasp system. In Proceedings
of the COLING/ACL on Interactive presentation ses-
sions, 2006.
Hoa Trang Dang. Investigations into the Role of Lexical
Semantics in Word Sense Disambiguation. PhD thesis,
CIS, University of Pennsylvania, 2004.
Katrin Erk, Andrea Kowalski, Sebastian Pad´o, and Man-
fred Pinkal. Towards a resource for lexical semantics:
a large german corpus with extensive semantic anno-
tation. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics - Volume
1, ACL ’03, pages 537–544, Stroudsburg, PA, USA,
2003. Association for Computational Linguistics.
Eva Esteve Ferrer. Towards a semantic classification
of spanish verbs based on subcategorisation informa-
tion. In Proceedings of the ACL 2004 workshop on
Student research, ACLstudent ’04, Stroudsburg, PA,
USA, 2004. Association for Computational Linguis-
tics.
Douglas H. Fisher. Knowledge acquisition via incremen-
tal conceptual clustering. Machine Learning, 2:139–
172, 1987. ISSN 0885-6125.
David Graff. North american news text corpus. Linguistic
Data Consortium, 1995.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
Comlex syntax: Building a computational lexicon. In
COLING, pages 268–272, 1994.
Katherine A. Heller and Zoubin Ghahramani. Bayesian
hierarchical clustering. In Proceedings of the 22nd
international conference on Machine learning, pages
297–304. ACM, 2005. ISBN 1595931805.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. Ontonotes: the
90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, NAACL-Short ’06,
pages 57–60, Stroudsburg, PA, USA, 2006. Associa-
tion for Computational Linguistics.
Lawrence Hubert and Phipps Arabie. Comparing par-
titions. Journal of Classification, 2:193–218, 1985.
ISSN 0176-4268.
Eric Joanis, Suzanne Stevenson, and David James. A
general feature space for automatic verb classification.
Natural Language Engineering, 14(3):337–367, 2008.
Karin Kipper. VerbNet: A broad-coverage, comprehen-
sive verb lexicon. 2005.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
The Choice of Features for Classification of Verbs in
Biomedical Texts. In Proceedings of COLING, 2008.
Claudia Kunze and Lothar Lemnitzer. GermaNet-
representation, visualization, application. In Proceed-
ings of LREC, 2002.
Geoffrey Leech. 100 million words of english: the
british national corpus. Language Research, 28(1):1–
13, 1992.
Beth. Levin. English verb classes and alternations: A
preliminary investigation. Chicago, IL, 1993.
Jianguo Li and Chris Brew. Which Are the Best Features
for Automatic Verb Classification. In Proceedings of
ACL, 2008.
Yu-Ru Lin, Yun Chi, Shenghuo Zhu, Hari Sundaram, and
Belle L. Tseng. Facetnet: a framework for analyz-
ing communities and their evolutions in dynamic net-
works. In Proceeding of the 17th international confer-
ence on World Wide Web, pages 685–694, New York,
NY, USA, 2008. ACM.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA,
2008. ISBN 0521865719, 9780521865715.
Yutaka Matsuo, Takeshi Sakaki, Kˆoki Uchiyama, and
Mitsuru Ishizuka. Graph-based word clustering using
a web search engine. In Proceedings of the EMNLP,
pages 542–550, 2006.
</reference>
<page confidence="0.843204">
1032
</page>
<reference confidence="0.999924846153847">
George A. Miller. WordNet: a lexical database for En-
glish. Communications of the ACM, 38(11):39–41,
1995.
Travis E. Oliphant. Python for scientific computing.
Computing in Science and Engineering, 9:10–20,
2007. ISSN 1521-9615.
Diarmuid O´ S´eaghdha and Ann Copestake. Semantic
classification with distributional kernels. In Proceed-
ings of COLING, 2008.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106, 2005.
Judita Preiss, Ted Briscoe, and Anna Korhonen. A sys-
tem for large-scale acquisition of verbal, nominal and
adjectival subcategorization frames from corpora. In
Proceedings of ACL, pages 912–919, 2007.
Andrew Rosenberg and Julia Hirschberg. V-measure: A
conditional entropy-based external cluster evaluation
measure. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
2007.
Sabine Schulte im Walde. Experiments on the automatic
induction of german semantic verb classes. Computa-
tional Linguistics, 32(2), 2006.
Sabine Schulte im Walde. Human associations and the
choice of features for semantic verb classification.
Research on Language and Computation, 6:79–111,
2008. ISSN 1570-7075.
Sabine Schulte im Walde and Chris Brew. Inducing ger-
man semantic verb classes from purely syntactic sub-
categorisation information. In Proceedings of ACL,
pages 223–230, 2002.
Jianbo Shi and Jitendra Malik. Normalized cuts and im-
age segmentation. IEEE Transactions on pattern anal-
ysis and machine intelligence, 22(8):888–905, 2000.
Lei Shi and Rada Mihalcea. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proceedings of CICLING,
2005.
Suzanne Stevenson and Eric Joanis. Semi-supervised
verb class discovery using noisy features. In Proceed-
ings of HLT-NAACL 2003, pages 71–78, 2003.
Lin Sun and Anna Korhonen. Improving verb clustering
with automatically acquired selectional preferences. In
Proceedings of the EMNLP 2009, 2009.
Lin Sun, Anna Korhonen, and Yuval Krymolowski. Verb
class discovery from rich syntactic data. Lecture Notes
in Computer Science, 4919:16, 2008.
Robert Swier and Suzanne Stevenson. Unsupervised
semantic role labelling. In Proceedings of EMNLP,
pages 95–102, 2004.
Yee Whye Teh, Hal Daum´e III, and Daniel Roy. Bayesian
agglomerative clustering with coalescents. In Ad-
vances in Neural Information Processing Systems, vol-
ume 20, 2008.
Akira Ushioda. Hierarchical clustering of words. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 1159–1162. Association
for Computational Linguistics, 1996.
Gloria V´azquez, Ana Fern´andez-Montraveta, and
M. Ant`onia Martf. Clasificaci´on verbal:(alternancias
de di´atesis). Universitat de Lleida, 2000. ISBN
8484090671.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. Infor-
mation theoretic measures for clusterings comparison:
is a correction for chance necessary? In ICML ’09:
Proceedings of the 26th Annual International Confer-
ence on Machine Learning, pages 1073–1080, New
York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-
1.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. Unsupervised and constrained dirichlet process
mixture models for verb clustering. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 74–82, 2009.
Joe H. Ward Jr. Hierarchical grouping to optimize an ob-
jective function. Journal of the American statistical as-
sociation, 58(301):236–244, 1963. ISSN 0162-1459.
Zhenyu Wu and Richard Leahy. An optimal graph the-
oretic approach to data clustering: Theory and its ap-
plication to image segmentation. IEEE transactions
on pattern analysis and machine intelligence, pages
1101–1113, 1993. ISSN 0162-8828.
Kai Yu, Shipeng Yu, and Volker Tresp. Soft clustering on
graphs. Advances in Neural Information Processing
Systems, 18:1553, 2006.
Be˜nat Zapirain, Eneko Agirre, and Llufs M`arquez. Ro-
bustness and generalization of role sets: PropBank vs.
VerbNet. In Proceedings ofACL-08: HLT, pages 550–
558, 2008.
</reference>
<page confidence="0.978902">
1033
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.579840">
<title confidence="0.999951">Hierarchical Verb Clustering Using Graph Factorization</title>
<author confidence="0.998725">Lin Sun</author>
<author confidence="0.998725">Anna</author>
<affiliation confidence="0.973497">University of Cambridge, Computer</affiliation>
<address confidence="0.63923">15 JJ Thomson Avenue, Cambridge CB3 0GD,</address>
<email confidence="0.994106">ls418,alk23@cl.cam.ac.uk</email>
<abstract confidence="0.99460580952381">Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data, although many manually built classifications are taxonomic in nature. Also Language Processing applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification. We introduce a new clustering method called Hierar- Graph Factorization Clustering and extend it so that it is optimal for the task. results show that the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arik Azran</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>A new approach to data driven clustering.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning, ICML ’06,</booktitle>
<pages>57--64</pages>
<location>New York, NY, USA,</location>
<contexts>
<context position="18871" citStr="Azran and Ghahramani, 2006" startWordPosition="3185" endWordPosition="3188"> to neighbors. A walker can also go to other vertices via multi-hop transitions. According to the chain rule of the Markov process, the multi-hop transitions indicate a decaying similarity function on the graph (Yu et al., 2006). After t transitions, the similarity matrix (Wt) becomes: Wt = Wt−1D−1 0 W0 Yu et al. (2006) proved the correspondence between the HGFC levels (l) and the random walk time: t = 2l−1. So the vertices at level l induce a similarity matrix of verbs after t-hop transitions. The decaying similarity function captures the different scales of clustering structure in the data (Azran and Ghahramani, 2006b). The upper levels would have a smaller number of clusters which represent a more global structure. After several levels, all the verbs are expected to be grouped into one cluster. The number of levels and clusters at each level can thus be learned automatically. We therefore propose a method that uses the decaying similarity function to learn the hierarchical clustering structure. One simple modification to algorithm 1 is to set the number of clusters at level l p(vpll)|vi) = E �... Vl−1 V1 1027 (ml) to be ml−1 − 1. m is denoted as the number of clusters that have at least one member accord</context>
</contexts>
<marker>Azran, Ghahramani, 2006</marker>
<rawString>Arik Azran and Zoubin Ghahramani. A new approach to data driven clustering. In Proceedings of the 23rd international conference on Machine learning, ICML ’06, pages 57–64, New York, NY, USA, 2006a. ISBN 1-59593-383-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arik Azran</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Spectral methods for automatic multiscale data clustering.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Volume 1,</booktitle>
<pages>190--197</pages>
<publisher>IEEE Computer Society</publisher>
<location>Washington, DC, USA,</location>
<contexts>
<context position="18871" citStr="Azran and Ghahramani, 2006" startWordPosition="3185" endWordPosition="3188"> to neighbors. A walker can also go to other vertices via multi-hop transitions. According to the chain rule of the Markov process, the multi-hop transitions indicate a decaying similarity function on the graph (Yu et al., 2006). After t transitions, the similarity matrix (Wt) becomes: Wt = Wt−1D−1 0 W0 Yu et al. (2006) proved the correspondence between the HGFC levels (l) and the random walk time: t = 2l−1. So the vertices at level l induce a similarity matrix of verbs after t-hop transitions. The decaying similarity function captures the different scales of clustering structure in the data (Azran and Ghahramani, 2006b). The upper levels would have a smaller number of clusters which represent a more global structure. After several levels, all the verbs are expected to be grouped into one cluster. The number of levels and clusters at each level can thus be learned automatically. We therefore propose a method that uses the decaying similarity function to learn the hierarchical clustering structure. One simple modification to algorithm 1 is to set the number of clusters at level l p(vpll)|vi) = E �... Vl−1 V1 1027 (ml) to be ml−1 − 1. m is denoted as the number of clusters that have at least one member accord</context>
</contexts>
<marker>Azran, Ghahramani, 2006</marker>
<rawString>Arik Azran and Zoubin Ghahramani. Spectral methods for automatic multiscale data clustering. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Volume 1, pages 190–197. IEEE Computer Society Washington, DC, USA, 2006b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In In COLING-ACL,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="1325" citStr="Baker et al., 1998" startWordPosition="197" endWordPosition="200">lts show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification. 1 Introduction A variety of verb classifications have been built to support NLP tasks. These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The berkeley framenet project. In In COLING-ACL, pages 86–90, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Teresa Pazienza Basili</author>
<author>Paola Velardi</author>
</authors>
<title>Hierarchical clustering of verbs.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text,</booktitle>
<marker>Basili, Velardi, 1993</marker>
<rawString>Roberto. Basili, Maria Teresa Pazienza, and Paola Velardi. Hierarchical clustering of verbs. In Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikoletta Bassiou</author>
<author>Constantine Kotropoulos</author>
</authors>
<title>Long distance bigram models applied to word clustering.</title>
<date>2011</date>
<booktitle>Pattern Recogn.,</booktitle>
<pages>44--145</pages>
<contexts>
<context position="3605" citStr="Bassiou and Kotropoulos, 2011" startWordPosition="553" endWordPosition="556">eline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clus1023 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’s cluster membership at any level until a full graph is available, minimising the problem of error propagation) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons (Yu et al., 2006). The method has been applied to the identification of social network communities (Lin et al., 2008), but has not been us</context>
</contexts>
<marker>Bassiou, Kotropoulos, 2011</marker>
<rawString>Nikoletta Bassiou and Constantine Kotropoulos. Long distance bigram models applied to word clustering. Pattern Recogn., 44:145–158, January 2011. ISSN 0031-3203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the rasp system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions,</booktitle>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. The second release of the rasp system. In Proceedings of the COLING/ACL on Interactive presentation sessions, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Investigations into the Role of Lexical Semantics in Word Sense Disambiguation.</title>
<date>2004</date>
<tech>PhD thesis,</tech>
<institution>CIS, University of Pennsylvania,</institution>
<contexts>
<context position="1983" citStr="Dang, 2004" startWordPosition="297" endWordPosition="298">, 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). The best of such approaches ha</context>
</contexts>
<marker>Dang, 2004</marker>
<rawString>Hoa Trang Dang. Investigations into the Role of Lexical Semantics in Word Sense Disambiguation. PhD thesis, CIS, University of Pennsylvania, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>Towards a resource for lexical semantics: a large german corpus with extensive semantic annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>537--544</pages>
<location>Stroudsburg, PA, USA,</location>
<marker>Erk, Kowalski, Pad´o, Pinkal, 2003</marker>
<rawString>Katrin Erk, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. Towards a resource for lexical semantics: a large german corpus with extensive semantic annotation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 537–544, Stroudsburg, PA, USA, 2003. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Esteve Ferrer</author>
</authors>
<title>Towards a semantic classification of spanish verbs based on subcategorisation information.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL 2004 workshop on Student research, ACLstudent ’04,</booktitle>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="3237" citStr="Ferrer, 2004" startWordPosition="500" endWordPosition="502">y have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clus1023 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a </context>
<context position="33357" citStr="Ferrer (2004)" startWordPosition="5695" endWordPosition="5696">t al., 2009). However, some works have employed hierarchical clustering as a method to infer flat clustering. For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs. This gave better results than random initialization. Stevenson and Joanis (2003) used AGG for flat clustering on T1. They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cut-off. Our evaluation in the previous section shows that HGFC outperforms their implementation of AGG. AGG was also used by Ferrer (2004) who performed hierarchical clustering of 514 Spanish verbs. The results were evaluated against a hierarchical gold standard resembling that of Levin’s classification in English (V´azquez et al., 2000). Radj of 0.07 was reported for a 15-way classification which is comparable to the result of Stevenson and Joanis (2003). Hierarchical clustering has also been performed for the related task of semantic verb classification. For example, Basili et al. (1993) identified the problems of AGG, and applied a conceptual clustering algorithm (Fisher, 1987) to Italian verbs. They used semi-automatically a</context>
</contexts>
<marker>Ferrer, 2004</marker>
<rawString>Eva Esteve Ferrer. Towards a semantic classification of spanish verbs based on subcategorisation information. In Proceedings of the ACL 2004 workshop on Student research, ACLstudent ’04, Stroudsburg, PA, USA, 2004. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas H Fisher</author>
</authors>
<title>Knowledge acquisition via incremental conceptual clustering.</title>
<date>1987</date>
<booktitle>Machine Learning, 2:139– 172,</booktitle>
<pages>0885--6125</pages>
<contexts>
<context position="33908" citStr="Fisher, 1987" startWordPosition="5780" endWordPosition="5781">heir implementation of AGG. AGG was also used by Ferrer (2004) who performed hierarchical clustering of 514 Spanish verbs. The results were evaluated against a hierarchical gold standard resembling that of Levin’s classification in English (V´azquez et al., 2000). Radj of 0.07 was reported for a 15-way classification which is comparable to the result of Stevenson and Joanis (2003). Hierarchical clustering has also been performed for the related task of semantic verb classification. For example, Basili et al. (1993) identified the problems of AGG, and applied a conceptual clustering algorithm (Fisher, 1987) to Italian verbs. They used semi-automatically acquired semantic roles and the concept types as features. No quantitative results were reported. The qualitative evaluation shows that the resulting clusters are very fine-grained. Schulte im Walde (2008) performed hierarchical clustering of German verbs using human verb association as features and AGG as a method. They focussed on two small collections of 56 and 104 verbs and evaluated the result against flat gold standard extracted from GermaNet (Kunze and Lemnitzer, 2002) and German FrameNet (Erk et al., 2003), respectively. They reported F o</context>
</contexts>
<marker>Fisher, 1987</marker>
<rawString>Douglas H. Fisher. Knowledge acquisition via incremental conceptual clustering. Machine Learning, 2:139– 172, 1987. ISSN 0885-6125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>North american news text corpus. Linguistic Data Consortium,</title>
<date>1995</date>
<contexts>
<context position="9063" citStr="Graff, 1995" startWordPosition="1447" endWordPosition="1448">ethods in the following two subsecof classification. T3 provides classification of 357 tions, respectively. The subsequent two subsections verbs into 11 top level, 14 second level, and 32 third present our extensions to HGFC: (i) automatically level classes. determining the cluster structure and (ii) adding soft For each verb appearing in T1-T3, we extracted constraints to guide clustering performance. all the occurrences (up to 10,000) from the British 3.2.1 Agglomerative clustering National Corpus (Leech, 1992) and North American AGG is a method which treats each verb as a News Text Corpus (Graff, 1995). singleton cluster and then successively merges two 3 Method closest clusters until all the clusters have been 3.1 Features and feature extraction merged into one. We used the SciPy’s implePrevious works on Levin style verb classification mentation (Oliphant, 2007) of the algorithm. The have investigated optimal features for this task cluster distance is measured using linkage criteria. (Stevenson and Joanis, 2003; Li and Brew, 2008; We experimented with four commonly used linkSun and Korhonen, 2009)). We adopt for our exper- age criteria: Single, Average, Complete and Ward’s iments a set of </context>
</contexts>
<marker>Graff, 1995</marker>
<rawString>David Graff. North american news text corpus. Linguistic Data Consortium, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Catherine Macleod</author>
<author>Adam Meyers</author>
</authors>
<title>Comlex syntax: Building a computational lexicon.</title>
<date>1994</date>
<booktitle>In COLING,</booktitle>
<pages>268--272</pages>
<contexts>
<context position="1291" citStr="Grishman et al., 1994" startWordPosition="191" endWordPosition="194"> it is optimal for the task. Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification. 1 Introduction A variety of verb classifications have been built to support NLP tasks. These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information ex</context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>Ralph Grishman, Catherine Macleod, and Adam Meyers. Comlex syntax: Building a computational lexicon. In COLING, pages 268–272, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine A Heller</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Bayesian hierarchical clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>297--304</pages>
<publisher>ACM,</publisher>
<marker>Heller, Ghahramani, 2005</marker>
<rawString>Katherine A. Heller and Zoubin Ghahramani. Bayesian hierarchical clustering. In Proceedings of the 22nd international conference on Machine learning, pages 297–304. ACM, 2005. ISBN 1595931805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06,</booktitle>
<pages>57--60</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="1380" citStr="Hovy et al., 2006" startWordPosition="207" endWordPosition="210">omerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification. 1 Introduction A variety of verb classifications have been built to support NLP tasks. These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. Ontonotes: the 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06, pages 57–60, Stroudsburg, PA, USA, 2006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Hubert</author>
<author>Phipps Arabie</author>
</authors>
<title>Comparing partitions.</title>
<date>1985</date>
<journal>Journal of Classification,</journal>
<volume>2</volume>
<pages>0176--4268</pages>
<contexts>
<context position="21282" citStr="Hubert and Arabie, 1985" startWordPosition="3600" endWordPosition="3603"> or features) is reduced according to a. 4 Experimental evaluation We applied the clustering methods introduced in section 3 to the test sets described in section 2 and evaluated them both quantitatively and qualitatively, as described in the subsequent sections. 4.1 Evaluation methods We used class based accuracy (ACC) and adjusted rand index (Radj) to evaluate the results on the flat test set T1 (see section 2 for details of T1-T3). ACC is the proportion of members of dominant clusters DOM-CLUSTi within all classes ci. PC i=1 verbs in DOM-CLUSTi ACC = number of verbs The formula of Radj is (Hubert and Arabie, 1985): P �nij� − P �ni· ~P �n·j�/�n � i,j 2 i 2 j 2 Radj = �ni· �n·j 2 2[P � + P �n·j�] − P �ni· ~P �/�n� 1 i 2 j 2 i 2 j 2 2 where nij is the size of the intersection between class i and cluster j. We used normalized mutual information (NMI) and F-Score (F) to evaluate hierarchical clustering results on T2 and T3. NMI measures the amount of statistical information shared by two random variables representing the clustering result and the goldstandard labels. Given random variables A and B: I(A; B) NMI(A, B) = [H(A) + H(B)]/2 |(vk n cj |log N |vk n cj | N |vk||cj| where |vk n cj |is the number of sh</context>
</contexts>
<marker>Hubert, Arabie, 1985</marker>
<rawString>Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classification, 2:193–218, 1985. ISSN 0176-4268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Joanis</author>
<author>Suzanne Stevenson</author>
<author>David James</author>
</authors>
<title>A general feature space for automatic verb classification.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="2434" citStr="Joanis et al., 2008" startWordPosition="372" endWordPosition="375">. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerati</context>
<context position="32639" citStr="Joanis et al., 2008" startWordPosition="5571" endWordPosition="5574">igh accuracy. When the number of clusters and levels is learned automatically, the unconstrained method produces a multi-level hierarchy. Our evaluation against a 3-level gold standard shows that such a hierarchy is fairly accurate. Finally, the results from our qualitative evaluation show that both constrained and unconstrained versions of HGFC are capable of learning valuable novel information not included in the gold standards. The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). However, some works have employed hierarchical clustering as a method to infer flat clustering. For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs. This gave better results than random initialization. Stevenson and Joanis (2003) used AGG for flat clustering on T1. They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cut-off. Our evaluati</context>
</contexts>
<marker>Joanis, Stevenson, James, 2008</marker>
<rawString>Eric Joanis, Suzanne Stevenson, and David James. A general feature space for automatic verb classification. Natural Language Engineering, 14(3):337–367, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
</authors>
<title>VerbNet: A broad-coverage, comprehensive verb lexicon.</title>
<date>2005</date>
<contexts>
<context position="1360" citStr="Kipper, 2005" startWordPosition="205" endWordPosition="206">ntly used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification. 1 Introduction A variety of verb classifications have been built to support NLP tasks. These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stev</context>
<context position="6400" citStr="Kipper, 2005" startWordPosition="1007" endWordPosition="1008">es and levels of hierarchy. 2 Target classification and test sets The taxonomy of Levin (1993) groups English verbs (e.g. break, fracture, rip) into classes (e.g. 45.1 Break verbs) on the basis of their shared meaning components and (morpho-)syntactic behaviour, defined in terms of diathesis alternations (e.g. the causative/inchoative alternation, where an NP frame alternates with an intransitive frame: Tony broke the window ↔ The window broke). It classifies over 3000 verbs in 57 top level classes, some of which divide further into subclasses. The extended version of the taxonomy in VerbNet (Kipper, 2005) classifies 5757 verbs. Its 5 level taxonomy includes 101 top level and 369 subclasses. We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003). We included this small gold standard in our experiments so that we could compare the flat version of our method against previously published methods. Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 time</context>
</contexts>
<marker>Kipper, 2005</marker>
<rawString>Karin Kipper. VerbNet: A broad-coverage, comprehensive verb lexicon. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
<author>Nigel Collier</author>
</authors>
<title>The Choice of Features for Classification of Verbs in Biomedical Texts.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<contexts>
<context position="2494" citStr="Korhonen et al., 2008" startWordPosition="384" endWordPosition="387">ng, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acqu</context>
<context position="32699" citStr="Korhonen et al., 2008" startWordPosition="5583" endWordPosition="5586">arned automatically, the unconstrained method produces a multi-level hierarchy. Our evaluation against a 3-level gold standard shows that such a hierarchy is fairly accurate. Finally, the results from our qualitative evaluation show that both constrained and unconstrained versions of HGFC are capable of learning valuable novel information not included in the gold standards. The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). However, some works have employed hierarchical clustering as a method to infer flat clustering. For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs. This gave better results than random initialization. Stevenson and Joanis (2003) used AGG for flat clustering on T1. They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cut-off. Our evaluation in the previous section shows that HGFC outperforms their</context>
</contexts>
<marker>Korhonen, Krymolowski, Collier, 2008</marker>
<rawString>Anna Korhonen, Yuval Krymolowski, and Nigel Collier. The Choice of Features for Classification of Verbs in Biomedical Texts. In Proceedings of COLING, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Kunze</author>
<author>Lothar Lemnitzer</author>
</authors>
<title>GermaNetrepresentation, visualization, application.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC,</booktitle>
<contexts>
<context position="34436" citStr="Kunze and Lemnitzer, 2002" startWordPosition="5858" endWordPosition="5861">1993) identified the problems of AGG, and applied a conceptual clustering algorithm (Fisher, 1987) to Italian verbs. They used semi-automatically acquired semantic roles and the concept types as features. No quantitative results were reported. The qualitative evaluation shows that the resulting clusters are very fine-grained. Schulte im Walde (2008) performed hierarchical clustering of German verbs using human verb association as features and AGG as a method. They focussed on two small collections of 56 and 104 verbs and evaluated the result against flat gold standard extracted from GermaNet (Kunze and Lemnitzer, 2002) and German FrameNet (Erk et al., 2003), respectively. They reported F of 62.69% for the 56 verbs, and F of 34.68% for the 104 verbs. In the future, we plan to extend this research line in several directions. First, we will try to determine optimal features for different levels of clustering. For example, the general syntactic features (e.g. SCF) may perform the best at top levels of a hierarchy while more specific or refined features (e.g. SCF+pp) may be optimal at lower levels. We also plan to investigate incorporating semantic features, like verb selectional preferences, in our feature set.</context>
</contexts>
<marker>Kunze, Lemnitzer, 2002</marker>
<rawString>Claudia Kunze and Lothar Lemnitzer. GermaNetrepresentation, visualization, application. In Proceedings of LREC, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
</authors>
<title>100 million words of english: the british national corpus.</title>
<date>1992</date>
<journal>Language Research,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="8969" citStr="Leech, 1992" startWordPosition="1429" endWordPosition="1430">l Graph Factorization Clustering pare the impact of constraints across several levels (HGFC) methods in the following two subsecof classification. T3 provides classification of 357 tions, respectively. The subsequent two subsections verbs into 11 top level, 14 second level, and 32 third present our extensions to HGFC: (i) automatically level classes. determining the cluster structure and (ii) adding soft For each verb appearing in T1-T3, we extracted constraints to guide clustering performance. all the occurrences (up to 10,000) from the British 3.2.1 Agglomerative clustering National Corpus (Leech, 1992) and North American AGG is a method which treats each verb as a News Text Corpus (Graff, 1995). singleton cluster and then successively merges two 3 Method closest clusters until all the clusters have been 3.1 Features and feature extraction merged into one. We used the SciPy’s implePrevious works on Levin style verb classification mentation (Oliphant, 2007) of the algorithm. The have investigated optimal features for this task cluster distance is measured using linkage criteria. (Stevenson and Joanis, 2003; Li and Brew, 2008; We experimented with four commonly used linkSun and Korhonen, 2009)</context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>Geoffrey Leech. 100 million words of english: the british national corpus. Language Research, 28(1):1– 13, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levin</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation.</title>
<date>1993</date>
<location>Chicago, IL,</location>
<contexts>
<context position="1602" citStr="Levin (1993)" startWordPosition="242" endWordPosition="243">as to extend existing ones on the basis of some prior knowledge about the classification. 1 Introduction A variety of verb classifications have been built to support NLP tasks. These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effect</context>
<context position="5881" citStr="Levin (1993)" startWordPosition="928" endWordPosition="929">tter than AGG at all levels of gold standard classification. The constrained version of HGFC performs the best, as expected, demonstrating the usefulness of soft constraints for extending partial classifications. Our qualitative analysis shows that HGFC is capable of detecting novel information not included in our gold standards. The unconstrained version can be used to acquire novel classifications from scratch while the constrained version can be used to extend existing ones with additional class members, classes and levels of hierarchy. 2 Target classification and test sets The taxonomy of Levin (1993) groups English verbs (e.g. break, fracture, rip) into classes (e.g. 45.1 Break verbs) on the basis of their shared meaning components and (morpho-)syntactic behaviour, defined in terms of diathesis alternations (e.g. the causative/inchoative alternation, where an NP frame alternates with an intransitive frame: Tony broke the window ↔ The window broke). It classifies over 3000 verbs in 57 top level classes, some of which divide further into subclasses. The extended version of the taxonomy in VerbNet (Kipper, 2005) classifies 5757 verbs. Its 5 level taxonomy includes 101 top level and 369 subcl</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth. Levin. English verb classes and alternations: A preliminary investigation. Chicago, IL, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianguo Li</author>
<author>Chris Brew</author>
</authors>
<title>Which Are the Best Features for Automatic Verb Classification.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<contexts>
<context position="2471" citStr="Li and Brew, 2008" startWordPosition="380" endWordPosition="383">emantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been </context>
<context position="9500" citStr="Li and Brew, 2008" startWordPosition="1511" endWordPosition="1514"> 10,000) from the British 3.2.1 Agglomerative clustering National Corpus (Leech, 1992) and North American AGG is a method which treats each verb as a News Text Corpus (Graff, 1995). singleton cluster and then successively merges two 3 Method closest clusters until all the clusters have been 3.1 Features and feature extraction merged into one. We used the SciPy’s implePrevious works on Levin style verb classification mentation (Oliphant, 2007) of the algorithm. The have investigated optimal features for this task cluster distance is measured using linkage criteria. (Stevenson and Joanis, 2003; Li and Brew, 2008; We experimented with four commonly used linkSun and Korhonen, 2009)). We adopt for our exper- age criteria: Single, Average, Complete and Ward’s iments a set of features which have performed well (Ward Jr., 1963). Ward’s criterion performed the in recent verb clustering works: best and was used in all the experiments in this paA: Subcategorization frames (SCFs) and their rela- per. It measures the increase in variance after two tive frequencies with individual verbs. clusters are merged. The output of AGG tends to B: A with SCFs parameterized for prepositions. have excessive number of levels</context>
<context position="32676" citStr="Li and Brew, 2008" startWordPosition="5579" endWordPosition="5582">rs and levels is learned automatically, the unconstrained method produces a multi-level hierarchy. Our evaluation against a 3-level gold standard shows that such a hierarchy is fairly accurate. Finally, the results from our qualitative evaluation show that both constrained and unconstrained versions of HGFC are capable of learning valuable novel information not included in the gold standards. The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). However, some works have employed hierarchical clustering as a method to infer flat clustering. For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs. This gave better results than random initialization. Stevenson and Joanis (2003) used AGG for flat clustering on T1. They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cut-off. Our evaluation in the previous section shows that</context>
</contexts>
<marker>Li, Brew, 2008</marker>
<rawString>Jianguo Li and Chris Brew. Which Are the Best Features for Automatic Verb Classification. In Proceedings of ACL, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Ru Lin</author>
<author>Yun Chi</author>
<author>Shenghuo Zhu</author>
<author>Hari Sundaram</author>
<author>Belle L Tseng</author>
</authors>
<title>Facetnet: a framework for analyzing communities and their evolutions in dynamic networks.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th international conference on World Wide Web,</booktitle>
<pages>685--694</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="4184" citStr="Lin et al., 2008" startWordPosition="646" endWordPosition="649">, 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’s cluster membership at any level until a full graph is available, minimising the problem of error propagation) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons (Yu et al., 2006). The method has been applied to the identification of social network communities (Lin et al., 2008), but has not been used (to the best of our knowledge) in NLP before. We modify HGFC with a new tree extraction algorithm which ensures a more consistent result, and we propose two novel extensions to it. The first is a method for automatically determining the tree structure (i.e. number of clusters to be produced for each level of the hierarchy). This avoids the need to predetermine the number of clusters manually. The second is addition of soft constraints to guide the clustering performance (Vlachos et al., 2009). This is useful for situations where a partial (e.g. a flat) verb classificati</context>
</contexts>
<marker>Lin, Chi, Zhu, Sundaram, Tseng, 2008</marker>
<rawString>Yu-Ru Lin, Yun Chi, Shenghuo Zhu, Hari Sundaram, and Belle L. Tseng. Facetnet: a framework for analyzing communities and their evolutions in dynamic networks. In Proceeding of the 17th international conference on World Wide Web, pages 685–694, New York, NY, USA, 2008. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schtze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<journal>ISBN</journal>
<volume>0521865719</volume>
<pages>9780521865715</pages>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="22085" citStr="Manning et al., 2008" startWordPosition="3756" endWordPosition="3759"> cluster j. We used normalized mutual information (NMI) and F-Score (F) to evaluate hierarchical clustering results on T2 and T3. NMI measures the amount of statistical information shared by two random variables representing the clustering result and the goldstandard labels. Given random variables A and B: I(A; B) NMI(A, B) = [H(A) + H(B)]/2 |(vk n cj |log N |vk n cj | N |vk||cj| where |vk n cj |is the number of shared membership between cluster vk and gold-standard class cj. The normalized variant of mutual information (MI) enables the comparison of clustering with different cluster numbers (Manning et al., 2008). F is the harmonic mean of precision (P) and recall (R). P is calculated using modified purity – a global measure which evaluates the mean precision of clusters. Each cluster is associated with its prevalent class. The number of verbs in a cluster K that take this class is denoted by nprevalent(K). nprevalent(ki)&gt;2 nprevalent(ki) number of verbs R is calculated using ACC. F= 2 · mPUR · ACC mPUR + ACC F is not suitable for comparing results with different cluster numbers (Rosenberg and Hirschberg, 2007). Therefore, we only report NMI when the number of classes in clustering and gold-standard i</context>
</contexts>
<marker>Manning, Raghavan, Schtze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008. ISBN 0521865719, 9780521865715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaka Matsuo</author>
<author>Takeshi Sakaki</author>
<author>Kˆoki Uchiyama</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Graph-based word clustering using a web search engine.</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>542--550</pages>
<contexts>
<context position="3573" citStr="Matsuo et al., 2006" startWordPosition="549" endWordPosition="552">. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clus1023 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’s cluster membership at any level until a full graph is available, minimising the problem of error propagation) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons (Yu et al., 2006). The method has been applied to the identification of social network communities (Lin et</context>
</contexts>
<marker>Matsuo, Sakaki, Uchiyama, Ishizuka, 2006</marker>
<rawString>Yutaka Matsuo, Takeshi Sakaki, Kˆoki Uchiyama, and Mitsuru Ishizuka. Graph-based word clustering using a web search engine. In Proceedings of the EMNLP, pages 542–550, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1305" citStr="Miller, 1995" startWordPosition="195" endWordPosition="196">task. Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification. 1 Introduction A variety of verb classifications have been built to support NLP tasks. These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, ques</context>
<context position="7417" citStr="Miller, 1995" startWordPosition="1182" endWordPosition="1183">that we could compare the flat version of our method against previously published methods. Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus. This gave us 260 verbs in total. T2: The second gold standard is a large, hierarchical gold standard which we extracted from VerbNet as follows: 1) We removed all the verbs that have less than 1000 occurrences in our corpus. 2) In order to minimise the problem of polysemy, we assigned each verb to the class which, according to VerbNet, corresponds to its predominant sense in WordNet (Miller, 1995). 3) In order to minimise the sparse data problem with very finegrained classes, we converted the resulting classification into a 3-level representation so that the classes at the 4th and 5th level were combined. For example, the sub-classes of Declare verbs (numbered as 29.4.1.1.{1,2,3}) were combined into 29.4.1. 4) The classes that have fewer than 5 members were discarded. The total number of verb senses in the resulting gold standard is 1750, which is 33.2% of the verbs in VerbNet. T2 has 51 top level, 117 second level, and 133 third level classes. T3: The third gold standard is a subset o</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Travis E Oliphant</author>
</authors>
<title>Python for scientific computing.</title>
<date>2007</date>
<journal>Computing in Science and Engineering,</journal>
<volume>9</volume>
<pages>1521--9615</pages>
<contexts>
<context position="9329" citStr="Oliphant, 2007" startWordPosition="1487" endWordPosition="1488">rmining the cluster structure and (ii) adding soft For each verb appearing in T1-T3, we extracted constraints to guide clustering performance. all the occurrences (up to 10,000) from the British 3.2.1 Agglomerative clustering National Corpus (Leech, 1992) and North American AGG is a method which treats each verb as a News Text Corpus (Graff, 1995). singleton cluster and then successively merges two 3 Method closest clusters until all the clusters have been 3.1 Features and feature extraction merged into one. We used the SciPy’s implePrevious works on Levin style verb classification mentation (Oliphant, 2007) of the algorithm. The have investigated optimal features for this task cluster distance is measured using linkage criteria. (Stevenson and Joanis, 2003; Li and Brew, 2008; We experimented with four commonly used linkSun and Korhonen, 2009)). We adopt for our exper- age criteria: Single, Average, Complete and Ward’s iments a set of features which have performed well (Ward Jr., 1963). Ward’s criterion performed the in recent verb clustering works: best and was used in all the experiments in this paA: Subcategorization frames (SCFs) and their rela- per. It measures the increase in variance after</context>
</contexts>
<marker>Oliphant, 2007</marker>
<rawString>Travis E. Oliphant. Python for scientific computing. Computing in Science and Engineering, 9:10–20, 2007. ISSN 1521-9615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Ann Copestake</author>
</authors>
<title>Semantic classification with distributional kernels.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<marker>S´eaghdha, Copestake, 2008</marker>
<rawString>Diarmuid O´ S´eaghdha and Ann Copestake. Semantic classification with distributional kernels. In Proceedings of COLING, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1346" citStr="Palmer et al., 2005" startWordPosition="201" endWordPosition="204">utperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification. 1 Introduction A variety of verb classifications have been built to support NLP tasks. These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
<author>Ted Briscoe</author>
<author>Anna Korhonen</author>
</authors>
<title>A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>912--919</pages>
<contexts>
<context position="11552" citStr="Preiss et al. (2007)" startWordPosition="1850" endWordPosition="1853">any tactic features) in the recent work of Sun and Ko- level. For example, in order to group clusters reprhonen (2009), we left such features for future work resenting Levin classes 9.1, 9.2 and 9.3 into a sinbecause we noticed that different levels of classifi- gle cluster representing class 9, the method has to cation are likely to require semantic features at dif- produce intermediate clusters, e.g. 9.{1,2} and 9.3. ferent granularities. Such clusters do not always have a semantic interWe extracted the syntactic features using the sys- pretation. Although they can be removed using a tem of Preiss et al. (2007). The system tags, lemma- cut-based method, this requires a pre-defined cut-off tizes and parses corpus data using the RASP (Robust value which is difficult to set (Stevenson and Joanis, Accurate Statistical Parsing toolkit (Briscoe et al., 2003). In addition, a significant amount of informa2006)), and on the basis of the resulting grammat- tion is lost in pair-wise clustering. In the above exical relations, assigns each occurrence of a verb as ample, only the clusters 9.{1,2} and 9.3 are consida member of one of the 168 verbal SCFs. We pa- ered, while alternative clusters 9.{1,3} and 9.2 are </context>
</contexts>
<marker>Preiss, Briscoe, Korhonen, 2007</marker>
<rawString>Judita Preiss, Ted Briscoe, and Anna Korhonen. A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora. In Proceedings of ACL, pages 912–919, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>V-measure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<contexts>
<context position="22593" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="3845" endWordPosition="3848">riant of mutual information (MI) enables the comparison of clustering with different cluster numbers (Manning et al., 2008). F is the harmonic mean of precision (P) and recall (R). P is calculated using modified purity – a global measure which evaluates the mean precision of clusters. Each cluster is associated with its prevalent class. The number of verbs in a cluster K that take this class is denoted by nprevalent(K). nprevalent(ki)&gt;2 nprevalent(ki) number of verbs R is calculated using ACC. F= 2 · mPUR · ACC mPUR + ACC F is not suitable for comparing results with different cluster numbers (Rosenberg and Hirschberg, 2007). Therefore, we only report NMI when the number of classes in clustering and gold-standard is substantially different. Finally, we supplemented quantitative evaluation with qualitative evaluation of clusters produced by different methods. 4.2 Quantitative evaluation We first evaluated AGG and the basic (unconstrained) HGFC on the small flat test set T1. The main purpose of this evaluation was to compare the results of our methods against previously published results on the same test set. The number of clusters (K) and levels (L) were inferred automatically for HGFC as described in section 3.2.</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Experiments on the automatic induction of german semantic verb classes.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="2413" citStr="Walde, 2006" startWordPosition="370" endWordPosition="371">including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical</context>
<context position="32618" citStr="Walde, 2006" startWordPosition="5569" endWordPosition="5570">ndards with high accuracy. When the number of clusters and levels is learned automatically, the unconstrained method produces a multi-level hierarchy. Our evaluation against a 3-level gold standard shows that such a hierarchy is fairly accurate. Finally, the results from our qualitative evaluation show that both constrained and unconstrained versions of HGFC are capable of learning valuable novel information not included in the gold standards. The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). However, some works have employed hierarchical clustering as a method to infer flat clustering. For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs. This gave better results than random initialization. Stevenson and Joanis (2003) used AGG for flat clustering on T1. They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good </context>
</contexts>
<marker>Walde, 2006</marker>
<rawString>Sabine Schulte im Walde. Experiments on the automatic induction of german semantic verb classes. Computational Linguistics, 32(2), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Human associations and the choice of features for semantic verb classification.</title>
<date>2008</date>
<journal>Research on Language and Computation,</journal>
<volume>6</volume>
<pages>1570--7075</pages>
<contexts>
<context position="3262" citStr="Walde, 2008" startWordPosition="505" endWordPosition="506"> acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clus1023 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’s cluster membership</context>
<context position="34161" citStr="Walde (2008)" startWordPosition="5815" endWordPosition="5816">l., 2000). Radj of 0.07 was reported for a 15-way classification which is comparable to the result of Stevenson and Joanis (2003). Hierarchical clustering has also been performed for the related task of semantic verb classification. For example, Basili et al. (1993) identified the problems of AGG, and applied a conceptual clustering algorithm (Fisher, 1987) to Italian verbs. They used semi-automatically acquired semantic roles and the concept types as features. No quantitative results were reported. The qualitative evaluation shows that the resulting clusters are very fine-grained. Schulte im Walde (2008) performed hierarchical clustering of German verbs using human verb association as features and AGG as a method. They focussed on two small collections of 56 and 104 verbs and evaluated the result against flat gold standard extracted from GermaNet (Kunze and Lemnitzer, 2002) and German FrameNet (Erk et al., 2003), respectively. They reported F of 62.69% for the 56 verbs, and F of 34.68% for the 104 verbs. In the future, we plan to extend this research line in several directions. First, we will try to determine optimal features for different levels of clustering. For example, the general syntac</context>
</contexts>
<marker>Walde, 2008</marker>
<rawString>Sabine Schulte im Walde. Human associations and the choice of features for semantic verb classification. Research on Language and Computation, 6:79–111, 2008. ISSN 1570-7075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Chris Brew</author>
</authors>
<title>Inducing german semantic verb classes from purely syntactic subcategorisation information.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>223--230</pages>
<contexts>
<context position="32899" citStr="Walde and Brew (2002)" startWordPosition="5616" endWordPosition="5619"> our qualitative evaluation show that both constrained and unconstrained versions of HGFC are capable of learning valuable novel information not included in the gold standards. The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). However, some works have employed hierarchical clustering as a method to infer flat clustering. For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs. This gave better results than random initialization. Stevenson and Joanis (2003) used AGG for flat clustering on T1. They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cut-off. Our evaluation in the previous section shows that HGFC outperforms their implementation of AGG. AGG was also used by Ferrer (2004) who performed hierarchical clustering of 514 Spanish verbs. The results were evaluated against a hierarchical gold standard resembling that o</context>
</contexts>
<marker>Walde, Brew, 2002</marker>
<rawString>Sabine Schulte im Walde and Chris Brew. Inducing german semantic verb classes from purely syntactic subcategorisation information. In Proceedings of ACL, pages 223–230, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianbo Shi</author>
<author>Jitendra Malik</author>
</authors>
<title>Normalized cuts and image segmentation.</title>
<date>2000</date>
<journal>IEEE Transactions on pattern analysis and machine intelligence,</journal>
<volume>22</volume>
<issue>8</issue>
<contexts>
<context position="10211" citStr="Shi and Malik, 2000" startWordPosition="1627" endWordPosition="1630">per- age criteria: Single, Average, Complete and Ward’s iments a set of features which have performed well (Ward Jr., 1963). Ward’s criterion performed the in recent verb clustering works: best and was used in all the experiments in this paA: Subcategorization frames (SCFs) and their rela- per. It measures the increase in variance after two tive frequencies with individual verbs. clusters are merged. The output of AGG tends to B: A with SCFs parameterized for prepositions. have excessive number of levels. Cut-based methC: B with SCFs parameterized for subjects appear- ods (Wu and Leahy, 1993; Shi and Malik, 2000) are ing in grammatical relations associated with the frequently applied to extract a simplified view. We verb in parsed data. followed previous verb clustering works and cut the D: B with SCFs parameterized for objects appear- AGG hierarchy manually. ing in grammatical relations associated with the AGG suffers from two problems. The first is erverb in parsed data. ror propagation. When a verb is misclassified at a These features are purely syntactic. Although lower level, the error propagates to all the upper levsemantic features – verb selectional preferences – els. The second is local pairw</context>
</contexts>
<marker>Shi, Malik, 2000</marker>
<rawString>Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8):888–905, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Putting pieces together: Combining FrameNet, VerbNet and WordNet for robust semantic parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLING,</booktitle>
<contexts>
<context position="2007" citStr="Shi and Mihalcea, 2005" startWordPosition="299" endWordPosition="302">ssifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). The best of such approaches have yielded promising res</context>
</contexts>
<marker>Shi, Mihalcea, 2005</marker>
<rawString>Lei Shi and Rada Mihalcea. Putting pieces together: Combining FrameNet, VerbNet and WordNet for robust semantic parsing. In Proceedings of CICLING, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Stevenson</author>
<author>Eric Joanis</author>
</authors>
<title>Semi-supervised verb class discovery using noisy features.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="3159" citStr="Stevenson and Joanis, 2003" startWordPosition="485" endWordPosition="488">achos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clus1023 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering a</context>
<context position="6743" citStr="Stevenson and Joanis, 2003" startWordPosition="1060" endWordPosition="1063">hoative alternation, where an NP frame alternates with an intransitive frame: Tony broke the window ↔ The window broke). It classifies over 3000 verbs in 57 top level classes, some of which divide further into subclasses. The extended version of the taxonomy in VerbNet (Kipper, 2005) classifies 5757 verbs. Its 5 level taxonomy includes 101 top level and 369 subclasses. We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003). We included this small gold standard in our experiments so that we could compare the flat version of our method against previously published methods. Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus. This gave us 260 verbs in total. T2: The second gold standard is a large, hierarchical gold standard which we extracted from VerbNet as follows: 1) We removed all the verbs that have less than 1000 occurrences in our corpus. 2) In order to minimise the problem of polysemy, we assigned each verb to the class which, according </context>
<context position="9481" citStr="Stevenson and Joanis, 2003" startWordPosition="1507" endWordPosition="1510">. all the occurrences (up to 10,000) from the British 3.2.1 Agglomerative clustering National Corpus (Leech, 1992) and North American AGG is a method which treats each verb as a News Text Corpus (Graff, 1995). singleton cluster and then successively merges two 3 Method closest clusters until all the clusters have been 3.1 Features and feature extraction merged into one. We used the SciPy’s implePrevious works on Levin style verb classification mentation (Oliphant, 2007) of the algorithm. The have investigated optimal features for this task cluster distance is measured using linkage criteria. (Stevenson and Joanis, 2003; Li and Brew, 2008; We experimented with four commonly used linkSun and Korhonen, 2009)). We adopt for our exper- age criteria: Single, Average, Complete and Ward’s iments a set of features which have performed well (Ward Jr., 1963). Ward’s criterion performed the in recent verb clustering works: best and was used in all the experiments in this paA: Subcategorization frames (SCFs) and their rela- per. It measures the increase in variance after two tive frequencies with individual verbs. clusters are merged. The output of AGG tends to B: A with SCFs parameterized for prepositions. have excessi</context>
<context position="23546" citStr="Stevenson and Joanis (2003)" startWordPosition="4004" endWordPosition="4007">n the small flat test set T1. The main purpose of this evaluation was to compare the results of our methods against previously published results on the same test set. The number of clusters (K) and levels (L) were inferred automatically for HGFC as described in section 3.2.3. However, to I(A, B) = X X k j P mPUR = 1028 make the results comparable with previously published ones, we cut the resulting hierarchy at the level of closest match (12 clusters) to the K (13) in the gold-standard. For AGG, we cut the hierarchy at 13 clusters. Method ACC Radj HGFC 41.2 17.4 AGG (reproduced) 32.7 9.9 AGG (Stevenson and Joanis (2003) 31.0 9.0 Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features). Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion. In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%. When using this simple feature set, HGFC outperforms the best performing AGG clearly: 8.5% in ACC and 7.3% in Radj. We also compared HGFC against the best reported </context>
<context position="33047" citStr="Stevenson and Joanis (2003)" startWordPosition="5637" endWordPosition="5640">ot included in the gold standards. The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). However, some works have employed hierarchical clustering as a method to infer flat clustering. For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs. This gave better results than random initialization. Stevenson and Joanis (2003) used AGG for flat clustering on T1. They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cut-off. Our evaluation in the previous section shows that HGFC outperforms their implementation of AGG. AGG was also used by Ferrer (2004) who performed hierarchical clustering of 514 Spanish verbs. The results were evaluated against a hierarchical gold standard resembling that of Levin’s classification in English (V´azquez et al., 2000). Radj of 0.07 was reported for a 15-way classification which is comparable to the result</context>
</contexts>
<marker>Stevenson, Joanis, 2003</marker>
<rawString>Suzanne Stevenson and Eric Joanis. Semi-supervised verb class discovery using noisy features. In Proceedings of HLT-NAACL 2003, pages 71–78, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preferences.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<contexts>
<context position="5199" citStr="Sun and Korhonen, 2009" startWordPosition="823" endWordPosition="826">umber of clusters manually. The second is addition of soft constraints to guide the clustering performance (Vlachos et al., 2009). This is useful for situations where a partial (e.g. a flat) verb classification is available and the goal is to extend it. Adopting a set of lexical and syntactic features which have performed well in previous works, we compare the performance of the two methods on test sets extracted from Levin and VerbNet. When evaluated on a flat clustering task, HGFC outperforms AGG and performs very similarly with the best flat clustering method reported on the same test set (Sun and Korhonen, 2009). When evaluated on a hierarchical task, HGFC performs considerably better than AGG at all levels of gold standard classification. The constrained version of HGFC performs the best, as expected, demonstrating the usefulness of soft constraints for extending partial classifications. Our qualitative analysis shows that HGFC is capable of detecting novel information not included in our gold standards. The unconstrained version can be used to acquire novel classifications from scratch while the constrained version can be used to extend existing ones with additional class members, classes and level</context>
<context position="9569" citStr="Sun and Korhonen, 2009" startWordPosition="1521" endWordPosition="1525">al Corpus (Leech, 1992) and North American AGG is a method which treats each verb as a News Text Corpus (Graff, 1995). singleton cluster and then successively merges two 3 Method closest clusters until all the clusters have been 3.1 Features and feature extraction merged into one. We used the SciPy’s implePrevious works on Levin style verb classification mentation (Oliphant, 2007) of the algorithm. The have investigated optimal features for this task cluster distance is measured using linkage criteria. (Stevenson and Joanis, 2003; Li and Brew, 2008; We experimented with four commonly used linkSun and Korhonen, 2009)). We adopt for our exper- age criteria: Single, Average, Complete and Ward’s iments a set of features which have performed well (Ward Jr., 1963). Ward’s criterion performed the in recent verb clustering works: best and was used in all the experiments in this paA: Subcategorization frames (SCFs) and their rela- per. It measures the increase in variance after two tive frequencies with individual verbs. clusters are merged. The output of AGG tends to B: A with SCFs parameterized for prepositions. have excessive number of levels. Cut-based methC: B with SCFs parameterized for subjects appear- ods</context>
<context position="24234" citStr="Sun and Korhonen (2009)" startWordPosition="4123" endWordPosition="4126">’s result on T1 (using similar features). Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion. In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%. When using this simple feature set, HGFC outperforms the best performing AGG clearly: 8.5% in ACC and 7.3% in Radj. We also compared HGFC against the best reported clustering method on T1 to date – that of spectral clustering by Sun and Korhonen (2009). We used the feature sets C and D which are similar to the features (SCF parameterized by lexical prefences) in their experiments. HGFC obtains F of 49.93% on T1 which is 5% lower than the result of Sun and Korhonen (2009). The difference comes from the tree consistency requirement. When the HGFC is forced to produce a flat clustering (a one level tree only), it achieves the F of 52.55% which is very close to the performance of spectral clustering. We then evaluated our methods on the hierarchical test sets T2 and T3. In the first set of experiments, we pre-defined the tree structure for HGFC</context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>Lin Sun and Anna Korhonen. Improving verb clustering with automatically acquired selectional preferences. In Proceedings of the EMNLP 2009, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
</authors>
<title>Verb class discovery from rich syntactic data.</title>
<date>2008</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>4919</volume>
<contexts>
<context position="2452" citStr="Sun et al., 2008" startWordPosition="376" endWordPosition="379"> disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG</context>
<context position="32657" citStr="Sun et al., 2008" startWordPosition="5575" endWordPosition="5578">e number of clusters and levels is learned automatically, the unconstrained method produces a multi-level hierarchy. Our evaluation against a 3-level gold standard shows that such a hierarchy is fairly accurate. Finally, the results from our qualitative evaluation show that both constrained and unconstrained versions of HGFC are capable of learning valuable novel information not included in the gold standards. The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). However, some works have employed hierarchical clustering as a method to infer flat clustering. For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs. This gave better results than random initialization. Stevenson and Joanis (2003) used AGG for flat clustering on T1. They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cut-off. Our evaluation in the previous</context>
</contexts>
<marker>Sun, Korhonen, Krymolowski, 2008</marker>
<rawString>Lin Sun, Anna Korhonen, and Yuval Krymolowski. Verb class discovery from rich syntactic data. Lecture Notes in Computer Science, 4919:16, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Swier</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised semantic role labelling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="1971" citStr="Swier and Stevenson, 2004" startWordPosition="293" endWordPosition="296">; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). The best of such a</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>Robert Swier and Suzanne Stevenson. Unsupervised semantic role labelling. In Proceedings of EMNLP, pages 95–102, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Hal Daum´e</author>
<author>Daniel Roy</author>
</authors>
<title>Bayesian agglomerative clustering with coalescents.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>20</volume>
<marker>Teh, Daum´e, Roy, 2008</marker>
<rawString>Yee Whye Teh, Hal Daum´e III, and Daniel Roy. Bayesian agglomerative clustering with coalescents. In Advances in Neural Information Processing Systems, volume 20, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Ushioda</author>
</authors>
<title>Hierarchical clustering of words.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics-Volume 2,</booktitle>
<pages>1159--1162</pages>
<contexts>
<context position="3552" citStr="Ushioda, 1996" startWordPosition="547" endWordPosition="548">tyle clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clus1023 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’s cluster membership at any level until a full graph is available, minimising the problem of error propagation) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons (Yu et al., 2006). The method has been applied to the identification of social networ</context>
</contexts>
<marker>Ushioda, 1996</marker>
<rawString>Akira Ushioda. Hierarchical clustering of words. In Proceedings of the 16th conference on Computational linguistics-Volume 2, pages 1159–1162. Association for Computational Linguistics, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gloria V´azquez</author>
<author>Ana Fern´andez-Montraveta</author>
<author>M Ant`onia Martf</author>
</authors>
<title>Clasificaci´on verbal:(alternancias de di´atesis). Universitat de Lleida,</title>
<date>2000</date>
<pages>8484090671</pages>
<marker>V´azquez, Fern´andez-Montraveta, Martf, 2000</marker>
<rawString>Gloria V´azquez, Ana Fern´andez-Montraveta, and M. Ant`onia Martf. Clasificaci´on verbal:(alternancias de di´atesis). Universitat de Lleida, 2000. ISBN 8484090671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Xuan Vinh</author>
<author>Julien Epps</author>
<author>James Bailey</author>
</authors>
<title>Information theoretic measures for clusterings comparison: is a correction for chance necessary? In</title>
<date>2009</date>
<journal>ACM. ISBN</journal>
<booktitle>ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>1073--1080</pages>
<location>New York, NY, USA,</location>
<contexts>
<context position="27256" citStr="Vinh et al., 2009" startWordPosition="4678" endWordPosition="4681">the basis of soft constraints set at the bottom level, as described earlier in section 3.2.4. As a consequence, NMI and F are both greater than 90% at the bottom level and the results at the top level are notably lower because the impact of the constraints degrades the further away one moves from the bottom level. Yet, the relatively high result across all levels shows that the constrained version of HGFC can be employed a useful method to extend the hierarchical structure of known classifications. 1NMI is higher on T2, however, because NMI has a higher baseline for larger number of clusters (Vinh et al., 2009). NMI is not ideal for comparing the results of T2 and T3. 1029 T2 T3 N, Nl HGFC N, Nl HGFC 148 133 53.26 64 32 54.91 97 117 49.85 35 32 50.83 46 51 33.55 20 14 44.02 19 51 25.80 10 14 34.41 9 51 19.17 6 11 32.27 3 51 13.06 Table 4: NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically. Finally, Table 4 shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not pre-defined but inferred automatically as described in section 3.2.3. 6 levels are learned for T2 and 5 for T3. The number of clusters produced ranges from 3 to 148 for T2 and fr</context>
</contexts>
<marker>Vinh, Epps, Bailey, 2009</marker>
<rawString>Nguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings comparison: is a correction for chance necessary? In ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning, pages 1073–1080, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and constrained dirichlet process mixture models for verb clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>74--82</pages>
<contexts>
<context position="2551" citStr="Vlachos et al., 2009" startWordPosition="393" endWordPosition="397">ne translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joani</context>
<context position="4705" citStr="Vlachos et al., 2009" startWordPosition="739" endWordPosition="742">. The method has been applied to the identification of social network communities (Lin et al., 2008), but has not been used (to the best of our knowledge) in NLP before. We modify HGFC with a new tree extraction algorithm which ensures a more consistent result, and we propose two novel extensions to it. The first is a method for automatically determining the tree structure (i.e. number of clusters to be produced for each level of the hierarchy). This avoids the need to predetermine the number of clusters manually. The second is addition of soft constraints to guide the clustering performance (Vlachos et al., 2009). This is useful for situations where a partial (e.g. a flat) verb classification is available and the goal is to extend it. Adopting a set of lexical and syntactic features which have performed well in previous works, we compare the performance of the two methods on test sets extracted from Levin and VerbNet. When evaluated on a flat clustering task, HGFC outperforms AGG and performs very similarly with the best flat clustering method reported on the same test set (Sun and Korhonen, 2009). When evaluated on a hierarchical task, HGFC performs considerably better than AGG at all levels of gold </context>
<context position="20339" citStr="Vlachos et al. (2009)" startWordPosition="3438" endWordPosition="3441">r levels, which are pruned in the tree extraction. 3.2.4 Adding constraints to HGFC The basic version of HGFC makes no prior assumptions about the classification. It is useful for learning novel verb classifications from scratch. However, when wishing to extend an existing classification (e.g. VerbNet) it may be desirable to guide the clustering performance on the basis of information that is already known. We propose a constrained version of HGFC which makes uses of labels at the bottom level to learn upper level classifications. We do this by adding soft constraints to clustering, following Vlachos et al. (2009). We modify the similarity matrix W as follows: If two verbs have different labels (li =� lj), the similarity between them is decreased by a factor a, and a &lt; 1. We set a to 0.5 in the experiments. The resulting tree is generally consistent with the original classification. The influence of the underlying data (domain or features) is reduced according to a. 4 Experimental evaluation We applied the clustering methods introduced in section 3 to the test sets described in section 2 and evaluated them both quantitatively and qualitatively, as described in the subsequent sections. 4.1 Evaluation me</context>
<context position="32756" citStr="Vlachos et al., 2009" startWordPosition="5592" endWordPosition="5595">multi-level hierarchy. Our evaluation against a 3-level gold standard shows that such a hierarchy is fairly accurate. Finally, the results from our qualitative evaluation show that both constrained and unconstrained versions of HGFC are capable of learning valuable novel information not included in the gold standards. The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). However, some works have employed hierarchical clustering as a method to infer flat clustering. For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs. This gave better results than random initialization. Stevenson and Joanis (2003) used AGG for flat clustering on T1. They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cut-off. Our evaluation in the previous section shows that HGFC outperforms their implementation of AGG. AGG was also used by Ferrer (2004</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. Unsupervised and constrained dirichlet process mixture models for verb clustering. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 74–82, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joe H Ward Jr</author>
</authors>
<title>Hierarchical grouping to optimize an objective function.</title>
<date>1963</date>
<journal>Journal of the American statistical association,</journal>
<volume>58</volume>
<issue>301</issue>
<pages>0162--1459</pages>
<marker>Jr, 1963</marker>
<rawString>Joe H. Ward Jr. Hierarchical grouping to optimize an objective function. Journal of the American statistical association, 58(301):236–244, 1963. ISSN 0162-1459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenyu Wu</author>
<author>Richard Leahy</author>
</authors>
<title>An optimal graph theoretic approach to data clustering: Theory and its application to image segmentation. IEEE transactions on pattern analysis and machine intelligence,</title>
<date>1993</date>
<pages>1101--1113</pages>
<contexts>
<context position="10189" citStr="Wu and Leahy, 1993" startWordPosition="1623" endWordPosition="1626"> We adopt for our exper- age criteria: Single, Average, Complete and Ward’s iments a set of features which have performed well (Ward Jr., 1963). Ward’s criterion performed the in recent verb clustering works: best and was used in all the experiments in this paA: Subcategorization frames (SCFs) and their rela- per. It measures the increase in variance after two tive frequencies with individual verbs. clusters are merged. The output of AGG tends to B: A with SCFs parameterized for prepositions. have excessive number of levels. Cut-based methC: B with SCFs parameterized for subjects appear- ods (Wu and Leahy, 1993; Shi and Malik, 2000) are ing in grammatical relations associated with the frequently applied to extract a simplified view. We verb in parsed data. followed previous verb clustering works and cut the D: B with SCFs parameterized for objects appear- AGG hierarchy manually. ing in grammatical relations associated with the AGG suffers from two problems. The first is erverb in parsed data. ror propagation. When a verb is misclassified at a These features are purely syntactic. Although lower level, the error propagates to all the upper levsemantic features – verb selectional preferences – els. The</context>
</contexts>
<marker>Wu, Leahy, 1993</marker>
<rawString>Zhenyu Wu and Richard Leahy. An optimal graph theoretic approach to data clustering: Theory and its application to image segmentation. IEEE transactions on pattern analysis and machine intelligence, pages 1101–1113, 1993. ISSN 0162-8828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Yu</author>
<author>Shipeng Yu</author>
<author>Volker Tresp</author>
</authors>
<title>Soft clustering on graphs.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>18--1553</pages>
<contexts>
<context position="3713" citStr="Yu et al., 2006" startWordPosition="570" endWordPosition="573"> flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clus1023 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’s cluster membership at any level until a full graph is available, minimising the problem of error propagation) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons (Yu et al., 2006). The method has been applied to the identification of social network communities (Lin et al., 2008), but has not been used (to the best of our knowledge) in NLP before. We modify HGFC with a new tree extraction algorithm which e</context>
<context position="12531" citStr="Yu et al., 2006" startWordPosition="2005" endWordPosition="2008">-wise clustering. In the above exical relations, assigns each occurrence of a verb as ample, only the clusters 9.{1,2} and 9.3 are consida member of one of the 168 verbal SCFs. We pa- ered, while alternative clusters 9.{1,3} and 9.2 are rameterized the SCFs as described above using the ignored. Ideally, information about all the possible information provided by the system. intermediate clusters should be aggregated, but this 1025 is intractable in practice. 3.2.2 Hierarchical Graph Factorization Clustering Our new method HGFC derives a probabilistic bipartite graph from the similarity matrix (Yu et al., 2006). The local and global clustering structures are learned via the random walk properties of the graph. The method does not suffer from the above problems with AGG. Firstly, there is no error propagation because the decision on a verb’s membership at any level is delayed until the full bipartite graph is available and until a tree structure can be extracted from it by aggregating probabilistic information from all the levels. Secondly, the bipartite graph enables the construction of a hierarchical structure without any intermediate classes. For example, we can group classes 9.{1,2,3} directly in</context>
<context position="14560" citStr="Yu et al. (2006)" startWordPosition="2366" endWordPosition="2369">evel of clustering. A flat clustering algorithm can be induced by computing B. The bipartite graph K also induces a similarity bipbjp (W0) between vi and vj: w0 ij = Pm λp = p=1 (BA−1BT)ij where A = diag(λ1, ..., λm). Therefore, B can be found by approximating the similarity matrix W of G using W0 derived from K. Given a distance function ( between two similarity matrices, B approximates W by minimizing the cost function ((W, BA−1BT). The coupling between B and A is removed by setting H = BA−1: min ((W, HAHT ), s.t. H,Λ We use the divergence distance: ((X, Y ) = Pij(xij log xij yij −xij+yij). Yu et al. (2006) showed that this cost function is non-increasing under the update rule: wij X λphjp s.t. ˜hip = 1 (2) (HAHT )ij wij hiphjp s.t. X λp =Xwij (3) (HAHT )ij p ij wij can be interpreted as the probability of the direct transition between vi and vj: wij = p(vi, vj), when Pij wij = 1. bip can be interpreted as: D = diag(d1, ..., dn) where di = p(up, uq) is the similarity between the clusters. It takes into account of a weighted average of contributions from all the data. This is different from the linkage method where only the data from two clusters are considered. Given the cluster similarity p(up,</context>
<context position="15884" citStr="Yu et al. (2006)" startWordPosition="2623" endWordPosition="2626">an be applied again (Figure 1(e)). This process can go on iteratively, leading to a hierarchical graph. Algorithm 1 HGFC algorithm (Yu et al., 2006) Require: N verbs V , number of clusters ml for L levels Compute the similarity matrix W0 from V Build the graph G0 from W0 , and m0 ← n for l = 1,2 to L do Factorize Gl−1 to obtain bipartite graph Kl with the adjacency matrix Bl (eq. 1, 2 and 3) Build a graph Gl with similarity matrix Wl = BTl D−1 l Bl according to equation 4 end for return BL, BL−1...B1 Additional steps need to be performed in order to extract a tree from the hierarchical graph. Yu et al. (2006) performs the extraction via a propagation of probabilities from the bottom level clusters. For a verb vi, the probability of assigning it to clusterv(l) p at level l is given by: Xn hip = 1 (1) i=1 X˜hip oc hip j i X˜λp oc λp j p(up,uq) = p(up)p(up|uq) = = (BT D−1B)pq (4) Xn i=1 bipbiq di bip m X p=0 1026 V2 V6 V4 V3 V1 V V9 V7 V8 V, V2 V3 V4 V V6 V7 V8 V9 U, U2 U3 V6 V2 V4 U , V3 U3 V, V V9 V7 U2 V8 U, U2 U3 v1 v2 v3 v4 v v6 v7 v8 v9 u1 u2 u3 q1 q2 (a) (b) (c) (d) (e) Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; </context>
<context position="18473" citStr="Yu et al., 2006" startWordPosition="3116" endWordPosition="3119">rs for HGFC HGFC needs the number of levels and clusters at each level as input. However, this information is not always available (e.g. when the goal is to actually learn this information automatically). We therefore propose a method for inferring the cluster structure from data. As shown in figure 1, a similarity matrix W models one-hop transitions that follow the links from vertices to neighbors. A walker can also go to other vertices via multi-hop transitions. According to the chain rule of the Markov process, the multi-hop transitions indicate a decaying similarity function on the graph (Yu et al., 2006). After t transitions, the similarity matrix (Wt) becomes: Wt = Wt−1D−1 0 W0 Yu et al. (2006) proved the correspondence between the HGFC levels (l) and the random walk time: t = 2l−1. So the vertices at level l induce a similarity matrix of verbs after t-hop transitions. The decaying similarity function captures the different scales of clustering structure in the data (Azran and Ghahramani, 2006b). The upper levels would have a smaller number of clusters which represent a more global structure. After several levels, all the verbs are expected to be grouped into one cluster. The number of level</context>
</contexts>
<marker>Yu, Yu, Tresp, 2006</marker>
<rawString>Kai Yu, Shipeng Yu, and Volker Tresp. Soft clustering on graphs. Advances in Neural Information Processing Systems, 18:1553, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Be˜nat Zapirain</author>
</authors>
<title>Eneko Agirre, and Llufs M`arquez. Robustness and generalization of role sets: PropBank vs. VerbNet.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>550--558</pages>
<marker>Zapirain, 2008</marker>
<rawString>Be˜nat Zapirain, Eneko Agirre, and Llufs M`arquez. Robustness and generalization of role sets: PropBank vs. VerbNet. In Proceedings ofACL-08: HLT, pages 550– 558, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>