<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.995553">
Soft-Supervised Learning for Text Classification
</title>
<author confidence="0.999174">
Amarnag Subramanya &amp; Jeff Bilmes
</author>
<affiliation confidence="0.9987875">
Dept. of Electrical Engineering,
University of Washington, Seattle, WA 98195, USA.
</affiliation>
<email confidence="0.998523">
{asubram,bilmes}@ee.washington.edu
</email>
<sectionHeader confidence="0.996656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917235294118">
We propose a new graph-based semi-
supervised learning (SSL) algorithm and
demonstrate its application to document
categorization. Each document is represented
by a vertex within a weighted undirected
graph and our proposed framework minimizes
the weighted Kullback-Leibler divergence
between distributions that encode the class
membership probabilities of each vertex. The
proposed objective is convex with guaranteed
convergence using an alternating minimiza-
tion procedure. Further, it generalizes in
a straightforward manner to multi-class
problems. We present results on two stan-
dard tasks, namely Reuters-21578 and
WebKB, showing that the proposed algorithm
significantly outperforms the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999795510638298">
Semi-supervised learning (SSL) employs small
amounts of labeled data with relatively large
amounts of unlabeled data to train classifiers. In
many problems, such as speech recognition, doc-
ument classification, and sentiment recognition,
annotating training data is both time-consuming
and tedious, while unlabeled data are easily ob-
tained thus making these problems useful appli-
cations of SSL. Classic examples of SSL algo-
rithms include self-training (Yarowsky, 1995) and
co-training (Blum and Mitchell, 1998). Graph-
based SSL algorithms are an important class of SSL
techniques that have attracted much of attention of
late (Blum and Chawla, 2001; Zhu et al., 2003).
Here one assumes that the data (both labeled and
unlabeled) is embedded within a low-dimensional
manifold expressed by a graph. In other words,
each data sample is represented by a vertex within
a weighted graph with the weights providing a mea-
sure of similarity between vertices.
Most graph-based SSL algorithms fall under one
of two categories – those that use the graph structure
to spread labels from labeled to unlabeled samples
(Szummer and Jaakkola, 2001; Zhu and Ghahra-
mani, 2002) and those that optimize a loss function
based on smoothness constraints derived from the
graph (Blum and Chawla, 2001; Zhu et al., 2003;
Joachims, 2003; Belkin et al., 2005). Sometimes the
two categories are similar in that they can be shown
to optimize the same underlying objective (Zhu and
Ghahramani, 2002; Zhu et al., 2003). In general
graph-based SSL algorithms are non-parametric and
transductive.1 A learning algorithm is said to be
transductive if it is expected to work only on a closed
data set, where a test set is revealed at the time of
training. In practice, however, transductive learners
can be modified to handle unseen data (Zhu, 2005a;
Sindhwani et al., 2005). A common drawback of
many graph-based SSL algorithms (e.g. (Blum and
Chawla, 2001; Joachims, 2003; Belkin et al., 2005))
is that they assume binary classification tasks and
thus require the use of sub-optimal (and often com-
putationally expensive) approaches such as one vs.
rest to solve multi-class problems, let alone struc-
tured domains such as strings and trees. There are
also issues related to degenerate solutions (all un-
labeled samples classified as belonging to a single
</bodyText>
<footnote confidence="0.819548">
1Excluding Manifold Regularization (Belkin et al., 2005).
</footnote>
<page confidence="0.853068">
1090
</page>
<note confidence="0.8960645">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090–1099,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.99511675">
class) (Blum and Chawla, 2001; Joachims, 2003;
Zhu and Ghahramani, 2002). For more background
on graph-based and general SSL and their applica-
tions, see (Zhu, 2005a; Chapelle et al., 2007; Blitzer
and Zhu, 2008).
In this paper we propose a new algorithm for
graph-based SSL and use the task of text classifica-
tion to demonstrate its benefits over the current state-
of-the-art. Text classification involves automatically
assigning a given document to a fixed number of se-
mantic categories. Each document may belong to
one, many, or none of the categories. In general,
text classification is a multi-class problem (more
than 2 categories). Training fully-supervised text
classifiers requires large amounts of labeled data
whose annotation can be expensive (Dumais et al.,
1998). As a result there has been interest is us-
ing SSL techniques for text classification (Joachims,
1999; Joachims, 2003). However past work in semi-
supervised text classification has relied primarily on
one vs. rest approaches to overcome the inherent
multi-class nature of this problem. We believe such
an approach may be sub-optimal because, disregard-
ing data overlap, the different classifiers have train-
ing procedures that are independent of one other.
In order to address the above drawback we pro-
pose a new framework based on optimizing a loss
function composed of Kullback-Leibler divergence
(KL-divergence) (Cover and Thomas, 1991) terms
between probability distributions defined for each
graph vertex. The use of probability distributions,
rather than fixed integer labels, not only leads to a
straightforward multi-class generalization, but also
allows us to exploit other well-defined functions of
distributions, such as entropy, to improve system
performance and to allow for the measure of uncer-
tainty. For example, with a single integer, at most all
we know is its assignment. With a distribution, we
can continuously move from knowing an assignment
with certainty (i.e., an entropy of zero) to expres-
sions of doubt or multiple valid possibilities (i.e., an
entropy greater than zero). This is particularly use-
ful for document classification as we will see. We
also show how one can use the alternating minimiza-
tion (Csiszar and Tusnady, 1984) algorithm to op-
timize our objective leading to a relatively simple,
fast, easy-to-implement, guaranteed to converge, it-
erative, and closed form update for each iteration.
</bodyText>
<sectionHeader confidence="0.7663815" genericHeader="method">
2 Proposed Graph-Based Learning
Framework
</sectionHeader>
<bodyText confidence="0.999030611111111">
We consider the transductive learning problem, i.e.,
given a training set D = {Dl, Du}, where Dl and Du
are the sets of labeled and unlabeled samples respec-
tively, the task is to infer the labels for the samples
in Du. In other words, Du is the “test-set.” Here
Dl = {(xi, yi)}li=1, Du = {xi}l+u
i=l+1, xi ∈ X (the
input space of the classifier, and corresponds to vec-
tors of features) and yi ∈ Y (the space of classifier
outputs, and for our case is the space of non-negative
integers). Thus |Y |= 2 yields binary classifica-
tion while |Y |&gt; 2 yields multi-class. We define
n = l + u, the total number of samples in the train-
ing set. Given D, most graph-based SSL algorithms
utilize an undirected weighted graph G = (V, E)
where V = {1, ... , n} are the data points in D
and E = V × V are the set of undirected edges
between vertices. We use wij ∈ W to denote the
weight of the edge between vertices i and j. W is
referred to as the weight (or affinity) matrix of G.
As will be seen shortly, the input features xi effect
the final classification results via W, i.e., the graph.
Thus graph construction is crucial to the success of
any graph-based SSL algorithm. Graph construction
“is more of an art, than science” (Zhu, 2005b) and
is an active research area (Alexandrescu and Kirch-
hoff, 2007). In general the weights are formed as
wij = sim(xi, xj)S(j ∈ K(i)). Here K(i) is the set
of i’s k-nearest-neighbors (KNN), sim(xi, xj) is a
given measure of similarity between xi and xj, and
S(c) returns a 1 if c is true and 0 otherwise. Getting
the similarity measure right is crucial for the success
of any SSL algorithm as that is what determines the
graph. Note that setting K(i) = |V  |= n results
in a fully-connected graph. Some popular similarity
measures include
</bodyText>
<equation confidence="0.919747">
sim(xi, xj) = e
sim(xi, xj) = cos(xi, xj) = hxi,xji
k xi k22k xj k2 2
</equation>
<bodyText confidence="0.999506166666667">
where k xi k2 is the L2 norm, and hxi, xji is the
inner product of xi and xj. The first similarity mea-
sure is an RBF kernel applied on the squared Eu-
clidean distance while the second is cosine similar-
ity. In this paper all graphs are constructed using
cosine similarity.
</bodyText>
<equation confidence="0.8148855">
������ �2 2
02 or
</equation>
<page confidence="0.95331">
1091
</page>
<bodyText confidence="0.999711791666667">
We next introduce our proposed approach. For
every i E V , we define a probability distribution pi
over the elements of Y. In addition let rj, j = 1... l
be another set of probability distributions again over
the elements of Y (recall, Y is the space of classi-
fier outputs). Here {rj}j represents the labels of the
supervised portion of the training data. If the label
for a given labeled data point consists only of a sin-
gle integer, then the entropy of the corresponding rj
is zero (the probability of that integer will be unity,
with the remaining probabilities being zero). If, on
the other hand, the “label” for a given labeled data
point consists of a set of integers (e.g., if the object
is a member of multiple classes), then rj is able to
represent this property accordingly (see below). We
emphasize again that both pi and rj are probability
distributions, with rj fixed throughout training. The
goal of learning in this paper is to find the best set
of distributions pi, Vi that attempt to: 1) agree with
the labeled data rj wherever it is available; 2) agree
with each other (when they are close according to a
graph); and 3) be smooth in some way. These cri-
teria are captured in the following new multi-class
SSL optimization procedure:
</bodyText>
<equation confidence="0.9984954">
l
C1 (p), where C1 (p) = f 1: DKL (ri J Jpi)
i=1
�H(pi) �,
(1)
</equation>
<bodyText confidence="0.986461642857143">
and where p °_ (p1, ... , pn) denotes the en-
tire set of distributions to be learned, H(pi) =
− Ey pi(y) log pi(y) is the standard Shannon en-
tropy function of pi, DKL(piJJqj) is the KL-
divergence between pi and qj, and µ and ν are hy-
perparameters whose selection we discuss in section
5. The distributions ri are derived from Dl (as men-
tioned above) and this can be done in one of the fol-
lowing ways: (a) if yi is the single supervised label
for input xi then ri(y) = δ(y = yi), which means
that ri gives unity probability for y equaling the la-
bel yi; (b) if yi = ��y(1)
i , ... ,�y(�)
i I, k &lt; JYJ is a set
of possible outputs for input xi, meaning an object
validly falls into all of the corresponding categories,
we set ri(y) = (1/k)δ(y E yi) meaning that ri is
uniform over only the possible categories and zero
otherwise; (c) if the labels are somehow provided
in the form of a set of non-negative scores, or even
a probability distribution itself, we just set ri to be
equal to those scores (possibly) normalized to be-
come a valid probability distribution. Among these
three cases, case (b) is particularly relevant to text
classification as a given document many belong to
(and in practice may be labeled as) many classes.
The final classification results, i.e., the final labels
for D,,, are then given by y� = argmax pi(y).
</bodyText>
<equation confidence="0.378301">
yEY
</equation>
<bodyText confidence="0.99995454054054">
We next provide further intuition on our objective
function. SSL on a graph consists of finding a la-
beling D,, that is consistent with both the labels pro-
vided in Dl and the geometry of the data induced
by the graph. The first term of C1 will penalize
the solution pi i E 11, ... ,l1, when it is far away
from the labeled training data Dl, but it does not in-
sist that pi = ri, as allowing for deviations from ri
can help especially with noisy labels (Bengio et al.,
2007) or when the graph is extremely dense in cer-
tain regions. As explained above, our framework al-
lows for the case where supervised training is uncer-
tain or ambiguous. We consider it reasonable to call
our approach soft-supervised learning, generalizing
the notion of semi-supervised learning, since there
is even more of a continuum here between fully su-
pervised and fully unsupervised learning than what
typically exists with SSL. Soft-supervised learning
allows uncertainty to be expressed (via a probability
distribution) about any of the labels individually.
The second term of C1 penalizes a lack of con-
sistency with the geometry of the data and can be
seen as a graph regularizer. If wij is large, we prefer
a solution in which pi and pj are close in the KL-
divergence sense. While KL-divergence is asym-
metric, given that g is undirected implies W is sym-
metric (wij = wji) and as a result the second term
is inherently symmetric.
The last term encourages each pi to be close to
the uniform distribution if not preferred to the con-
trary by the first two terms. This acts as a guard
against degenerate solutions commonly encountered
in SSL (Blum and Chawla, 2001; Joachims, 2003).
For example, consider the case where part of the
graph is almost completely disconnected from any
labeled vertex (which is possible in the k-nearest
neighbor case). In such situations the third term en-
</bodyText>
<equation confidence="0.756991166666667">
min
p
+µ
n n
� wijDKL (piJJpj) − ν�
i j i=1
</equation>
<page confidence="0.932">
1092
</page>
<bodyText confidence="0.999995870967742">
sures that the nodes in this disconnected region are
encouraged to yield a uniform distribution, validly
expressing the fact that we do not know the labels of
these nodes based on the nature of the graph. More
generally, we conjecture that by maximizing the en-
tropy of each pi, the classifier has a better chance of
producing high entropy results in graph regions of
low confidence (e.g. close to the decision boundary
and/or low density regions). This overcomes a com-
mon drawback of a large number of state-of-the-art
classifiers that tend to be confident even in regions
close to the decision boundary.
We conclude this section by summarizing some of
the features of our proposed framework. It should
be clear that C1 uses the “manifold assumption”
for SSL (see chapter 2 in (Chapelle et al., 2007))
— it assumes that the input data can be embed-
ded within a low-dimensional manifold (the graph).
As the objective is defined in terms of probability
distributions over integers rather than just integers
(or to real-valued relaxations of integers (Joachims,
2003; Zhu et al., 2003)), the framework general-
izes in a straightforward manner to multi-class prob-
lems. Further, all the parameters are estimated
jointly (compare to one vs. rest approaches which
involve solving |Y |independent problems). Fur-
thermore, the objective is capable of handling label
training data uncertainty (Pearl, 1990). Of course,
this objective would be useless if it wasn’t possible
to efficiently and easily optimize it on large data sets.
We next describe a method that can do this.
</bodyText>
<sectionHeader confidence="0.599328" genericHeader="method">
3 Learning with Alternating Minimization
</sectionHeader>
<bodyText confidence="0.9980186">
As long as µ, ν &gt; 0, the objective C1(p) is con-
vex. This follows since Dxz(pi||pj) is convex in
the pair (pi,pj) (Cover and Thomas, 1991), nega-
tive entropy is convex, and a positive-weighted lin-
ear combination of a set of convex functions is con-
vex. Thus, the problem of minimizing C1 over the
space of collections of probability distributions (a
convex set) constitutes a convex programming prob-
lem (Bertsekas, 2004). This property is extremely
beneficial since there is a unique global optimum
and there are a variety of methods that can be used
to yield that global optimum. One possible method
might take the derivative of the objective along with
Lagrange multipliers to ensure that we stay within
the space of probability distributions. This method
can sometimes yield a closed form single-step an-
alytical expression for the globally optimum solu-
tion. Unfortunately, however, our problem does not
admit such a closed form solution because the gra-
dient of C1(p) with respect to pi(y) is of the form,
k1pi(y) log pi(y) + k2pi(y) + k3 (where k1, k2, k3
are fixed constants). Sometimes, optimizing the dual
of the objective can also produce a solution, but un-
fortunately again the dual of our objective also does
not yield a closed form solution. The typical next
step, then, is to resort to iterative techniques such
as gradient descent along with modifications to en-
sure that the solution stays within the set of proba-
bility distributions (the gradient of C1 alone will not
necessarily point in the direction where p is still a
valid distribution) - one such modification is called
the method of multipliers (MOM). Another solu-
tion would be to use computationally complex (and
complicated) algorithms like interior point methods
(IPM). While all of the above methods (described
in detail in (Bertsekas, 2004)) are feasible ways to
solve our problem, they each have their own draw-
backs. Using MOM, for example, requires the care-
ful tuning of a number of additional parameters such
as learning rates, growth factors, and so on. IPM in-
volves inverting a matrix of the order of the number
of variables and constraints during each iteration.
We instead adopt a different strategy based on al-
ternating minimization (Csiszar and Tusnady, 1984).
This approach has a single additional optimization
parameter (contrasted with MOM), admits a closed
form solution for each iteration not involving any
matrix inversion (contrasted with IPM), and yields
guaranteed convergence to the global optimum. In
order to render our approach amenable to AM, how-
ever, we relax our objective C1 by defining a new
(third) set of distributions for all training samples qi,
i = 1, ... , n denoted collectively like the above us-
ing the notation q °_ (q1, ... , qn). We define a new
objective to be optimized as follows:
</bodyText>
<equation confidence="0.980555875">
&amp;quot; l
C2 (p, q) , where C2 (p, q) =XDxz (ri   ||qi )
i=1
min
p,q
+µ
Xn X w� � − ν Xn ⎤H(pi) ⎦ .
i=1 jEAr(i) ijDxz�pi||qj i=1
</equation>
<page confidence="0.821799">
1093
</page>
<bodyText confidence="0.99999392">
Before going further, the reader may be wondering
at this juncture how might it be desirable for us to
have apparently complicated the objective function
in an attempt to yield a more computationally and
methodologically superior machine learning proce-
dure. This is indeed the case as will be spelled out
below. First, in C2 we have defined a new weight
matrix [W0]ij = w0ij of the same size as the original
where W0 = W + αIn, where In is the n x n iden-
tity matrix, and where α &gt; 0 is a non-negative con-
stant (this is the optimization related parameter men-
tioned above). This has the effect that w0ii &gt; wii.
In the original objective C1, wii is irrelevant since
DKL(pJJp) = 0 for all p, but since there are now two
distributions for each training point, there should be
encouragement for the two to approach each other.
Like C1, the first term of C2 ensures that the la-
beled training data is respected and the last term is
a smoothness regularizer, but these are done via dif-
ferent sets of distributions, q and p respectively —
this choice is what makes possible the relatively sim-
ple analytical update equations given below. Next,
we see that the two objective functions in fact have
identical solutions when the optimization enforces
the constraint that p and q are equal:
</bodyText>
<equation confidence="0.914831666666667">
(p,q):p=q
min C2(p, q) = min C1(p).
p
</equation>
<bodyText confidence="0.9823355">
Indeed, as α gets large, the solutions considered vi-
able are those only where p = q. We thus have that:
</bodyText>
<equation confidence="0.8608725">
C2(p, q) = min C1(p).
p
</equation>
<bodyText confidence="0.918206818181818">
Therefore, the two objectives should yield the same
solution as long as α &gt; wij for all i, j. A key advan-
tage of this relaxed objective is that it is amenable to
alternating minimization, a method to produce a se-
quence of sets of distributions (pn, qn) as follows:
pn = argmin C2(p, qn−1), qn = argmin C2(pn, q).
p q
It can be shown (we omit the rather lengthy proof
due to space constraints) that the sequence gener-
ated using the above minimizations converges to the
minimum of C2(p, q), i.e.,
</bodyText>
<equation confidence="0.746387">
n→∞
lim C2(p(n), q(n)) = inf C2(p, q),
p,q
</equation>
<bodyText confidence="0.999436333333333">
provided we start with a distribution that is initial-
ized properly q(0)(y) &gt; 0 b y E Y. The update
equations for p(n) and q(n) are given by
</bodyText>
<equation confidence="0.9979525">
1 3�n−1)(y)
pin) (y) = Z exp yz
Z ,
(n) ri(y)δ(i &lt; l) + µPj w�jipjn)(y)
qi (y) = δ(i &lt; l) + µ P ,
j w� ji
</equation>
<bodyText confidence="0.701263">
where
</bodyText>
<equation confidence="0.9873436">
Xγi = ν + µ
j
(y) = −ν + µ X wij(log q(n−1)
0 j (y) − 1)
j
</equation>
<bodyText confidence="0.999982857142857">
and where Zi is a normalizing constant to ensure pi
is a valid probability distribution. Note that each it-
eration of the proposed framework has a closed form
solution and is relatively simple to implement, even
for very large graphs. Henceforth we refer to the
proposed objective optimized using alternating min-
imization as AM.
</bodyText>
<sectionHeader confidence="0.967492" genericHeader="method">
4 Connections to Other Approaches
</sectionHeader>
<bodyText confidence="0.999867166666667">
Label propagation (LP) (Zhu and Ghahramani,
2002) is a graph-based SSL algorithms that per-
forms Markov random walks on the graph and has
a straightforward extension to multi-class problems.
The update equations for LP (which also we use for
our LP implementations) may be written as
</bodyText>
<equation confidence="0.9890475">
(n) ri(y)b(i &lt; l) + δ(i &gt; l) Pj wijp�n 1)(y)
pi (y) = δ(i &lt; l) + δ(i &gt; l) Pj wij
</equation>
<bodyText confidence="0.997843416666667">
Note the similarity to the update equation for q(n)
i in
our AM case. It has been shown that the squared-
loss based SSL algorithm (Zhu et al., 2003) and LP
have similar updates (Bengio et al., 2007).
The proposed objective C1 is similar in spirit to
the squared-loss based objective in (Zhu et al., 2003;
Bengio et al., 2007). Our method, however, differs
in that we are optimizing the KL-divergence over
probability distributions. We show in section 5 that
KL-divergence based loss significantly outperforms
the squared-loss. We believe that this could be due
</bodyText>
<figure confidence="0.9233315">
lim min
α→∞ p,q
0
wij,
</figure>
<page confidence="0.984173">
1094
</page>
<bodyText confidence="0.999807554054054">
to the following: 1) squared loss is appropriate un-
der a Gaussian loss model which may not be opti-
mal under many circumstances (e.g. classification);
2) KL-divergence DKL(p||q) is based on a relative
(relative to p) rather than an absolute error; and 3)
under certain natural assumptions, KL-divergence is
asymptotically consistent with respect to the under-
lying probability distributions.
AM is also similar to the spectral graph trans-
ducer (Joachims, 2003) in that they both attempt
to find labellings over the unlabeled data that re-
spect the smoothness constraints of the graph. While
spectral graph transduction is an approximate solu-
tion to a discrete optimization problem (which is NP
hard), AM is an exact solution obtained by optimiz-
ing a convex function over a continuous space. Fur-
ther, while spectral graph transduction assumes bi-
nary classification problems, AM naturally extends
to multi-class situations without loss of convexity.
Entropy Minimization (EnM) (Grandvalet and
Bengio, 2004) uses the entropy of the unlabeled data
as a regularizer while optimizing a parametric loss
function defined over the labeled data. While the
objectives in the case of both AM and EnM make
use of the entropy of the unlabeled data, there are
several important differences: (a) EnM is not graph-
based, (b) EnM is parametric whereas our proposed
approach is non-parametric, and most importantly,
(c) EnM attempts to minimize entropy while the pro-
posed approach aims to maximize entropy. While
this may seem a triviality, it has catastrophic conse-
quences in terms of both the mathematics and mean-
ing. The objective in case of EnM is not convex,
whereas in our case we have a convex formulation
with simple update equations and convergence guar-
antees.
(Wang et al., 2008) is a graph-based SSL al-
gorithm that also employs alternating minimiza-
tion style optimization. However, it is inherently
squared-loss based which our proposed approach
out-performs (see section 5). Further, they do not
provide or state convergence guarantees and one
side of their update approximates an NP-complete
optimization procedure.
The information regularization (IR) (Corduneanu
and Jaakkola, 2003) algorithm also makes use of
a KL-divergence based loss for SSL. Here the in-
put space is divided into regions {Ri} which might
or might not overlap. For a given point xi E Ri,
IR attempts to minimize the KL-divergence between
pi(yi|xi) and PR,(y), the agglomerative distribution
for region Ri. Given a graph, one can define a re-
gion to be a vertex and its neighbor thus making IR
amenable to graph-based SSL. In (Corduneanu and
Jaakkola, 2003), the agglomeration is performed by
a simple averaging (arithmetic mean). While IR sug-
gests (without proof of convergence) the use of al-
ternating minimization for optimization, one of the
steps of the optimization does not admit a closed-
form solution. This is a serious practical drawback
especially in the case of large data sets. (Tsuda,
2005) (hereafter referred to as PD) is an extension of
the IR algorithm to hypergraphs where the agglom-
eration is performed using the geometric mean. This
leads to closed form solutions in both steps of the al-
ternating minimization. There are several important
differences between IR and PD on one side and our
proposed approach: (a) neither IR nor PD use an
entropy regularizer, and (b) the update equation for
one of the steps of the optimization in the case of
PD (equation 13 in (Tsuda, 2005)) is actually a spe-
cial case of our update equation for pi(y) and may
be obtained by setting wij = 1/2. Further, our work
here may be easily extended to hypergraphs.
</bodyText>
<sectionHeader confidence="0.999921" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999895">
We compare our algorithm (AM) with other
state-of-the-art SSL-based text categorization al-
gorithms, namely, (a) SVM (Joachims, 1999),
</bodyText>
<listItem confidence="0.7786125">
(b) Transductive-SVM (TSVM) (Joachims, 1999),
(c) Spectral Graph Transduction (SGT) (Joachims,
2003), and (d) Label Propagation (LP) (Zhu and
Ghahramani, 2002). Note that only SGT and LP
are graph-based algorithms, while SVM is fully-
supervised (i.e., it does not make use of any of the
unlabeled data). We implemented SVM and TSVM
using SVM Light (Joachims, b) and SGT using SGT
Light (Joachims, a). In the case of SVM, TSVM and
SGT we trained |Y |classifiers (one for each class) in
a one vs. rest manner precisely following (Joachims,
2003).
</listItem>
<subsectionHeader confidence="0.984794">
5.1 Reuters-21578
</subsectionHeader>
<bodyText confidence="0.999382">
We used the “ModApte” split of the Reuters-21578
dataset collected from the Reuters newswire in
</bodyText>
<page confidence="0.98565">
1095
</page>
<bodyText confidence="0.998884645833333">
1987 (Lewis et al., 1987). The corpus has 9,603
training (not to be confused with D) and 3,299 test
documents (which represents Du). Of the 135 poten-
tial topic categories only the 10 most frequent cate-
gories are used (Joachims, 1999). Categories outside
the 10 most frequent were collapsed into one class
and assigned a label “other”. For each document i
in the training and test sets, we extract features xi in
the following manner: stop-words are removed fol-
lowed by the removal of case and information about
inflection (i.e., stemming) (Porter, 1980). We then
compute TFIDF features for each document (Salton
and Buckley, 1987). All graphs were constructed us-
ing cosine similarity with TFIDF features.
For this task Y = { earn, acq, money, grain,
crude, trade, interest, ship, wheat, corn, average}.
For LP and AM, we use the output space Y&apos; = YU{
other }. For documents in Dl that are labeled with
multiple categories, we initialize ri to have equal
non-zero probability for each such category. For
example, if document i is annotated as belonging
to classes { acq, grain, wheat}, then ri(acq) =
ri(grain) = ri(wheat) = 1/3.
We created 21 transduction sets by randomly sam-
pling l documents from the training set with the con-
straint that each of 11 categories (top 10 categories
and the class other) are represented at least once in
each set. These samples constitute Dl. All algo-
rithms used the same transduction sets. In the case
of SGT, LP and AM, the first transduction set was
used to tune the hyperparameters which we then held
fixed for all the remaining 20 transduction sets. For
all the graph-based approaches, we ran a search over
K E {2, 10, 50, 100, 250, 500, 1000, 2000, n} (note
K = n represents a fully connected graph). In addi-
tion, in the case of AM, we set α = 2 for all exper-
iments, and we ran a search over µ E {1e–8, 1e–4,
0.01, 0.1, 1, 10, 100} and v E {1e–8, 1e–6, 1e–4,
0.01, 0.1}, for SGT the search was over c E {3000,
3200, 3400, 3800, 5000, 100000} (see (Joachims,
2003)).
We report precision-recall break even point
(PRBEP) results on the 3,299 test documents in Ta-
ble 1. PRBEP has been a popular measure in infor-
mation retrieval (see e.g. (Raghavan et al., 1989)).
It is defined as that value for which precision and
recall are equal. Results for each category in Ta-
ble 1 were obtained by averaging the PRBEP over
</bodyText>
<table confidence="0.999764166666667">
Category SVM TSVM SGT LP AM
earn 91.3 95.4 90.4 96.3 97.9
acq 67.8 76.6 91.9 90.8 97.2
money 41.3 60.0 65.6 57.1 73.9
grain 56.2 68.5 43.1 33.6 41.3
crude 40.9 83.6 65.9 74.8 55.5
trade 29.5 34.0 36.0 56.0 47.0
interest 35.6 50.8 50.7 47.9 78.0
ship 32.5 46.3 49.0 26.4 39.6
wheat 47.9 44.4 59.1 58.2 64.3
corn 41.3 33.7 51.2 55.9 68.3
average 48.9 59.3 60.3 59.7 66.3
</table>
<tableCaption confidence="0.835186666666667">
Table 1: P/R Break Even Points (PRBEP) for the top
10 categories in the Reuters data set with l = 20 and
u = 3299. All results are averages over 20 randomly
generated transduction sets. The last row is the macro-
average over all the categories. Note AM is the proposed
approach.
</tableCaption>
<bodyText confidence="0.999528217391304">
the 20 transduction sets. The final row “average”
was obtained by macro-averaging (average of av-
erages). The optimal value of the hyperparame-
ters in case of LP was K = 100; in case of AM,
K = 2000, µ = 1e–4, v = 1e–2; and in the case
of SGT, K = 100, c = 3400. The results show
that AM outperforms the state-of-the-art on 6 out of
10 categories and is competitive in 3 of the remain-
ing 4 categories. Further it significantly outperforms
all other approaches in case of the macro-averages.
AM is significant over its best competitor SGT at
the 0.0001 level according to the difference of pro-
portions significance test.
Figure 1 shows the variation of “average” PRBEP
against the number of labeled documents (l). For
each value of l, we tuned the hyperparameters over
the first transduction set and used these values for
all the other 20 sets. Figure 1 also shows error-
bars (f standard deviation) all the experiments. As
expected, the performance of all the approaches
improves with increasing number of labeled docu-
ments. Once again in this case, AM, outperforms
the other approaches for all values of l.
</bodyText>
<subsectionHeader confidence="0.995939">
5.2 WebKB Collection
</subsectionHeader>
<bodyText confidence="0.996981">
World Wide Knowledge Base (WebKB) is a collec-
tion of 8282 web pages obtained from four academic
</bodyText>
<page confidence="0.986881">
1096
</page>
<figure confidence="0.9966625">
0 50 100 150 200 250 300 350 400 450 500
Number of Labeled Documents
</figure>
<figureCaption confidence="0.991835666666667">
Figure 1: Average PRBEP over all classes vs.
number of labeled documents (l) for Reuters data
set
</figureCaption>
<bodyText confidence="0.999973696969697">
domains. The web pages in the WebKB set are la-
beled using two different polychotomies. The first
is according to topic and the second is according to
web domain. In our experiments we only consid-
ered the first polychotomy, which consists of 7 cat-
egories: course, department, faculty, project, staff,
student, and other. Following (Nigam et al., 1998)
we only use documents from categories course, de-
partment, faculty, project which gives 4199 docu-
ments for the four categories. Each of the documents
is in HTML format containing text as well as other
information such as HTML tags, links, etc. We used
both textual and non-textual information to construct
the feature vectors. In this case we did not use ei-
ther stop-word removal or stemming as this has been
found to hurt performance on this task (Nigam et al.,
1998). As in the the case of the Reuters data set
we extracted TFIDF features for each document and
constructed the graph using cosine similarity.
As in (Bekkerman et al., 2003), we created four
roughly-equal random partitions of the data set. In
order to obtain Dl, we first randomly choose a split
and then sample l documents from that split. The
other three splits constitute D, We believe this is
more realistic than sampling the labeled web-pages
from a single university and testing web-pages from
the other universities (Joachims, 1999). This method
of creating transduction sets allows us to better eval-
uate the generalization performance of the various
algorithms. Once again we create 21 transduction
sets and the first set was used to tune the hyperpa-
rameters. Further, we ran a search over the same grid
as used in the case of Reuters. We report precision-
</bodyText>
<figure confidence="0.983832">
0 100 200 300 400 500 600
Number of Labeled Documents
</figure>
<figureCaption confidence="0.984583">
Figure 2: Average PRBEP over all classes vs.
number of labeled documents (l) for WebKB col-
lection.
</figureCaption>
<table confidence="0.999723833333333">
Class SVM TSVM SGT LP AM
course 46.5 43.9 29.9 45.0 67.6
faculty 14.5 31.2 42.9 40.3 42.5
project 15.8 17.2 17.5 27.8 42.3
student 15.0 24.5 56.6 51.8 55.0
average 23.0 29.2 36.8 41.2 51.9
</table>
<tableCaption confidence="0.988593">
Table 2: P/R Break Even Points (PRBEP) for the WebKB
</tableCaption>
<bodyText confidence="0.940278533333333">
data set with l = 48 and u = 3148. All results are aver-
ages over 20 randomly generated transduction sets. The
last row is the macro-average over all the classes. AM is
the proposed approach.
recall break even point (PRBEP) results on the 3,148
test documents in Table 2. For this task, we found
that the optimal value of the hyperparameter were:
in the case of LP, K = 1000; in case of AM,
K = 1000, µ = 1e–2, ν = 1e–4; and in case of
SGT, K = 100, c = 3200. Once again, AM is sig-
nificant at the 0.0001 level over its closest competi-
tor LP. Figure 2 shows the variation of PRBEP with
number of labeled documents (l) and was generated
in a similar fashion as in the case of the Reuters data
set.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.997053">
We note that LP may be cast into an AM-like frame-
work by using the following sequence of updates,
</bodyText>
<figure confidence="0.938324454545454">
pi (y) = δ(i G l)ri(y) + δ(i &gt; l)q(n−1)
(n) i ,
(n) �j wijp(n)(y)
qi (y)
Average PRBEP
85
80
75
70
65
60
55
50
45
AM
SGT
LP
TSVM
SVM
80
70
60
50
40
30
20
AM
SGT
LP
TSVM
SVM
Average PRBEP
Ej wij
</figure>
<page confidence="0.991013">
1097
</page>
<bodyText confidence="0.999984870967742">
To compare the behavior of AM and LP, we ap-
plied this form of LP along with AM on a simple
5-node binary-classification SSL graph where two
nodes are labeled (node 1 and 2) and the remaining
nodes are unlabeled (see Figure 3, top). Since this is
binary classification (IY I = 2), each distribution pi
or qi can be depicted using only a single real num-
ber between 0 and 1 corresponding to the probability
that each vertex is class 2 (yes two). We show how
both LP and AM evolve starting from exactly the
same random starting point qO (Figure 3, bottom).
For each algorithm, the figure shows that both algo-
rithms clearly converge. Each alternate iteration of
LP is such that the labeled vertices oscillate due to
its clamping back to the labeled distribution, but that
is not the case for AM. We see, moreover, qualitative
differences in the solutions as well – e.g., AM’s so-
lution for the pendant node 5 is less confident than is
LP’s solution. More empirical comparative analysis
between the two algorithms of this sort will appear
in future work.
We have proposed a new algorithm for semi-
supervised text categorization. Empirical results
show that the proposed approach significantly out-
performs the state-of-the-art. In addition the pro-
posed approach is relatively simple to implement
and has guaranteed convergence properties. While
in this work, we use relatively simple features to
construct the graph, use of more sophisticated fea-
tures and/or similarity measures could lead to further
improved results.
</bodyText>
<sectionHeader confidence="0.996991" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996345">
This work was supported by ONR MURI grant
N000140510388, by NSF grant IIS-0093430, by
the Companions project (IST programme under EC
grant IST-FP6-034434), and by a Microsoft Re-
search Fellowship.
</bodyText>
<sectionHeader confidence="0.997494" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.938042714285714">
Alexandrescu, A. and Kirchhoff, K. (2007). Data-driven
graph construction for semi-supervised graph-based
learnning in nlp. In Proc. of the Human Language
Technologies Conference (HLT-NAACL).
Bekkerman, R., El-Yaniv, R., Tishby, N., and Winter, Y.
(2003). Distributional word clusters vs. words for text
categorization. J. Mach. Learn. Res., 3:1183–1208.
</reference>
<figureCaption confidence="0.965863">
Figure 3: Graph (top), and alternating values of pl, ql
for increasing n for AM and LP.
</figureCaption>
<figure confidence="0.885351718181818">
p(1)
p(5)
p(7)
p(6)
p(2)
p(3)
p(4)
p(3)
p(9)
p(12)
p(13)
p(10)
p(14)
p(15)
p(11)
2
3
4
5
q(0)
1
q(1)
Node 2
Label 2
q(2)
0.6
q(3)
q(4)
Node 3
Unlabeled
q(5)
q(6)
Node 5
Unlabeled
0.8
q(7)
q(3)
0.8
q(9)
q(10)
0.2
0.8
q(11)
Node 1
Label 1
q(12)
q(13)
Node 4
Unlabeled
q(14)
q(15)
p(1)
p(2)
p(3)
p(4)
p(5)
p(6)
p(7)
p(3)
p(9)
p(10)
p(11)
p(12)
p(13)
p(14)
p(15)
2
3
4
5
1
q(0)
q(1)
q(2)
q(3)
q(4)
q(5)
q(6)
q(7)
q(3)
q(9)
q(10)
q(11)
q(12)
q(13)
q(14)
q(15)
AM iteration (and distribution pair) number
LP iteration (and distribution pair) number
vertex (data piont) number
vertex (data piont) number
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
</figure>
<page confidence="0.952245">
1098
</page>
<reference confidence="0.999681663461539">
Belkin, M., Niyogi, P., and Sindhwani, V. (2005). On
manifold regularization. In Proc. of the Conference on
Artificial Intelligence and Statistics (AISTATS).
Bengio, Y., Delalleau, O., and Roux, N. L. (2007). Semi-
Supervised Learning, chapter Label Propogation and
Quadratic Criterion. MIT Press.
Bertsekas, D. (2004). Nonlinear Programming. Athena
Scientific Publishing.
Blitzer, J. and Zhu, J. (2008). ACL 2008 tutorial on
Semi-Supervised learning. http://ssl-acl08.
wikidot.com/.
Blum, A. and Chawla, S. (2001). Learning from labeled
and unlabeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning, pages 19–
26. Morgan Kaufmann, San Francisco, CA.
Blum, A. and Mitchell, T. (1998). Combining labeled
and unlabeled data with co-training. In COLT: Pro-
ceedings of the Workshop on Computational Learning
Theory.
Chapelle, O., Scholkopf, B., and Zien, A. (2007). Semi-
Supervised Learning. MIT Press.
Corduneanu, A. and Jaakkola, T. (2003). On informa-
tion regularization. In Uncertainty in Artificial Intelli-
gence.
Cover, T. M. and Thomas, J. A. (1991). Elements of In-
formation Theory. Wiley Series in Telecommunica-
tions. Wiley, New York.
Csiszar, I. and Tusnady, G. (1984). Information Geome-
try and Alternating Minimization Procedures. Statis-
tics and Decisions.
Dumais, S., Platt, J., Heckerman, D., and Sahami, M.
(1998). Inductive learning algorithms and represen-
tations for text categorization. In CIKM ’98: Proceed-
ings of the seventh international conference on Infor-
mation and knowledge management, New York, NY,
USA.
Grandvalet, Y. and Bengio, Y. (2004). Semi-supervised
learning by entropy minimization. In Advances in
Neural Information Processing Systems (NIPS).
Joachims, T. SGT Light. http://sgt.joachims.
org.
Joachims, T. SVM Light. http://svmlight.
joachims.org.
Joachims, T. (1999). Transductive inference for text clas-
sification using support vector machines. In Proc. of
the International Conference on Machine Learning
(ICML).
Joachims, T. (2003). Transductive learning via spectral
graph partitioning. In Proc. of the International Con-
ference on Machine Learning (ICML).
Lewis, D. et al. (1987). Reuters-21578. http:
//www.daviddlewis.com/resources/
testcollections/reuters21578.
Nigam, K., McCallum, A., Thrun, S., and Mitchell, T.
(1998). Learning to classify text from labeled and un-
labeled documents. In AAAI ’98/IAAI ’98: Proceed-
ings of the fifteenth national/tenth conference on Arti-
ficial intelligence/Innovative applications of artificial
intelligence, pages 792–799.
Pearl, J. (1990). Jeffrey’s Rule, Passage of Experience
and Neo-Bayesianism in Knowledge Representation
and Defeasible Reasoning. Kluwer Academic Pub-
lishers.
Porter, M. (1980). An algorithm for suffix stripping. Pro-
gram, 14(3):130–137.
Raghavan, V., Bollmann, P., and Jung, G. S. (1989). A
critical investigation of recall and precision as mea-
sures of retrieval system performance. ACM Trans.
Inf. Syst., 7(3):205–229.
Salton, G. and Buckley, C. (1987). Term weighting ap-
proaches in automatic text retrieval. Technical report,
Ithaca, NY, USA.
Sindhwani, V., Niyogi, P., and Belkin, M. (2005). Be-
yond the point cloud: from transductive to semi-
supervised learning. In Proc. of the International Con-
ference on Machine Learning (ICML).
Szummer, M. and Jaakkola, T. (2001). Partially la-
beled classification with Markov random walks. In
Advances in Neural Information Processing Systems,
volume 14.
Tsuda, K. (2005). Propagating distributions on a hyper-
graph by dual information regularization. In Proceed-
ings of the 22nd International Conference on Machine
Learning.
Wang, J., Jebara, T., and Chang, S.-F. (2008). Graph
transduction via alternating minimization. In Proc. of
the International Conference on Machine Learning
(ICML).
Yarowsky, D. (1995). Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics.
Zhu, X. (2005a). Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Uni-
versity of Wisconsin-Madison.
Zhu, X. (2005b). Semi-Supervised Learning with
Graphs. PhD thesis, Carnegie Mellon University.
Zhu, X. and Ghahramani, Z. (2002). Learning from
labeled and unlabeled data with label propagation.
Technical report, Carnegie Mellon University.
Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of the International Con-
ference on Machine Learning (ICML).
</reference>
<page confidence="0.997292">
1099
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.536286">
<title confidence="0.999388">Soft-Supervised Learning for Text Classification</title>
<author confidence="0.991363">Amarnag Subramanya</author>
<author confidence="0.991363">Jeff</author>
<affiliation confidence="0.779251">Dept. of Electrical University of Washington, Seattle, WA 98195,</affiliation>
<abstract confidence="0.997855833333333">We propose a new graph-based semisupervised learning (SSL) algorithm and demonstrate its application to document categorization. Each document is represented by a vertex within a weighted undirected graph and our proposed framework minimizes the weighted Kullback-Leibler divergence between distributions that encode the class membership probabilities of each vertex. The proposed objective is convex with guaranteed convergence using an alternating minimization procedure. Further, it generalizes in a straightforward manner to multi-class problems. We present results on two standard tasks, namely Reuters-21578 and WebKB, showing that the proposed algorithm significantly outperforms the state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Alexandrescu</author>
<author>K Kirchhoff</author>
</authors>
<title>Data-driven graph construction for semi-supervised graph-based learnning in nlp.</title>
<date>2007</date>
<booktitle>In Proc. of the Human Language Technologies Conference (HLT-NAACL).</booktitle>
<contexts>
<context position="7225" citStr="Alexandrescu and Kirchhoff, 2007" startWordPosition="1137" endWordPosition="1141">SSL algorithms utilize an undirected weighted graph G = (V, E) where V = {1, ... , n} are the data points in D and E = V × V are the set of undirected edges between vertices. We use wij ∈ W to denote the weight of the edge between vertices i and j. W is referred to as the weight (or affinity) matrix of G. As will be seen shortly, the input features xi effect the final classification results via W, i.e., the graph. Thus graph construction is crucial to the success of any graph-based SSL algorithm. Graph construction “is more of an art, than science” (Zhu, 2005b) and is an active research area (Alexandrescu and Kirchhoff, 2007). In general the weights are formed as wij = sim(xi, xj)S(j ∈ K(i)). Here K(i) is the set of i’s k-nearest-neighbors (KNN), sim(xi, xj) is a given measure of similarity between xi and xj, and S(c) returns a 1 if c is true and 0 otherwise. Getting the similarity measure right is crucial for the success of any SSL algorithm as that is what determines the graph. Note that setting K(i) = |V |= n results in a fully-connected graph. Some popular similarity measures include sim(xi, xj) = e sim(xi, xj) = cos(xi, xj) = hxi,xji k xi k22k xj k2 2 where k xi k2 is the L2 norm, and hxi, xji is the inner pr</context>
</contexts>
<marker>Alexandrescu, Kirchhoff, 2007</marker>
<rawString>Alexandrescu, A. and Kirchhoff, K. (2007). Data-driven graph construction for semi-supervised graph-based learnning in nlp. In Proc. of the Human Language Technologies Conference (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
<author>R El-Yaniv</author>
<author>N Tishby</author>
<author>Y Winter</author>
</authors>
<title>Distributional word clusters vs. words for text categorization.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1183</pages>
<contexts>
<context position="30542" citStr="Bekkerman et al., 2003" startWordPosition="5254" endWordPosition="5257">ments from categories course, department, faculty, project which gives 4199 documents for the four categories. Each of the documents is in HTML format containing text as well as other information such as HTML tags, links, etc. We used both textual and non-textual information to construct the feature vectors. In this case we did not use either stop-word removal or stemming as this has been found to hurt performance on this task (Nigam et al., 1998). As in the the case of the Reuters data set we extracted TFIDF features for each document and constructed the graph using cosine similarity. As in (Bekkerman et al., 2003), we created four roughly-equal random partitions of the data set. In order to obtain Dl, we first randomly choose a split and then sample l documents from that split. The other three splits constitute D, We believe this is more realistic than sampling the labeled web-pages from a single university and testing web-pages from the other universities (Joachims, 1999). This method of creating transduction sets allows us to better evaluate the generalization performance of the various algorithms. Once again we create 21 transduction sets and the first set was used to tune the hyperparameters. Furth</context>
</contexts>
<marker>Bekkerman, El-Yaniv, Tishby, Winter, 2003</marker>
<rawString>Bekkerman, R., El-Yaniv, R., Tishby, N., and Winter, Y. (2003). Distributional word clusters vs. words for text categorization. J. Mach. Learn. Res., 3:1183–1208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Belkin</author>
<author>P Niyogi</author>
<author>V Sindhwani</author>
</authors>
<title>On manifold regularization.</title>
<date>2005</date>
<booktitle>In Proc. of the Conference on Artificial Intelligence and Statistics (AISTATS).</booktitle>
<contexts>
<context position="2262" citStr="Belkin et al., 2005" startWordPosition="323" endWordPosition="326">ed and unlabeled) is embedded within a low-dimensional manifold expressed by a graph. In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices. Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels from labeled to unlabeled samples (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002) and those that optimize a loss function based on smoothness constraints derived from the graph (Blum and Chawla, 2001; Zhu et al., 2003; Joachims, 2003; Belkin et al., 2005). Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003). In general graph-based SSL algorithms are non-parametric and transductive.1 A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training. In practice, however, transductive learners can be modified to handle unseen data (Zhu, 2005a; Sindhwani et al., 2005). A common drawback of many graph-based SSL algorithms (e.g. (Blum and Chawla, 2001; Joachims,</context>
</contexts>
<marker>Belkin, Niyogi, Sindhwani, 2005</marker>
<rawString>Belkin, M., Niyogi, P., and Sindhwani, V. (2005). On manifold regularization. In Proc. of the Conference on Artificial Intelligence and Statistics (AISTATS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>O Delalleau</author>
<author>N L Roux</author>
</authors>
<title>SemiSupervised Learning, chapter Label Propogation and Quadratic Criterion.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11180" citStr="Bengio et al., 2007" startWordPosition="1885" endWordPosition="1888">practice may be labeled as) many classes. The final classification results, i.e., the final labels for D,,, are then given by y� = argmax pi(y). yEY We next provide further intuition on our objective function. SSL on a graph consists of finding a labeling D,, that is consistent with both the labels provided in Dl and the geometry of the data induced by the graph. The first term of C1 will penalize the solution pi i E 11, ... ,l1, when it is far away from the labeled training data Dl, but it does not insist that pi = ri, as allowing for deviations from ri can help especially with noisy labels (Bengio et al., 2007) or when the graph is extremely dense in certain regions. As explained above, our framework allows for the case where supervised training is uncertain or ambiguous. We consider it reasonable to call our approach soft-supervised learning, generalizing the notion of semi-supervised learning, since there is even more of a continuum here between fully supervised and fully unsupervised learning than what typically exists with SSL. Soft-supervised learning allows uncertainty to be expressed (via a probability distribution) about any of the labels individually. The second term of C1 penalizes a lack </context>
<context position="20349" citStr="Bengio et al., 2007" startWordPosition="3509" endWordPosition="3512">ting minimization as AM. 4 Connections to Other Approaches Label propagation (LP) (Zhu and Ghahramani, 2002) is a graph-based SSL algorithms that performs Markov random walks on the graph and has a straightforward extension to multi-class problems. The update equations for LP (which also we use for our LP implementations) may be written as (n) ri(y)b(i &lt; l) + δ(i &gt; l) Pj wijp�n 1)(y) pi (y) = δ(i &lt; l) + δ(i &gt; l) Pj wij Note the similarity to the update equation for q(n) i in our AM case. It has been shown that the squaredloss based SSL algorithm (Zhu et al., 2003) and LP have similar updates (Bengio et al., 2007). The proposed objective C1 is similar in spirit to the squared-loss based objective in (Zhu et al., 2003; Bengio et al., 2007). Our method, however, differs in that we are optimizing the KL-divergence over probability distributions. We show in section 5 that KL-divergence based loss significantly outperforms the squared-loss. We believe that this could be due lim min α→∞ p,q 0 wij, 1094 to the following: 1) squared loss is appropriate under a Gaussian loss model which may not be optimal under many circumstances (e.g. classification); 2) KL-divergence DKL(p||q) is based on a relative (relative</context>
</contexts>
<marker>Bengio, Delalleau, Roux, 2007</marker>
<rawString>Bengio, Y., Delalleau, O., and Roux, N. L. (2007). SemiSupervised Learning, chapter Label Propogation and Quadratic Criterion. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bertsekas</author>
</authors>
<title>Nonlinear Programming. Athena Scientific Publishing.</title>
<date>2004</date>
<contexts>
<context position="14632" citStr="Bertsekas, 2004" startWordPosition="2468" endWordPosition="2469">, this objective would be useless if it wasn’t possible to efficiently and easily optimize it on large data sets. We next describe a method that can do this. 3 Learning with Alternating Minimization As long as µ, ν &gt; 0, the objective C1(p) is convex. This follows since Dxz(pi||pj) is convex in the pair (pi,pj) (Cover and Thomas, 1991), negative entropy is convex, and a positive-weighted linear combination of a set of convex functions is convex. Thus, the problem of minimizing C1 over the space of collections of probability distributions (a convex set) constitutes a convex programming problem (Bertsekas, 2004). This property is extremely beneficial since there is a unique global optimum and there are a variety of methods that can be used to yield that global optimum. One possible method might take the derivative of the objective along with Lagrange multipliers to ensure that we stay within the space of probability distributions. This method can sometimes yield a closed form single-step analytical expression for the globally optimum solution. Unfortunately, however, our problem does not admit such a closed form solution because the gradient of C1(p) with respect to pi(y) is of the form, k1pi(y) log </context>
<context position="16027" citStr="Bertsekas, 2004" startWordPosition="2699" endWordPosition="2700">bjective also does not yield a closed form solution. The typical next step, then, is to resort to iterative techniques such as gradient descent along with modifications to ensure that the solution stays within the set of probability distributions (the gradient of C1 alone will not necessarily point in the direction where p is still a valid distribution) - one such modification is called the method of multipliers (MOM). Another solution would be to use computationally complex (and complicated) algorithms like interior point methods (IPM). While all of the above methods (described in detail in (Bertsekas, 2004)) are feasible ways to solve our problem, they each have their own drawbacks. Using MOM, for example, requires the careful tuning of a number of additional parameters such as learning rates, growth factors, and so on. IPM involves inverting a matrix of the order of the number of variables and constraints during each iteration. We instead adopt a different strategy based on alternating minimization (Csiszar and Tusnady, 1984). This approach has a single additional optimization parameter (contrasted with MOM), admits a closed form solution for each iteration not involving any matrix inversion (c</context>
</contexts>
<marker>Bertsekas, 2004</marker>
<rawString>Bertsekas, D. (2004). Nonlinear Programming. Athena Scientific Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>J Zhu</author>
</authors>
<date>2008</date>
<journal>ACL</journal>
<note>tutorial on Semi-Supervised learning. http://ssl-acl08. wikidot.com/.</note>
<contexts>
<context position="3701" citStr="Blitzer and Zhu, 2008" startWordPosition="547" endWordPosition="550"> let alone structured domains such as strings and trees. There are also issues related to degenerate solutions (all unlabeled samples classified as belonging to a single 1Excluding Manifold Regularization (Belkin et al., 2005). 1090 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090–1099, Honolulu, October 2008. c�2008 Association for Computational Linguistics class) (Blum and Chawla, 2001; Joachims, 2003; Zhu and Ghahramani, 2002). For more background on graph-based and general SSL and their applications, see (Zhu, 2005a; Chapelle et al., 2007; Blitzer and Zhu, 2008). In this paper we propose a new algorithm for graph-based SSL and use the task of text classification to demonstrate its benefits over the current stateof-the-art. Text classification involves automatically assigning a given document to a fixed number of semantic categories. Each document may belong to one, many, or none of the categories. In general, text classification is a multi-class problem (more than 2 categories). Training fully-supervised text classifiers requires large amounts of labeled data whose annotation can be expensive (Dumais et al., 1998). As a result there has been interest</context>
</contexts>
<marker>Blitzer, Zhu, 2008</marker>
<rawString>Blitzer, J. and Zhu, J. (2008). ACL 2008 tutorial on Semi-Supervised learning. http://ssl-acl08. wikidot.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>S Chawla</author>
</authors>
<title>Learning from labeled and unlabeled data using graph mincuts.</title>
<date>2001</date>
<booktitle>In Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>19--26</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="1579" citStr="Blum and Chawla, 2001" startWordPosition="210" endWordPosition="213">(SSL) employs small amounts of labeled data with relatively large amounts of unlabeled data to train classifiers. In many problems, such as speech recognition, document classification, and sentiment recognition, annotating training data is both time-consuming and tedious, while unlabeled data are easily obtained thus making these problems useful applications of SSL. Classic examples of SSL algorithms include self-training (Yarowsky, 1995) and co-training (Blum and Mitchell, 1998). Graphbased SSL algorithms are an important class of SSL techniques that have attracted much of attention of late (Blum and Chawla, 2001; Zhu et al., 2003). Here one assumes that the data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph. In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices. Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels from labeled to unlabeled samples (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002) and those that optimize a loss function based on smoothness constraints derived from the g</context>
<context position="2851" citStr="Blum and Chawla, 2001" startWordPosition="420" endWordPosition="423">hims, 2003; Belkin et al., 2005). Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003). In general graph-based SSL algorithms are non-parametric and transductive.1 A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training. In practice, however, transductive learners can be modified to handle unseen data (Zhu, 2005a; Sindhwani et al., 2005). A common drawback of many graph-based SSL algorithms (e.g. (Blum and Chawla, 2001; Joachims, 2003; Belkin et al., 2005)) is that they assume binary classification tasks and thus require the use of sub-optimal (and often computationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone structured domains such as strings and trees. There are also issues related to degenerate solutions (all unlabeled samples classified as belonging to a single 1Excluding Manifold Regularization (Belkin et al., 2005). 1090 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090–1099, Honolulu, October 2008. c�2008 Ass</context>
<context position="12343" citStr="Blum and Chawla, 2001" startWordPosition="2084" endWordPosition="2087">bels individually. The second term of C1 penalizes a lack of consistency with the geometry of the data and can be seen as a graph regularizer. If wij is large, we prefer a solution in which pi and pj are close in the KLdivergence sense. While KL-divergence is asymmetric, given that g is undirected implies W is symmetric (wij = wji) and as a result the second term is inherently symmetric. The last term encourages each pi to be close to the uniform distribution if not preferred to the contrary by the first two terms. This acts as a guard against degenerate solutions commonly encountered in SSL (Blum and Chawla, 2001; Joachims, 2003). For example, consider the case where part of the graph is almost completely disconnected from any labeled vertex (which is possible in the k-nearest neighbor case). In such situations the third term enmin p +µ n n � wijDKL (piJJpj) − ν� i j i=1 1092 sures that the nodes in this disconnected region are encouraged to yield a uniform distribution, validly expressing the fact that we do not know the labels of these nodes based on the nature of the graph. More generally, we conjecture that by maximizing the entropy of each pi, the classifier has a better chance of producing high </context>
</contexts>
<marker>Blum, Chawla, 2001</marker>
<rawString>Blum, A. and Chawla, S. (2001). Learning from labeled and unlabeled data using graph mincuts. In Proc. 18th International Conf. on Machine Learning, pages 19– 26. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT: Proceedings of the Workshop on Computational Learning Theory.</booktitle>
<contexts>
<context position="1442" citStr="Blum and Mitchell, 1998" startWordPosition="187" endWordPosition="190">1578 and WebKB, showing that the proposed algorithm significantly outperforms the state-of-the-art. 1 Introduction Semi-supervised learning (SSL) employs small amounts of labeled data with relatively large amounts of unlabeled data to train classifiers. In many problems, such as speech recognition, document classification, and sentiment recognition, annotating training data is both time-consuming and tedious, while unlabeled data are easily obtained thus making these problems useful applications of SSL. Classic examples of SSL algorithms include self-training (Yarowsky, 1995) and co-training (Blum and Mitchell, 1998). Graphbased SSL algorithms are an important class of SSL techniques that have attracted much of attention of late (Blum and Chawla, 2001; Zhu et al., 2003). Here one assumes that the data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph. In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices. Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels from labeled to unlabeled samples (Szummer</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Blum, A. and Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In COLT: Proceedings of the Workshop on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>B Scholkopf</author>
<author>A Zien</author>
</authors>
<title>SemiSupervised Learning.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3677" citStr="Chapelle et al., 2007" startWordPosition="543" endWordPosition="546">e multi-class problems, let alone structured domains such as strings and trees. There are also issues related to degenerate solutions (all unlabeled samples classified as belonging to a single 1Excluding Manifold Regularization (Belkin et al., 2005). 1090 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090–1099, Honolulu, October 2008. c�2008 Association for Computational Linguistics class) (Blum and Chawla, 2001; Joachims, 2003; Zhu and Ghahramani, 2002). For more background on graph-based and general SSL and their applications, see (Zhu, 2005a; Chapelle et al., 2007; Blitzer and Zhu, 2008). In this paper we propose a new algorithm for graph-based SSL and use the task of text classification to demonstrate its benefits over the current stateof-the-art. Text classification involves automatically assigning a given document to a fixed number of semantic categories. Each document may belong to one, many, or none of the categories. In general, text classification is a multi-class problem (more than 2 categories). Training fully-supervised text classifiers requires large amounts of labeled data whose annotation can be expensive (Dumais et al., 1998). As a result</context>
<context position="13412" citStr="Chapelle et al., 2007" startWordPosition="2269" endWordPosition="2272">n the nature of the graph. More generally, we conjecture that by maximizing the entropy of each pi, the classifier has a better chance of producing high entropy results in graph regions of low confidence (e.g. close to the decision boundary and/or low density regions). This overcomes a common drawback of a large number of state-of-the-art classifiers that tend to be confident even in regions close to the decision boundary. We conclude this section by summarizing some of the features of our proposed framework. It should be clear that C1 uses the “manifold assumption” for SSL (see chapter 2 in (Chapelle et al., 2007)) — it assumes that the input data can be embedded within a low-dimensional manifold (the graph). As the objective is defined in terms of probability distributions over integers rather than just integers (or to real-valued relaxations of integers (Joachims, 2003; Zhu et al., 2003)), the framework generalizes in a straightforward manner to multi-class problems. Further, all the parameters are estimated jointly (compare to one vs. rest approaches which involve solving |Y |independent problems). Furthermore, the objective is capable of handling label training data uncertainty (Pearl, 1990). Of co</context>
</contexts>
<marker>Chapelle, Scholkopf, Zien, 2007</marker>
<rawString>Chapelle, O., Scholkopf, B., and Zien, A. (2007). SemiSupervised Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Corduneanu</author>
<author>T Jaakkola</author>
</authors>
<title>On information regularization.</title>
<date>2003</date>
<booktitle>In Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="22913" citStr="Corduneanu and Jaakkola, 2003" startWordPosition="3912" endWordPosition="3915">quences in terms of both the mathematics and meaning. The objective in case of EnM is not convex, whereas in our case we have a convex formulation with simple update equations and convergence guarantees. (Wang et al., 2008) is a graph-based SSL algorithm that also employs alternating minimization style optimization. However, it is inherently squared-loss based which our proposed approach out-performs (see section 5). Further, they do not provide or state convergence guarantees and one side of their update approximates an NP-complete optimization procedure. The information regularization (IR) (Corduneanu and Jaakkola, 2003) algorithm also makes use of a KL-divergence based loss for SSL. Here the input space is divided into regions {Ri} which might or might not overlap. For a given point xi E Ri, IR attempts to minimize the KL-divergence between pi(yi|xi) and PR,(y), the agglomerative distribution for region Ri. Given a graph, one can define a region to be a vertex and its neighbor thus making IR amenable to graph-based SSL. In (Corduneanu and Jaakkola, 2003), the agglomeration is performed by a simple averaging (arithmetic mean). While IR suggests (without proof of convergence) the use of alternating minimizatio</context>
</contexts>
<marker>Corduneanu, Jaakkola, 2003</marker>
<rawString>Corduneanu, A. and Jaakkola, T. (2003). On information regularization. In Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<booktitle>Series in Telecommunications.</booktitle>
<publisher>Wiley</publisher>
<location>New York.</location>
<contexts>
<context position="4900" citStr="Cover and Thomas, 1991" startWordPosition="732" endWordPosition="735"> there has been interest is using SSL techniques for text classification (Joachims, 1999; Joachims, 2003). However past work in semisupervised text classification has relied primarily on one vs. rest approaches to overcome the inherent multi-class nature of this problem. We believe such an approach may be sub-optimal because, disregarding data overlap, the different classifiers have training procedures that are independent of one other. In order to address the above drawback we propose a new framework based on optimizing a loss function composed of Kullback-Leibler divergence (KL-divergence) (Cover and Thomas, 1991) terms between probability distributions defined for each graph vertex. The use of probability distributions, rather than fixed integer labels, not only leads to a straightforward multi-class generalization, but also allows us to exploit other well-defined functions of distributions, such as entropy, to improve system performance and to allow for the measure of uncertainty. For example, with a single integer, at most all we know is its assignment. With a distribution, we can continuously move from knowing an assignment with certainty (i.e., an entropy of zero) to expressions of doubt or multip</context>
<context position="14352" citStr="Cover and Thomas, 1991" startWordPosition="2421" endWordPosition="2424">raightforward manner to multi-class problems. Further, all the parameters are estimated jointly (compare to one vs. rest approaches which involve solving |Y |independent problems). Furthermore, the objective is capable of handling label training data uncertainty (Pearl, 1990). Of course, this objective would be useless if it wasn’t possible to efficiently and easily optimize it on large data sets. We next describe a method that can do this. 3 Learning with Alternating Minimization As long as µ, ν &gt; 0, the objective C1(p) is convex. This follows since Dxz(pi||pj) is convex in the pair (pi,pj) (Cover and Thomas, 1991), negative entropy is convex, and a positive-weighted linear combination of a set of convex functions is convex. Thus, the problem of minimizing C1 over the space of collections of probability distributions (a convex set) constitutes a convex programming problem (Bertsekas, 2004). This property is extremely beneficial since there is a unique global optimum and there are a variety of methods that can be used to yield that global optimum. One possible method might take the derivative of the objective along with Lagrange multipliers to ensure that we stay within the space of probability distribut</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T. M. and Thomas, J. A. (1991). Elements of Information Theory. Wiley Series in Telecommunications. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Csiszar</author>
<author>G Tusnady</author>
</authors>
<title>Information Geometry and Alternating Minimization Procedures. Statistics and Decisions.</title>
<date>1984</date>
<contexts>
<context position="5718" citStr="Csiszar and Tusnady, 1984" startWordPosition="860" endWordPosition="863">generalization, but also allows us to exploit other well-defined functions of distributions, such as entropy, to improve system performance and to allow for the measure of uncertainty. For example, with a single integer, at most all we know is its assignment. With a distribution, we can continuously move from knowing an assignment with certainty (i.e., an entropy of zero) to expressions of doubt or multiple valid possibilities (i.e., an entropy greater than zero). This is particularly useful for document classification as we will see. We also show how one can use the alternating minimization (Csiszar and Tusnady, 1984) algorithm to optimize our objective leading to a relatively simple, fast, easy-to-implement, guaranteed to converge, iterative, and closed form update for each iteration. 2 Proposed Graph-Based Learning Framework We consider the transductive learning problem, i.e., given a training set D = {Dl, Du}, where Dl and Du are the sets of labeled and unlabeled samples respectively, the task is to infer the labels for the samples in Du. In other words, Du is the “test-set.” Here Dl = {(xi, yi)}li=1, Du = {xi}l+u i=l+1, xi ∈ X (the input space of the classifier, and corresponds to vectors of features) </context>
<context position="16455" citStr="Csiszar and Tusnady, 1984" startWordPosition="2769" endWordPosition="2772">OM). Another solution would be to use computationally complex (and complicated) algorithms like interior point methods (IPM). While all of the above methods (described in detail in (Bertsekas, 2004)) are feasible ways to solve our problem, they each have their own drawbacks. Using MOM, for example, requires the careful tuning of a number of additional parameters such as learning rates, growth factors, and so on. IPM involves inverting a matrix of the order of the number of variables and constraints during each iteration. We instead adopt a different strategy based on alternating minimization (Csiszar and Tusnady, 1984). This approach has a single additional optimization parameter (contrasted with MOM), admits a closed form solution for each iteration not involving any matrix inversion (contrasted with IPM), and yields guaranteed convergence to the global optimum. In order to render our approach amenable to AM, however, we relax our objective C1 by defining a new (third) set of distributions for all training samples qi, i = 1, ... , n denoted collectively like the above using the notation q °_ (q1, ... , qn). We define a new objective to be optimized as follows: &amp;quot; l C2 (p, q) , where C2 (p, q) =XDxz (ri ||qi</context>
</contexts>
<marker>Csiszar, Tusnady, 1984</marker>
<rawString>Csiszar, I. and Tusnady, G. (1984). Information Geometry and Alternating Minimization Procedures. Statistics and Decisions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>J Platt</author>
<author>D Heckerman</author>
<author>M Sahami</author>
</authors>
<title>Inductive learning algorithms and representations for text categorization.</title>
<date>1998</date>
<booktitle>In CIKM ’98: Proceedings of the seventh international conference on Information and knowledge management,</booktitle>
<location>New York, NY, USA.</location>
<contexts>
<context position="4264" citStr="Dumais et al., 1998" startWordPosition="634" endWordPosition="637">Zhu, 2005a; Chapelle et al., 2007; Blitzer and Zhu, 2008). In this paper we propose a new algorithm for graph-based SSL and use the task of text classification to demonstrate its benefits over the current stateof-the-art. Text classification involves automatically assigning a given document to a fixed number of semantic categories. Each document may belong to one, many, or none of the categories. In general, text classification is a multi-class problem (more than 2 categories). Training fully-supervised text classifiers requires large amounts of labeled data whose annotation can be expensive (Dumais et al., 1998). As a result there has been interest is using SSL techniques for text classification (Joachims, 1999; Joachims, 2003). However past work in semisupervised text classification has relied primarily on one vs. rest approaches to overcome the inherent multi-class nature of this problem. We believe such an approach may be sub-optimal because, disregarding data overlap, the different classifiers have training procedures that are independent of one other. In order to address the above drawback we propose a new framework based on optimizing a loss function composed of Kullback-Leibler divergence (KL-</context>
</contexts>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>Dumais, S., Platt, J., Heckerman, D., and Sahami, M. (1998). Inductive learning algorithms and representations for text categorization. In CIKM ’98: Proceedings of the seventh international conference on Information and knowledge management, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Grandvalet</author>
<author>Y Bengio</author>
</authors>
<title>Semi-supervised learning by entropy minimization.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="21741" citStr="Grandvalet and Bengio, 2004" startWordPosition="3726" endWordPosition="3729">y distributions. AM is also similar to the spectral graph transducer (Joachims, 2003) in that they both attempt to find labellings over the unlabeled data that respect the smoothness constraints of the graph. While spectral graph transduction is an approximate solution to a discrete optimization problem (which is NP hard), AM is an exact solution obtained by optimizing a convex function over a continuous space. Further, while spectral graph transduction assumes binary classification problems, AM naturally extends to multi-class situations without loss of convexity. Entropy Minimization (EnM) (Grandvalet and Bengio, 2004) uses the entropy of the unlabeled data as a regularizer while optimizing a parametric loss function defined over the labeled data. While the objectives in the case of both AM and EnM make use of the entropy of the unlabeled data, there are several important differences: (a) EnM is not graphbased, (b) EnM is parametric whereas our proposed approach is non-parametric, and most importantly, (c) EnM attempts to minimize entropy while the proposed approach aims to maximize entropy. While this may seem a triviality, it has catastrophic consequences in terms of both the mathematics and meaning. The </context>
</contexts>
<marker>Grandvalet, Bengio, 2004</marker>
<rawString>Grandvalet, Y. and Bengio, Y. (2004). Semi-supervised learning by entropy minimization. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="false">
<authors>
<author>T SGT Light Joachims</author>
</authors>
<note>http://sgt.joachims. org.</note>
<marker>Joachims, </marker>
<rawString>Joachims, T. SGT Light. http://sgt.joachims. org.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T SVM Light Joachims</author>
</authors>
<note>http://svmlight. joachims.org.</note>
<marker>Joachims, </marker>
<rawString>Joachims, T. SVM Light. http://svmlight. joachims.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="4365" citStr="Joachims, 1999" startWordPosition="653" endWordPosition="654">ph-based SSL and use the task of text classification to demonstrate its benefits over the current stateof-the-art. Text classification involves automatically assigning a given document to a fixed number of semantic categories. Each document may belong to one, many, or none of the categories. In general, text classification is a multi-class problem (more than 2 categories). Training fully-supervised text classifiers requires large amounts of labeled data whose annotation can be expensive (Dumais et al., 1998). As a result there has been interest is using SSL techniques for text classification (Joachims, 1999; Joachims, 2003). However past work in semisupervised text classification has relied primarily on one vs. rest approaches to overcome the inherent multi-class nature of this problem. We believe such an approach may be sub-optimal because, disregarding data overlap, the different classifiers have training procedures that are independent of one other. In order to address the above drawback we propose a new framework based on optimizing a loss function composed of Kullback-Leibler divergence (KL-divergence) (Cover and Thomas, 1991) terms between probability distributions defined for each graph v</context>
<context position="24500" citStr="Joachims, 1999" startWordPosition="4185" endWordPosition="4186">both steps of the alternating minimization. There are several important differences between IR and PD on one side and our proposed approach: (a) neither IR nor PD use an entropy regularizer, and (b) the update equation for one of the steps of the optimization in the case of PD (equation 13 in (Tsuda, 2005)) is actually a special case of our update equation for pi(y) and may be obtained by setting wij = 1/2. Further, our work here may be easily extended to hypergraphs. 5 Results We compare our algorithm (AM) with other state-of-the-art SSL-based text categorization algorithms, namely, (a) SVM (Joachims, 1999), (b) Transductive-SVM (TSVM) (Joachims, 1999), (c) Spectral Graph Transduction (SGT) (Joachims, 2003), and (d) Label Propagation (LP) (Zhu and Ghahramani, 2002). Note that only SGT and LP are graph-based algorithms, while SVM is fullysupervised (i.e., it does not make use of any of the unlabeled data). We implemented SVM and TSVM using SVM Light (Joachims, b) and SGT using SGT Light (Joachims, a). In the case of SVM, TSVM and SGT we trained |Y |classifiers (one for each class) in a one vs. rest manner precisely following (Joachims, 2003). 5.1 Reuters-21578 We used the “ModApte” split of the R</context>
<context position="30908" citStr="Joachims, 1999" startWordPosition="5315" endWordPosition="5316"> as this has been found to hurt performance on this task (Nigam et al., 1998). As in the the case of the Reuters data set we extracted TFIDF features for each document and constructed the graph using cosine similarity. As in (Bekkerman et al., 2003), we created four roughly-equal random partitions of the data set. In order to obtain Dl, we first randomly choose a split and then sample l documents from that split. The other three splits constitute D, We believe this is more realistic than sampling the labeled web-pages from a single university and testing web-pages from the other universities (Joachims, 1999). This method of creating transduction sets allows us to better evaluate the generalization performance of the various algorithms. Once again we create 21 transduction sets and the first set was used to tune the hyperparameters. Further, we ran a search over the same grid as used in the case of Reuters. We report precision0 100 200 300 400 500 600 Number of Labeled Documents Figure 2: Average PRBEP over all classes vs. number of labeled documents (l) for WebKB collection. Class SVM TSVM SGT LP AM course 46.5 43.9 29.9 45.0 67.6 faculty 14.5 31.2 42.9 40.3 42.5 project 15.8 17.2 17.5 27.8 42.3 </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. (1999). Transductive inference for text classification using support vector machines. In Proc. of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Transductive learning via spectral graph partitioning.</title>
<date>2003</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="2240" citStr="Joachims, 2003" startWordPosition="321" endWordPosition="322">data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph. In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices. Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels from labeled to unlabeled samples (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002) and those that optimize a loss function based on smoothness constraints derived from the graph (Blum and Chawla, 2001; Zhu et al., 2003; Joachims, 2003; Belkin et al., 2005). Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003). In general graph-based SSL algorithms are non-parametric and transductive.1 A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training. In practice, however, transductive learners can be modified to handle unseen data (Zhu, 2005a; Sindhwani et al., 2005). A common drawback of many graph-based SSL algorithms (e.g. (Blum and C</context>
<context position="3535" citStr="Joachims, 2003" startWordPosition="522" endWordPosition="523">ssification tasks and thus require the use of sub-optimal (and often computationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone structured domains such as strings and trees. There are also issues related to degenerate solutions (all unlabeled samples classified as belonging to a single 1Excluding Manifold Regularization (Belkin et al., 2005). 1090 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090–1099, Honolulu, October 2008. c�2008 Association for Computational Linguistics class) (Blum and Chawla, 2001; Joachims, 2003; Zhu and Ghahramani, 2002). For more background on graph-based and general SSL and their applications, see (Zhu, 2005a; Chapelle et al., 2007; Blitzer and Zhu, 2008). In this paper we propose a new algorithm for graph-based SSL and use the task of text classification to demonstrate its benefits over the current stateof-the-art. Text classification involves automatically assigning a given document to a fixed number of semantic categories. Each document may belong to one, many, or none of the categories. In general, text classification is a multi-class problem (more than 2 categories). Training</context>
<context position="12360" citStr="Joachims, 2003" startWordPosition="2088" endWordPosition="2089">second term of C1 penalizes a lack of consistency with the geometry of the data and can be seen as a graph regularizer. If wij is large, we prefer a solution in which pi and pj are close in the KLdivergence sense. While KL-divergence is asymmetric, given that g is undirected implies W is symmetric (wij = wji) and as a result the second term is inherently symmetric. The last term encourages each pi to be close to the uniform distribution if not preferred to the contrary by the first two terms. This acts as a guard against degenerate solutions commonly encountered in SSL (Blum and Chawla, 2001; Joachims, 2003). For example, consider the case where part of the graph is almost completely disconnected from any labeled vertex (which is possible in the k-nearest neighbor case). In such situations the third term enmin p +µ n n � wijDKL (piJJpj) − ν� i j i=1 1092 sures that the nodes in this disconnected region are encouraged to yield a uniform distribution, validly expressing the fact that we do not know the labels of these nodes based on the nature of the graph. More generally, we conjecture that by maximizing the entropy of each pi, the classifier has a better chance of producing high entropy results i</context>
<context position="13674" citStr="Joachims, 2003" startWordPosition="2312" endWordPosition="2313">This overcomes a common drawback of a large number of state-of-the-art classifiers that tend to be confident even in regions close to the decision boundary. We conclude this section by summarizing some of the features of our proposed framework. It should be clear that C1 uses the “manifold assumption” for SSL (see chapter 2 in (Chapelle et al., 2007)) — it assumes that the input data can be embedded within a low-dimensional manifold (the graph). As the objective is defined in terms of probability distributions over integers rather than just integers (or to real-valued relaxations of integers (Joachims, 2003; Zhu et al., 2003)), the framework generalizes in a straightforward manner to multi-class problems. Further, all the parameters are estimated jointly (compare to one vs. rest approaches which involve solving |Y |independent problems). Furthermore, the objective is capable of handling label training data uncertainty (Pearl, 1990). Of course, this objective would be useless if it wasn’t possible to efficiently and easily optimize it on large data sets. We next describe a method that can do this. 3 Learning with Alternating Minimization As long as µ, ν &gt; 0, the objective C1(p) is convex. This fo</context>
<context position="21198" citStr="Joachims, 2003" startWordPosition="3645" endWordPosition="3646">We show in section 5 that KL-divergence based loss significantly outperforms the squared-loss. We believe that this could be due lim min α→∞ p,q 0 wij, 1094 to the following: 1) squared loss is appropriate under a Gaussian loss model which may not be optimal under many circumstances (e.g. classification); 2) KL-divergence DKL(p||q) is based on a relative (relative to p) rather than an absolute error; and 3) under certain natural assumptions, KL-divergence is asymptotically consistent with respect to the underlying probability distributions. AM is also similar to the spectral graph transducer (Joachims, 2003) in that they both attempt to find labellings over the unlabeled data that respect the smoothness constraints of the graph. While spectral graph transduction is an approximate solution to a discrete optimization problem (which is NP hard), AM is an exact solution obtained by optimizing a convex function over a continuous space. Further, while spectral graph transduction assumes binary classification problems, AM naturally extends to multi-class situations without loss of convexity. Entropy Minimization (EnM) (Grandvalet and Bengio, 2004) uses the entropy of the unlabeled data as a regularizer </context>
<context position="24602" citStr="Joachims, 2003" startWordPosition="4197" endWordPosition="4198">on one side and our proposed approach: (a) neither IR nor PD use an entropy regularizer, and (b) the update equation for one of the steps of the optimization in the case of PD (equation 13 in (Tsuda, 2005)) is actually a special case of our update equation for pi(y) and may be obtained by setting wij = 1/2. Further, our work here may be easily extended to hypergraphs. 5 Results We compare our algorithm (AM) with other state-of-the-art SSL-based text categorization algorithms, namely, (a) SVM (Joachims, 1999), (b) Transductive-SVM (TSVM) (Joachims, 1999), (c) Spectral Graph Transduction (SGT) (Joachims, 2003), and (d) Label Propagation (LP) (Zhu and Ghahramani, 2002). Note that only SGT and LP are graph-based algorithms, while SVM is fullysupervised (i.e., it does not make use of any of the unlabeled data). We implemented SVM and TSVM using SVM Light (Joachims, b) and SGT using SGT Light (Joachims, a). In the case of SVM, TSVM and SGT we trained |Y |classifiers (one for each class) in a one vs. rest manner precisely following (Joachims, 2003). 5.1 Reuters-21578 We used the “ModApte” split of the Reuters-21578 dataset collected from the Reuters newswire in 1095 1987 (Lewis et al., 1987). The corpus</context>
<context position="27166" citStr="Joachims, 2003" startWordPosition="4653" endWordPosition="4654">hms used the same transduction sets. In the case of SGT, LP and AM, the first transduction set was used to tune the hyperparameters which we then held fixed for all the remaining 20 transduction sets. For all the graph-based approaches, we ran a search over K E {2, 10, 50, 100, 250, 500, 1000, 2000, n} (note K = n represents a fully connected graph). In addition, in the case of AM, we set α = 2 for all experiments, and we ran a search over µ E {1e–8, 1e–4, 0.01, 0.1, 1, 10, 100} and v E {1e–8, 1e–6, 1e–4, 0.01, 0.1}, for SGT the search was over c E {3000, 3200, 3400, 3800, 5000, 100000} (see (Joachims, 2003)). We report precision-recall break even point (PRBEP) results on the 3,299 test documents in Table 1. PRBEP has been a popular measure in information retrieval (see e.g. (Raghavan et al., 1989)). It is defined as that value for which precision and recall are equal. Results for each category in Table 1 were obtained by averaging the PRBEP over Category SVM TSVM SGT LP AM earn 91.3 95.4 90.4 96.3 97.9 acq 67.8 76.6 91.9 90.8 97.2 money 41.3 60.0 65.6 57.1 73.9 grain 56.2 68.5 43.1 33.6 41.3 crude 40.9 83.6 65.9 74.8 55.5 trade 29.5 34.0 36.0 56.0 47.0 interest 35.6 50.8 50.7 47.9 78.0 ship 32.5</context>
</contexts>
<marker>Joachims, 2003</marker>
<rawString>Joachims, T. (2003). Transductive learning via spectral graph partitioning. In Proc. of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<date>1987</date>
<note>Reuters-21578. http: //www.daviddlewis.com/resources/ testcollections/reuters21578.</note>
<marker>Lewis, 1987</marker>
<rawString>Lewis, D. et al. (1987). Reuters-21578. http: //www.daviddlewis.com/resources/ testcollections/reuters21578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Learning to classify text from labeled and unlabeled documents.</title>
<date>1998</date>
<booktitle>In AAAI ’98/IAAI ’98: Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence,</booktitle>
<pages>792--799</pages>
<contexts>
<context position="29902" citStr="Nigam et al., 1998" startWordPosition="5143" endWordPosition="5146"> World Wide Knowledge Base (WebKB) is a collection of 8282 web pages obtained from four academic 1096 0 50 100 150 200 250 300 350 400 450 500 Number of Labeled Documents Figure 1: Average PRBEP over all classes vs. number of labeled documents (l) for Reuters data set domains. The web pages in the WebKB set are labeled using two different polychotomies. The first is according to topic and the second is according to web domain. In our experiments we only considered the first polychotomy, which consists of 7 categories: course, department, faculty, project, staff, student, and other. Following (Nigam et al., 1998) we only use documents from categories course, department, faculty, project which gives 4199 documents for the four categories. Each of the documents is in HTML format containing text as well as other information such as HTML tags, links, etc. We used both textual and non-textual information to construct the feature vectors. In this case we did not use either stop-word removal or stemming as this has been found to hurt performance on this task (Nigam et al., 1998). As in the the case of the Reuters data set we extracted TFIDF features for each document and constructed the graph using cosine si</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 1998</marker>
<rawString>Nigam, K., McCallum, A., Thrun, S., and Mitchell, T. (1998). Learning to classify text from labeled and unlabeled documents. In AAAI ’98/IAAI ’98: Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, pages 792–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<title>Jeffrey’s Rule, Passage of Experience and Neo-Bayesianism in Knowledge Representation and Defeasible Reasoning.</title>
<date>1990</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="14005" citStr="Pearl, 1990" startWordPosition="2361" endWordPosition="2362">pelle et al., 2007)) — it assumes that the input data can be embedded within a low-dimensional manifold (the graph). As the objective is defined in terms of probability distributions over integers rather than just integers (or to real-valued relaxations of integers (Joachims, 2003; Zhu et al., 2003)), the framework generalizes in a straightforward manner to multi-class problems. Further, all the parameters are estimated jointly (compare to one vs. rest approaches which involve solving |Y |independent problems). Furthermore, the objective is capable of handling label training data uncertainty (Pearl, 1990). Of course, this objective would be useless if it wasn’t possible to efficiently and easily optimize it on large data sets. We next describe a method that can do this. 3 Learning with Alternating Minimization As long as µ, ν &gt; 0, the objective C1(p) is convex. This follows since Dxz(pi||pj) is convex in the pair (pi,pj) (Cover and Thomas, 1991), negative entropy is convex, and a positive-weighted linear combination of a set of convex functions is convex. Thus, the problem of minimizing C1 over the space of collections of probability distributions (a convex set) constitutes a convex programmin</context>
</contexts>
<marker>Pearl, 1990</marker>
<rawString>Pearl, J. (1990). Jeffrey’s Rule, Passage of Experience and Neo-Bayesianism in Knowledge Representation and Defeasible Reasoning. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="25718" citStr="Porter, 1980" startWordPosition="4386" endWordPosition="4387">s-21578 dataset collected from the Reuters newswire in 1095 1987 (Lewis et al., 1987). The corpus has 9,603 training (not to be confused with D) and 3,299 test documents (which represents Du). Of the 135 potential topic categories only the 10 most frequent categories are used (Joachims, 1999). Categories outside the 10 most frequent were collapsed into one class and assigned a label “other”. For each document i in the training and test sets, we extract features xi in the following manner: stop-words are removed followed by the removal of case and information about inflection (i.e., stemming) (Porter, 1980). We then compute TFIDF features for each document (Salton and Buckley, 1987). All graphs were constructed using cosine similarity with TFIDF features. For this task Y = { earn, acq, money, grain, crude, trade, interest, ship, wheat, corn, average}. For LP and AM, we use the output space Y&apos; = YU{ other }. For documents in Dl that are labeled with multiple categories, we initialize ri to have equal non-zero probability for each such category. For example, if document i is annotated as belonging to classes { acq, grain, wheat}, then ri(acq) = ri(grain) = ri(wheat) = 1/3. We created 21 transducti</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M. (1980). An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Raghavan</author>
<author>P Bollmann</author>
<author>G S Jung</author>
</authors>
<title>A critical investigation of recall and precision as measures of retrieval system performance.</title>
<date>1989</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="27360" citStr="Raghavan et al., 1989" startWordPosition="4684" endWordPosition="4687">duction sets. For all the graph-based approaches, we ran a search over K E {2, 10, 50, 100, 250, 500, 1000, 2000, n} (note K = n represents a fully connected graph). In addition, in the case of AM, we set α = 2 for all experiments, and we ran a search over µ E {1e–8, 1e–4, 0.01, 0.1, 1, 10, 100} and v E {1e–8, 1e–6, 1e–4, 0.01, 0.1}, for SGT the search was over c E {3000, 3200, 3400, 3800, 5000, 100000} (see (Joachims, 2003)). We report precision-recall break even point (PRBEP) results on the 3,299 test documents in Table 1. PRBEP has been a popular measure in information retrieval (see e.g. (Raghavan et al., 1989)). It is defined as that value for which precision and recall are equal. Results for each category in Table 1 were obtained by averaging the PRBEP over Category SVM TSVM SGT LP AM earn 91.3 95.4 90.4 96.3 97.9 acq 67.8 76.6 91.9 90.8 97.2 money 41.3 60.0 65.6 57.1 73.9 grain 56.2 68.5 43.1 33.6 41.3 crude 40.9 83.6 65.9 74.8 55.5 trade 29.5 34.0 36.0 56.0 47.0 interest 35.6 50.8 50.7 47.9 78.0 ship 32.5 46.3 49.0 26.4 39.6 wheat 47.9 44.4 59.1 58.2 64.3 corn 41.3 33.7 51.2 55.9 68.3 average 48.9 59.3 60.3 59.7 66.3 Table 1: P/R Break Even Points (PRBEP) for the top 10 categories in the Reuters</context>
</contexts>
<marker>Raghavan, Bollmann, Jung, 1989</marker>
<rawString>Raghavan, V., Bollmann, P., and Jung, G. S. (1989). A critical investigation of recall and precision as measures of retrieval system performance. ACM Trans. Inf. Syst., 7(3):205–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1987</date>
<tech>Technical report,</tech>
<location>Ithaca, NY, USA.</location>
<contexts>
<context position="25795" citStr="Salton and Buckley, 1987" startWordPosition="4396" endWordPosition="4399">(Lewis et al., 1987). The corpus has 9,603 training (not to be confused with D) and 3,299 test documents (which represents Du). Of the 135 potential topic categories only the 10 most frequent categories are used (Joachims, 1999). Categories outside the 10 most frequent were collapsed into one class and assigned a label “other”. For each document i in the training and test sets, we extract features xi in the following manner: stop-words are removed followed by the removal of case and information about inflection (i.e., stemming) (Porter, 1980). We then compute TFIDF features for each document (Salton and Buckley, 1987). All graphs were constructed using cosine similarity with TFIDF features. For this task Y = { earn, acq, money, grain, crude, trade, interest, ship, wheat, corn, average}. For LP and AM, we use the output space Y&apos; = YU{ other }. For documents in Dl that are labeled with multiple categories, we initialize ri to have equal non-zero probability for each such category. For example, if document i is annotated as belonging to classes { acq, grain, wheat}, then ri(acq) = ri(grain) = ri(wheat) = 1/3. We created 21 transduction sets by randomly sampling l documents from the training set with the const</context>
</contexts>
<marker>Salton, Buckley, 1987</marker>
<rawString>Salton, G. and Buckley, C. (1987). Term weighting approaches in automatic text retrieval. Technical report, Ithaca, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sindhwani</author>
<author>P Niyogi</author>
<author>M Belkin</author>
</authors>
<title>Beyond the point cloud: from transductive to semisupervised learning.</title>
<date>2005</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="2768" citStr="Sindhwani et al., 2005" startWordPosition="407" endWordPosition="410">ess constraints derived from the graph (Blum and Chawla, 2001; Zhu et al., 2003; Joachims, 2003; Belkin et al., 2005). Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003). In general graph-based SSL algorithms are non-parametric and transductive.1 A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training. In practice, however, transductive learners can be modified to handle unseen data (Zhu, 2005a; Sindhwani et al., 2005). A common drawback of many graph-based SSL algorithms (e.g. (Blum and Chawla, 2001; Joachims, 2003; Belkin et al., 2005)) is that they assume binary classification tasks and thus require the use of sub-optimal (and often computationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone structured domains such as strings and trees. There are also issues related to degenerate solutions (all unlabeled samples classified as belonging to a single 1Excluding Manifold Regularization (Belkin et al., 2005). 1090 Proceedings of the 2008 Conference on Empirical Methods </context>
</contexts>
<marker>Sindhwani, Niyogi, Belkin, 2005</marker>
<rawString>Sindhwani, V., Niyogi, P., and Belkin, M. (2005). Beyond the point cloud: from transductive to semisupervised learning. In Proc. of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Szummer</author>
<author>T Jaakkola</author>
</authors>
<title>Partially labeled classification with Markov random walks.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>14</volume>
<contexts>
<context position="2061" citStr="Szummer and Jaakkola, 2001" startWordPosition="289" endWordPosition="292">, 1998). Graphbased SSL algorithms are an important class of SSL techniques that have attracted much of attention of late (Blum and Chawla, 2001; Zhu et al., 2003). Here one assumes that the data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph. In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices. Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels from labeled to unlabeled samples (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002) and those that optimize a loss function based on smoothness constraints derived from the graph (Blum and Chawla, 2001; Zhu et al., 2003; Joachims, 2003; Belkin et al., 2005). Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003). In general graph-based SSL algorithms are non-parametric and transductive.1 A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training. In practice</context>
</contexts>
<marker>Szummer, Jaakkola, 2001</marker>
<rawString>Szummer, M. and Jaakkola, T. (2001). Partially labeled classification with Markov random walks. In Advances in Neural Information Processing Systems, volume 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tsuda</author>
</authors>
<title>Propagating distributions on a hypergraph by dual information regularization.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning.</booktitle>
<contexts>
<context position="23701" citStr="Tsuda, 2005" startWordPosition="4048" endWordPosition="4049">IR attempts to minimize the KL-divergence between pi(yi|xi) and PR,(y), the agglomerative distribution for region Ri. Given a graph, one can define a region to be a vertex and its neighbor thus making IR amenable to graph-based SSL. In (Corduneanu and Jaakkola, 2003), the agglomeration is performed by a simple averaging (arithmetic mean). While IR suggests (without proof of convergence) the use of alternating minimization for optimization, one of the steps of the optimization does not admit a closedform solution. This is a serious practical drawback especially in the case of large data sets. (Tsuda, 2005) (hereafter referred to as PD) is an extension of the IR algorithm to hypergraphs where the agglomeration is performed using the geometric mean. This leads to closed form solutions in both steps of the alternating minimization. There are several important differences between IR and PD on one side and our proposed approach: (a) neither IR nor PD use an entropy regularizer, and (b) the update equation for one of the steps of the optimization in the case of PD (equation 13 in (Tsuda, 2005)) is actually a special case of our update equation for pi(y) and may be obtained by setting wij = 1/2. Furth</context>
</contexts>
<marker>Tsuda, 2005</marker>
<rawString>Tsuda, K. (2005). Propagating distributions on a hypergraph by dual information regularization. In Proceedings of the 22nd International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
<author>T Jebara</author>
<author>S-F Chang</author>
</authors>
<title>Graph transduction via alternating minimization.</title>
<date>2008</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="22506" citStr="Wang et al., 2008" startWordPosition="3856" endWordPosition="3859">tives in the case of both AM and EnM make use of the entropy of the unlabeled data, there are several important differences: (a) EnM is not graphbased, (b) EnM is parametric whereas our proposed approach is non-parametric, and most importantly, (c) EnM attempts to minimize entropy while the proposed approach aims to maximize entropy. While this may seem a triviality, it has catastrophic consequences in terms of both the mathematics and meaning. The objective in case of EnM is not convex, whereas in our case we have a convex formulation with simple update equations and convergence guarantees. (Wang et al., 2008) is a graph-based SSL algorithm that also employs alternating minimization style optimization. However, it is inherently squared-loss based which our proposed approach out-performs (see section 5). Further, they do not provide or state convergence guarantees and one side of their update approximates an NP-complete optimization procedure. The information regularization (IR) (Corduneanu and Jaakkola, 2003) algorithm also makes use of a KL-divergence based loss for SSL. Here the input space is divided into regions {Ri} which might or might not overlap. For a given point xi E Ri, IR attempts to mi</context>
</contexts>
<marker>Wang, Jebara, Chang, 2008</marker>
<rawString>Wang, J., Jebara, T., and Chang, S.-F. (2008). Graph transduction via alternating minimization. In Proc. of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1400" citStr="Yarowsky, 1995" startWordPosition="183" endWordPosition="184"> standard tasks, namely Reuters-21578 and WebKB, showing that the proposed algorithm significantly outperforms the state-of-the-art. 1 Introduction Semi-supervised learning (SSL) employs small amounts of labeled data with relatively large amounts of unlabeled data to train classifiers. In many problems, such as speech recognition, document classification, and sentiment recognition, annotating training data is both time-consuming and tedious, while unlabeled data are easily obtained thus making these problems useful applications of SSL. Classic examples of SSL algorithms include self-training (Yarowsky, 1995) and co-training (Blum and Mitchell, 1998). Graphbased SSL algorithms are an important class of SSL techniques that have attracted much of attention of late (Blum and Chawla, 2001; Zhu et al., 2003). Here one assumes that the data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph. In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices. Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2005</date>
<tech>Technical Report 1530,</tech>
<institution>Computer Sciences, University of Wisconsin-Madison.</institution>
<contexts>
<context position="2742" citStr="Zhu, 2005" startWordPosition="405" endWordPosition="406">d on smoothness constraints derived from the graph (Blum and Chawla, 2001; Zhu et al., 2003; Joachims, 2003; Belkin et al., 2005). Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003). In general graph-based SSL algorithms are non-parametric and transductive.1 A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training. In practice, however, transductive learners can be modified to handle unseen data (Zhu, 2005a; Sindhwani et al., 2005). A common drawback of many graph-based SSL algorithms (e.g. (Blum and Chawla, 2001; Joachims, 2003; Belkin et al., 2005)) is that they assume binary classification tasks and thus require the use of sub-optimal (and often computationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone structured domains such as strings and trees. There are also issues related to degenerate solutions (all unlabeled samples classified as belonging to a single 1Excluding Manifold Regularization (Belkin et al., 2005). 1090 Proceedings of the 2008 Confer</context>
<context position="7157" citStr="Zhu, 2005" startWordPosition="1129" endWordPosition="1130">the training set. Given D, most graph-based SSL algorithms utilize an undirected weighted graph G = (V, E) where V = {1, ... , n} are the data points in D and E = V × V are the set of undirected edges between vertices. We use wij ∈ W to denote the weight of the edge between vertices i and j. W is referred to as the weight (or affinity) matrix of G. As will be seen shortly, the input features xi effect the final classification results via W, i.e., the graph. Thus graph construction is crucial to the success of any graph-based SSL algorithm. Graph construction “is more of an art, than science” (Zhu, 2005b) and is an active research area (Alexandrescu and Kirchhoff, 2007). In general the weights are formed as wij = sim(xi, xj)S(j ∈ K(i)). Here K(i) is the set of i’s k-nearest-neighbors (KNN), sim(xi, xj) is a given measure of similarity between xi and xj, and S(c) returns a 1 if c is true and 0 otherwise. Getting the similarity measure right is crucial for the success of any SSL algorithm as that is what determines the graph. Note that setting K(i) = |V |= n results in a fully-connected graph. Some popular similarity measures include sim(xi, xj) = e sim(xi, xj) = cos(xi, xj) = hxi,xji k xi k22</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>Zhu, X. (2005a). Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
</authors>
<title>Semi-Supervised Learning with Graphs.</title>
<date>2005</date>
<tech>PhD thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="2742" citStr="Zhu, 2005" startWordPosition="405" endWordPosition="406">d on smoothness constraints derived from the graph (Blum and Chawla, 2001; Zhu et al., 2003; Joachims, 2003; Belkin et al., 2005). Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003). In general graph-based SSL algorithms are non-parametric and transductive.1 A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training. In practice, however, transductive learners can be modified to handle unseen data (Zhu, 2005a; Sindhwani et al., 2005). A common drawback of many graph-based SSL algorithms (e.g. (Blum and Chawla, 2001; Joachims, 2003; Belkin et al., 2005)) is that they assume binary classification tasks and thus require the use of sub-optimal (and often computationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone structured domains such as strings and trees. There are also issues related to degenerate solutions (all unlabeled samples classified as belonging to a single 1Excluding Manifold Regularization (Belkin et al., 2005). 1090 Proceedings of the 2008 Confer</context>
<context position="7157" citStr="Zhu, 2005" startWordPosition="1129" endWordPosition="1130">the training set. Given D, most graph-based SSL algorithms utilize an undirected weighted graph G = (V, E) where V = {1, ... , n} are the data points in D and E = V × V are the set of undirected edges between vertices. We use wij ∈ W to denote the weight of the edge between vertices i and j. W is referred to as the weight (or affinity) matrix of G. As will be seen shortly, the input features xi effect the final classification results via W, i.e., the graph. Thus graph construction is crucial to the success of any graph-based SSL algorithm. Graph construction “is more of an art, than science” (Zhu, 2005b) and is an active research area (Alexandrescu and Kirchhoff, 2007). In general the weights are formed as wij = sim(xi, xj)S(j ∈ K(i)). Here K(i) is the set of i’s k-nearest-neighbors (KNN), sim(xi, xj) is a given measure of similarity between xi and xj, and S(c) returns a 1 if c is true and 0 otherwise. Getting the similarity measure right is crucial for the success of any SSL algorithm as that is what determines the graph. Note that setting K(i) = |V |= n results in a fully-connected graph. Some popular similarity measures include sim(xi, xj) = e sim(xi, xj) = cos(xi, xj) = hxi,xji k xi k22</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>Zhu, X. (2005b). Semi-Supervised Learning with Graphs. PhD thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="2088" citStr="Zhu and Ghahramani, 2002" startWordPosition="293" endWordPosition="297">rithms are an important class of SSL techniques that have attracted much of attention of late (Blum and Chawla, 2001; Zhu et al., 2003). Here one assumes that the data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph. In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices. Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels from labeled to unlabeled samples (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002) and those that optimize a loss function based on smoothness constraints derived from the graph (Blum and Chawla, 2001; Zhu et al., 2003; Joachims, 2003; Belkin et al., 2005). Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003). In general graph-based SSL algorithms are non-parametric and transductive.1 A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training. In practice, however, transductive lea</context>
<context position="3562" citStr="Zhu and Ghahramani, 2002" startWordPosition="524" endWordPosition="527">s and thus require the use of sub-optimal (and often computationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone structured domains such as strings and trees. There are also issues related to degenerate solutions (all unlabeled samples classified as belonging to a single 1Excluding Manifold Regularization (Belkin et al., 2005). 1090 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090–1099, Honolulu, October 2008. c�2008 Association for Computational Linguistics class) (Blum and Chawla, 2001; Joachims, 2003; Zhu and Ghahramani, 2002). For more background on graph-based and general SSL and their applications, see (Zhu, 2005a; Chapelle et al., 2007; Blitzer and Zhu, 2008). In this paper we propose a new algorithm for graph-based SSL and use the task of text classification to demonstrate its benefits over the current stateof-the-art. Text classification involves automatically assigning a given document to a fixed number of semantic categories. Each document may belong to one, many, or none of the categories. In general, text classification is a multi-class problem (more than 2 categories). Training fully-supervised text clas</context>
<context position="19837" citStr="Zhu and Ghahramani, 2002" startWordPosition="3409" endWordPosition="3412">e equations for p(n) and q(n) are given by 1 3�n−1)(y) pin) (y) = Z exp yz Z , (n) ri(y)δ(i &lt; l) + µPj w�jipjn)(y) qi (y) = δ(i &lt; l) + µ P , j w� ji where Xγi = ν + µ j (y) = −ν + µ X wij(log q(n−1) 0 j (y) − 1) j and where Zi is a normalizing constant to ensure pi is a valid probability distribution. Note that each iteration of the proposed framework has a closed form solution and is relatively simple to implement, even for very large graphs. Henceforth we refer to the proposed objective optimized using alternating minimization as AM. 4 Connections to Other Approaches Label propagation (LP) (Zhu and Ghahramani, 2002) is a graph-based SSL algorithms that performs Markov random walks on the graph and has a straightforward extension to multi-class problems. The update equations for LP (which also we use for our LP implementations) may be written as (n) ri(y)b(i &lt; l) + δ(i &gt; l) Pj wijp�n 1)(y) pi (y) = δ(i &lt; l) + δ(i &gt; l) Pj wij Note the similarity to the update equation for q(n) i in our AM case. It has been shown that the squaredloss based SSL algorithm (Zhu et al., 2003) and LP have similar updates (Bengio et al., 2007). The proposed objective C1 is similar in spirit to the squared-loss based objective in </context>
<context position="24661" citStr="Zhu and Ghahramani, 2002" startWordPosition="4204" endWordPosition="4207">r IR nor PD use an entropy regularizer, and (b) the update equation for one of the steps of the optimization in the case of PD (equation 13 in (Tsuda, 2005)) is actually a special case of our update equation for pi(y) and may be obtained by setting wij = 1/2. Further, our work here may be easily extended to hypergraphs. 5 Results We compare our algorithm (AM) with other state-of-the-art SSL-based text categorization algorithms, namely, (a) SVM (Joachims, 1999), (b) Transductive-SVM (TSVM) (Joachims, 1999), (c) Spectral Graph Transduction (SGT) (Joachims, 2003), and (d) Label Propagation (LP) (Zhu and Ghahramani, 2002). Note that only SGT and LP are graph-based algorithms, while SVM is fullysupervised (i.e., it does not make use of any of the unlabeled data). We implemented SVM and TSVM using SVM Light (Joachims, b) and SGT using SGT Light (Joachims, a). In the case of SVM, TSVM and SGT we trained |Y |classifiers (one for each class) in a one vs. rest manner precisely following (Joachims, 2003). 5.1 Reuters-21578 We used the “ModApte” split of the Reuters-21578 dataset collected from the Reuters newswire in 1095 1987 (Lewis et al., 1987). The corpus has 9,603 training (not to be confused with D) and 3,299 t</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Zhu, X. and Ghahramani, Z. (2002). Learning from labeled and unlabeled data with label propagation. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="1598" citStr="Zhu et al., 2003" startWordPosition="214" endWordPosition="217">unts of labeled data with relatively large amounts of unlabeled data to train classifiers. In many problems, such as speech recognition, document classification, and sentiment recognition, annotating training data is both time-consuming and tedious, while unlabeled data are easily obtained thus making these problems useful applications of SSL. Classic examples of SSL algorithms include self-training (Yarowsky, 1995) and co-training (Blum and Mitchell, 1998). Graphbased SSL algorithms are an important class of SSL techniques that have attracted much of attention of late (Blum and Chawla, 2001; Zhu et al., 2003). Here one assumes that the data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph. In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices. Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels from labeled to unlabeled samples (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002) and those that optimize a loss function based on smoothness constraints derived from the graph (Blum and Chaw</context>
<context position="13693" citStr="Zhu et al., 2003" startWordPosition="2314" endWordPosition="2317"> common drawback of a large number of state-of-the-art classifiers that tend to be confident even in regions close to the decision boundary. We conclude this section by summarizing some of the features of our proposed framework. It should be clear that C1 uses the “manifold assumption” for SSL (see chapter 2 in (Chapelle et al., 2007)) — it assumes that the input data can be embedded within a low-dimensional manifold (the graph). As the objective is defined in terms of probability distributions over integers rather than just integers (or to real-valued relaxations of integers (Joachims, 2003; Zhu et al., 2003)), the framework generalizes in a straightforward manner to multi-class problems. Further, all the parameters are estimated jointly (compare to one vs. rest approaches which involve solving |Y |independent problems). Furthermore, the objective is capable of handling label training data uncertainty (Pearl, 1990). Of course, this objective would be useless if it wasn’t possible to efficiently and easily optimize it on large data sets. We next describe a method that can do this. 3 Learning with Alternating Minimization As long as µ, ν &gt; 0, the objective C1(p) is convex. This follows since Dxz(pi|</context>
<context position="20299" citStr="Zhu et al., 2003" startWordPosition="3500" endWordPosition="3503"> the proposed objective optimized using alternating minimization as AM. 4 Connections to Other Approaches Label propagation (LP) (Zhu and Ghahramani, 2002) is a graph-based SSL algorithms that performs Markov random walks on the graph and has a straightforward extension to multi-class problems. The update equations for LP (which also we use for our LP implementations) may be written as (n) ri(y)b(i &lt; l) + δ(i &gt; l) Pj wijp�n 1)(y) pi (y) = δ(i &lt; l) + δ(i &gt; l) Pj wij Note the similarity to the update equation for q(n) i in our AM case. It has been shown that the squaredloss based SSL algorithm (Zhu et al., 2003) and LP have similar updates (Bengio et al., 2007). The proposed objective C1 is similar in spirit to the squared-loss based objective in (Zhu et al., 2003; Bengio et al., 2007). Our method, however, differs in that we are optimizing the KL-divergence over probability distributions. We show in section 5 that KL-divergence based loss significantly outperforms the squared-loss. We believe that this could be due lim min α→∞ p,q 0 wij, 1094 to the following: 1) squared loss is appropriate under a Gaussian loss model which may not be optimal under many circumstances (e.g. classification); 2) KL-div</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). Semisupervised learning using gaussian fields and harmonic functions. In Proc. of the International Conference on Machine Learning (ICML).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>