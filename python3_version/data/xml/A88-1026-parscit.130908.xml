<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000074">
<note confidence="0.619736166666667">
LUKE: AN EXPERIMENT IN THE EARLY INTEGRATION
OF NATURAL LANGUAGE PROCESSING
David A. Wroblewski, Elaine A. Rich
MCC
Human Interface Laboratory
3500 West Balcones Center Drive
</note>
<sectionHeader confidence="0.696456" genericHeader="abstract">
Austin, Texas 78759
Abstract
</sectionHeader>
<bodyText confidence="0.99930544">
Luke is a knowledge editor designed to support two
tasks; the first is editing the classes and relations in a
knowledge base. The second is editing and maintaining the
semantic mapping knowledge neccesary to allow a natural
language interface to understand sentences with respect to
that knowledge base. In order to emphasize design
decisions shared between the two tasks, Luke provides
facilities to concurrently debug the application and the
natural language interface. Luke also makes natural
language available in its own user interface. This makes it
possible for a knowledge base builder to exploit natural
language both as a way of locating desired concepts within
the knowledge base and as a a way of doing consistency
checking on the knowledge base as it is being built.
base, Luke makes natural language available in
its own interface. This makes it possible for the
knowledge base builder to exploit natural
language both as a way of referring to objects in
the knowledge base and as a way of doing
consistency checking on the objects themselves.
In this paper, we will describe both what Luke
does and how doing that supports this
productive view of the interaction between
building a knowledge based system and building
an associated natural language interface.
</bodyText>
<sectionHeader confidence="0.9554215" genericHeader="keywords">
Background And Motivation
Introduction
</sectionHeader>
<bodyText confidence="0.999959935483871">
Luke is a knowledge base editor that has
been enhanced to support entering and
maintaining the semantic mappings needed by a
natural language interface to a knowledge base.
Thus Luke supports a team of system builders
who are simultaneously building a knowledge-
based program and building a natural language
interface to that program. It makes sense for a
single tool to support both of these efforts
because the efforts themselves are logically
intertwined in two important ways, both of which
result from the fact that the application program
and its NL interface must share a single
knowledge base. (This sharing is necessary
because otherwise the NL system will not be
able to communicate with the application). The
first way in which the two efforts Luke supports
are related is that, although they produce two
systems that are different and may thus place
different demands on their associated
knowledge bases, both must share a single such
knowledge base. By supporting the early
integration of the application program and the
NL interface as this single knowledge base is
being built, Luke helps to ensure that it will be
adequate, with respect to both its content and its
structure, to support both these target tasks.
The second way in which the two system
building tasks are related is that one can support
the other. By associating natural language with
concepts as they are entered into a knowledge
</bodyText>
<subsectionHeader confidence="0.311521">
A Model Of Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.976342321428572">
All of the following discussion is based on a
model of semantic analysis similar to that
proposed in (Hobbs, 1985). Under this model,
syntactic and semantic analysis are done as
separate operations. The first stage of semantic
analysis is a conversion to initial logical form, in
which the surface content of the sentence is
encoded in a set of expressions that look like
logical terms, but whose predicates are taken
directly from the words used in the sentence.
Initial logical form captures the predicational
structure of the sentence, without expressing it
in terms of the knowledge base.
Once produced, the expressions in initial
logical form are individually translated into final
logical form, which is a set of first-order terms
whose predicates are those used in the
application&apos;s knowledge base. The translation
from initial logical form to final logical form is
done via a set of rules known as semantic
mappings, and it is the acquisition of these
semantic mappings that is the subject of this
paperl. The control of and exact details of
semantic mappings are irrelevant for this
lln reality, we further subdivide the semantic mappings
into mappings and compoundings. Mappings we described
above. Compoundings are rules that specify how two nouns
can be compounded.
</bodyText>
<page confidence="0.997544">
186
</page>
<bodyText confidence="0.999524545454545">
discussion; it is enough to know that semantic
mappings roughly translate from the surface
form of the English input to expressions built in
terms of the target knowledge base.
The general form of a semantic mapping is
shown below, along with several examples. A
semantic mapping is a rule for translating one
initial logical form into zero or more final logical
forms. A semantic lexicon is then a collection of
semantic mappings that specify translations for
the words in the syntactic lexicon.
</bodyText>
<figure confidence="0.819230666666667">
Generally:
ilf --&gt; fit flf2, flfn
Examples:
(dog ?x) --&gt; (canine ?x) (1)
(make ?i ?x ?y) --&gt; (2)
(creating ?i)
(agent ?i ?x)
(object ?i ?y)
(graphic-obj ?y)
</figure>
<bodyText confidence="0.99844959375">
A mapping for the noun &amp;quot;dog&amp;quot; is shown in (1).
This rule states that the referent of a noun
phrase whose head is &amp;quot;dog&amp;quot; must be a member
of the class canine. Mapping (2) shows that
sortal restrictions can be included in the
mapping, in this case restricting the direct object
of the verb &amp;quot;make&amp;quot; to be a member of the class
graphic-obj. An ILF may match the left hand
side of many semantic mappings, and so
ambiguity is captured in the semantic lexicon.
In our model of semantic analysis, these
semantic mappings are used to build a picture of
what was said in the sentence by posting
constraints. In fact, each semantic mapping
exploits two kinds of constraints. Lexical
constraints define the applicability of a mapping
as a function of the words that appear in a
sentence. These constraints always appear on
the left hand side of a semantic mapping.
Knowledge-base constraints define the
applicability of a mapping as a function of the
meanings of the current word, as well as the
other words in a sentence. These constraints
always appear on the right hand side of a
semantic mapping. Viewed this way, mapping
(1) constrains the referent of &amp;quot;a dog&amp;quot; (or &amp;quot;the
dog&amp;quot; or any noun phrase with &amp;quot;dog&amp;quot; as its head)
to be a member of the class canine, but does
not specify what (if any) specialization of
canine the referent might refer to. For
example, it does not commit to the class
schnauzer versus the class dachshund.
</bodyText>
<subsectionHeader confidence="0.930803">
Past Experience
</subsectionHeader>
<bodyText confidence="0.999992142857143">
Our early attempts at porting our natural
language understanding system, Lucy (Rich,
1987), consisted of &amp;quot;hand-crafting&amp;quot; a set of
semantic mappings for an existing knowledge
base. The application program was an
intelligent advice system (Miller, 1987) that
would accept questions from a user about
operating a statistical analysis program and try
to provide advice based on its knowledge of the
program&apos;s interface and internal structure.
Creating the semantic mappings was a long
and tedious chore. Starting with a mostly-
complete knowledge base, finding the correct
semantic mappings was a matter of knowledge
acquisition, in which we asked the knowledge
base designers what knowledge structure a
particular word might map onto. Many times this
was almost as difficult for the knowledge base
designers as it was for the &amp;quot;semanticians&amp;quot;, since
the knowledge base was quite large, and
developed by several people. Often, the
knowledge base designer being interviewed was
not familiar with the area of the knowledge base
being mapped into, and thus could not
accurately answer questions, especially with
respect to completeness (i.e., &amp;quot;this is the only
class that the word could map into.&amp;quot;)
Furthermore, defining the semantic mappings
often uncovered inconsistencies in the
knowledge base. When this happened, it was
not always immediately clear what the correct
action was; we could either fix the knowledge
base or live with the inconsistencies (which
usually meant semantic ambiguity where none
was really necessary.)
Even worse, there were many cases where
defining any semantic mapping was problematic.
In these cases, representational decisions that
had already been made either precluded or
made very difficult any principled mapping of
English expressions into the knowledge base.
This happened when information was needed to
analyze a syntactic constituent (perhaps a noun
phrase like &amp;quot;the mouse&amp;quot;) but the referent of the
constituent (the mouse icon on the screen), was
not represented in the knowledge base. Thus,
no semantic mapping could be written. The
problem could be solved by simply introducing
the relevent knowledge, but sometimes a better
solution would have involved redesigning a
portion of the knowledge base to represent more
clearly important features of the domain.
Usually this was too costly an option to consider.
Finally, we quickly discovered that the dream
of establishing the semantic mappings once and
for all was a fallacy. Any significant knowledge
</bodyText>
<page confidence="0.996868">
187
</page>
<bodyText confidence="0.999901857142857">
operations performed at the editor interface are
translated into a series of function calls via a
well-defined functional interface to the
knowledge representation system. The base
editor is a complete system: it can be run
independently of any of the extensions
described hereafter. The base editor knows
nothing of the Lucy natural language
understanding system.
base is &amp;quot;under construction&amp;quot; for a long period of
time; introducing semantic mappings before the
knowledge base is totally done necessarily
implies maintenance of the semantic mappings
in the face of a changing knowledge base. This
is a paradox: on the one hand, it would be best
to have a completed knowledge base before
doing any semantic mapping. On the other
hand, to avoid problematic semantic mappings it
would be best to introduce semantic mappings
and &amp;quot;debug&amp;quot; them as early as possible in the
development of the knowledge base.
</bodyText>
<subsectionHeader confidence="0.822848">
The Dual-Application Development
Model
</subsectionHeader>
<bodyText confidence="0.99993203030303">
In order to avoid the problems mentioned in
the last section, Luke endorses a
dual-application model of the development
process. Under such a model, there are two
related applications being developed. One is a
natural language interface (NLI), responsible for
forming a syntactic, semantic, and pragmatic
analysis of user input, and passing the
interpreted input to the domain application. The
domain application, of course, could be
anything. We focused on knowledge-based
applications so that we could assume that a
knowledge base was a part of the domain
application. We assume that the natural
language understanding component and the
domain component both have access to the
knowledge base, and that semantic analysis
should be done with respect to that knowledge
base.
The dual-application model highlights the
design interplay between the domain application
and the interface. In particular, knowledge base
design decisions motivated exclusively by the
domain application or the NLI, without regard for
the other application, are likely to be inadequate
in the final, integrated, system. Such ill-informed
decisions might be avoided in a development
environment that allows the earliest possible
integration of the applications. Luke is our first
attempt to provide such an environment, and is
built to support the work done during early
prototyping and full-scale development of an
application.
</bodyText>
<subsectionHeader confidence="0.971016">
Luke&apos;s Architecture
</subsectionHeader>
<bodyText confidence="0.9854548">
Luke is an enhanced version of a simple
knowledge editor, as illustrated in Figure 1. In
the discussion that follows, we will refer to this
as the base editor, because it forms the
foundation upon which Luke is built. All
</bodyText>
<figure confidence="0.987818333333333">
NL
Analysis
Algorithms
</figure>
<figureCaption confidence="0.999983">
Figure 1: Luke&apos;s Architecture
</figureCaption>
<bodyText confidence="0.999382736842105">
The base editor allows two types of
commarids: terminological and assertional
commands2. These terms are taken from
(Brachman, 1983), which defines a knowledge
base as consisting of two &amp;quot;boxes&amp;quot;. The Tbox
holds the terminological information of the
knowledge base, information that defines what
symbols are valid class identifiers, and what the
names, arities, domains and ranges of those
relations are. Brachman and Levesque liken the
terminological knowledge to the &amp;quot;noun phrases&amp;quot;
of the knowledge base.
2Actually, there is at least one other type of command:
management. Management commands handle such prosaic
issues as saving and loading knowledge bases. While these
commands will not be described in detail in this paper, the
reader should be aware that a significant effort was also
required to upgrade these to handle managing both the
knowledge base and the semantic lexicon.
</bodyText>
<page confidence="0.997835">
188
</page>
<tableCaption confidence="0.93165">
Table 1: Knowledge Editing Operations
</tableCaption>
<table confidence="0.980620363636364">
and Their Effects
Operation Semantic Lexicon Effect
Create Class New mappings possible.
Create Slot Old mappings may have to be refined.
Delete Class Existing mappings may be invalid
because they refer to a now nonexistent class.
Delete Slot Some existing mappings may be invalid
because they refer to a now nonexistent slot.
Attach Superclass Some existing mappings may be invalid
Detach Superclass because inheritance paths have changed.
Rename (anything) Existing mappings may be invalid due to renaming.
</table>
<bodyText confidence="0.999556383333334">
The Abox holds assertional information,
described by using logical connectives such as
&amp;quot;and&amp;quot;, &amp;quot;or&amp;quot; and &amp;quot;not&amp;quot; and the predicates defined
in the Tbox to form logical sentences. While the
terminological component describes what it is
possible to say, the assertional component holds
a theory of the world: a set of axioms describing
the valid inferences in the knowledge base.
As shown in Figure 1, Luke extends the base
editor by additionally maintaining a semantic
lexicon. Each time an operation is performed on
the knowledge base, Luke must update the
semantic lexicon so that the set of semantic
mappings it contains remains consistent with the
updated knowledge base. Table 1 shows some
operations and their effect on the semantic
lexicon.
As can be seen from this table, operations
that change the terminological content of the
knowledge base (such as Create Class or
Create Slot) may change the number or
structure of the semantic mappings known. For
example, consider the case of the create
Class command. By adding a new class to the
knowledge base, we have extended the Thor
since the knowledge base is now able to
describe something it could not describe before,
some English noun phrases that were previously
uninterpretable can now be mapped into this
class. Existing mappings may have to be
changed, since the act of adding a class may
constitute a refinement of an existing class and
its associated mappings.
For instance, one might add a set of
subclasses of canine where none used to
exist. If the current set of semantic mappings
map &amp;quot;poodle&amp;quot; and &amp;quot;doberman&amp;quot; into canine,
then these rules may have to be refined to map
into the correct subclass. Extending the
terminological component of the knowledge
base extends the range of or precision with
which syntactic constituents may be
semantically analyzed.
Operations that alter the Abox have less well-
defined effects on the semantic lexicon. For
instance, without detailed knowledge of the
domain application and the domain itself, the
addition of an inference rule to the knowledge
base implies nothing about the possible
semantic mappings or the validity of current
mappings. In general, it is very difficult to use
the assertional component of a knowledge base
during semantic processing; for this reason, we
will concentrate on terminological operations for
the remainder of this paper.
Luke, then, is a &amp;quot;base editor&amp;quot; extended to
account for the semantic mapping side effects of
knowledge editing operations. Luke reacts in
predictable ways to each editing operation,
based on the information shown in Table 1:
</bodyText>
<listItem confidence="0.997835615384615">
• New mappings possible: Luke reacts to
this condition by conducting an &amp;quot;interview&amp;quot;
with the user. Each interview is designed
to collect the minimum information
necessary to infer any new semantic
mappings. In a word, the response to
possible new mappings is &amp;quot;acquisition&amp;quot;.
• Old mappings possibly invalid: Luke
reacts to this condition by trying to identify
the affected mappings and requesting the
user verify their correctness. In a word, the
response to possibly invalid mappings is
&amp;quot;verification&amp;quot;.
</listItem>
<page confidence="0.99014">
189
</page>
<table confidence="0.994037588235294">
MCC Luke
Close Store Revert Show Hook Configure
MI AA • 01 ,-.. i Aa 0 to- 3-C
FICTION.0111PUTINC USER-AGENDA (6 tasks) (an Instance a FICEMe)
RINTINC 42 tasks:
._ -DAVE-ME-NON-USER I
-. - :i Verify napping CORTR-1 (jb) 11/2508
11111111-. - - - -ELRINE-rHE-USER 1 i Verify new mord -DATA- (jb) 11/25/11
,... Verify new mord -COMPUTATION- (jb)
--SUSAN-rHE-CURRENT-USER Verify new mord -COMPUTE- (jb) 1102
TOP -.. -COMPUrER-2 I Verify napping N.FILE-1 (jb) 11/25/11
THIN ROCESSOZ■ .c --.: :: : ..... -GONPUrER-1 , Verify new mord -FILE- (jb) 1102508
011PUTEA-EQUIP _. -PRINTER-FOR-2-2 , Define nouns FILE-STAUCT (jb) 11/250
__, -- H Define nouns ORTA-STAUCT (jb) II/150
UTPUT-0Elt.. = - -PRItirER-FOR-CONPUTER-1
-.FREE-PRINTER
ATA-STAUCt-STAUCT
-----FILE
[1 131 ‘31 1% AA 0 ti4&amp;quot;. 3C
TASK
-63 CS tasks) (an instance of VF_RIFY-Slen
Verify napping PREP.FOR-1 (dew) 11/1
via
Referred to David Wroblewski jb
Jim Sarmat 11/25/87 19:26:53:
Please take a look at this maw
Jim Barnett 11/25/87 19:26:17:
Could this napping rule be combi
30 Search For (a quoted noun phrase, for nnnnn le, &apos;a dog&apos;) the current user&apos;
For interpretation 1 of &apos;the current user&apos; (see also frame AESPONSEI1S)
Query: Looking for 7X-117 such that:
(ACCOUNTS-ON 7X-117 73-120)
(CURRENTLY-OCCED-IN-ON TX-117 ?3-12I)
(CI:MEMBER ?3-12I PROCESSOR)
(CUMEMBEA 7X-117 NARA)
Rnsmers:
SUSAN-THE-CURRENT-USER
Click left on any displayed value to inspect it.
DO II
*1 El .4 AA 0 164- 3—c
PREP.FOR-1 (1 task) (an Instance of PREPOSITICee-MRPP
SOURCE: ((PREP.FOR
CATEGORY-TRIGGER: :PREP
,, LEXICAL-TRIGGER: -FOR-
LEXICAL-CONSTRAINTS: ((:I . :ANY
, CATEGORY-CONSTRAINTS: ((:I . :ANY
, VARIABLES: (:I :X :Y)
EXISTENTIAL-VARIABLES: NIL
LHS: (FOR :I :X
RHS: ((OUTPUT-DE
VALID: T
</table>
<figure confidence="0.674425">
6
.
</figure>
<figureCaption confidence="0.996419">
Figure 2: The Luke Window
</figureCaption>
<bodyText confidence="0.933324413043478">
Base Editor Facilities: Windows and
Agendas
Figure 2 shows the screen as it might typically
appear during an editing session with Luke. The
user is provided with a suite of inspectors to
display the class hierarchy or view individual
frames in detail. Each inspector provides an
iconic menu of operatioos that can be performed
on it or its contents. Components of frames in
the inspectors, such as the names of slots, are
mouse-sensitive and provide the main
machanism for editing the frames themselves.
Also provided is an agenda of tasks to be
performed. A user may manually queue up
tasks to perform as reminders, annotate tasks,
or refer tasks to other members of the
development team. Tasks may be scheduled
automatically as a side effect of various editing
commands. There are two main types of tasks:
verification tasks and acquisition tasks.
Verification tasks are reminders to inspect some
part of the knowledge base to ensure its
consistency. Acquisition tasks are (typically)
interviews that Luke has requested with the
user.
The base editor also provides a method of
delaying tasks. Some tasks, such as acquisition
tasks, are started at a default time, usually
immediately after the action that inspired them.
The user has the option, at any point during the
task, of pressing the delay key, causing the task
to be stopped, and an agenda item created for it
if none already exists. Through this delaying
mechanism, the user has control of when tasks
are executed.
The agenda is shown in the upper right
inspector in Figure 2. It is implemented as a
frame (an instance of the built-in class agenda,
and may be inspected via the normal editing
commands of the base editor. Each task is
represented as an instance of the class task,
and includes a description of the event that
inspired it. Although the base editor makes very
little use of the agenda mechanism, Luke
schedules a large number of interviews and
verification tasks through the agenda.
</bodyText>
<page confidence="0.993506">
190
</page>
<subsectionHeader confidence="0.881717">
User Tasks, User Models
</subsectionHeader>
<bodyText confidence="0.999993153846154">
Luke is different from most other tools of its
kind for three reasons. It provides support for
both the acquisition and maintenance of
semantic mappings. Because it then knows
those semantic mappings, it makes natural
language available in its own interface. And in
order to do these things, it must assume more
sophistication on the part of its users. The
intended users of Luke are members of a
knowledge engineering team. These people are
assumed to be familiar with the content and
structure of the knowledge base, or to be
capable of discovering what they need to know
by inspecting the knowledge base. Although
they are not assumed to have an extensive
linguistics background nor extensive familiarity
with the implementation of the semantic
processing algorithms of Lucy, they are
assumed to have a &amp;quot;qualitative model&amp;quot; of
semantic processing (as presented earlier).
Moreover, since we assume that a team of
engineers will be building the applications, some
with special interests or talents, tasks that might
require greater linguistic sophistication may be
delayed until the &amp;quot;linguistics specialist&amp;quot; can be
brought in.
Luke provides tools for the acquisition of
semantic mappings and the maintenance of
those mappings once collected. Although
traditionally, little attention has been paid to the
latter task, we believe that it may prove to be the
more important of the two; once a large base of
mappings has been established, it is only
practical to maintain them with tools specifically
designed for that task. The next part of of this
section will describe tools provided by Luke for
both tasks. Then the remainder of the section
will show how these mappings can be used to
inhance the user interface of Luke itself.
</bodyText>
<subsectionHeader confidence="0.795653">
Acquiring Semantic Mappings
</subsectionHeader>
<bodyText confidence="0.9995705">
The Luke acquisition modules are built with
the following design guidelines:
</bodyText>
<listItem confidence="0.97451">
1. Perform acquisition tasks temporally
near the event that causes them.
2. Allow the user to delay acquisition at will.
3. Allow the user to specify the minimum
information from which semantic
mappings can be deduced.
4. Remember that people are better at
verifying a proposed structure than they
are at creating correct structures from
scratch.
5. Try to repay the user for the work
expended in the interviews by using the
semantic mappings for knowledge base
debugging, navigation, and consistency
checking.
6. Project a correct model of semantic
processing to the user throughout the
acquisition process.
</listItem>
<bodyText confidence="0.996222395833333">
In the Luke environment, acquiring semantic
mappings turns out to be quite simple. The
scheme we use in Luke involves a three-stage
process. In the first stage, Luke collects
associations. Simply put, an association is a
triple of the form
&lt;word, part-of-speech, structure&gt;
In the second stage, a set of heuristics inspects
the associations and compiles them into
semantic mappings. For instance, the
association &lt;&amp;quot;dog&amp;quot;, noun, canine&gt; might be
built during acquisition to indicate that some
noun sense of the word &amp;quot;dog&amp;quot; maps into the
class canine. In the final stage, the mapping
rule deduced from the association is built,
presented to the user for refinement via a
special mapping editor, and entered into the
semantic lexicon. Occassionally, Luke uses the
new mapping to inspire other mappings, such as
the nominalizations of a verb. In this case, once
a verb mapping is known, nomimalizations of it
are collected and created in the same manner,
and heuristics take advantage of the fact that the
new nouns are nomimalizations of a verb whose
mapping is known. Thus the constraints on the
complements of the verb are used to generate
mappings for prepositions that can be used to
specify the complements of the nominalization of
that verb.
Although the basic acquisition technique is
simple, obeying guideline 6 can be tricky. For
instance, in an early version of Luke we
temporally separated the interviews from the
heuristic construction of associations. Further,
we did not submit the mappings to the user
when they were guessed. The mappings were
guessed later, in a background process, usually
invisible to the Luke user. Yet semantic
analyses often succeeded, giving users the
impression that the associations were driving the
semantic analysis routines, not the semantic
mappings deduced from them. With such a
model of the process, the user was confused
and unprepared when semantic mappings
(&amp;quot;where did they come from?&amp;quot;) were incorrect
and had to be inspected, debugged, and edited.
In the current version of Luke, the semantic
mappinas are presented to the user at the end
</bodyText>
<page confidence="0.996896">
191
</page>
<bodyText confidence="0.992635">
of the interview, to be reviewed and edited
immediately. Connecting the process of
associating with the mapping creation process
leads to much less confusion.
</bodyText>
<subsectionHeader confidence="0.992397">
Managing the Semantic Lexicon
</subsectionHeader>
<bodyText confidence="0.996386363636364">
Once a semantic lexicon exists, maintaining it
becomes a significant chore. During routine
knowledge base editing a user may change the
terminological content in such a way that
existing semantic mappings become invalid.
Deleting a class, for example, clearly makes any
semantic mappings that mention it incorrect. If a
large semantic lexicon exists, changing the
terminological content of the knowledge base
may entail editing a very large number of
semantic mappings.
Luke provides a number of tools to help
manage the semantic lexicon. These tools fall
roughly into two categories, those that support
editing and those that aid in consistency
checking. The editing tools allow a user to
request all the mappings that target a specific
frame, or all the mappings that map from a given
surface form, via a special mappings browser.
Users may edit semantic mappings at any time
using the ordinary editing tools of the base
editor, because semantic mappings themselves
are stored as frames in the knowledge base.
The biggest maintenance service Luke
provides is consistency checking. When a
frame is deleted, entered, or specialized in the
knowledge base, or after any terminological
editing operation, Luke collects all of the
semantic mappings that might be affected and
creates a set of tasks to verify their continuing
correctness. As always, the user can choose to
handle such tasks immediately, or delay for later
consideration.
</bodyText>
<subsectionHeader confidence="0.978125">
Exploiting Natural Language in Luke Itself
</subsectionHeader>
<bodyText confidence="0.998426383561644">
The overall goal in building Luke is to provide
a set of &amp;quot;power tools&amp;quot; (Sheils, 1983) that support
the dual application model, and Luke is our first
step in that direction. One potential problem in
Luke&apos;s design is increasing the overhead of
building a knowledge base, since various tasks
are continually scheduled for the user. This fear
is mitigated by the following observations. First,
the added overhead doesn&apos;t represent extra
work to be done by the user, only a different
time for the user to do it. If there is to be a NLI
for the application, then the developer is in a
&amp;quot;pay me now or pay me later&apos; bind, where late
payment can be very costly. Viewed this way,
Luke is helping the user trade a short-term loss
(interviews and verification tasks during editing)
for a long-term gain (smaller NLI development
effort after the domain application is finished).
Second, with the additional information provided
by concurrently developing the NLI and the
domain knowledge base, Luke can &amp;quot;pay back&amp;quot;
the user at editing time by strategically using this
information to support both extending and
debugging a knowledge base. In the rest of this
section we describe some of the ways in which
this is done.
Luke provides the Search For command,
which accepts a noun phrase as its argument.
Search For converts that noun phrase into a
knowledge base query by using the Lucy natural
language understanding system. The noun
phrase is parsed and semantically analyzed
using any known semantic mappings. When the
resulting query is executed, the matching frames
are stored into a response frame, along with
information concerning what mappings were
used in the interpretation process. Then the
user is presented the frames in a menu. Thus,
Search For provides both a way of exercising
the semantic mappings and retrieving frames
from the knowledge base during normal editing.
Note that such &amp;quot;retrieval by description&amp;quot; facilities
are not usually provided in knowledge editors
because it would require a sophisticated query
language allowing abstraction and arbitrary user
extensions. Because Luke already has access
to a natural language analysis component,
providing this service to the user is
straightforward. Also note that such a service is
vital to editing and maintaining large knowledge
bases -- finding a frame using just graphical
displays of the class hierarchy and detailed
single-frame displays does not provide any sort
of &amp;quot;random access&amp;quot; capabilities, and finding a
specific frame using only such tools can be very
difficult.
Luke also provides a method of testing the
analysis of entire sentences. The developer can
submit a sentence for analysis to the NLI
processing algorithms. The analysis of the
sentence is returned as a frame in the
knowledge base, recording the interpretations
found, and a record of the mappings used to get
the interpretations. This can be further
processed by a &amp;quot;default command loop&amp;quot; used to
simulate the behaviour of the application
program. Using this facility, it is easy for the
application developer to place her/himself in the
place of the application program, and to envision
the sorts of responses neccesary.
Furthermore, the process of interviewing is a
form of documentation. During an editing
session, the user leaves throughout the
</bodyText>
<page confidence="0.994458">
192
</page>
<bodyText confidence="0.994747902439025">
knowledge base a &amp;quot;trail&amp;quot; of semantic hints that
venous customized commands can take
advantage of. For instance, the Show
Associated Nouns command pops up a quick
menu of words associated with the frame in
question, providing a handy documentation
function.
variety of ways, including scheduling, executing,
annotating, or referring them between members
of the development team.
Future Plans
Finally, Luke can catch several knowledge
editing mistakes that the base editor cannot.
One of the most common is class duplication --
unwittingly creating a class intended to
represent the same set of entities as an already-
existing class. Often this happens when the
knowledge base is being built by a team of
people or because it has grown too complex for
an individual to visualize. Luke helps solve the
problem using the existing semantic mappings.
After associating a noun with a class, Luke
warns the user of the total number of mappings
for that noun and some indication of the frames
it might map into. This simple mechanisms
detects many cases of class duplication.
At present, Luke is a useful, competent
knowledge editor and provides a substrate of
tools for concurrently managing the
development of an application knowledge base
and the NLI that will ultimately operate with it.
Ultimately, we hope to make Luke itself a
knowledge-based program, adding to it the
heuristics that an &amp;quot;expert NLI engineer&amp;quot; might
have, and expanding its role to that of an
intelligent assistant. The groundwork is laid for
such a step; Luke is already driven by a model
of itself, the knowledge base, Lucy&apos;s algorithms,
and its users. In the near term we plan to
expand and refine the role that such knowledge
plays in Luke&apos;s operation.
</bodyText>
<subsectionHeader confidence="0.662873">
Comparison To Other Work
</subsectionHeader>
<bodyText confidence="0.9999638125">
Luke appears to be different than previous
systems of its ilk in a number of ways. Most
importantly, Luke is built to support the dual-
application model of development. Systems
such as TEAM (Grosz, 1987), TELI (Ballard,
1986), and to a lesser degree, IRACQ (Ayuso,
1987), all aim for portability between existing,
untouchable, applications (usually DBMS&apos;s).
These tools have generally emphasized building
a database schema in order to supply the
(missing) terminological component of the
database. We have rejected such an approach
on the grounds that it is only useful for building
sentence-to-single-command translators, not for
wholesale integration of a NLI with an
application. Luke is an attempt to help design in
the natural language interface from the start.
Because of this basic assumption, Luke is
more oriented toward users as sophisticated
system builders than as linguistically naive end-
users or &amp;quot;database experts&amp;quot;. Luke users will
understand some linguistics, either by
educational background, hands-on experience,
or special primers and training.
Finally, Luke is designed to support a team of
users, not a single user. Luke provides a
flexible agenda and task management system
that allows users to handle tasks for reviewing
existing mappings, investigating potential
conflicts in the semantic lexicon, and creating
new mappings for new objects in the knowledge
base. Such tasks can be operated on in a
</bodyText>
<sectionHeader confidence="0.998301" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999354555555555">
The ideas in this paper are the product of the
entire LINGO team. Mike Barnett designed and
implemented the agenda facility described
herein, and Kevin Knight designed some of the
semantic debugging aids. Additionally, most of
the ideas about the way this version Luke
operates sprang from a working group including
Mike Barnett, Jim Barnett, Kevin Knight, and the
authors.
</bodyText>
<sectionHeader confidence="0.998826" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999384823529412">
Damaris M. Ayuso, Varda Shaked and Ralph
M. Weischedel. (July 1987). An
Environment For Acquiring Semantic
Information. Proceedings of the 25th Annual
Meeting of the Association of Computational
Linguistics. .
B.W. Ballard and D.E. Stumberger. (1986).
Semantic Acquisition in TELI: A
Transportable, User-Customized Natural
Language Processor. Proceedings of the
24th Annual Meeting of the Association of
Computational Linguistics. .
R.J. Brachman, R.E. Fikes and H.J. Levesque.
(October 1983). Krypton: A Functional
Approach to Knowledge Representation.
IEEE Computer, Special Issue on
Knowledge Representation„ pp. 67-73.
</reference>
<page confidence="0.989693">
193
</page>
<reference confidence="0.998780388888889">
B.J. Grosz, D.E. Appelt P.A. Martin and F.C.N.
Periera. (May 1987). TEAM: An
Experiment in the Design of Transportable
English Interfaces. Artificial Intelligence,
32(2), 173-244.
G. Hobbs. (1985). Ontological Promiscuity.
Proceedings of the 23th Annual Meeting of
the Association of Computational Linguistics.
Miller, J. R., Hill, W. C., McKendree, J., Masson,
M. E. J., Blumenthal, B., Terveen, L., &amp;
Zaback, J. (1987). The role of the system
image in intelligent user assistance.
Proceedings of INTERACT87. Stuttgart.
Rich, E. A., J. Bamett, K. Wittenburg &amp;
D. Wroblewski. (July 1987). Ambiguity
Procrastination. Proceedings of AAAI-87. .
B. Sheils. (1983). Power Tools for
Programmers. Datamation„ pp. 131-144.
</reference>
<page confidence="0.998794">
194
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961271">
<title confidence="0.992057">LUKE: AN EXPERIMENT IN THE EARLY INTEGRATION OF NATURAL LANGUAGE PROCESSING</title>
<author confidence="0.999985">David A Wroblewski</author>
<author confidence="0.999985">Elaine A Rich</author>
<affiliation confidence="0.992398">MCC Human Interface Laboratory</affiliation>
<address confidence="0.9969355">3500 West Balcones Center Drive Austin, Texas 78759</address>
<abstract confidence="0.999937269230769">Luke is a knowledge editor designed to support two tasks; the first is editing the classes and relations in a knowledge base. The second is editing and maintaining the semantic mapping knowledge neccesary to allow a natural language interface to understand sentences with respect to that knowledge base. In order to emphasize design decisions shared between the two tasks, Luke provides facilities to concurrently debug the application and the natural language interface. Luke also makes natural language available in its own user interface. This makes it possible for a knowledge base builder to exploit natural language both as a way of locating desired concepts within the knowledge base and as a a way of doing consistency checking on the knowledge base as it is being built. base, Luke makes natural language available in its own interface. This makes it possible for the knowledge base builder to exploit natural language both as a way of referring to objects in the knowledge base and as a way of doing consistency checking on the objects themselves. In this paper, we will describe both what Luke does and how doing that supports this productive view of the interaction between building a knowledge based system and building an associated natural language interface.</abstract>
<intro confidence="0.997306">Background And Motivation</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Damaris M Ayuso</author>
<author>Varda Shaked</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An Environment For Acquiring Semantic Information.</title>
<date>1987</date>
<booktitle>Proceedings of the 25th Annual Meeting of the Association of Computational Linguistics. .</booktitle>
<marker>Ayuso, Shaked, Weischedel, 1987</marker>
<rawString>Damaris M. Ayuso, Varda Shaked and Ralph M. Weischedel. (July 1987). An Environment For Acquiring Semantic Information. Proceedings of the 25th Annual Meeting of the Association of Computational Linguistics. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Ballard</author>
<author>D E Stumberger</author>
</authors>
<title>Semantic Acquisition in TELI: A Transportable, User-Customized Natural Language Processor.</title>
<date>1986</date>
<booktitle>Proceedings of the 24th Annual Meeting of the Association of Computational Linguistics. .</booktitle>
<marker>Ballard, Stumberger, 1986</marker>
<rawString>B.W. Ballard and D.E. Stumberger. (1986). Semantic Acquisition in TELI: A Transportable, User-Customized Natural Language Processor. Proceedings of the 24th Annual Meeting of the Association of Computational Linguistics. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Brachman</author>
<author>R E Fikes</author>
<author>H J Levesque</author>
</authors>
<title>Krypton: A Functional Approach to Knowledge Representation.</title>
<date>1983</date>
<journal>IEEE Computer, Special Issue on Knowledge Representation„</journal>
<pages>67--73</pages>
<marker>Brachman, Fikes, Levesque, 1983</marker>
<rawString>R.J. Brachman, R.E. Fikes and H.J. Levesque. (October 1983). Krypton: A Functional Approach to Knowledge Representation. IEEE Computer, Special Issue on Knowledge Representation„ pp. 67-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>D E Appelt P A Martin</author>
<author>F C N Periera</author>
</authors>
<title>TEAM: An Experiment in the Design of Transportable English Interfaces.</title>
<date>1987</date>
<journal>Artificial Intelligence,</journal>
<volume>32</volume>
<issue>2</issue>
<pages>173--244</pages>
<marker>Grosz, Martin, Periera, 1987</marker>
<rawString>B.J. Grosz, D.E. Appelt P.A. Martin and F.C.N. Periera. (May 1987). TEAM: An Experiment in the Design of Transportable English Interfaces. Artificial Intelligence, 32(2), 173-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hobbs</author>
</authors>
<title>Ontological Promiscuity.</title>
<date>1985</date>
<booktitle>Proceedings of the 23th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="3072" citStr="Hobbs, 1985" startWordPosition="498" endWordPosition="499">uch knowledge base. By supporting the early integration of the application program and the NL interface as this single knowledge base is being built, Luke helps to ensure that it will be adequate, with respect to both its content and its structure, to support both these target tasks. The second way in which the two system building tasks are related is that one can support the other. By associating natural language with concepts as they are entered into a knowledge A Model Of Semantic Analysis All of the following discussion is based on a model of semantic analysis similar to that proposed in (Hobbs, 1985). Under this model, syntactic and semantic analysis are done as separate operations. The first stage of semantic analysis is a conversion to initial logical form, in which the surface content of the sentence is encoded in a set of expressions that look like logical terms, but whose predicates are taken directly from the words used in the sentence. Initial logical form captures the predicational structure of the sentence, without expressing it in terms of the knowledge base. Once produced, the expressions in initial logical form are individually translated into final logical form, which is a se</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>G. Hobbs. (1985). Ontological Promiscuity. Proceedings of the 23th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Miller</author>
<author>W C Hill</author>
<author>J McKendree</author>
<author>M E J Masson</author>
<author>B Blumenthal</author>
<author>L Terveen</author>
<author>J Zaback</author>
</authors>
<title>The role of the system image in intelligent user assistance.</title>
<date>1987</date>
<booktitle>Proceedings of INTERACT87.</booktitle>
<location>Stuttgart.</location>
<marker>Miller, Hill, McKendree, Masson, Blumenthal, Terveen, Zaback, 1987</marker>
<rawString>Miller, J. R., Hill, W. C., McKendree, J., Masson, M. E. J., Blumenthal, B., Terveen, L., &amp; Zaback, J. (1987). The role of the system image in intelligent user assistance. Proceedings of INTERACT87. Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Rich</author>
<author>J Bamett</author>
<author>K Wittenburg</author>
<author>D Wroblewski</author>
</authors>
<date>1987</date>
<booktitle>Ambiguity Procrastination. Proceedings of AAAI-87. .</booktitle>
<marker>Rich, Bamett, Wittenburg, Wroblewski, 1987</marker>
<rawString>Rich, E. A., J. Bamett, K. Wittenburg &amp; D. Wroblewski. (July 1987). Ambiguity Procrastination. Proceedings of AAAI-87. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sheils</author>
</authors>
<title>Power Tools for Programmers. Datamation„</title>
<date>1983</date>
<pages>131--144</pages>
<contexts>
<context position="25994" citStr="Sheils, 1983" startWordPosition="4169" endWordPosition="4170">appings themselves are stored as frames in the knowledge base. The biggest maintenance service Luke provides is consistency checking. When a frame is deleted, entered, or specialized in the knowledge base, or after any terminological editing operation, Luke collects all of the semantic mappings that might be affected and creates a set of tasks to verify their continuing correctness. As always, the user can choose to handle such tasks immediately, or delay for later consideration. Exploiting Natural Language in Luke Itself The overall goal in building Luke is to provide a set of &amp;quot;power tools&amp;quot; (Sheils, 1983) that support the dual application model, and Luke is our first step in that direction. One potential problem in Luke&apos;s design is increasing the overhead of building a knowledge base, since various tasks are continually scheduled for the user. This fear is mitigated by the following observations. First, the added overhead doesn&apos;t represent extra work to be done by the user, only a different time for the user to do it. If there is to be a NLI for the application, then the developer is in a &amp;quot;pay me now or pay me later&apos; bind, where late payment can be very costly. Viewed this way, Luke is helping</context>
</contexts>
<marker>Sheils, 1983</marker>
<rawString>B. Sheils. (1983). Power Tools for Programmers. Datamation„ pp. 131-144.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>