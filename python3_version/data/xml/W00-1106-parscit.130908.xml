<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.641056">
Corpus-Based Learning of Compound Noun Indexing
Byung-Kwan Kwak,
Jee-Hyub Kim,
and Geunbae Leet
NLP Lab., Dept. of CSE
Pohang University of
Science &amp; Technology
</note>
<email confidence="0.4310345">
(POSTECH)
{nerguri,gblee}Opostech.ac.kr
</email>
<sectionHeader confidence="0.981425" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999827545454546">
In this paper, we present a corpus-
based learning method that can
index diverse types of compound
nouns using rules automatically ex-
tracted from a large tagged corpus.
We develop an efficient way of ex-
tracting the compound noun index-
ing rules automatically and perform
extensive experiments to evaluate
our indexing rules. The automatic
learning method shows about the
same performance compared with
the manual linguistic approach but
is more portable and requires no
human efforts. We also evaluate
the seven different filtering meth-
ods based on both the effectiveness
and the efficiency, and present a
new method to solve the problems of
compound noun over-generation and
data sparseness in statistical com-
pound noun processing.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999696125">
Compound nouns are more specific and ex-
pressive than simple nouns, so they are more
valuable as index terms and can increase
the precision in search experiments. There
are many definitions for the compound noun
which cause ambiguities as to whether a given
continuous noun sequence is a compound
noun or not. We, therefore, need a clean
</bodyText>
<listItem confidence="0.74780575">
â€¢ This research was supported by KOSEF special
purpose basic research (1997.9 - 2000.8 #970-1020-
301-3)
t Corresponding author
</listItem>
<note confidence="0.8912668">
Jung Yun Seo
NLP Lab.,
Dept. of Computer Science
Sogang University
seojyaccs.sogang.ac.kr
</note>
<bodyText confidence="0.960807970588236">
definition of compound nouns in terms of in-
formation retrieval, so we define a compound
noun as &amp;quot;any continuous noun sequence that
appears frequently in documents.&amp;quot;&apos;
In Korean documents, compound nouns are
represented in various forms (shown in Table
1), so there is a difficulty in indexing all types
of compound nouns. Until now, there have
been much works on compound noun index-
ing, but they still have limitations of cover-
ing all types of compound nouns and require
much linguistic knowledge to accomplish this
goal. In this paper, we propose a corpus-
based learning method for compound noun
indexing which can extract the rules automat-
ically with little linguistic knowledge.
Table 1: Various types of Korean compound
noun with regard to &amp;quot;jeong-bo geom-saeg (in-
formation retrieval)&amp;quot;
jeong-bo-geona-saeg (information-retrieval)
jeong-bo-eui geom-saeg (retrieval of information)
jeong-bo geom-saeg (information retrieval)
jeong-bo-leul geom-saeg-ha-neun
(retrieving information)
jeong-bo-geom-sa.eg si-sen-tem
(information-retrieval system)
As the number of the documents is growing
retrieval, efficiency also becomes as important
as effectiveness. To increase the efficiency, we
focus on reducing the number of indexed spu-
rious compound nouns. We perform experi-
ments on several filtering methods to find the
algorithm that can reduce spurious compound
nouns most efficiently.
</bodyText>
<footnote confidence="0.82352">
1 The frequency threshold can be adjusted accord-
ing to application systems.
</footnote>
<page confidence="0.998708">
57
</page>
<bodyText confidence="0.9998654">
The remainder of this paper is organized
as follows. Section 2 describes previous com-
pound noun indexing methods for Korean and
compound noun filtering methods. We show
overall compound noun indexing system ar-
chitecture in Section 3, and explain each mod-
ule of the system in Section 4 and 5 in de-
tail. We evaluate our method with standard
Korean test collections in Section 6. Finally,
concluding remarks are given in Section 7.
</bodyText>
<sectionHeader confidence="0.98298" genericHeader="method">
2 Previous Research
</sectionHeader>
<subsectionHeader confidence="0.91011">
2.1 Compound Noun Indexing
</subsectionHeader>
<bodyText confidence="0.994236869565217">
There have been two different methods
for compound noun indexing: statistical
and linguistic. In one statistical method,
(Fagan, 1989) indexed phrases using six
different parameters, including information
on co-occurrence of phrase elements, rela-
tive location of phrase elements, etc., and
achieved reasonable performance. However,
his method couldn&apos;t reveal consistent sub-
stantial improvements on five experimental
document collections in effectiveness. (Strza-
lkowski et al., 1996; Evans and Zhai, 1996)
indexed subcompounds from complex noun
phrases using noun-phrase analysis. These
methods need to find the head-modifier rela-
tions from noun phrases and therefore require
difficult syntactic parsing in Korean.
For Korean, in one statistical method, (Lee
and Ahn, 1996) indexed general Korean nouns
using n-grams without linguistic knowledge
and the experiment results showed that the
proposed method might be almost as effec-
tive as the linguistic noun indexing. How-
ever, this method can generate many spuri-
ous n-grams which decrease the precision in
search performance. In linguistic methods,
(Kim, 1994) used five manually chosen com-
pound noun indexing rule patterns based on
linguistic knowledge. However, this method
cannot index the diverse types of compound
nouns. (Won et al., 2000) used a full parser
and increased the precision in search experi-
ments. However, this linguistic method can-
not be applied to unrestricted texts robustly.
In summary, the previous methods,
whether they are statistical or linguistic,
have their own shortcomings. Statistical
methods require significant amounts of
co-occurrence information for reasonable
performance and can not index the diverse
types of compound nouns. Linguistic meth-
ods need compound noun indexing rules
described by human and sometimes result
in meaningless compound nouns, which
decreases the performance of information
retrieval systems. They cannot also cover the
various types of compound nouns because of
the limitation of human linguistic knowledge.
In this paper, we present a hybrid method
that uses linguistic rules but these rules are
automatically acquired from a large corpus
through statistical learning. Our method gen-
erates more diverse compound noun index-
ing rule patterns than the previous standard
methods (Kim, 1994; Lee et al., 1997), be-
cause previous methods use only most gen-
eral rule patterns (shown in Table 2) and are
based solely on human linguistic knowledge.
Table 2: Typical hand-written compound
noun indexing rule patterns for Korean
Noun without case makers / Noun
Noun with a genitive case maker / Noun
Noun with a nominal case maker or
an accusative case maker /
Verbal common noun or adjectival common noun
Noun with an adnominal ending / Noun
Noun within predicate particle phrase / Noun
(The two nouns before and after a slash
in the pattern can form a single compound noun.)
</bodyText>
<subsectionHeader confidence="0.998607">
2.2 Compound Noun Filtering
</subsectionHeader>
<bodyText confidence="0.99996075">
Compound noun indexing methods, whether
they are statistical or linguistic, tend to gen-
erate spurious compound nouns when they
are actually applied. Since an information re-
trieval system can be evaluated by its effec-
tiveness and also by its efficiency (van Rijs-
bergen, 1979), the spurious compound nouns
should be efficiently filtered. (Kando et al.,
1998) insisted that, for Japanese, the smaller
the number of index terms is, the better the
performance of the information retrieval sys-
tem should be.
</bodyText>
<page confidence="0.996872">
58
</page>
<bodyText confidence="0.999782863636364">
For Korean, (Won et al., 2000) showed
that segmentation of compound nouns is more
efficient than compound noun synthesis in
search performance. There have been many
works on compound noun filtering methods;
(Kim, 1994) used mutual information only,
and (Yun et al., 1997) used mutual informa-
tion and relative frequency of POS (Part-Of-
Speech) pairs together. (Lee et al., 1997) used
stop word dictionaries which were constructed
manually. Most of the previous methods for
compound noun filtering utilized only one
consistent method for generated compound
nouns irrespective of the different origin of
compound noun indexing rules, and the meth-
ods cause many problems due to data sparse-
ness in dictionary and training data. Our
approach solves the data sparseness problem
by using co-occurrence information on auto-
matically extracted compound noun elements
together with a statistical precision measure
which fits best to each rule.
</bodyText>
<sectionHeader confidence="0.982761" genericHeader="method">
3 Overall System Architecture
</sectionHeader>
<bodyText confidence="0.9993815">
The compound noun indexing system pro-
posed in this paper consists of two major
modules: one for automatically extracting
compound noun indexing rules (in Figure 1)
and the other for indexing documents, fil-
tering the automatically generated compound
nouns, and weighting the indexed compound
nouns (in Figure 2).
</bodyText>
<table confidence="0.993704857142857">
Compound Tagged Corpus
Noun Seeds
Compound Extracted Rules
Noun Statistical Filtered Rules
Information
Rules with
Precision
</table>
<figureCaption confidence="0.998501">
Figure 1: Compound noun indexing-rule ex-
</figureCaption>
<bodyText confidence="0.51716">
traction module (control flow data flow
-4)
</bodyText>
<figureCaption confidence="0.983567">
Figure 2: Compound noun indexing, filtering,
</figureCaption>
<bodyText confidence="0.9644275">
and weighting module (control flow data
flow -+)
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="method">
4 Automatic Extraction of
</sectionHeader>
<subsectionHeader confidence="0.981384">
Compound Noun Indexing Rules
</subsectionHeader>
<bodyText confidence="0.999997571428571">
There are three major steps in automatically
extracting compound noun indexing rules.
The first step is to collect compound noun
statistical information, and the second step is
to extract the rules from a large tagged cor-
pus using the collected statistical information.
The final step is to learn each rule&apos;s precision.
</bodyText>
<subsectionHeader confidence="0.9898255">
4.1 Collecting Compound Noun
Statistics
</subsectionHeader>
<bodyText confidence="0.999925625">
We collect initial compound noun seeds which
were gathered from various types of well-
balanced documents such as ETRI Kemong
encyclopaRdia2 and many dictionaries on the
Internet, and we collected 10,368 seeds, as
shown in Table 3. The small number of seeds
are bootstrapped to extract the compound
noun indexing rules for various corpora.
</bodyText>
<tableCaption confidence="0.998519">
Table 3: Collected compound noun seeds
</tableCaption>
<table confidence="0.70883175">
No. of 2 3 Total
component elements
ETRI Kemong encyclomedia 5,100 2,088 7,188
Internet dictionaries 2,071 1,109 3,180
</table>
<bodyText confidence="0.992239666666667">
To collect more practical statistics on the
compound nouns, we made a 1,000,000 eo-
jeol(Korean spacing unit which corresponds
</bodyText>
<figure confidence="0.97746692">
2 Courteously provided by ETRI, Korea.
orpholo.&amp;quot;
Analysis
Compound Noun
(Part.of-SpeeiN1Statistical
Tagging Information
Weighted
Compound
Nouns
Weighting
Compound
Nouns
Indexing
en
Filtering
Compound
Nouns
Docum
Compound Noun
Indexing
Rules with
Precision
Filtered
Compound
Nouns
</figure>
<page confidence="0.993952">
59
</page>
<bodyText confidence="0.9374362">
to an English word or phrase) tagged cor-
pus for a compound noun indexing experi-
ment from a large document set (Korean In-
formation Base). We collected complete com-
pound nouns (a continuous noun sequence
composed of at least two nouns on the condi-
tion that both the preceding and the following
POS of the sequence are not nouns (Yoon et
al., 1998)) composed of 1 - 3 nouns from the
tagged training corpus (Table 4).
Table 4: Statistics for complete compound
nouns
No. of 1 2 3
component elements
Vocabulary 264,359 200,455 63,790
</bodyText>
<subsectionHeader confidence="0.997389">
4.2 Extracting Indexing Rules
</subsectionHeader>
<bodyText confidence="0.99964080952381">
We define a template (in Table 5) to extract
the compound noun indexing rules from a
POS tagged corpus.
The template means that if a front-
condition-tag, a rear-condition-tag, and sub-
string-tags are coincident with input sentence
tags, the lexical item in the synthesis position
of the sentence can be indexed as a compound
noun as &amp;quot;x/ y (for 3-noun compounds, x /
y / z)&amp;quot;. The tags used in the template are
POS (Part-Of-Speech) tags and we use the
POSTAG set (Table 17).
The following is an algorithm to extract
compound noun indexing rules from a large
tagged corpus using the two-noun compound
seeds and the template defined above. The
rule extraction scope is limited to the end
of a sentence or, if there is a conjunctive
ending (eCC) in the sentence, only to the
conjunctive ending of the sentence. A rule
extraction example is shown in Figure 3.
</bodyText>
<figure confidence="0.9026523">
Algorithm 1: Extracting compound noun
indexing rules (for 2-noun compounds)
Read Template
Read Seed
(Consist of Constituent 1 / Constituent 2)
Tokenize Seed into Constituents
Put Constituent 1 into Keyl and Constituent 2
into Key2
While (Not(End of Documents))
Read Initial Tag of Sentence
While (Not(End of Sentence or eCC))
{
Read Next Tag of Sentence
If (Read Tag == Key1)
{
While (Not(End of Sentence or eCC))
{
Read Next Tag of Sentence
If (Current Tag == Key2)
Write Rule according
</figure>
<subsectionHeader confidence="0.70276">
to the Template
</subsectionHeader>
<bodyText confidence="0.989135333333333">
The next step is to refine the extracted
rules to select the proper ones. We used a rule
filtering algorithm (Algorithm 2) using the
frequency together with the heuristics that
the rules with negative lexical items (shown in
Table 6) will make spurious compound nouns.
</bodyText>
<listItem confidence="0.821533111111111">
Algorithm 2: Filtering extracted rules us-
ing frequency and heuristics
1. For each compound noun seed, select
the rules whose frequency is greater than 2.
2. Among rules selected by step 1, select
only rules that are extracted
at least by 2 seeds.
3. Discard rules which contain
negative lexical items.
</listItem>
<tableCaption confidence="0.867335090909091">
Table 5: The template to extract the com-
pound noun indexing rules
front-condition-tag I
sub-string-tags (tag 1 tag 2 ... tag n-1 tag n)
rear-condition-tag)
synthesis locations (x y)
lexicon x / lexicon y
(for 3-noun compounds,
synthesis locations (x, y, z)
lexicon x / lexicon y / lexicon z)
Table 6: Negative lexical item examples
</tableCaption>
<bodyText confidence="0.737136916666667">
negative items (tags) example phrases
no-jo-leul je-oe-han hoe-eui
(meeting excluding union)
sa-gwa-ga eobs-neun na-mu
(tree without apple)
dog-lib-eul mos-han g-ug-ga
(country that
cannot be liberated)
je-oe(MC) (exclude)
eobs(E) (not exist)
mos-ha(D) (can not)
We automatically extracted and filtered out
</bodyText>
<page confidence="0.956365">
60
</page>
<table confidence="0.941041166666666">
frcmcsondition_tag I sub_string_tags (tag 1 tag2 ... tag n-1 tag n)
&apos;rear eondition_tag I synthesis location (x â€”&gt; lexicon x / lexicon y
Tagged Sentence Example Seat
B&lt;bbal-Ib. jeong-bo/geom-saeg
MC&lt; jeong-bo &gt; (information/retrieval)
jCdeul&gt;
MC&lt; georn-saeg &gt;
y&lt;ha&gt;
eCNMG&lt;neun&gt;
Extracted Compound Noun
Indexing Rule:
B I MCjC&lt;Ieul&gt; MC lyi 1 3
</table>
<figureCaption confidence="0.989237">
Figure 3: Rule Extraction Process Example
</figureCaption>
<bodyText confidence="0.998770875">
2,036 rules from the large tagged corpus (Ko-
rean Information Base, 1,000,000 eojeol) us-
ing the above Algorithm 2. Among the fil-
tered rules, there are 19 rules with negative
lexical items and we finally selected 2,017
rules. Table 7 shows a distribution of the final
rules according to the number of elements in
their sub-string-tags.
</bodyText>
<tableCaption confidence="0.8314735">
Table 8: Comparison between the automati-
cally extracted rules and the manual rules
</tableCaption>
<table confidence="0.992909">
Method No. of No. of
general lexical terms
rule patterns used in rule patterns
Manual 5 16
linguistic
method
Our method 23 78
</table>
<tableCaption confidence="0.949478">
Table 9: Examples of newly added rule pat-
terns
</tableCaption>
<table confidence="0.5920628">
Rule
Noun + bound noun / Noun
Noun + suffix / Noun
Noun + suffix + assignment verb +
adnominal ending / Noun
counting how many indexed compound noun
candidates generated by the rule are actual
compound nouns:
Nactual
Prec(rule) =
</table>
<tableCaption confidence="0.877976">
Table 7: Distribution of extracted rules by
number of elements in sub-string-tags
</tableCaption>
<table confidence="0.978784428571428">
No. Distribution Example
2 tags 79.6 % MC MC
3 tags 12.6 % MC j0(eui) MC
4 tags 4.7 % MC y eCNMG MC
5 tags 1.5 % MC MC j0(e)
DI(sog-ha-neun) MC
over 6 tags 1.6 %
</table>
<bodyText confidence="0.999747545454545">
The automatically extracted rules have
more rule patterns and lexical items than
human-made rules so they can cover more
diverse types of compound nouns (Table 8).
When checking the overlap between the two
rule collections, we found that the manual lin-
guistic rules are a subset of our automatically
generated statistical rules. Table 9 shows
some of the example rules newly generated
from our extraction algorithm, which were
originally missing in the manual rule patterns.
</bodyText>
<subsectionHeader confidence="0.9995915">
4.3 Learning the Precision of
Extracted Rules
</subsectionHeader>
<bodyText confidence="0.999989">
In the proposed method, we use the precision
of rules to solve the compound noun over-
generation and the data sparseness problems.
The precision of a rule can be defined by
</bodyText>
<subsubsectionHeader confidence="0.628364">
Ncandidate
</subsubsectionHeader>
<bodyText confidence="0.999961956521739">
where Prec(rule) is the precision of a rule,
Nactuai is the number of actual compound
nouns, and Ncandidate is the number of com-
pound noun candidates generated by the au-
tomatic indexing rules.
To calculate the precision, we need a defin-
ing measurement for compound noun identi-
fication. (Su et al., 1994) showed that the
average mutual information of a compound
noun tends to be higher than that of a non-
compound noun, so we try to use the mutual
information as the measure for identifying the
compound nouns. If the mutual information
of the compound noun candidate is higher
than the average mutual information of the
compound noun seeds, we decide that it is
a compound noun. For mutual information
(MI), we use two different equations: one for
two-element compound nouns (Church and
Hanks, 1990) and the other for three-element
compound nouns (Su et al., 1994). The equa-
tion for two-element compound nouns is as
follow:
</bodyText>
<equation confidence="0.999287">
P(x,y)
I(x; y) = log2
P(x) x P(y)
</equation>
<page confidence="0.984154">
61
</page>
<bodyText confidence="0.939542913043478">
where x and y are two words in the corpus,
and /(x; y) is the mutual information of these
two words (in this order). Table 10 shows
the average MI value of the two and three
elements.
Table 10: Average value of the mutual infor-
mation (MI) of compound noun seeds
Number of elements 2 3
Average MI 3.56 3.62
The MI was calculated from the statistics of
the complete compound nouns collected from
the tagged training corpus (see Section 4.1).
However, complete compound nouns are
continuous noun sequences and cause the
data sparseness problem. Therefore, we need
to expand the statistics. Figure 4 shows
the architecture of the precision learning
module by expanding the statistics of the
complete compound nouns along with an
algorithmic explanation (Algorithm 3) of the
process. Table 11 shows the improvement in
the average precision during the repetitive
execution of this learning process.
</bodyText>
<figureCaption confidence="0.761641333333333">
Figure 4: Learning the precision of the com-
pound noun indexing rules (The steps are
shown in Algorithm 3)
</figureCaption>
<listItem confidence="0.923270222222222">
Algorithm 3:
1. Calculate all rules&apos; initial precision
using initial complete compound noun
statistical information.
2. Calculate the average precision
of the rules.
3. Multiply a rule&apos;s precision by
the frequency of the compound noun made
by the rule.
</listItem>
<bodyText confidence="0.813727222222222">
We call this value the modified frequency
(MF).
4. Collect the same compound nouns, and
sum all the modified frequencies
for each compound noun.
5. If the summed modified frequency is greater
than a threshold, add this compound noun
to the complete compound noun
statistical information.
</bodyText>
<listItem confidence="0.8074395">
6. Calculate all rules&apos; precision again
using the changed complete compound noun
statistical information.
7. Calculate the average precision of the rules.
</listItem>
<bodyText confidence="0.990626666666667">
8. If the average precision of the rules is
equal to the previous average precision,
stop. Otherwise, go to step 2.
</bodyText>
<tableCaption confidence="0.8666265">
Table 11: Improvement in the average preci-
sion of rules
</tableCaption>
<table confidence="0.98290375">
Learning 1 2 3 4 5 6
cycles
Avg. prec. 0.19 0.23 0.39 0.44 0.45 0.45
of rules
</table>
<sectionHeader confidence="0.977098" genericHeader="method">
5 Compound Noun Indexing,
Filtering, and Weighting
</sectionHeader>
<bodyText confidence="0.99997475">
In this section, we explain how to use the au-
tomatically extracted rules to actually index
the compound nouns, and describe how to fil-
ter and weight the indexed compound nouns.
</bodyText>
<subsectionHeader confidence="0.98735">
5.1 Compound Noun Indexing
</subsectionHeader>
<bodyText confidence="0.999986666666667">
To index compound nouns from documents,
we use a natural language processing engine,
SKOPE (Standard KOrean Processing En-
gine) (Cha et al., 1998), which processes doc-
uments by analysing words into morphemes
and tagging part-of-speeches. The tagging
results are compared with the automatically
learned compound noun indexing rules and, if
they are coincident with each other, we index
them as compound nouns. Figure 5 shows a
process of the compound noun indexing with
an example.
</bodyText>
<subsectionHeader confidence="0.996887">
5.2 Compound Noun Filtering
</subsectionHeader>
<bodyText confidence="0.999789428571429">
Among the indexed compound nouns above,
still there can be meaningless compound
nouns, which increases the number of index
terms and the search time. To solve com-
pound noun over-generation problem, we ex-
periment with seven different filtering meth-
ods (shown in Table 12) by analyzing their
</bodyText>
<figure confidence="0.999619">
Rules with
Initial
Precision
Rules with
Precision
Compound
Noun with MF
Update Complete
Compound Noun
(Step 5)
Calculate Precision
of Rules
(Step 2, 7)
Compare Average
Rule Precision
(Step 8)
omplete Compoun
Noun Statistical
Information
Tagged
Corpus
Ste 3, 4
5 Step 7
</figure>
<page confidence="0.838017">
62
</page>
<figureCaption confidence="0.999723">
Figure 5: Compound noun indexing process
</figureCaption>
<bodyText confidence="0.999920263157895">
relative effectiveness and efficiency, as shown
in Table 16. These methods can be divided
into three categories: first one using MI, sec-
ond one using the frequency of the compound
nouns (FC), and the last one using the fre-
quency of the compound noun elements (FE).
MI (Mutual Information) is a measure of word
association, and used under the assumption
that a highly associated word n-gram is more
likely to be a compound noun. FC is used
under the assumption that a frequently en-
countered word n-gram is more likely to be a
compound than a rarely encountered n-gram.
FE is used under the assumption that a word
n-gram with a frequently encountered specific
element is more likely to be a compound. In
the method of C, D, E, and F, each threshold
was decided by calculating the average num-
ber of compound nouns of each method.
</bodyText>
<tableCaption confidence="0.996192">
Table 12: Seven different filtering methods
</tableCaption>
<table confidence="0.8757812">
(MI) A. Mutual information of compound
noun elements (0)
(MI) B. Mutual information of compound
noun elements
(average of MI of compound noun seeds)
(FC) C. Frequency of compound nouns
in the training corpus (4)
(FC) D. Frequency of compound nouns
in the test corpus (2)
(FE) E. Frequency of compound noun heads
in the training corpus (5)
(FE) F. Frequency of compound noun modifiers
in the training corpus (5)
G. No filtering
(The value in parantheses is a threshold.)
</table>
<bodyText confidence="0.994290888888889">
Among these methods, method B gener-
ated the smallest number of compound nouns
best efficiency and showed the reasonable ef-
fectiveness (Table 16). On the basis of this
filtering method, we develop a smoothing
method by combining the precision of rules
with the mutual information of the compound
noun elements, and propose our final filtering
method (H) as follows:
</bodyText>
<equation confidence="0.9979105">
P(x Y2 â€žx Precision
T(x,y) = 1Â°g2 p(x) x P(y)
</equation>
<bodyText confidence="0.998415">
where a is a weighting coefficient and Preci-
sion is the applied rules learned in Section 4.3.
For the three-element compound nouns, the
MI part is replaced with the three-element MI
equation3 (Su et al., 1994).
</bodyText>
<sectionHeader confidence="0.987922" genericHeader="evaluation">
6 Experiment Results
</sectionHeader>
<bodyText confidence="0.99997">
To calculate the similarity between a docu-
ment and a query, we use the p-norm retrieval
model (Fox, 1983) and use 2.0 as the p-value.
We also use the component nouns in a com-
pound as the indexing terms. We follow the
standard TREC evaluation schemes (Salton
and Buckley, 1991). For single index terms,
we use the weighting method atn.ntc (Lee,
1995).
</bodyText>
<subsectionHeader confidence="0.9763985">
6.1 Compound Noun Indexing
Experiments
</subsectionHeader>
<bodyText confidence="0.999920785714286">
This experiment shows how well the proposed
method can index diverse types of compound
nouns than the previous popular methods
which use human-generated compound noun
indexing rules (Kim, 1994; Lee et al., 1997).
For simplicity, we filtered the generated com-
pound nouns using the mutual information of
the compound noun elements with a thresh-
old of zero (method A in Table 12).
Table 13 shows that the terms indexed by
previous linguistic approach are a subset of
the ones made by our statistical approach.
This means that the proposed method can
cover more diverse compound nouns than the
</bodyText>
<equation confidence="0.814442">
3
PD z)
gx; y; z) = log2 pi(x, y, z)
</equation>
<figure confidence="0.9987434">
Tagging Result
13&lt;bbal-Ib.
MC&lt; jeong-bo &gt;
jC&lt;Ieul&gt;
MC&lt; geom-saeg &gt;
y&lt;ha&gt;
eCNMG&lt;neun&gt;
Indexed
and Filtered
Compound Nouns
jeong-bo/
geom-saeg
(information /
retrieval)
Input Sentence:
bbal-li jeong-bo-leul
geom-saeg-ha-neun
(retrieving
information
quickly)
Compound Noun
Indexing Rules
B I MC jC&lt;Ieut&gt; MC Iy1 13
Complete
Compound Noun
Statistical Information
mpoun
Noun
Indexing
Filtering
</figure>
<page confidence="0.995741">
63
</page>
<tableCaption confidence="0.746829">
Table 13: Compound noun indexing coverage
</tableCaption>
<bodyText confidence="0.971224625">
experiment (With a 200,000 eojeol Korean In-
formation Base)
manual linguistic rule method. We perform a
retrieval experiment to evaluate the automat-
ically extracted rules. Table le and table 155
show that our method has slightly better re-
call and 11-point average precision than the
manual linguistic rule method.
</bodyText>
<tableCaption confidence="0.856704">
Table 14: Compound noun indexing effective-
ness experiment I
</tableCaption>
<table confidence="0.999784">
Manual linguistic Our automatic
rule patterns rule patterns
Avg. recall 82.66 83.62
(+1.16 %)
11-pt. 42.24 42.33
avg. precision (+0.21 %)
No. of 504,040 515,801
index terms (+2.33 %)
</table>
<tableCaption confidence="0.916979">
Table 15: Compound noun indexing effective-
ness experiment II
</tableCaption>
<table confidence="0.99923525">
Manual linguistic Our automatic
rule patterns rule patterns
Avg. recall 86.32 87.50
(+1.35 %)
11-pt. avg. 34.33 34.54
precision (+0.61 %)
No. of 1,242,458 1,282,818
index terms (+3.15 %)
</table>
<tableCaption confidence="0.3123085">
With KTSET2.0 test collections (Courteously
provided by KT, Korea. (4,410 documents and 50
queries))
5 With KRIST2.0 test collection (Courteously pro-
vided by KORDIC, Korea. (13,514 documents and 30
queries))
</tableCaption>
<subsectionHeader confidence="0.7357975">
6.2 Retrieval Experiments Using
Various Filtering Methods
</subsectionHeader>
<bodyText confidence="0.99996975">
In this experiment, we compare the seven fil-
tering methods to find out which one is the
best in terms of effectiveness and efficiency.
For this experiment, we used our automatic
rules for the compound noun indexing, and
the test collection KTSET2.0. To check the
effectiveness, we used recall and 11-point av-
erage precision. To check the efficiency, we
used the number of index terms. Table 16
shows the results of the various filtering ex-
periments.
From Table 16, the methods using mu-
tual information reduce the number of in-
dex terms, whereas they have lower precision.
The reason of this lower precision is that MI
has a bias, i.e., scoring in favor of rare terms
over common terms, so MI seems to have a
problem in its sensitivity to probability es-
timation error (Yang and Pedersen, 1997).
In this experiment6, we see that method B
generates the smallest number of compound
nouns (best efficiency) and our final propos-
ing method H has the best recall and precision
(effectiveness) with the reasonable number of
compound nouns (efficiency). We can con-
clude that the filtering method H is the best,
considering the effectiveness and the efficiency
at the same time.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999974153846154">
In this paper, we presented a method to ex-
tract the compound noun indexing rules au-
tomatically from a large tagged corpus, and
showed that this method can index compound
nouns appearing in diverse types of docu-
ments.
In the view of effectiveness, this method is
slightly better than the previous linguistic ap-
proaches but requires no human effort.
The proposed method also uses no parser
and no rules described by humans, there-
fore, it can be applied to unrestricted texts
very robustly and has high domain porta-
</bodyText>
<footnote confidence="0.6328755">
6 Our Korean NLQ (Natural Lan-
guage Querying) demo system (located in
gattp://n1p.postech.ac.kr/Resarch/POSNLQP)
can be tested.
</footnote>
<figure confidence="0.995622384615385">
Manual
linguistic
rule patterns
ur
automatic
rule patterns
No. of 30,168
generated actual (+35.4 %)
compound nouns
No. of 7,892
generated actual
compound nouns
without overlap
</figure>
<page confidence="0.99313">
64
</page>
<tableCaption confidence="0.998942">
Table 16: Retrieval experiment results of various filtering methods
</tableCaption>
<table confidence="0.9607872">
A B _ D E F G H
C
Average 83.62 83.62 83.62 83.62 83.62 83.62 84.32 84.32
recall (+0.00) (+0.00) (+0.00) (+0.00) (+0.00) (+0.84) (+0.84)
11-pt. avg. 42.45 42.42 42.49 42.55 42.72 42.48 42.48 42.75
precision (-0.07) (+0.09) (+0.24) (+0.64) (+0.07) (+0.07) (+0.71)
Precision 52.11 52.44 52.07 52.80 52.26 51.89 52.81 52.98
at 10 Docs.
No. of 515,80 508,20 514,54 547,27 572,36 574,04 705,98 509,90
index terms (-1.47) (-0.24) (+6.10) (+10.97) (+11.29) (+36.87) (-1.14)
</table>
<bodyText confidence="0.999669846153846">
bility. We also presented a filtering method
to solve the compound noun over-generation
problem. Our proposed filtering method (H)
shows good retrieval performance both in the
view of the effectiveness and the efficiency.
In the future, we need to perform some
experiments on much larger commercial
databases to test the practicality of our
method.
Finally, our method doesn&apos;t require lan-
guage dependent knowledge, so it needs to be
verified whether it can be easily applied to
other languages.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999664225806452">
Jeongwon Cha, Geunbae Lee, and Jong-Hyeok
Lee. 1998. Generalized unknown morpheme
guessing for hybrid pos tagging of korean.
In Proceedings of SIXTH WORKSHOP ON
VERY LARGE CORPORA in Coling-ACL 98.
K. W. Church and P. Hanks. 1990. Word associ-
ation norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22-29.
David A. Evans and Chenpdang Zhai. 1996.
Noun-phrase analysis in unrestricted text for
information retrieval. In Proceedingof the 34th
Annual Meetinof the Association for Computa-
tional Linguistics, Santa Cruz, CA, pages 17-
24.
Joel L. Fagan. 1989. The effectiveness of a non-
syntactic approach to automatic phrase index-
ing for document retrieval. JASIS, 40(2):115-
132.
E. A. Fox. 1983. Extending the Boolean and Vec-
tor Space Models of Information Retrieval with
P-norm Queries and Multiple Concept Types.
Ph.D. thesis, Cornell Univ.
Noriko Kando, Kyo Kageura, Masaharu Yoshoka,
and Keizo Oyama. 1998. Phrase processing
methods for japanase text retrieval. SIGIR fo-
rum, 32(2):23-28.
Pan Koo Kim. 1994. The automatic indexing
of compound words from korean text based on
mutual information. Journal of KISS (in Ko-
rean), 21(7):1333-1340.
Joon Ho Lee and Jeong Soo Alm. 1996. Using
n-grams for korean text retrieval. In SIGIR&apos;96,
pages 216-224.
Hyun-A Lee, Jong-Hyeok Lee, and Geunbae Lee.
1997. Noun phrase indexing using clausal
segmentation. Journal of KISS (in Korean),
24(3):302-311.
Joon Ho Lee. 1995. Combining multiple evidence
from different properties of weighting schemes.
In SIGIR&apos;95, pages 180-188.
Gerard Salton and Chris Buckley. 1991.
Text retrieval conferences evaluation pro-
gram. In ftp://ftp.cs. cornell. edu/pub/smart/,
trec_eva1.7.0beta.tar.gz.
Tomek Strzalkowski, Louise Guthrie, Jussi Karl-
gren, Jum Leistensnider, Fang Lin, Jose Perez-
Carballo, Troy Straszheim, Jin Wang, and Jon
Wilding. 1996. Natural language information
retrieval: Trec-5 report. In The Fifth Text
REtrieval conference (TREC-5), NIST Special
publication, pages 500-238.
Keh-Yih Su, Ming-Wen Wu, and Jing-Shin
Chang. 1994. A corpus-based approach to au-
tomatic compound extraction. In Proceedings
of ACL 94, pages 242-247.
C. J. van Rijsbergen. 1979. Information Re-
trieval. University of Computing Science,
Lodon.
Hyungsuk Won, Mihwa Park, and Geunbae Lee.
2000. Integrated multi-level indexing method
for compound noun processing. In Journal of
KISS, 27(1) (in Korean), pages 84-95.
</reference>
<page confidence="0.999883">
65
</page>
<tableCaption confidence="0.997908">
Table 17: The POS (Part-Of-Speech) set of POSTAG
</tableCaption>
<bodyText confidence="0.894596285714286">
Tag Description Tag Description Tag Description
MC common noun MP proper noun MD bound noun
T pronoun G adnoun S numeral
B adverb K interjection DR regular verb
DI irregular verb HR regular adjective HI irregular adjective
I assignment verb E existential predicate jC case particle
jS auxiliary particle j0 other particle eGE final ending
eGS prefinal ending eCNDI aux conj ending eCNDC quote conj ending
eCNMM nominal ending eCNMG adnominal ending eCNB adverbial ending
eCC conjunctive ending Y predicative particle b auxiliary verb
+ prefix - suffix su unit symbol
so other symbol s&apos; left parenthesis s&apos; right parenthesis
s. sentence closer s- sentence connection s, sentence comma
sf foreign word sh Chinese character
</bodyText>
<reference confidence="0.99767494117647">
Yiming Yang and Jan 0. Pedersen. 1997. A com-
parative study on feature selection in text cat-
egorization. In Douglas H. Fisher, editor, Pro-
ceedings of ICML-97, 14th International Con-
ference on Machine Learning, pages 412-420,
Nashville, US. Morgan Kaufmann Publishers,
San Francisco, US.
Jun-Tae Yoon, Eui-Seok Jong, and Mansuk Song.
1998. Analysis of korean compound noun in-
dexing using lexical information between nouns.
Journal of KISS (in Korean), 25(11):1716-
1725.
Bo-Hyun Yun, Yong-Jae Kwak, and Hae-Chang
Rim. 1997. A korean information retrieval
model alleviating syntactic term mismatches.
In Proceedings of the Natural Language Process-
ing Pacific Rim Symposium, pages 107-112.
</reference>
<page confidence="0.98858">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.258235">
<title confidence="0.995091">Corpus-Based Learning of Compound Noun Indexing</title>
<author confidence="0.6245715">Byung-Kwan Jee-Hyub</author>
<affiliation confidence="0.872955">NLP Lab., Dept. of Pohang University Science &amp;</affiliation>
<email confidence="0.932846">nerguriOpostech.ac.kr</email>
<email confidence="0.932846">gbleeOpostech.ac.kr</email>
<abstract confidence="0.998555695652174">In this paper, we present a corpusbased learning method that can index diverse types of compound nouns using rules automatically extracted from a large tagged corpus. We develop an efficient way of extracting the compound noun indexing rules automatically and perform extensive experiments to evaluate our indexing rules. The automatic learning method shows about the same performance compared with the manual linguistic approach but is more portable and requires no human efforts. We also evaluate the seven different filtering methods based on both the effectiveness and the efficiency, and present a new method to solve the problems of compound noun over-generation and data sparseness in statistical compound noun processing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeongwon Cha</author>
<author>Geunbae Lee</author>
<author>Jong-Hyeok Lee</author>
</authors>
<title>Generalized unknown morpheme guessing for hybrid pos tagging of korean.</title>
<date>1998</date>
<booktitle>In Proceedings of SIXTH WORKSHOP ON VERY LARGE CORPORA in Coling-ACL 98.</booktitle>
<contexts>
<context position="18330" citStr="Cha et al., 1998" startWordPosition="2917" endWordPosition="2920">e rules is equal to the previous average precision, stop. Otherwise, go to step 2. Table 11: Improvement in the average precision of rules Learning 1 2 3 4 5 6 cycles Avg. prec. 0.19 0.23 0.39 0.44 0.45 0.45 of rules 5 Compound Noun Indexing, Filtering, and Weighting In this section, we explain how to use the automatically extracted rules to actually index the compound nouns, and describe how to filter and weight the indexed compound nouns. 5.1 Compound Noun Indexing To index compound nouns from documents, we use a natural language processing engine, SKOPE (Standard KOrean Processing Engine) (Cha et al., 1998), which processes documents by analysing words into morphemes and tagging part-of-speeches. The tagging results are compared with the automatically learned compound noun indexing rules and, if they are coincident with each other, we index them as compound nouns. Figure 5 shows a process of the compound noun indexing with an example. 5.2 Compound Noun Filtering Among the indexed compound nouns above, still there can be meaningless compound nouns, which increases the number of index terms and the search time. To solve compound noun over-generation problem, we experiment with seven different filt</context>
</contexts>
<marker>Cha, Lee, Lee, 1998</marker>
<rawString>Jeongwon Cha, Geunbae Lee, and Jong-Hyeok Lee. 1998. Generalized unknown morpheme guessing for hybrid pos tagging of korean. In Proceedings of SIXTH WORKSHOP ON VERY LARGE CORPORA in Coling-ACL 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--1</pages>
<contexts>
<context position="15823" citStr="Church and Hanks, 1990" startWordPosition="2499" endWordPosition="2502">c indexing rules. To calculate the precision, we need a defining measurement for compound noun identification. (Su et al., 1994) showed that the average mutual information of a compound noun tends to be higher than that of a noncompound noun, so we try to use the mutual information as the measure for identifying the compound nouns. If the mutual information of the compound noun candidate is higher than the average mutual information of the compound noun seeds, we decide that it is a compound noun. For mutual information (MI), we use two different equations: one for two-element compound nouns (Church and Hanks, 1990) and the other for three-element compound nouns (Su et al., 1994). The equation for two-element compound nouns is as follow: P(x,y) I(x; y) = log2 P(x) x P(y) 61 where x and y are two words in the corpus, and /(x; y) is the mutual information of these two words (in this order). Table 10 shows the average MI value of the two and three elements. Table 10: Average value of the mutual information (MI) of compound noun seeds Number of elements 2 3 Average MI 3.56 3.62 The MI was calculated from the statistics of the complete compound nouns collected from the tagged training corpus (see Section 4.1)</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Evans</author>
<author>Chenpdang Zhai</author>
</authors>
<title>Noun-phrase analysis in unrestricted text for information retrieval.</title>
<date>1996</date>
<booktitle>In Proceedingof the 34th Annual Meetinof the Association for Computational Linguistics,</booktitle>
<pages>17--24</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="3948" citStr="Evans and Zhai, 1996" startWordPosition="587" endWordPosition="590">ons in Section 6. Finally, concluding remarks are given in Section 7. 2 Previous Research 2.1 Compound Noun Indexing There have been two different methods for compound noun indexing: statistical and linguistic. In one statistical method, (Fagan, 1989) indexed phrases using six different parameters, including information on co-occurrence of phrase elements, relative location of phrase elements, etc., and achieved reasonable performance. However, his method couldn&apos;t reveal consistent substantial improvements on five experimental document collections in effectiveness. (Strzalkowski et al., 1996; Evans and Zhai, 1996) indexed subcompounds from complex noun phrases using noun-phrase analysis. These methods need to find the head-modifier relations from noun phrases and therefore require difficult syntactic parsing in Korean. For Korean, in one statistical method, (Lee and Ahn, 1996) indexed general Korean nouns using n-grams without linguistic knowledge and the experiment results showed that the proposed method might be almost as effective as the linguistic noun indexing. However, this method can generate many spurious n-grams which decrease the precision in search performance. In linguistic methods, (Kim, 1</context>
</contexts>
<marker>Evans, Zhai, 1996</marker>
<rawString>David A. Evans and Chenpdang Zhai. 1996. Noun-phrase analysis in unrestricted text for information retrieval. In Proceedingof the 34th Annual Meetinof the Association for Computational Linguistics, Santa Cruz, CA, pages 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel L Fagan</author>
</authors>
<title>The effectiveness of a nonsyntactic approach to automatic phrase indexing for document retrieval.</title>
<date>1989</date>
<journal>JASIS,</journal>
<pages>40--2</pages>
<contexts>
<context position="3578" citStr="Fagan, 1989" startWordPosition="540" endWordPosition="541">7 The remainder of this paper is organized as follows. Section 2 describes previous compound noun indexing methods for Korean and compound noun filtering methods. We show overall compound noun indexing system architecture in Section 3, and explain each module of the system in Section 4 and 5 in detail. We evaluate our method with standard Korean test collections in Section 6. Finally, concluding remarks are given in Section 7. 2 Previous Research 2.1 Compound Noun Indexing There have been two different methods for compound noun indexing: statistical and linguistic. In one statistical method, (Fagan, 1989) indexed phrases using six different parameters, including information on co-occurrence of phrase elements, relative location of phrase elements, etc., and achieved reasonable performance. However, his method couldn&apos;t reveal consistent substantial improvements on five experimental document collections in effectiveness. (Strzalkowski et al., 1996; Evans and Zhai, 1996) indexed subcompounds from complex noun phrases using noun-phrase analysis. These methods need to find the head-modifier relations from noun phrases and therefore require difficult syntactic parsing in Korean. For Korean, in one s</context>
</contexts>
<marker>Fagan, 1989</marker>
<rawString>Joel L. Fagan. 1989. The effectiveness of a nonsyntactic approach to automatic phrase indexing for document retrieval. JASIS, 40(2):115-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Fox</author>
</authors>
<title>Extending the Boolean and Vector Space Models of Information Retrieval with P-norm Queries and Multiple Concept Types.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>Cornell Univ.</institution>
<contexts>
<context position="21385" citStr="Fox, 1983" startWordPosition="3432" endWordPosition="3433">le 16). On the basis of this filtering method, we develop a smoothing method by combining the precision of rules with the mutual information of the compound noun elements, and propose our final filtering method (H) as follows: P(x Y2 â€žx Precision T(x,y) = 1Â°g2 p(x) x P(y) where a is a weighting coefficient and Precision is the applied rules learned in Section 4.3. For the three-element compound nouns, the MI part is replaced with the three-element MI equation3 (Su et al., 1994). 6 Experiment Results To calculate the similarity between a document and a query, we use the p-norm retrieval model (Fox, 1983) and use 2.0 as the p-value. We also use the component nouns in a compound as the indexing terms. We follow the standard TREC evaluation schemes (Salton and Buckley, 1991). For single index terms, we use the weighting method atn.ntc (Lee, 1995). 6.1 Compound Noun Indexing Experiments This experiment shows how well the proposed method can index diverse types of compound nouns than the previous popular methods which use human-generated compound noun indexing rules (Kim, 1994; Lee et al., 1997). For simplicity, we filtered the generated compound nouns using the mutual information of the compound </context>
</contexts>
<marker>Fox, 1983</marker>
<rawString>E. A. Fox. 1983. Extending the Boolean and Vector Space Models of Information Retrieval with P-norm Queries and Multiple Concept Types. Ph.D. thesis, Cornell Univ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Kando</author>
<author>Kyo Kageura</author>
<author>Masaharu Yoshoka</author>
<author>Keizo Oyama</author>
</authors>
<title>Phrase processing methods for japanase text retrieval.</title>
<date>1998</date>
<journal>SIGIR forum,</journal>
<pages>32--2</pages>
<contexts>
<context position="6702" citStr="Kando et al., 1998" startWordPosition="1016" endWordPosition="1019">ative case maker / Verbal common noun or adjectival common noun Noun with an adnominal ending / Noun Noun within predicate particle phrase / Noun (The two nouns before and after a slash in the pattern can form a single compound noun.) 2.2 Compound Noun Filtering Compound noun indexing methods, whether they are statistical or linguistic, tend to generate spurious compound nouns when they are actually applied. Since an information retrieval system can be evaluated by its effectiveness and also by its efficiency (van Rijsbergen, 1979), the spurious compound nouns should be efficiently filtered. (Kando et al., 1998) insisted that, for Japanese, the smaller the number of index terms is, the better the performance of the information retrieval system should be. 58 For Korean, (Won et al., 2000) showed that segmentation of compound nouns is more efficient than compound noun synthesis in search performance. There have been many works on compound noun filtering methods; (Kim, 1994) used mutual information only, and (Yun et al., 1997) used mutual information and relative frequency of POS (Part-OfSpeech) pairs together. (Lee et al., 1997) used stop word dictionaries which were constructed manually. Most of the p</context>
</contexts>
<marker>Kando, Kageura, Yoshoka, Oyama, 1998</marker>
<rawString>Noriko Kando, Kyo Kageura, Masaharu Yoshoka, and Keizo Oyama. 1998. Phrase processing methods for japanase text retrieval. SIGIR forum, 32(2):23-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pan Koo Kim</author>
</authors>
<title>The automatic indexing of compound words from korean text based on mutual information.</title>
<date>1994</date>
<journal>Journal of KISS (in Korean),</journal>
<pages>21--7</pages>
<contexts>
<context position="4552" citStr="Kim, 1994" startWordPosition="679" endWordPosition="680"> 1996) indexed subcompounds from complex noun phrases using noun-phrase analysis. These methods need to find the head-modifier relations from noun phrases and therefore require difficult syntactic parsing in Korean. For Korean, in one statistical method, (Lee and Ahn, 1996) indexed general Korean nouns using n-grams without linguistic knowledge and the experiment results showed that the proposed method might be almost as effective as the linguistic noun indexing. However, this method can generate many spurious n-grams which decrease the precision in search performance. In linguistic methods, (Kim, 1994) used five manually chosen compound noun indexing rule patterns based on linguistic knowledge. However, this method cannot index the diverse types of compound nouns. (Won et al., 2000) used a full parser and increased the precision in search experiments. However, this linguistic method cannot be applied to unrestricted texts robustly. In summary, the previous methods, whether they are statistical or linguistic, have their own shortcomings. Statistical methods require significant amounts of co-occurrence information for reasonable performance and can not index the diverse types of compound noun</context>
<context position="7069" citStr="Kim, 1994" startWordPosition="1077" endWordPosition="1078">uns when they are actually applied. Since an information retrieval system can be evaluated by its effectiveness and also by its efficiency (van Rijsbergen, 1979), the spurious compound nouns should be efficiently filtered. (Kando et al., 1998) insisted that, for Japanese, the smaller the number of index terms is, the better the performance of the information retrieval system should be. 58 For Korean, (Won et al., 2000) showed that segmentation of compound nouns is more efficient than compound noun synthesis in search performance. There have been many works on compound noun filtering methods; (Kim, 1994) used mutual information only, and (Yun et al., 1997) used mutual information and relative frequency of POS (Part-OfSpeech) pairs together. (Lee et al., 1997) used stop word dictionaries which were constructed manually. Most of the previous methods for compound noun filtering utilized only one consistent method for generated compound nouns irrespective of the different origin of compound noun indexing rules, and the methods cause many problems due to data sparseness in dictionary and training data. Our approach solves the data sparseness problem by using co-occurrence information on automatica</context>
<context position="21862" citStr="Kim, 1994" startWordPosition="3509" endWordPosition="3510">994). 6 Experiment Results To calculate the similarity between a document and a query, we use the p-norm retrieval model (Fox, 1983) and use 2.0 as the p-value. We also use the component nouns in a compound as the indexing terms. We follow the standard TREC evaluation schemes (Salton and Buckley, 1991). For single index terms, we use the weighting method atn.ntc (Lee, 1995). 6.1 Compound Noun Indexing Experiments This experiment shows how well the proposed method can index diverse types of compound nouns than the previous popular methods which use human-generated compound noun indexing rules (Kim, 1994; Lee et al., 1997). For simplicity, we filtered the generated compound nouns using the mutual information of the compound noun elements with a threshold of zero (method A in Table 12). Table 13 shows that the terms indexed by previous linguistic approach are a subset of the ones made by our statistical approach. This means that the proposed method can cover more diverse compound nouns than the 3 PD z) gx; y; z) = log2 pi(x, y, z) Tagging Result 13&lt;bbal-Ib. MC&lt; jeong-bo &gt; jC&lt;Ieul&gt; MC&lt; geom-saeg &gt; y&lt;ha&gt; eCNMG&lt;neun&gt; Indexed and Filtered Compound Nouns jeong-bo/ geom-saeg (information / retrieval</context>
</contexts>
<marker>Kim, 1994</marker>
<rawString>Pan Koo Kim. 1994. The automatic indexing of compound words from korean text based on mutual information. Journal of KISS (in Korean), 21(7):1333-1340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joon Ho Lee</author>
<author>Jeong Soo Alm</author>
</authors>
<title>Using n-grams for korean text retrieval.</title>
<date>1996</date>
<booktitle>In SIGIR&apos;96,</booktitle>
<pages>216--224</pages>
<marker>Lee, Alm, 1996</marker>
<rawString>Joon Ho Lee and Jeong Soo Alm. 1996. Using n-grams for korean text retrieval. In SIGIR&apos;96, pages 216-224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyun-A Lee</author>
<author>Jong-Hyeok Lee</author>
<author>Geunbae Lee</author>
</authors>
<title>Noun phrase indexing using clausal segmentation.</title>
<date>1997</date>
<journal>Journal of KISS (in Korean),</journal>
<pages>24--3</pages>
<contexts>
<context position="5758" citStr="Lee et al., 1997" startWordPosition="858" endWordPosition="861">mpound nouns. Linguistic methods need compound noun indexing rules described by human and sometimes result in meaningless compound nouns, which decreases the performance of information retrieval systems. They cannot also cover the various types of compound nouns because of the limitation of human linguistic knowledge. In this paper, we present a hybrid method that uses linguistic rules but these rules are automatically acquired from a large corpus through statistical learning. Our method generates more diverse compound noun indexing rule patterns than the previous standard methods (Kim, 1994; Lee et al., 1997), because previous methods use only most general rule patterns (shown in Table 2) and are based solely on human linguistic knowledge. Table 2: Typical hand-written compound noun indexing rule patterns for Korean Noun without case makers / Noun Noun with a genitive case maker / Noun Noun with a nominal case maker or an accusative case maker / Verbal common noun or adjectival common noun Noun with an adnominal ending / Noun Noun within predicate particle phrase / Noun (The two nouns before and after a slash in the pattern can form a single compound noun.) 2.2 Compound Noun Filtering Compound nou</context>
<context position="7227" citStr="Lee et al., 1997" startWordPosition="1101" endWordPosition="1104">gen, 1979), the spurious compound nouns should be efficiently filtered. (Kando et al., 1998) insisted that, for Japanese, the smaller the number of index terms is, the better the performance of the information retrieval system should be. 58 For Korean, (Won et al., 2000) showed that segmentation of compound nouns is more efficient than compound noun synthesis in search performance. There have been many works on compound noun filtering methods; (Kim, 1994) used mutual information only, and (Yun et al., 1997) used mutual information and relative frequency of POS (Part-OfSpeech) pairs together. (Lee et al., 1997) used stop word dictionaries which were constructed manually. Most of the previous methods for compound noun filtering utilized only one consistent method for generated compound nouns irrespective of the different origin of compound noun indexing rules, and the methods cause many problems due to data sparseness in dictionary and training data. Our approach solves the data sparseness problem by using co-occurrence information on automatically extracted compound noun elements together with a statistical precision measure which fits best to each rule. 3 Overall System Architecture The compound no</context>
<context position="21881" citStr="Lee et al., 1997" startWordPosition="3511" endWordPosition="3514">eriment Results To calculate the similarity between a document and a query, we use the p-norm retrieval model (Fox, 1983) and use 2.0 as the p-value. We also use the component nouns in a compound as the indexing terms. We follow the standard TREC evaluation schemes (Salton and Buckley, 1991). For single index terms, we use the weighting method atn.ntc (Lee, 1995). 6.1 Compound Noun Indexing Experiments This experiment shows how well the proposed method can index diverse types of compound nouns than the previous popular methods which use human-generated compound noun indexing rules (Kim, 1994; Lee et al., 1997). For simplicity, we filtered the generated compound nouns using the mutual information of the compound noun elements with a threshold of zero (method A in Table 12). Table 13 shows that the terms indexed by previous linguistic approach are a subset of the ones made by our statistical approach. This means that the proposed method can cover more diverse compound nouns than the 3 PD z) gx; y; z) = log2 pi(x, y, z) Tagging Result 13&lt;bbal-Ib. MC&lt; jeong-bo &gt; jC&lt;Ieul&gt; MC&lt; geom-saeg &gt; y&lt;ha&gt; eCNMG&lt;neun&gt; Indexed and Filtered Compound Nouns jeong-bo/ geom-saeg (information / retrieval) Input Sentence: b</context>
</contexts>
<marker>Lee, Lee, Lee, 1997</marker>
<rawString>Hyun-A Lee, Jong-Hyeok Lee, and Geunbae Lee. 1997. Noun phrase indexing using clausal segmentation. Journal of KISS (in Korean), 24(3):302-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joon Ho Lee</author>
</authors>
<title>Combining multiple evidence from different properties of weighting schemes.</title>
<date>1995</date>
<booktitle>In SIGIR&apos;95,</booktitle>
<pages>180--188</pages>
<contexts>
<context position="21629" citStr="Lee, 1995" startWordPosition="3475" endWordPosition="3476">on T(x,y) = 1Â°g2 p(x) x P(y) where a is a weighting coefficient and Precision is the applied rules learned in Section 4.3. For the three-element compound nouns, the MI part is replaced with the three-element MI equation3 (Su et al., 1994). 6 Experiment Results To calculate the similarity between a document and a query, we use the p-norm retrieval model (Fox, 1983) and use 2.0 as the p-value. We also use the component nouns in a compound as the indexing terms. We follow the standard TREC evaluation schemes (Salton and Buckley, 1991). For single index terms, we use the weighting method atn.ntc (Lee, 1995). 6.1 Compound Noun Indexing Experiments This experiment shows how well the proposed method can index diverse types of compound nouns than the previous popular methods which use human-generated compound noun indexing rules (Kim, 1994; Lee et al., 1997). For simplicity, we filtered the generated compound nouns using the mutual information of the compound noun elements with a threshold of zero (method A in Table 12). Table 13 shows that the terms indexed by previous linguistic approach are a subset of the ones made by our statistical approach. This means that the proposed method can cover more d</context>
</contexts>
<marker>Lee, 1995</marker>
<rawString>Joon Ho Lee. 1995. Combining multiple evidence from different properties of weighting schemes. In SIGIR&apos;95, pages 180-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chris Buckley</author>
</authors>
<title>Text retrieval conferences evaluation program. In ftp://ftp.cs.</title>
<date>1991</date>
<note>cornell. edu/pub/smart/, trec_eva1.7.0beta.tar.gz.</note>
<contexts>
<context position="21556" citStr="Salton and Buckley, 1991" startWordPosition="3461" endWordPosition="3464"> noun elements, and propose our final filtering method (H) as follows: P(x Y2 â€žx Precision T(x,y) = 1Â°g2 p(x) x P(y) where a is a weighting coefficient and Precision is the applied rules learned in Section 4.3. For the three-element compound nouns, the MI part is replaced with the three-element MI equation3 (Su et al., 1994). 6 Experiment Results To calculate the similarity between a document and a query, we use the p-norm retrieval model (Fox, 1983) and use 2.0 as the p-value. We also use the component nouns in a compound as the indexing terms. We follow the standard TREC evaluation schemes (Salton and Buckley, 1991). For single index terms, we use the weighting method atn.ntc (Lee, 1995). 6.1 Compound Noun Indexing Experiments This experiment shows how well the proposed method can index diverse types of compound nouns than the previous popular methods which use human-generated compound noun indexing rules (Kim, 1994; Lee et al., 1997). For simplicity, we filtered the generated compound nouns using the mutual information of the compound noun elements with a threshold of zero (method A in Table 12). Table 13 shows that the terms indexed by previous linguistic approach are a subset of the ones made by our s</context>
</contexts>
<marker>Salton, Buckley, 1991</marker>
<rawString>Gerard Salton and Chris Buckley. 1991. Text retrieval conferences evaluation program. In ftp://ftp.cs. cornell. edu/pub/smart/, trec_eva1.7.0beta.tar.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Louise Guthrie</author>
<author>Jussi Karlgren</author>
<author>Jum Leistensnider</author>
<author>Fang Lin</author>
<author>Jose PerezCarballo</author>
<author>Troy Straszheim</author>
<author>Jin Wang</author>
<author>Jon Wilding</author>
</authors>
<title>Natural language information retrieval:</title>
<date>1996</date>
<booktitle>Trec-5 report. In The Fifth Text REtrieval conference (TREC-5), NIST Special publication,</booktitle>
<pages>500--238</pages>
<contexts>
<context position="3925" citStr="Strzalkowski et al., 1996" startWordPosition="582" endWordPosition="586">andard Korean test collections in Section 6. Finally, concluding remarks are given in Section 7. 2 Previous Research 2.1 Compound Noun Indexing There have been two different methods for compound noun indexing: statistical and linguistic. In one statistical method, (Fagan, 1989) indexed phrases using six different parameters, including information on co-occurrence of phrase elements, relative location of phrase elements, etc., and achieved reasonable performance. However, his method couldn&apos;t reveal consistent substantial improvements on five experimental document collections in effectiveness. (Strzalkowski et al., 1996; Evans and Zhai, 1996) indexed subcompounds from complex noun phrases using noun-phrase analysis. These methods need to find the head-modifier relations from noun phrases and therefore require difficult syntactic parsing in Korean. For Korean, in one statistical method, (Lee and Ahn, 1996) indexed general Korean nouns using n-grams without linguistic knowledge and the experiment results showed that the proposed method might be almost as effective as the linguistic noun indexing. However, this method can generate many spurious n-grams which decrease the precision in search performance. In ling</context>
</contexts>
<marker>Strzalkowski, Guthrie, Karlgren, Leistensnider, Lin, PerezCarballo, Straszheim, Wang, Wilding, 1996</marker>
<rawString>Tomek Strzalkowski, Louise Guthrie, Jussi Karlgren, Jum Leistensnider, Fang Lin, Jose PerezCarballo, Troy Straszheim, Jin Wang, and Jon Wilding. 1996. Natural language information retrieval: Trec-5 report. In The Fifth Text REtrieval conference (TREC-5), NIST Special publication, pages 500-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Ming-Wen Wu</author>
<author>Jing-Shin Chang</author>
</authors>
<title>A corpus-based approach to automatic compound extraction.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL 94,</booktitle>
<pages>242--247</pages>
<contexts>
<context position="15328" citStr="Su et al., 1994" startWordPosition="2415" endWordPosition="2418">ur extraction algorithm, which were originally missing in the manual rule patterns. 4.3 Learning the Precision of Extracted Rules In the proposed method, we use the precision of rules to solve the compound noun overgeneration and the data sparseness problems. The precision of a rule can be defined by Ncandidate where Prec(rule) is the precision of a rule, Nactuai is the number of actual compound nouns, and Ncandidate is the number of compound noun candidates generated by the automatic indexing rules. To calculate the precision, we need a defining measurement for compound noun identification. (Su et al., 1994) showed that the average mutual information of a compound noun tends to be higher than that of a noncompound noun, so we try to use the mutual information as the measure for identifying the compound nouns. If the mutual information of the compound noun candidate is higher than the average mutual information of the compound noun seeds, we decide that it is a compound noun. For mutual information (MI), we use two different equations: one for two-element compound nouns (Church and Hanks, 1990) and the other for three-element compound nouns (Su et al., 1994). The equation for two-element compound </context>
<context position="21257" citStr="Su et al., 1994" startWordPosition="3408" endWordPosition="3411">g these methods, method B generated the smallest number of compound nouns best efficiency and showed the reasonable effectiveness (Table 16). On the basis of this filtering method, we develop a smoothing method by combining the precision of rules with the mutual information of the compound noun elements, and propose our final filtering method (H) as follows: P(x Y2 â€žx Precision T(x,y) = 1Â°g2 p(x) x P(y) where a is a weighting coefficient and Precision is the applied rules learned in Section 4.3. For the three-element compound nouns, the MI part is replaced with the three-element MI equation3 (Su et al., 1994). 6 Experiment Results To calculate the similarity between a document and a query, we use the p-norm retrieval model (Fox, 1983) and use 2.0 as the p-value. We also use the component nouns in a compound as the indexing terms. We follow the standard TREC evaluation schemes (Salton and Buckley, 1991). For single index terms, we use the weighting method atn.ntc (Lee, 1995). 6.1 Compound Noun Indexing Experiments This experiment shows how well the proposed method can index diverse types of compound nouns than the previous popular methods which use human-generated compound noun indexing rules (Kim,</context>
</contexts>
<marker>Su, Wu, Chang, 1994</marker>
<rawString>Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang. 1994. A corpus-based approach to automatic compound extraction. In Proceedings of ACL 94, pages 242-247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<journal>University of Computing Science, Lodon.</journal>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. University of Computing Science, Lodon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyungsuk Won</author>
<author>Mihwa Park</author>
<author>Geunbae Lee</author>
</authors>
<title>Integrated multi-level indexing method for compound noun processing.</title>
<date>2000</date>
<journal>In Journal of KISS,</journal>
<volume>27</volume>
<issue>1</issue>
<pages>84--95</pages>
<note>(in Korean),</note>
<contexts>
<context position="4736" citStr="Won et al., 2000" startWordPosition="706" endWordPosition="709">difficult syntactic parsing in Korean. For Korean, in one statistical method, (Lee and Ahn, 1996) indexed general Korean nouns using n-grams without linguistic knowledge and the experiment results showed that the proposed method might be almost as effective as the linguistic noun indexing. However, this method can generate many spurious n-grams which decrease the precision in search performance. In linguistic methods, (Kim, 1994) used five manually chosen compound noun indexing rule patterns based on linguistic knowledge. However, this method cannot index the diverse types of compound nouns. (Won et al., 2000) used a full parser and increased the precision in search experiments. However, this linguistic method cannot be applied to unrestricted texts robustly. In summary, the previous methods, whether they are statistical or linguistic, have their own shortcomings. Statistical methods require significant amounts of co-occurrence information for reasonable performance and can not index the diverse types of compound nouns. Linguistic methods need compound noun indexing rules described by human and sometimes result in meaningless compound nouns, which decreases the performance of information retrieval </context>
<context position="6881" citStr="Won et al., 2000" startWordPosition="1047" endWordPosition="1050">lash in the pattern can form a single compound noun.) 2.2 Compound Noun Filtering Compound noun indexing methods, whether they are statistical or linguistic, tend to generate spurious compound nouns when they are actually applied. Since an information retrieval system can be evaluated by its effectiveness and also by its efficiency (van Rijsbergen, 1979), the spurious compound nouns should be efficiently filtered. (Kando et al., 1998) insisted that, for Japanese, the smaller the number of index terms is, the better the performance of the information retrieval system should be. 58 For Korean, (Won et al., 2000) showed that segmentation of compound nouns is more efficient than compound noun synthesis in search performance. There have been many works on compound noun filtering methods; (Kim, 1994) used mutual information only, and (Yun et al., 1997) used mutual information and relative frequency of POS (Part-OfSpeech) pairs together. (Lee et al., 1997) used stop word dictionaries which were constructed manually. Most of the previous methods for compound noun filtering utilized only one consistent method for generated compound nouns irrespective of the different origin of compound noun indexing rules, </context>
</contexts>
<marker>Won, Park, Lee, 2000</marker>
<rawString>Hyungsuk Won, Mihwa Park, and Geunbae Lee. 2000. Integrated multi-level indexing method for compound noun processing. In Journal of KISS, 27(1) (in Korean), pages 84-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>Proceedings of ICML-97, 14th International Conference on Machine Learning,</booktitle>
<pages>412--420</pages>
<editor>In Douglas H. Fisher, editor,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Nashville, US.</location>
<marker>Yang, Jan, 1997</marker>
<rawString>Yiming Yang and Jan 0. Pedersen. 1997. A comparative study on feature selection in text categorization. In Douglas H. Fisher, editor, Proceedings of ICML-97, 14th International Conference on Machine Learning, pages 412-420, Nashville, US. Morgan Kaufmann Publishers, San Francisco, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Tae Yoon</author>
<author>Eui-Seok Jong</author>
<author>Mansuk Song</author>
</authors>
<title>Analysis of korean compound noun indexing using lexical information between nouns.</title>
<date>1998</date>
<journal>Journal of KISS (in Korean),</journal>
<pages>25--11</pages>
<contexts>
<context position="10090" citStr="Yoon et al., 1998" startWordPosition="1538" endWordPosition="1541">sly provided by ETRI, Korea. orpholo.&amp;quot; Analysis Compound Noun (Part.of-SpeeiN1Statistical Tagging Information Weighted Compound Nouns Weighting Compound Nouns Indexing en Filtering Compound Nouns Docum Compound Noun Indexing Rules with Precision Filtered Compound Nouns 59 to an English word or phrase) tagged corpus for a compound noun indexing experiment from a large document set (Korean Information Base). We collected complete compound nouns (a continuous noun sequence composed of at least two nouns on the condition that both the preceding and the following POS of the sequence are not nouns (Yoon et al., 1998)) composed of 1 - 3 nouns from the tagged training corpus (Table 4). Table 4: Statistics for complete compound nouns No. of 1 2 3 component elements Vocabulary 264,359 200,455 63,790 4.2 Extracting Indexing Rules We define a template (in Table 5) to extract the compound noun indexing rules from a POS tagged corpus. The template means that if a frontcondition-tag, a rear-condition-tag, and substring-tags are coincident with input sentence tags, the lexical item in the synthesis position of the sentence can be indexed as a compound noun as &amp;quot;x/ y (for 3-noun compounds, x / y / z)&amp;quot;. The tags used </context>
</contexts>
<marker>Yoon, Jong, Song, 1998</marker>
<rawString>Jun-Tae Yoon, Eui-Seok Jong, and Mansuk Song. 1998. Analysis of korean compound noun indexing using lexical information between nouns. Journal of KISS (in Korean), 25(11):1716-1725.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo-Hyun Yun</author>
<author>Yong-Jae Kwak</author>
<author>Hae-Chang Rim</author>
</authors>
<title>A korean information retrieval model alleviating syntactic term mismatches.</title>
<date>1997</date>
<booktitle>In Proceedings of the Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>107--112</pages>
<contexts>
<context position="7122" citStr="Yun et al., 1997" startWordPosition="1084" endWordPosition="1087">nformation retrieval system can be evaluated by its effectiveness and also by its efficiency (van Rijsbergen, 1979), the spurious compound nouns should be efficiently filtered. (Kando et al., 1998) insisted that, for Japanese, the smaller the number of index terms is, the better the performance of the information retrieval system should be. 58 For Korean, (Won et al., 2000) showed that segmentation of compound nouns is more efficient than compound noun synthesis in search performance. There have been many works on compound noun filtering methods; (Kim, 1994) used mutual information only, and (Yun et al., 1997) used mutual information and relative frequency of POS (Part-OfSpeech) pairs together. (Lee et al., 1997) used stop word dictionaries which were constructed manually. Most of the previous methods for compound noun filtering utilized only one consistent method for generated compound nouns irrespective of the different origin of compound noun indexing rules, and the methods cause many problems due to data sparseness in dictionary and training data. Our approach solves the data sparseness problem by using co-occurrence information on automatically extracted compound noun elements together with a </context>
</contexts>
<marker>Yun, Kwak, Rim, 1997</marker>
<rawString>Bo-Hyun Yun, Yong-Jae Kwak, and Hae-Chang Rim. 1997. A korean information retrieval model alleviating syntactic term mismatches. In Proceedings of the Natural Language Processing Pacific Rim Symposium, pages 107-112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>