<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.041036">
<title confidence="0.973799">
A Cross-language Study on Automatic Speech Disfluency Detection
</title>
<author confidence="0.949801">
Wen Wang
</author>
<affiliation confidence="0.723413">
SRI International
</affiliation>
<address confidence="0.701486">
Menlo Park, CA
</address>
<email confidence="0.985452">
wwang@speech.sri.com
</email>
<author confidence="0.918715">
Andreas Stolcke
</author>
<affiliation confidence="0.861807">
Microsoft Research
</affiliation>
<address confidence="0.807409">
Mountain View, CA
</address>
<email confidence="0.990223">
anstolck@microsoft.com
</email>
<author confidence="0.996707">
Jiahong Yuan, Mark Liberman
</author>
<affiliation confidence="0.99816">
University of Pennsylvania
</affiliation>
<address confidence="0.763608">
Philadelphia, PA
</address>
<email confidence="0.990114">
jiahong.yuan@gmail.com
markyliberman@gmail.com
</email>
<sectionHeader confidence="0.995536" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999639">
We investigate two systems for automatic dis-
fluency detection on English and Mandarin
conversational speech data. The first system
combines various lexical and prosodic fea-
tures in a Conditional Random Field model for
detecting edit disfluencies. The second system
combines acoustic and language model scores
for detecting filled pauses through constrained
speech recognition. We compare the contri-
butions of different knowledge sources to de-
tection performance between these two lan-
guages.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999718928571429">
Speech disfluencies are common phenomena in
spontaneous speech. They consist of spoken words
and phrases that represent self-correction, hesitation,
and floor-grabbing behaviors, but do not add seman-
tic information; removing them yields the intended,
fluent utterance. The presence of disfluencies in
conversational speech data can cause problems for
both downstream processing (parsing and other nat-
ural language processing tasks) and human readabil-
ity of speech transcripts. There has been much re-
search effort on automatic disfluency detection in
recent years (Shriberg and Stolcke, 1997; Snover
et al., 2004; Liu et al., 2006; Lin and Lee, 2009;
Schuler et al., 2010; Georgila et al., 2010; Zwarts
and Johnson, 2011), particularly from the DARPA
EARS (Effective, Affordable, Reusable Speech-to-
Text) MDE (MetaData Extraction) (DARPA Infor-
mation Processing Technology Office, 2003) pro-
gram, which focused on the automatic transcription
of sizable amounts of speech data and rendering
such transcripts in readable form, for both conversa-
tional telephone speech (CTS) and broadcast news
(BN).
However, the EARS MDE effort was focused on
English only, and there hasn’t been much research
on the effectiveness of similar automatic disfluency
detection approaches for multiple languages. This
paper presents three main innovations. First, we
extend the EARS MDE-style disfluency detection
approach combining lexical and prosodic features
using a Conditional Random Field (CRF) model,
which was employed for detecting disfluency on En-
glish conversational speech data (Liu et al., 2005),
to Mandarin conversational speech, as presented in
Section 2. Second, we implement an automatic
filled pause detection approach through constrained
speech recognition, as presented in Section 3. Third,
for both disfluency detection systems, we compare
side-by-side contributions of different knowledge
sources to detection performance for two languages,
English and Mandarin, as presented in Section 4.
Conclusions appear in Section 5.
</bodyText>
<sectionHeader confidence="0.9718755" genericHeader="method">
2 EARS MDE Style Automatic Disfluency
Detection
</sectionHeader>
<bodyText confidence="0.999946625">
We focus on two types of disfluencies, Fillers and
Edit disfluencies, following the EARS MDE disflu-
ency types modeled in (Liu et al., 2006). Fillers in-
clude filled pauses (FP), discourse markers (DM),
and explicit editing terms (ET). FPs are words used
by the speakers as floor holders to maintain con-
trol of a conversation. They can also indicate hes-
itations of the speaker. In this work, English FPs
</bodyText>
<page confidence="0.982486">
703
</page>
<subsectionHeader confidence="0.293399">
Proceedings of NAACL-HLT 2013, pages 703–708,
</subsectionHeader>
<bodyText confidence="0.995871663043478">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
comprise uh and um, based on English CTS cor-
pora. For Mandarin, Zhao and Jurafsky found that
Mandarin speakers intensively used both demonstra-
tives zhege (literally ‘this’) and nage (literally ‘that’)
and uh/mm as FPs based on a large speech corpus of
Mandarin telephone conversation (Zhao and Juraf-
sky, 2005). We study the same set of Chinese FPs in
this study. DMs are words or phrases related to the
structure of the discourse and help taking or keeping
a turn, or serving as acknowledgment, for example,
I mean, you know. An explicit ET is an editing term
in an edit disfluency that is not an FP or a DM. For
example, we have two action items x sorry three ac-
tion items from the meeting, where sorry is an ex-
plicit ET.
Edit disfluencies involve syntactically relevant
content that is either repeated, revised, or aban-
doned. The basic pattern for edit disfluencies has
the form (reparandum) x &lt;editing term&gt; correc-
tion. The reparandum is the portion of the utterance
that is corrected or abandoned entirely (in the case
of restarts). An interruption point (IP), marked with
‘x’ in the pattern, is the point at which the speaker
breaks off the original utterance and then repeats,
revises, or restarts the utterance. The editing term
is optional and consists of one or more filler words.
The correction is the portion of the utterance that
corrects the original reparandum. Revisions denote
the cases when a speaker modifies the original utter-
ance with a similar syntactic structure, e.g., we have
two action itemsxsorry three action items from the
meeting. Restarts denote the cases when a speaker
abandons an utterance or a constituent and restarts
all over again, e.g., HexI like this idea.
We used a CRF model to combine lexical features,
shallow syntactic features, and prosodic features for
joint detection of edit words and IP words. A CRF
defines a global log-linear distribution of the state
(or label) sequenceEconditioned on an observation
sequence, in our case including the word sequence
Wand the featuresF, and optimized globally over
the entire sequence considering the context event in-
formation for making decisions at each point. We
used the Mallet package (McCallum, 2002) to im-
plement the CRF model. We used a first-order model
that includes only two sequential events in the fea-
ture set. The CRF model is trained to maximize
the conditional log-likelihood of a given training
set P(E W, F). During testing, the most likely se-
quence E is found using the Viterbi algorithm. To
avoid over-fitting, a zero-mean Gaussian prior (Mc-
Callum and Li, 2003) was applied to the parame-
ters, where the variance of the prior was optimized
on the development test set. Each word is associ-
ated with a class label, representing whether it is
an edit word or not. We included IP in the target
classes and used five states, as outside edit (O), be-
gin edit with an IP (B-E+IP), begin edit (B-E), in-
side edit with an IP (I-E+IP), and inside edit (I-
E) (Liu et al., 2006). State transitions are also the
same as in (Liu et al., 2006). We built a Hidden
Markov Model (HMM) based part-of-speech (POS)
taggers for English conversational speech and Man-
darin broadcast conversation data. After employing
the co-training approach described in (Wang et al.,
2007), we achieved 94% POS tagging accuracy for
both data sets. The features for CRF modeling in-
clude: n-grams from words and automatically gen-
erated POS tags, speaker turns, whether there is a
repeated word sequence ending at a word bound-
ary, whether a word is a fragment, whether there
is a predefined filler phrase after the word bound-
ary, and the prosody model posterior probabilities
from a decision tree model (Shriberg and Stolcke,
1997) and discretized by cumulative binning (Liu et
al., 2006). The prosodic features were computed
for each interword boundary from words and pho-
netic alignments of the manual transcriptions. We
extracted the same set of prosodic features for En-
glish and Mandarin data, based on duration, funda-
mental frequency (f0), energy, and pause informa-
tion, and nonprosodic information such as speaker
gender and speaker change, for training and apply-
ing the decision-tree-based prosody model (Liu et
al., 2006).
We implemented a rule-based system for filler
word detection. We defined a list of possible Chi-
nese and English filler words, including filled pauses
and discourse markers. The rules also explore POS
tags assigned by our Chinese and English POS tag-
gers.
</bodyText>
<page confidence="0.996388">
704
</page>
<sectionHeader confidence="0.857662" genericHeader="method">
3 Constrained Speech Recognition for
Filled Pause Detection
</sectionHeader>
<bodyText confidence="0.999973913978495">
We also propose an alternative approach for auto-
matic detection of FPs given speech transcripts that
omit FPs but are otherwise accurate. This approach
is motivated by situations where only an edited,
“cleaned-up” transcript is available, but where an
accurate verbatim transcript is to be recovered au-
tomatically. We treat this task as a constrained
speech recognition problem, and investigate how ef-
fectively it is solved by a state-of-the-art large vo-
cabulary continuous speech recognition (LVCSR)
system. Hence, this approach can be considered as
combining LVCSR acoustic model (AM) and lan-
guage model (LM) knowledge sources in a search
framework for FP detection. Compared to the FP
detection component in the disfluency detection sys-
tems described in Section 2, this alternative ap-
proach explores different knowledge sources. In
particular, the AMs explore different front-end fea-
tures compared to the lexical and prosodic features
explored in those disfluency detection systems pre-
sented in Section 2. Details of the front-end features
are illustrated below.
We evaluated this approach on both English and
Mandarin conversational speech. For detecting FPs
in English conversational speech, we used a mod-
ified and simplified form of the recognition sys-
tem developed for the 2004 NIST Rich Transcrip-
tion Conversational Telephone Speech (CTS) eval-
uations, described in (Stolcke et al., 2006). The
first pass of the recognizer uses a within-word
MFCC+MLP model (i.e, trained on Mel-frequency
cepstral coefficient (MFCC) features augmented
with Multi-Layer Perceptron (MLP) based phone-
posterior features), while the second pass uses a
cross-word model trained on Perceptual Linear Pre-
diction (PLP) features adapted (by speaker) to the
output of the first pass. For purposes of FP detec-
tion, the recognition is constrained to a word lat-
tice formed by the manually transcribed non-FP ref-
erence words, with optional FP words inserted be-
tween any two words and at the beginning and end
of each utterance. Both first and second pass de-
coding was constrained by the optional-FP lattices.
In the second pass, HTK lattices were generated
with bigram LM probabilities and rescored with a
4-gram LM. The consensus decoding output from
the rescored lattices was used for scoring FP detec-
tion. The system thus evaluates the posterior prob-
ability of an FP at every word boundary using both
acoustic model (AM) and language model (LM) ev-
idence. The acoustic model for the English recog-
nition system was trained on about 2300 hours of
CTS data. The language models (which models FP
like any other word) are bigram and 4-gram statisti-
cal word n-gram LMs estimated from the same data
plus additional non-CTS data and web data.
For detecting FPs in Mandarin broadcast con-
versation speech, we used a modified form of
the recognition system developed for the 2008
DARPA GALE (Global Autonomous Language Ex-
ploitation) Speech-to-Text evaluation, described in
(Lei et al., 2009). The system conducted a con-
strained decoding on the optional-FP lattices, using
a speaker-independent within-word triphone MPE-
trained MFCC+pitch+MLP model and a pruned
trigram LM. For the Mandarin ASR system, the
MFCC+MLP front-end features were augmented
with 3-dimension smoothed pitch features (Lei et al.,
2006). HTK lattices were generated with probabil-
ities from the pruned trigram LM and rescored by
the full trigram LM. The consensus decoding output
from the rescored lattices was used for scoring FP
detection. The AMs for this system were trained on
1642 hours of Mandarin broadcast news and conver-
sation speech data and the LMs were trained on 1.4
billion words comprising a variety of resources. De-
tails of training data and system development were
illustrated in (Lei et al., 2009).
This procedure is similar to forced aligning the
word lattices to the audio data (Finke and Waibel,
1997). Both Finke et al.’s approach (Finke and
Waibel, 1997) and our approach built a lattice from
each transcription sentence (in our approach, op-
tional filled pauses are inserted between any two
words and at the beginning and end of each utter-
ance). Then Finke et al. force-aligned the lattice
with utterance; whereas, we used multi-pass con-
strained decoding with within-word and cross-word
models, MLLR adaptation of the acoustic models,
and rescoring with a higher-order n-gram LM, so the
performance will be better than just flexible align-
ment to the lattices. Note that when constructing
the word lattices with optional FP words, for En-
</bodyText>
<page confidence="0.996548">
705
</page>
<bodyText confidence="0.999353571428571">
glish, the optional FP words are a choice between
uh and um. For Mandarin, the optional FP words
are a choice between uh, mm, zhege, and nage. We
assigned equal weights to FP words.
to investigate the prosodic features considering the
special characteristics of edited disfluencies in Man-
darin studied in (Lin and Lee, 2009).
</bodyText>
<sectionHeader confidence="0.996763" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999904375">
Scoring of EARS MDE-style automatic disfluency
detection output is done using the NIST tools 1,
computing the error rate as the average number of
misclassified words per reference event word. For
English, the training and evaluation data were from
the 40 hours CTS data in the NIST RT-04F MDE
training data including speech, their transcriptions
and disfluency annotations by LDC. We randomly
held out two 3-hour subsets from this training data
set for evaluation and parameter tuning respectively,
and used the remaining data for training. Note
that for Mandarin, there is no LDC released Man-
darin MDE training data. We adapted the English
MDE annotation guidelines for Mandarin and man-
ually annotated the manual transcripts of 92 Man-
darin broadcast conversation (BC) shows released
by LDC under the DARPA GALE program, for edit
disfluencies and filler words. We randomly held out
two 3-hour subsets from the 92 shows for evalu-
ation and parameter tuning respectively, and man-
ually corrected disfluency annotation errors on the
evaluation set.
Table 1 shows the results in NIST error rate (%)
for edit word, IP, and filler word detection. We ob-
serve that adding POS features improves edit word,
edit IP, and filler word detection for both languages,
and adding a prosody model produced further im-
provement (note that filler word detection systems
did not employ prosodic features). The gains from
combining the word, POS, and prosody model over
the word n-gram baseline are statistically significant
for both languages (confidence levelp&lt;0.05using
matched pair test). Also, adding the prosody model
over word+POS yielded a larger relative gain in edit
word+IP detection performance for Mandarin than
for English data. A preliminary study of these re-
sults has shown that the prosody model contributes
differently for different types of disfluencies for En-
glish and Mandarin conversational speech and we
will continue this study in future work. We also plan
</bodyText>
<footnote confidence="0.840978">
1www.itl.nist.gov/iad/mig/tests/rt/2004-fall/index.html
</footnote>
<tableCaption confidence="0.990971">
Table 1: NIST error rate (%) for edit word, IP, and filler
word detection on the English and Mandarin test set,
using word n-gram features, POS n-gram features, and
prosody model.
</tableCaption>
<table confidence="0.999812">
Feature NIST Error Rate (%)
Edit Word Edit IP Filler Word
Word English
53.0 38.7 31.2
+POS 52.6 38.2 29.8
++Prosody 52.3 38.0 29.8
Word Mandarin
58.5 42.8 33.4
+POS 57.7 42.1 32.9
++Prosody 56.9 41.5 32.9
</table>
<bodyText confidence="0.998757821428571">
For evaluating constrained speech recognition for
FP detection, the English test set of conversational
speech data and word transcripts is derived from
the CTS subset of the NIST 2002 Rich Transcrip-
tion evaluation. The waveforms were segmented ac-
cording to utterance boundaries given by the human-
generated transcripts, resulting in 6554 utterance
segments with a total duration of 6.8 hours. We then
excluded turns that have fewer than five tokens or
have two or more FPs in a row (such as ‘uh um’ and
‘uh, uh’), resulting in 3359 segments. This yields
the test set from which we computed English FP de-
tection scores. The transcripts of this test set con-
tain 54511 non-FP words and 1394 FPs, transcribed
as either uh or um. When evaluating FP detection
performance, these two orthographical forms were
mapped to a single token type, so recognizing one
form as the other is not penalized. The Mandarin
test set is the DARPA GALE 2008 Mandarin speech-
to-text development test set of 1 hour duration. The
transcripts of this test set contain 9820 non-FP words
and 370 FP words, transcribed as uh, mm, zhege,
and nage. We collapsed them to a single token type
for FP scoring. We evaluated FP detection perfor-
mance in terms of both false alarm (incorrect detec-
tion) and miss (failed detection) rates, shown in Ta-
ble 2. We observed that adding pronunciation scores
didn’t change thePf,,andP,,�ss. On the English
</bodyText>
<page confidence="0.994215">
706
</page>
<bodyText confidence="0.999663714285714">
test set, adding LM scores degraded Pm ss but im-
proved Pfa . However, on the Mandarin test set, in-
creasing LM weight improved bothPmissandPfa,
suggesting that for the Mandarin LVCSR system in
this study, the LM could provide complementary in-
formation to the AM to discriminate FP and non-FP
words.
</bodyText>
<tableCaption confidence="0.9755745">
Table 2: Probabilities of false alarms (FAs) and misses in
FP detection on the English and Mandarin test set w.r.t.
acoustic model weightwa, language model weightwg,
and pronunciation score weightwp.
</tableCaption>
<table confidence="0.994903142857143">
fZVa,ZVg,ZVpg FAs (%) Misses (%)
f1,0,8g English
1.76 3.23
f1,8,8g 1.18 4.73
f1,0,8g Mandarin
1.19 19.68
f1,8,8g 0.76 16.76
</table>
<sectionHeader confidence="0.997642" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999994583333333">
In conclusion, we have presented two automatic dis-
fluency detection systems, one combining various
lexical and prosodic features, and the other com-
bining LVCSR acoustic and language model knowl-
edge sources. We observed significant improve-
ments in combining lexical and prosodic features
over just employing word n-gram features, for both
languages. When combining AM and LM knowl-
edge sources for FP detection in constrained speech
recognition, we found increasing LM weight im-
proved both false alarm and miss rates for Mandarin
but degraded the miss rate for English.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999776666666667">
The authors thank all the anonymous reviewers of
this paper for valuable suggestions. This work is
supported in part by NSF grant IIS-0964556.
</bodyText>
<sectionHeader confidence="0.998626" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998690677966102">
DARPA Information Processing Technology Office.
2003. Effective,affordable, reusable speech-to-text
(EARS). http://www.darpa.mil/ipto/programs/ears.
Michael Finke and Alex Waibel. 1997. Flexible tran-
scription alignment. In IEEE Workshop on Speech
Recognition and Understanding, pages 34–40.
K. Georgila, N. Wang, and J. Gratch. 2010. Cross-
domain speech disfluency detection. In Proceedings
of SIGDIAL, pages 237–240, Tokyo.
X. Lei, M. Siu, M.Y. Hwang, M. Ostendorf, and T. Lee.
2006. Improved tone modeling for Mandarin broad-
cast news speech recognition. In Proceedings ofInter-
speech.
X. Lei, W. Wu, W. Wang, A. Mandal, and A. Stolcke.
2009. Development of the 2008 SRI Mandarin speech-
to-text system for broadcast news and conversation. In
Proceedings ofInterspeech, Brighton, UK.
C. K. Lin and L. S. Lee. 2009. Improved features
and models for detecting edit disfluencies in transcrib-
ing spontaneous mandarin speech. IEEE Transac-
tions on Audio, Speech, and Language Processing,
17(7):1263–1278, September.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, and Mary
Harper. 2005. Comparing HMM, maximum entropy,
and conditional random fields for disfluency detec-
tion. In Proc. Interspeech, pages 3313–3316, Lisbon,
September.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 14(5):1526–1540, September. Special Issue
on Progress in Rich Transcription.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields. In
Proceedings of the CoNLL.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
W. Schuler, S. AbdelRahman, T. Miller, and L. Schwartz.
2010. Broad-coverage incremental parsing using
human-like memory constraints. Computational Lin-
guistics, 36(1).
E. Shriberg and A. Stolcke. 1997. A prosody-only
decision-tree model for disfluency detection. In Pro-
ceedings of Eurospeech, pages 2383–2386.
M. Snover, B. Dorr, and R. Schwartz. 2004. A lexically-
driven algorithm for disfluency detection. In Su-
san Dumais, Daniel Marcu, and Salim Roukos, edi-
tors, Proc. HLT-NAACL, Boston, May. Association for
Computational Linguistics.
Andreas Stolcke, Barry Chen, Horacio Franco, Venkata
Ramana Rao Gadde, Martin Graciarena, Mei-Yuh
Hwang, Katrin Kirchhoff, Arindam Mandal, Nelson
Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal
S¨onmez, Anand Venkataraman, Dimitra Vergyri, Wen
Wang, Jing Zheng, and Qifeng Zhu. 2006. Recent in-
novations in speech-to-text transcription at SRI-ICSI-
UW. IEEE Trans. Audio, Speech, and Lang. Pro-
</reference>
<page confidence="0.970435">
707
</page>
<reference confidence="0.999677076923077">
cess., 14(5):1729–1744, September. Special Issue on
Progress in Rich Transcription.
W. Wang, Z. Huang, and M. P. Harper. 2007. Semi-
supervised learning for part-of-speech tagging of Man-
darin transcribed speech. In Proceedings of ICASSP,
pages 137–140.
Y. Zhao and D. Jurafsky. 2005. A preliminary study of
mandarin filled pause. In Proceedings of DISS, pages
179–182, Aix-en-Provence.
S. Zwarts and M. Johnson. 2011. The impact of lan-
guage models and loss functions on repair disfluency
detection. In Proceedings of ACL/HLT, pages 703–
711, Portland.
</reference>
<page confidence="0.997065">
708
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.483764">
<title confidence="0.999937">A Cross-language Study on Automatic Speech Disfluency Detection</title>
<author confidence="0.975913">Wen</author>
<affiliation confidence="0.948656">SRI</affiliation>
<address confidence="0.967617">Menlo Park, CA</address>
<email confidence="0.99926">wwang@speech.sri.com</email>
<author confidence="0.955419">Andreas</author>
<affiliation confidence="0.974483">Microsoft</affiliation>
<address confidence="0.967643">Mountain View, CA</address>
<email confidence="0.99848">anstolck@microsoft.com</email>
<author confidence="0.997266">Jiahong Yuan</author>
<author confidence="0.997266">Mark</author>
<affiliation confidence="0.999853">University of</affiliation>
<address confidence="0.657577">Philadelphia, PA</address>
<email confidence="0.998804">markyliberman@gmail.com</email>
<abstract confidence="0.989448076923077">We investigate two systems for automatic disfluency detection on English and Mandarin conversational speech data. The first system combines various lexical and prosodic features in a Conditional Random Field model for detecting edit disfluencies. The second system combines acoustic and language model scores for detecting filled pauses through constrained speech recognition. We compare the contributions of different knowledge sources to detection performance between these two languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2003</date>
<booktitle>DARPA Information Processing Technology Office.</booktitle>
<note>Effective,affordable, reusable speech-to-text (EARS). http://www.darpa.mil/ipto/programs/ears.</note>
<marker>2003</marker>
<rawString>DARPA Information Processing Technology Office. 2003. Effective,affordable, reusable speech-to-text (EARS). http://www.darpa.mil/ipto/programs/ears.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Finke</author>
<author>Alex Waibel</author>
</authors>
<title>Flexible transcription alignment.</title>
<date>1997</date>
<booktitle>In IEEE Workshop on Speech Recognition and Understanding,</booktitle>
<pages>34--40</pages>
<contexts>
<context position="11807" citStr="Finke and Waibel, 1997" startWordPosition="1856" endWordPosition="1859">hed pitch features (Lei et al., 2006). HTK lattices were generated with probabilities from the pruned trigram LM and rescored by the full trigram LM. The consensus decoding output from the rescored lattices was used for scoring FP detection. The AMs for this system were trained on 1642 hours of Mandarin broadcast news and conversation speech data and the LMs were trained on 1.4 billion words comprising a variety of resources. Details of training data and system development were illustrated in (Lei et al., 2009). This procedure is similar to forced aligning the word lattices to the audio data (Finke and Waibel, 1997). Both Finke et al.’s approach (Finke and Waibel, 1997) and our approach built a lattice from each transcription sentence (in our approach, optional filled pauses are inserted between any two words and at the beginning and end of each utterance). Then Finke et al. force-aligned the lattice with utterance; whereas, we used multi-pass constrained decoding with within-word and cross-word models, MLLR adaptation of the acoustic models, and rescoring with a higher-order n-gram LM, so the performance will be better than just flexible alignment to the lattices. Note that when constructing the word la</context>
</contexts>
<marker>Finke, Waibel, 1997</marker>
<rawString>Michael Finke and Alex Waibel. 1997. Flexible transcription alignment. In IEEE Workshop on Speech Recognition and Understanding, pages 34–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>N Wang</author>
<author>J Gratch</author>
</authors>
<title>Crossdomain speech disfluency detection.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>237--240</pages>
<location>Tokyo.</location>
<contexts>
<context position="1530" citStr="Georgila et al., 2010" startWordPosition="209" endWordPosition="212">sist of spoken words and phrases that represent self-correction, hesitation, and floor-grabbing behaviors, but do not add semantic information; removing them yields the intended, fluent utterance. The presence of disfluencies in conversational speech data can cause problems for both downstream processing (parsing and other natural language processing tasks) and human readability of speech transcripts. There has been much research effort on automatic disfluency detection in recent years (Shriberg and Stolcke, 1997; Snover et al., 2004; Liu et al., 2006; Lin and Lee, 2009; Schuler et al., 2010; Georgila et al., 2010; Zwarts and Johnson, 2011), particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) program, which focused on the automatic transcription of sizable amounts of speech data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN). However, the EARS MDE effort was focused on English only, and there hasn’t been much research on the effectiveness of similar automatic disfluency detection approaches for multiple languages. This pap</context>
</contexts>
<marker>Georgila, Wang, Gratch, 2010</marker>
<rawString>K. Georgila, N. Wang, and J. Gratch. 2010. Crossdomain speech disfluency detection. In Proceedings of SIGDIAL, pages 237–240, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Lei</author>
<author>M Siu</author>
<author>M Y Hwang</author>
<author>M Ostendorf</author>
<author>T Lee</author>
</authors>
<title>Improved tone modeling for Mandarin broadcast news speech recognition.</title>
<date>2006</date>
<booktitle>In Proceedings ofInterspeech.</booktitle>
<contexts>
<context position="11221" citStr="Lei et al., 2006" startWordPosition="1757" endWordPosition="1760">he same data plus additional non-CTS data and web data. For detecting FPs in Mandarin broadcast conversation speech, we used a modified form of the recognition system developed for the 2008 DARPA GALE (Global Autonomous Language Exploitation) Speech-to-Text evaluation, described in (Lei et al., 2009). The system conducted a constrained decoding on the optional-FP lattices, using a speaker-independent within-word triphone MPEtrained MFCC+pitch+MLP model and a pruned trigram LM. For the Mandarin ASR system, the MFCC+MLP front-end features were augmented with 3-dimension smoothed pitch features (Lei et al., 2006). HTK lattices were generated with probabilities from the pruned trigram LM and rescored by the full trigram LM. The consensus decoding output from the rescored lattices was used for scoring FP detection. The AMs for this system were trained on 1642 hours of Mandarin broadcast news and conversation speech data and the LMs were trained on 1.4 billion words comprising a variety of resources. Details of training data and system development were illustrated in (Lei et al., 2009). This procedure is similar to forced aligning the word lattices to the audio data (Finke and Waibel, 1997). Both Finke e</context>
</contexts>
<marker>Lei, Siu, Hwang, Ostendorf, Lee, 2006</marker>
<rawString>X. Lei, M. Siu, M.Y. Hwang, M. Ostendorf, and T. Lee. 2006. Improved tone modeling for Mandarin broadcast news speech recognition. In Proceedings ofInterspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Lei</author>
<author>W Wu</author>
<author>W Wang</author>
<author>A Mandal</author>
<author>A Stolcke</author>
</authors>
<title>Development of the 2008 SRI Mandarin speechto-text system for broadcast news and conversation.</title>
<date>2009</date>
<booktitle>In Proceedings ofInterspeech,</booktitle>
<location>Brighton, UK.</location>
<contexts>
<context position="10905" citStr="Lei et al., 2009" startWordPosition="1712" endWordPosition="1715">n FP at every word boundary using both acoustic model (AM) and language model (LM) evidence. The acoustic model for the English recognition system was trained on about 2300 hours of CTS data. The language models (which models FP like any other word) are bigram and 4-gram statistical word n-gram LMs estimated from the same data plus additional non-CTS data and web data. For detecting FPs in Mandarin broadcast conversation speech, we used a modified form of the recognition system developed for the 2008 DARPA GALE (Global Autonomous Language Exploitation) Speech-to-Text evaluation, described in (Lei et al., 2009). The system conducted a constrained decoding on the optional-FP lattices, using a speaker-independent within-word triphone MPEtrained MFCC+pitch+MLP model and a pruned trigram LM. For the Mandarin ASR system, the MFCC+MLP front-end features were augmented with 3-dimension smoothed pitch features (Lei et al., 2006). HTK lattices were generated with probabilities from the pruned trigram LM and rescored by the full trigram LM. The consensus decoding output from the rescored lattices was used for scoring FP detection. The AMs for this system were trained on 1642 hours of Mandarin broadcast news a</context>
</contexts>
<marker>Lei, Wu, Wang, Mandal, Stolcke, 2009</marker>
<rawString>X. Lei, W. Wu, W. Wang, A. Mandal, and A. Stolcke. 2009. Development of the 2008 SRI Mandarin speechto-text system for broadcast news and conversation. In Proceedings ofInterspeech, Brighton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Lin</author>
<author>L S Lee</author>
</authors>
<title>Improved features and models for detecting edit disfluencies in transcribing spontaneous mandarin speech.</title>
<date>2009</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>17</volume>
<issue>7</issue>
<contexts>
<context position="1485" citStr="Lin and Lee, 2009" startWordPosition="201" endWordPosition="204">phenomena in spontaneous speech. They consist of spoken words and phrases that represent self-correction, hesitation, and floor-grabbing behaviors, but do not add semantic information; removing them yields the intended, fluent utterance. The presence of disfluencies in conversational speech data can cause problems for both downstream processing (parsing and other natural language processing tasks) and human readability of speech transcripts. There has been much research effort on automatic disfluency detection in recent years (Shriberg and Stolcke, 1997; Snover et al., 2004; Liu et al., 2006; Lin and Lee, 2009; Schuler et al., 2010; Georgila et al., 2010; Zwarts and Johnson, 2011), particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) program, which focused on the automatic transcription of sizable amounts of speech data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN). However, the EARS MDE effort was focused on English only, and there hasn’t been much research on the effectiveness of similar automatic disfluency detectio</context>
<context position="12772" citStr="Lin and Lee, 2009" startWordPosition="2015" endWordPosition="2018">decoding with within-word and cross-word models, MLLR adaptation of the acoustic models, and rescoring with a higher-order n-gram LM, so the performance will be better than just flexible alignment to the lattices. Note that when constructing the word lattices with optional FP words, for En705 glish, the optional FP words are a choice between uh and um. For Mandarin, the optional FP words are a choice between uh, mm, zhege, and nage. We assigned equal weights to FP words. to investigate the prosodic features considering the special characteristics of edited disfluencies in Mandarin studied in (Lin and Lee, 2009). 4 Experimental Results Scoring of EARS MDE-style automatic disfluency detection output is done using the NIST tools 1, computing the error rate as the average number of misclassified words per reference event word. For English, the training and evaluation data were from the 40 hours CTS data in the NIST RT-04F MDE training data including speech, their transcriptions and disfluency annotations by LDC. We randomly held out two 3-hour subsets from this training data set for evaluation and parameter tuning respectively, and used the remaining data for training. Note that for Mandarin, there is n</context>
</contexts>
<marker>Lin, Lee, 2009</marker>
<rawString>C. K. Lin and L. S. Lee. 2009. Improved features and models for detecting edit disfluencies in transcribing spontaneous mandarin speech. IEEE Transactions on Audio, Speech, and Language Processing, 17(7):1263–1278, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Mary Harper</author>
</authors>
<title>Comparing HMM, maximum entropy, and conditional random fields for disfluency detection.</title>
<date>2005</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>3313--3316</pages>
<location>Lisbon,</location>
<contexts>
<context position="2418" citStr="Liu et al., 2005" startWordPosition="336" endWordPosition="339">ch data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN). However, the EARS MDE effort was focused on English only, and there hasn’t been much research on the effectiveness of similar automatic disfluency detection approaches for multiple languages. This paper presents three main innovations. First, we extend the EARS MDE-style disfluency detection approach combining lexical and prosodic features using a Conditional Random Field (CRF) model, which was employed for detecting disfluency on English conversational speech data (Liu et al., 2005), to Mandarin conversational speech, as presented in Section 2. Second, we implement an automatic filled pause detection approach through constrained speech recognition, as presented in Section 3. Third, for both disfluency detection systems, we compare side-by-side contributions of different knowledge sources to detection performance for two languages, English and Mandarin, as presented in Section 4. Conclusions appear in Section 5. 2 EARS MDE Style Automatic Disfluency Detection We focus on two types of disfluencies, Fillers and Edit disfluencies, following the EARS MDE disfluency types mode</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Harper, 2005</marker>
<rawString>Yang Liu, Elizabeth Shriberg, Andreas Stolcke, and Mary Harper. 2005. Comparing HMM, maximum entropy, and conditional random fields for disfluency detection. In Proc. Interspeech, pages 3313–3316, Lisbon, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dustin Hillard</author>
<author>Mari Ostendorf</author>
<author>Mary Harper</author>
</authors>
<title>Enriching speech recognition with automatic detection of sentence boundaries and disfluencies.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<booktitle>Special Issue on Progress in Rich Transcription.</booktitle>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="1466" citStr="Liu et al., 2006" startWordPosition="197" endWordPosition="200">encies are common phenomena in spontaneous speech. They consist of spoken words and phrases that represent self-correction, hesitation, and floor-grabbing behaviors, but do not add semantic information; removing them yields the intended, fluent utterance. The presence of disfluencies in conversational speech data can cause problems for both downstream processing (parsing and other natural language processing tasks) and human readability of speech transcripts. There has been much research effort on automatic disfluency detection in recent years (Shriberg and Stolcke, 1997; Snover et al., 2004; Liu et al., 2006; Lin and Lee, 2009; Schuler et al., 2010; Georgila et al., 2010; Zwarts and Johnson, 2011), particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) program, which focused on the automatic transcription of sizable amounts of speech data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN). However, the EARS MDE effort was focused on English only, and there hasn’t been much research on the effectiveness of similar automatic </context>
<context position="3043" citStr="Liu et al., 2006" startWordPosition="427" endWordPosition="430">darin conversational speech, as presented in Section 2. Second, we implement an automatic filled pause detection approach through constrained speech recognition, as presented in Section 3. Third, for both disfluency detection systems, we compare side-by-side contributions of different knowledge sources to detection performance for two languages, English and Mandarin, as presented in Section 4. Conclusions appear in Section 5. 2 EARS MDE Style Automatic Disfluency Detection We focus on two types of disfluencies, Fillers and Edit disfluencies, following the EARS MDE disfluency types modeled in (Liu et al., 2006). Fillers include filled pauses (FP), discourse markers (DM), and explicit editing terms (ET). FPs are words used by the speakers as floor holders to maintain control of a conversation. They can also indicate hesitations of the speaker. In this work, English FPs 703 Proceedings of NAACL-HLT 2013, pages 703–708, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics comprise uh and um, based on English CTS corpora. For Mandarin, Zhao and Jurafsky found that Mandarin speakers intensively used both demonstratives zhege (literally ‘this’) and nage (literally ‘that’) and</context>
<context position="6407" citStr="Liu et al., 2006" startWordPosition="998" endWordPosition="1001">conditional log-likelihood of a given training set P(E W, F). During testing, the most likely sequence E is found using the Viterbi algorithm. To avoid over-fitting, a zero-mean Gaussian prior (McCallum and Li, 2003) was applied to the parameters, where the variance of the prior was optimized on the development test set. Each word is associated with a class label, representing whether it is an edit word or not. We included IP in the target classes and used five states, as outside edit (O), begin edit with an IP (B-E+IP), begin edit (B-E), inside edit with an IP (I-E+IP), and inside edit (IE) (Liu et al., 2006). State transitions are also the same as in (Liu et al., 2006). We built a Hidden Markov Model (HMM) based part-of-speech (POS) taggers for English conversational speech and Mandarin broadcast conversation data. After employing the co-training approach described in (Wang et al., 2007), we achieved 94% POS tagging accuracy for both data sets. The features for CRF modeling include: n-grams from words and automatically generated POS tags, speaker turns, whether there is a repeated word sequence ending at a word boundary, whether a word is a fragment, whether there is a predefined filler phrase af</context>
<context position="7628" citStr="Liu et al., 2006" startWordPosition="1193" endWordPosition="1196">the word boundary, and the prosody model posterior probabilities from a decision tree model (Shriberg and Stolcke, 1997) and discretized by cumulative binning (Liu et al., 2006). The prosodic features were computed for each interword boundary from words and phonetic alignments of the manual transcriptions. We extracted the same set of prosodic features for English and Mandarin data, based on duration, fundamental frequency (f0), energy, and pause information, and nonprosodic information such as speaker gender and speaker change, for training and applying the decision-tree-based prosody model (Liu et al., 2006). We implemented a rule-based system for filler word detection. We defined a list of possible Chinese and English filler words, including filled pauses and discourse markers. The rules also explore POS tags assigned by our Chinese and English POS taggers. 704 3 Constrained Speech Recognition for Filled Pause Detection We also propose an alternative approach for automatic detection of FPs given speech transcripts that omit FPs but are otherwise accurate. This approach is motivated by situations where only an edited, “cleaned-up” transcript is available, but where an accurate verbatim transcript</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Hillard, Ostendorf, Harper, 2006</marker>
<rawString>Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin Hillard, Mari Ostendorf, and Mary Harper. 2006. Enriching speech recognition with automatic detection of sentence boundaries and disfluencies. IEEE Transactions on Audio, Speech, and Language Processing, 14(5):1526–1540, September. Special Issue on Progress in Rich Transcription.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>W Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the CoNLL.</booktitle>
<contexts>
<context position="6006" citStr="McCallum and Li, 2003" startWordPosition="919" endWordPosition="923">rvation sequence, in our case including the word sequence Wand the featuresF, and optimized globally over the entire sequence considering the context event information for making decisions at each point. We used the Mallet package (McCallum, 2002) to implement the CRF model. We used a first-order model that includes only two sequential events in the feature set. The CRF model is trained to maximize the conditional log-likelihood of a given training set P(E W, F). During testing, the most likely sequence E is found using the Viterbi algorithm. To avoid over-fitting, a zero-mean Gaussian prior (McCallum and Li, 2003) was applied to the parameters, where the variance of the prior was optimized on the development test set. Each word is associated with a class label, representing whether it is an edit word or not. We included IP in the target classes and used five states, as outside edit (O), begin edit with an IP (B-E+IP), begin edit (B-E), inside edit with an IP (I-E+IP), and inside edit (IE) (Liu et al., 2006). State transitions are also the same as in (Liu et al., 2006). We built a Hidden Markov Model (HMM) based part-of-speech (POS) taggers for English conversational speech and Mandarin broadcast conver</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>A. McCallum and W. Li. 2003. Early results for named entity recognition with conditional random fields. In Proceedings of the CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="5631" citStr="McCallum, 2002" startWordPosition="856" endWordPosition="857">he cases when a speaker abandons an utterance or a constituent and restarts all over again, e.g., HexI like this idea. We used a CRF model to combine lexical features, shallow syntactic features, and prosodic features for joint detection of edit words and IP words. A CRF defines a global log-linear distribution of the state (or label) sequenceEconditioned on an observation sequence, in our case including the word sequence Wand the featuresF, and optimized globally over the entire sequence considering the context event information for making decisions at each point. We used the Mallet package (McCallum, 2002) to implement the CRF model. We used a first-order model that includes only two sequential events in the feature set. The CRF model is trained to maximize the conditional log-likelihood of a given training set P(E W, F). During testing, the most likely sequence E is found using the Viterbi algorithm. To avoid over-fitting, a zero-mean Gaussian prior (McCallum and Li, 2003) was applied to the parameters, where the variance of the prior was optimized on the development test set. Each word is associated with a class label, representing whether it is an edit word or not. We included IP in the targ</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Schuler</author>
<author>S AbdelRahman</author>
<author>T Miller</author>
<author>L Schwartz</author>
</authors>
<title>Broad-coverage incremental parsing using human-like memory constraints.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="1507" citStr="Schuler et al., 2010" startWordPosition="205" endWordPosition="208">neous speech. They consist of spoken words and phrases that represent self-correction, hesitation, and floor-grabbing behaviors, but do not add semantic information; removing them yields the intended, fluent utterance. The presence of disfluencies in conversational speech data can cause problems for both downstream processing (parsing and other natural language processing tasks) and human readability of speech transcripts. There has been much research effort on automatic disfluency detection in recent years (Shriberg and Stolcke, 1997; Snover et al., 2004; Liu et al., 2006; Lin and Lee, 2009; Schuler et al., 2010; Georgila et al., 2010; Zwarts and Johnson, 2011), particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) program, which focused on the automatic transcription of sizable amounts of speech data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN). However, the EARS MDE effort was focused on English only, and there hasn’t been much research on the effectiveness of similar automatic disfluency detection approaches for multi</context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2010</marker>
<rawString>W. Schuler, S. AbdelRahman, T. Miller, and L. Schwartz. 2010. Broad-coverage incremental parsing using human-like memory constraints. Computational Linguistics, 36(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>A prosody-only decision-tree model for disfluency detection.</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<pages>2383--2386</pages>
<contexts>
<context position="1427" citStr="Shriberg and Stolcke, 1997" startWordPosition="189" endWordPosition="192">these two languages. 1 Introduction Speech disfluencies are common phenomena in spontaneous speech. They consist of spoken words and phrases that represent self-correction, hesitation, and floor-grabbing behaviors, but do not add semantic information; removing them yields the intended, fluent utterance. The presence of disfluencies in conversational speech data can cause problems for both downstream processing (parsing and other natural language processing tasks) and human readability of speech transcripts. There has been much research effort on automatic disfluency detection in recent years (Shriberg and Stolcke, 1997; Snover et al., 2004; Liu et al., 2006; Lin and Lee, 2009; Schuler et al., 2010; Georgila et al., 2010; Zwarts and Johnson, 2011), particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) program, which focused on the automatic transcription of sizable amounts of speech data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN). However, the EARS MDE effort was focused on English only, and there hasn’t been much research on </context>
<context position="7131" citStr="Shriberg and Stolcke, 1997" startWordPosition="1116" endWordPosition="1119">MM) based part-of-speech (POS) taggers for English conversational speech and Mandarin broadcast conversation data. After employing the co-training approach described in (Wang et al., 2007), we achieved 94% POS tagging accuracy for both data sets. The features for CRF modeling include: n-grams from words and automatically generated POS tags, speaker turns, whether there is a repeated word sequence ending at a word boundary, whether a word is a fragment, whether there is a predefined filler phrase after the word boundary, and the prosody model posterior probabilities from a decision tree model (Shriberg and Stolcke, 1997) and discretized by cumulative binning (Liu et al., 2006). The prosodic features were computed for each interword boundary from words and phonetic alignments of the manual transcriptions. We extracted the same set of prosodic features for English and Mandarin data, based on duration, fundamental frequency (f0), energy, and pause information, and nonprosodic information such as speaker gender and speaker change, for training and applying the decision-tree-based prosody model (Liu et al., 2006). We implemented a rule-based system for filler word detection. We defined a list of possible Chinese a</context>
</contexts>
<marker>Shriberg, Stolcke, 1997</marker>
<rawString>E. Shriberg and A. Stolcke. 1997. A prosody-only decision-tree model for disfluency detection. In Proceedings of Eurospeech, pages 2383–2386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>A lexicallydriven algorithm for disfluency detection.</title>
<date>2004</date>
<editor>In Susan Dumais, Daniel Marcu, and Salim Roukos, editors, Proc. HLT-NAACL, Boston, May.</editor>
<publisher>Association for Computational Linguistics.</publisher>
<contexts>
<context position="1448" citStr="Snover et al., 2004" startWordPosition="193" endWordPosition="196">duction Speech disfluencies are common phenomena in spontaneous speech. They consist of spoken words and phrases that represent self-correction, hesitation, and floor-grabbing behaviors, but do not add semantic information; removing them yields the intended, fluent utterance. The presence of disfluencies in conversational speech data can cause problems for both downstream processing (parsing and other natural language processing tasks) and human readability of speech transcripts. There has been much research effort on automatic disfluency detection in recent years (Shriberg and Stolcke, 1997; Snover et al., 2004; Liu et al., 2006; Lin and Lee, 2009; Schuler et al., 2010; Georgila et al., 2010; Zwarts and Johnson, 2011), particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) program, which focused on the automatic transcription of sizable amounts of speech data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN). However, the EARS MDE effort was focused on English only, and there hasn’t been much research on the effectiveness of </context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2004</marker>
<rawString>M. Snover, B. Dorr, and R. Schwartz. 2004. A lexicallydriven algorithm for disfluency detection. In Susan Dumais, Daniel Marcu, and Salim Roukos, editors, Proc. HLT-NAACL, Boston, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andreas Stolcke</author>
<author>Barry Chen</author>
<author>Horacio Franco</author>
</authors>
<title>Venkata Ramana Rao Gadde, Martin Graciarena, Mei-Yuh Hwang, Katrin Kirchhoff,</title>
<date>2006</date>
<journal>IEEE Trans. Audio, Speech, and Lang. Process.,</journal>
<booktitle>Special Issue on Progress in Rich Transcription.</booktitle>
<volume>14</volume>
<issue>5</issue>
<location>Arindam Mandal, Nelson Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal S¨onmez, Anand Venkataraman, Dimitra Vergyri, Wen</location>
<contexts>
<context position="9343" citStr="Stolcke et al., 2006" startWordPosition="1457" endWordPosition="1460">lternative approach explores different knowledge sources. In particular, the AMs explore different front-end features compared to the lexical and prosodic features explored in those disfluency detection systems presented in Section 2. Details of the front-end features are illustrated below. We evaluated this approach on both English and Mandarin conversational speech. For detecting FPs in English conversational speech, we used a modified and simplified form of the recognition system developed for the 2004 NIST Rich Transcription Conversational Telephone Speech (CTS) evaluations, described in (Stolcke et al., 2006). The first pass of the recognizer uses a within-word MFCC+MLP model (i.e, trained on Mel-frequency cepstral coefficient (MFCC) features augmented with Multi-Layer Perceptron (MLP) based phoneposterior features), while the second pass uses a cross-word model trained on Perceptual Linear Prediction (PLP) features adapted (by speaker) to the output of the first pass. For purposes of FP detection, the recognition is constrained to a word lattice formed by the manually transcribed non-FP reference words, with optional FP words inserted between any two words and at the beginning and end of each utt</context>
</contexts>
<marker>Stolcke, Chen, Franco, 2006</marker>
<rawString>Andreas Stolcke, Barry Chen, Horacio Franco, Venkata Ramana Rao Gadde, Martin Graciarena, Mei-Yuh Hwang, Katrin Kirchhoff, Arindam Mandal, Nelson Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal S¨onmez, Anand Venkataraman, Dimitra Vergyri, Wen Wang, Jing Zheng, and Qifeng Zhu. 2006. Recent innovations in speech-to-text transcription at SRI-ICSIUW. IEEE Trans. Audio, Speech, and Lang. Process., 14(5):1729–1744, September. Special Issue on Progress in Rich Transcription.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>Z Huang</author>
<author>M P Harper</author>
</authors>
<title>Semisupervised learning for part-of-speech tagging of Mandarin transcribed speech.</title>
<date>2007</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>137--140</pages>
<contexts>
<context position="6692" citStr="Wang et al., 2007" startWordPosition="1042" endWordPosition="1045">optimized on the development test set. Each word is associated with a class label, representing whether it is an edit word or not. We included IP in the target classes and used five states, as outside edit (O), begin edit with an IP (B-E+IP), begin edit (B-E), inside edit with an IP (I-E+IP), and inside edit (IE) (Liu et al., 2006). State transitions are also the same as in (Liu et al., 2006). We built a Hidden Markov Model (HMM) based part-of-speech (POS) taggers for English conversational speech and Mandarin broadcast conversation data. After employing the co-training approach described in (Wang et al., 2007), we achieved 94% POS tagging accuracy for both data sets. The features for CRF modeling include: n-grams from words and automatically generated POS tags, speaker turns, whether there is a repeated word sequence ending at a word boundary, whether a word is a fragment, whether there is a predefined filler phrase after the word boundary, and the prosody model posterior probabilities from a decision tree model (Shriberg and Stolcke, 1997) and discretized by cumulative binning (Liu et al., 2006). The prosodic features were computed for each interword boundary from words and phonetic alignments of </context>
</contexts>
<marker>Wang, Huang, Harper, 2007</marker>
<rawString>W. Wang, Z. Huang, and M. P. Harper. 2007. Semisupervised learning for part-of-speech tagging of Mandarin transcribed speech. In Proceedings of ICASSP, pages 137–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>D Jurafsky</author>
</authors>
<title>A preliminary study of mandarin filled pause.</title>
<date>2005</date>
<booktitle>In Proceedings of DISS,</booktitle>
<pages>179--182</pages>
<contexts>
<context position="3748" citStr="Zhao and Jurafsky, 2005" startWordPosition="538" endWordPosition="542">g terms (ET). FPs are words used by the speakers as floor holders to maintain control of a conversation. They can also indicate hesitations of the speaker. In this work, English FPs 703 Proceedings of NAACL-HLT 2013, pages 703–708, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics comprise uh and um, based on English CTS corpora. For Mandarin, Zhao and Jurafsky found that Mandarin speakers intensively used both demonstratives zhege (literally ‘this’) and nage (literally ‘that’) and uh/mm as FPs based on a large speech corpus of Mandarin telephone conversation (Zhao and Jurafsky, 2005). We study the same set of Chinese FPs in this study. DMs are words or phrases related to the structure of the discourse and help taking or keeping a turn, or serving as acknowledgment, for example, I mean, you know. An explicit ET is an editing term in an edit disfluency that is not an FP or a DM. For example, we have two action items x sorry three action items from the meeting, where sorry is an explicit ET. Edit disfluencies involve syntactically relevant content that is either repeated, revised, or abandoned. The basic pattern for edit disfluencies has the form (reparandum) x &lt;editing term</context>
</contexts>
<marker>Zhao, Jurafsky, 2005</marker>
<rawString>Y. Zhao and D. Jurafsky. 2005. A preliminary study of mandarin filled pause. In Proceedings of DISS, pages 179–182, Aix-en-Provence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zwarts</author>
<author>M Johnson</author>
</authors>
<title>The impact of language models and loss functions on repair disfluency detection.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL/HLT,</booktitle>
<pages>703--711</pages>
<location>Portland.</location>
<contexts>
<context position="1557" citStr="Zwarts and Johnson, 2011" startWordPosition="213" endWordPosition="216">d phrases that represent self-correction, hesitation, and floor-grabbing behaviors, but do not add semantic information; removing them yields the intended, fluent utterance. The presence of disfluencies in conversational speech data can cause problems for both downstream processing (parsing and other natural language processing tasks) and human readability of speech transcripts. There has been much research effort on automatic disfluency detection in recent years (Shriberg and Stolcke, 1997; Snover et al., 2004; Liu et al., 2006; Lin and Lee, 2009; Schuler et al., 2010; Georgila et al., 2010; Zwarts and Johnson, 2011), particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) program, which focused on the automatic transcription of sizable amounts of speech data and rendering such transcripts in readable form, for both conversational telephone speech (CTS) and broadcast news (BN). However, the EARS MDE effort was focused on English only, and there hasn’t been much research on the effectiveness of similar automatic disfluency detection approaches for multiple languages. This paper presents three main inno</context>
</contexts>
<marker>Zwarts, Johnson, 2011</marker>
<rawString>S. Zwarts and M. Johnson. 2011. The impact of language models and loss functions on repair disfluency detection. In Proceedings of ACL/HLT, pages 703– 711, Portland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>