<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.157407">
<title confidence="0.983921">
PLIS: a Probabilistic Lexical Inference System
</title>
<author confidence="0.996317">
Eyal Shnarch&apos;, Erel Segal-haLevi&apos;, Jacob Goldberger&apos;, Ido Dagan&apos;
</author>
<affiliation confidence="0.9971345">
&apos;Computer Science Department, Bar-Ilan University, Israel
&apos;Faculty of Engineering, Bar-Ilan University, Israel
</affiliation>
<email confidence="0.979721">
{shey,erelsgl,dagan}@cs.biu.ac.il
goldbej@eng.biu.ac.il
</email>
<sectionHeader confidence="0.993718" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9989336">
This paper presents PLIS, an open source
Probabilistic Lexical Inference System
which combines two functionalities: (i)
a tool for integrating lexical inference
knowledge from diverse resources, and (ii)
a framework for scoring textual inferences
based on the integrated knowledge. We
provide PLIS with two probabilistic im-
plementation of this framework. PLIS is
available for download and developers of
text processing applications can use it as
an off-the-shelf component for injecting
lexical knowledge into their applications.
PLIS is easily configurable, components
can be extended or replaced with user gen-
erated ones to enable system customiza-
tion and further research. PLIS includes
an online interactive viewer, which is a
powerful tool for investigating lexical in-
ference processes.
</bodyText>
<sectionHeader confidence="0.97295" genericHeader="categories and subject descriptors">
1 Introduction and background
</sectionHeader>
<bodyText confidence="0.998464327586207">
Semantic Inference is the process by which ma-
chines perform reasoning over natural language
texts. A semantic inference system is expected to
be able to infer the meaning of one text from the
meaning of another, identify parts of texts which
convey a target meaning, and manipulate text units
in order to deduce new meanings.
Semantic inference is needed for many Natural
Language Processing (NLP) applications. For in-
stance, a Question Answering (QA) system may
encounter the following question and candidate
answer (Example 1):
Q: which explorer discovered the New World?
A: Christopher Columbus revealed America.
As there are no overlapping words between the
two sentences, to identify that A holds an answer
for Q, background world knowledge is needed
to link Christopher Columbus with explorer and
America with New World. Linguistic knowledge
is also needed to identify that reveal and discover
refer to the same concept.
Knowledge is needed in order to bridge the gap
between text fragments, which may be dissimilar
on their surface form but share a common mean-
ing. For the purpose of semantic inference, such
knowledge can be derived from various resources
(e.g. WordNet (Fellbaum, 1998) and others, de-
tailed in Section 2.1) in a form which we denote as
inference links (often called inference/entailment
rules), each is an ordered pair of elements in which
the first implies the meaning of the second. For in-
stance, the link ship‚Üívessel can be derived from
the hypernym relation of WordNet.
Other applications can benefit from utilizing in-
ference links to identify similarity between lan-
guage expressions. In Information Retrieval, the
user‚Äôs information need may be expressed in rele-
vant documents differently than it is expressed in
the query. Summarization systems should identify
text snippets which convey the same meaning.
Our work addresses a generic, application in-
dependent, setting of lexical inference. We there-
fore adopt the terminology of Textual Entailment
(Dagan et al., 2006), a generic paradigm for ap-
plied semantic inference which captures inference
needs of many NLP applications in a common un-
derlying task: given two textual fragments, termed
hypothesis (H) and text (T), the task is to recog-
nize whether T implies the meaning of H, denoted
T‚ÜíH. For instance, in a QA application, H rep-
resents the question, and T a candidate answer. In
this setting, T is likely to hold an answer for the
question if it entails the question.
It is challenging to properly extract the needed
inference knowledge from available resources,
and to effectively utilize it within the inference
process. The integration of resources, each has its
own format, is technically complex and the quality
</bodyText>
<page confidence="0.996411">
97
</page>
<note confidence="0.626941">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 97‚Äì102,
Sofia, Bulgaria, August 4-9 2013. cÔøΩ2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9863532">
Figure 1: PLIS schema - a text-hypothesis pair is processed
by the Lexical Integrator which uses a set of lexical resources
to extract inference chains which connect the two. The Lexi-
cal Inference component provides probability estimations for
the validity of each level of the process.
</figureCaption>
<bodyText confidence="0.999950518518518">
of the resulting inference links is often unknown in
advance and varies considerably. For coping with
this challenge we developed PLIS, a Probabilis-
tic Lexical Inference System1. PLIS, illustrated in
Fig 1, has two main modules: the Lexical Integra-
tor (Section 2) accepts a set of lexical resources
and a text-hypothesis pair, and finds all the lex-
ical inference relations between any pair of text
term ti and hypothesis term hj, based on the avail-
able lexical relations found in the resources (and
their combination). The Lexical Inference module
(Section 3) provides validity scores for these rela-
tions. These term-level scores are used to estimate
the sentence-level likelihood that the meaning of
the hypothesis can be inferred from the text, thus
making PLIS a complete lexical inference system.
Lexical inference systems do not look into the
structure of texts but rather consider them as bag
of terms (words or multi-word expressions). These
systems are easy to implement, fast to run, practi-
cal across different genres and languages, while
maintaining a competitive level of performance.
PLIS can be used as a stand-alone efficient in-
ference system or as the lexical component of any
NLP application. PLIS is a flexible system, al-
lowing users to choose the set of knowledge re-
sources as well as the model by which inference
</bodyText>
<footnote confidence="0.9862485">
1The complete software package is available at http://
www.cs.biu.ac.il/nlp/downloads/PLIS.html and an online in-
teractive viewer is available for examination at http://irsrv2.
cs.biu.ac.il/nlp-net/PLIS.html.
</footnote>
<bodyText confidence="0.9993265">
is done. PLIS can be easily extended with new
knowledge resources and new inference models. It
comes with a set of ready-to-use plug-ins for many
common lexical resources (Section 2.1) as well
as two implementation of the scoring framework.
These implementations, described in (Shnarch et
al., 2011; Shnarch et al., 2012), provide probabil-
ity estimations for inference. PLIS has an inter-
active online viewer (Section 4) which provides a
visualization of the entire inference process, and is
very helpful for analysing lexical inference mod-
els and lexical resources usability.
</bodyText>
<sectionHeader confidence="0.969642" genericHeader="method">
2 Lexical integrator
</sectionHeader>
<bodyText confidence="0.999907514285714">
The input for the lexical integrator is a set of lex-
ical resources and a pair of text T and hypothe-
sis H. The lexical integrator extracts lexical in-
ference links from the various lexical resources to
connect each text term ti ‚ààT with each hypothesis
term hj ‚àà H2. A lexical inference link indicates a
semantic relation between two terms. It could be
a directional relation (Columbus‚Üínavigator) or a
bidirectional one (car ‚Üê‚Üí automobile).
Since knowledge resources vary in their rep-
resentation methods, the lexical integrator wraps
each lexical resource in a common plug-in inter-
face which encapsulates resource‚Äôs inner repre-
sentation method and exposes its knowledge as a
list of inference links. The implemented plug-ins
that come with PLIS are described in Section 2.1.
Adding a new lexical resource and integrating it
with the others only demands the implementation
of the plug-in interface.
As the knowledge needed to connect a pair of
terms, ti and hj, may be scattered across few re-
sources, the lexical integrator combines inference
links into lexical inference chains to deduce new
pieces of knowledge, such as Columbus resource1
‚àí‚àí‚àí‚àí‚àí‚àí‚Üí
navigator resource2 ‚àí‚àí‚àí‚àí‚àí‚àí‚Üíexplorer. Therefore, the only
assumption the lexical integrator makes, regarding
its input lexical resources, is that the inferential
lexical relations they provide are transitive.
The lexical integrator generates lexical infer-
ence chains by expanding the text and hypothesis
terms with inference links. These links lead to new
terms (e.g. navigator in the above chain example
and t&apos; in Fig 1) which can be further expanded,
as all inference links are transitive. A transitivity
</bodyText>
<footnote confidence="0.9146675">
2Where i and j run from 1 to the length of the text and
hypothesis respectively.
</footnote>
<figure confidence="0.9971520625">
Text
Hypothesis
Lexical Integrator
t1 t2 t3 t4
WordNet
Wikipedia
...
h1 h2 h3
P(T ‚Üí h3)
t‚Ä≤
P(t3 ‚Üí h2)
VerbOcean
PT‚Üí ùêª
Lexical
Resources
Lexical Inference
</figure>
<page confidence="0.998603">
98
</page>
<bodyText confidence="0.999921">
limit is set by the user to determine the maximal
length for inference chains.
The lexical integrator uses a graph-based rep-
resentation for the inference chains, as illustrates
in Fig 1. A node holds the lemma, part-of-speech
and sense of a single term. The sense is the ordi-
nal number of WordNet sense. Whenever we do
not know the sense of a term we implement the
most frequent sense heuristic.3 An edge represents
an inference link and is labeled with the semantic
relation of this link (e.g. cytokine‚Üíprotein is la-
beled with the WordNet relation hypernym).
</bodyText>
<subsectionHeader confidence="0.99448">
2.1 Available plug-ins for lexical resources
</subsectionHeader>
<bodyText confidence="0.999985964285714">
We have implemented plug-ins for the follow-
ing resources: the English lexicon WordNet
(Fellbaum, 1998)(based on either JWI, JWNL
or extJWNL java APIs4), CatVar (Habash and
Dorr, 2003), a categorial variations database,
Wikipedia-based resource (Shnarch et al., 2009),
which applies several extraction methods to de-
rive inference links from the text and structure
of Wikipedia, VerbOcean (Chklovski and Pantel,
2004), a knowledge base of fine-grained semantic
relations between verbs, Lin‚Äôs distributional simi-
larity thesaurus (Lin, 1998), and DIRECT (Kotler-
man et al., 2010), a directional distributional simi-
larity thesaurus geared for lexical inference.
To summarize, the lexical integrator finds all
possible inference chains (of a predefined length),
resulting from any combination of inference links
extracted from lexical resources, which link any
t, h pair of a given text-hypothesis. Developers
can use this tool to save the hassle of interfac-
ing with the different lexical knowledge resources,
and spare the labor of combining their knowledge
via inference chains.
The lexical inference model, described next,
provides a mean to decide whether a given hypoth-
esis is inferred from a given text, based on weigh-
ing the lexical inference chains extracted by the
lexical integrator.
</bodyText>
<sectionHeader confidence="0.99638" genericHeader="method">
3 Lexical inference
</sectionHeader>
<bodyText confidence="0.996583">
There are many ways to implement an infer-
ence model which identifies inference relations
between texts. A simple model may consider the
</bodyText>
<footnote confidence="0.9255912">
3This disambiguation policy was better than considering
all senses of an ambiguous term in preliminary experiments.
However, it is a matter of changing a variable in the configu-
ration of PLIS to switch between these two policies.
4http://wordnet.princeton.edu/wordnet/related-projects/
</footnote>
<bodyText confidence="0.999850411764706">
number of hypothesis terms for which inference
chains, originated from text terms, were found. In
PLIS, the inference model is a plug-in, similar to
the lexical knowledge resources, and can be easily
replaced to change the inference logic.
We provide PLIS with two implemented base-
line lexical inference models which are mathemat-
ically based. These are two Probabilistic Lexical
Models (PLMs), HN-PLM and M-PLM which are
described in (Shnarch et al., 2011; Shnarch et al.,
2012) respectively.
A PLM provides probability estimations for the
three parts of the inference process (as shown in
Fig 1): the validity probability of each inference
chain (i.e. the probability for a valid inference re-
lation between its endpoint terms) P(ti ‚Üí hj), the
probability of each hypothesis term to be inferred
by the entire text P(T ‚Üí hj) (term-level proba-
bility), and the probability of the entire hypothesis
to be inferred by the text P(T ‚Üí H) (sentence-
level probability).
HN-PLM describes a generative process by
which the hypothesis is generated from the text.
Its parameters are the reliability level of each of
the resources it utilizes (that is, the prior proba-
bility that applying an arbitrary inference link de-
rived from each resource corresponds to a valid in-
ference). For learning these parameters HN-PLM
applies a schema of the EM algorithm (Demp-
ster et al., 1977). Its performance on the recog-
nizing textual entailment task, RTE (Bentivogli et
al., 2009; Bentivogli et al., 2010), are in line with
the state of the art inference systems, including
complex systems which perform syntactic analy-
sis. This model is improved by M-PLM, which de-
duces sentence-level probability from term-level
probabilities by a Markovian process. PLIS with
this model was used for a passage retrieval for a
question answering task (Wang et al., 2007), and
outperformed state of the art inference systems.
Both PLMs model the following prominent as-
pects of the lexical inference phenomenon: (i)
considering the different reliability levels of the
input knowledge resources, (ii) reducing inference
chain probability as its length increases, and (iii)
increasing term-level probability as we have more
inference chains which suggest that the hypothesis
term is inferred by the text. Both PLMs only need
sentence-level annotations from which they derive
term-level inference probabilities.
To summarize, the lexical inference module
</bodyText>
<page confidence="0.984567">
99
</page>
<figure confidence="0.92488675">
P(T ‚Äì3, hj)
4
3
2
1
P(ti ‚Äì3, hj)
P(T ‚Äì3, H)
configuration
</figure>
<figureCaption confidence="0.9930335">
Figure 2: PLIS interactive viewer with Example 1 demonstrates knowledge integration of multiple inference chains and
resource combination (additional explanations, which are not part of the demo, are provided in orange).
</figureCaption>
<bodyText confidence="0.9996181">
provides the setting for interfacing with the lexi-
cal integrator. Additionally, the module provides
the framework for probabilistic inference models
which estimate term-level probabilities and inte-
grate them into a sentence-level inference deci-
sion, while implementing prominent aspects of
lexical inference. The user can choose to apply
another inference logic, not necessarily probabilis-
tic, by plugging a different lexical inference model
into the provided inference infrastructure.
</bodyText>
<sectionHeader confidence="0.990388" genericHeader="method">
4 The PLIS interactive system
</sectionHeader>
<bodyText confidence="0.999928222222222">
PLIS comes with an online interactive viewer5 in
which the user sets the parameters of PLIS, inserts
a text-hypothesis pair and gets a visualization of
the entire inference process. This is a powerful
tool for investigating knowledge integration and
lexical inference models.
Fig 2 presents a screenshot of the processing of
Example 1. On the right side, the user configures
the system by selecting knowledge resources, ad-
justing their configuration, setting the transitivity
limit, and choosing the lexical inference model to
be applied by PLIS.
After inserting a text and a hypothesis to the
appropriate text boxes, the user clicks on the in-
fer button and PLIS generates all lexical inference
chains, of length up to the transitivity limit, that
connect text terms with hypothesis terms, as avail-
able from the combination of the selected input re-
</bodyText>
<footnote confidence="0.709374">
5http://irsrv2.cs.biu.ac.il/nlp-net/PLIS.html
</footnote>
<bodyText confidence="0.996279580645161">
sources. Each inference chain is presented in a line
between the text and hypothesis.
PLIS also displays the probability estimations
for all inference levels; the probability of each
chain is presented at the end of its line. For each
hypothesis term, term-level probability, which
weighs all inference chains found for it, is given
below the dashed line. The overall sentence-level
probability integrates the probabilities of all hy-
pothesis terms and is displayed in the box at the
bottom right corner.
Next, we detail the inference process of Exam-
ple 1, as presented in Fig 2. In this QA example,
the probability of the candidate answer (set as the
text) to be relevant for the given question (the hy-
pothesis) is estimated. When utilizing only two
knowledge resources (WordNet and Wikipedia),
PLIS is able to recognize that explorer is inferred
by Christopher Columbus and that New World is
inferred by America. Each one of these pairs has
two independent inference chains, numbered 1‚Äì4,
as evidence for its inference relation.
Both inference chains 1 and 3 include a single
inference link, each derived from a different rela-
tion of the Wikipedia-based resource. The infer-
ence model assigns a higher probability for chain
1 since the BeComp relation is much more reliable
than the Link relation. This comparison illustrates
the ability of the inference model to learn how to
differ knowledge resources by their reliability.
Comparing the probability assigned by the in-
</bodyText>
<page confidence="0.982506">
100
</page>
<bodyText confidence="0.9996894375">
ference model for inference chain 2 with the prob-
abilities assigned for chains 1 and 3, reveals the
sophisticated way by which the inference model
integrates lexical knowledge. Inference chain 2
is longer than chain 1, therefore its probability is
lower. However, the inference model assigns chain
2 a higher probability than chain 3, even though
the latter is shorter, since the model is sensitive
enough to consider the difference in reliability lev-
els between the two highly reliable hypernym re-
lations (from WordNet) of chain 2 and the less re-
liable Link relation (from Wikipedia) of chain 3.
Another aspect of knowledge integration is ex-
emplified in Fig 2 by the three circled probabili-
ties. The inference model takes into consideration
the multiple pieces of evidence for the inference
of New World (inference chains 3 and 4, whose
probabilities are circled). This results in a term-
level probability estimation for New World (the
third circled probability) which is higher than the
probabilities of each chain separately.
The third term of the hypothesis, discover, re-
mains uncovered by the text as no inference chain
was found for it. Therefore, the sentence-level
inference probability is very low, 37%. In order
to identify that the hypothesis is indeed inferred
from the text, the inference model should be pro-
vided with indications for the inference of dis-
cover. To that end, the user may increase the tran-
sitivity limit in hope that longer inference chains
provide the needed information. In addition, the
user can examine other knowledge resources in
search for the missing inference link. In this ex-
ample, it is enough to add VerbOcean to the in-
put of PLIS to expose two inference chains which
connect reveal with discover by combining an in-
ference link from WordNet and another one from
VerbOcean. With this additional information, the
sentence-level probability increases to 76%. This
is a typical scenario of utilizing PLIS, either via
the interactive system or via the software, for ana-
lyzing the usability of the different knowledge re-
sources and their combination.
A feature of the interactive system, which is
useful for lexical resources analysis, is that each
term in a chain is clickable and links to another
screen which presents all the terms that are in-
ferred from it and those from which it is inferred.
Additionally, the interactive system communi-
cates with a server which runs PLIS, in a full-
duplex WebSocket connection6. This mode of op-
eration is publicly available and provides a method
for utilizing PLIS, without having to install it or
the lexical resources it uses.
Finally, since PLIS is a lexical system it can
easily be adjusted to other languages. One only
needs to replace the basic lexical text processing
tools and plug in knowledge resources in the tar-
get language. If PLIS is provided with bilingual
resources,7 it can operate also as a cross-lingual
inference system (Negri et al., 2012). For instance,
the text in Fig 3 is given in English, while the hy-
pothesis is written in Spanish (given as a list of
lemma:part-of-speech). The left side of the figure
depicts a cross-lingual inference process in which
the only lexical knowledge resource used is a man-
ually built English-Spanish dictionary. As can be
seen, two Spanish terms, jugador and casa remain
uncovered since the dictionary alone cannot con-
nect them to any of the English terms in the text.
As illustrated in the right side of Fig 3,
PLIS enables the combination of the bilingual
dictionary with monolingual resources to pro-
duce cross-lingual inference chains, such as foot-
baller ‚àí‚àí‚àí‚àí‚àí‚àí‚Üíplayer manual
hypernym ‚àí‚àí‚àí‚àí‚àí‚Üíjugador. Such in-
ference chains have the capability to overcome
monolingual language variability (the first link
in this chain) as well as to provide cross-lingual
translation (the second link).
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999383125">
To utilize PLIS one should gather lexical re-
sources, obtain sentence-level annotations and
train the inference model. Annotations are avail-
able in common data sets for task such as QA,
Information Retrieval (queries are hypotheses and
snippets are texts) and Student Response Analysis
(reference answers are the hypotheses that should
be inferred by the student answers).
For developers of NLP applications, PLIS of-
fers a ready-to-use lexical knowledge integrator
which can interface with many common lexical
knowledge resources and constructs lexical in-
ference chains which combine the knowledge in
them. A developer who wants to overcome lex-
ical language variability, or to incorporate back-
ground knowledge, can utilize PLIS to inject lex-
</bodyText>
<footnote confidence="0.99889325">
6We used the socket.io implementation.
7A bilingual resource holds inference links which connect
terms in different languages (e.g. an English-Spanish dictio-
nary can provide the inference link explorer‚Üíexplorador).
</footnote>
<page confidence="0.995382">
101
</page>
<figureCaption confidence="0.9980075">
Figure 3: PLIS as a cross-lingual inference system. Left: the process with a single manual bilingual resource. Right: PLIS
composes cross-lingual inference chains to increase hypothesis coverage and increase sentence-level inference probability.
</figureCaption>
<bodyText confidence="0.9990224375">
ical knowledge into any text understanding appli-
cation. PLIS can be used as a lightweight infer-
ence system or as the lexical component of larger,
more complex inference systems.
Additionally, PLIS provides scores for infer-
ence chains and determines the way to combine
them in order to recognize sentence-level infer-
ence. PLIS comes with two probabilistic lexical
inference models which achieved competitive per-
formance levels in the tasks of recognizing textual
entailment and passage retrieval for QA.
All aspects of PLIS are configurable. The user
can easily switch between the built-in lexical re-
sources, inference models and even languages, or
extend the system with additional lexical resources
and new inference models.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999918714285714">
The authors thank Eden Erez for his help with
the interactive viewer and Miquel Espl`a Gomis
for the bilingual dictionaries. This work was par-
tially supported by the European Community‚Äôs
7th Framework Programme (FP7/2007-2013) un-
der grant agreement no. 287923 (EXCITEMENT)
and the Israel Science Foundation grant 880/12.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863234042553">
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
fifth PASCAL recognizing textual entailment chal-
lenge. In Proc. of TAC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge.
In Proc. of TAC.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. of EMNLP.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177‚Äì190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the royal statistical soci-
ety, series [B], 39(1):1‚Äì38.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proc. of NAACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359‚Äì389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of COLOING-ACL.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: Cross-lingual textual entail-
ment for content synchronization. In Proc. of Se-
mEval.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proc. of ACL.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
Towards a probabilistic model for lexical entailment.
In Proc. of the TextInfer Workshop.
Eyal Shnarch, Ido Dagan, and Jacob Goldberger. 2012.
A probabilistic lexical model for ranking textual in-
ferences. In Proc. of *SEM.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In Proc. of EMNLP.
</reference>
<page confidence="0.99862">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935961">
<title confidence="0.999884">PLIS: a Probabilistic Lexical Inference System</title>
<author confidence="0.999886">Erel Jacob Ido</author>
<affiliation confidence="0.9870525">Science Department, Bar-Ilan University, of Engineering, Bar-Ilan University,</affiliation>
<email confidence="0.997564">goldbej@eng.biu.ac.il</email>
<abstract confidence="0.998231095238095">paper presents an open source Probabilistic Lexical Inference System which combines two functionalities: (i) a tool for integrating lexical inference knowledge from diverse resources, and (ii) a framework for scoring textual inferences based on the integrated knowledge. We two probabilistic imof this framework. available for download and developers of text processing applications can use it as an off-the-shelf component for injecting lexical knowledge into their applications. easily configurable, components can be extended or replaced with user generated ones to enable system customizaand further research. an online interactive viewer, which is a powerful tool for investigating lexical inference processes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
</authors>
<title>The fifth PASCAL recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In Proc. of TAC.</booktitle>
<contexts>
<context position="12102" citStr="Bentivogli et al., 2009" startWordPosition="1882" endWordPosition="1885">m-level probability), and the probability of the entire hypothesis to be inferred by the text P(T ‚Üí H) (sentencelevel probability). HN-PLM describes a generative process by which the hypothesis is generated from the text. Its parameters are the reliability level of each of the resources it utilizes (that is, the prior probability that applying an arbitrary inference link derived from each resource corresponds to a valid inference). For learning these parameters HN-PLM applies a schema of the EM algorithm (Dempster et al., 1977). Its performance on the recognizing textual entailment task, RTE (Bentivogli et al., 2009; Bentivogli et al., 2010), are in line with the state of the art inference systems, including complex systems which perform syntactic analysis. This model is improved by M-PLM, which deduces sentence-level probability from term-level probabilities by a Markovian process. PLIS with this model was used for a passage retrieval for a question answering task (Wang et al., 2007), and outperformed state of the art inference systems. Both PLMs model the following prominent aspects of the lexical inference phenomenon: (i) considering the different reliability levels of the input knowledge resources, (</context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth PASCAL recognizing textual entailment challenge. In Proc. of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>The sixth PASCAL recognizing textual entailment challenge.</title>
<date>2010</date>
<booktitle>In Proc. of TAC.</booktitle>
<contexts>
<context position="12128" citStr="Bentivogli et al., 2010" startWordPosition="1886" endWordPosition="1889"> the probability of the entire hypothesis to be inferred by the text P(T ‚Üí H) (sentencelevel probability). HN-PLM describes a generative process by which the hypothesis is generated from the text. Its parameters are the reliability level of each of the resources it utilizes (that is, the prior probability that applying an arbitrary inference link derived from each resource corresponds to a valid inference). For learning these parameters HN-PLM applies a schema of the EM algorithm (Dempster et al., 1977). Its performance on the recognizing textual entailment task, RTE (Bentivogli et al., 2009; Bentivogli et al., 2010), are in line with the state of the art inference systems, including complex systems which perform syntactic analysis. This model is improved by M-PLM, which deduces sentence-level probability from term-level probabilities by a Markovian process. PLIS with this model was used for a passage retrieval for a question answering task (Wang et al., 2007), and outperformed state of the art inference systems. Both PLMs model the following prominent aspects of the lexical inference phenomenon: (i) considering the different reliability levels of the input knowledge resources, (ii) reducing inference cha</context>
</contexts>
<marker>Bentivogli, Clark, Dagan, Dang, Giampiccolo, 2010</marker>
<rawString>Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2010. The sixth PASCAL recognizing textual entailment challenge. In Proc. of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="9337" citStr="Chklovski and Pantel, 2004" startWordPosition="1450" endWordPosition="1453">c.3 An edge represents an inference link and is labeled with the semantic relation of this link (e.g. cytokine‚Üíprotein is labeled with the WordNet relation hypernym). 2.1 Available plug-ins for lexical resources We have implemented plug-ins for the following resources: the English lexicon WordNet (Fellbaum, 1998)(based on either JWI, JWNL or extJWNL java APIs4), CatVar (Habash and Dorr, 2003), a categorial variations database, Wikipedia-based resource (Shnarch et al., 2009), which applies several extraction methods to derive inference links from the text and structure of Wikipedia, VerbOcean (Chklovski and Pantel, 2004), a knowledge base of fine-grained semantic relations between verbs, Lin‚Äôs distributional similarity thesaurus (Lin, 1998), and DIRECT (Kotlerman et al., 2010), a directional distributional similarity thesaurus geared for lexical inference. To summarize, the lexical integrator finds all possible inference chains (of a predefined length), resulting from any combination of inference links extracted from lexical resources, which link any t, h pair of a given text-hypothesis. Developers can use this tool to save the hassle of interfacing with the different lexical knowledge resources, and spare th</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Lecture Notes in Computer Science,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<contexts>
<context position="3110" citStr="Dagan et al., 2006" startWordPosition="461" endWordPosition="464">es the meaning of the second. For instance, the link ship‚Üívessel can be derived from the hypernym relation of WordNet. Other applications can benefit from utilizing inference links to identify similarity between language expressions. In Information Retrieval, the user‚Äôs information need may be expressed in relevant documents differently than it is expressed in the query. Summarization systems should identify text snippets which convey the same meaning. Our work addresses a generic, application independent, setting of lexical inference. We therefore adopt the terminology of Textual Entailment (Dagan et al., 2006), a generic paradigm for applied semantic inference which captures inference needs of many NLP applications in a common underlying task: given two textual fragments, termed hypothesis (H) and text (T), the task is to recognize whether T implies the meaning of H, denoted T‚ÜíH. For instance, in a QA application, H represents the question, and T a candidate answer. In this setting, T is likely to hold an answer for the question if it entails the question. It is challenging to properly extract the needed inference knowledge from available resources, and to effectively utilize it within the inferenc</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Lecture Notes in Computer Science, volume 3944, pages 177‚Äì190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the royal statistical society, series [B],</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="12012" citStr="Dempster et al., 1977" startWordPosition="1867" endWordPosition="1871"> the probability of each hypothesis term to be inferred by the entire text P(T ‚Üí hj) (term-level probability), and the probability of the entire hypothesis to be inferred by the text P(T ‚Üí H) (sentencelevel probability). HN-PLM describes a generative process by which the hypothesis is generated from the text. Its parameters are the reliability level of each of the resources it utilizes (that is, the prior probability that applying an arbitrary inference link derived from each resource corresponds to a valid inference). For learning these parameters HN-PLM applies a schema of the EM algorithm (Dempster et al., 1977). Its performance on the recognizing textual entailment task, RTE (Bentivogli et al., 2009; Bentivogli et al., 2010), are in line with the state of the art inference systems, including complex systems which perform syntactic analysis. This model is improved by M-PLM, which deduces sentence-level probability from term-level probabilities by a Markovian process. PLIS with this model was used for a passage retrieval for a question answering task (Wang et al., 2007), and outperformed state of the art inference systems. Both PLMs model the following prominent aspects of the lexical inference phenom</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the royal statistical society, series [B], 39(1):1‚Äì38.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Bonnie Dorr</author>
</authors>
<title>A categorial variation database for English.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="9105" citStr="Habash and Dorr, 2003" startWordPosition="1418" endWordPosition="1421">llustrates in Fig 1. A node holds the lemma, part-of-speech and sense of a single term. The sense is the ordinal number of WordNet sense. Whenever we do not know the sense of a term we implement the most frequent sense heuristic.3 An edge represents an inference link and is labeled with the semantic relation of this link (e.g. cytokine‚Üíprotein is labeled with the WordNet relation hypernym). 2.1 Available plug-ins for lexical resources We have implemented plug-ins for the following resources: the English lexicon WordNet (Fellbaum, 1998)(based on either JWI, JWNL or extJWNL java APIs4), CatVar (Habash and Dorr, 2003), a categorial variations database, Wikipedia-based resource (Shnarch et al., 2009), which applies several extraction methods to derive inference links from the text and structure of Wikipedia, VerbOcean (Chklovski and Pantel, 2004), a knowledge base of fine-grained semantic relations between verbs, Lin‚Äôs distributional similarity thesaurus (Lin, 1998), and DIRECT (Kotlerman et al., 2010), a directional distributional similarity thesaurus geared for lexical inference. To summarize, the lexical integrator finds all possible inference chains (of a predefined length), resulting from any combinati</context>
</contexts>
<marker>Habash, Dorr, 2003</marker>
<rawString>Nizar Habash and Bonnie Dorr. 2003. A categorial variation database for English. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="9496" citStr="Kotlerman et al., 2010" startWordPosition="1472" endWordPosition="1476">m). 2.1 Available plug-ins for lexical resources We have implemented plug-ins for the following resources: the English lexicon WordNet (Fellbaum, 1998)(based on either JWI, JWNL or extJWNL java APIs4), CatVar (Habash and Dorr, 2003), a categorial variations database, Wikipedia-based resource (Shnarch et al., 2009), which applies several extraction methods to derive inference links from the text and structure of Wikipedia, VerbOcean (Chklovski and Pantel, 2004), a knowledge base of fine-grained semantic relations between verbs, Lin‚Äôs distributional similarity thesaurus (Lin, 1998), and DIRECT (Kotlerman et al., 2010), a directional distributional similarity thesaurus geared for lexical inference. To summarize, the lexical integrator finds all possible inference chains (of a predefined length), resulting from any combination of inference links extracted from lexical resources, which link any t, h pair of a given text-hypothesis. Developers can use this tool to save the hassle of interfacing with the different lexical knowledge resources, and spare the labor of combining their knowledge via inference chains. The lexical inference model, described next, provides a mean to decide whether a given hypothesis is</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359‚Äì389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of COLOING-ACL.</booktitle>
<contexts>
<context position="9459" citStr="Lin, 1998" startWordPosition="1468" endWordPosition="1469">WordNet relation hypernym). 2.1 Available plug-ins for lexical resources We have implemented plug-ins for the following resources: the English lexicon WordNet (Fellbaum, 1998)(based on either JWI, JWNL or extJWNL java APIs4), CatVar (Habash and Dorr, 2003), a categorial variations database, Wikipedia-based resource (Shnarch et al., 2009), which applies several extraction methods to derive inference links from the text and structure of Wikipedia, VerbOcean (Chklovski and Pantel, 2004), a knowledge base of fine-grained semantic relations between verbs, Lin‚Äôs distributional similarity thesaurus (Lin, 1998), and DIRECT (Kotlerman et al., 2010), a directional distributional similarity thesaurus geared for lexical inference. To summarize, the lexical integrator finds all possible inference chains (of a predefined length), resulting from any combination of inference links extracted from lexical resources, which link any t, h pair of a given text-hypothesis. Developers can use this tool to save the hassle of interfacing with the different lexical knowledge resources, and spare the labor of combining their knowledge via inference chains. The lexical inference model, described next, provides a mean to</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of COLOING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Alessandro Marchetti</author>
<author>Yashar Mehdad</author>
<author>Luisa Bentivogli</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>Semeval-2012 task 8: Cross-lingual textual entailment for content synchronization.</title>
<date>2012</date>
<booktitle>In Proc. of SemEval.</booktitle>
<contexts>
<context position="19157" citStr="Negri et al., 2012" startWordPosition="3006" endWordPosition="3009">h it is inferred. Additionally, the interactive system communicates with a server which runs PLIS, in a fullduplex WebSocket connection6. This mode of operation is publicly available and provides a method for utilizing PLIS, without having to install it or the lexical resources it uses. Finally, since PLIS is a lexical system it can easily be adjusted to other languages. One only needs to replace the basic lexical text processing tools and plug in knowledge resources in the target language. If PLIS is provided with bilingual resources,7 it can operate also as a cross-lingual inference system (Negri et al., 2012). For instance, the text in Fig 3 is given in English, while the hypothesis is written in Spanish (given as a list of lemma:part-of-speech). The left side of the figure depicts a cross-lingual inference process in which the only lexical knowledge resource used is a manually built English-Spanish dictionary. As can be seen, two Spanish terms, jugador and casa remain uncovered since the dictionary alone cannot connect them to any of the English terms in the text. As illustrated in the right side of Fig 3, PLIS enables the combination of the bilingual dictionary with monolingual resources to prod</context>
</contexts>
<marker>Negri, Marchetti, Mehdad, Bentivogli, Giampiccolo, 2012</marker>
<rawString>Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa Bentivogli, and Danilo Giampiccolo. 2012. Semeval-2012 task 8: Cross-lingual textual entailment for content synchronization. In Proc. of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Libby Barak</author>
<author>Ido Dagan</author>
</authors>
<title>Extracting lexical reference rules from Wikipedia.</title>
<date>2009</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="9188" citStr="Shnarch et al., 2009" startWordPosition="1428" endWordPosition="1431">rm. The sense is the ordinal number of WordNet sense. Whenever we do not know the sense of a term we implement the most frequent sense heuristic.3 An edge represents an inference link and is labeled with the semantic relation of this link (e.g. cytokine‚Üíprotein is labeled with the WordNet relation hypernym). 2.1 Available plug-ins for lexical resources We have implemented plug-ins for the following resources: the English lexicon WordNet (Fellbaum, 1998)(based on either JWI, JWNL or extJWNL java APIs4), CatVar (Habash and Dorr, 2003), a categorial variations database, Wikipedia-based resource (Shnarch et al., 2009), which applies several extraction methods to derive inference links from the text and structure of Wikipedia, VerbOcean (Chklovski and Pantel, 2004), a knowledge base of fine-grained semantic relations between verbs, Lin‚Äôs distributional similarity thesaurus (Lin, 1998), and DIRECT (Kotlerman et al., 2010), a directional distributional similarity thesaurus geared for lexical inference. To summarize, the lexical integrator finds all possible inference chains (of a predefined length), resulting from any combination of inference links extracted from lexical resources, which link any t, h pair of</context>
</contexts>
<marker>Shnarch, Barak, Dagan, 2009</marker>
<rawString>Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Extracting lexical reference rules from Wikipedia. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Jacob Goldberger</author>
<author>Ido Dagan</author>
</authors>
<title>Towards a probabilistic model for lexical entailment.</title>
<date>2011</date>
<booktitle>In Proc. of the TextInfer Workshop.</booktitle>
<contexts>
<context position="6129" citStr="Shnarch et al., 2011" startWordPosition="937" endWordPosition="940"> flexible system, allowing users to choose the set of knowledge resources as well as the model by which inference 1The complete software package is available at http:// www.cs.biu.ac.il/nlp/downloads/PLIS.html and an online interactive viewer is available for examination at http://irsrv2. cs.biu.ac.il/nlp-net/PLIS.html. is done. PLIS can be easily extended with new knowledge resources and new inference models. It comes with a set of ready-to-use plug-ins for many common lexical resources (Section 2.1) as well as two implementation of the scoring framework. These implementations, described in (Shnarch et al., 2011; Shnarch et al., 2012), provide probability estimations for inference. PLIS has an interactive online viewer (Section 4) which provides a visualization of the entire inference process, and is very helpful for analysing lexical inference models and lexical resources usability. 2 Lexical integrator The input for the lexical integrator is a set of lexical resources and a pair of text T and hypothesis H. The lexical integrator extracts lexical inference links from the various lexical resources to connect each text term ti ‚ààT with each hypothesis term hj ‚àà H2. A lexical inference link indicates a </context>
<context position="11106" citStr="Shnarch et al., 2011" startWordPosition="1717" endWordPosition="1720">er, it is a matter of changing a variable in the configuration of PLIS to switch between these two policies. 4http://wordnet.princeton.edu/wordnet/related-projects/ number of hypothesis terms for which inference chains, originated from text terms, were found. In PLIS, the inference model is a plug-in, similar to the lexical knowledge resources, and can be easily replaced to change the inference logic. We provide PLIS with two implemented baseline lexical inference models which are mathematically based. These are two Probabilistic Lexical Models (PLMs), HN-PLM and M-PLM which are described in (Shnarch et al., 2011; Shnarch et al., 2012) respectively. A PLM provides probability estimations for the three parts of the inference process (as shown in Fig 1): the validity probability of each inference chain (i.e. the probability for a valid inference relation between its endpoint terms) P(ti ‚Üí hj), the probability of each hypothesis term to be inferred by the entire text P(T ‚Üí hj) (term-level probability), and the probability of the entire hypothesis to be inferred by the text P(T ‚Üí H) (sentencelevel probability). HN-PLM describes a generative process by which the hypothesis is generated from the text. Its p</context>
</contexts>
<marker>Shnarch, Goldberger, Dagan, 2011</marker>
<rawString>Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011. Towards a probabilistic model for lexical entailment. In Proc. of the TextInfer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>A probabilistic lexical model for ranking textual inferences.</title>
<date>2012</date>
<booktitle>In Proc. of *SEM.</booktitle>
<contexts>
<context position="6152" citStr="Shnarch et al., 2012" startWordPosition="941" endWordPosition="944">wing users to choose the set of knowledge resources as well as the model by which inference 1The complete software package is available at http:// www.cs.biu.ac.il/nlp/downloads/PLIS.html and an online interactive viewer is available for examination at http://irsrv2. cs.biu.ac.il/nlp-net/PLIS.html. is done. PLIS can be easily extended with new knowledge resources and new inference models. It comes with a set of ready-to-use plug-ins for many common lexical resources (Section 2.1) as well as two implementation of the scoring framework. These implementations, described in (Shnarch et al., 2011; Shnarch et al., 2012), provide probability estimations for inference. PLIS has an interactive online viewer (Section 4) which provides a visualization of the entire inference process, and is very helpful for analysing lexical inference models and lexical resources usability. 2 Lexical integrator The input for the lexical integrator is a set of lexical resources and a pair of text T and hypothesis H. The lexical integrator extracts lexical inference links from the various lexical resources to connect each text term ti ‚ààT with each hypothesis term hj ‚àà H2. A lexical inference link indicates a semantic relation betwe</context>
<context position="11129" citStr="Shnarch et al., 2012" startWordPosition="1721" endWordPosition="1724">changing a variable in the configuration of PLIS to switch between these two policies. 4http://wordnet.princeton.edu/wordnet/related-projects/ number of hypothesis terms for which inference chains, originated from text terms, were found. In PLIS, the inference model is a plug-in, similar to the lexical knowledge resources, and can be easily replaced to change the inference logic. We provide PLIS with two implemented baseline lexical inference models which are mathematically based. These are two Probabilistic Lexical Models (PLMs), HN-PLM and M-PLM which are described in (Shnarch et al., 2011; Shnarch et al., 2012) respectively. A PLM provides probability estimations for the three parts of the inference process (as shown in Fig 1): the validity probability of each inference chain (i.e. the probability for a valid inference relation between its endpoint terms) P(ti ‚Üí hj), the probability of each hypothesis term to be inferred by the entire text P(T ‚Üí hj) (term-level probability), and the probability of the entire hypothesis to be inferred by the text P(T ‚Üí H) (sentencelevel probability). HN-PLM describes a generative process by which the hypothesis is generated from the text. Its parameters are the relia</context>
</contexts>
<marker>Shnarch, Dagan, Goldberger, 2012</marker>
<rawString>Eyal Shnarch, Ido Dagan, and Jacob Goldberger. 2012. A probabilistic lexical model for ranking textual inferences. In Proc. of *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy model? A quasisynchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="12478" citStr="Wang et al., 2007" startWordPosition="1942" endWordPosition="1945"> from each resource corresponds to a valid inference). For learning these parameters HN-PLM applies a schema of the EM algorithm (Dempster et al., 1977). Its performance on the recognizing textual entailment task, RTE (Bentivogli et al., 2009; Bentivogli et al., 2010), are in line with the state of the art inference systems, including complex systems which perform syntactic analysis. This model is improved by M-PLM, which deduces sentence-level probability from term-level probabilities by a Markovian process. PLIS with this model was used for a passage retrieval for a question answering task (Wang et al., 2007), and outperformed state of the art inference systems. Both PLMs model the following prominent aspects of the lexical inference phenomenon: (i) considering the different reliability levels of the input knowledge resources, (ii) reducing inference chain probability as its length increases, and (iii) increasing term-level probability as we have more inference chains which suggest that the hypothesis term is inferred by the text. Both PLMs only need sentence-level annotations from which they derive term-level inference probabilities. To summarize, the lexical inference module 99 P(T ‚Äì3, hj) 4 3 2</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? A quasisynchronous grammar for QA. In Proc. of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>