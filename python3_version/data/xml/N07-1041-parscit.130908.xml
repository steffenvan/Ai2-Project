<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.974523">
Combining Probability-Based Rankers for Action-Item Detection
</title>
<author confidence="0.977145">
Jaime G. Carbonell
</author>
<affiliation confidence="0.984529">
Language Technologies Institute, Carnegie Mellon
</affiliation>
<address confidence="0.789221">
5000 Forbes Ave
Pittsburgh, PA 15213
</address>
<email confidence="0.996949">
jgc@cs.cmu.edu
</email>
<note confidence="0.617244666666667">
Paul N. Bennett
Microsoft Research*
One Microsoft Way
</note>
<address confidence="0.824779">
Redmond, WA 98052
</address>
<email confidence="0.992039">
paul.n.bennett@microsoft.com
</email>
<sectionHeader confidence="0.998543" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99999425">
This paper studies methods that automat-
ically detect action-items in e-mail, an
important category for assisting users in
identifying new tasks, tracking ongoing
ones, and searching for completed ones.
Since action-items consist of a short span
of text, classifiers that detect action-items
can be built from a document-level or a
sentence-level view. Rather than com-
mit to either view, we adapt a context-
sensitive metaclassification framework to
this problem to combine the rankings pro-
duced by different algorithms as well as
different views. While this framework is
known to work well for standard classi-
fication, its suitability for fusing rankers
has not been studied. In an empirical eval-
uation, the resulting approach yields im-
proved rankings that are less sensitive to
training set variation, and furthermore, the
theoretically-motivated reliability indica-
tors we introduce enable the metaclassi-
fier to now be applicable in any problem
where the base classifiers are used.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997034125">
From business people to the everyday person, e-
mail plays an increasingly central role in a modern
lifestyle. With this shift, e-mail users desire im-
proved tools to help process, search, and organize
the information present in their ever-expanding in-
boxes. A system that ranks e-mails according to the
This work was performed primarily while the first author
was supported by Carnegie Mellon University.
</bodyText>
<note confidence="0.526175">
From: Henry Hutchins &lt;hhutchins@innovative.company.com&gt;
</note>
<author confidence="0.329468">
To: Sara Smith; Joe Johnson; William Woolings
</author>
<table confidence="0.947397555555556">
Subject: meeting with prospective customers
Hi All,
I’d like to remind all of you that the group from GRTY will
be visiting us next Friday at 4:30 p.m. The schedule is:
+ 9:30 a.m. Informal Breakfast and Discussion in Cafeteria
+ 10:30 a.m. Company Overview
+ 11:00 a.m. Individual Meetings (Continue Over Lunch)
+ 2:00 p.m. Tour of Facilities
+ 3:00 p.m. Sales Pitch
</table>
<figure confidence="0.6527368">
In order to have this go off smoothly, I would like to practice
the presentation well in advance. As a result, I will need each
ofyour parts by Wednesday.
Keep up the good work!
–Henry
</figure>
<figureCaption confidence="0.999692">
Figure 1: An E-mail with Action-Item (italics added).
</figureCaption>
<bodyText confidence="0.999401352941176">
likelihood of containing “to-do” or action-items can
alleviate a user’s time burden and is the subject of
ongoing research throughout the literature.
In particular, an e-mail user may not always pro-
cess all e-mails, but even when one does, some
emails are likely to be of greater response urgency
than others. These messages often contain action-
items. Thus, while importance and urgency are not
equal to action-item content, an effective action-item
detection system can form one prominent subcom-
ponent in a larger prioritization system.
Action-item detection differs from standard text
classification in two important ways. First, the user
is interested both in detecting whether an email
contains action-items and in locating exactly where
these action-item requests are contained within the
email body. Second, action-item detection attempts
</bodyText>
<page confidence="0.983684">
324
</page>
<note confidence="0.798048">
Proceedings of NAACL HLT 2007, pages 324–331,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999639047619048">
to recover the sender’s intent — whether she means
to elicit response or action on the part of the receiver.
In this paper, we focus on the primary problem
of presenting e-mails in a ranked order according to
their likelihood of containing an action-item. Since
action-items typically consist of a short text span —
a phrase, sentence, or small passage — supervised
input to a learning system can either come at the
document-level where an e-mail is labeled yes/no
as to whether it contains an action-item or at the
sentence-level where each span that is an action-
item is explicitly identified. Then, a corresponding
document-level classifier or aggregated predictions
from a sentence-level classifier can be used to esti-
mate the overall likelihood for the e-mail.
Rather than commit to either view, we use a com-
bination technique to capture the information each
viewpoint has to offer on the current example. The
STRIVE approach (Bennett et al., 2005) has been
shown to provide robust combinations of heteroge-
neous models for standard topic classification by
capturing areas of high and low reliability via the
use of reliability indicators.
However, using STRIVE in order to produce im-
proved rankings has not been previously studied.
Furthermore, while they introduce some reliabil-
ity indicators that are general for text classification
problems as well as ones specifically tied to naive
Bayes models, they do not address other classifica-
tion models. We introduce a series of reliability in-
dicators connected to areas of high/low reliability in
kNN, SVMs, and decision trees to allow the combi-
nation model to include such factors as the sparse-
ness of training example neighbors around the cur-
rent example being classified. In addition, we pro-
vide a more formal motivation for the role these vari-
ables play in the resulting metaclassification model.
Empirical evidence demonstrates that the result-
ing approach yields a context-sensitive combination
model that improves the quality of rankings gener-
ated as well as reducing the variance of the ranking
quality across training splits.
</bodyText>
<sectionHeader confidence="0.939865" genericHeader="method">
2 Problem Approach
</sectionHeader>
<bodyText confidence="0.999877470588235">
In contrast to related combination work, we focus on
improving rankings through the use of a metaclass-
ification framework. In addition, rather than sim-
ply focusing on combining models from different
classification algorithms, we also examine combin-
ing models that have different views, in that both the
qualitative nature of the labeled data and the applica-
tion of the learned base models differ. Furthermore,
we improve upon work on context-sensitive com-
bination by introducing reliability indicators which
model the sensitivity of a classifier’s output around
the current prediction point. Finally, we focus on the
application of these methods to action-item data —
a growing area of interest which has been demon-
strated to behave differently than more standard text
classification problems (e.g. topic) in the literature
(Bennett and Carbonell, 2005).
</bodyText>
<subsectionHeader confidence="0.973377">
2.1 Action-Item Detection
</subsectionHeader>
<bodyText confidence="0.999986">
There are three basic problems for action-item de-
tection. (1) Document detection: Classify an e-mail
as to whether or not it contains an action-item. (2)
Document ranking: Rank the e-mails such that all
e-mail containing action-items occur as high as pos-
sible in the ranking. (3) Sentence detection: Classify
each sentence in an e-mail as to whether or not it is
an action-item.
Here we focus on the document ranking problem.
Improving the overall ranking not only helps users
find e-mails with action-items quicker (Bennett and
Carbonell, 2005) but can decrease response times
and help ensure that key e-mails are not overlooked.
Since a typical user will eventually process all
received mail, we assume that producing a quality
ranking will more directly measure the impact on
the user than accuracy or F1. Therefore, we focus on
ROC curves and area under the curve (AUC) since
both reflect the quality of the ranking produced.
</bodyText>
<subsectionHeader confidence="0.998335">
2.2 Combining Classifiers with Metaclassifiers
</subsectionHeader>
<bodyText confidence="0.999959176470588">
One of the most common approaches to classi-
fier combination is stacking (Wolpert, 1992). In
this approach, a metaclassifier observes a past his-
tory of classifier predictions to learn how to weight
the classifiers according to their demonstrated ac-
curacies and interactions. To build the history,
cross-validation over the training set is used to ob-
tain predictions from each base classifier. Next, a
metalevel representation of the training set is con-
structed where each example consists of the class
label and the predictions of the base classifiers. Fi-
nally, a metaclassifier is trained on the metalevel rep-
resentation to learn a model of how to combine the
base classifiers.
However, it might be useful to augment the his-
tory with information other than the predicted prob-
abilities. For example, during peer review, reviewers
</bodyText>
<page confidence="0.995838">
325
</page>
<figure confidence="0.994788875">
w1
Unigram
SVM
Reliability
Indicators
� � �
class
rn
r1
r2
Metaclassifier
w2
w3
� � �
wn
class
</figure>
<figureCaption confidence="0.9978255">
Figure 2: Architecture of STRIVE. In STRIVE, an additional layer of learning is added where the metaclassifier can use the context
established by the reliability indicators and the output of the base classifiers to make an improved decision.
</figureCaption>
<bodyText confidence="0.999936875">
typically provide both a 1-5 acceptance rating and a
1-5 confidence. The first of these is related to an es-
timate of class membership, P(“accept&amp;quot;  |paper),
but the second is closer to a measure of expertise or
a self-assessment of the reviewer’s reliability on an
example-by-example basis.
Automatically deriving such self-assessments for
classification algorithms is non-trivial. The Stacked
Reliability Indicator Variable Ensemble framework,
or STRIVE, demonstrates how to extend stacking by
incorporating such self-assessments as a layer of re-
liability indicators and introduces a candidate set of
functions (Bennett et al., 2005).
The STRIVE architecture is depicted in Figure 2.
From left to right: (1) a bag-of-words representation
of the document is extracted and used by the base
classifiers to predict class probabilities; (2) reliabil-
ity indicator functions use the predicted probabili-
ties and the features of the document to characterize
whether this document falls within the “expertise”
of the classifiers; (3) a metalevel classifier uses the
base classifier predictions and the reliability indica-
tors to make a more reliable combined prediction.
From the perspective of improving action-item
rankings, we are interested in whether stacking or
striving can improve the quality of rankings. How-
ever, we hypothesize that striving will perform better
since it can learn a model that varies the combination
rule based on the current example and thus, better
capture when a particular classifier at the document-
level or sentence-level, bag-of-words or n-gram rep-
resentation, etc. will produce a reliable prediction.
</bodyText>
<subsectionHeader confidence="0.99313">
2.3 Formally Motivating Reliability Indicators
</subsectionHeader>
<bodyText confidence="0.999900818181818">
While STRIVE has been shown to provide robust
combination for topic classification, a formal moti-
vation is lacking for the type of reliability indicators
that are the most useful in classifier combination.
Assume we restrict our choice of metaclassifier to
a linear model. One natural choice is to rank the
e-mails according to the estimated posterior proba-
bility, P�(class = action item  |x), but in a linear
combination framework it is actually more conve-
nient to work with the estimated log-odds or logit
transform which is monotone in the posterior, λ� =
</bodyText>
<equation confidence="0.987541">
logPˆ(class=action item|x)
1− Pˆ (class=action item|x) (Kahn, 2004).
</equation>
<bodyText confidence="0.998737074074074">
Now, consider applying a metaclassifier to a sin-
gle base classifier. Given only a classifier’s probabil-
ity estimates, a metaclassifier cannot improve on the
estimates if they are well-calibrated (DeGroot and
Fienberg, 1986). Thus a metaclassifier applied to
a single base classifier corresponds to recalibration
(Kahn, 2004).
Assume each of the n base models gives an un-
calibrated log-odds estimate �i. Then the com-
bination model would have the form k(x) =
W0(x)+Eni=1 Wi(x)�i(x) where the Wi are exam-
ple dependent weight functions that the combination
model learns. The obvious implication is that our
reliability indicators can be informed by the optimal
values for the weighting functions.
We can determine the optimal weights in a sim-
plified case with a single base classifier by assuming
we are given “true” log-odds values, λ, and a family
of distributions Ax such that Ax = p(z  |x)
encodes what is local to x by giving the probability
of drawing a point z near to x. We use A instead of
Ax for notational simplicity. Since A encodes the
example dependent nature of the weights, we can
drop x from the weight functions. To find weights
that minimize the squared difference between the
true log-odds and the estimated log-odds in the A
vicinity of x, we can solve a standard regression
</bodyText>
<page confidence="0.707702">
2
</page>
<equation confidence="0.6138995">
problem, argminw0 w1 EΔ [(w1 λ + w0 − λ)].
� �
</equation>
<bodyText confidence="0.813198">
Under the assumption VARΔ λ� =� 0, this yields:
</bodyText>
<page confidence="0.991662">
326
</page>
<figure confidence="0.949520428571429">
2
1
5
x
6
2 3
4
</figure>
<equation confidence="0.954732285714286">
h i
w0 = EΔ[λ] − w1EΔ λ� (1)
COVΔ �λ, λ
VARΔ λ�
h i
h i σλ ρλ,ˆλ (2)
σˆλ
</equation>
<bodyText confidence="0.9999862">
where σ and ρ are the stdev and correlation co-
efficient under A, respectively. The first parame-
ter is a measure of calibration that addresses the
question, “How far off on average is the estimated
log-odds from the true log-odds in the local con-
text?” The second is a measure of correlation, “How
closely does the estimated log-odds vary with the
true log-odds?” Note that the second parameter de-
pends on the local sensitivity of the base classifier,
VARA N = σˆλ. Although we do not have true
log-odds, we can introduce local density models to
estimate the local sensitivity of the model.
In particular, we introduce a series of relia-
bility indicators by first defining a A distribu-
tion and either computing VARΔ h�i, EΔ h�i or
</bodyText>
<equation confidence="0.818874">
h i
the closely related terms VARΔ �λ(z) − �λ(x) ,
h i
EΔ �λ(z) − �λ(x) . We use the resulting values for
</equation>
<bodyText confidence="0.999952333333333">
an example as features for a linear metaclassifier.
Thus we use a context-dependent bias term but leave
the more general model for future work.
</bodyText>
<subsectionHeader confidence="0.997347">
2.4 Model-Based Reliability Indicators
</subsectionHeader>
<bodyText confidence="0.999985052631579">
As discussed in Section 2.3, we wish to define local
distributions in order to compute the local sensitivity
and similar terms for the base classification models.
To do so, we define local distributions that have the
same “flavor” as the base classification model.
First, consider the kNN classifier. Since we are
concerned with how the decision function would
change as we move locally around the current pre-
diction point, it is natural to consider a set of shifts
defined by the k neighbors. In particular, let di de-
note the document that has been shifted by a factor
βi toward the ith neighbor, i.e. di = d+βi(ni −d).
We use the largest βi such that the closest neighbor
to the new point is the original document, i.e. the
boundary of the Voronoi cell (see Figure 3). Clearly,
βi will not exceed 0.5, and we can find it efficiently
using a simple bisection algorithm. Now, let A be
a uniform point-mass distribution over the shifted
points and �λkNN, the output score of the kNN model.
</bodyText>
<figure confidence="0.99539625">
1.5
1
0.5
0
−0.5
−1
−1.5
−1.5 −1 −0.5 0 0.5 1 1.5 2
</figure>
<figureCaption confidence="0.993229">
Figure 3: Illustration of the kNN shifts produced for a predic-
tion point x using the numbered points as its neighborhood.
</figureCaption>
<bodyText confidence="0.9707525">
Given this definition of A, it is now straight-
forward to compute the kNN based reliabil-
</bodyText>
<equation confidence="0.889547">
ity indicators: EΔ[�λkNN(z) − �λkNN(x)] and
Var1/2
Δ [�λkNN(z) − �λkNN(x)].
</equation>
<bodyText confidence="0.983414176470588">
Similarly, we define variables for the SVM class-
ifier by considering a document’s locality in terms
of nearby support vectors from the set of support
vectors, V. To determine βi, we define it in terms
of the closest support vector in V to d. Let c be
half the distance to the nearest point in V, i.e. c =
2 minv�� kv − dk. Then βi = �
1 IIvi�dII.1 Thus, the
shift vectors are all rescaled to have the same length.
Now, we must define a probability for the shift. We
use a simple exponential based on c and the rela-
tive distance from the document to the support vec-
tor defining this shift. Let di ∼ A where PΔ(di) ∝
exp(−kvi − dk + 2E) and PVi=1 PΔ(di) = 1.2
Given this definition of A, we compute the
SVM based reliability indicators: EΔ[ �λSVM(z) −
�λSVM(x)] and Var1/2
</bodyText>
<equation confidence="0.95758">
Δ [�λSVM(z) − �λSVM(x)].
</equation>
<bodyText confidence="0.999723333333333">
Space prevents us from presenting all the deriva-
tions here. However, we also define decision-tree
based variables where the locality distribution gives
high probability to documents that would land in
nearby leaves. For a multinomial naive Bayes model
(NB), we define a distribution of documents iden-
tical to the prediction document except having an
occurrence of a single feature deleted. For the
multivariate Bernoulli naive Bayes (MBNB) model
</bodyText>
<footnote confidence="0.9985574">
1We assume that the minimum distance is not zero. If it is
zero, then we return zero for all of the variables.
2As is standard to handle different document lengths, we
take the distance between documents after they have been nor-
malized to the unit sphere.
</footnote>
<equation confidence="0.54827">
w1 =
</equation>
<page confidence="0.990215">
327
</page>
<bodyText confidence="0.999873411764706">
that models feature presence/absence, we use a
distribution over all documents that has one pres-
ence/absence bit flipped from the prediction docu-
ment. It is interesting to note that the variables from
the naive Bayes models can be shown to be equiva-
lent to variables introduced by Bennett et al. (2005)
— although those were derived in a different fashion
by analyzing the weight a single feature carries with
respect to the overall prediction.
Furthermore, from this starting point, we go on to
define similar variables of possible interest. Includ-
ing the two for each model described here, we define
10 kNN variables, 5 SVM variables, 2 decision-tree
variables, 6 NB model based variables, and 6 MBNB
variables. We describe these variables as well as im-
plementation details and computational complexity
results in (Bennett, 2006).
</bodyText>
<sectionHeader confidence="0.997264" genericHeader="method">
3 Experimental Analysis
</sectionHeader>
<subsectionHeader confidence="0.965432">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.998948375">
Our corpus consists of e-mails obtained from vol-
unteers at an educational institution and covers
subjects such as: organizing a research work-
shop, arranging for job-candidate interviews, pub-
lishing proceedings, and talk announcements. Af-
ter eliminating duplicate e-mails, the corpus con-
tains 744 messages with a total of 6301 automat-
ically segmented sentences. A human panel la-
beled each phrase or sentence that contained an
explicit request for information or action. 416 e-
mails have no action-items and 328 e-mails con-
tain action-items. Additional information such
as annotator agreement, distribution of message
length, etc. can be found in (Bennett and Car-
bonell, 2005). An anonymized corpus is available
at http://www.cs.cmu.edu/˜pbennett/action-item-dataset.html.
</bodyText>
<subsectionHeader confidence="0.999033">
3.2 Feature Representation
</subsectionHeader>
<bodyText confidence="0.999973857142857">
We use two types of feature representation: a bag-
of-words representation which uses all unigram to-
kens as the feature pool; and a bag-of-n-grams
where n includes all n-grams where n &lt; 4. For
both representations at both the document-level and
sentence-level, we used only the top 300 features by
the chi-squared statistic.
</bodyText>
<subsectionHeader confidence="0.685531">
3.3 Document-Level Classifiers
</subsectionHeader>
<bodyText confidence="0.937993">
kNN
We used a s-cut variant of kNN common in text
classification (Yang, 1999) and a tfidf-weighting
of the terms with a distance-weighted vote of the
neighbors to compute the output. k was set to be
2(Flo92 N] + 1) where N is the number of training
points. 3 The score used as the uncalibrated log-
odds estimate of being an action-item is:
</bodyText>
<equation confidence="0.87860925">
��λkNN(x) = �cos(x, n) − cos(x, n).
n∈kNN(x)|c(n)=action
item n∈kNN(x)|c(n)6=action
item
</equation>
<sectionHeader confidence="0.433141" genericHeader="method">
SVM
</sectionHeader>
<bodyText confidence="0.999877428571428">
We used a linear SVM as implemented in the
SVMlight package v6.01 (Joachims, 1999) with a
tfidf feature representation and L2-norm. All de-
fault settings were used. SVM’s margin score,
E αiyi K(xi, xj), has been shown to empirically
behave like an uncalibrated log-odds estimate (Platt,
1999).
</bodyText>
<sectionHeader confidence="0.665261" genericHeader="method">
Decision Trees
</sectionHeader>
<bodyText confidence="0.9904738">
For the decision-tree implementation, we used the
WinMine toolkit and refer to this as Dnet below (Mi-
crosoft Corporation, 2001). Dnet builds decision
trees using a Bayesian machine learning algorithm
(Chickering et al., 1997; Heckerman et al., 2000).
The estimated log-odds is computed from a Laplace
correction to the empirical probability at a leaf node.
Naive Bayes
We use a multinomial naive Bayes (NB) and a mul-
tivariate Bernoulli naive Bayes classifier (MBNB)
(McCallum and Nigam, 1998). For these classifi-
ers, we smoothed word and class probabilities us-
ing a Bayesian estimate (with the word prior) and
a Laplace m-estimate, respectively. Since these are
probabilistic, they issue log-odds estimates directly.
</bodyText>
<subsectionHeader confidence="0.862952">
3.4 Sentence-Level Classifiers
</subsectionHeader>
<bodyText confidence="0.898393636363636">
Each e-mail is automatically segmented into sen-
tences using RASP (Carroll, 2002). Since the cor-
pus has fine grained labels, we can train classifiers
to classify a sentence. Each classifier in Section 3.3
is also used to learn a sentence classifier. However,
we then must make a document-level prediction.
In order to produce a ranking score, the con-
fidence that the document contains an action-item is:
� 1n(d) Es∈d|7r(s)=1 λ(s), IsEd|π(s) = 1
λ(d)_
n(d) maxs∈d �λ(s) 1o.w.
</bodyText>
<footnote confidence="0.69818675">
3This rule is not guaranteed be optimal for a particular value
of N but is motivated by theoretical results which show such a
rule converges to the optimal classifier as the number of training
points increases (Devroye et al., 1996).
</footnote>
<page confidence="0.997119">
328
</page>
<bodyText confidence="0.999994333333333">
where s is a sentence in document d, 7r is the class-
ifier’s 1/0 prediction, � is the score the classifier as-
signs as its confidence that 7r(s) = 1, and n(d) is
the greater of 1 and the number of (unigram) to-
kens in the document. In other words, when any
sentence is predicted positive, the document score
is the length normalized sum of the sentence scores
above threshold. When no sentence is predicted pos-
itive, the document score is the maximum sentence
score normalized by length. The length normaliza-
tion compensates for the fact that we are more likely
to emit a false positive the longer a document is.
</bodyText>
<subsectionHeader confidence="0.990045">
3.5 Stacking
</subsectionHeader>
<bodyText confidence="0.999798888888889">
To examine the hypothesis that the reliability in-
dicators provide utility beyond the information
present in the output of the 20 base classifiers
(2 representations*2 views*5 classifiers), we con-
struct a linear stacking model which uses only the
base classifier outputs and no reliability indicators as
a baseline. For the implementation, we use SVMlight
with default settings. The inputs to this classifier are
normalized to have zero mean and a scaled variance.
</bodyText>
<subsectionHeader confidence="0.997424">
3.6 Striving
</subsectionHeader>
<bodyText confidence="0.998767909090909">
Since we are constructing base classifiers for both
the bag-of-words and bag-of-n-grams representa-
tions, this gives 58 reliability indicators from com-
puting the variables in Section 2.4 for the document-
level classifiers (58 = 2 * [6 + 6 + 10 + 5 + 2]).
Although the model-based indicators are defined
for each sentence prediction, to use them at the
document-level we must somehow combine the re-
liability indicators over each sentence. The simplest
method is to average each classifier-based indicator
across the sentences in the document. We do so and
thus obtain another 58 reliability indicators.
Furthermore, our model might benefit from some
of the structure a sentence-level classifier offers
when combining document predictions. Analogous
to the sensitivity of each base model, we can con-
sider such indicators as the mean and standard de-
viation of the classifier confidences across the sen-
tences within a document. For each sentence-level
base classifier, these become two more indicators
which we can benefit from when combining docu-
ment predictions. This introduces 20 more variables
(20 = 2 representations * 2 * 5 classifiers).
Finally, we include the 2 basic voting statistic
reliability-indicators (PercentPredictingPositive and
PercentAgreeWBest) that Bennett et al. (2005) found
useful for topic classification. This yields a total of
138 reliability-indicators (138 = 58 + 20 + 58 + 2).
With the 20 classifier outputs, there are a total of 158
input features for striving to handle.
As with stacking, we use SVMlight with default
settings and normalize the inputs to this classifier to
have zero mean and a scaled variance.
</bodyText>
<subsectionHeader confidence="0.994318">
3.7 Performance Measures
</subsectionHeader>
<bodyText confidence="0.98864652">
We wish to improve the rankings of the e-mails in
the inbox such that action-item e-mails occur higher
in the inbox. Therefore, we use the area under the
curve (AUC) of an ROC curve as a measure of rank-
ing performance. AUC is a measure of overall model
and ranking quality that has gained wider adoption
recently and is equivalent to the Mann-Whitney-
Wilcoxon sum of ranks test (Hanley and McNeil,
1982). To put improvement in perspective, we can
write our relative reduction in residual area (RRA)
as 1_AUCUC . We present gains relative to the
basel- e
best AUC performer (bRRA), and relative to perfect
dynamic selection performance, (dRRA), which as-
sumes we could accurately dynamically choose the
best classifier per cross-validation run.
The F1 measure is the harmonic mean of preci-
sion and recall and is common throughout text class-
ification (Yang and Liu, 1999). Although we are not
concerned with F1 performance here, some users of
the system might be interested in improving rank-
ing while having negligible negative effect on F1.
Therefore, we examine F1 to ensure that an improve-
ment in ranking will not come at the cost of a statis-
tically significant decrease in F1.
</bodyText>
<subsectionHeader confidence="0.989022">
3.8 Experimental Methodology
</subsectionHeader>
<bodyText confidence="0.999990444444445">
To evaluate performance of the combination sys-
tems, we perform 10-fold cross-validation and com-
pute the average performance. For significance tests,
we use a two-tailed t-test (Yang and Liu, 1999)
to compare the values obtained during each cross-
validation fold with a p-value of 0.05.
We examine two hypotheses: Stacking will out-
perform all of the base classifiers; Striving will out-
perform all the base classifiers and stacking.
</bodyText>
<subsectionHeader confidence="0.58629">
3.9 Results &amp; Discussion
</subsectionHeader>
<bodyText confidence="0.998491">
Table 1 presents the summary of results. The best
performer in each column is in bold. If a combi-
nation method statistically significantly outperforms
all base classifiers, it is underlined.
</bodyText>
<page confidence="0.995884">
329
</page>
<table confidence="0.996414285714285">
F1 AUC bRRA dRRA
Document-Level, Bag-of-Words Representation
Dnet 0.7398 0.8423 1.41 1.78
NB 0.6905 0.7537 2.27 2.91
MBNB 0.6729 0.7745 2.00 2.49
SVM 0.6918 0.8367 1.48 1.87
kNN 0.6695 0.7669 2.17 2.74
Document-Level, Ngram Representation
Dnet 0.7412 0.8473 1.38 1.77
NB 0.7361 0.8114 1.75 2.23
MBNB 0.7534 0.8537 1.30 1.61
SVM 0.7392 0.8640 1.24 1.59
kNN 0.7021 0.8244 1.62 2.01
Sentence-Level, Bag-of-Words Representation
Dnet 0.7793 0.8885 1.00 1.27
NB 0.7731 0.8645 1.21 1.50
MBNB 0.7888 0.8699 1.14 1.42
SVM 0.6985 0.8548 1.34 1.70
kNN 0.6328 0.6823 2.98 3.88
Sentence-Level, Ngram Representation
Dnet 0.7521 0.8723 1.13 1.42
NB 0.8012 0.8723 1.15 1.46
MBNB 0.8010 0.8777 1.10 1.38
SVM 0.7842 0.8620 1.23 1.58
kNN 0.6811 0.8078 1.76 2.29
Metaclassifiers
Stacking 0.7765 0.8996 0.88 1.12
STRIVE 0.7813 0.9145 0.76 0.94
</table>
<figure confidence="0.980962555555555">
0.5
0.6
0.7
0.8
0.9
1
True Positive Rate
0 0.2 0.4 0.6 0.8 1
False Positive Rate
</figure>
<figureCaption confidence="0.963419">
Figure 4: ROC curves (rotated).
</figureCaption>
<figure confidence="0.71104425">
MBNB (sent,ngram)
SVM (sent,ngram)
Stacking
STRIVE
</figure>
<tableCaption confidence="0.993213">
Table 1: Base classifier and combiner performance
</tableCaption>
<bodyText confidence="0.999966695652174">
Now, we turn to the issue of whether combination
improves the ranking of the documents. Examining
the results in Table 1, we see that STRIVE statistically
significantly beats every other classifier according to
AUC. Stacking outperforms the base classifiers with
respect to AUC but not statistically significantly.
Examining F1, we see that neither combination
method outperforms the best base classifier, NB
(sent,ngram). If we examine the hypothesis of
whether this base classifier significantly outperforms
either combination method, the hypothesis is re-
jected. Thus, STRIVE improves the overall ranking
with a negligible effect on F1.
Finally, we compare the ROC curves of striving,
stacking, and two of the most competitive base class-
ifiers in Figure 4. We see that striving loses by a
slight amount to stacking early in the curve but still
beats the base classifiers. Later in the curve, it dom-
inates all the classifiers. If we examine the curves
using error bars, we see that the variance of STRIVE
drops faster than the other classifiers as we move fur-
ther along the x-axis. Thus, STRIVE’s ranking quality
varies less with changes to the training set.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9998965">
Several researchers have considered text classifi-
cation tasks similar to action-item detection. Co-
hen et al. (2004) describe an ontology of “speech
acts”, such as “Propose a Meeting”, and attempt
to predict when an e-mail contains one of these
speech acts. Corston-Oliver et al. (2004) con-
sider detecting items in e-mail to “Put on a To-Do
List” using a sentence-level classifier. In earlier
work (Bennett and Carbonell, 2005), we demon-
strated that sentence-level classifiers typically out-
perform document-level classifiers on this problem
and examined the underlying reasons why this was
</bodyText>
<page confidence="0.991386">
330
</page>
<bodyText confidence="0.999922714285714">
the case. Furthermore, we presented user studies
demonstrating that users identify action-items more
rapidly when using the system.
In terms of classifier combination, a wide variety
of work has been done in the arena. The STRIVE
metaclassification approach (Bennett et al., 2005)
extended Wolpert’s stacking framework (Wolpert,
1992) to use reliability indicators. In recent work,
Lee et al. (2006) derive variance estimates for naive
Bayes and tree-augmented naive Bayes and use
them in the combination model. Our work comple-
ments theirs by laying groundwork for how to com-
pute variance estimates for models such as kNN that
have no obvious probabilistic component.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="discussions">
5 Future Work and Conclusion
</sectionHeader>
<bodyText confidence="0.999987823529412">
While there are many interesting directions for fu-
ture work, the most interesting is to directly integrate
the sensitivity and calibration quantities derived into
the more general model discussed in Section 2.3.
In this paper, we took an existing approach to
context-dependent combination, STRIVE, that used
many ad hoc reliability indicators and derived a
formal motivation for classifier model-based local
sensitivity indicators. These new reliability indi-
cators are efficiently computable, and the resulting
combination outperformed a vast array of alterna-
tive base classifiers for ranking in an action-item de-
tection task. Furthermore, the combination results
yielded a more robust performance relative to varia-
tion in the training sets. Finally, we demonstrated
that the STRIVE method could be successfully ap-
plied to ranking.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.840533857142857">
This work was supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract No.
NBCHD030010. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the au-
thor(s) and do not necessarily reflect the views of the Defense
Advanced Research Projects Agency (DARPA), or the Depart-
ment of Interior-National Business Center (DOI-NBC).
</reference>
<sectionHeader confidence="0.980237" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999784639344263">
Paul N. Bennett and Jaime Carbonell. 2005. Feature repre-
sentation for effective action-item detection. In SIGIR ’05,
Beyond Bag-of-Words Workshop.
Paul N. Bennett, Susan T. Dumais, and Eric Horvitz. 2005.
The combination of text classifiers using reliability indica-
tors. Information Retrieval, 8:67–100.
Paul N. Bennett. 2006. Building Reliable Metaclassifiers for
Text Learning. Ph.D. thesis, CMU. CMU-CS-06-121.
John Carroll. 2002. High precision extraction of grammatical
relations. In COLING ’02.
D.M. Chickering, D. Heckerman, and C. Meek. 1997. A
Bayesian approach to learning Bayesian networks with lo-
cal structure. In UAI ’97.
William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell.
2004. Learning to classify email into “speech acts”. In
EMNLP ’04.
Simon Corston-Oliver, Eric Ringger, Michael Gamon, and
Richard Campbell. 2004. Task-focused summarization of
email. In Text Summarization Branches Out: Proceedings of
the ACL ’04 Workshop.
Morris H. DeGroot and Stephen E. Fienberg. 1986. Comparing
probability forecasters: Basic binary concepts and multivari-
ate extensions. In P. Goel and A. Zellner, editors, Bayesian
Inference and Decision Techniques. Elsevier.
Luc Devroye, L´aszl´o Gy¨orfi, and G´abor Lugosi. 1996. A Prob-
abilistic Theory of Pattern Recognition. Springer-Verlag,
New York, NY.
James A. Hanley and Barbara J. McNeil. 1982. The meaning
and use of the area under a recever operating characteristic
(roc) curve. Radiology, 143(1):29–36.
D. Heckerman, D.M. Chickering, C. Meek, R. Rounthwaite,
and C. Kadie. 2000. Dependency networks for inference,
collaborative filtering, and data visualization. JMLR, 1:49–
75.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In Bernhard Sch¨olkopf, Christopher J. Burges, and
Alexander J. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Joseph M. Kahn. 2004. Bayesian Aggregation of Probabil-
ity Forecasts on Categorical Events. Ph.D. thesis, Stanford
University, June.
Chi-Hoon Lee, Russ Greiner, and Shaojun Wang. 2006. Using
query-specific variance estimates to combine bayesian class-
ifiers. In ICML ’06.
Andrew McCallum and Kamal Nigam. 1998. A comparison
of event models for naive bayes text classification. In AAAI
’98, Workshops. TR WS-98-05.
Microsoft Corporation. 2001. WinMine
Toolkit v1.0. http://research.microsoft.com/
˜dmax/WinMine/ContactInfo.html.
John C. Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likelihood
methods. In Alexander J. Smola, Peter Bartlett, Bern-
hard Scholkopf, and Dale Schuurmans, editors, Advances in
Large Margin Classifiers. MIT Press.
David H. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5:241–259.
Yiming Yang and Xin Liu. 1999. A re-examination of text
categorization methods. In SIGIR ’99.
Yiming Yang. 1999. An evaluation of statistical approaches to
text categorization. Information Retrieval, 1(1/2):67–88.
</reference>
<page confidence="0.998807">
331
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797460">
<title confidence="0.999937">Combining Probability-Based Rankers for Action-Item Detection</title>
<author confidence="0.999945">Jaime G Carbonell</author>
<affiliation confidence="0.956586">Language Technologies Institute, Carnegie</affiliation>
<address confidence="0.9882915">5000 Forbes Pittsburgh, PA</address>
<email confidence="0.999671">jgc@cs.cmu.edu</email>
<author confidence="0.981202">Paul N Bennett</author>
<address confidence="0.937884">One Microsoft Way Redmond, WA 98052</address>
<email confidence="0.99959">paul.n.bennett@microsoft.com</email>
<abstract confidence="0.99981904">This paper studies methods that automatdetect e-mail, an important category for assisting users in identifying new tasks, tracking ongoing ones, and searching for completed ones. Since action-items consist of a short span of text, classifiers that detect action-items can be built from a document-level or a sentence-level view. Rather than commit to either view, we adapt a contextsensitive metaclassification framework to problem to combine the produced by different algorithms as well as different views. While this framework is known to work well for standard classification, its suitability for fusing rankers has not been studied. In an empirical evaluation, the resulting approach yields improved rankings that are less sensitive to training set variation, and furthermore, the theoretically-motivated reliability indicators we introduce enable the metaclassifier to now be applicable in any problem where the base classifiers are used.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work was supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the</title>
<booktitle>Defense Advanced Research Projects Agency (DARPA), or the</booktitle>
<institution>Department of Interior-National Business Center (DOI-NBC).</institution>
<marker></marker>
<rawString>This work was supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of Interior-National Business Center (DOI-NBC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul N Bennett</author>
<author>Jaime Carbonell</author>
</authors>
<title>Feature representation for effective action-item detection.</title>
<date>2005</date>
<booktitle>In SIGIR ’05, Beyond Bag-of-Words Workshop.</booktitle>
<contexts>
<context position="6328" citStr="Bennett and Carbonell, 2005" startWordPosition="974" endWordPosition="977"> we also examine combining models that have different views, in that both the qualitative nature of the labeled data and the application of the learned base models differ. Furthermore, we improve upon work on context-sensitive combination by introducing reliability indicators which model the sensitivity of a classifier’s output around the current prediction point. Finally, we focus on the application of these methods to action-item data — a growing area of interest which has been demonstrated to behave differently than more standard text classification problems (e.g. topic) in the literature (Bennett and Carbonell, 2005). 2.1 Action-Item Detection There are three basic problems for action-item detection. (1) Document detection: Classify an e-mail as to whether or not it contains an action-item. (2) Document ranking: Rank the e-mails such that all e-mail containing action-items occur as high as possible in the ranking. (3) Sentence detection: Classify each sentence in an e-mail as to whether or not it is an action-item. Here we focus on the document ranking problem. Improving the overall ranking not only helps users find e-mails with action-items quicker (Bennett and Carbonell, 2005) but can decrease response </context>
<context position="17671" citStr="Bennett and Carbonell, 2005" startWordPosition="2875" endWordPosition="2879">volunteers at an educational institution and covers subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements. After eliminating duplicate e-mails, the corpus contains 744 messages with a total of 6301 automatically segmented sentences. A human panel labeled each phrase or sentence that contained an explicit request for information or action. 416 emails have no action-items and 328 e-mails contain action-items. Additional information such as annotator agreement, distribution of message length, etc. can be found in (Bennett and Carbonell, 2005). An anonymized corpus is available at http://www.cs.cmu.edu/˜pbennett/action-item-dataset.html. 3.2 Feature Representation We use two types of feature representation: a bagof-words representation which uses all unigram tokens as the feature pool; and a bag-of-n-grams where n includes all n-grams where n &lt; 4. For both representations at both the document-level and sentence-level, we used only the top 300 features by the chi-squared statistic. 3.3 Document-Level Classifiers kNN We used a s-cut variant of kNN common in text classification (Yang, 1999) and a tfidf-weighting of the terms with a di</context>
<context position="27608" citStr="Bennett and Carbonell, 2005" startWordPosition="4476" endWordPosition="4479"> see that the variance of STRIVE drops faster than the other classifiers as we move further along the x-axis. Thus, STRIVE’s ranking quality varies less with changes to the training set. 4 Related Work Several researchers have considered text classification tasks similar to action-item detection. Cohen et al. (2004) describe an ontology of “speech acts”, such as “Propose a Meeting”, and attempt to predict when an e-mail contains one of these speech acts. Corston-Oliver et al. (2004) consider detecting items in e-mail to “Put on a To-Do List” using a sentence-level classifier. In earlier work (Bennett and Carbonell, 2005), we demonstrated that sentence-level classifiers typically outperform document-level classifiers on this problem and examined the underlying reasons why this was 330 the case. Furthermore, we presented user studies demonstrating that users identify action-items more rapidly when using the system. In terms of classifier combination, a wide variety of work has been done in the arena. The STRIVE metaclassification approach (Bennett et al., 2005) extended Wolpert’s stacking framework (Wolpert, 1992) to use reliability indicators. In recent work, Lee et al. (2006) derive variance estimates for nai</context>
</contexts>
<marker>Bennett, Carbonell, 2005</marker>
<rawString>Paul N. Bennett and Jaime Carbonell. 2005. Feature representation for effective action-item detection. In SIGIR ’05, Beyond Bag-of-Words Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul N Bennett</author>
<author>Susan T Dumais</author>
<author>Eric Horvitz</author>
</authors>
<title>The combination of text classifiers using reliability indicators. Information Retrieval,</title>
<date>2005</date>
<pages>8--67</pages>
<contexts>
<context position="4323" citStr="Bennett et al., 2005" startWordPosition="664" endWordPosition="667">nce, or small passage — supervised input to a learning system can either come at the document-level where an e-mail is labeled yes/no as to whether it contains an action-item or at the sentence-level where each span that is an actionitem is explicitly identified. Then, a corresponding document-level classifier or aggregated predictions from a sentence-level classifier can be used to estimate the overall likelihood for the e-mail. Rather than commit to either view, we use a combination technique to capture the information each viewpoint has to offer on the current example. The STRIVE approach (Bennett et al., 2005) has been shown to provide robust combinations of heterogeneous models for standard topic classification by capturing areas of high and low reliability via the use of reliability indicators. However, using STRIVE in order to produce improved rankings has not been previously studied. Furthermore, while they introduce some reliability indicators that are general for text classification problems as well as ones specifically tied to naive Bayes models, they do not address other classification models. We introduce a series of reliability indicators connected to areas of high/low reliability in kNN,</context>
<context position="9135" citStr="Bennett et al., 2005" startWordPosition="1419" endWordPosition="1422">rovide both a 1-5 acceptance rating and a 1-5 confidence. The first of these is related to an estimate of class membership, P(“accept&amp;quot; |paper), but the second is closer to a measure of expertise or a self-assessment of the reviewer’s reliability on an example-by-example basis. Automatically deriving such self-assessments for classification algorithms is non-trivial. The Stacked Reliability Indicator Variable Ensemble framework, or STRIVE, demonstrates how to extend stacking by incorporating such self-assessments as a layer of reliability indicators and introduces a candidate set of functions (Bennett et al., 2005). The STRIVE architecture is depicted in Figure 2. From left to right: (1) a bag-of-words representation of the document is extracted and used by the base classifiers to predict class probabilities; (2) reliability indicator functions use the predicted probabilities and the features of the document to characterize whether this document falls within the “expertise” of the classifiers; (3) a metalevel classifier uses the base classifier predictions and the reliability indicators to make a more reliable combined prediction. From the perspective of improving action-item rankings, we are interested</context>
<context position="16434" citStr="Bennett et al. (2005)" startWordPosition="2684" endWordPosition="2687">r the multivariate Bernoulli naive Bayes (MBNB) model 1We assume that the minimum distance is not zero. If it is zero, then we return zero for all of the variables. 2As is standard to handle different document lengths, we take the distance between documents after they have been normalized to the unit sphere. w1 = 327 that models feature presence/absence, we use a distribution over all documents that has one presence/absence bit flipped from the prediction document. It is interesting to note that the variables from the naive Bayes models can be shown to be equivalent to variables introduced by Bennett et al. (2005) — although those were derived in a different fashion by analyzing the weight a single feature carries with respect to the overall prediction. Furthermore, from this starting point, we go on to define similar variables of possible interest. Including the two for each model described here, we define 10 kNN variables, 5 SVM variables, 2 decision-tree variables, 6 NB model based variables, and 6 MBNB variables. We describe these variables as well as implementation details and computational complexity results in (Bennett, 2006). 3 Experimental Analysis 3.1 Data Our corpus consists of e-mails obtai</context>
<context position="22736" citStr="Bennett et al. (2005)" startWordPosition="3683" endWordPosition="3686">sentence-level classifier offers when combining document predictions. Analogous to the sensitivity of each base model, we can consider such indicators as the mean and standard deviation of the classifier confidences across the sentences within a document. For each sentence-level base classifier, these become two more indicators which we can benefit from when combining document predictions. This introduces 20 more variables (20 = 2 representations * 2 * 5 classifiers). Finally, we include the 2 basic voting statistic reliability-indicators (PercentPredictingPositive and PercentAgreeWBest) that Bennett et al. (2005) found useful for topic classification. This yields a total of 138 reliability-indicators (138 = 58 + 20 + 58 + 2). With the 20 classifier outputs, there are a total of 158 input features for striving to handle. As with stacking, we use SVMlight with default settings and normalize the inputs to this classifier to have zero mean and a scaled variance. 3.7 Performance Measures We wish to improve the rankings of the e-mails in the inbox such that action-item e-mails occur higher in the inbox. Therefore, we use the area under the curve (AUC) of an ROC curve as a measure of ranking performance. AUC</context>
<context position="28055" citStr="Bennett et al., 2005" startWordPosition="4541" endWordPosition="4544">cts. Corston-Oliver et al. (2004) consider detecting items in e-mail to “Put on a To-Do List” using a sentence-level classifier. In earlier work (Bennett and Carbonell, 2005), we demonstrated that sentence-level classifiers typically outperform document-level classifiers on this problem and examined the underlying reasons why this was 330 the case. Furthermore, we presented user studies demonstrating that users identify action-items more rapidly when using the system. In terms of classifier combination, a wide variety of work has been done in the arena. The STRIVE metaclassification approach (Bennett et al., 2005) extended Wolpert’s stacking framework (Wolpert, 1992) to use reliability indicators. In recent work, Lee et al. (2006) derive variance estimates for naive Bayes and tree-augmented naive Bayes and use them in the combination model. Our work complements theirs by laying groundwork for how to compute variance estimates for models such as kNN that have no obvious probabilistic component. 5 Future Work and Conclusion While there are many interesting directions for future work, the most interesting is to directly integrate the sensitivity and calibration quantities derived into the more general mod</context>
</contexts>
<marker>Bennett, Dumais, Horvitz, 2005</marker>
<rawString>Paul N. Bennett, Susan T. Dumais, and Eric Horvitz. 2005. The combination of text classifiers using reliability indicators. Information Retrieval, 8:67–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul N Bennett</author>
</authors>
<title>Building Reliable Metaclassifiers for Text Learning.</title>
<date>2006</date>
<tech>Ph.D. thesis, CMU. CMU-CS-06-121.</tech>
<contexts>
<context position="16963" citStr="Bennett, 2006" startWordPosition="2770" endWordPosition="2771">models can be shown to be equivalent to variables introduced by Bennett et al. (2005) — although those were derived in a different fashion by analyzing the weight a single feature carries with respect to the overall prediction. Furthermore, from this starting point, we go on to define similar variables of possible interest. Including the two for each model described here, we define 10 kNN variables, 5 SVM variables, 2 decision-tree variables, 6 NB model based variables, and 6 MBNB variables. We describe these variables as well as implementation details and computational complexity results in (Bennett, 2006). 3 Experimental Analysis 3.1 Data Our corpus consists of e-mails obtained from volunteers at an educational institution and covers subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements. After eliminating duplicate e-mails, the corpus contains 744 messages with a total of 6301 automatically segmented sentences. A human panel labeled each phrase or sentence that contained an explicit request for information or action. 416 emails have no action-items and 328 e-mails contain action-items. Additional information suc</context>
</contexts>
<marker>Bennett, 2006</marker>
<rawString>Paul N. Bennett. 2006. Building Reliable Metaclassifiers for Text Learning. Ph.D. thesis, CMU. CMU-CS-06-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
</authors>
<title>High precision extraction of grammatical relations.</title>
<date>2002</date>
<booktitle>In COLING ’02.</booktitle>
<contexts>
<context position="19717" citStr="Carroll, 2002" startWordPosition="3193" endWordPosition="3194"> al., 1997; Heckerman et al., 2000). The estimated log-odds is computed from a Laplace correction to the empirical probability at a leaf node. Naive Bayes We use a multinomial naive Bayes (NB) and a multivariate Bernoulli naive Bayes classifier (MBNB) (McCallum and Nigam, 1998). For these classifiers, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. Since these are probabilistic, they issue log-odds estimates directly. 3.4 Sentence-Level Classifiers Each e-mail is automatically segmented into sentences using RASP (Carroll, 2002). Since the corpus has fine grained labels, we can train classifiers to classify a sentence. Each classifier in Section 3.3 is also used to learn a sentence classifier. However, we then must make a document-level prediction. In order to produce a ranking score, the confidence that the document contains an action-item is: � 1n(d) Es∈d|7r(s)=1 λ(s), IsEd|π(s) = 1 λ(d)_ n(d) maxs∈d �λ(s) 1o.w. 3This rule is not guaranteed be optimal for a particular value of N but is motivated by theoretical results which show such a rule converges to the optimal classifier as the number of training points increa</context>
</contexts>
<marker>Carroll, 2002</marker>
<rawString>John Carroll. 2002. High precision extraction of grammatical relations. In COLING ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Chickering</author>
<author>D Heckerman</author>
<author>C Meek</author>
</authors>
<title>A Bayesian approach to learning Bayesian networks with local structure.</title>
<date>1997</date>
<booktitle>In UAI ’97.</booktitle>
<contexts>
<context position="19113" citStr="Chickering et al., 1997" startWordPosition="3100" endWordPosition="3103">λkNN(x) = �cos(x, n) − cos(x, n). n∈kNN(x)|c(n)=action item n∈kNN(x)|c(n)6=action item SVM We used a linear SVM as implemented in the SVMlight package v6.01 (Joachims, 1999) with a tfidf feature representation and L2-norm. All default settings were used. SVM’s margin score, E αiyi K(xi, xj), has been shown to empirically behave like an uncalibrated log-odds estimate (Platt, 1999). Decision Trees For the decision-tree implementation, we used the WinMine toolkit and refer to this as Dnet below (Microsoft Corporation, 2001). Dnet builds decision trees using a Bayesian machine learning algorithm (Chickering et al., 1997; Heckerman et al., 2000). The estimated log-odds is computed from a Laplace correction to the empirical probability at a leaf node. Naive Bayes We use a multinomial naive Bayes (NB) and a multivariate Bernoulli naive Bayes classifier (MBNB) (McCallum and Nigam, 1998). For these classifiers, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. Since these are probabilistic, they issue log-odds estimates directly. 3.4 Sentence-Level Classifiers Each e-mail is automatically segmented into sentences using RASP (Carroll, 2</context>
</contexts>
<marker>Chickering, Heckerman, Meek, 1997</marker>
<rawString>D.M. Chickering, D. Heckerman, and C. Meek. 1997. A Bayesian approach to learning Bayesian networks with local structure. In UAI ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
<author>Tom M Mitchell</author>
</authors>
<title>Learning to classify email into “speech acts”.</title>
<date>2004</date>
<booktitle>In EMNLP ’04.</booktitle>
<contexts>
<context position="27297" citStr="Cohen et al. (2004)" startWordPosition="4424" endWordPosition="4428">es of striving, stacking, and two of the most competitive base classifiers in Figure 4. We see that striving loses by a slight amount to stacking early in the curve but still beats the base classifiers. Later in the curve, it dominates all the classifiers. If we examine the curves using error bars, we see that the variance of STRIVE drops faster than the other classifiers as we move further along the x-axis. Thus, STRIVE’s ranking quality varies less with changes to the training set. 4 Related Work Several researchers have considered text classification tasks similar to action-item detection. Cohen et al. (2004) describe an ontology of “speech acts”, such as “Propose a Meeting”, and attempt to predict when an e-mail contains one of these speech acts. Corston-Oliver et al. (2004) consider detecting items in e-mail to “Put on a To-Do List” using a sentence-level classifier. In earlier work (Bennett and Carbonell, 2005), we demonstrated that sentence-level classifiers typically outperform document-level classifiers on this problem and examined the underlying reasons why this was 330 the case. Furthermore, we presented user studies demonstrating that users identify action-items more rapidly when using th</context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell. 2004. Learning to classify email into “speech acts”. In EMNLP ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Eric Ringger</author>
<author>Michael Gamon</author>
<author>Richard Campbell</author>
</authors>
<title>Task-focused summarization of email.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL ’04 Workshop.</booktitle>
<contexts>
<context position="27467" citStr="Corston-Oliver et al. (2004)" startWordPosition="4453" endWordPosition="4456">rve but still beats the base classifiers. Later in the curve, it dominates all the classifiers. If we examine the curves using error bars, we see that the variance of STRIVE drops faster than the other classifiers as we move further along the x-axis. Thus, STRIVE’s ranking quality varies less with changes to the training set. 4 Related Work Several researchers have considered text classification tasks similar to action-item detection. Cohen et al. (2004) describe an ontology of “speech acts”, such as “Propose a Meeting”, and attempt to predict when an e-mail contains one of these speech acts. Corston-Oliver et al. (2004) consider detecting items in e-mail to “Put on a To-Do List” using a sentence-level classifier. In earlier work (Bennett and Carbonell, 2005), we demonstrated that sentence-level classifiers typically outperform document-level classifiers on this problem and examined the underlying reasons why this was 330 the case. Furthermore, we presented user studies demonstrating that users identify action-items more rapidly when using the system. In terms of classifier combination, a wide variety of work has been done in the arena. The STRIVE metaclassification approach (Bennett et al., 2005) extended Wo</context>
</contexts>
<marker>Corston-Oliver, Ringger, Gamon, Campbell, 2004</marker>
<rawString>Simon Corston-Oliver, Eric Ringger, Michael Gamon, and Richard Campbell. 2004. Task-focused summarization of email. In Text Summarization Branches Out: Proceedings of the ACL ’04 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morris H DeGroot</author>
<author>Stephen E Fienberg</author>
</authors>
<title>Comparing probability forecasters: Basic binary concepts and multivariate extensions.</title>
<date>1986</date>
<booktitle>Bayesian Inference and Decision Techniques.</booktitle>
<editor>In P. Goel and A. Zellner, editors,</editor>
<publisher>Elsevier.</publisher>
<contexts>
<context position="11023" citStr="DeGroot and Fienberg, 1986" startWordPosition="1708" endWordPosition="1711">t our choice of metaclassifier to a linear model. One natural choice is to rank the e-mails according to the estimated posterior probability, P�(class = action item |x), but in a linear combination framework it is actually more convenient to work with the estimated log-odds or logit transform which is monotone in the posterior, λ� = logPˆ(class=action item|x) 1− Pˆ (class=action item|x) (Kahn, 2004). Now, consider applying a metaclassifier to a single base classifier. Given only a classifier’s probability estimates, a metaclassifier cannot improve on the estimates if they are well-calibrated (DeGroot and Fienberg, 1986). Thus a metaclassifier applied to a single base classifier corresponds to recalibration (Kahn, 2004). Assume each of the n base models gives an uncalibrated log-odds estimate �i. Then the combination model would have the form k(x) = W0(x)+Eni=1 Wi(x)�i(x) where the Wi are example dependent weight functions that the combination model learns. The obvious implication is that our reliability indicators can be informed by the optimal values for the weighting functions. We can determine the optimal weights in a simplified case with a single base classifier by assuming we are given “true” log-odds v</context>
</contexts>
<marker>DeGroot, Fienberg, 1986</marker>
<rawString>Morris H. DeGroot and Stephen E. Fienberg. 1986. Comparing probability forecasters: Basic binary concepts and multivariate extensions. In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Devroye</author>
<author>L´aszl´o Gy¨orfi</author>
<author>G´abor Lugosi</author>
</authors>
<title>A Probabilistic Theory of Pattern Recognition.</title>
<date>1996</date>
<publisher>Springer-Verlag,</publisher>
<location>New York, NY.</location>
<marker>Devroye, Gy¨orfi, Lugosi, 1996</marker>
<rawString>Luc Devroye, L´aszl´o Gy¨orfi, and G´abor Lugosi. 1996. A Probabilistic Theory of Pattern Recognition. Springer-Verlag, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James A Hanley</author>
<author>Barbara J McNeil</author>
</authors>
<title>The meaning and use of the area under a recever operating characteristic (roc) curve.</title>
<date>1982</date>
<journal>Radiology,</journal>
<volume>143</volume>
<issue>1</issue>
<contexts>
<context position="23516" citStr="Hanley and McNeil, 1982" startWordPosition="3820" endWordPosition="3823">e a total of 158 input features for striving to handle. As with stacking, we use SVMlight with default settings and normalize the inputs to this classifier to have zero mean and a scaled variance. 3.7 Performance Measures We wish to improve the rankings of the e-mails in the inbox such that action-item e-mails occur higher in the inbox. Therefore, we use the area under the curve (AUC) of an ROC curve as a measure of ranking performance. AUC is a measure of overall model and ranking quality that has gained wider adoption recently and is equivalent to the Mann-WhitneyWilcoxon sum of ranks test (Hanley and McNeil, 1982). To put improvement in perspective, we can write our relative reduction in residual area (RRA) as 1_AUCUC . We present gains relative to the basel- e best AUC performer (bRRA), and relative to perfect dynamic selection performance, (dRRA), which assumes we could accurately dynamically choose the best classifier per cross-validation run. The F1 measure is the harmonic mean of precision and recall and is common throughout text classification (Yang and Liu, 1999). Although we are not concerned with F1 performance here, some users of the system might be interested in improving ranking while havin</context>
</contexts>
<marker>Hanley, McNeil, 1982</marker>
<rawString>James A. Hanley and Barbara J. McNeil. 1982. The meaning and use of the area under a recever operating characteristic (roc) curve. Radiology, 143(1):29–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Heckerman</author>
<author>D M Chickering</author>
<author>C Meek</author>
<author>R Rounthwaite</author>
<author>C Kadie</author>
</authors>
<title>Dependency networks for inference, collaborative filtering, and data visualization.</title>
<date>2000</date>
<journal>JMLR,</journal>
<volume>1</volume>
<pages>75</pages>
<contexts>
<context position="19138" citStr="Heckerman et al., 2000" startWordPosition="3104" endWordPosition="3107">s(x, n). n∈kNN(x)|c(n)=action item n∈kNN(x)|c(n)6=action item SVM We used a linear SVM as implemented in the SVMlight package v6.01 (Joachims, 1999) with a tfidf feature representation and L2-norm. All default settings were used. SVM’s margin score, E αiyi K(xi, xj), has been shown to empirically behave like an uncalibrated log-odds estimate (Platt, 1999). Decision Trees For the decision-tree implementation, we used the WinMine toolkit and refer to this as Dnet below (Microsoft Corporation, 2001). Dnet builds decision trees using a Bayesian machine learning algorithm (Chickering et al., 1997; Heckerman et al., 2000). The estimated log-odds is computed from a Laplace correction to the empirical probability at a leaf node. Naive Bayes We use a multinomial naive Bayes (NB) and a multivariate Bernoulli naive Bayes classifier (MBNB) (McCallum and Nigam, 1998). For these classifiers, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. Since these are probabilistic, they issue log-odds estimates directly. 3.4 Sentence-Level Classifiers Each e-mail is automatically segmented into sentences using RASP (Carroll, 2002). Since the corpus ha</context>
</contexts>
<marker>Heckerman, Chickering, Meek, Rounthwaite, Kadie, 2000</marker>
<rawString>D. Heckerman, D.M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. 2000. Dependency networks for inference, collaborative filtering, and data visualization. JMLR, 1:49– 75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods -Support Vector Learning.</booktitle>
<editor>In Bernhard Sch¨olkopf, Christopher J. Burges, and Alexander J. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="18663" citStr="Joachims, 1999" startWordPosition="3033" endWordPosition="3034">evel, we used only the top 300 features by the chi-squared statistic. 3.3 Document-Level Classifiers kNN We used a s-cut variant of kNN common in text classification (Yang, 1999) and a tfidf-weighting of the terms with a distance-weighted vote of the neighbors to compute the output. k was set to be 2(Flo92 N] + 1) where N is the number of training points. 3 The score used as the uncalibrated logodds estimate of being an action-item is: ��λkNN(x) = �cos(x, n) − cos(x, n). n∈kNN(x)|c(n)=action item n∈kNN(x)|c(n)6=action item SVM We used a linear SVM as implemented in the SVMlight package v6.01 (Joachims, 1999) with a tfidf feature representation and L2-norm. All default settings were used. SVM’s margin score, E αiyi K(xi, xj), has been shown to empirically behave like an uncalibrated log-odds estimate (Platt, 1999). Decision Trees For the decision-tree implementation, we used the WinMine toolkit and refer to this as Dnet below (Microsoft Corporation, 2001). Dnet builds decision trees using a Bayesian machine learning algorithm (Chickering et al., 1997; Heckerman et al., 2000). The estimated log-odds is computed from a Laplace correction to the empirical probability at a leaf node. Naive Bayes We us</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale svm learning practical. In Bernhard Sch¨olkopf, Christopher J. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods -Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph M Kahn</author>
</authors>
<title>Bayesian Aggregation of Probability Forecasts on Categorical Events.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University,</institution>
<contexts>
<context position="10798" citStr="Kahn, 2004" startWordPosition="1677" endWordPosition="1678">as been shown to provide robust combination for topic classification, a formal motivation is lacking for the type of reliability indicators that are the most useful in classifier combination. Assume we restrict our choice of metaclassifier to a linear model. One natural choice is to rank the e-mails according to the estimated posterior probability, P�(class = action item |x), but in a linear combination framework it is actually more convenient to work with the estimated log-odds or logit transform which is monotone in the posterior, λ� = logPˆ(class=action item|x) 1− Pˆ (class=action item|x) (Kahn, 2004). Now, consider applying a metaclassifier to a single base classifier. Given only a classifier’s probability estimates, a metaclassifier cannot improve on the estimates if they are well-calibrated (DeGroot and Fienberg, 1986). Thus a metaclassifier applied to a single base classifier corresponds to recalibration (Kahn, 2004). Assume each of the n base models gives an uncalibrated log-odds estimate �i. Then the combination model would have the form k(x) = W0(x)+Eni=1 Wi(x)�i(x) where the Wi are example dependent weight functions that the combination model learns. The obvious implication is that</context>
</contexts>
<marker>Kahn, 2004</marker>
<rawString>Joseph M. Kahn. 2004. Bayesian Aggregation of Probability Forecasts on Categorical Events. Ph.D. thesis, Stanford University, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Hoon Lee</author>
<author>Russ Greiner</author>
<author>Shaojun Wang</author>
</authors>
<title>Using query-specific variance estimates to combine bayesian classifiers.</title>
<date>2006</date>
<booktitle>In ICML ’06.</booktitle>
<contexts>
<context position="28174" citStr="Lee et al. (2006)" startWordPosition="4558" endWordPosition="4561">fier. In earlier work (Bennett and Carbonell, 2005), we demonstrated that sentence-level classifiers typically outperform document-level classifiers on this problem and examined the underlying reasons why this was 330 the case. Furthermore, we presented user studies demonstrating that users identify action-items more rapidly when using the system. In terms of classifier combination, a wide variety of work has been done in the arena. The STRIVE metaclassification approach (Bennett et al., 2005) extended Wolpert’s stacking framework (Wolpert, 1992) to use reliability indicators. In recent work, Lee et al. (2006) derive variance estimates for naive Bayes and tree-augmented naive Bayes and use them in the combination model. Our work complements theirs by laying groundwork for how to compute variance estimates for models such as kNN that have no obvious probabilistic component. 5 Future Work and Conclusion While there are many interesting directions for future work, the most interesting is to directly integrate the sensitivity and calibration quantities derived into the more general model discussed in Section 2.3. In this paper, we took an existing approach to context-dependent combination, STRIVE, that</context>
</contexts>
<marker>Lee, Greiner, Wang, 2006</marker>
<rawString>Chi-Hoon Lee, Russ Greiner, and Shaojun Wang. 2006. Using query-specific variance estimates to combine bayesian classifiers. In ICML ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classification.</title>
<date>1998</date>
<booktitle>In AAAI ’98, Workshops. TR</booktitle>
<pages>98--05</pages>
<contexts>
<context position="19381" citStr="McCallum and Nigam, 1998" startWordPosition="3143" endWordPosition="3146">n score, E αiyi K(xi, xj), has been shown to empirically behave like an uncalibrated log-odds estimate (Platt, 1999). Decision Trees For the decision-tree implementation, we used the WinMine toolkit and refer to this as Dnet below (Microsoft Corporation, 2001). Dnet builds decision trees using a Bayesian machine learning algorithm (Chickering et al., 1997; Heckerman et al., 2000). The estimated log-odds is computed from a Laplace correction to the empirical probability at a leaf node. Naive Bayes We use a multinomial naive Bayes (NB) and a multivariate Bernoulli naive Bayes classifier (MBNB) (McCallum and Nigam, 1998). For these classifiers, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. Since these are probabilistic, they issue log-odds estimates directly. 3.4 Sentence-Level Classifiers Each e-mail is automatically segmented into sentences using RASP (Carroll, 2002). Since the corpus has fine grained labels, we can train classifiers to classify a sentence. Each classifier in Section 3.3 is also used to learn a sentence classifier. However, we then must make a document-level prediction. In order to produce a ranking score, th</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for naive bayes text classification. In AAAI ’98, Workshops. TR WS-98-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Microsoft Corporation</author>
</authors>
<title>WinMine Toolkit v1.0.</title>
<date>2001</date>
<note>http://research.microsoft.com/ ˜dmax/WinMine/ContactInfo.html.</note>
<contexts>
<context position="19016" citStr="Corporation, 2001" startWordPosition="3088" endWordPosition="3089">points. 3 The score used as the uncalibrated logodds estimate of being an action-item is: ��λkNN(x) = �cos(x, n) − cos(x, n). n∈kNN(x)|c(n)=action item n∈kNN(x)|c(n)6=action item SVM We used a linear SVM as implemented in the SVMlight package v6.01 (Joachims, 1999) with a tfidf feature representation and L2-norm. All default settings were used. SVM’s margin score, E αiyi K(xi, xj), has been shown to empirically behave like an uncalibrated log-odds estimate (Platt, 1999). Decision Trees For the decision-tree implementation, we used the WinMine toolkit and refer to this as Dnet below (Microsoft Corporation, 2001). Dnet builds decision trees using a Bayesian machine learning algorithm (Chickering et al., 1997; Heckerman et al., 2000). The estimated log-odds is computed from a Laplace correction to the empirical probability at a leaf node. Naive Bayes We use a multinomial naive Bayes (NB) and a multivariate Bernoulli naive Bayes classifier (MBNB) (McCallum and Nigam, 1998). For these classifiers, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. Since these are probabilistic, they issue log-odds estimates directly. 3.4 Senten</context>
</contexts>
<marker>Corporation, 2001</marker>
<rawString>Microsoft Corporation. 2001. WinMine Toolkit v1.0. http://research.microsoft.com/ ˜dmax/WinMine/ContactInfo.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classifiers.</booktitle>
<editor>In Alexander J. Smola, Peter Bartlett, Bernhard Scholkopf, and Dale Schuurmans, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="18872" citStr="Platt, 1999" startWordPosition="3066" endWordPosition="3067">s with a distance-weighted vote of the neighbors to compute the output. k was set to be 2(Flo92 N] + 1) where N is the number of training points. 3 The score used as the uncalibrated logodds estimate of being an action-item is: ��λkNN(x) = �cos(x, n) − cos(x, n). n∈kNN(x)|c(n)=action item n∈kNN(x)|c(n)6=action item SVM We used a linear SVM as implemented in the SVMlight package v6.01 (Joachims, 1999) with a tfidf feature representation and L2-norm. All default settings were used. SVM’s margin score, E αiyi K(xi, xj), has been shown to empirically behave like an uncalibrated log-odds estimate (Platt, 1999). Decision Trees For the decision-tree implementation, we used the WinMine toolkit and refer to this as Dnet below (Microsoft Corporation, 2001). Dnet builds decision trees using a Bayesian machine learning algorithm (Chickering et al., 1997; Heckerman et al., 2000). The estimated log-odds is computed from a Laplace correction to the empirical probability at a leaf node. Naive Bayes We use a multinomial naive Bayes (NB) and a multivariate Bernoulli naive Bayes classifier (MBNB) (McCallum and Nigam, 1998). For these classifiers, we smoothed word and class probabilities using a Bayesian estimate</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Alexander J. Smola, Peter Bartlett, Bernhard Scholkopf, and Dale Schuurmans, editors, Advances in Large Margin Classifiers. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural Networks,</journal>
<pages>5--241</pages>
<contexts>
<context position="7420" citStr="Wolpert, 1992" startWordPosition="1151" endWordPosition="1152">nking not only helps users find e-mails with action-items quicker (Bennett and Carbonell, 2005) but can decrease response times and help ensure that key e-mails are not overlooked. Since a typical user will eventually process all received mail, we assume that producing a quality ranking will more directly measure the impact on the user than accuracy or F1. Therefore, we focus on ROC curves and area under the curve (AUC) since both reflect the quality of the ranking produced. 2.2 Combining Classifiers with Metaclassifiers One of the most common approaches to classifier combination is stacking (Wolpert, 1992). In this approach, a metaclassifier observes a past history of classifier predictions to learn how to weight the classifiers according to their demonstrated accuracies and interactions. To build the history, cross-validation over the training set is used to obtain predictions from each base classifier. Next, a metalevel representation of the training set is constructed where each example consists of the class label and the predictions of the base classifiers. Finally, a metaclassifier is trained on the metalevel representation to learn a model of how to combine the base classifiers. However, </context>
<context position="28109" citStr="Wolpert, 1992" startWordPosition="4549" endWordPosition="4550"> e-mail to “Put on a To-Do List” using a sentence-level classifier. In earlier work (Bennett and Carbonell, 2005), we demonstrated that sentence-level classifiers typically outperform document-level classifiers on this problem and examined the underlying reasons why this was 330 the case. Furthermore, we presented user studies demonstrating that users identify action-items more rapidly when using the system. In terms of classifier combination, a wide variety of work has been done in the arena. The STRIVE metaclassification approach (Bennett et al., 2005) extended Wolpert’s stacking framework (Wolpert, 1992) to use reliability indicators. In recent work, Lee et al. (2006) derive variance estimates for naive Bayes and tree-augmented naive Bayes and use them in the combination model. Our work complements theirs by laying groundwork for how to compute variance estimates for models such as kNN that have no obvious probabilistic component. 5 Future Work and Conclusion While there are many interesting directions for future work, the most interesting is to directly integrate the sensitivity and calibration quantities derived into the more general model discussed in Section 2.3. In this paper, we took an</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Stacked generalization. Neural Networks, 5:241–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In SIGIR ’99.</booktitle>
<contexts>
<context position="23981" citStr="Yang and Liu, 1999" startWordPosition="3895" endWordPosition="3898">odel and ranking quality that has gained wider adoption recently and is equivalent to the Mann-WhitneyWilcoxon sum of ranks test (Hanley and McNeil, 1982). To put improvement in perspective, we can write our relative reduction in residual area (RRA) as 1_AUCUC . We present gains relative to the basel- e best AUC performer (bRRA), and relative to perfect dynamic selection performance, (dRRA), which assumes we could accurately dynamically choose the best classifier per cross-validation run. The F1 measure is the harmonic mean of precision and recall and is common throughout text classification (Yang and Liu, 1999). Although we are not concerned with F1 performance here, some users of the system might be interested in improving ranking while having negligible negative effect on F1. Therefore, we examine F1 to ensure that an improvement in ranking will not come at the cost of a statistically significant decrease in F1. 3.8 Experimental Methodology To evaluate performance of the combination systems, we perform 10-fold cross-validation and compute the average performance. For significance tests, we use a two-tailed t-test (Yang and Liu, 1999) to compare the values obtained during each crossvalidation fold </context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. In SIGIR ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
</authors>
<title>An evaluation of statistical approaches to text categorization. Information Retrieval,</title>
<date>1999</date>
<pages>1--1</pages>
<contexts>
<context position="18226" citStr="Yang, 1999" startWordPosition="2958" endWordPosition="2959">ngth, etc. can be found in (Bennett and Carbonell, 2005). An anonymized corpus is available at http://www.cs.cmu.edu/˜pbennett/action-item-dataset.html. 3.2 Feature Representation We use two types of feature representation: a bagof-words representation which uses all unigram tokens as the feature pool; and a bag-of-n-grams where n includes all n-grams where n &lt; 4. For both representations at both the document-level and sentence-level, we used only the top 300 features by the chi-squared statistic. 3.3 Document-Level Classifiers kNN We used a s-cut variant of kNN common in text classification (Yang, 1999) and a tfidf-weighting of the terms with a distance-weighted vote of the neighbors to compute the output. k was set to be 2(Flo92 N] + 1) where N is the number of training points. 3 The score used as the uncalibrated logodds estimate of being an action-item is: ��λkNN(x) = �cos(x, n) − cos(x, n). n∈kNN(x)|c(n)=action item n∈kNN(x)|c(n)6=action item SVM We used a linear SVM as implemented in the SVMlight package v6.01 (Joachims, 1999) with a tfidf feature representation and L2-norm. All default settings were used. SVM’s margin score, E αiyi K(xi, xj), has been shown to empirically behave like a</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yiming Yang. 1999. An evaluation of statistical approaches to text categorization. Information Retrieval, 1(1/2):67–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>