<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003220">
<title confidence="0.997814">
A Shallow Discourse Parsing System Based On Maximum Entropy Model
</title>
<author confidence="0.996052">
Jia Sun, Peijia Li, Weiqun Xu, Yonghong Yan
</author>
<affiliation confidence="0.9900065">
The Key Laboratory of Speech Acoustics and Content Understanding
Institute of Acoustics, Chinese Academy of Sciences
</affiliation>
<address confidence="0.859318">
No. 21 North 4th Ring West Road, Haidian District, 100190 Beijing, China
</address>
<email confidence="0.998902">
{sunjia,lipeijia,xuweiqun,yanyonghong}@hccl.ioa.ac.cn
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981363636364">
This paper describes our system for Shal-
low Discourse Parsing - the CoNLL 2015
Shared Task. We regard this as a classi-
fication task and build a cascaded system
based on Maximum Entropy to identify
the discourse connective, the spans of two
arguments and the sense of the discourse
connective. We trained the cascaded mod-
els with a variety of features such as lexi-
cal and syntactic features. We also report
the results achieved by our team.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963666666667">
Discourse parsing is one of the most challenging
tasks in natural language processing (NLP) field.
It focuses on parsing the structure of a piece of
text into a set of discourse relations between in-
ter sentences. There is considerable interest in
discourse parsing, both as an end in itself and as
an intermediate step in a variety of NLP appli-
cations like question answering (Verberne et al.,
2007), text summarization (Louis et al., 2010),
sentiment analysis and opinion mining (Somasun-
daran, 2010).
There are many approaches working on identi-
fying the discourse relations and data-driven ap-
proaches are dominated. A number of pioneers
take the discourse relations identification as a clas-
sification task (Marcu and Echihabi, 2002; Pitler
et al., 2009; Duverle and Prendinger, 2009) by the
construction of features like lexical, syntactic and
constituent features. Some take the argument seg-
mentation task as a semantic role detection task
(Wellner and Pustejovsky, 2007) and a sequence
labeling task (Ghosh et al., 2011). However, some
of the previous research is based on different cor-
pus, lacking an common evaluation data set. This
has been addressed with the release of Penn Dis-
course Treebank (PDTB) 2.0 corpus (Prasad et al.,
2008) which provides detailed annotations about
the discourse relations and argument spans ad-
dresses this problem. Besides, much research
about discourse parsing working on the PDTB ap-
pears (Prasad et al., 2010; Lin et al., 2009) and
they put more attention on the “harder” part - la-
belling the arguments. Lin (Lin et al., 2014) de-
signed an end-to-end discourse parser with the
PDTB including the explicit, implicit sense and
the argument spans identification.
Shallow Discourse Parsing (Xue et al., 2015)
is the CoNLL shared task this year1 which takes
a piece of newswire text as input and returns all
the discourse relations in the form of a discourse
connective (explicit or implicit) taking two argu-
ments (which can be clauses, sentences, or multi-
sentence segments) in JSON format. A relation
will be parsed as correct if the explicit discourse
connective (e.g., “because”, “however”) once it
has, the spans of text that serve as the two argu-
ments for each discourse connective and the sense
(e.g., “Comparison”) are all correct. The F1 score
of the parser’s performance is the evaluation met-
ric.
In this paper, we describe our system details in
Section 2, the evaluation result and subsequent ex-
periments in Section 3. Finally, we draw some
conclusions in Section 4.
</bodyText>
<sectionHeader confidence="0.90259" genericHeader="method">
2 Our System
</sectionHeader>
<subsectionHeader confidence="0.863311">
2.1 Resources
</subsectionHeader>
<bodyText confidence="0.999385625">
The resources used in our system are as follows:
Labeled training and development data: The
training and development (dev) data is derived
from the PDTB 2.0 Section 2-21 and Section 22
in JSON format . There are 32535 relations and
1436 relations annotated in the training data and
the dev data respectively. Table 1 shows the dis-
tribution of the four types in the data. There are
</bodyText>
<footnote confidence="0.997737">
1http://www.cs.brandeis.edu/ clp/conll15st/
</footnote>
<page confidence="0.982895">
84
</page>
<note confidence="0.913749">
Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 84–88,
Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.998314166666667">
Type Train data Dev data
Explicit 14,727 680
Implicit 13,163 522
EntRel 4,133 215
AltLex 524 19
all 32,535 1,436
</table>
<tableCaption confidence="0.9828205">
Table 1: Distribution of the four discourse relation
types in the data sets.
</tableCaption>
<table confidence="0.9998695625">
Sense level 1 Sense level 2or3 Train Dev
A.P 1,277 78
Temporal A.S 1,014 55
Synchrony 499 100
Cause.Reason 3,344 147
Contingency Cause.Result 2,137 81
Condition 1,197 52
Comparison Contrast 4,714 257
Concession 1,293 17
Conjunction 7,817 310
Instantiation 1,403 58
Restatement 2,699 110
Expansion Alternative 210 6
A.C 241 6
Exception 15 0
EntRel 4,133 215
</table>
<tableCaption confidence="0.958088">
Table 2: Distribution of the 15 senses from the
</tableCaption>
<bodyText confidence="0.96784365">
different data sets. A.P, A.S, A.C are the abbre-
viations of “Asynchronous.Precedence”, “Asyn-
chronous.Succession”, “Alternative.Chosen alter-
native” respectively .
15 valid senses including the second-level “types”
as well as a selected number of third-level “sub-
types”. Table 2 shows the distribution of the 15
senses in the data.
Test data: There are two test data sets. One
is the blind set which contains 20,000 to 30,000
words of newswire text annotated following the
PDTB annotation guidelines. The other test set is
Section 23 of the PDTB which is used for compar-
ison with previous work.
The connectives list: A list contains 100 dis-
course connectives in the PDTB and three syntac-
tic categories form (Knott, 1996).
Opennlp-maxent: We used the open source
package Opennlp-maxent2 to construct the classi-
fication models.
</bodyText>
<footnote confidence="0.941296">
2http://sourceforge.net/projects/maxent/files/
</footnote>
<figureCaption confidence="0.999857">
Figure 1: The structure of the system.
</figureCaption>
<subsectionHeader confidence="0.998096">
2.2 System overview and Features
</subsectionHeader>
<bodyText confidence="0.999943428571428">
Our system mainly follows the work of (Lin et
al., 2014), which consists of two parts: the ex-
plicit relation parser and the non-explicit relation
parser. The explicit relation parser is composed
of the connective classifier, the argument posi-
tion classifier, the argument extractor and the ex-
plicit sense classifier while the non-Explicit rela-
tion parser contains the AltLex classifier and the
implicit classifier. The structure of our system is
shown in Figure 1.
The set of features used in our system are listed
in Table 3. All the features fall into four classes:
lexical features, part-of-speech (POS) features,
syntactic features and positional features.
</bodyText>
<listItem confidence="0.981556333333333">
• Lexical features: The lexical features (F1-
F10) contain the connectives C, their contex-
tual words and word-pair features (i.e., F7
(wi,wj) where wi is a word from Arg1 and
wj is a word from Arg2) .
• POS features: F11-F17 belong to the POS
features .
• Syntactic features: The syntactic features
(F18-F26) include the connectives’ syntactic
category (F18): subordinating, coordinating,
or discourse adverbial, the path of syntactic
trees (F19, F20, F23), the number of siblings
</listItem>
<page confidence="0.997168">
85
</page>
<figureCaption confidence="0.974095566666667">
Feature Description
F1. C string
F2. the first word before C
F3. the second word before C
F4. F2 + C
F5. F3 + C
F6. C + the next word after C
F7. word-pair
F8. the first word of Arg2
F9. the second word of Arg2
F10. the third word of Arg2
F11. the POS of C
F12. the POS of F2
F13. F11 + F12
F14. the POS of the word after C
F15. F11 + F14
F16. the POS of F3
F17. F11 + F16
F18. the syntactic category of C
F19. the path of C’s parent to root
F20. the compressed path of F19
F21. the number of left siblings of C
F22. the number of right siblings of C
the path of C’s parent to N
whether the C’s left sibling number
is greater than 1
the constituent rules
the dependency rules
the relative position of N to C
the position of C in the sentence
</figureCaption>
<bodyText confidence="0.8614678">
Table 3: The features used in our system. “C” de-
notes the connectives. N means a current node in
the constituent tree used in Section 2.3.2.
(F21, F22, F24), constituent rules (F25) and
dependency rules (F26).
</bodyText>
<listItem confidence="0.921583">
• Position features: F27 is the relative posi-
tion in the syntactic tree structure (left, mid-
dle or right), while F28 is the connectives’
positions in the sentence (start, middle or
end).
</listItem>
<subsectionHeader confidence="0.855837">
2.3 Training
2.3.1 The Connective Classifier
</subsectionHeader>
<bodyText confidence="0.999976625">
All the 100 connectives that appeared in one dis-
course were extracted whether it functioned as a
connective or not. We converted all upper case let-
ters in connective to lower case ones.
The connective classifier decides whether a con-
nective is functioned as a discourse connective.
The features used were F4, F6, F11-F15, F19-F20
in Table 3.
</bodyText>
<subsectionHeader confidence="0.594695">
2.3.2 The Argument Labeller
</subsectionHeader>
<bodyText confidence="0.935318777777778">
Once the connective is identified, the argument la-
beller identifies the Arg1 and Arg2 spans of this
instance. This is accomplished in two steps: (1)
Classifying the locations of Arg1 by the Argument
Position Classifier. (2) Labelling the spans of Arg1
and Arg2 by the Argument Extrator.
The Argument Position Classifier: Normally
Arg2 immediately follows the connective while
the position of Arg1 is uncertain. In this model,
we classified the Arg1’s locations into two classes:
Arg1 was located within the same sentence of the
connective (SS) or in the previous sentence of con-
nective (PS) (Prasad et al., 2008).
We implemented this as a binary classification
task. In this step, features F1-F5, F11, F13, F16-
F17, F28 in Table 3 were adopted to train the
model. After the position label of Arg1 was de-
termined, the result was passed to the argument
extractor.
The Argument Extractor: In this module, our
classifier labelled the previous sentence as Arg1
immediately for the PS case. The argument spans
for the SS case were extracted described as below.
• Classify each internal node N in the con-
stituent tree as Arg1-node, Arg2-node, or
None with features F1, F18, F21-F24, F27 in
Table 3.
</bodyText>
<listItem confidence="0.999831384615385">
• Label a node as Arg1-node once its Arg1-
node predicted probability is greater than 0.1
(which is tuned on the dev data set).
• Select only one Arg1-node and one Arg2-
node in one instance with the maximal prob-
ability of the respective label.
• Extract the Arg1 and Arg2 spans by tree sub-
traction. If the Arg1 node is the ancestor
of the Arg2 node, the span of Arg1 should
be subtracted from the Arg2 span, and vice
versa.
• Remove punctuation tokens and connectives
out of the exact argument spans.
</listItem>
<subsubsectionHeader confidence="0.711081">
2.3.3 The Explicit Sense Classifier
</subsubsectionHeader>
<bodyText confidence="0.9966425">
After recognizing the discourse connective and its
two arguments spans, the next step is to decide the
</bodyText>
<page confidence="0.994823">
86
</page>
<table confidence="0.999463">
Data Connective Span Sense Parser
Dev 0.9152 0.2668 0.1367 0.1814
Test 0.9064 0.2336 0.1191 0.1505
Blind 0.826 0.2195 0.1232 0.1262
</table>
<tableCaption confidence="0.998746">
Table 4: The results F1 score obtained by our team
</tableCaption>
<bodyText confidence="0.9922356">
sense of the connective. We trained this model us-
ing features F1, F4, F11 in Table 3. We picked the
output whose maximal sense probability is greater
than 0.45 which was experientially determined on
dev data set.
</bodyText>
<subsectionHeader confidence="0.803908">
2.3.4 The AltLex classifier
</subsectionHeader>
<bodyText confidence="0.99999575">
We extracted all adjacent sentence pairs within
each paragraph and removed the pairs that were
identified by the explicit relation parser. Then
we trained the AltLex Classifier which decided
whether the pairs were AltLex pairs and classified
the senses with features F8-F10 in Table 3. The
pairs labelled as non-AltLex relations were passed
to the next implicit relation classifier.
</bodyText>
<subsectionHeader confidence="0.552976">
2.3.5 The Implicit relation classifier
</subsectionHeader>
<bodyText confidence="0.999988857142857">
The implicit relation classifier classified the sense
of each pair into one of the 15 valid senses or
NoRel with F7, F25-F26 in Table 3. After predict-
ing, we kept the implicit discourse relations whose
maximal sense probability were greater than a
threshold (0.25 in our case) which was determined
on the dev data set .
</bodyText>
<sectionHeader confidence="0.996463" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9992868125">
There are two test data sets this year as described
in Section 2.1 and the organizers reported the re-
sults on the two test data sets and the dev data set.
The results of our system obtained are shown in
Table 4. We ranked the 10th on every data set.
After the deadline of evaluation, we made some
improvements in the module of implicit relation
classifier inspired by (Lin et al., 2009). We se-
lected the word-pair features (F7) while the ex-
periments showed a little degradation in F1 score
through selecting the constituent rules and the de-
pendency rules (F25, F26) on the dev data set.
We computed the mutual information between
each word-pair feature and the 15 valid senses and
then selected the top N as the features. Table 5
shows the improvement of different N.
</bodyText>
<table confidence="0.999387666666667">
N Non-Explicit Overall
50 0.0683 0.1876
100 0.0683 0.1876
200 0.0614 0.1870
300 0.0603 0.1870
Baseline 0.0435 0.1814
</table>
<tableCaption confidence="0.682433333333333">
Table 5: The F1 score in non-explict and overall
parser when selecting features F7 using different
N on dev data set.
</tableCaption>
<sectionHeader confidence="0.998959" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999993636363636">
We divided the complex task of discourse pars-
ing into a set of classification subtasks and glued
them together. A variety of features, including lex-
ical, part-of-speech, syntactic and positional fea-
ture were employed to train the baseline with open
Maximum Entropy package, then the system was
improved by setting probability-output threshold.
We did not utilize any additional resources and
only used the annotations the official provided.
Our system ranked the 10th among seventeenth
teams on the two test data sets.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.988880818181818">
We would like to thank the shared task orga-
nizers for their support throughout this work.
This work is partially supported by the Na-
tional Natural Science Foundation of China
(Nos. 11161140319, 91120001, 61271426),
the Strategic Priority Research Program of the
Chinese Academy of Sciences (Grant Nos.
XDA06030100, XDA06030500), the National
863 Program (No. 2012AA012503) and the CAS
Priority Deployment Project (No. KGZD-EW-
103-2).
</bodyText>
<sectionHeader confidence="0.997146" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.930343384615384">
David A Duverle and Helmut Prendinger. 2009. A
novel discourse parser based on support vector ma-
chine classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 665–673. Association for Compu-
tational Linguistics.
Sucheta Ghosh, Richard Johansson, and Sara Tonelli.
2011. Shallow discourse parsing with conditional
random fields. In In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2011).
</reference>
<page confidence="0.995637">
87
</page>
<reference confidence="0.983902">
Alistair Knott. 1996. A data-driven methodology for
motivating a set of coherence relations.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 343–351.
Association for Computational Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A pdtb-styled end-to-end discourse parser. Natural
Language Engineering, 20(2):151–184.
Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Di-
alogue, pages 147–156. Association for Computa-
tional Linguistics.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 368–375. Association for Computational Lin-
guistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 683–691. Association for Computational
Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K. Joshi, and Bon-
nie L. Webber. 2008. The penn discourse treebank
2.0. In Proceedings of the International Confer-
ence on Language Resources and Evaluation, LREC
2008, 26 May - 1 June 2008, Marrakech, Morocco.
Rashmi Prasad, Aravind K. Joshi, and Bonnie L. Web-
ber. 2010. Exploiting scope for shallow discourse
parsing. In Proceedings of the International Confer-
ence on Language Resources and Evaluation, LREC
2010, 17-23 May 2010, Valletta, Malta.
Swapna Somasundaran. 2010. Discourse-level rela-
tions for Opinion Analysis. Ph.D. thesis, University
of Pittsburgh.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2007. Evaluating discourse-
based answer extraction for why-question answer-
ing. In Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 735–736. ACM.
Ben Wellner and James Pustejovsky. 2007. Automat-
ically identifying the arguments of discourse con-
nectives. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Coumptational Natural Language
Learning (EMNLP- CoNLL), pages 92–101.
Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi
Prasad, Christopher Bryant, and Attapol Ruther-
ford. 2015. The CoNLL-2015 shared task on shal-
low discourse parsing. In Proceedings of the Nine-
teenth Conference on Computational Natural Lan-
guage Learning.
</reference>
<page confidence="0.999404">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.361060">
<title confidence="0.991103">A Shallow Discourse Parsing System Based On Maximum Entropy Model</title>
<author confidence="0.946461">Jia Sun</author>
<author confidence="0.946461">Peijia Li</author>
<author confidence="0.946461">Weiqun Xu</author>
<author confidence="0.946461">Yonghong</author>
<affiliation confidence="0.604218">The Key Laboratory of Speech Acoustics and Content Institute of Acoustics, Chinese Academy of</affiliation>
<address confidence="0.397116">No. 21 North 4th Ring West Road, Haidian District, 100190 Beijing,</address>
<abstract confidence="0.99879175">This paper describes our system for Shallow Discourse Parsing the CoNLL 2015 Shared Task. We regard this as a classification task and build a cascaded system based on Maximum Entropy to identify the discourse connective, the spans of two arguments and the sense of the discourse connective. We trained the cascaded models with a variety of features such as lexical and syntactic features. We also report the results achieved by our team.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David A Duverle</author>
<author>Helmut Prendinger</author>
</authors>
<title>A novel discourse parser based on support vector machine classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>665--673</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1602" citStr="Duverle and Prendinger, 2009" startWordPosition="248" endWordPosition="251">nto a set of discourse relations between inter sentences. There is considerable interest in discourse parsing, both as an end in itself and as an intermediate step in a variety of NLP applications like question answering (Verberne et al., 2007), text summarization (Louis et al., 2010), sentiment analysis and opinion mining (Somasundaran, 2010). There are many approaches working on identifying the discourse relations and data-driven approaches are dominated. A number of pioneers take the discourse relations identification as a classification task (Marcu and Echihabi, 2002; Pitler et al., 2009; Duverle and Prendinger, 2009) by the construction of features like lexical, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lacking an common evaluation data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addresses this problem. Besides, much research about disco</context>
</contexts>
<marker>Duverle, Prendinger, 2009</marker>
<rawString>David A Duverle and Helmut Prendinger. 2009. A novel discourse parser based on support vector machine classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 665–673. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Richard Johansson</author>
<author>Sara Tonelli</author>
</authors>
<title>Shallow discourse parsing with conditional random fields. In</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<contexts>
<context position="1841" citStr="Ghosh et al., 2011" startWordPosition="285" endWordPosition="288">t summarization (Louis et al., 2010), sentiment analysis and opinion mining (Somasundaran, 2010). There are many approaches working on identifying the discourse relations and data-driven approaches are dominated. A number of pioneers take the discourse relations identification as a classification task (Marcu and Echihabi, 2002; Pitler et al., 2009; Duverle and Prendinger, 2009) by the construction of features like lexical, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lacking an common evaluation data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addresses this problem. Besides, much research about discourse parsing working on the PDTB appears (Prasad et al., 2010; Lin et al., 2009) and they put more attention on the “harder” part - labelling the arguments. Lin (Lin et al., 2014) designed an end-to-end discourse parser with the PDTB inclu</context>
</contexts>
<marker>Ghosh, Johansson, Tonelli, 2011</marker>
<rawString>Sucheta Ghosh, Richard Johansson, and Sara Tonelli. 2011. Shallow discourse parsing with conditional random fields. In In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
</authors>
<title>A data-driven methodology for motivating a set of coherence relations.</title>
<date>1996</date>
<contexts>
<context position="5288" citStr="Knott, 1996" startWordPosition="848" endWordPosition="849">ssion”, “Alternative.Chosen alternative” respectively . 15 valid senses including the second-level “types” as well as a selected number of third-level “subtypes”. Table 2 shows the distribution of the 15 senses in the data. Test data: There are two test data sets. One is the blind set which contains 20,000 to 30,000 words of newswire text annotated following the PDTB annotation guidelines. The other test set is Section 23 of the PDTB which is used for comparison with previous work. The connectives list: A list contains 100 discourse connectives in the PDTB and three syntactic categories form (Knott, 1996). Opennlp-maxent: We used the open source package Opennlp-maxent2 to construct the classification models. 2http://sourceforge.net/projects/maxent/files/ Figure 1: The structure of the system. 2.2 System overview and Features Our system mainly follows the work of (Lin et al., 2014), which consists of two parts: the explicit relation parser and the non-explicit relation parser. The explicit relation parser is composed of the connective classifier, the argument position classifier, the argument extractor and the explicit sense classifier while the non-Explicit relation parser contains the AltLex </context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>Alistair Knott. 1996. A data-driven methodology for motivating a set of coherence relations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the penn discourse treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>343--351</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2282" citStr="Lin et al., 2009" startWordPosition="357" endWordPosition="360">nstituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lacking an common evaluation data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addresses this problem. Besides, much research about discourse parsing working on the PDTB appears (Prasad et al., 2010; Lin et al., 2009) and they put more attention on the “harder” part - labelling the arguments. Lin (Lin et al., 2014) designed an end-to-end discourse parser with the PDTB including the explicit, implicit sense and the argument spans identification. Shallow Discourse Parsing (Xue et al., 2015) is the CoNLL shared task this year1 which takes a piece of newswire text as input and returns all the discourse relations in the form of a discourse connective (explicit or implicit) taking two arguments (which can be clauses, sentences, or multisentence segments) in JSON format. A relation will be parsed as correct if th</context>
<context position="11627" citStr="Lin et al., 2009" startWordPosition="1934" endWordPosition="1937">, F25-F26 in Table 3. After predicting, we kept the implicit discourse relations whose maximal sense probability were greater than a threshold (0.25 in our case) which was determined on the dev data set . 3 Experiments and Results There are two test data sets this year as described in Section 2.1 and the organizers reported the results on the two test data sets and the dev data set. The results of our system obtained are shown in Table 4. We ranked the 10th on every data set. After the deadline of evaluation, we made some improvements in the module of implicit relation classifier inspired by (Lin et al., 2009). We selected the word-pair features (F7) while the experiments showed a little degradation in F1 score through selecting the constituent rules and the dependency rules (F25, F26) on the dev data set. We computed the mutual information between each word-pair feature and the 15 valid senses and then selected the top N as the features. Table 5 shows the improvement of different N. N Non-Explicit Overall 50 0.0683 0.1876 100 0.0683 0.1876 200 0.0614 0.1870 300 0.0603 0.1870 Baseline 0.0435 0.1814 Table 5: The F1 score in non-explict and overall parser when selecting features F7 using different N </context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the penn discourse treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 343–351. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A pdtb-styled end-to-end discourse parser.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="2381" citStr="Lin et al., 2014" startWordPosition="376" endWordPosition="379">lner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lacking an common evaluation data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addresses this problem. Besides, much research about discourse parsing working on the PDTB appears (Prasad et al., 2010; Lin et al., 2009) and they put more attention on the “harder” part - labelling the arguments. Lin (Lin et al., 2014) designed an end-to-end discourse parser with the PDTB including the explicit, implicit sense and the argument spans identification. Shallow Discourse Parsing (Xue et al., 2015) is the CoNLL shared task this year1 which takes a piece of newswire text as input and returns all the discourse relations in the form of a discourse connective (explicit or implicit) taking two arguments (which can be clauses, sentences, or multisentence segments) in JSON format. A relation will be parsed as correct if the explicit discourse connective (e.g., “because”, “however”) once it has, the spans of text that se</context>
<context position="5569" citStr="Lin et al., 2014" startWordPosition="884" endWordPosition="887">blind set which contains 20,000 to 30,000 words of newswire text annotated following the PDTB annotation guidelines. The other test set is Section 23 of the PDTB which is used for comparison with previous work. The connectives list: A list contains 100 discourse connectives in the PDTB and three syntactic categories form (Knott, 1996). Opennlp-maxent: We used the open source package Opennlp-maxent2 to construct the classification models. 2http://sourceforge.net/projects/maxent/files/ Figure 1: The structure of the system. 2.2 System overview and Features Our system mainly follows the work of (Lin et al., 2014), which consists of two parts: the explicit relation parser and the non-explicit relation parser. The explicit relation parser is composed of the connective classifier, the argument position classifier, the argument extractor and the explicit sense classifier while the non-Explicit relation parser contains the AltLex classifier and the implicit classifier. The structure of our system is shown in Figure 1. The set of features used in our system are listed in Table 3. All the features fall into four classes: lexical features, part-of-speech (POS) features, syntactic features and positional featu</context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A pdtb-styled end-to-end discourse parser. Natural Language Engineering, 20(2):151–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Aravind Joshi</author>
<author>Ani Nenkova</author>
</authors>
<title>Discourse indicators for content selection in summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>147--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1258" citStr="Louis et al., 2010" startWordPosition="197" endWordPosition="200">urse connective. We trained the cascaded models with a variety of features such as lexical and syntactic features. We also report the results achieved by our team. 1 Introduction Discourse parsing is one of the most challenging tasks in natural language processing (NLP) field. It focuses on parsing the structure of a piece of text into a set of discourse relations between inter sentences. There is considerable interest in discourse parsing, both as an end in itself and as an intermediate step in a variety of NLP applications like question answering (Verberne et al., 2007), text summarization (Louis et al., 2010), sentiment analysis and opinion mining (Somasundaran, 2010). There are many approaches working on identifying the discourse relations and data-driven approaches are dominated. A number of pioneers take the discourse relations identification as a classification task (Marcu and Echihabi, 2002; Pitler et al., 2009; Duverle and Prendinger, 2009) by the construction of features like lexical, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some o</context>
</contexts>
<marker>Louis, Joshi, Nenkova, 2010</marker>
<rawString>Annie Louis, Aravind Joshi, and Ani Nenkova. 2010. Discourse indicators for content selection in summarization. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147–156. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>368--375</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1550" citStr="Marcu and Echihabi, 2002" startWordPosition="240" endWordPosition="243">s on parsing the structure of a piece of text into a set of discourse relations between inter sentences. There is considerable interest in discourse parsing, both as an end in itself and as an intermediate step in a variety of NLP applications like question answering (Verberne et al., 2007), text summarization (Louis et al., 2010), sentiment analysis and opinion mining (Somasundaran, 2010). There are many approaches working on identifying the discourse relations and data-driven approaches are dominated. A number of pioneers take the discourse relations identification as a classification task (Marcu and Echihabi, 2002; Pitler et al., 2009; Duverle and Prendinger, 2009) by the construction of features like lexical, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lacking an common evaluation data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addres</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 368–375. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>683--691</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1571" citStr="Pitler et al., 2009" startWordPosition="244" endWordPosition="247"> of a piece of text into a set of discourse relations between inter sentences. There is considerable interest in discourse parsing, both as an end in itself and as an intermediate step in a variety of NLP applications like question answering (Verberne et al., 2007), text summarization (Louis et al., 2010), sentiment analysis and opinion mining (Somasundaran, 2010). There are many approaches working on identifying the discourse relations and data-driven approaches are dominated. A number of pioneers take the discourse relations identification as a classification task (Marcu and Echihabi, 2002; Pitler et al., 2009; Duverle and Prendinger, 2009) by the construction of features like lexical, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lacking an common evaluation data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addresses this problem. Bes</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 683–691. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, 26 May - 1</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="2058" citStr="Prasad et al., 2008" startWordPosition="322" endWordPosition="325">umber of pioneers take the discourse relations identification as a classification task (Marcu and Echihabi, 2002; Pitler et al., 2009; Duverle and Prendinger, 2009) by the construction of features like lexical, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lacking an common evaluation data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addresses this problem. Besides, much research about discourse parsing working on the PDTB appears (Prasad et al., 2010; Lin et al., 2009) and they put more attention on the “harder” part - labelling the arguments. Lin (Lin et al., 2014) designed an end-to-end discourse parser with the PDTB including the explicit, implicit sense and the argument spans identification. Shallow Discourse Parsing (Xue et al., 2015) is the CoNLL shared task this year1 which takes a piece of newswire text as input and returns all t</context>
<context position="8824" citStr="Prasad et al., 2008" startWordPosition="1451" endWordPosition="1454">abeller Once the connective is identified, the argument labeller identifies the Arg1 and Arg2 spans of this instance. This is accomplished in two steps: (1) Classifying the locations of Arg1 by the Argument Position Classifier. (2) Labelling the spans of Arg1 and Arg2 by the Argument Extrator. The Argument Position Classifier: Normally Arg2 immediately follows the connective while the position of Arg1 is uncertain. In this model, we classified the Arg1’s locations into two classes: Arg1 was located within the same sentence of the connective (SS) or in the previous sentence of connective (PS) (Prasad et al., 2008). We implemented this as a binary classification task. In this step, features F1-F5, F11, F13, F16- F17, F28 in Table 3 were adopted to train the model. After the position label of Arg1 was determined, the result was passed to the argument extractor. The Argument Extractor: In this module, our classifier labelled the previous sentence as Arg1 immediately for the PS case. The argument spans for the SS case were extracted described as below. • Classify each internal node N in the constituent tree as Arg1-node, Arg2-node, or None with features F1, F18, F21-F24, F27 in Table 3. • Label a node as A</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K. Joshi, and Bonnie L. Webber. 2008. The penn discourse treebank 2.0. In Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, 26 May - 1 June 2008, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>Exploiting scope for shallow discourse parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation, LREC</booktitle>
<pages>17--23</pages>
<location>Valletta,</location>
<contexts>
<context position="2263" citStr="Prasad et al., 2010" startWordPosition="353" endWordPosition="356">cal, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lacking an common evaluation data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addresses this problem. Besides, much research about discourse parsing working on the PDTB appears (Prasad et al., 2010; Lin et al., 2009) and they put more attention on the “harder” part - labelling the arguments. Lin (Lin et al., 2014) designed an end-to-end discourse parser with the PDTB including the explicit, implicit sense and the argument spans identification. Shallow Discourse Parsing (Xue et al., 2015) is the CoNLL shared task this year1 which takes a piece of newswire text as input and returns all the discourse relations in the form of a discourse connective (explicit or implicit) taking two arguments (which can be clauses, sentences, or multisentence segments) in JSON format. A relation will be pars</context>
</contexts>
<marker>Prasad, Joshi, Webber, 2010</marker>
<rawString>Rashmi Prasad, Aravind K. Joshi, and Bonnie L. Webber. 2010. Exploiting scope for shallow discourse parsing. In Proceedings of the International Conference on Language Resources and Evaluation, LREC 2010, 17-23 May 2010, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
</authors>
<title>Discourse-level relations for Opinion Analysis.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="1318" citStr="Somasundaran, 2010" startWordPosition="206" endWordPosition="208">ty of features such as lexical and syntactic features. We also report the results achieved by our team. 1 Introduction Discourse parsing is one of the most challenging tasks in natural language processing (NLP) field. It focuses on parsing the structure of a piece of text into a set of discourse relations between inter sentences. There is considerable interest in discourse parsing, both as an end in itself and as an intermediate step in a variety of NLP applications like question answering (Verberne et al., 2007), text summarization (Louis et al., 2010), sentiment analysis and opinion mining (Somasundaran, 2010). There are many approaches working on identifying the discourse relations and data-driven approaches are dominated. A number of pioneers take the discourse relations identification as a classification task (Marcu and Echihabi, 2002; Pitler et al., 2009; Duverle and Prendinger, 2009) by the construction of features like lexical, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lackin</context>
</contexts>
<marker>Somasundaran, 2010</marker>
<rawString>Swapna Somasundaran. 2010. Discourse-level relations for Opinion Analysis. Ph.D. thesis, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzan Verberne</author>
<author>Lou Boves</author>
<author>Nelleke Oostdijk</author>
<author>Peter-Arno Coppen</author>
</authors>
<title>Evaluating discoursebased answer extraction for why-question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>735--736</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1217" citStr="Verberne et al., 2007" startWordPosition="191" endWordPosition="194"> of two arguments and the sense of the discourse connective. We trained the cascaded models with a variety of features such as lexical and syntactic features. We also report the results achieved by our team. 1 Introduction Discourse parsing is one of the most challenging tasks in natural language processing (NLP) field. It focuses on parsing the structure of a piece of text into a set of discourse relations between inter sentences. There is considerable interest in discourse parsing, both as an end in itself and as an intermediate step in a variety of NLP applications like question answering (Verberne et al., 2007), text summarization (Louis et al., 2010), sentiment analysis and opinion mining (Somasundaran, 2010). There are many approaches working on identifying the discourse relations and data-driven approaches are dominated. A number of pioneers take the discourse relations identification as a classification task (Marcu and Echihabi, 2002; Pitler et al., 2009; Duverle and Prendinger, 2009) by the construction of features like lexical, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling t</context>
</contexts>
<marker>Verberne, Boves, Oostdijk, Coppen, 2007</marker>
<rawString>Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen. 2007. Evaluating discoursebased answer extraction for why-question answering. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 735–736. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Wellner</author>
<author>James Pustejovsky</author>
</authors>
<title>Automatically identifying the arguments of discourse connectives.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Coumptational Natural Language Learning (EMNLP- CoNLL),</booktitle>
<pages>92--101</pages>
<contexts>
<context position="1791" citStr="Wellner and Pustejovsky, 2007" startWordPosition="276" endWordPosition="279">ications like question answering (Verberne et al., 2007), text summarization (Louis et al., 2010), sentiment analysis and opinion mining (Somasundaran, 2010). There are many approaches working on identifying the discourse relations and data-driven approaches are dominated. A number of pioneers take the discourse relations identification as a classification task (Marcu and Echihabi, 2002; Pitler et al., 2009; Duverle and Prendinger, 2009) by the construction of features like lexical, syntactic and constituent features. Some take the argument segmentation task as a semantic role detection task (Wellner and Pustejovsky, 2007) and a sequence labeling task (Ghosh et al., 2011). However, some of the previous research is based on different corpus, lacking an common evaluation data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addresses this problem. Besides, much research about discourse parsing working on the PDTB appears (Prasad et al., 2010; Lin et al., 2009) and they put more attention on the “harder” part - labelling the arguments. Lin (Lin et al., 2014) designed </context>
</contexts>
<marker>Wellner, Pustejovsky, 2007</marker>
<rawString>Ben Wellner and James Pustejovsky. 2007. Automatically identifying the arguments of discourse connectives. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Coumptational Natural Language Learning (EMNLP- CoNLL), pages 92–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Hwee Tou Ng</author>
<author>Sameer Pradhan</author>
<author>Rashmi Prasad</author>
<author>Christopher Bryant</author>
<author>Attapol Rutherford</author>
</authors>
<title>The CoNLL-2015 shared task on shallow discourse parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the Nineteenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="2558" citStr="Xue et al., 2015" startWordPosition="402" endWordPosition="405">data set. This has been addressed with the release of Penn Discourse Treebank (PDTB) 2.0 corpus (Prasad et al., 2008) which provides detailed annotations about the discourse relations and argument spans addresses this problem. Besides, much research about discourse parsing working on the PDTB appears (Prasad et al., 2010; Lin et al., 2009) and they put more attention on the “harder” part - labelling the arguments. Lin (Lin et al., 2014) designed an end-to-end discourse parser with the PDTB including the explicit, implicit sense and the argument spans identification. Shallow Discourse Parsing (Xue et al., 2015) is the CoNLL shared task this year1 which takes a piece of newswire text as input and returns all the discourse relations in the form of a discourse connective (explicit or implicit) taking two arguments (which can be clauses, sentences, or multisentence segments) in JSON format. A relation will be parsed as correct if the explicit discourse connective (e.g., “because”, “however”) once it has, the spans of text that serve as the two arguments for each discourse connective and the sense (e.g., “Comparison”) are all correct. The F1 score of the parser’s performance is the evaluation metric. In </context>
</contexts>
<marker>Xue, Ng, Pradhan, Prasad, Bryant, Rutherford, 2015</marker>
<rawString>Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi Prasad, Christopher Bryant, and Attapol Rutherford. 2015. The CoNLL-2015 shared task on shallow discourse parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>