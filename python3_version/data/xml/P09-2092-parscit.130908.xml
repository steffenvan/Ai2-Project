<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001749">
<title confidence="0.988432">
Composite Kernels For Relation Extraction
</title>
<author confidence="0.654368">
Frank Reichartz Hannes Korte Gerhard Paass
</author>
<note confidence="0.5533675">
Fraunhofer IAIS Fraunhofer IAIS Fraunhofer IAIS
St. Augustin, Germany St. Augustin, Germany St. Augustin, Germany
</note>
<email confidence="0.934193">
{frank.reichartz,hannes .korte,gerhard.paass}@iais.fraunhofer.de
</email>
<sectionHeader confidence="0.992314" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999883142857143">
The automatic extraction of relations be-
tween entities expressed in natural lan-
guage text is an important problem for IR
and text understanding. In this paper we
show how different kernels for parse trees
can be combined to improve the relation
extraction quality. On a public benchmark
dataset the combination of a kernel for
phrase grammar parse trees and for depen-
dency parse trees outperforms all known
tree kernel approaches alone suggesting
that both types of trees contain comple-
mentary information for relation extrac-
tion.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99965864">
The same semantic relation between entities in
natural text can be expressed in many ways, e.g.
“Obama was educated at Harvard”, “Obama is a
graduate of Harvard Law School”, or, “Obama
went to Harvard College”. Relation extraction
aims at identifying such semantic relations in an
automatic fashion.
As a preprocessing step named entity taggers
detect persons, locations, schools, etc. men-
tioned in the text. These techniques have reached
a sufficient performance level on many datasets
(Tjong et al., 2003). In the next step relations be-
tween recognized entities, e.g. person-educated-
in-school(Obama,Harvard) are identified.
Parse trees provide extensive information on
syntactic structure. While feature-based meth-
ods may compare only a limited number of struc-
tural details, kernel-based methods may explore
an often exponential number of characteristics
of trees without explicitly representing the fea-
tures. Zelenko et al. (2003) and Culotta and
Sorensen (2004) proposed kernels for dependency
trees (DTs) inspired by string kernels. Zhang et
al. (2006) suggested a kernel for phrase grammar
parse trees. Bunescu and Mooney (2005) investi-
gated a kernel that computes similarities between
nodes on the shortest path of a DT connecting the
entities. Reichartz et al. (2009) presented DT ker-
nels comparing substructures in a more sophisti-
cated way.
Up to now no studies exist on how kernels for
different types of parse trees may support each
other. To tackle this we present a study on how
those kernels for relation extractions can be com-
bined. We implement four state-of-the-art ker-
nels. Subsequently we combine pairs of kernels
linearly or by polynomial expansion. On a pub-
lic benchmark dataset we show that the combined
phrase grammar parse tree kernel and dependency
parse tree kernel outperforms all others by 5.7%
F-Measure reaching an F-Measure of 71.2%. This
result shows that both types of parse trees contain
relevant information for relation extraction.
The remainder of the paper is organized as fol-
lows. In the next section we describe the inves-
tigated tree kernels. Subsequently we present the
method to combine two kernels. The fourth sec-
tion details the experiments on a public benchmark
dataset. We close with a summary and conclu-
sions.
</bodyText>
<sectionHeader confidence="0.996746" genericHeader="method">
2 Kernels for Relation Extraction
</sectionHeader>
<bodyText confidence="0.999577727272727">
Relation extraction aims at learning a relation
from a number of positive and negative instances
in natural language sentences. As a classifier we
use Support Vector Machines (SVMs) (Joachims,
1999) which can compare complex structures, e.g.
trees, by kernels. Given the kernel function, the
SVM tries to find a hyperplane that separates pos-
itive from negative examples of the relation. This
type of max-margin separator has been shown both
empirically and theoretically to provide good gen-
eralization performance on new examples.
</bodyText>
<page confidence="0.988989">
365
</page>
<note confidence="0.9645725">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 365–368,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<subsectionHeader confidence="0.570619">
2.1 Parse Trees the dependecy tree of the relation argument nodes
</subsectionHeader>
<bodyText confidence="0.999957727272727">
A sentence can be processed by a parser to gener-
ate a parse tree, which can be further categorized
in phrase grammar parse trees (PTs) and depen-
dency parse trees (DTs). For DTs there is a bijec-
tive mapping between the words in a sentence and
the nodes in the tree. DTs have a natural ordering
of the children of the nodes induced by the posi-
tion of the corresponding words in the sentence. In
contrast PTs introduce new intermediate nodes to
better express the syntactical structures of a sen-
tence in terms of phrases.
</bodyText>
<subsectionHeader confidence="0.998276">
2.2 Path-enclosed PT Kernel
</subsectionHeader>
<bodyText confidence="0.9954624">
The Path-enclosed PT Tree Kernel (Zhang et al.,
2008) operates on PTs. It is based on the Convolu-
tion Tree Kernel of Collins and Duffy (2001). The
Path-enclosed Tree is the parse tree pruned to the
nodes that are connected to leaves (words) that be-
long to the path connecting both relation entities.
The leaves (and connected inner nodes) in front of
the first relation entity node and behind the sec-
ond one are simply removed. In addition, for the
entities there are new artificial nodes labeled with
the relation argument index, and the entity type.
Let KCD(T1,T2) be the Convolution Tree Kernel
(Collins and Duffy, 2001) of two trees T1, T2, then
the Path-enclosed PT Kernel (ZhangPT) is de-
fined as
KZhangPT(X,Y ) = KCD(X*,Y*)
where X* and Y * are the subtrees of the origi-
nal tree pruned to the nodes enclosed by the path
connecting the two entities in the phrase grammer
parse trees as described by Zhang et al. (2008).
</bodyText>
<subsectionHeader confidence="0.998562">
2.3 Dependency Tree Kernel
</subsectionHeader>
<bodyText confidence="0.999960615384615">
The Dependency Tree Kernel (DTK) of Culotta
and Sorensen(2004) is based on the work of Ze-
lenko et al. (2003). It employs a node kernel
Δ(u, v) measuring the similarity of two tree nodes
u, v and its substructures. Nodes may be described
by different features like POS-tags, chunk tags,
etc.. If the corresponding word describes an en-
tity, the entity type and the mention is provided. To
compare relations in two instance sentences X, Y
Culotta and Sorensen (2004) proposes to compare
the subtrees induced by the relation arguments
x1, x2 and y1, y2, i.e. computing the node kernel
between the two lowest common ancestors (lca) in
</bodyText>
<equation confidence="0.77871">
KDTK(X, Y ) = Δ(lca(x1, x2), lca(y1, y2))
</equation>
<bodyText confidence="0.9999757">
The node kernel Δ(u, v) is definend over two
nodes u and v as the sum of the node similarity
and their children similarity. The children simi-
larity function Qs, t) uses a modified version of
the String Subsequence Kernel of Shawe-Taylor
and Christianini (2004) to compute recursively the
sum of node kernel values of subsequences of
node sequences s and t. The function Qs, t) sums
up the similarities of all subsequences in which ev-
ery node matches its corresponding node.
</bodyText>
<subsectionHeader confidence="0.992133">
2.4 All-Pairs Dependency Tree Kernel
</subsectionHeader>
<bodyText confidence="0.9994306">
The All-Pairs Dependency Tree Kernel (All-Pairs-
DTK) (Reichartz et al., 2009) sums up the node
kernels of all possible combinations of nodes con-
tained in the two subtrees implied by the relation
argument nodes as
</bodyText>
<equation confidence="0.9110485">
�KAll-Pairs(X, Y ) = E
UEV. VEVy
</equation>
<bodyText confidence="0.9997626">
where V,, and Vy are sets containing the nodes of
the complete subtrees rooted at the respective low-
est common ancestors. The consideration of all
possible pairs of nodes and their similarity ensure
that relevant information in the subtrees is utilized.
</bodyText>
<subsectionHeader confidence="0.943428">
2.5 Dependency Path Tree Kernel
</subsectionHeader>
<bodyText confidence="0.999932">
The Dependency Path Tree Kernel (Path-DTK)
(Reichartz et al., 2009) not only measures the
similarity of the root nodes and its descendents
(Culotta and Sorensen, 2004) or the similari-
ties of nodes on the path (Bunescu and Mooney,
2005). It considers the similarities of all nodes
(and substructures) using the node kernel Δ on
the path connecting the two relation argument en-
tity nodes. To this end the pairwise comparison
is performed using the ideas of the subsequence
kernel of Shawe-Taylor and Cristianini (2004),
therefore relaxing the “same length” restriction of
(Bunescu and Mooney, 2005). The Path-DTK ef-
fectively compares the nodes from paths with dif-
ferent lengths while maintaining the ordering in-
formation and considering the similarities of sub-
structures.
The parameter q is the upper bound on the node
distance whereas the parameter p, 0 &lt; p &lt; 1,
is a factor that penalizes gaps. The Path-DTK is
</bodyText>
<equation confidence="0.803099">
Δ(u, v)
</equation>
<page confidence="0.98594">
366
</page>
<table confidence="0.999850857142857">
Kernel 5-times 5-fold Cross-Validation on Training Set At Part Test Set Rec F
At Part Role Prec Rec F Role Prec
DTK 54.9 52.8 72.3 71.7 53.7 61.4 (0.32) 50.3 43.4 68.5 79.5 44.0 56.7
All-Pairs-DTK 59.1 53.6 73.0 73.1 57.8 64.5 (0.26) 54.3 53.9 71.8 80.2 49.6 61.3
Path-DTK 64.8 62.9 77.2 80.2 61.2 69.4 (0.09) 54.9 55.6 73.5 76.7 52.8 62.5
ZhangPT 66.8 69.1 77.7 80.6 65.0 71.9 (0.21) 62.9 64.2 72.2 82.0 54.5 65.5
ZhangPT + Path-DTK 70.1 76.6 80.8 84.6 68.2 75.5 (0.20) 66.3 71.3 77.7 85.7 60.9 71.2
</table>
<tableCaption confidence="0.776554666666667">
Table 1: F-values for 3 selected relations and micro-averaged precision, recall and F-score (with standard
error) for all 5 relations on the training (CV) and test set in percent.
µd(&apos;)+d(�)A&apos;(x(i), y(j))
</tableCaption>
<bodyText confidence="0.999250375">
where x and y are the paths in the dependency
tree between the relation arguments and x(i) is
the subsequence of the nodes indexed by i, anal-
ogously for j. Ik is the set of all possible in-
dex sequences with highest index k and d(i) =
max(i) − min(i) + 1 is the covered distance. The
function A&apos; is the sum of the pairwise applications
of the node kernel A.
</bodyText>
<sectionHeader confidence="0.993603" genericHeader="method">
3 Kernel composition
</sectionHeader>
<bodyText confidence="0.99657625">
In this paper we use the following two ap-
proaches to combine two normalized1 kernels
K1, K2 (Schoelkopf and Smola, 2001). For a
weighting factor a we have the composite kernel:
</bodyText>
<equation confidence="0.594111">
K,(X, Y ) = aK1(X, Y ) + (1 − a)K2(X, Y )
</equation>
<bodyText confidence="0.9994082">
Furthermore it is possible to use polynomial ex-
pansion on the single kernels, i.e. Kp(X,Y ) =
(K(X, Y ) + 1)p. Our experiments are performed
with a = 0.5 and the sum of linear kernels (L) or
poly kernels (P) with p = 2.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999888333333333">
In this section we present the results of the ex-
periments with kernel-based methods for relation
extraction. Throughout this section we will com-
pare the approaches considering their classifica-
tion quality on the publicly available benchmark
dataset ACE-2003 (Mitchell et al., 2003). It con-
sists of news documents containing 176825 words
splitted in a test and training set. Entities and the
relations between them were manually annotated.
</bodyText>
<equation confidence="0.444314">
1Kernel normalization: Kn(X, Y ) = K(X,
VK(X,X)-K•K(Y,Y ) )
</equation>
<bodyText confidence="0.9999296">
The entities are marked by the types named (e.g.
“Albert Einstein”) , nominal (e.g. “University”)
and pronominal (e.g. “he”). There are 5 top level
relation types role, part, near, social and at, which
are further differentiated into 24 subtypes.
</bodyText>
<subsectionHeader confidence="0.989529">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999972095238095">
We implemented the tree-kernels for relation
extraction in Java and used Joachim’s (1999)
SVMUght with the JNI Kernel Extension using
the implementation details from the original pa-
pers. For the generation of the parse trees we used
the Stanford Parser (Klein and Manning, 2003).
We restricted our experiments to relations between
named entities, where NER approaches may be
used to extract the arguments. Without any modi-
fication the kernels could also be applied to the all
types setting as well. We conducted classification
tests on the five top level relations of the dataset.
For each relation we trained a separate SVM fol-
lowing the one vs. all scheme for multi-class
classification. We also employed a standard grid-
search on the training set with a 5-times repeated
5-fold cross validation to optimize the parameters
of all kernels as well as the SVM-parameter C for
the classification runs on the separate test set. We
use the standard evaluation measures for classifi-
cation accuracy: precision, recall and F-measure.
</bodyText>
<subsectionHeader confidence="0.759293">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999877545454545">
Table 1 shows F-values for three selected rela-
tions and micro-averaged results for all 5 relations
on the training and test set. In addition the F-
scores for the three relations containing the most
instances are provided. Kernel and SVM parame-
ters are optimized solely on the training set. Note
that the training set results were obtained on the
left-out folds of cross-validation. The composite
kernel ZhangPT + Path-DTK performs the best on
the cross validations run as well as on the test-set.
It outperforms all previously suggested solutions
</bodyText>
<equation confidence="0.794801">
defined as
KPath-DTK(X, Y ) =
iEIjxj, jEIjyj,
jij=jjj, d(i),d(j)5q
</equation>
<page confidence="0.990225">
367
</page>
<table confidence="0.9955916">
DTK All-Pairs-DTK Path-DTK ZhangPT
ZhangPT 63.5 (70.2) PP 67.9 (72.8) PP 71.2 (75.5) LP 65.5 (71.9)
Path-DTK 62.7 (67.7) PP 62.9 (69.5) PL 62.5 (69.4)
All-Pairs-DTK 60.0 (64.7) PP 61.3 (64.5)
DTK 56.7 (61.4)
</table>
<tableCaption confidence="0.980833">
Table 2: Micro-averaged F-values for the Single and Combined Kernels on the Test Set (outside paren-
</tableCaption>
<bodyText confidence="0.9941904375">
thesis) and with 5-times repeated 5-fold CV on the Training Set (inside parenthesis). LP denotes the
combination type linear and polynomial, analogously PP and PL.
by at least 5.7% F-Measure on the prespecified
test-set and by 3.6% F-Measure on the cross val-
idation. Table 2 shows the F-values of the differ-
ent combinational kernels on the test set as well
as on the cross validation on the training set. The
ZhangPT + Path-DTK performs the best out of all
possible combinations. The difference in F-values
between ZhangPT + Path-DTK and ZhangPT is
according to corrected resampled t-test (Bouckaert
and Frank, 2004) significant at a level of 99.9%.
These results show that the simultanous consider-
ation of phrase grammar parse trees and depen-
dency parse trees by the combination of the two
kernels is meaningful for relation extraction.
</bodyText>
<sectionHeader confidence="0.991089" genericHeader="evaluation">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9994932">
In this paper we presented a study on the combi-
nation of state of the art kernels to improve re-
lation extraction quality. We were able to show
that a combination of a kernel for phrase gram-
mar parse trees and one for dependency parse trees
outperforms all other published parse tree ker-
nel approaches indicating that both kernels cap-
tures complementary information for relation ex-
traction. A promising direction for future work is
the usage of more sophisticated features aiming at
capturing the semantics of words e.g. word sense
disambiguation (Paaß and Reichartz, 2009). Other
promising directions are the study on the applica-
bility of the kernel to other languages and explor-
ing combinations of more than two kernels.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="conclusions">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999927">
The work presented here was funded by the Ger-
man Federal Ministry of Economy and Technol-
ogy (BMWi) under the THESEUS project.
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999625930232558">
Remco R. Bouckaert and Eibe Frank. 2004. Evaluat-
ing the replicability of significance tests for compar-
ing learning algorithms. In PAKDD ’04.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proc. HLT/EMNLP, pages 724 – 731.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proc. NIPS ’01.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL ’04.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. ACL ’03.
Alexis Mitchell et al. 2003. ACE-2 Version 1.0; Cor-
pus LDC2003T11. Linguistic Data Consortium.
Gerhard Paaß and Frank Reichartz. 2009. Exploiting
semantic constraints for estimating supersenses with
CRFs. In Proc. SDM 2009.
Frank Reichartz, Hannes Korte, and Gerhard Paass.
2009. Dependency tree kernels for relation extrac-
tion from natural language text. In ECML ’09.
Bernhard Schoelkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Erik F. Tjong, Kim Sang, and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
CoRR cs.CL/0306050:.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083–1106.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing syntactic features for relation extraction using a
convolution tree kernel. In Proc. HLT/NAACL’06.
Min Zhang, GuoDong Zhou, and Aiti Aw. 2008. Ex-
ploring syntactic structured features over parse trees
for relation extraction using kernel methods. Inf.
Process. Manage., 44(2):687–701.
</reference>
<page confidence="0.99826">
368
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.584965">
<title confidence="0.861396333333333">Composite Kernels For Relation Extraction Frank Reichartz Hannes Korte Gerhard Paass Fraunhofer IAIS Fraunhofer IAIS Fraunhofer IAIS</title>
<author confidence="0.961861">Germany St Augustin Augustin</author>
<author confidence="0.961861">Germany St Augustin</author>
<author confidence="0.961861">Germany</author>
<abstract confidence="0.987979333333334">The automatic extraction of relations between entities expressed in natural language text is an important problem for IR and text understanding. In this paper we show how different kernels for parse trees can be combined to improve the relation extraction quality. On a public benchmark dataset the combination of a kernel for phrase grammar parse trees and for dependency parse trees outperforms all known tree kernel approaches alone suggesting that both types of trees contain complementary information for relation extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Remco R Bouckaert</author>
<author>Eibe Frank</author>
</authors>
<title>Evaluating the replicability of significance tests for comparing learning algorithms.</title>
<date>2004</date>
<booktitle>In PAKDD ’04.</booktitle>
<contexts>
<context position="12861" citStr="Bouckaert and Frank, 2004" startWordPosition="2120" endWordPosition="2123">side parenthesis) and with 5-times repeated 5-fold CV on the Training Set (inside parenthesis). LP denotes the combination type linear and polynomial, analogously PP and PL. by at least 5.7% F-Measure on the prespecified test-set and by 3.6% F-Measure on the cross validation. Table 2 shows the F-values of the different combinational kernels on the test set as well as on the cross validation on the training set. The ZhangPT + Path-DTK performs the best out of all possible combinations. The difference in F-values between ZhangPT + Path-DTK and ZhangPT is according to corrected resampled t-test (Bouckaert and Frank, 2004) significant at a level of 99.9%. These results show that the simultanous consideration of phrase grammar parse trees and dependency parse trees by the combination of the two kernels is meaningful for relation extraction. 5 Conclusion and Future Work In this paper we presented a study on the combination of state of the art kernels to improve relation extraction quality. We were able to show that a combination of a kernel for phrase grammar parse trees and one for dependency parse trees outperforms all other published parse tree kernel approaches indicating that both kernels captures complement</context>
</contexts>
<marker>Bouckaert, Frank, 2004</marker>
<rawString>Remco R. Bouckaert and Eibe Frank. 2004. Evaluating the replicability of significance tests for comparing learning algorithms. In PAKDD ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proc. HLT/EMNLP,</booktitle>
<pages>724--731</pages>
<contexts>
<context position="1953" citStr="Bunescu and Mooney (2005)" startWordPosition="280" endWordPosition="283">In the next step relations between recognized entities, e.g. person-educatedin-school(Obama,Harvard) are identified. Parse trees provide extensive information on syntactic structure. While feature-based methods may compare only a limited number of structural details, kernel-based methods may explore an often exponential number of characteristics of trees without explicitly representing the features. Zelenko et al. (2003) and Culotta and Sorensen (2004) proposed kernels for dependency trees (DTs) inspired by string kernels. Zhang et al. (2006) suggested a kernel for phrase grammar parse trees. Bunescu and Mooney (2005) investigated a kernel that computes similarities between nodes on the shortest path of a DT connecting the entities. Reichartz et al. (2009) presented DT kernels comparing substructures in a more sophisticated way. Up to now no studies exist on how kernels for different types of parse trees may support each other. To tackle this we present a study on how those kernels for relation extractions can be combined. We implement four state-of-the-art kernels. Subsequently we combine pairs of kernels linearly or by polynomial expansion. On a public benchmark dataset we show that the combined phrase g</context>
<context position="7289" citStr="Bunescu and Mooney, 2005" startWordPosition="1180" endWordPosition="1183">ontained in the two subtrees implied by the relation argument nodes as �KAll-Pairs(X, Y ) = E UEV. VEVy where V,, and Vy are sets containing the nodes of the complete subtrees rooted at the respective lowest common ancestors. The consideration of all possible pairs of nodes and their similarity ensure that relevant information in the subtrees is utilized. 2.5 Dependency Path Tree Kernel The Dependency Path Tree Kernel (Path-DTK) (Reichartz et al., 2009) not only measures the similarity of the root nodes and its descendents (Culotta and Sorensen, 2004) or the similarities of nodes on the path (Bunescu and Mooney, 2005). It considers the similarities of all nodes (and substructures) using the node kernel Δ on the path connecting the two relation argument entity nodes. To this end the pairwise comparison is performed using the ideas of the subsequence kernel of Shawe-Taylor and Cristianini (2004), therefore relaxing the “same length” restriction of (Bunescu and Mooney, 2005). The Path-DTK effectively compares the nodes from paths with different lengths while maintaining the ordering information and considering the similarities of substructures. The parameter q is the upper bound on the node distance whereas t</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proc. HLT/EMNLP, pages 724 – 731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language. In</title>
<date>2001</date>
<booktitle>Proc. NIPS ’01.</booktitle>
<contexts>
<context position="4529" citStr="Collins and Duffy (2001)" startWordPosition="708" endWordPosition="711">, which can be further categorized in phrase grammar parse trees (PTs) and dependency parse trees (DTs). For DTs there is a bijective mapping between the words in a sentence and the nodes in the tree. DTs have a natural ordering of the children of the nodes induced by the position of the corresponding words in the sentence. In contrast PTs introduce new intermediate nodes to better express the syntactical structures of a sentence in terms of phrases. 2.2 Path-enclosed PT Kernel The Path-enclosed PT Tree Kernel (Zhang et al., 2008) operates on PTs. It is based on the Convolution Tree Kernel of Collins and Duffy (2001). The Path-enclosed Tree is the parse tree pruned to the nodes that are connected to leaves (words) that belong to the path connecting both relation entities. The leaves (and connected inner nodes) in front of the first relation entity node and behind the second one are simply removed. In addition, for the entities there are new artificial nodes labeled with the relation argument index, and the entity type. Let KCD(T1,T2) be the Convolution Tree Kernel (Collins and Duffy, 2001) of two trees T1, T2, then the Path-enclosed PT Kernel (ZhangPT) is defined as KZhangPT(X,Y ) = KCD(X*,Y*) where X* an</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proc. NIPS ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In ACL ’04.</booktitle>
<contexts>
<context position="1784" citStr="Culotta and Sorensen (2004)" startWordPosition="254" endWordPosition="257">aggers detect persons, locations, schools, etc. mentioned in the text. These techniques have reached a sufficient performance level on many datasets (Tjong et al., 2003). In the next step relations between recognized entities, e.g. person-educatedin-school(Obama,Harvard) are identified. Parse trees provide extensive information on syntactic structure. While feature-based methods may compare only a limited number of structural details, kernel-based methods may explore an often exponential number of characteristics of trees without explicitly representing the features. Zelenko et al. (2003) and Culotta and Sorensen (2004) proposed kernels for dependency trees (DTs) inspired by string kernels. Zhang et al. (2006) suggested a kernel for phrase grammar parse trees. Bunescu and Mooney (2005) investigated a kernel that computes similarities between nodes on the shortest path of a DT connecting the entities. Reichartz et al. (2009) presented DT kernels comparing substructures in a more sophisticated way. Up to now no studies exist on how kernels for different types of parse trees may support each other. To tackle this we present a study on how those kernels for relation extractions can be combined. We implement four</context>
<context position="5803" citStr="Culotta and Sorensen (2004)" startWordPosition="930" endWordPosition="933">ned to the nodes enclosed by the path connecting the two entities in the phrase grammer parse trees as described by Zhang et al. (2008). 2.3 Dependency Tree Kernel The Dependency Tree Kernel (DTK) of Culotta and Sorensen(2004) is based on the work of Zelenko et al. (2003). It employs a node kernel Δ(u, v) measuring the similarity of two tree nodes u, v and its substructures. Nodes may be described by different features like POS-tags, chunk tags, etc.. If the corresponding word describes an entity, the entity type and the mention is provided. To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x1, x2 and y1, y2, i.e. computing the node kernel between the two lowest common ancestors (lca) in KDTK(X, Y ) = Δ(lca(x1, x2), lca(y1, y2)) The node kernel Δ(u, v) is definend over two nodes u and v as the sum of the node similarity and their children similarity. The children similarity function Qs, t) uses a modified version of the String Subsequence Kernel of Shawe-Taylor and Christianini (2004) to compute recursively the sum of node kernel values of subsequences of node sequences s and t. The function Qs, t) sums up the si</context>
<context position="7221" citStr="Culotta and Sorensen, 2004" startWordPosition="1167" endWordPosition="1170">2009) sums up the node kernels of all possible combinations of nodes contained in the two subtrees implied by the relation argument nodes as �KAll-Pairs(X, Y ) = E UEV. VEVy where V,, and Vy are sets containing the nodes of the complete subtrees rooted at the respective lowest common ancestors. The consideration of all possible pairs of nodes and their similarity ensure that relevant information in the subtrees is utilized. 2.5 Dependency Path Tree Kernel The Dependency Path Tree Kernel (Path-DTK) (Reichartz et al., 2009) not only measures the similarity of the root nodes and its descendents (Culotta and Sorensen, 2004) or the similarities of nodes on the path (Bunescu and Mooney, 2005). It considers the similarities of all nodes (and substructures) using the node kernel Δ on the path connecting the two relation argument entity nodes. To this end the pairwise comparison is performed using the ideas of the subsequence kernel of Shawe-Taylor and Cristianini (2004), therefore relaxing the “same length” restriction of (Bunescu and Mooney, 2005). The Path-DTK effectively compares the nodes from paths with different lengths while maintaining the ordering information and considering the similarities of substructure</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In ACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods -Support Vector Learning.</booktitle>
<contexts>
<context position="3308" citStr="Joachims, 1999" startWordPosition="503" endWordPosition="504">lt shows that both types of parse trees contain relevant information for relation extraction. The remainder of the paper is organized as follows. In the next section we describe the investigated tree kernels. Subsequently we present the method to combine two kernels. The fourth section details the experiments on a public benchmark dataset. We close with a summary and conclusions. 2 Kernels for Relation Extraction Relation extraction aims at learning a relation from a number of positive and negative instances in natural language sentences. As a classifier we use Support Vector Machines (SVMs) (Joachims, 1999) which can compare complex structures, e.g. trees, by kernels. Given the kernel function, the SVM tries to find a hyperplane that separates positive from negative examples of the relation. This type of max-margin separator has been shown both empirically and theoretically to provide good generalization performance on new examples. 365 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 365–368, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP 2.1 Parse Trees the dependecy tree of the relation argument nodes A sentence can be processed by a parser to generate a parse tree, w</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Advances in Kernel Methods -Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. ACL ’03.</booktitle>
<contexts>
<context position="10561" citStr="Klein and Manning, 2003" startWordPosition="1746" endWordPosition="1749">ally annotated. 1Kernel normalization: Kn(X, Y ) = K(X, VK(X,X)-K•K(Y,Y ) ) The entities are marked by the types named (e.g. “Albert Einstein”) , nominal (e.g. “University”) and pronominal (e.g. “he”). There are 5 top level relation types role, part, near, social and at, which are further differentiated into 24 subtypes. 4.1 Experimental Setup We implemented the tree-kernels for relation extraction in Java and used Joachim’s (1999) SVMUght with the JNI Kernel Extension using the implementation details from the original papers. For the generation of the parse trees we used the Stanford Parser (Klein and Manning, 2003). We restricted our experiments to relations between named entities, where NER approaches may be used to extract the arguments. Without any modification the kernels could also be applied to the all types setting as well. We conducted classification tests on the five top level relations of the dataset. For each relation we trained a separate SVM following the one vs. all scheme for multi-class classification. We also employed a standard gridsearch on the training set with a 5-times repeated 5-fold cross validation to optimize the parameters of all kernels as well as the SVM-parameter C for the </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Mitchell</author>
</authors>
<date>2003</date>
<booktitle>ACE-2 Version 1.0; Corpus LDC2003T11. Linguistic Data Consortium.</booktitle>
<marker>Mitchell, 2003</marker>
<rawString>Alexis Mitchell et al. 2003. ACE-2 Version 1.0; Corpus LDC2003T11. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Paaß</author>
<author>Frank Reichartz</author>
</authors>
<title>Exploiting semantic constraints for estimating supersenses with CRFs.</title>
<date>2009</date>
<booktitle>In Proc. SDM</booktitle>
<marker>Paaß, Reichartz, 2009</marker>
<rawString>Gerhard Paaß and Frank Reichartz. 2009. Exploiting semantic constraints for estimating supersenses with CRFs. In Proc. SDM 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Reichartz</author>
<author>Hannes Korte</author>
<author>Gerhard Paass</author>
</authors>
<title>Dependency tree kernels for relation extraction from natural language text.</title>
<date>2009</date>
<booktitle>In ECML ’09.</booktitle>
<contexts>
<context position="2094" citStr="Reichartz et al. (2009)" startWordPosition="303" endWordPosition="306">ive information on syntactic structure. While feature-based methods may compare only a limited number of structural details, kernel-based methods may explore an often exponential number of characteristics of trees without explicitly representing the features. Zelenko et al. (2003) and Culotta and Sorensen (2004) proposed kernels for dependency trees (DTs) inspired by string kernels. Zhang et al. (2006) suggested a kernel for phrase grammar parse trees. Bunescu and Mooney (2005) investigated a kernel that computes similarities between nodes on the shortest path of a DT connecting the entities. Reichartz et al. (2009) presented DT kernels comparing substructures in a more sophisticated way. Up to now no studies exist on how kernels for different types of parse trees may support each other. To tackle this we present a study on how those kernels for relation extractions can be combined. We implement four state-of-the-art kernels. Subsequently we combine pairs of kernels linearly or by polynomial expansion. On a public benchmark dataset we show that the combined phrase grammar parse tree kernel and dependency parse tree kernel outperforms all others by 5.7% F-Measure reaching an F-Measure of 71.2%. This resul</context>
<context position="6599" citStr="Reichartz et al., 2009" startWordPosition="1064" endWordPosition="1067">, Y ) = Δ(lca(x1, x2), lca(y1, y2)) The node kernel Δ(u, v) is definend over two nodes u and v as the sum of the node similarity and their children similarity. The children similarity function Qs, t) uses a modified version of the String Subsequence Kernel of Shawe-Taylor and Christianini (2004) to compute recursively the sum of node kernel values of subsequences of node sequences s and t. The function Qs, t) sums up the similarities of all subsequences in which every node matches its corresponding node. 2.4 All-Pairs Dependency Tree Kernel The All-Pairs Dependency Tree Kernel (All-PairsDTK) (Reichartz et al., 2009) sums up the node kernels of all possible combinations of nodes contained in the two subtrees implied by the relation argument nodes as �KAll-Pairs(X, Y ) = E UEV. VEVy where V,, and Vy are sets containing the nodes of the complete subtrees rooted at the respective lowest common ancestors. The consideration of all possible pairs of nodes and their similarity ensure that relevant information in the subtrees is utilized. 2.5 Dependency Path Tree Kernel The Dependency Path Tree Kernel (Path-DTK) (Reichartz et al., 2009) not only measures the similarity of the root nodes and its descendents (Culot</context>
</contexts>
<marker>Reichartz, Korte, Paass, 2009</marker>
<rawString>Frank Reichartz, Hannes Korte, and Gerhard Paass. 2009. Dependency tree kernels for relation extraction from natural language text. In ECML ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Schoelkopf</author>
<author>Alexander J Smola</author>
</authors>
<title>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond.</title>
<date>2001</date>
<contexts>
<context position="9181" citStr="Schoelkopf and Smola, 2001" startWordPosition="1513" endWordPosition="1516">all and F-score (with standard error) for all 5 relations on the training (CV) and test set in percent. µd(&apos;)+d(�)A&apos;(x(i), y(j)) where x and y are the paths in the dependency tree between the relation arguments and x(i) is the subsequence of the nodes indexed by i, analogously for j. Ik is the set of all possible index sequences with highest index k and d(i) = max(i) − min(i) + 1 is the covered distance. The function A&apos; is the sum of the pairwise applications of the node kernel A. 3 Kernel composition In this paper we use the following two approaches to combine two normalized1 kernels K1, K2 (Schoelkopf and Smola, 2001). For a weighting factor a we have the composite kernel: K,(X, Y ) = aK1(X, Y ) + (1 − a)K2(X, Y ) Furthermore it is possible to use polynomial expansion on the single kernels, i.e. Kp(X,Y ) = (K(X, Y ) + 1)p. Our experiments are performed with a = 0.5 and the sum of linear kernels (L) or poly kernels (P) with p = 2. 4 Experiments In this section we present the results of the experiments with kernel-based methods for relation extraction. Throughout this section we will compare the approaches considering their classification quality on the publicly available benchmark dataset ACE-2003 (Mitchell</context>
</contexts>
<marker>Schoelkopf, Smola, 2001</marker>
<rawString>Bernhard Schoelkopf and Alexander J. Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7570" citStr="Shawe-Taylor and Cristianini (2004)" startWordPosition="1225" endWordPosition="1228">des and their similarity ensure that relevant information in the subtrees is utilized. 2.5 Dependency Path Tree Kernel The Dependency Path Tree Kernel (Path-DTK) (Reichartz et al., 2009) not only measures the similarity of the root nodes and its descendents (Culotta and Sorensen, 2004) or the similarities of nodes on the path (Bunescu and Mooney, 2005). It considers the similarities of all nodes (and substructures) using the node kernel Δ on the path connecting the two relation argument entity nodes. To this end the pairwise comparison is performed using the ideas of the subsequence kernel of Shawe-Taylor and Cristianini (2004), therefore relaxing the “same length” restriction of (Bunescu and Mooney, 2005). The Path-DTK effectively compares the nodes from paths with different lengths while maintaining the ordering information and considering the similarities of substructures. The parameter q is the upper bound on the node distance whereas the parameter p, 0 &lt; p &lt; 1, is a factor that penalizes gaps. The Path-DTK is Δ(u, v) 366 Kernel 5-times 5-fold Cross-Validation on Training Set At Part Test Set Rec F At Part Role Prec Rec F Role Prec DTK 54.9 52.8 72.3 71.7 53.7 61.4 (0.32) 50.3 43.4 68.5 79.5 44.0 56.7 All-Pairs-</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong</author>
<author>Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In CoRR cs.CL/0306050:.</booktitle>
<marker>Tjong, Sang, De Meulder, 2003</marker>
<rawString>Erik F. Tjong, Kim Sang, and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoRR cs.CL/0306050:.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1083</pages>
<contexts>
<context position="1752" citStr="Zelenko et al. (2003)" startWordPosition="249" endWordPosition="252">essing step named entity taggers detect persons, locations, schools, etc. mentioned in the text. These techniques have reached a sufficient performance level on many datasets (Tjong et al., 2003). In the next step relations between recognized entities, e.g. person-educatedin-school(Obama,Harvard) are identified. Parse trees provide extensive information on syntactic structure. While feature-based methods may compare only a limited number of structural details, kernel-based methods may explore an often exponential number of characteristics of trees without explicitly representing the features. Zelenko et al. (2003) and Culotta and Sorensen (2004) proposed kernels for dependency trees (DTs) inspired by string kernels. Zhang et al. (2006) suggested a kernel for phrase grammar parse trees. Bunescu and Mooney (2005) investigated a kernel that computes similarities between nodes on the shortest path of a DT connecting the entities. Reichartz et al. (2009) presented DT kernels comparing substructures in a more sophisticated way. Up to now no studies exist on how kernels for different types of parse trees may support each other. To tackle this we present a study on how those kernels for relation extractions ca</context>
<context position="5448" citStr="Zelenko et al. (2003)" startWordPosition="869" endWordPosition="873">, for the entities there are new artificial nodes labeled with the relation argument index, and the entity type. Let KCD(T1,T2) be the Convolution Tree Kernel (Collins and Duffy, 2001) of two trees T1, T2, then the Path-enclosed PT Kernel (ZhangPT) is defined as KZhangPT(X,Y ) = KCD(X*,Y*) where X* and Y * are the subtrees of the original tree pruned to the nodes enclosed by the path connecting the two entities in the phrase grammer parse trees as described by Zhang et al. (2008). 2.3 Dependency Tree Kernel The Dependency Tree Kernel (DTK) of Culotta and Sorensen(2004) is based on the work of Zelenko et al. (2003). It employs a node kernel Δ(u, v) measuring the similarity of two tree nodes u, v and its substructures. Nodes may be described by different features like POS-tags, chunk tags, etc.. If the corresponding word describes an entity, the entity type and the mention is provided. To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x1, x2 and y1, y2, i.e. computing the node kernel between the two lowest common ancestors (lca) in KDTK(X, Y ) = Δ(lca(x1, x2), lca(y1, y2)) The node kernel Δ(u, v) is definend </context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. J. Mach. Learn. Res., 3:1083–1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
</authors>
<title>Exploring syntactic features for relation extraction using a convolution tree kernel.</title>
<date>2006</date>
<booktitle>In Proc. HLT/NAACL’06.</booktitle>
<contexts>
<context position="1876" citStr="Zhang et al. (2006)" startWordPosition="268" endWordPosition="271"> a sufficient performance level on many datasets (Tjong et al., 2003). In the next step relations between recognized entities, e.g. person-educatedin-school(Obama,Harvard) are identified. Parse trees provide extensive information on syntactic structure. While feature-based methods may compare only a limited number of structural details, kernel-based methods may explore an often exponential number of characteristics of trees without explicitly representing the features. Zelenko et al. (2003) and Culotta and Sorensen (2004) proposed kernels for dependency trees (DTs) inspired by string kernels. Zhang et al. (2006) suggested a kernel for phrase grammar parse trees. Bunescu and Mooney (2005) investigated a kernel that computes similarities between nodes on the shortest path of a DT connecting the entities. Reichartz et al. (2009) presented DT kernels comparing substructures in a more sophisticated way. Up to now no studies exist on how kernels for different types of parse trees may support each other. To tackle this we present a study on how those kernels for relation extractions can be combined. We implement four state-of-the-art kernels. Subsequently we combine pairs of kernels linearly or by polynomia</context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring syntactic features for relation extraction using a convolution tree kernel. In Proc. HLT/NAACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>GuoDong Zhou</author>
<author>Aiti Aw</author>
</authors>
<title>Exploring syntactic structured features over parse trees for relation extraction using kernel methods.</title>
<date>2008</date>
<journal>Inf. Process. Manage.,</journal>
<volume>44</volume>
<issue>2</issue>
<contexts>
<context position="4441" citStr="Zhang et al., 2008" startWordPosition="691" endWordPosition="694">ion argument nodes A sentence can be processed by a parser to generate a parse tree, which can be further categorized in phrase grammar parse trees (PTs) and dependency parse trees (DTs). For DTs there is a bijective mapping between the words in a sentence and the nodes in the tree. DTs have a natural ordering of the children of the nodes induced by the position of the corresponding words in the sentence. In contrast PTs introduce new intermediate nodes to better express the syntactical structures of a sentence in terms of phrases. 2.2 Path-enclosed PT Kernel The Path-enclosed PT Tree Kernel (Zhang et al., 2008) operates on PTs. It is based on the Convolution Tree Kernel of Collins and Duffy (2001). The Path-enclosed Tree is the parse tree pruned to the nodes that are connected to leaves (words) that belong to the path connecting both relation entities. The leaves (and connected inner nodes) in front of the first relation entity node and behind the second one are simply removed. In addition, for the entities there are new artificial nodes labeled with the relation argument index, and the entity type. Let KCD(T1,T2) be the Convolution Tree Kernel (Collins and Duffy, 2001) of two trees T1, T2, then the</context>
</contexts>
<marker>Zhang, Zhou, Aw, 2008</marker>
<rawString>Min Zhang, GuoDong Zhou, and Aiti Aw. 2008. Exploring syntactic structured features over parse trees for relation extraction using kernel methods. Inf. Process. Manage., 44(2):687–701.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>