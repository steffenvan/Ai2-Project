<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000134">
<note confidence="0.959822333333333">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 71-78
Edmonton, May-June 2003
</note>
<title confidence="0.9968705">
Automatic Evaluation of Summaries Using N-gram
Co-Occurrence Statistics
</title>
<author confidence="0.993358">
Chin-Yew Lin and Eduard Hovy
</author>
<affiliation confidence="0.995741">
Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.897672">
4676 Admiralty Way
Marina del Rey, CA 90292
</address>
<email confidence="0.999663">
{cyl,hovy}@isi.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929818181818">
Following the recent adoption by the machine
translation community of automatic evalua-
tion using the BLEU/NIST scoring process,
we conduct an in-depth study of a similar idea
for evaluating summaries. The results show
that automatic evaluation using unigram co-
occurrences between summary pairs correlates
surprising well with human evaluations, based
on various statistical metrics; while direct ap-
plication of the BLEU evaluation procedure
does not always give good results.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958128205128">
Automated text summarization has drawn a lot of inter-
est in the natural language processing and information
retrieval communities in the recent years. A series of
workshops on automatic text summarization (WAS
2000, 2001, 2002), special topic sessions in ACL,
COLING, and SIGIR, and government sponsored
evaluation efforts in the United States (DUC 2002) and
Japan (Fukusima and Okumura 2001) have advanced
the technology and produced a couple of experimental
online systems (Radev et al. 2001, McKeown et al.
2002). Despite these efforts, however, there are no
common, convenient, and repeatable evaluation meth-
ods that can be easily applied to support system devel-
opment and just-in-time comparison among different
summarization methods.
The Document Understanding Conference (DUC 2002)
run by the National Institute of Standards and Technol-
ogy (NIST) sets out to address this problem by provid-
ing annual large scale common evaluations in text
summarization. However, these evaluations involve
human judges and hence are subject to variability (Rath
et al. 1961). For example, Lin and Hovy (2002) pointed
out that 18% of the data contained multiple judgments
in the DUC 2001 single document evaluation1.
To further progress in automatic summarization, in this
paper we conduct an in-depth study of automatic
evaluation methods based on n-gram co-occurrence in
the context of DUC. Due to the setup in DUC, the
evaluations we discussed here are intrinsic evaluations
(Sparck Jones and Galliers 1996). Section 2 gives an
overview of the evaluation procedure used in DUC.
Section 3 discusses the IBM BLEU (Papineni et al.
2001) and NIST (2002) n-gram co-occurrence scoring
procedures and the application of a similar idea in
evaluating summaries. Section 4 compares n-gram co-
occurrence scoring procedures in terms of their correla-
tion to human results and on the recall and precision of
statistical significance prediction. Section 5 concludes
this paper and discusses future directions.
</bodyText>
<sectionHeader confidence="0.990348" genericHeader="method">
2 Document Understanding Conference
</sectionHeader>
<bodyText confidence="0.996107">
The 2002 Document Understanding Conference2 in-
cluded the follow two main tasks:
</bodyText>
<listItem confidence="0.995779625">
• Fully automatic single-document summarization:
given a document, participants were required to
create a generic 100-word summary. The training
set comprised 30 sets of approximately 10 docu-
ments each, together with their 100-word human
written summaries. The test set comprised 30 un-
seen documents.
• Fully automatic multi-document summarization:
</listItem>
<bodyText confidence="0.9121984">
given a set of documents about a single subject,
participants were required to create 4 generic sum-
maries of the entire set, containing 50, 100, 200,
and 400 words respectively. The document sets
were of four types: a single natural disaster event; a
</bodyText>
<footnote confidence="0.9413845">
1 Multiple judgments occur when more than one performance
score is given to the same system (or human) and human sum-
mary pairs by the same human judge.
2 DUC 2001 and DUC 2002 have similar tasks, but summaries
of 10, 50, 100, and 200 words are requested in the multi-
document task in DUC 2002.
</footnote>
<figureCaption confidence="0.999638">
Figure 1. SEE in an evaluation session.
</figureCaption>
<bodyText confidence="0.999518">
single event; multiple instances of a type of event;
and information about an individual. The training
set comprised 30 sets of approximately 10 docu-
ments, each provided with their 50, 100, 200, and
400-word human written summaries. The test set
comprised 30 unseen sets.
A total of 11 systems participated in the single-
document summarization task and 12 systems partici-
pated in the multi-document task.
</bodyText>
<subsectionHeader confidence="0.958568">
2.1 Evaluation Materials
</subsectionHeader>
<bodyText confidence="0.999910928571429">
For each document or document set, one human sum-
mary was created as the `ideal&apos; model summary at each
specified length. Two other human summaries were
also created at each length. In addition, baseline sum-
maries were created automatically for each length as
reference points. For the multi-document summariza-
tion task, one baseline, lead baseline, took the first 50,
100, 200, and 400 words in the last document in the
collection. A second baseline, coverage baseline, took
the first sentence in the first document, the first sentence
in the second document and so on until it had a sum-
mary of 50, 100, 200, or 400 words. Only one baseline
(baseline1) was created for the single document summa-
rization task.
</bodyText>
<subsectionHeader confidence="0.998618">
2.2 Summary Evaluation Environment
</subsectionHeader>
<bodyText confidence="0.999914333333333">
To evaluate system performance NIST assessors who
created the `ideal&apos; written summaries did pairwise com-
parisons of their summaries to the system-generated
summaries, other assessors&apos; summaries, and baseline
summaries. They used the Summary Evaluation Envi-
ronment (SEE) 2.0 developed by (Lin 2001) to support
the process. Using SEE, the assessors compared the
system&apos;s text (the peer text) to the ideal (the model
text). As shown in Figure 1, each text was decomposed
into a list of units and displayed in separate windows.
SEE 2.0 provides interfaces for assessors to judge both
the content and the quality of summaries. To measure
content, assessors step through each model unit, mark
all system units sharing content with the current model
unit (green/dark gray highlight in the model summary
window), and specify that the marked system units ex-
press all, most, some, or hardly any of the content of the
current model unit. To measure quality, assessors rate
grammaticality3, cohesion4, and coherence5 at five dif-
ferent levels: all, most, some, hardly any, or none6. For
example, as shown in Figure 1, an assessor marked sys-
tem units 1.1 and 10.4 (red/dark underlines in the left
pane) as sharing some content with the current model
unit 2.2 (highlighted green/dark gray in the right).
</bodyText>
<subsectionHeader confidence="0.998814">
2.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.995518777777778">
Recall at different compression ratios has been used in
summarization research to measure how well an auto-
matic system retains important content of original
documents (Mani et al. 1998). However, the simple sen-
tence recall measure cannot differentiate system per-
formance appropriately, as is pointed out by Donaway
et al. (2000). Therefore, instead of pure sentence recall
score, we use coverage score C. We define it as fol-
lows7:
</bodyText>
<equation confidence="0.391263">
(Number of MUs marked) E
•
C=
</equation>
<bodyText confidence="0.924482">
Total number of MUs in the model summary
E, the ratio of completeness, ranges from 1 to 0: 1 for
all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0
for none. If we ignore E (set it to 1), we obtain simple
sentence recall score. We use average coverage scores
derived from human judgments as the references to
evaluate various automatic scoring methods in the fol-
lowing sections.
</bodyText>
<sectionHeader confidence="0.985933" genericHeader="method">
3 BLEU and N-gram Co-Occurrence
</sectionHeader>
<bodyText confidence="0.999805428571429">
To automatically evaluate machine translations the ma-
chine translation community recently adopted an n-gram
co-occurrence scoring procedure BLEU (Papineni et al.
2001). The NIST (NIST 2002) scoring metric is based
on BLEU. The main idea of BLEU is to measure the
translation closeness between a candidate translation
and a set of reference translations with a numerical met-
ric. To achieve this goal, they used a weighted average
of variable length n-gram matches between system
translations and a set of human reference translations
and showed that a weighted average metric, i.e. BLEU,
correlating highly with human assessments.
Similarly, following the BLEU idea, we assume that the
closer an automatic summary to a professional human
</bodyText>
<footnote confidence="0.958173181818182">
3 Does the summary observe English grammatical rules inde-
pendent of its content?
4 Do sentences in the summary fit in with their surrounding
sentences?
5 Is the content of the summary expressed and organized in an
effective way?
6 These category labels are changed to numerical values of
100%, 80%, 60%, 40%, 20%, and 0% in DUC 2002.
7 DUC 2002 uses a length adjusted version of coverage metric
C&apos;, where C&apos; = α*C + (1-α)*B. B is the brevity and α is a pa-
rameter reflecting relative importance (DUC 2002).
</footnote>
<bodyText confidence="0.999914833333333">
summary, the better it is. The question is: &amp;quot;Can we ap-
ply BLEU directly without any modifications to evalu-
ate summaries as well?&amp;quot;. We first ran IBM&apos;s BLEU
evaluation script unmodified over the DUC 2001 model
and peer summary set. The resulting Spearman rank
order correlation coefficient (ρ) between BLEU and the
human assessment for the single document task is 0.66
using one reference summary and 0.82 using three ref-
erence summaries; while Spearman ρ for the multi-
document task is 0.67 using one reference and 0.70 us-
ing three. These numbers indicate that they positively
correlate at α = 0.018. Therefore, BLEU seems a prom-
ising automatic scoring metric for summary evaluation.
According to Papineni et al. (2001), BLEU is essentially
a precision metric. It measures how well a machine
translation overlaps with multiple human translations
using n-gram co-occurrence statistics. N-gram precision
in BLEU is computed as follows:
</bodyText>
<equation confidence="0.9978421">
∑ ∑ Countclip (n −gram)
C Candidates
∈ { } n gram C
− ∈
p = (2)
n ∑ ∑Count n gram
( − )
C Candidates
∈ { } n gram C
− ∈
</equation>
<bodyText confidence="0.995572571428571">
Where Countclip(n-gram) is the maximum number of n-
grams co-occurring in a candidate translation and a ref-
erence translation, and Count(n-gram) is the number of
n-grams in the candidate translation. To prevent very
short translations that try to maximize their precision
scores, BLEU adds a brevity penalty, BP, to the for-
mula:
</bodyText>
<equation confidence="0.949294125">
 1 if c r
&gt; 
 
e −
(1   ||/  ||)
r c if c r
≤

</equation>
<bodyText confidence="0.974686333333333">
Where |c |is the length of the candidate translation and
|r |is the length of the reference translation. The BLEU
formula is then written as follows:
</bodyText>
<equation confidence="0.9969292">
N
 
BLEU BP
= •exp∑ wn log pn
n=1 
</equation>
<bodyText confidence="0.999099333333333">
N is set at 4 and wn, the weighting factor, is set at 1/N.
For summaries by analogy, we can express equation (1)
in terms of n-gram matches following equation (2):
</bodyText>
<equation confidence="0.9602271">
∑ ∑ Count n gram
match ( − )
Cn C Model Units ( 5 )
∈{ } n gram C
− ∈
∑ ∑ Count n gram
( − )
C Model Units
∈{ } n gram C
− ∈
</equation>
<bodyText confidence="0.994385125">
Where Countmatch(n-gram) is the maximum number of
n-grams co-occurring in a peer summary and a model
unit and Count(n-gram) is the number of n-grams in the
model unit. Notice that the average n-gram coverage
score, Cn, as shown in equation 5 is a recall metric
8 The number of instances is 14 (11 systems, 2 humans, and 1
baseline) for the single document task and is 16 (12 systems, 2
humans, and 2 baselines) for the multi-document task.
</bodyText>
<figure confidence="0.9976227">
1
)
(
BP
(
3
)
(
4
)
</figure>
<figureCaption confidence="0.999147">
Figure 2. Scatter plot of Ngram(1,4)n score rank-
</figureCaption>
<bodyText confidence="0.9609838">
ings versus human ranking for the multi-
document task data from DUC 2001. The same
system is at each vertical line with ranking given
by different Ngram(1,4)n scores. The straight line
(AvgC) is the human ranking and n marks sum-
maries of different sizes. Ngram(1,4)all combines
results from all sizes.
instead of a precision one as pn. Since the denominator
of equation 5 is the total sum of the number of n-grams
occurring at the model summary side instead of the peer
side and only one model summary is used for each
evaluation; while there could be multiple references
used in BLEU and Count�lip(n-gram) could come from
matching different reference translations. Furthermore,
instead of a brevity penalty that punishes overly short
translations, a brevity bonus, BB, should be awarded to
shorter summaries that contain equivalent content. In
fact, a length adjusted average coverage score was used
as an alternative performance metric in DUC 2002.
However, we set the brevity bonus (or penalty) to 1 for
all our experiments in this paper. In summary, the n-
gram co-occurrence statistics we use in the following
sections are based on the following formula:
Where j ≥ i, i and j range from 1 to 4, and wn is 1/(j-
i+1). Ngram(1, 4) is a weighted variable length n-gram
match score similar to the IBM BLEU score; while
Ngram(k, k), i.e. i = j = k, is simply the average k-gram
coverage score Ck.
With these formulas, we describe how to evaluate them
in the next section.
</bodyText>
<sectionHeader confidence="0.9943075" genericHeader="method">
4 Evaluations of N-gram Co-Occurrence
Metrics
</sectionHeader>
<bodyText confidence="0.9987605">
In order to evaluate the effectiveness of automatic
evaluation metrics, we propose two criteria:
</bodyText>
<table confidence="0.996567">
SD-100 MD-All MD-50 MD-100 MD-200 MD-400
SX 0.604 0.875 0.546 0.575 0.775 0.861
S 0.615 0.832 0.646 0.529 0.814 0.843
</table>
<tableCaption confidence="0.987776">
Table 1. Spearman rank order correlation coeffi-
</tableCaption>
<bodyText confidence="0.992282">
cients of different DUC 2001 data between
Ngram(1, 4)n rankings and human rankings includ-
ing (S) and excluding (SX) stopwords. SD-100 is
for single document summaries of 100 words and
MD-50, 100, 200, and 400 are for multi-document
summaries of 50, 100, 200, and 400 words. MD-All
averages results from summaries of all sizes.
</bodyText>
<listItem confidence="0.9964545">
1. Automatic evaluations should correlate highly,
positively, and consistently with human assess-
ments.
2. The statistical significance of automatic evaluations
</listItem>
<bodyText confidence="0.9987368">
should be a good predictor of the statistical signifi-
cance of human assessments with high reliability.
The first criterion ensures whenever a human recognizes
a good summary/translation/system, an automatic
evaluation will do the same with high probability. This
enables us to use an automatic evaluation procedure in
place of human assessments to compare system per-
formance, as in the NIST MT evaluations (NIST 2002).
The second criterion is critical in interpreting the sig-
nificance of automatic evaluation results. For example,
if an automatic evaluation shows there is a significant
difference between run A and run B at α = 0.05 using
the z-test (t-test or bootstrap resampling), how does this
translate to &amp;quot;real&amp;quot; significance, i.e. the statistical signifi-
cance in a human assessment of run A and run B? Ide-
ally, we would like there to be a positive correlation
between them. If this can be asserted with strong reli-
ability (high recall and precision), then we can use the
automatic evaluation to assist system development and
to be reasonably sure that we have made progress.
</bodyText>
<subsectionHeader confidence="0.999854">
4.1 Correlation with Human Assessments
</subsectionHeader>
<bodyText confidence="0.999992277777778">
As stated in Section 3, direct application of BLEU on
the DUC 2001 data showed promising results. However,
BLEU is a precision-based metric while the human
evaluation protocol in DUC is essentially recall-based.
We therefore prefer the metric given by equation 6 and
use it in all our experiments. Using DUC 2001 data, we
compute average Ngram(1,4) scores for each peer sys-
tem at different summary sizes and rank systems ac-
cording to their scores. We then compare the
Ngram(1,4) ranking with the human ranking. Figure 2
shows the result of DUC 2001 multi-document data.
Stopwords are ignored during the computation of
Ngram(1,4) scores and words are stemmed using a Por-
ter stemmer (Porter 1980). The x-axis is the human
ranking and the y-axis gives the corresponding
Ngram(1,4) rankings for summaries of difference sizes.
The straight line marked by AvgC is the ranking given
by human assessment. For example, a system at (5,8)
</bodyText>
<figure confidence="0.99573690625">
Ngiam (1, 4)n Ranking
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Human Ranking
AvgC
Ngram(1, 4)50
Ngram(1, 4)100
Ngram(1, 4)200
Ngram(1, 4)400
Ngram(1, 4)all
(i, j) = BB •exp ∑wn log Cn
j
�
n=i
Ngram
(
6
)
</figure>
<bodyText confidence="0.992896462962963">
means that human ranks its performance at the 5th rank
while Ngram(1,4)400 ranks it at the 8th. If an automatic
ranking fully matches the human ranking, its plot will
coincide with the heavy diagonal. A line with less de-
viation from the heavy diagonal line indicates better
correlation with the human assessment.
To quantify the correlation, we compute the Spearman
rank order correlation coefficient (p) for each N-
gram(1,4)n run at different summary sizes (n). We also
test the effect of inclusion or exclusion of stopwords.
The results are summarized in Table 1.
Although these results are statistically significant (α _
0.025) and are comparable to IBM BLEU&apos;s correlation
figures shown in Section 3, they are not consistent
across summary sizes and tasks. For example, the corre-
lations of the single document task are at the 60% level;
while they range from 50% to 80% for the multi-
document task. The inclusion or exclusion of stopwords
also shows mixed results. In order to meet the require-
ment of the first criterion stated in Section 3, we need
better results.
The Ngram(1,4)n score is a weighted average of variable
length n-gram matches. By taking a log sum of the n-
gram matches, the Ngram(1,4)n favors match of longer
n-grams. For example, if &amp;quot;United States of America&amp;quot;
occurs in a reference summary, while one peer sum-
mary, A, uses &amp;quot;United States&amp;quot; and another summary, B,
uses the full phrase &amp;quot;United States of America&amp;quot;, sum-
mary B gets more contribution to its overall score sim-
ply due to the longer version of the name. However,
intuitively one should prefer a short version of the name
in summarization. Therefore, we need to change the
weighting scheme to not penalize or even reward shorter
equivalents. We conduct experiments to understand the
effect of individual n-gram co-occurrence scores in ap-
proximating human assessments. Tables 2 and 3 show
the results of these runs without and with stopwords
respectively.
For each set of DUC 2001 data, single document 100-
word summarization task, multi-document 50, 100, 200,
and 400 -word summarization tasks, we compute 4 dif-
ferent correlation statistics: Spearman rank order corre-
lation coefficient (Spearman p), linear regression t-test
(LRt, 11 degree of freedom for single document task and
13 degree of freedom for multi-document task), Pearson
product moment coefficient of correlation (Pearson p),
and coefficient of determination (CD) for each
Ngram(i,�) evaluation metric. Among them Spearman p
is a nonparametric test, a higher number indicates
higher correlation; while the other three tests are para-
metric tests. Higher LRt, Pearson p, and CD also sug-
gests higher linear correlation.
Analyzing all runs according to Tables 2 and 3, we
make the following observations:
</bodyText>
<listItem confidence="0.329207">
(1) Simple unigram, Ngram(1,1), and bi-gram,
Ngram(2,2), co-occurrence statistics consistently
</listItem>
<table confidence="0.999929357142857">
Ngram(1,4) Ngram(1,1) Ngram(2,2) Ngram(3,3) Ngram(4,4)
Single Doc Spearman p 0.604 0.989 0.868 0.527 0.505
100
LR, 1.025 7.130 2.444 0.704 0.053
Parson p 0.295 0.907 0.593 0.208 0.016
Z 0.087 0.822 0.352 0.043 0.000
Multi-Doc Spearman p 0.875 0.993 0.950 0.782 0.736
All LRt 3.910 13.230 5.830 3.356 2.480
Pearson p 0.735 0.965 0.851 0.681 0.567
CD 0.540 0.931 0.723 0.464 0.321
Multi-Doc Spearman p 0.546 0.879 0.746 0.496 0.343
50
LRt 2.142 5.681 3.350 2.846 2.664
Pearson p 0.511 0.844 0.681 0.620 0.594
CD 0.261 0.713 0.463 0.384 0.353
Multi-Doc Spearman p 0.575 0.896 0.761 0.543 0.468
100 LRt 2.369 7.873 3.641 1.828 1.385
Parson p 0.549 0.909 0.711 0.452 0.359
Z 0.301 0.827 0.505 0.204 0.129
Multi-Doc Spearman p 0.775 0.979 0.904 0.782 0.754
200
LRt 3.243 15.648 4.929 2.772 2.126
Pearson p 0.669 0.974 0.807 0.609 0.508
CD 0.447 0.950 0.651 0.371 0.258
Multi-Doc Spearman p 0.861 0.982 0.961 0.854 0.661
400 LRt 4.390 10.569 6.409 3.907 2.755
Parson p 0.773 0.946 0.872 0.735 0.607
Z 0.597 0.896 0.760 0.540 0.369
</table>
<tableCaption confidence="0.594029333333333">
Table 2. Various Ngram(i,�) rank/score correlations
for 4 different statistics (without stopwords): Spear-
man rank order coefficient correlation (Spearman p),
linear regression t-test (LRt), Pearson product mo-
ment coefficient of correlation (Pearson p), and co-
efficient of determination (CD).
</tableCaption>
<table confidence="0.999933964285714">
Ngram(1,4) Ngram (1,1) Ngram(2,2) Ngram(3,3) Ngram(4,4)
Single Doc Spearman p 0.615 0.951 0.863 0.615 0.533
100
LRt 1.076 4.873 2.228 0.942 0.246
Pearson p 0.309 0.827 0.558 0.273 0.074
CD 0.095 0.683 0.311 0.075 0.005
Multi-Doc Spearman p 0.832 0.918 0.936 0.832 0.732
All LRt 3.752 6.489 5.451 3.745 2.640
Pearson p 0.721 0.874 0.834 0.720 0.591
CD 0.520 0.764 0.696 0.519 0.349
Multi-Doc Spearman p 0.646 0.586 0.650 0.589 0.600
50
LRt 2.611 2.527 2.805 2.314 1.691
Pearson p 0.587 0.574 0.614 0.540 0.425
CD 0.344 0.329 0.377 0.292 0.180
Multi-Doc Spearman p 0.529 0.636 0.625 0.571 0.468
100 LRt 2.015 3.338 2.890 2.039 1.310
Pearson p 0.488 0.679 0.625 0.492 0.342
CD 0.238 0.462 0.391 0.242 0.117
Multi-Doc Spearman p 0.814 0.964 0.879 0.814 0.746
200
LRt 3.204 10.134 4.926 3.328 2.173
Pearson p 0.664 0.942 0.807 0.678 0.516
CD 0.441 0.888 0.651 0.460 0.266
Multi-Doc Spearman p 0.843 0.914 0.946 0.857 0.721
400 LRt 4.344 5.358 6.344 4.328 3.066
Pearson p 0.769 0.830 0.869 0.768 0.648
CD 0.592 0.688 0.756 0.590 0.420
</table>
<tableCaption confidence="0.952402">
Table 3. Various Ngram(i, J) rank/score correlations
for 4 different statistics (with stopwords).
</tableCaption>
<bodyText confidence="0.990919">
outperform (0.99 ≥ Spearman p ≥ 0.75) the
weighted average of n-gram of variable length
Ngram(1, 4) (0.88 ≥ Spearman p ≥ 0.55) in single
and multiple document tasks when stopwords are
ignored. Importantly, unigram performs especially
well with Spearman p ranging from 0.88 to 0.99
that is better than the best case in which weighted
average of variable length n-gram matches is used
and is consistent across different data sets.
(2) The performance of weighted average n-gram
scores is in the range between bi-gram and tri-gram
co-occurrence scores. This might suggest some
summaries are over-penalized by the weighted av-
erage metric due to the lack of longer n-gram
matches. For example, given a model string
&amp;quot;United States, Japan, and Taiwan&amp;quot;, a candidate
string &amp;quot;United States, Taiwan, and Japan&amp;quot; has a
unigram score of 1, bi-gram score of 0.5, and tri-
gram and 4-gram scores of 0 when the stopword
&amp;quot;and&apos; is ignored. The weighted average n-gram
score for the candidate string is 0.
(3) Excluding stopwords in computing n-gram co-
occurrence statistics generally achieves better cor-
relation than including stopwords.
</bodyText>
<subsectionHeader confidence="0.992259">
4.2 Statistical Significance of N-gram Co-
Occurrence Scores versus Human As-
sessments
</subsectionHeader>
<bodyText confidence="0.9997055">
We have shown that simple unigram, Ngram(1,1), or bi-
gram, Ngram(2,2), co-occurrence statistics based on
equation 6 outperform the weighted average of n-gram
matches, Ngram(1,4), in the previous section. To exam-
ine how well the statistical significance in the automatic
Ngram(i,�) metrics translates to real significance when
human assessments are involved, we set up the follow-
ing test procedures:
</bodyText>
<listItem confidence="0.987955052631579">
(1) Compute pairwise statistical significance test such
as z-test or t-test for a system pair (X,Y) at certain α
level, for example α = 0.05, using automatic met-
rics and human assigned scores.
(2) Count the number of cases a z-test indicates there is
a significant difference between X and Y based on
the automatic metric. Call this number NAs.
(3) Count the number of cases a z-test indicates there is
a significant difference between X and Y based on
the human assessment. Call this number Nas.
(4) Count the cases when an automatic metric predicts
a significant difference and the human assessment
also does. Call this Nhit. For example, if a z-test in-
dicates system X is significantly different from Y
with α = 0.05 based on the automatic metric scores
and the corresponding z-test also suggests the same
based on the human agreement, then we have a hit.
(5) Compute the recall and precision using the follow-
ing formulas:
</listItem>
<equation confidence="0.990898333333333">
recall =
Nas
precision =
</equation>
<bodyText confidence="0.9960701">
A good automatic metric should have high recall and
precision. This implies that if a statistical test indicates a
significant difference between two runs using the auto-
matic metric then very probably there is also a signifi-
cant difference in the manual evaluation. This would be
very useful during the system development cycle to
gauge if an improvement is really significant or not.
Figure 3 shows the recall and precision curves for the
DUC 2001 single document task at different α levels
and Figure 4 is for the multi-document task with differ-
</bodyText>
<subsectionHeader confidence="0.444712">
Significance Predication Recall and Precision Curve
Recall
</subsectionHeader>
<bodyText confidence="0.7649672">
Figure 3. Recall and precision curves of N-
gram co-occurrence statistics versus human
assessment for DUC 2001 single document
task. The 5 points on each curve represent val-
ues for the 5 ❑ levels.
</bodyText>
<subsectionHeader confidence="0.609203">
Recall
</subsectionHeader>
<bodyText confidence="0.9891541875">
Figure 4. Recall and precision curves of N-gram
co-occurrence statistics versus human assessment
for DUC 2001 multi-document task. Dark (black)
solid lines are for average of all summary sizes,
light (red) solid lines are for 50-word summaries,
dashed (green) lines are for 100-word summaries,
dash-dot lines (blue) are for 200-word summaries,
and dotted (magenta) lines are for 400-word
summaries.
ent summary sizes. Both of them exclude stopwords.
We use z-test in all the significance tests with ❑ level at
0.10, 0.05, 0.25, 0.01, and 0.005.
From Figures 3 and 4, we can see Ngram(1,1) and
Ngram(2,2) reside on the upper right corner of the recall
and precision graphs. Ngram(1,1) has the best overall
behavior. These graphs confirm Ngram(1,1) (simple
</bodyText>
<figure confidence="0.997016761904762">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Ngram(1,4)
Ngram(1,1)
Ngram(2,2)
Ngram(3,3)
Ngram(4,4)
Significance Predication Recall and Precision Curve
1
0.9
0.8
0.7
0.6
0.4
0.3
0.2
0.1
Ngram(1,4)
Ngram(1j)
Ngram(2,2)
Ngram(3,3)
Ngram(4,4)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
Precision
0.5
N
hit
N
hit
NAs
</figure>
<bodyText confidence="0.76667">
unigram) is a good automatic scoring metric with good
statistical significance prediction power.
</bodyText>
<sectionHeader confidence="0.99611" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999490230769231">
In this paper, we gave a brief introduction of the manual
summary evaluation protocol used in the Document
Understanding Conference. We then discussed the IBM
BLEU MT evaluation metric, its application to sum-
mary evaluation, and the difference between precision-
based BLEU translation evaluation and recall-based
DUC summary evaluation. The discrepancy led us to
examine the effectiveness of individual n-gram co-
occurrence statistics as a substitute for expensive and
error-prone manual evaluation of summaries. To evalu-
ate the performance of automatic scoring metrics, we
proposed two test criteria. One was to make sure system
rankings produced by automatic scoring metrics were
similar to human rankings. This was quantified by
Spearman&apos;s rank order correlation coefficient and three
other parametric correlation coefficients. Another was
to compare the statistical significance test results be-
tween automatic scoring metrics and human assess-
ments. We used recall and precision of the agreement
between the test statistics results to identify good auto-
matic scoring metrics.
According to our experiments, we found that unigram
co-occurrence statistics is a good automatic scoring
metric. It consistently correlated highly with human
assessments and had high recall and precision in signifi-
cance test with manual evaluation results. In contrast,
the weighted average of variable length n-gram matches
derived from IBM BLEU did not always give good cor-
relation and high recall and precision. We surmise that a
reason for the difference between summarization and
machine translation might be that extraction-based
summaries do not really suffer from grammar problems,
while translations do. Longer n-grams tend to score for
grammaticality rather than content.
It is encouraging to know that the simple unigram co-
occurrence metric works in the DUC 2001 setup. The
reason for this might be that most of the systems par-
ticipating in DUC generate summaries by sentence ex-
traction. We plan to run similar experiments on DUC
2002 data to see if unigram does as well. If it does, we
will make available our code available via a website to
the summarization community.
Although this study shows that unigram co-occurrence
statistics exhibit some good properties in summary
evaluation, it still does not correlate to human assess-
ment 100% of the time. There is more to be desired in
the recall and precision of significance test agreement
with manual evaluation. We are starting to explore vari-
ous metrics suggested in Donaway et al. (2000). For
example, weight n-gram matches differently according
to their information content measured by tf, tfidf, or
SVD. In fact, NIST MT automatic scoring metric (NIST
2002) already integrates such modifications.
One future direction includes using an automatic ques-
tion answer test as demonstrated in the pilot study in
SUMMAC (Mani et al. 1998). In that study, an auto-
matic scoring script developed by Chris Buckley
showed high correlation with human evaluations, al-
though the experiment was only tested on a small set of
3 topics.
According to Over (2003), NIST spent about 3,000 man
hours each in DUC 2001 and 2002 for topic and docu-
ment selection, summary creation, and manual evalua-
tion. Therefore, it would be wise to use these valuable
resources, i.e. manual summaries and evaluation results,
not only in the formal evaluation every year but also in
developing systems and designing automatic evaluation
metrics. We would like to propose an annual automatic
evaluation track in DUC that encourages participants to
invent new automated evaluation metrics. Each year the
human evaluation results can be used to evaluate the
effectiveness of the various automatic evaluation met-
rics. The best automatic metric will be posted at the
DUC website and used as an alternative in-house and
repeatable evaluation mechanism during the next year.
In this way the evaluation technologies can advance at
the same pace as the summarization technologies im-
prove.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999830138461538">
Donaway, R.L., Drummey, K.W., and Mather, L.A.
2000. A Comparison of Rankings Produced by
Summarization Evaluation Measures. In Proceeding
of the Workshop on Automatic Summarization, post-
conference workshop of ANLP-NAACL-2000, pp.
69-78, Seattle, WA, 2000.
DUC. 2002. The Document Understanding Conference.
http://duc.nist.gov.
Fukusima, T. and Okumura, M. 2001. Text Summariza-
tion Challenge: Text Summarization Evaluation at
NTCIR Workshop2. In Proceedings of the Second
NTCIR Workshop on Research in Chinese &amp; Japa-
nese Text Retrieval and Text Summarization, NII,
Tokyo, Japan, 2001.
Lin, C.-Y. 2001. Summary Evaluation Environment.
http://www.isi.edu/~cyl/SEE.
Lin, C.-Y. and E. Hovy. 2002. Manual and Automatic
Evaluations of Summaries. In Proceedings of the
Workshop on Automatic Summarization, post-
conference workshop of ACL-2002, pp. 45-51, Phila-
delphia, PA, 2002.
McKeown, K., R. Barzilay, D. Evans, V. Hatzivassi-
loglou, J. L. Klavans, A. Nenkova, C. Sable, B.
Schiffman, S. Sigelman. Tracking and Summarizing
News on a Daily Basis with Columbia&apos;s Newsblaster.
In Proceedings of Human Language Technology
Conference 2002 (HLT 2002). San Diego, CA, 2002.
Mani, I., D. House, G. Klein, L. Hirschman, L. Obrst, T.
Firmin, M. Chrzanowski, and B. Sundheim. 1998.
The TIPSTER SUMMAC Text Summarization
Evaluation: Final Report. MITRE Corp. Tech. Re-
port.
NIST. 2002. Automatic Evaluation of Machine Transla-
tion Quality using N-gram Co-Occurrence Statistics.
Over, P. 2003. Personal Communication.
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. IBM Research Report RC22176
(W0109-022).
Porter, M. F. 1980. An Algorithm for Suffix Stripping.
Program, 14, pp. 130-137.
Radev, D. R., S. Blair-Goldensohn, Z. Zhang, and R.
S. Raghavan. Newsinessence: A System for Domain-
Independent, Real-Time News Clustering and Multi-
Document Summarization. In Proceedings of human
Language Technology Conference (HLT 2001), San
Diego, CA, 2001.
Sparck Jones, K. and J. R. Galliers. 1996. Evaluating
Natural Language Processing Systems: An Analysis
and Review. New York: Springer.
Rath, G.J., Resnick, A., and Savage, T.R. 1961. The
Formation of Abstracts by the Selection of Sen-
tences. American Documentation, 12(2), pp. 139-
143. Reprinted in Mani, I., and Maybury, M., eds,
Advances in Automatic Text Summarization, MIT
Press, pp. 287-292.
WAS. 2000. Workshop on Automatic Summarization,
post-conference workshop of ANLP-NAACL-2000,
Seattle, WA, 2000.
WAS. 2001. Workshop on Automatic Summarization,
pre-conference workshop of NAACL-2001, Pitts-
burgh, PA, 2001.
WAS. 2002. Workshop on Automatic Summarization,
post-conference workshop of ACL-2002, Philadel-
phia, PA, 2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898909">
<note confidence="0.984414666666667">Proceedings of HLT-NAACL 2003 Main Papers , pp. 71-78 Edmonton, May-June 2003</note>
<title confidence="0.9975145">Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics</title>
<author confidence="0.973015">Chin-Yew Lin</author>
<author confidence="0.973015">Eduard</author>
<affiliation confidence="0.9974005">Information Sciences University of Southern</affiliation>
<address confidence="0.992061">4676 Admiralty Marina del Rey, CA 90292</address>
<email confidence="0.999232">cyl@isi.edu</email>
<email confidence="0.999232">hovy@isi.edu</email>
<abstract confidence="0.999068666666667">Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R L Donaway</author>
<author>K W Drummey</author>
<author>L A Mather</author>
</authors>
<title>A Comparison of Rankings Produced by Summarization Evaluation Measures.</title>
<date>2000</date>
<booktitle>In Proceeding of the Workshop on Automatic Summarization, postconference workshop of ANLP-NAACL-2000,</booktitle>
<pages>69--78</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="6654" citStr="Donaway et al. (2000)" startWordPosition="1039" endWordPosition="1042">ent levels: all, most, some, hardly any, or none6. For example, as shown in Figure 1, an assessor marked system units 1.1 and 10.4 (red/dark underlines in the left pane) as sharing some content with the current model unit 2.2 (highlighted green/dark gray in the right). 2.3 Evaluation Metrics Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents (Mani et al. 1998). However, the simple sentence recall measure cannot differentiate system performance appropriately, as is pointed out by Donaway et al. (2000). Therefore, instead of pure sentence recall score, we use coverage score C. We define it as follows7: (Number of MUs marked) E • C= Total number of MUs in the model summary E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none. If we ignore E (set it to 1), we obtain simple sentence recall score. We use average coverage scores derived from human judgments as the references to evaluate various automatic scoring methods in the following sections. 3 BLEU and N-gram Co-Occurrence To automatically evaluate machine translations t</context>
<context position="27800" citStr="Donaway et al. (2000)" startWordPosition="4582" endWordPosition="4585">ms participating in DUC generate summaries by sentence extraction. We plan to run similar experiments on DUC 2002 data to see if unigram does as well. If it does, we will make available our code available via a website to the summarization community. Although this study shows that unigram co-occurrence statistics exhibit some good properties in summary evaluation, it still does not correlate to human assessment 100% of the time. There is more to be desired in the recall and precision of significance test agreement with manual evaluation. We are starting to explore various metrics suggested in Donaway et al. (2000). For example, weight n-gram matches differently according to their information content measured by tf, tfidf, or SVD. In fact, NIST MT automatic scoring metric (NIST 2002) already integrates such modifications. One future direction includes using an automatic question answer test as demonstrated in the pilot study in SUMMAC (Mani et al. 1998). In that study, an automatic scoring script developed by Chris Buckley showed high correlation with human evaluations, although the experiment was only tested on a small set of 3 topics. According to Over (2003), NIST spent about 3,000 man hours each in </context>
</contexts>
<marker>Donaway, Drummey, Mather, 2000</marker>
<rawString>Donaway, R.L., Drummey, K.W., and Mather, L.A. 2000. A Comparison of Rankings Produced by Summarization Evaluation Measures. In Proceeding of the Workshop on Automatic Summarization, postconference workshop of ANLP-NAACL-2000, pp. 69-78, Seattle, WA, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>The Document Understanding Conference.</title>
<date>2002</date>
<note>http://duc.nist.gov.</note>
<contexts>
<context position="1161" citStr="DUC 2002" startWordPosition="165" endWordPosition="166">luation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results. 1 Introduction Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years. A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al. 2002). Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods. The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summariz</context>
<context position="3684" citStr="DUC 2002" startWordPosition="558" endWordPosition="559">of approximately 10 documents each, together with their 100-word human written summaries. The test set comprised 30 unseen documents. • Fully automatic multi-document summarization: given a set of documents about a single subject, participants were required to create 4 generic summaries of the entire set, containing 50, 100, 200, and 400 words respectively. The document sets were of four types: a single natural disaster event; a 1 Multiple judgments occur when more than one performance score is given to the same system (or human) and human summary pairs by the same human judge. 2 DUC 2001 and DUC 2002 have similar tasks, but summaries of 10, 50, 100, and 200 words are requested in the multidocument task in DUC 2002. Figure 1. SEE in an evaluation session. single event; multiple instances of a type of event; and information about an individual. The training set comprised 30 sets of approximately 10 documents, each provided with their 50, 100, 200, and 400-word human written summaries. The test set comprised 30 unseen sets. A total of 11 systems participated in the singledocument summarization task and 12 systems participated in the multi-document task. 2.1 Evaluation Materials For each docu</context>
<context position="8275" citStr="DUC 2002" startWordPosition="1315" endWordPosition="1316">s between system translations and a set of human reference translations and showed that a weighted average metric, i.e. BLEU, correlating highly with human assessments. Similarly, following the BLEU idea, we assume that the closer an automatic summary to a professional human 3 Does the summary observe English grammatical rules independent of its content? 4 Do sentences in the summary fit in with their surrounding sentences? 5 Is the content of the summary expressed and organized in an effective way? 6 These category labels are changed to numerical values of 100%, 80%, 60%, 40%, 20%, and 0% in DUC 2002. 7 DUC 2002 uses a length adjusted version of coverage metric C&apos;, where C&apos; = α*C + (1-α)*B. B is the brevity and α is a parameter reflecting relative importance (DUC 2002). summary, the better it is. The question is: &amp;quot;Can we apply BLEU directly without any modifications to evaluate summaries as well?&amp;quot;. We first ran IBM&apos;s BLEU evaluation script unmodified over the DUC 2001 model and peer summary set. The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference </context>
<context position="11811" citStr="DUC 2002" startWordPosition="1966" endWordPosition="1967"> the denominator of equation 5 is the total sum of the number of n-grams occurring at the model summary side instead of the peer side and only one model summary is used for each evaluation; while there could be multiple references used in BLEU and Count�lip(n-gram) could come from matching different reference translations. Furthermore, instead of a brevity penalty that punishes overly short translations, a brevity bonus, BB, should be awarded to shorter summaries that contain equivalent content. In fact, a length adjusted average coverage score was used as an alternative performance metric in DUC 2002. However, we set the brevity bonus (or penalty) to 1 for all our experiments in this paper. In summary, the ngram co-occurrence statistics we use in the following sections are based on the following formula: Where j ≥ i, i and j range from 1 to 4, and wn is 1/(ji+1). Ngram(1, 4) is a weighted variable length n-gram match score similar to the IBM BLEU score; while Ngram(k, k), i.e. i = j = k, is simply the average k-gram coverage score Ck. With these formulas, we describe how to evaluate them in the next section. 4 Evaluations of N-gram Co-Occurrence Metrics In order to evaluate the effectiven</context>
<context position="27292" citStr="DUC 2002" startWordPosition="4499" endWordPosition="4500"> IBM BLEU did not always give good correlation and high recall and precision. We surmise that a reason for the difference between summarization and machine translation might be that extraction-based summaries do not really suffer from grammar problems, while translations do. Longer n-grams tend to score for grammaticality rather than content. It is encouraging to know that the simple unigram cooccurrence metric works in the DUC 2001 setup. The reason for this might be that most of the systems participating in DUC generate summaries by sentence extraction. We plan to run similar experiments on DUC 2002 data to see if unigram does as well. If it does, we will make available our code available via a website to the summarization community. Although this study shows that unigram co-occurrence statistics exhibit some good properties in summary evaluation, it still does not correlate to human assessment 100% of the time. There is more to be desired in the recall and precision of significance test agreement with manual evaluation. We are starting to explore various metrics suggested in Donaway et al. (2000). For example, weight n-gram matches differently according to their information content meas</context>
</contexts>
<marker>DUC, 2002</marker>
<rawString>DUC. 2002. The Document Understanding Conference. http://duc.nist.gov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fukusima</author>
<author>M Okumura</author>
</authors>
<title>Text Summarization Challenge: Text Summarization Evaluation at NTCIR Workshop2.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second NTCIR Workshop on Research in Chinese &amp; Japanese Text Retrieval and Text Summarization,</booktitle>
<location>NII, Tokyo, Japan,</location>
<contexts>
<context position="1199" citStr="Fukusima and Okumura 2001" startWordPosition="169" endWordPosition="172"> cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results. 1 Introduction Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years. A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al. 2002). Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods. The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization. However, these evaluations invo</context>
</contexts>
<marker>Fukusima, Okumura, 2001</marker>
<rawString>Fukusima, T. and Okumura, M. 2001. Text Summarization Challenge: Text Summarization Evaluation at NTCIR Workshop2. In Proceedings of the Second NTCIR Workshop on Research in Chinese &amp; Japanese Text Retrieval and Text Summarization, NII, Tokyo, Japan, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>Summary Evaluation Environment.</title>
<date>2001</date>
<note>http://www.isi.edu/~cyl/SEE.</note>
<contexts>
<context position="5311" citStr="Lin 2001" startWordPosition="821" endWordPosition="822">A second baseline, coverage baseline, took the first sentence in the first document, the first sentence in the second document and so on until it had a summary of 50, 100, 200, or 400 words. Only one baseline (baseline1) was created for the single document summarization task. 2.2 Summary Evaluation Environment To evaluate system performance NIST assessors who created the `ideal&apos; written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors&apos; summaries, and baseline summaries. They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process. Using SEE, the assessors compared the system&apos;s text (the peer text) to the ideal (the model text). As shown in Figure 1, each text was decomposed into a list of units and displayed in separate windows. SEE 2.0 provides interfaces for assessors to judge both the content and the quality of summaries. To measure content, assessors step through each model unit, mark all system units sharing content with the current model unit (green/dark gray highlight in the model summary window), and specify that the marked system units express all, most, some, or hardly any of the conte</context>
</contexts>
<marker>Lin, 2001</marker>
<rawString>Lin, C.-Y. 2001. Summary Evaluation Environment. http://www.isi.edu/~cyl/SEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Manual and Automatic Evaluations of Summaries.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization, postconference workshop of ACL-2002,</booktitle>
<pages>45--51</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1905" citStr="Lin and Hovy (2002)" startWordPosition="276" endWordPosition="279">ev et al. 2001, McKeown et al. 2002). Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods. The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization. However, these evaluations involve human judges and hence are subject to variability (Rath et al. 1961). For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1. To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC. Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations (Sparck Jones and Galliers 1996). Section 2 gives an overview of the evaluation procedure used in DUC. Section 3 discusses the IBM BLEU (Papineni et al. 2001) and NIST (2002) n-gram co-occurrence scoring procedures and the applicat</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>Lin, C.-Y. and E. Hovy. 2002. Manual and Automatic Evaluations of Summaries. In Proceedings of the Workshop on Automatic Summarization, postconference workshop of ACL-2002, pp. 45-51, Philadelphia, PA, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>R Barzilay</author>
<author>D Evans</author>
<author>V Hatzivassiloglou</author>
<author>J L Klavans</author>
<author>A Nenkova</author>
<author>C Sable</author>
<author>B Schiffman</author>
<author>S Sigelman</author>
</authors>
<title>Tracking and Summarizing News on a Daily Basis with Columbia&apos;s Newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of Human Language Technology Conference</booktitle>
<location>San Diego, CA,</location>
<contexts>
<context position="1322" citStr="McKeown et al. 2002" startWordPosition="189" endWordPosition="192">le direct application of the BLEU evaluation procedure does not always give good results. 1 Introduction Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years. A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al. 2002). Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods. The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization. However, these evaluations involve human judges and hence are subject to variability (Rath et al. 1961). For example, Lin and Hovy (2002) pointed out that</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>McKeown, K., R. Barzilay, D. Evans, V. Hatzivassiloglou, J. L. Klavans, A. Nenkova, C. Sable, B. Schiffman, S. Sigelman. Tracking and Summarizing News on a Daily Basis with Columbia&apos;s Newsblaster. In Proceedings of Human Language Technology Conference 2002 (HLT 2002). San Diego, CA, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>D House</author>
<author>G Klein</author>
<author>L Hirschman</author>
<author>L Obrst</author>
<author>T Firmin</author>
<author>M Chrzanowski</author>
<author>B Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC Text Summarization Evaluation: Final Report.</title>
<date>1998</date>
<tech>MITRE Corp. Tech. Report.</tech>
<contexts>
<context position="6511" citStr="Mani et al. 1998" startWordPosition="1017" endWordPosition="1020"> any of the content of the current model unit. To measure quality, assessors rate grammaticality3, cohesion4, and coherence5 at five different levels: all, most, some, hardly any, or none6. For example, as shown in Figure 1, an assessor marked system units 1.1 and 10.4 (red/dark underlines in the left pane) as sharing some content with the current model unit 2.2 (highlighted green/dark gray in the right). 2.3 Evaluation Metrics Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents (Mani et al. 1998). However, the simple sentence recall measure cannot differentiate system performance appropriately, as is pointed out by Donaway et al. (2000). Therefore, instead of pure sentence recall score, we use coverage score C. We define it as follows7: (Number of MUs marked) E • C= Total number of MUs in the model summary E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none. If we ignore E (set it to 1), we obtain simple sentence recall score. We use average coverage scores derived from human judgments as the references to evaluat</context>
<context position="28145" citStr="Mani et al. 1998" startWordPosition="4635" endWordPosition="4638">mmary evaluation, it still does not correlate to human assessment 100% of the time. There is more to be desired in the recall and precision of significance test agreement with manual evaluation. We are starting to explore various metrics suggested in Donaway et al. (2000). For example, weight n-gram matches differently according to their information content measured by tf, tfidf, or SVD. In fact, NIST MT automatic scoring metric (NIST 2002) already integrates such modifications. One future direction includes using an automatic question answer test as demonstrated in the pilot study in SUMMAC (Mani et al. 1998). In that study, an automatic scoring script developed by Chris Buckley showed high correlation with human evaluations, although the experiment was only tested on a small set of 3 topics. According to Over (2003), NIST spent about 3,000 man hours each in DUC 2001 and 2002 for topic and document selection, summary creation, and manual evaluation. Therefore, it would be wise to use these valuable resources, i.e. manual summaries and evaluation results, not only in the formal evaluation every year but also in developing systems and designing automatic evaluation metrics. We would like to propose </context>
</contexts>
<marker>Mani, House, Klein, Hirschman, Obrst, Firmin, Chrzanowski, Sundheim, 1998</marker>
<rawString>Mani, I., D. House, G. Klein, L. Hirschman, L. Obrst, T. Firmin, M. Chrzanowski, and B. Sundheim. 1998. The TIPSTER SUMMAC Text Summarization Evaluation: Final Report. MITRE Corp. Tech. Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality using N-gram Co-Occurrence Statistics.</title>
<date>2002</date>
<tech>Personal Communication.</tech>
<location>Over, P.</location>
<contexts>
<context position="2448" citStr="NIST (2002)" startWordPosition="366" endWordPosition="367"> to variability (Rath et al. 1961). For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1. To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC. Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations (Sparck Jones and Galliers 1996). Section 2 gives an overview of the evaluation procedure used in DUC. Section 3 discusses the IBM BLEU (Papineni et al. 2001) and NIST (2002) n-gram co-occurrence scoring procedures and the application of a similar idea in evaluating summaries. Section 4 compares n-gram cooccurrence scoring procedures in terms of their correlation to human results and on the recall and precision of statistical significance prediction. Section 5 concludes this paper and discusses future directions. 2 Document Understanding Conference The 2002 Document Understanding Conference2 included the follow two main tasks: • Fully automatic single-document summarization: given a document, participants were required to create a generic 100-word summary. The tra</context>
<context position="7395" citStr="NIST 2002" startWordPosition="1169" endWordPosition="1170">= Total number of MUs in the model summary E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none. If we ignore E (set it to 1), we obtain simple sentence recall score. We use average coverage scores derived from human judgments as the references to evaluate various automatic scoring methods in the following sections. 3 BLEU and N-gram Co-Occurrence To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001). The NIST (NIST 2002) scoring metric is based on BLEU. The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric. To achieve this goal, they used a weighted average of variable length n-gram matches between system translations and a set of human reference translations and showed that a weighted average metric, i.e. BLEU, correlating highly with human assessments. Similarly, following the BLEU idea, we assume that the closer an automatic summary to a professional human 3 Does the summary observe English grammatical rules</context>
<context position="13542" citStr="NIST 2002" startWordPosition="2250" endWordPosition="2251">lts from summaries of all sizes. 1. Automatic evaluations should correlate highly, positively, and consistently with human assessments. 2. The statistical significance of automatic evaluations should be a good predictor of the statistical significance of human assessments with high reliability. The first criterion ensures whenever a human recognizes a good summary/translation/system, an automatic evaluation will do the same with high probability. This enables us to use an automatic evaluation procedure in place of human assessments to compare system performance, as in the NIST MT evaluations (NIST 2002). The second criterion is critical in interpreting the significance of automatic evaluation results. For example, if an automatic evaluation shows there is a significant difference between run A and run B at α = 0.05 using the z-test (t-test or bootstrap resampling), how does this translate to &amp;quot;real&amp;quot; significance, i.e. the statistical significance in a human assessment of run A and run B? Ideally, we would like there to be a positive correlation between them. If this can be asserted with strong reliability (high recall and precision), then we can use the automatic evaluation to assist system d</context>
<context position="27972" citStr="NIST 2002" startWordPosition="4610" endWordPosition="4611">le our code available via a website to the summarization community. Although this study shows that unigram co-occurrence statistics exhibit some good properties in summary evaluation, it still does not correlate to human assessment 100% of the time. There is more to be desired in the recall and precision of significance test agreement with manual evaluation. We are starting to explore various metrics suggested in Donaway et al. (2000). For example, weight n-gram matches differently according to their information content measured by tf, tfidf, or SVD. In fact, NIST MT automatic scoring metric (NIST 2002) already integrates such modifications. One future direction includes using an automatic question answer test as demonstrated in the pilot study in SUMMAC (Mani et al. 1998). In that study, an automatic scoring script developed by Chris Buckley showed high correlation with human evaluations, although the experiment was only tested on a small set of 3 topics. According to Over (2003), NIST spent about 3,000 man hours each in DUC 2001 and 2002 for topic and document selection, summary creation, and manual evaluation. Therefore, it would be wise to use these valuable resources, i.e. manual summar</context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co-Occurrence Statistics. Over, P. 2003. Personal Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<journal>IBM Research Report</journal>
<volume>22176</volume>
<pages>0109--022</pages>
<contexts>
<context position="2432" citStr="Papineni et al. 2001" startWordPosition="361" endWordPosition="364">dges and hence are subject to variability (Rath et al. 1961). For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1. To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC. Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations (Sparck Jones and Galliers 1996). Section 2 gives an overview of the evaluation procedure used in DUC. Section 3 discusses the IBM BLEU (Papineni et al. 2001) and NIST (2002) n-gram co-occurrence scoring procedures and the application of a similar idea in evaluating summaries. Section 4 compares n-gram cooccurrence scoring procedures in terms of their correlation to human results and on the recall and precision of statistical significance prediction. Section 5 concludes this paper and discusses future directions. 2 Document Understanding Conference The 2002 Document Understanding Conference2 included the follow two main tasks: • Fully automatic single-document summarization: given a document, participants were required to create a generic 100-word </context>
<context position="7373" citStr="Papineni et al. 2001" startWordPosition="1163" endWordPosition="1166">ws7: (Number of MUs marked) E • C= Total number of MUs in the model summary E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none. If we ignore E (set it to 1), we obtain simple sentence recall score. We use average coverage scores derived from human judgments as the references to evaluate various automatic scoring methods in the following sections. 3 BLEU and N-gram Co-Occurrence To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001). The NIST (NIST 2002) scoring metric is based on BLEU. The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric. To achieve this goal, they used a weighted average of variable length n-gram matches between system translations and a set of human reference translations and showed that a weighted average metric, i.e. BLEU, correlating highly with human assessments. Similarly, following the BLEU idea, we assume that the closer an automatic summary to a professional human 3 Does the summary observe Eng</context>
<context position="9166" citStr="Papineni et al. (2001)" startWordPosition="1465" endWordPosition="1468">ns to evaluate summaries as well?&amp;quot;. We first ran IBM&apos;s BLEU evaluation script unmodified over the DUC 2001 model and peer summary set. The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three. These numbers indicate that they positively correlate at α = 0.018. Therefore, BLEU seems a promising automatic scoring metric for summary evaluation. According to Papineni et al. (2001), BLEU is essentially a precision metric. It measures how well a machine translation overlaps with multiple human translations using n-gram co-occurrence statistics. N-gram precision in BLEU is computed as follows: ∑ ∑ Countclip (n −gram) C Candidates ∈ { } n gram C − ∈ p = (2) n ∑ ∑Count n gram ( − ) C Candidates ∈ { } n gram C − ∈ Where Countclip(n-gram) is the maximum number of ngrams co-occurring in a candidate translation and a reference translation, and Count(n-gram) is the number of n-grams in the candidate translation. To prevent very short translations that try to maximize their preci</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001. BLEU: a Method for Automatic Evaluation of Machine Translation. IBM Research Report RC22176 (W0109-022).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An Algorithm for Suffix</title>
<date>1980</date>
<journal>Stripping. Program,</journal>
<volume>14</volume>
<pages>130--137</pages>
<contexts>
<context position="14940" citStr="Porter 1980" startWordPosition="2482" endWordPosition="2483">mising results. However, BLEU is a precision-based metric while the human evaluation protocol in DUC is essentially recall-based. We therefore prefer the metric given by equation 6 and use it in all our experiments. Using DUC 2001 data, we compute average Ngram(1,4) scores for each peer system at different summary sizes and rank systems according to their scores. We then compare the Ngram(1,4) ranking with the human ranking. Figure 2 shows the result of DUC 2001 multi-document data. Stopwords are ignored during the computation of Ngram(1,4) scores and words are stemmed using a Porter stemmer (Porter 1980). The x-axis is the human ranking and the y-axis gives the corresponding Ngram(1,4) rankings for summaries of difference sizes. The straight line marked by AvgC is the ranking given by human assessment. For example, a system at (5,8) Ngiam (1, 4)n Ranking 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Human Ranking AvgC Ngram(1, 4)50 Ngram(1, 4)100 Ngram(1, 4)200 Ngram(1, 4)400 Ngram(1, 4)all (i, j) = BB •exp ∑wn log Cn j � n=i Ngram ( 6 ) means that human ranks its performance at the 5th rank while Ngram(1,4)400 ranks it at the 8th. If an automatic ranking fully match</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M. F. 1980. An Algorithm for Suffix Stripping. Program, 14, pp. 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>S Blair-Goldensohn</author>
<author>Z Zhang</author>
<author>R S Raghavan</author>
</authors>
<title>Newsinessence: A System for DomainIndependent, Real-Time News Clustering and MultiDocument Summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of human Language Technology Conference (HLT 2001),</booktitle>
<location>San Diego, CA,</location>
<contexts>
<context position="1300" citStr="Radev et al. 2001" startWordPosition="185" endWordPosition="188">stical metrics; while direct application of the BLEU evaluation procedure does not always give good results. 1 Introduction Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years. A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al. 2002). Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods. The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization. However, these evaluations involve human judges and hence are subject to variability (Rath et al. 1961). For example, Lin and Hovy (</context>
</contexts>
<marker>Radev, Blair-Goldensohn, Zhang, Raghavan, 2001</marker>
<rawString>Radev, D. R., S. Blair-Goldensohn, Z. Zhang, and R. S. Raghavan. Newsinessence: A System for DomainIndependent, Real-Time News Clustering and MultiDocument Summarization. In Proceedings of human Language Technology Conference (HLT 2001), San Diego, CA, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>K</author>
<author>J R Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review.</title>
<date>1996</date>
<publisher>Springer.</publisher>
<location>New York:</location>
<marker>Jones, K, Galliers, 1996</marker>
<rawString>Sparck Jones, K. and J. R. Galliers. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review. New York: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J Rath</author>
<author>A Resnick</author>
<author>T R Savage</author>
</authors>
<title>The Formation of Abstracts by the Selection of Sentences.</title>
<date>1961</date>
<journal>American Documentation,</journal>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<volume>12</volume>
<issue>2</issue>
<pages>139--143</pages>
<publisher>MIT Press,</publisher>
<note>Reprinted in</note>
<contexts>
<context position="1871" citStr="Rath et al. 1961" startWordPosition="270" endWordPosition="273">experimental online systems (Radev et al. 2001, McKeown et al. 2002). Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods. The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization. However, these evaluations involve human judges and hence are subject to variability (Rath et al. 1961). For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1. To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC. Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations (Sparck Jones and Galliers 1996). Section 2 gives an overview of the evaluation procedure used in DUC. Section 3 discusses the IBM BLEU (Papineni et al. 2001) and NIST (2002) n-gram co-occurrence s</context>
</contexts>
<marker>Rath, Resnick, Savage, 1961</marker>
<rawString>Rath, G.J., Resnick, A., and Savage, T.R. 1961. The Formation of Abstracts by the Selection of Sentences. American Documentation, 12(2), pp. 139-143. Reprinted in Mani, I., and Maybury, M., eds, Advances in Automatic Text Summarization, MIT Press, pp. 287-292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>WAS</author>
</authors>
<title>Workshop on Automatic Summarization, post-conference workshop of ANLP-NAACL-2000,</title>
<date>2000</date>
<location>Seattle, WA,</location>
<contexts>
<context position="1021" citStr="WAS 2000" startWordPosition="144" endWordPosition="145">the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results. 1 Introduction Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years. A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al. 2002). Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods. The Document Understanding Conference (DUC 2002) run by the National Insti</context>
</contexts>
<marker>WAS, 2000</marker>
<rawString>WAS. 2000. Workshop on Automatic Summarization, post-conference workshop of ANLP-NAACL-2000, Seattle, WA, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>WAS</author>
</authors>
<title>Workshop on Automatic Summarization, pre-conference workshop of NAACL-2001,</title>
<date>2001</date>
<location>Pittsburgh, PA,</location>
<marker>WAS, 2001</marker>
<rawString>WAS. 2001. Workshop on Automatic Summarization, pre-conference workshop of NAACL-2001, Pittsburgh, PA, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>WAS</author>
</authors>
<title>Workshop on Automatic Summarization, post-conference workshop of ACL-2002,</title>
<date>2002</date>
<location>Philadelphia, PA,</location>
<marker>WAS, 2002</marker>
<rawString>WAS. 2002. Workshop on Automatic Summarization, post-conference workshop of ACL-2002, Philadelphia, PA, 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>