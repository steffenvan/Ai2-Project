<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000274">
<title confidence="0.980373">
Character-Level Machine Translation Evaluation
for Languages with Ambiguous Word Boundaries
</title>
<author confidence="0.998491">
Chang Liu and Hwee Tou Ng
</author>
<affiliation confidence="0.999944">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.86773">
13 Computing Drive, Singapore 117417
</address>
<email confidence="0.998248">
{liuchan1,nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.96643455">
In this work, we introduce the TESLA-
CELAB metric (Translation Evaluation of
Sentences with Linear-programming-based
Analysis – Character-level Evaluation for
Languages with Ambiguous word Bound-
aries) for automatic machine translation eval-
uation. For languages such as Chinese where
words usually have meaningful internal struc-
ture and word boundaries are often fuzzy,
TESLA-CELAB acknowledges the advantage
of character-level evaluation over word-level
evaluation. By reformulating the problem in
the linear programming framework, TESLA-
CELAB addresses several drawbacks of the
character-level metrics, in particular the mod-
eling of synonyms spanning multiple char-
acters. We show empirically that TESLA-
CELAB significantly outperforms character-
level BLEU in the English-Chinese translation
evaluation tasks.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999922040816327">
Since the introduction of BLEU (Papineni et al.,
2002), automatic machine translation (MT) eval-
uation has received a lot of research interest.
The Workshop on Statistical Machine Transla-
tion (WMT) hosts regular campaigns comparing
different machine translation evaluation metrics
(Callison-Burch et al., 2009; Callison-Burch et al.,
2010; Callison-Burch et al., 2011). In the WMT
shared tasks, many new generation metrics, such as
METEOR (Banerjee and Lavie, 2005), TER (Snover
et al., 2006), and TESLA (Liu et al., 2010) have con-
sistently outperformed BLEU as judged by the cor-
relations with human judgments.
The research on automatic machine translation
evaluation is important for a number of reasons. Au-
tomatic translation evaluation gives machine trans-
lation researchers a cheap and reproducible way to
guide their research and makes it possible to com-
pare machine translation methods across different
studies. In addition, machine translation system
parameters are tuned by maximizing the automatic
scores. Some recent research (Liu et al., 2011) has
shown evidence that replacing BLEU by a newer
metric, TESLA, can improve the human judged
translation quality.
Despite the importance and the research inter-
est on automatic MT evaluation, almost all existing
work has focused on European languages, in partic-
ular on English. Although many methods aim to
be language neutral, languages with very different
characteristics such as Chinese do present additional
challenges. The most obvious challenge for Chinese
is that of word segmentation.
Unlike European languages, written Chinese is
not split into words. Segmenting Chinese sentences
into words is a natural language processing task
in its own right (Zhao and Liu, 2010; Low et al.,
2005). However, many different segmentation stan-
dards exist for different purposes, such as Microsoft
Research Asia (MSRA) for Named Entity Recog-
nition (NER), Chinese Treebank (CTB) for parsing
and part-of-speech (POS) tagging, and City Univer-
sity of Hong Kong (CITYU) and Academia Sinica
(AS) for general word segmentation and POS tag-
ging. It is not clear which standard is the best in a
given scenario.
The only prior work attempting to address the
problem of word segmentation in automatic MT
evaluation for Chinese that we are aware of is Li et
</bodyText>
<page confidence="0.96124">
921
</page>
<note confidence="0.9866945">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 921–929,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.952170166666667">
买 伞
buy umbrella
买 雨伞
buy umbrella
买 雨 伞
buy rain umbrella
</figure>
<figureCaption confidence="0.998245">
Figure 1: Three forms of the same expression buy um-
brella in Chinese
</figureCaption>
<bodyText confidence="0.9915005">
al. (2011). The work compared various MT eval-
uation metrics (BLEU, NIST, METEOR, GTM, 1
− TER) with different segmentation schemes, and
found that treating every single character as a token
(character-level MT evaluation) gives the best corre-
lation with human judgments.
</bodyText>
<sectionHeader confidence="0.985381" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999962666666667">
Li et al. (2011) identify two reasons that character-
based metrics outperform word-based metrics. For
illustrative purposes, we use Figure 1 as a running
example in this paper. All three expressions are se-
mantically identical (buy umbrella). The first two
forms are identical because 雨伞1 and 伞 are syn-
onyms. The last form is simply an (arguably wrong)
alternative segmented form of the second expres-
sion.
</bodyText>
<listItem confidence="0.997945111111111">
1. Word-based metrics do not award partial
matches, e.g., 买_雨伞 and 买_伞 would be
penalized for the mismatch between 雨伞 and
伞. Character-based metrics award the match
between characters 伞 and 伞.
2. Character-based metrics do not suffer from er-
rors and differences in word segmentation, so
买_雨伞 and 买_雨_伞 would be judged ex-
actly equal.
</listItem>
<bodyText confidence="0.9912514">
Li et al. (2011) conduct empirical experiments to
show that character-based metrics consistently out-
perform their word-based counterparts. Despite
that, we observe two important problems for the
character-based metrics:
</bodyText>
<listItem confidence="0.485245333333333">
1. Although partial matches are partially awarded,
the mechanism breaks down for n-grams where
1Literally, rain umbrella.
</listItem>
<bodyText confidence="0.981634818181818">
n &gt; 1. For example, between 买_雨_伞 and
买_伞, higher-order n-grams such as 买_雨 and
雨_伞 still have no match, and will be penal-
ized accordingly, even though 买_雨_伞 and
买_伞 should match exactly. N-grams such
as 买_雨 which cross natural word boundaries
and are meaningless by themselves can be par-
ticularly tricky.
2. Character-level metrics can utilize only a small
part of the Chinese synonym dictionary, such as
你 and 您 (you). The majority of Chinese syn-
onyms involve more than one character, such
as 雨伞 and 伞 (umbrella), and 儿童 and 小孩
(child).
In this work, we attempt to address both of these
issues by introducing TESLA-CELAB, a character-
level metric that also models word-level linguistic
phenomenon. We formulate the n-gram matching
process as a real-valued linear programming prob-
lem, which can be solved efficiently. The metric
is based on the TESLA automatic MT evaluation
framework (Liu et al., 2010; Dahlmeier et al., 2011).
</bodyText>
<sectionHeader confidence="0.980025" genericHeader="method">
3 The Algorithm
</sectionHeader>
<subsectionHeader confidence="0.999034">
3.1 Basic Matching
</subsectionHeader>
<bodyText confidence="0.9998871875">
We illustrate our matching algorithm using the ex-
amples in Figure 1. Let 买雨伞 be the reference,
and 买伞 be the candidate translation.
We use Cilin (同 义 i7 i7 林)2 as our synonym
dictionary. The basic n-gram matching problem is
shown in Figure 2. Two n-grams are connected if
they are identical, or if they are identified as syn-
onyms by Cilin. Notice that all n-grams are put in
the same matching problem regardless of n, unlike
in translation evaluation metrics designed for Eu-
ropean languages. This enables us to designate n-
grams with different values of n as synonyms, such
as 雨伞 (n = 2) and 伞 (n = 1).
In this example, we are able to make a total of two
successful matches. The recall is therefore 2/6 and
the precision is 2/3.
</bodyText>
<footnote confidence="0.989948">
2http://ir.hit.edu.cn/phpwebsite/index.php?module=pagemaster
&amp;PAGE_user_op=view_page&amp;PAGE_id=162
</footnote>
<page confidence="0.992602">
922
</page>
<figureCaption confidence="0.9999925">
Figure 2: The basic n-gram matching problem
Figure 4: A covered n-gram matching problem
</figureCaption>
<equation confidence="0.998671454545454">
wcand(买) = w(买, 买)
wcand(伞) = w(伞, 伞) + w(雨伞, 伞)
wcand(买伞) = w(买雨伞, 买伞)
买 雨 伞 买雨 雨伞 买雨伞
买 伞 买伞
买 雨 伞
买 伞
买雨 雨伞 买雨伞
买伞
雨 伞 雨伞
伞
</equation>
<bodyText confidence="0.964169">
where
</bodyText>
<figureCaption confidence="0.9834125">
Figure 3: The n-gram matching problem after phrase
matching
</figureCaption>
<subsectionHeader confidence="0.999442">
3.2 Phrase Matching
</subsectionHeader>
<bodyText confidence="0.999387357142857">
We note in Figure 2 that the trigram 买雨伞 and the
bigram 买伞 are still unmatched, even though the
match between 雨伞 and 伞 should imply the match
between 买雨伞 and 买伞.
We infer the matching of such phrases using a
dynamic programming algorithm. Two n-grams are
considered synonyms if they can be segmented into
synonyms that are aligned. With this extension,
we are able to match 买雨伞 and 买伞 (since 买
matches 买 and 雨伞 matches 伞). The matching
problem is now depicted by Figure 3.
The linear programming problem is mathemati-
cally described as follows. The variables w(·, ·) are
the weights assigned to the edges,
</bodyText>
<equation confidence="0.999346">
w(买, 买) E [0, 1]
w(伞, 伞) E [0, 1]
w(雨伞, 伞) E [0, 1]
w(买雨伞, 买伞) E [0, 1]
</equation>
<bodyText confidence="0.999503">
We require that for any node N, the sum of
weights assigned to edges linking N must not ex-
ceed one.
</bodyText>
<equation confidence="0.99988">
wref(买) = w(买, 买)
wref(伞) = w(伞, 伞)
wref(雨伞) = w(雨伞, 伞)
wref(买雨伞) = w(买雨伞, 买伞)
wref(X) E [0, 1] VX
wcand(X) E [0, 1] VX
</equation>
<bodyText confidence="0.7575215">
Now we maximize the total match,
w(买, 买)+w(伞, 伞)+w(雨伞, 伞)+w(买雨伞, 买伞)
In this example, the best match is 3, resulting in a
recall of 3/6 and a precision of 3/3.
</bodyText>
<subsectionHeader confidence="0.993574">
3.3 Covered Matching
</subsectionHeader>
<bodyText confidence="0.999931380952381">
In Figure 3, n-grams 雨 and 买雨 in the reference re-
main impossible to match, which implies misguided
penalty for the candidate translation. We observe
that since 买雨伞 has been matched, all its sub-n-
grams should be considered matched as well, includ-
ing 雨 and 买雨. We call this the covered n-gram
matching rule. This relationship is implicit in the
matching problem for English translation evaluation
metrics where words are well delimited. But with
phrase matching in Chinese, it must be modeled ex-
plicitly.
However, we cannot simply perform covered n-
gram matching as a post processing step. As an ex-
ample, suppose we are matching phrases 雨伞 and
伞, as shown in Figure 4. The linear programming
solver may come up with any of the solutions where
w(伞, 伞) + w(雨伞, 伞) = 1, w(伞, 伞) E [0, 1],
and w(雨伞, 伞) E [0, 1].
To give the maximum coverage for the node 雨,
only the solution w(伞, 伞) = 0, w(雨伞, 伞) = 1 is
accepted. This indicates the need to model covered
</bodyText>
<page confidence="0.993308">
923
</page>
<bodyText confidence="0.9987315">
n-gram matching in the linear programming prob-
lem itself.
We return to the matching of the reference 买雨
伞 and the candidate 买伞 in Figure 3. On top of the
w(·) variables already introduced, we add the vari-
ables maximum covering weights c(·). Each c(X)
represents the maximum w(Y ) variable where n-
gram Y completely covers n-gram X.
</bodyText>
<equation confidence="0.999875307692308">
cref(买) ≤ max(wref(买), wref(买雨),
wref(买雨伞))
cref(雨) ≤ max(wref(雨),wref(买雨),
wref(雨伞), wref(买雨伞))
cref(伞) ≤ max(wref(伞), wref(雨伞),
wref(买雨伞))
cref(买雨) ≤ max(wref(买雨), wref(买雨伞))
cref(雨伞) ≤ max(wref(雨伞), 雨
wref(买伞))
cref(买雨伞) ≤ wref(买雨伞)
ccand(买) ≤ max(wcand(买), wcand(买伞))
ccand(伞) ≤ max(wcand(伞), wcand(买伞))
ccand(买伞) ≤ wcand(买伞)
</equation>
<bodyText confidence="0.719563">
where
</bodyText>
<equation confidence="0.9985995">
cref(X) ∈ [0, 1] ∀X
ccand(X) ∈ [0, 1] ∀X
</equation>
<bodyText confidence="0.9996295">
However, the max(·) operator is not allowed in
the linear programming formulation. We get around
this by approximating max(·) with the sum instead.
Hence,
</bodyText>
<equation confidence="0.9988528">
cref(买) ≤ wref(买) + wref(买雨)+
wref(买雨伞)
cref(雨) ≤ wref(雨) + wref(买雨)+
wref(雨伞) + wref(买雨伞)
. . .
</equation>
<bodyText confidence="0.999742705882353">
We justify this approximation by the following
observation. Consider the sub-problem consisting
of just the w(·, ·), wref(·), wcand(·) variables and
their associated constraints. This sub-problem can
be seen as a maximum flow problem where all con-
stants are integers, hence there exists an optimal so-
lution where each of the w variables is assigned a
value of either 0 or 1. For such a solution, the
max and the sum forms are equivalent, since the
cref(·) and ccand(·) variables are also constrained to
the range [0, 1].
The maximum flow equivalence breaks down
when the c(·) variables are introduced, so in the gen-
eral case, replacing max with sum is only an approx-
imation.
Returning to our sample problem, the linear pro-
gramming solver simply needs to assign:
</bodyText>
<equation confidence="0.999817333333333">
w(买雨伞, 买伞) = 1
wref(买雨伞) = 1
wcand(买伞) = 1
</equation>
<bodyText confidence="0.99103125">
Consequently, due to the maximum covering
weights constraint, we can give the following value
assignment, implying that all n-grams have been
matched.
</bodyText>
<equation confidence="0.9990175">
cref(X) = 1 ∀X
ccand(X) = 1 ∀X
</equation>
<subsectionHeader confidence="0.940298">
3.4 The Objective Function
</subsectionHeader>
<bodyText confidence="0.9999055">
We now define our objective function in terms of
the c(·) variables. The recall is a function of
PX cref(X), and the precision is a function of
PY ccand(Y ), where X is the set of all n-grams of
the reference, and Y is the set of all n-grams of the
candidate translation.
Many prior translation evaluation metrics such as
MAXSIM (Chan and Ng, 2008) and TESLA (Liu
et al., 2010; Dahlmeier et al., 2011) use the F-0.8
measure as the final score:
</bodyText>
<equation confidence="0.872059">
= Precision × Recall
F0.8 0.8 × Precision + 0.2 × Recall
</equation>
<bodyText confidence="0.999119428571428">
Under some simplifying assumptions — specifi-
cally, that precision = recall — basic calculus shows
that F0.8 is four times as sensitive to recall than to
precision. Following the same reasoning, we want
to place more emphasis on recall than on precision.
We are also constrained by the linear programming
framework, hence we set the objective function as
</bodyText>
<equation confidence="0.982887">
!ccand(Y ) 0 &lt; f &lt; 1
1 X Xcref(X) + f
Z X Y
</equation>
<page confidence="0.980541">
924
</page>
<bodyText confidence="0.999434923076923">
We set f = 0.25 so that our objective function
is also four times as sensitive to recall than to pre-
cision.3 The value of this objective function is our
TESLA-CELAB score. Similar to the other TESLA
metrics, when there are N multiple references, we
match the candidate translation against each of them
and use the average of the N objective function val-
ues as the segment level score. System level score is
the average of all the segment level scores.
Z is a normalizing constant to scale the metric to
the range [0, 1], chosen so that when all the c(·) vari-
ables have the value of one, our metric score attains
the value of one.
</bodyText>
<sectionHeader confidence="0.999728" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.996961333333333">
In this section, we test the effectiveness of TESLA-
CELAB on some real-world English-Chinese trans-
lation tasks.
</bodyText>
<subsectionHeader confidence="0.87786">
4.1 IWSLT 2008 English-Chinese CT
</subsectionHeader>
<bodyText confidence="0.9999031">
The test set of the IWSLT 2008 (Paul, 2008)
English-Chinese ASR challenge task (CT) consists
of 300 sentences of spoken language text. The av-
erage English source sentence is 5.8 words long and
the average Chinese reference translation is 9.2 char-
acters long. The domain is travel expressions.
The test set was translated by seven MT systems,
and each translation has been manually judged for
adequacy and fluency. Adequacy measures whether
the translation conveys the correct meaning, even if
the translation is not fully fluent, whereas fluency
measures whether a translation is fluent, regardless
of whether the meaning is correct. Due to high
evaluation costs, adequacy and fluency assessments
were limited to the translation outputs of four sys-
tems. In addition, the translation outputs of the MT
systems are also manually ranked according to their
translation quality.
Inter-judge agreement is measured by the Kappa
coefficient, defined as:
</bodyText>
<equation confidence="0.9853705">
P(A) − P(E)
1 − P(E)
</equation>
<bodyText confidence="0.994026">
where P(A) is the percentage of agreement, and
P(E) is the percentage of agreement by pure
</bodyText>
<footnote confidence="0.964379333333333">
3Our empirical experiments suggest that the correlations do
plateau near this value. For simplicity, we choose not to tune f
on the training data.
</footnote>
<table confidence="0.917569">
Judgment Set 2 3
0.4406 0.4355
- 0.4134
</table>
<tableCaption confidence="0.98956">
Table 1: Inter-judge Kappa for the NIST 2008 English-
Chinese task
</tableCaption>
<bodyText confidence="0.9990866">
chance. The inter-judge Kappa is 0.41 for fluency,
0.40 for adequacy, and 0.57 for ranking. Kappa val-
ues between 0.4 and 0.6 are considered moderate,
and the numbers are in line with other comparable
experiments.
</bodyText>
<subsectionHeader confidence="0.928727">
4.2 NIST 2008 English-Chinese MT Task
</subsectionHeader>
<bodyText confidence="0.99336856">
The NIST 2008 English-Chinese MT task consists
of 127 documents with 1,830 segments, each with
four reference translations and eleven automatic
MT system translations. The data is available as
LDC2010T01 from the Linguistic Data Consortiuim
(LDC). The domain is newswire texts. The average
English source sentence is 21.5 words long and the
average Chinese reference translation is 43.2 char-
acters long.
Since no manual evaluation is given for the data
set, we recruited twelve bilingual judges to evalu-
ate the first thirty documents for adequacy and flu-
ency (355 segments for a total of 355 x 11 = 3, 905
translated segments). The final score of a sentence
is the average of its adequacy and fluency scores.
Each judge works on one quarter of the sentences so
that each translation is judged by three judges. The
judgments are concatenated to form three full sets of
judgments.
We ignore judgments where two sentences are
equal in quality, so that there are only two possible
outcomes (X is better than Y; or Y is better than X),
and P(E) = 1/2. The Kappa values are shown in
Table 1. The values indicate moderate agreement,
and are in line with other comparable experiments.
</bodyText>
<subsectionHeader confidence="0.888022">
4.3 Baseline Metrics
4.3.1 BLEU
</subsectionHeader>
<bodyText confidence="0.999859833333333">
Although word-level BLEU has often been found
inferior to the new-generation metrics when the
target language is English or other European lan-
guages, prior research has shown that character-level
BLEU is highly competitive when the target lan-
guage is Chinese (Li et al., 2011). Therefore, we
</bodyText>
<equation confidence="0.825691666666667">
Kappa =
1
2
</equation>
<page confidence="0.995359">
925
</page>
<table confidence="0.999880833333333">
Metric Type Segment Pearson Spearman rank
consistency correlation correlation
BLEU character-level 0.7004 0.9130 0.9643
TESLA-M word-level 0.6771 0.9167 0.8929
TESLA-CELAB− character-level 0.7018 0.9229 0.9643
TESLA-CELAB hybrid 0.7281* 0.9490** 0.9643
</table>
<tableCaption confidence="0.915732">
Table 2: Correlation with human judgment on the IWSLT 2008 English-Chinese challenge task. * denotes better than
the BLEU baseline at 5% significance level. ** denotes better than the BLEU baseline at 1% significance level.
</tableCaption>
<table confidence="0.999899666666667">
Metric Type Segment Pearson Spearman rank
consistency correlation correlation
BLEU character-level 0.7091 0.8429 0.7818
TESLA-M word-level 0.6969 0.8301 0.8091
TESLA-CELAB− character-level 0.7158 0.8514 0.8227
TESLA-CELAB hybrid 0.7162 0.8923** 0.8909**
</table>
<tableCaption confidence="0.9940795">
Table 3: Correlation with human judgment on the NIST 2008 English-Chinese MT task. ** denotes better than the
BLEU baseline at 1% significance level.
</tableCaption>
<bodyText confidence="0.999015692307692">
use character-level BLEU as our main baseline.
The correlations of character-level BLEU and the
average human judgments are shown in the first row
of Tables 2 and 3 for the IWSLT and the NIST
data set, respectively. Segment-level consistency is
defined as the number of correctly predicted pair-
wise rankings divided by the total number of pair-
wise rankings. Ties are excluded from the calcu-
lation. We also report the Pearson correlation and
the Spearman rank correlation of the system-level
scores. Note that in the IWSLT data set, the Spear-
man rank correlation is highly unstable due to the
small number of participating systems.
</bodyText>
<subsectionHeader confidence="0.818107">
4.3.2 TESLA-M
</subsectionHeader>
<bodyText confidence="0.999883434782609">
In addition to character-level BLEU, we also
present the correlations for the word-level metric
TESLA. Compared to BLEU, TESLA allows more
sophisticated weighting of n-grams and measures of
word similarity including synonym relations. It has
been shown to give better correlations than BLEU
for many European languages including English
(Callison-Burch et al., 2011). However, its use of
POS tags and synonym dictionaries prevents its use
at the character-level. We use TESLA as a represen-
tative of a competitive word-level metric.
We use the Stanford Chinese word segmenter
(Tseng et al., 2005) and POS tagger (Toutanova et
al., 2003) for preprocessing and Cilin for synonym
definition during matching. TESLA has several vari-
ants, and the simplest and often the most robust,
TESLA-M, is used in this work. The various cor-
relations are reported in the second row of Tables 2
and 3.
The scores show that word-level TESLA-M has
no clear advantage over character-level BLEU, de-
spite its use of linguistic features. We consider this
conclusion to be in line with that of Li et al. (2011).
</bodyText>
<subsectionHeader confidence="0.995616">
4.4 TESLA-CELAB
</subsectionHeader>
<bodyText confidence="0.999962529411765">
In all our experiments here we use TESLA-CELAB
with n-grams for n up to four, since the vast majority
of Chinese words, and therefore synonyms, are at
most four characters long.
The correlations between the TESLA-CELAB
scores and human judgments are shown in the last
row of Tables 2 and 3. We conducted significance
testing using the resampling method of (Koehn,
2004). Entries that outperform the BLEU base-
line at 5% significance level are marked with ‘*’,
and those that outperform at the 1% significance
level are marked with ‘**’. The results indicate that
TESLA-CELAB significantly outperforms BLEU.
For comparison, we also run TESLA-CELAB
without the use of the Cilin dictionary, reported
in the third row of Tables 2 and 3 and de-
noted as TESLA-CELAB−. This disables TESLA-
</bodyText>
<page confidence="0.996058">
926
</page>
<bodyText confidence="0.991729166666667">
CELAB’s ability to detect word-level synonyms and
turns TESLA-CELAB into a linear programming
based character-level metric. The performance of
TESLA-CELAB− is comparable to the character-
level BLEU baseline.
Note that
</bodyText>
<listItem confidence="0.991558285714286">
• TESLA-M can process word-level synonyms,
but does not award character-level matches.
• TESLA-CELAB− and character-level BLEU
award character-level matches, but do not con-
sider word-level synonyms.
• TESLA-CELAB can process word-level syn-
onyms and can award character-level matches.
</listItem>
<bodyText confidence="0.995734">
Therefore, the difference between TESLA-M
and TESLA-CELAB highlights the contribution
of character-level matching, and the difference
between TESLA-CELAB− and TESLA-CELAB
highlights the contribution of word-level synonyms.
</bodyText>
<subsectionHeader confidence="0.998792">
4.5 Sample Sentences
</subsectionHeader>
<bodyText confidence="0.99987825">
Some sample sentences taken from the IWSLT test
set are shown in Table 4 (some are simplified from
the original). The Cilin dictionary correctly identi-
fied the following as synonyms:
</bodyText>
<equation confidence="0.99967425">
周 = M期 week
女儿 = 17女 daughter
你 = 您 you
T作 = 上* work
</equation>
<bodyText confidence="0.983667">
The dictionary fails to recognize the following
synonyms:
</bodyText>
<equation confidence="0.998446">
一个 = 个 a
iV儿 = iV里 here
</equation>
<bodyText confidence="0.893306384615385">
However, partial awards are still given for the
matching characters iV and 个.
Based on these synonyms, TESLA-CELAB is
able to award less trivial n-gram matches, such as 下
周=下M期, 个女儿=个17女, and T作吗=上*吗,
as these pairs can all be segmented into aligned syn-
onyms. The covered n-gram matching rule is then
able to award tricky n-grams such as 下M, 个女, 个
17, 作吗 and *吗, which are covered by 下M期,
个女儿, 个17女, T作吗 and 上*吗 respectively.
Note also that the word segmentations shown in
these examples are for clarity only. The TESLA-
CELAB algorithm does not need pre-segmented
</bodyText>
<table confidence="0.998109583333333">
Reference: 下 周 。
Candidate: next week .
下 M期 。
next week .
Reference: 我 有 一个 女儿 。
I have a daughter .
Candidate: 我 有 个 17女 。
I have a daughter .
Reference: 你 在 iV儿 T作 吗 ?
you at here work qn ?
Candidate: 您 在 iV里 上* 吗 ?
you at here work qn ?
</table>
<tableCaption confidence="0.999715">
Table 4: Sample sentences from the IWSLT 2008 test set
</tableCaption>
<figure confidence="0.974133">
Schirm kaufen
umbrella buy
Regenschirm kaufen
umbrella buy
Regen schirm kaufen
rain umbrella buy
</figure>
<figureCaption confidence="0.999924">
Figure 5: Three forms of buy umbrella in German
</figureCaption>
<bodyText confidence="0.9792735">
sentences, and essentially finds multi-character syn-
onyms opportunistically.
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<subsectionHeader confidence="0.76468">
5.1 Other Languages with Ambiguous Word
Boundaries
</subsectionHeader>
<bodyText confidence="0.9998966875">
Although our experiments here are limited to Chi-
nese, many other languages have similarly ambigu-
ous word boundaries. For example, in German, the
exact counterpart to our example exists, as depicted
in Figure 5.
Regenschirm, literally rain-umbrella, is a syn-
onym of Schirm. The first two forms in Figure 5
appear in natural text, and in standard BLEU, they
would be penalized for the non-matching words
Schirm and Regenschirm. Since compound nouns
such as Regenschirm are very common in German
and generate many out-of-vocabulary words, a com-
mon preprocessing step in German translation (and
translation evaluation to a lesser extent) is to split
compound words, and we end up with the last form
Regen schirm kaufen. This process is analogous to
</bodyText>
<page confidence="0.989919">
927
</page>
<bodyText confidence="0.96143">
Chinese word segmentation.
We plan to conduct experiments on German and
other Asian languages with the same linguistic phe-
nomenon in future work.
</bodyText>
<subsectionHeader confidence="0.998866">
5.2 Fractional Similarity Measures
</subsectionHeader>
<bodyText confidence="0.9999895">
In the current formulation of TESLA-CELAB, two
n-grams X and Y are either synonyms which com-
pletely match each other, or are completely unre-
lated. In contrast, the linear-programming based
TESLA metric allows fractional similarity measures
between 0 (completely unrelated) and 1 (exact syn-
onyms). We can then award partial scores for related
words, such as those identified as such by WordNet
or those with the same POS tags.
Supporting fractional similarity measures is non-
trivial in the TESLA-CELAB framework. We plan
to address this in future work.
</bodyText>
<subsectionHeader confidence="0.987941">
5.3 Fractional Weights for N-grams
</subsectionHeader>
<bodyText confidence="0.999988714285714">
The TESLA-M metric allows each n-gram to have
a weight, which is primarily used to discount func-
tion words. TESLA-CELAB can support fractional
weights for n-grams as well by the following exten-
sion. We introduce a function m(X) that assigns a
weight in [0, 1] for each n-gram X. Accordingly, our
objective function is replaced by:
</bodyText>
<equation confidence="0.670895">
!m(Y )ccand(Y )
</equation>
<bodyText confidence="0.9948915">
where Z is a normalizing constant so that the metric
has a range of [0, 1].
</bodyText>
<equation confidence="0.997698">
XZ = m(X) + f X m(Y )
X Y
</equation>
<bodyText confidence="0.999888818181818">
However, experiments with different weight func-
tions m(·) on the test data set failed to find a better
weight function than the currently implied m(·) =
1. This is probably due to the linguistic character-
istics of Chinese, where human judges apparently
give equal importance to function words and con-
tent words. In contrast, TESLA-M found discount-
ing function words very effective for English and
other European languages such as German. We plan
to investigate this in the context of non-Chinese lan-
guages.
</bodyText>
<sectionHeader confidence="0.99821" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999902">
In this work, we devise a new MT evaluation met-
ric in the family of TESLA (Translation Evaluation
of Sentences with Linear-programming-based Anal-
ysis), called TESLA-CELAB (Character-level Eval-
uation for Languages with Ambiguous word Bound-
aries), to address the problem of fuzzy word bound-
aries in the Chinese language, although neither the
phenomenon nor the method is unique to Chinese.
Our metric combines the advantages of character-
level and word-level metrics:
</bodyText>
<listItem confidence="0.802769">
1. TESLA-CELAB is able to award scores for
partial word-level matches.
2. TESLA-CELAB does not have a segmentation
step, hence it will not introduce word segmen-
tation errors.
3. TESLA-CELAB is able to take full advantage
of the synonym dictionary, even when the syn-
onyms differ in the number of characters.
</listItem>
<bodyText confidence="0.9999195">
We show empirically that TESLA-CELAB
significantly outperforms the strong baseline
of character-level BLEU in two well known
English-Chinese MT evaluation data sets. The
source code of TESLA-CELAB is available from
http://nlp.comp.nus.edu.sg/software/.
</bodyText>
<sectionHeader confidence="0.997561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99958325">
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99101">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
</reference>
<equation confidence="0.692588666666667">
1 X X
Z m(X)cref(X) + f
X Y
</equation>
<page confidence="0.984636">
928
</page>
<reference confidence="0.999791557142857">
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM:
A maximum similarity metric for machine translation
evaluation. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011.
TESLA at WMT2011: Translation evaluation and tun-
able metric. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing.
Maoxi Li, Chengqing Zong, and Hwee Tou Ng. 2011.
Automatic evaluation of Chinese translation output:
word-level or character-level? In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics: Short Papers.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation evaluation of sentences
with linear-programming-based analysis. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Michael Paul. 2008. Overview of the iwslt 2008 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Association for Machine Trans-
lation in the Americas.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter for SIGHAN
bakeoff 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing.
Hongmei Zhao and Qun Liu. 2010. The CIPS-SIGHAN
CLP 2010 Chinese word segmentation bakeoff. In
Proceedings of the Joint Conference on Chinese Lan-
guage Processing.
</reference>
<page confidence="0.998539">
929
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918828">
<title confidence="0.999228">Character-Level Machine Translation for Languages with Ambiguous Word Boundaries</title>
<author confidence="0.998863">Liu Tou</author>
<affiliation confidence="0.999923">Department of Computer National University of</affiliation>
<address confidence="0.985904">13 Computing Drive, Singapore</address>
<email confidence="0.981502">liuchan1@comp.nus.edu.sg</email>
<email confidence="0.981502">nght@comp.nus.edu.sg</email>
<abstract confidence="0.994040571428571">In this work, we introduce the TESLA- CELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis – Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation. For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLA- CELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLA- CELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="1545" citStr="Banerjee and Lavie, 2005" startWordPosition="203" endWordPosition="206">haracters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine translation system parameters are tuned by maximizing the automatic scores. Some recent research (Liu et al., 2011) has shown e</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Findings of the 2009 workshop on statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1389" citStr="Callison-Burch et al., 2009" startWordPosition="179" endWordPosition="182">inear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different s</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 workshop on statistical machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar F Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR.</booktitle>
<contexts>
<context position="1418" citStr="Callison-Burch et al., 2010" startWordPosition="183" endWordPosition="186">TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar F. Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1448" citStr="Callison-Burch et al., 2011" startWordPosition="187" endWordPosition="190">drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine translation system parameters </context>
<context position="17821" citStr="Callison-Burch et al., 2011" startWordPosition="2923" endWordPosition="2926">e calculation. We also report the Pearson correlation and the Spearman rank correlation of the system-level scores. Note that in the IWSLT data set, the Spearman rank correlation is highly unstable due to the small number of participating systems. 4.3.2 TESLA-M In addition to character-level BLEU, we also present the correlations for the word-level metric TESLA. Compared to BLEU, TESLA allows more sophisticated weighting of n-grams and measures of word similarity including synonym relations. It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al., 2011). However, its use of POS tags and synonym dictionaries prevents its use at the character-level. We use TESLA as a representative of a competitive word-level metric. We use the Stanford Chinese word segmenter (Tseng et al., 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. TESLA has several variants, and the simplest and often the most robust, TESLA-M, is used in this work. The various correlations are reported in the second row of Tables 2 and 3. The scores show that word-level TESLA-M has no clear advantage over character-level </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>MAXSIM: A maximum similarity metric for machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association</booktitle>
<contexts>
<context position="11507" citStr="Chan and Ng, 2008" startWordPosition="1893" endWordPosition="1896"> needs to assign: w(买雨伞, 买伞) = 1 wref(买雨伞) = 1 wcand(买伞) = 1 Consequently, due to the maximum covering weights constraint, we can give the following value assignment, implying that all n-grams have been matched. cref(X) = 1 ∀X ccand(X) = 1 ∀X 3.4 The Objective Function We now define our objective function in terms of the c(·) variables. The recall is a function of PX cref(X), and the precision is a function of PY ccand(Y ), where X is the set of all n-grams of the reference, and Y is the set of all n-grams of the candidate translation. Many prior translation evaluation metrics such as MAXSIM (Chan and Ng, 2008) and TESLA (Liu et al., 2010; Dahlmeier et al., 2011) use the F-0.8 measure as the final score: = Precision × Recall F0.8 0.8 × Precision + 0.2 × Recall Under some simplifying assumptions — specifically, that precision = recall — basic calculus shows that F0.8 is four times as sensitive to recall than to precision. Following the same reasoning, we want to place more emphasis on recall than on precision. We are also constrained by the linear programming framework, hence we set the objective function as !ccand(Y ) 0 &lt; f &lt; 1 1 X Xcref(X) + f Z X Y 924 We set f = 0.25 so that our objective functio</context>
</contexts>
<marker>Chan, Ng, 2008</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A maximum similarity metric for machine translation evaluation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Chang Liu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>TESLA at WMT2011: Translation evaluation and tunable metric.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="5988" citStr="Dahlmeier et al., 2011" startWordPosition="913" endWordPosition="916">. 2. Character-level metrics can utilize only a small part of the Chinese synonym dictionary, such as 你 and 您 (you). The majority of Chinese synonyms involve more than one character, such as 雨伞 and 伞 (umbrella), and 儿童 and 小孩 (child). In this work, we attempt to address both of these issues by introducing TESLA-CELAB, a characterlevel metric that also models word-level linguistic phenomenon. We formulate the n-gram matching process as a real-valued linear programming problem, which can be solved efficiently. The metric is based on the TESLA automatic MT evaluation framework (Liu et al., 2010; Dahlmeier et al., 2011). 3 The Algorithm 3.1 Basic Matching We illustrate our matching algorithm using the examples in Figure 1. Let 买雨伞 be the reference, and 买伞 be the candidate translation. We use Cilin (同 义 i7 i7 林)2 as our synonym dictionary. The basic n-gram matching problem is shown in Figure 2. Two n-grams are connected if they are identical, or if they are identified as synonyms by Cilin. Notice that all n-grams are put in the same matching problem regardless of n, unlike in translation evaluation metrics designed for European languages. This enables us to designate ngrams with different values of n as synon</context>
<context position="11560" citStr="Dahlmeier et al., 2011" startWordPosition="1903" endWordPosition="1906">cand(买伞) = 1 Consequently, due to the maximum covering weights constraint, we can give the following value assignment, implying that all n-grams have been matched. cref(X) = 1 ∀X ccand(X) = 1 ∀X 3.4 The Objective Function We now define our objective function in terms of the c(·) variables. The recall is a function of PX cref(X), and the precision is a function of PY ccand(Y ), where X is the set of all n-grams of the reference, and Y is the set of all n-grams of the candidate translation. Many prior translation evaluation metrics such as MAXSIM (Chan and Ng, 2008) and TESLA (Liu et al., 2010; Dahlmeier et al., 2011) use the F-0.8 measure as the final score: = Precision × Recall F0.8 0.8 × Precision + 0.2 × Recall Under some simplifying assumptions — specifically, that precision = recall — basic calculus shows that F0.8 is four times as sensitive to recall than to precision. Following the same reasoning, we want to place more emphasis on recall than on precision. We are also constrained by the linear programming framework, hence we set the objective function as !ccand(Y ) 0 &lt; f &lt; 1 1 X Xcref(X) + f Z X Y 924 We set f = 0.25 so that our objective function is also four times as sensitive to recall than to p</context>
</contexts>
<marker>Dahlmeier, Liu, Ng, 2011</marker>
<rawString>Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011. TESLA at WMT2011: Translation evaluation and tunable metric. In Proceedings of the Sixth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="18925" citStr="Koehn, 2004" startWordPosition="3112" endWordPosition="3113"> row of Tables 2 and 3. The scores show that word-level TESLA-M has no clear advantage over character-level BLEU, despite its use of linguistic features. We consider this conclusion to be in line with that of Li et al. (2011). 4.4 TESLA-CELAB In all our experiments here we use TESLA-CELAB with n-grams for n up to four, since the vast majority of Chinese words, and therefore synonyms, are at most four characters long. The correlations between the TESLA-CELAB scores and human judgments are shown in the last row of Tables 2 and 3. We conducted significance testing using the resampling method of (Koehn, 2004). Entries that outperform the BLEU baseline at 5% significance level are marked with ‘*’, and those that outperform at the 1% significance level are marked with ‘**’. The results indicate that TESLA-CELAB significantly outperforms BLEU. For comparison, we also run TESLA-CELAB without the use of the Cilin dictionary, reported in the third row of Tables 2 and 3 and denoted as TESLA-CELAB−. This disables TESLA926 CELAB’s ability to detect word-level synonyms and turns TESLA-CELAB into a linear programming based character-level metric. The performance of TESLA-CELAB− is comparable to the character</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maoxi Li</author>
<author>Chengqing Zong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Automatic evaluation of Chinese translation output: word-level or character-level?</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Short Papers.</booktitle>
<contexts>
<context position="3998" citStr="Li et al. (2011)" startWordPosition="593" endWordPosition="596">dings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 921–929, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 买 伞 buy umbrella 买 雨伞 buy umbrella 买 雨 伞 buy rain umbrella Figure 1: Three forms of the same expression buy umbrella in Chinese al. (2011). The work compared various MT evaluation metrics (BLEU, NIST, METEOR, GTM, 1 − TER) with different segmentation schemes, and found that treating every single character as a token (character-level MT evaluation) gives the best correlation with human judgments. 2 Motivation Li et al. (2011) identify two reasons that characterbased metrics outperform word-based metrics. For illustrative purposes, we use Figure 1 as a running example in this paper. All three expressions are semantically identical (buy umbrella). The first two forms are identical because 雨伞1 and 伞 are synonyms. The last form is simply an (arguably wrong) alternative segmented form of the second expression. 1. Word-based metrics do not award partial matches, e.g., 买_雨伞 and 买_伞 would be penalized for the mismatch between 雨伞 and 伞. Character-based metrics award the match between characters 伞 and 伞. 2. Character-based </context>
<context position="15897" citStr="Li et al., 2011" startWordPosition="2640" endWordPosition="2643">udgments. We ignore judgments where two sentences are equal in quality, so that there are only two possible outcomes (X is better than Y; or Y is better than X), and P(E) = 1/2. The Kappa values are shown in Table 1. The values indicate moderate agreement, and are in line with other comparable experiments. 4.3 Baseline Metrics 4.3.1 BLEU Although word-level BLEU has often been found inferior to the new-generation metrics when the target language is English or other European languages, prior research has shown that character-level BLEU is highly competitive when the target language is Chinese (Li et al., 2011). Therefore, we Kappa = 1 2 925 Metric Type Segment Pearson Spearman rank consistency correlation correlation BLEU character-level 0.7004 0.9130 0.9643 TESLA-M word-level 0.6771 0.9167 0.8929 TESLA-CELAB− character-level 0.7018 0.9229 0.9643 TESLA-CELAB hybrid 0.7281* 0.9490** 0.9643 Table 2: Correlation with human judgment on the IWSLT 2008 English-Chinese challenge task. * denotes better than the BLEU baseline at 5% significance level. ** denotes better than the BLEU baseline at 1% significance level. Metric Type Segment Pearson Spearman rank consistency correlation correlation BLEU characte</context>
<context position="18538" citStr="Li et al. (2011)" startWordPosition="3046" endWordPosition="3049">se TESLA as a representative of a competitive word-level metric. We use the Stanford Chinese word segmenter (Tseng et al., 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. TESLA has several variants, and the simplest and often the most robust, TESLA-M, is used in this work. The various correlations are reported in the second row of Tables 2 and 3. The scores show that word-level TESLA-M has no clear advantage over character-level BLEU, despite its use of linguistic features. We consider this conclusion to be in line with that of Li et al. (2011). 4.4 TESLA-CELAB In all our experiments here we use TESLA-CELAB with n-grams for n up to four, since the vast majority of Chinese words, and therefore synonyms, are at most four characters long. The correlations between the TESLA-CELAB scores and human judgments are shown in the last row of Tables 2 and 3. We conducted significance testing using the resampling method of (Koehn, 2004). Entries that outperform the BLEU baseline at 5% significance level are marked with ‘*’, and those that outperform at the 1% significance level are marked with ‘**’. The results indicate that TESLA-CELAB signific</context>
</contexts>
<marker>Li, Zong, Ng, 2011</marker>
<rawString>Maoxi Li, Chengqing Zong, and Hwee Tou Ng. 2011. Automatic evaluation of Chinese translation output: word-level or character-level? In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>TESLA: Translation evaluation of sentences with linear-programming-based analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR.</booktitle>
<contexts>
<context position="1602" citStr="Liu et al., 2010" startWordPosition="214" endWordPosition="217">erforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine translation system parameters are tuned by maximizing the automatic scores. Some recent research (Liu et al., 2011) has shown evidence that replacing BLEU by a newer metric, TESLA, can</context>
<context position="5963" citStr="Liu et al., 2010" startWordPosition="909" endWordPosition="912">articularly tricky. 2. Character-level metrics can utilize only a small part of the Chinese synonym dictionary, such as 你 and 您 (you). The majority of Chinese synonyms involve more than one character, such as 雨伞 and 伞 (umbrella), and 儿童 and 小孩 (child). In this work, we attempt to address both of these issues by introducing TESLA-CELAB, a characterlevel metric that also models word-level linguistic phenomenon. We formulate the n-gram matching process as a real-valued linear programming problem, which can be solved efficiently. The metric is based on the TESLA automatic MT evaluation framework (Liu et al., 2010; Dahlmeier et al., 2011). 3 The Algorithm 3.1 Basic Matching We illustrate our matching algorithm using the examples in Figure 1. Let 买雨伞 be the reference, and 买伞 be the candidate translation. We use Cilin (同 义 i7 i7 林)2 as our synonym dictionary. The basic n-gram matching problem is shown in Figure 2. Two n-grams are connected if they are identical, or if they are identified as synonyms by Cilin. Notice that all n-grams are put in the same matching problem regardless of n, unlike in translation evaluation metrics designed for European languages. This enables us to designate ngrams with diffe</context>
<context position="11535" citStr="Liu et al., 2010" startWordPosition="1899" endWordPosition="1902"> 1 wref(买雨伞) = 1 wcand(买伞) = 1 Consequently, due to the maximum covering weights constraint, we can give the following value assignment, implying that all n-grams have been matched. cref(X) = 1 ∀X ccand(X) = 1 ∀X 3.4 The Objective Function We now define our objective function in terms of the c(·) variables. The recall is a function of PX cref(X), and the precision is a function of PY ccand(Y ), where X is the set of all n-grams of the reference, and Y is the set of all n-grams of the candidate translation. Many prior translation evaluation metrics such as MAXSIM (Chan and Ng, 2008) and TESLA (Liu et al., 2010; Dahlmeier et al., 2011) use the F-0.8 measure as the final score: = Precision × Recall F0.8 0.8 × Precision + 0.2 × Recall Under some simplifying assumptions — specifically, that precision = recall — basic calculus shows that F0.8 is four times as sensitive to recall than to precision. Following the same reasoning, we want to place more emphasis on recall than on precision. We are also constrained by the linear programming framework, hence we set the objective function as !ccand(Y ) 0 &lt; f &lt; 1 1 X Xcref(X) + f Z X Y 924 We set f = 0.25 so that our objective function is also four times as sens</context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2010</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010. TESLA: Translation evaluation of sentences with linear-programming-based analysis. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation metrics lead to better machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2133" citStr="Liu et al., 2011" startWordPosition="293" endWordPosition="296">OR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine translation system parameters are tuned by maximizing the automatic scores. Some recent research (Liu et al., 2011) has shown evidence that replacing BLEU by a newer metric, TESLA, can improve the human judged translation quality. Despite the importance and the research interest on automatic MT evaluation, almost all existing work has focused on European languages, in particular on English. Although many methods aim to be language neutral, languages with very different characteristics such as Chinese do present additional challenges. The most obvious challenge for Chinese is that of word segmentation. Unlike European languages, written Chinese is not split into words. Segmenting Chinese sentences into word</context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2011</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011. Better evaluation metrics lead to better machine translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="2828" citStr="Low et al., 2005" startWordPosition="401" endWordPosition="404">the human judged translation quality. Despite the importance and the research interest on automatic MT evaluation, almost all existing work has focused on European languages, in particular on English. Although many methods aim to be language neutral, languages with very different characteristics such as Chinese do present additional challenges. The most obvious challenge for Chinese is that of word segmentation. Unlike European languages, written Chinese is not split into words. Segmenting Chinese sentences into words is a natural language processing task in its own right (Zhao and Liu, 2010; Low et al., 2005). However, many different segmentation standards exist for different purposes, such as Microsoft Research Asia (MSRA) for Named Entity Recognition (NER), Chinese Treebank (CTB) for parsing and part-of-speech (POS) tagging, and City University of Hong Kong (CITYU) and Academia Sinica (AS) for general word segmentation and POS tagging. It is not clear which standard is the best in a given scenario. The only prior work attempting to address the problem of word segmentation in automatic MT evaluation for Chinese that we are aware of is Li et 921 Proceedings of the 50th Annual Meeting of the Associ</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to Chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1135" citStr="Papineni et al., 2002" startWordPosition="145" endWordPosition="148">guages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translatio</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
</authors>
<title>Overview of the iwslt 2008 evaluation campaign.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="12894" citStr="Paul, 2008" startWordPosition="2149" endWordPosition="2150">e N multiple references, we match the candidate translation against each of them and use the average of the N objective function values as the segment level score. System level score is the average of all the segment level scores. Z is a normalizing constant to scale the metric to the range [0, 1], chosen so that when all the c(·) variables have the value of one, our metric score attains the value of one. 4 Experiments In this section, we test the effectiveness of TESLACELAB on some real-world English-Chinese translation tasks. 4.1 IWSLT 2008 English-Chinese CT The test set of the IWSLT 2008 (Paul, 2008) English-Chinese ASR challenge task (CT) consists of 300 sentences of spoken language text. The average English source sentence is 5.8 words long and the average Chinese reference translation is 9.2 characters long. The domain is travel expressions. The test set was translated by seven MT systems, and each translation has been manually judged for adequacy and fluency. Adequacy measures whether the translation conveys the correct meaning, even if the translation is not fully fluent, whereas fluency measures whether a translation is fluent, regardless of whether the meaning is correct. Due to hi</context>
</contexts>
<marker>Paul, 2008</marker>
<rawString>Michael Paul. 2008. Overview of the iwslt 2008 evaluation campaign. In Proceedings of the International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="1572" citStr="Snover et al., 2006" startWordPosition="208" endWordPosition="211">hat TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine translation system parameters are tuned by maximizing the automatic scores. Some recent research (Liu et al., 2011) has shown evidence that replacing BLEU</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18090" citStr="Toutanova et al., 2003" startWordPosition="2968" endWordPosition="2971">to character-level BLEU, we also present the correlations for the word-level metric TESLA. Compared to BLEU, TESLA allows more sophisticated weighting of n-grams and measures of word similarity including synonym relations. It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al., 2011). However, its use of POS tags and synonym dictionaries prevents its use at the character-level. We use TESLA as a representative of a competitive word-level metric. We use the Stanford Chinese word segmenter (Tseng et al., 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. TESLA has several variants, and the simplest and often the most robust, TESLA-M, is used in this work. The various correlations are reported in the second row of Tables 2 and 3. The scores show that word-level TESLA-M has no clear advantage over character-level BLEU, despite its use of linguistic features. We consider this conclusion to be in line with that of Li et al. (2011). 4.4 TESLA-CELAB In all our experiments here we use TESLA-CELAB with n-grams for n up to four, since the vast majority of Chinese words, and therefore </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for SIGHAN bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="18050" citStr="Tseng et al., 2005" startWordPosition="2961" endWordPosition="2964"> systems. 4.3.2 TESLA-M In addition to character-level BLEU, we also present the correlations for the word-level metric TESLA. Compared to BLEU, TESLA allows more sophisticated weighting of n-grams and measures of word similarity including synonym relations. It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al., 2011). However, its use of POS tags and synonym dictionaries prevents its use at the character-level. We use TESLA as a representative of a competitive word-level metric. We use the Stanford Chinese word segmenter (Tseng et al., 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. TESLA has several variants, and the simplest and often the most robust, TESLA-M, is used in this work. The various correlations are reported in the second row of Tables 2 and 3. The scores show that word-level TESLA-M has no clear advantage over character-level BLEU, despite its use of linguistic features. We consider this conclusion to be in line with that of Li et al. (2011). 4.4 TESLA-CELAB In all our experiments here we use TESLA-CELAB with n-grams for n up to four, since the vast m</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for SIGHAN bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongmei Zhao</author>
<author>Qun Liu</author>
</authors>
<title>The CIPS-SIGHAN CLP 2010 Chinese word segmentation bakeoff.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Conference on Chinese Language Processing.</booktitle>
<contexts>
<context position="2809" citStr="Zhao and Liu, 2010" startWordPosition="397" endWordPosition="400"> TESLA, can improve the human judged translation quality. Despite the importance and the research interest on automatic MT evaluation, almost all existing work has focused on European languages, in particular on English. Although many methods aim to be language neutral, languages with very different characteristics such as Chinese do present additional challenges. The most obvious challenge for Chinese is that of word segmentation. Unlike European languages, written Chinese is not split into words. Segmenting Chinese sentences into words is a natural language processing task in its own right (Zhao and Liu, 2010; Low et al., 2005). However, many different segmentation standards exist for different purposes, such as Microsoft Research Asia (MSRA) for Named Entity Recognition (NER), Chinese Treebank (CTB) for parsing and part-of-speech (POS) tagging, and City University of Hong Kong (CITYU) and Academia Sinica (AS) for general word segmentation and POS tagging. It is not clear which standard is the best in a given scenario. The only prior work attempting to address the problem of word segmentation in automatic MT evaluation for Chinese that we are aware of is Li et 921 Proceedings of the 50th Annual Me</context>
</contexts>
<marker>Zhao, Liu, 2010</marker>
<rawString>Hongmei Zhao and Qun Liu. 2010. The CIPS-SIGHAN CLP 2010 Chinese word segmentation bakeoff. In Proceedings of the Joint Conference on Chinese Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>