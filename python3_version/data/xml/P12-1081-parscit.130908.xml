<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005967">
<title confidence="0.9990415">
Attacking Parsing Bottlenecks with Unlabeled Data and Relevant
Factorizations
</title>
<author confidence="0.995319">
Emily Pitler
</author>
<affiliation confidence="0.9971325">
Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.88994">
Philadelphia, PA 19104
</address>
<email confidence="0.999575">
epitler@seas.upenn.edu
</email>
<sectionHeader confidence="0.995452" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995621684210526">
Prepositions and conjunctions are two of
the largest remaining bottlenecks in parsing.
Across various existing parsers, these two
categories have the lowest accuracies, and
mistakes made have consequences for down-
stream applications. Prepositions and con-
junctions are often assumed to depend on lex-
ical dependencies for correct resolution. As
lexical statistics based on the training set only
are sparse, unlabeled data can help amelio-
rate this sparsity problem. By including un-
labeled data features into a factorization of
the problem which matches the representation
of prepositions and conjunctions, we achieve
a new state-of-the-art for English dependen-
cies with 93.55% correct attachments on the
current standard. Furthermore, conjunctions
are attached with an accuracy of 90.8%, and
prepositions with an accuracy of 87.4%.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999663531914894">
Prepositions and conjunctions are two large remain-
ing bottlenecks in parsing. Across various exist-
ing parsers, these two categories have the lowest
accuracies, and mistakes made on these have con-
sequences for downstream applications. Machine
translation is sensitive to parsing errors involving
prepositions and conjunctions, because in some lan-
guages different attachment decisions in the parse
of the source language sentence produce differ-
ent translations. Preposition attachment mistakes
are particularly bad when translating into Japanese
(Schwartz et al., 2003) which uses a different post-
position for different attachments; conjunction mis-
takes can cause word ordering mistakes when trans-
lating into Chinese (Huang, 1983).
Prepositions and conjunctions are often assumed
to depend on lexical dependencies for correct resolu-
tion (Jurafsky and Martin, 2008). However, lexical
statistics based on the training set only are typically
sparse and have only a small effect on overall pars-
ing performance (Gildea, 2001). Unlabeled data can
help ameliorate this sparsity problem. Backing off
to cluster membership features (Koo et al., 2008) or
by using association statistics from a larger corpus,
such as the web (Bansal and Klein, 2011; Zhou et
al., 2011), have both improved parsing.
Unlabeled data has been shown to improve the ac-
curacy of conjunctions within complex noun phrases
(Pitler et al., 2010; Bergsma et al., 2011). How-
ever, it has so far been less effective within full
parsing — while first-order web-scale counts notice-
ably improved overall parsing in Bansal and Klein
(2011), the accuracy on conjunctions actually de-
creased when the web-scale features were added
(Table 4 in that paper).
In this paper we show that unlabeled data can help
prepositions and conjunctions, provided that the de-
pendency representation is compatible with how the
parsing problem is decomposed for learning and in-
ference. By incorporating unlabeled data into factor-
izations which capture the relevant dependencies for
prepositions and conjunctions, we produce a parser
for English which has an unlabeled attachment ac-
curacy of 93.5%, over an 18% reduction in error
over the best previously published parser (Bansal
and Klein, 2011) on the current standard for depen-
dency parsing. The best model for conjunctions at-
</bodyText>
<page confidence="0.950782">
768
</page>
<note confidence="0.985876">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 768–776,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999270333333333">
taches them with 90.8% accuracy (42.5% reduction
in error over MSTParser), and the best model for
prepositions with 87.4% accuracy (18.2% reduction
in error over MSTParser).
We describe the dependency representations of
prepositions and conjunctions in Section 2. We dis-
cuss the implications of these representations for
how learning and inference for parsing are decom-
posed (Section 3) and how unlabeled data may be
used (Section 4). We then present experiments ex-
ploring the connection between representation, fac-
torization, and unlabeled data in Sections 5 and 6.
</bodyText>
<sectionHeader confidence="0.996573" genericHeader="method">
2 Dependency Representations
</sectionHeader>
<bodyText confidence="0.999848">
A dependency tree is a rooted, directed tree (or ar-
borescence), in which the vertices are the words in
the sentence plus an artificial root node, and each
edge (h, m) represents a directed dependency rela-
tion from the head h to the modifier m. Through-
out this section, we will use Y to denote a particular
parse tree, and (h, m) E Y to denote a particular
edge in Y .
The Wall Street Journal Penn Treebank (PTB)
(Marcus et al., 1993) contains parsed constituency
trees (where each sentence is represented as a
context-free-grammar derivation). Dependency
parsing requires a conversion from these con-
stituency trees to dependency trees. The Tree-
bank constituency trees left noun phrases (NPs)
flat, although there have been subsequent projects
which annotate the internal structure of noun phrases
(Vadas and Curran, 2007; Weischedel et al., 2011).
The presence or absence of these noun phrase in-
ternal annotations interacts with constituency-to-
dependency conversion program in ways which have
effects on conjunctions and prepositions.
We consider two such mapping regimes here:
</bodyText>
<listItem confidence="0.9112795">
1. PTB trees —* Penn2Malt1 —* Dependencies
2. PTB trees patched with NP-internal annota-
tions (Vadas and Curran, 2007) —* penncon-
verter2 —* Dependencies
</listItem>
<footnote confidence="0.97515325">
1http://w3.msi.vxu.se/˜nivre/research/
Penn2Malt.html
2Johansson and Nugues (2007) http://nlp.cs.lth.
se/software/treebank_converter/
</footnote>
<bodyText confidence="0.992692675675676">
Regime (1) is very commonly done in papers
which report dependency parsing experiments (e.g.,
(McDonald and Pereira, 2006; Nivre et al., 2007;
Zhang and Clark, 2008; Huang and Sagae, 2010;
Koo and Collins, 2010)). Penn2Malt uses the head
finding table from Yamada and Matsumoto (2003).
Regime (2) is based on the recommendations of
the two converter tools; as of the date of this writing,
the Penn2Malt website says: “Penn2Malt has been
superseded by the more sophisticated pennconverter,
which we strongly recommend”. The pennconverter
website “strongly recommends” patching the Tree-
bank with the NP annotations of Vadas and Curran
(2007). A version of pennconverter was used to pre-
pare the data for the CoNLL Shared Tasks of 2007-
2009, so the trees produced by Regime 2 are similar
(but not identical)3 to these shared tasks. As far as
we are aware, Bansal and Klein (2011) is the only
published work which uses both steps in Regime (2).
The dependency representations produced by
Regime 2 are designed to be more useful for ex-
tracting semantics (Johansson and Nugues, 2007).
The parsing attachment accuracy of MALTPARSER
(Nivre et al., 2007) was lower using pennconverter
than Penn2Malt, but using the output of MALT-
PARSER under the new format parses produces a
much better semantic role labeler than using its out-
put with Penn2Malt (Johansson and Nugues, 2007).
Figures 1 and 2 show how conjunctions and
prepositions, respectively, are represented after the
two different conversion processes. These differ-
ences are not rare–70.7% of conjunctions and 5.2%
of prepositions in the development set have a differ-
ent parent under the two conversion types. These
representational differences have serious implica-
tions for how well various factorizations will be able
to capture these two phenomena.
</bodyText>
<subsectionHeader confidence="0.728265">
3 Implications of Representations on the
Scope of Factorization
</subsectionHeader>
<bodyText confidence="0.9992685">
Parsing requires a) learning to score potential parse
trees, and b) given a particular scoring function,
finding the highest scoring tree according to that
function. The number of potential trees for a sen-
</bodyText>
<footnote confidence="0.982021666666667">
3The CoNLL data does not include the NP annotations; it
does include annotations of named entities (Weischedel and
Brunstein, 2005) so had some internal NP edges.
</footnote>
<page confidence="0.995467">
769
</page>
<figure confidence="0.999685431372549">
Conversion 1
Conversion 2
Conversion 2 Conversion 1
Committee
(a)
the House
Ways
and Means
Committee
the House
Ways
and
plan
in
law
(a)
plan
in
law
(b)
here
debt
notes and other
(c)
yesterday
opening of
trading
(c)
opening
here yesterday
trading
(d)
of
Means
(b)
whose
plans
for
sell
sell
or
600 by
merge
(f)
plans
for
whose
issues
(f)
issues
(e)
</figure>
<figureCaption confidence="0.78625575">
Figure 2: Examples of prepositions: plan in the S&amp;L
bailout law, opening of trading here yesterday, and whose
plans for major rights issues. The preposition is bolded
and the (semantic) head is underlined.
</figureCaption>
<figure confidence="0.982811428571429">
notes
and
debt
other
(d)
or merge 600 by
(e)
</figure>
<figureCaption confidence="0.856299">
Figure 1: Examples of conjunctions: the House Ways
and Means Committee, notes and other debt, and sell or
merge 600 by. The conjunction is bolded, the left con-
junct (in the linear order of the sentence) is underlined,
and the right conjunct is italicized.
</figureCaption>
<bodyText confidence="0.999976090909091">
tence is exponential, so parsing is made tractable by
decomposing the problem into a set of local sub-
structures which can be combined using dynamic
programming. Four possible factorizations are: sin-
gle edges (edge-based), pairs of edges which share
a parent (siblings), pairs of edges where the child
of one is the parent of the other (grandparents), and
triples of edges where the child of one is the parent
of two others (grandparent+sibling). In this section,
we discuss these factorizations and their relevance
to conjunction and preposition representations.
</bodyText>
<subsectionHeader confidence="0.994651">
3.1 Edge-based Scoring
</subsectionHeader>
<bodyText confidence="0.999950333333333">
One possible factorization corresponds to first-order
parsing, in which the score of a parse tree Y decom-
poses completely across the edges in the tree:
</bodyText>
<equation confidence="0.9791395">
S(Y ) � � S(h, m) (1)
(h,M)EY
</equation>
<bodyText confidence="0.997658409090909">
Conjunctions: Under Conversion 1, we can see
three different representations of conjunctions in
Figures 1(a), 1(c), and 1(e). Under edge-based scor-
ing, the conjunction would be scored along with nei-
ther of its conjuncts in 1(a). In Figure 1(c), the con-
junction is scored along with its right conjunct only;
in figure 1(e) along with its left conjunct only. The
inconsistency here is likely to make learning more
difficult, as what is learned is split across these three
cases. Furthermore, the conjunction is connected
with an edge to either zero or one of its two argu-
ments; at least one of the arguments is completely
ignored in terms of scoring the conjunction.
In Figures 1(c) and 1(e), the words being con-
joined are connected to each other by an edge. This
overloads the meaning of an edge; an edge indicates
both a head-modifier relationship and a conjunction
relationship. For example, compare the two natural
phrases dogs and cats and really nice. dogs and cats
are a good pair to conjoin, but cats is not a good
modifier for dogs, so there is a tension when scoring
an edge like (dogs, cats): it should get a high score
</bodyText>
<page confidence="0.969577">
770
</page>
<bodyText confidence="0.999967764705882">
when actually indicating a conjunction and low oth-
erwise. (nice, really) shows the opposite pattern–
really is a good modifier for nice, but nice and re-
ally are not two words which should be conjoined.
This may be partially compensated for by including
features about the surrounding words (McDonald et
al., 2005), but any feature templates which would be
identical across the two contexts will be in tension.
In Figures 1(b), 1(d) and 1(f), the conjunction par-
ticipates in a directed edge with each of the con-
juncts. Thus, in edge-based scoring, at least under
Conversion 2 neither of the conjuncts is being ig-
nored; however, the factorization scores each edge
independently, so how compatible these two con-
juncts are with each other cannot be included in the
scoring of a tree.
Prepositions: For all of the examples in Figure 2,
there is a directed edge from the head of the phrase
that the preposition modifies to the preposition. Dif-
ferences in head finding rules account for the dif-
ferences in preposition representations. In the sec-
ond example, the first conversion scheme chooses
yesterday as the head of the overall NP, resulting in
the edge yesterday—* of, while the second conver-
sion scheme ignores temporal phrases when finding
the head, resulting in the more semantically mean-
ingful opening—*of. Similarly, in the third example,
the preposition for attaches to the pronoun whose in
the first conversion scheme, while it attaches to the
noun plans in the second.
With edge-based scoring, the object is not acces-
sible when scoring where the preposition should at-
tach, and PP-attachment is known to depend on the
object of the preposition (Hindle and Rooth, 1993).
</bodyText>
<subsectionHeader confidence="0.999963">
3.2 Sibling Scoring
</subsectionHeader>
<bodyText confidence="0.999989571428572">
Another alternative factorization is to score sib-
lings as well as parent-child edges (McDonald and
Pereira, 2006). Scores decompose as:
where Sib(Y) is the set containing ordered and ad-
jacent sibling pairs in Y : if (m, s) E Sib(Y), there
must exist a shared parent h such that (h, m) E Y
and (h, s) E Y , m and s must be on the same side
of h, m must be closer to h than s in the linear order
of the sentence, and there must not exist any other
children of h in between m and s.
Under this factorization, two of the three ex-
amples in Conversion 1 (and none of the exam-
ples in Conversion 2) in Figure 1 now include the
conjunction and both conjuncts in the same score
(Figures 1(c) and 1(e)). The scoring for head-
modifier dependencies and conjunction dependen-
cies are again being overloaded: (debt, notes, and)
and (debt, and, other) are both sibling parts in Fig-
ure 1(c), yet only one of them represents a conjunc-
tion. The position of the conjunction in the sibling
is not enough to determine whether one is scoring a
true conjunction relation or just the conjunction and
a different sibling; in 1(c) the conjunction is on the
right of its sibling argument, while in 1(e) the con-
junction is on the left.
For none of the other preposition or conjunc-
tion examples does a sibling factorization bring
more of the arguments into the scope of what is
scored along with the preposition/conjunction. Sib-
ling scoring may have some benefit in that preposi-
tions/conjunctions should have only one argument,
so for prepositions (under both conversions) and
conjunctions (under Conversion 2), the model can
learn to disprefer the existence of any siblings and
thus enforce choosing a single child.
</bodyText>
<subsectionHeader confidence="0.998716">
3.3 Grandparent Scoring
</subsectionHeader>
<bodyText confidence="0.999952">
Another alternative over pairs of edges scores grand-
parents instead of siblings, with factorization:
</bodyText>
<equation confidence="0.950692">
�
S(Y ) =
{ )
(h, m, c) (h, m) E Y, (m, c) E Y
</equation>
<bodyText confidence="0.999011357142857">
Under Conversion 2, we would expect this fac-
torization to perform much better on conjunctions
and prepositions than edge-based or sibling-based
factorizations. Both conjunctions and prepositions
are consistently represented by exactly one grand-
parent relation (with one relevant argument as the
grandparent, the preposition/conjunction as the par-
ent, and the other argument as the child), so this is
the first factorization that has allowed the compati-
bility of the two arguments to affect the attachment
of the preposition/conjunction.
Under Conversion 1, this factorization is particu-
larly appropriate for prepositions, but would be un-
likely to help conjunctions, which have no children.
</bodyText>
<equation confidence="0.9930275">
S(Y ) = � S(h, m, s) (2)
I (h, m, s) (h, m) E Y, (h, s) E Y, 1
(m, s) E Sib(Y)
S(h, m, c) (3)
</equation>
<page confidence="0.985711">
771
</page>
<subsectionHeader confidence="0.939625">
3.4 Grandparent-Sibling Scoring
</subsectionHeader>
<bodyText confidence="0.999735285714286">
A further widening of the factorization takes grand-
parents and siblings simultaneously:
For projective parsing, dynamic programming for
this factorization was derived in Koo and Collins
(2010) (Model 1 in that paper), and for non-
projective parsing, dual decomposition was used for
this factorization in Koo et al. (2010).
This factorization should combine all the ben-
efits of the sibling and grandparent factorizations
described above–for Conversion 1, sibling scoring
may help conjunctions and grandparent scoring may
help prepositions, and for Conversion 2, grandparent
scoring should help both, while sibling scoring may
or may not add some additional gains.
</bodyText>
<sectionHeader confidence="0.950135" genericHeader="method">
4 Using Unlabeled Data Effectively
</sectionHeader>
<bodyText confidence="0.999673">
Associations from unlabeled data have the poten-
tial to improve both conjunctions and prepositions.
We predict that web counts which include both con-
juncts (for conjunctions), or which include both the
attachment site and the object of a preposition (for
prepositions) will lead to the largest improvements.
For the phrase dogs and cats, edge-based counts
would measure the associations between dogs and
and, and and and cats, but never any web counts
that include both dogs and cats. For the phrase ate
spaghetti with a fork, edge-based scoring would not
use any web counts involving both ate and fork.
We use associations rather than raw counts. The
phrases trading and transacting versus trading and
what provide an example of the difference between
associations and counts. The phrase trading and
what has a higher count than the phrase trading and
transacting, but trading and transacting are more
highly associated. In this paper, we use point-wise
mutual information (PMI) to measure the strength of
associations of words participating in potential con-
junctions or prepositions.4 For three words h, m, c,
this is calculated with:
</bodyText>
<equation confidence="0.998574">
P (h .* m .*c)
PMI(h,m,c) = log P(h)P(m)P(c) (5)
</equation>
<footnote confidence="0.560754666666667">
4PMI can be unreliable when frequency counts are small
(Church and Hanks, 1990), however the data used was thresh-
olded, so all counts used are at least 10.
</footnote>
<bodyText confidence="0.999639315789474">
The probabilities are estimated using web-scale
n-gram counts, which are looked up using the
tools and web-scale n-grams described in Lin et al.
(2010). Defining the joint probability using wild-
cards (rather than the exact sequence h m c) is
crucially important, as determiners, adjectives, and
other words may naturally intervene between the
words of interest.
Approaches which cluster words (i.e., Koo et
al. (2008)) are also designed to identify words
which are semantically related. As manually labeled
parsed data is sparse, this may help generalize across
similar words. However, if edges are not connected
to the semantic head, cluster-based methods may be
less effective. For example, the choice of yesterday
as the head of opening of trading here yesterday in
Figure 2(c) or whose in 2(e) may make cluster-based
features less useful than if the semantic heads were
chosen (opening and plans, respectively).
</bodyText>
<sectionHeader confidence="0.999355" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999962333333333">
The previous section motivated the use of unlabeled
data for attaching prepositions and conjunctions. We
have also hypothesized that these features will be
most effective when the data representation and the
learning representation both capture relevant prop-
erties of prepositions and conjunctions. We predict
that Conversion 2 and a factorization which includes
grand-parent scoring will achieve the highest perfor-
mance. In this section, we investigate the impact
of unlabeled data on parsing accuracy using the two
conversions and using each of the factorizations de-
scribed in Section 3.1-3.4.
</bodyText>
<subsectionHeader confidence="0.989222">
5.1 Unlabeled Data Feature Set
</subsectionHeader>
<bodyText confidence="0.9978021">
Clusters: We replicate the cluster-based features
from Koo et al. (2008), which includes features over
all edges (h, m), grand-parent triples (h, m, c), and
parent sibling triples (h, m, s). The features were
all derived from the publicly available clusters pro-
duced by running the Brown clustering algorithm
(Brown et al., 1992) over the BLLIP corpus with the
Penn Treebank sentences excluded.5
Preposition and conjunction-inspired features
(motivated by Section 4) are described below:
</bodyText>
<equation confidence="0.8350916">
5people.csail.mit.edu/maestro/papers/
bllip-clusters.gz
S(Y ) = � S(g, h, m, s) (4)
I (g, h, m, s) (g, h) E Y, (h, m) E Y, I
(h, s) E Y, (m, s) E Sib(Y)
</equation>
<page confidence="0.988129">
772
</page>
<bodyText confidence="0.999774684210527">
Web Counts: For each set of words of interest, we
compute the PMI between the words, and then in-
clude binary features for whether the mutual infor-
mation is undefined, if it is negative, and whether it
is greater than each positive integer.
For conjunctions, we only do this for triples of
both conjunct and the conjunction (and if the con-
junction is and or or and the two potential conjuncts
are the same coarse grained part-of-speech). For
prepositions, we consider only cases in which the
parent is a noun or a verb and the child is a noun
(this corresponds to the cases considered by Hindle
and Rooth (1993) and others). Prepositions use as-
sociation features to score both the triple (parent,
preposition, child) and all pairs within that triple.
The counts features are not used if all the words in-
volved are stopwords. For the scope of this paper we
use only the above counts related to prepositions and
conjunctions.
</bodyText>
<subsectionHeader confidence="0.986731">
5.2 Parser
</subsectionHeader>
<bodyText confidence="0.9999816">
We use the Model 1 version of dpo3, a state-of-the-
art third-order dependency parser (Koo and Collins,
2010))6. We augment the feature set used with the
web-counts-based features relevant to prepositions
and conjunctions and the cluster-based features. The
only other change to the parser’s existing feature set
was the addition of binary features for the part-of-
speech tag of the child of the root node, alone and
conjoined with the tags of its children. For further
details about the parser, see Koo and Collins (2010).
</bodyText>
<subsectionHeader confidence="0.947576">
5.3 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.999957153846154">
Training was done on Section 2-21 of the Penn
Treebank. Section 22 was used for development,
and Section 23 for test. We use automatic part-
of-speech tags for both training and testing (Rat-
naparkhi, 1996). The set of potential edges was
pruned using the marginals produced by a first-order
parser trained using exponentiated gradient descent
(Collins et al., 2008) as in Koo and Collins (2010).
We train the full parser for 15 iterations of averaged
perceptron training (Collins, 2002), choose the itera-
tion with the best unlabeled attachment score (UAS)
on the development set, and apply the model after
that iteration to the test set.
</bodyText>
<footnote confidence="0.83575">
6http://groups.csail.mit.edu/nlp/dpo3/
</footnote>
<bodyText confidence="0.999917333333333">
We also ran MSTParser (McDonald and Pereira,
2006), the Berkeley constituency parser (Petrov and
Klein, 2007), and the unmodified dpo3 Model 1
(Koo and Collins, 2010) using Conversion 2 (the
current recommendations) for comparison. Since
the converted Penn Treebank now contains a few
non-projective sentences, we ran both the projective
and non-projective versions of the second order (sib-
ling) MSTParser. The Berkeley parser was trained
on the constituency trees of the PTB patched with
Vadas and Curran (2007), and then the predicted
parses were converted using pennconverter.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999577625">
Table 1 shows the unlabeled attachment scores,
complete sentence exact match accuracies, and the
accuracies of conjunctions and prepositions under
Conversion 2.7 The incorporation of the unlabeled
data features (clusters and web counts) into the dpo3
parser yields a significantly better parser than dpo3
alone (93.54 UAS versus 93.21)8, and is more than
a 1.5% improvement over MSTParser.
</bodyText>
<subsectionHeader confidence="0.999835">
6.1 Impact of Factorization
</subsectionHeader>
<bodyText confidence="0.999562944444444">
In all four metrics (attachment of all non-
punctuation tokens, sentence accuracy, prepositions,
and conjunctions), there is no significant difference
between the version of the parser which uses the
grandparent and sibling factorization (Grand+Sib)
and the version which uses just the grandparent fac-
torization (Grand). A parser which uses only grand-
parents (referred to as Model 0 in Koo and Collins
(2010)) may therefore be preferable, as it contains
far fewer parameters than a third-order parser.
While the grandparent factorization and the sib-
ling factorization (Sib) are both “second-order”
parsers, scoring up to two edges (involving three
words) simultaneously, their results are quite dif-
ferent, with the sibling factorization scoring much
worse. This is particularly notable in the conjunc-
tion case, where the sibling model is over 5% abso-
lute worse in accuracy than the grandparent model.
</bodyText>
<footnote confidence="0.9994616">
7As is standard for English dependency parsing, five punc-
tuation symbols :, ,, “, ”, and. are excluded from the results
(Yamada and Matsumoto, 2003).
8If the (deprecated) Conversion 1 is used, the new features
improve the UAS of dpo3 from 93.04 to 93.51.
</footnote>
<page confidence="0.992687">
773
</page>
<table confidence="0.993526590909091">
UAS Exact Match Conjunctions Prepositions
91.96 38.9 84.0 84.2
91.98 38.7 83.8 84.6
90.98 36.0 85.6 84.3
93.21 44.8 89.6 86.9
93.12 43.6 85.3 87.0
93.15 43.7 85.5 86.8
93.55 46.1 90.6 87.5
93.54 46.0 90.8 87.4
93.10 45.0 90.5 87.5
93.52 45.8 89.9 87.1
Model
MSTParser (proj)
MSTParser (non-proj)
Berkeley (converted)
dpo3 (Grand+Sib)
dpo3+Unlabeled (Edges)
dpo3+Unlabeled (Sib)
dpo3+Unlabeled (Grand)
dpo3+Unlabeled (Grand+Sib)
- Clusters
- Prep,Conj Counts
</table>
<tableCaption confidence="0.970293666666667">
Table 1: Test set accuracies under Conversion 2 of unlabeled attachment scores, complete sentence exact match accu-
racies, conjunction accuracy, and preposition accuracy. Bolded items are the best in each column, or not significantly
different from the best in that column (sign test, p &lt; .05).
</tableCaption>
<subsectionHeader confidence="0.999428">
6.2 Impact of Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999996166666667">
The unlabeled data features improved the already
state-of-the-art dpo3 parser in UAS, complete sen-
tence accuracy, conjunctions, and prepositions.
However, because the sample sizes are much smaller
for the latter three cases, only the UAS improvement
is statistically significant.9 Overall, the results in Ta-
ble 1 show that while the inclusion of unlabeled data
improves parser performance, increasing the size of
factorization matters even more. Ablation experi-
ments showed that cluster features have a larger im-
pact on overall UAS, while count features have a
larger impact on prepositions and conjunctions.
</bodyText>
<subsectionHeader confidence="0.999896">
6.3 Comparison with Other Parsers
</subsectionHeader>
<bodyText confidence="0.999979133333333">
The resulting dpo3+Unlabeled parser is significantly
better than both versions of MSTParser and the
Berkeley parser converted to dependencies across all
four evaluations. dpo3+Unlabeled has an UAS 1.5%
higher than MSTParser, which has an UAS 1.0%
higher than the converted constituency parser. The
MSTParser uses sibling scoring, so it is unsurpris-
ing that it performs less well on the new conversion.
While the converted constituency parser is not
as good on dependencies as MSTParser overall,
note that it is over a percent and a half better than
MSTParser on attaching conjunctions (85.6% versus
84.0%). Conjunction scope may benefit from paral-
lelism and higher-level structure, which is easily ac-
cessible when joining two matching non-terminals
</bodyText>
<footnote confidence="0.825065">
9There are 52,308 non-punctuation tokens in the test set,
compared with 2416 sentences, 1373 conjunctions, and 5854
prepositions.
</footnote>
<bodyText confidence="0.999770307692308">
in a context-free grammar, but much harder to
determine in the local views of graph-based de-
pendency parsers. The dependencies arising from
the Berkeley constituency trees have higher con-
junction accuracies than either the edge-based or
sibling-based dpo3+Unlabeled parser. However,
once grandparents are included in the factorization,
the dpo3+Unlabeled is significantly better at attach-
ing conjunctions than the constituency parser, at-
taching conjunctions with an accuracy over 90%.
Therefore, some of the disadvantages of dependency
parsing compared with constituency parsing can be
compensated for with larger factorizations.
</bodyText>
<table confidence="0.997948428571429">
Scoring Conjunctions Conversion 2
Conversion 1
(deprecated)
Edge 86.3 85.3
Sib 87.8 85.5
Grand 87.2 90.6
Grand+Sib 88.3 90.8
</table>
<tableCaption confidence="0.997584666666667">
Table 2: Unlabeled attachment accuracy for conjunc-
tions. Bolded items are the best in each column, or not
significantly different (sign test, p &lt; .05).
</tableCaption>
<subsectionHeader confidence="0.998264">
6.4 Impact of Data Representation
</subsectionHeader>
<bodyText confidence="0.999233">
Tables 2 and 3 show the results of the
dpo3+Unlabeled parser for conjunctions and
prepositions, respectively, under the two different
conversions. The data representation has an impact
on which factorizations perform best. Under
Conversion 1, conjunctions are more accurate under
a sibling parser than a grandparent parser, while the
</bodyText>
<page confidence="0.986888">
774
</page>
<figure confidence="0.96321">
Scoring
Prepositions
Conversion 1
(deprecated)
87.4
87.5
87.9
88.4
</figure>
<bodyText confidence="0.988135">
attachments though: the prepositions whose parent
is not the preceeding word are attached more accu-
rately if the parent is the root word (usually corre-
sponding to the main verb) of the sentence (90.8%,
587 cases) than if the parent is lower in the tree
(72.7%, 1075 cases).
</bodyText>
<figure confidence="0.96477">
Edge
Sib
Grand
Grand+Sib
Conversion 2
87.0
86.8
87.5
87.4
</figure>
<tableCaption confidence="0.995704">
Table 3: Unlabeled attachment accuracy for prepositions.
Bolded items are the best in each column, or not signifi-
cantly different (sign test, p &lt; .05).
</tableCaption>
<bodyText confidence="0.998081785714286">
pattern is reversed for Conversion 2.
Conjunctions show a much stronger need for
higher order factorizations than prepositions do.
This is not too surprising, as prepositions have more
of a selectional preference than conjunctions, and
so the preposition itself is more informative about
where it should attach. While prepositions do im-
prove with larger factorizations, the improvement
beyond edge-based is not significant for Conversion
2. One hypothesis for why Conversion 1 shows more
of an improvement is that the wider scope leads to
the semantic head being included; in Conversion
2, the semantic head is chosen as the parent of the
preposition, so the wider scope is less necessary.
</bodyText>
<subsectionHeader confidence="0.994397">
6.5 Preposition Error Analysis
</subsectionHeader>
<bodyText confidence="0.999989904761905">
Prepositions are still the largest source of errors in
the dpo3+Unlabeled parser. We therefore analyze
the errors made on the development set to determine
whether the difficult remaining cases for parsers cor-
respond to the Hindle and Rooth (1993) style PP-
attachment classification task. In the PP-attachment
classification task, the two choices for where the
preposition attaches are the previous verb or the pre-
vious noun, and the preposition itself has a noun ob-
ject. The ones that do attach to the preceeding noun
or verb (not necessarily the preceeding word) and
have a noun object (2323 prepositions) are attached
by the dpo3+Unlabeled grandparent-scoring parser
with 92.4% accuracy, while those that do not fit that
categorization (1703 prepositions) have the correct
parent only 82.7% of the time.
Local attachments are more accurate — preposi-
tions are attached with 94.8% accuracy if the correct
parent is the immediately preceeding word (2364
cases) and only 79.1% accuracy if it is not (1662
cases). The preference is not necessarily for low
</bodyText>
<sectionHeader confidence="0.998372" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99994585">
Features derived from unlabeled data (clusters and
web counts) significantly improve a state-of-the-art
dependency parser for English. We showed how
well various factorizations are able to take advantage
of these unlabeled data features, focusing our anal-
ysis on conjunctions and prepositions. Including
grandparents in the factorization increases the accu-
racy of conjunctions over 5% absolute over edge-
based or sibling-based scoring. The representation
of the data is extremely important for how the prob-
lem should be factored–under the old Penn2Malt de-
pendency representation, a sibling parser was more
accurate than a grandparent parser. As some impor-
tant relationships were represented as siblings and
some as grandparents, there was a need to develop
third-order parsers which could exploit both simul-
taneously (Koo and Collins, 2010). Under the new
pennconverter standard, a grandparent parser is sig-
nificantly better than a sibling parser, and there is no
significant improvement when including both.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999732428571429">
I would like to thank Terry Koo for making the dpo3
parser publically available and for his help with us-
ing the parser. I would also like to thank Mitch Mar-
cus and Kenneth Church for useful discussions. This
material is based upon work supported under a Na-
tional Science Foundation Graduate Research Fel-
lowship.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999274714285714">
M. Bansal and D. Klein. 2011. Web-scale features for
full-scale parsing. In Proceedings of ACL, pages 693–
702.
S. Bergsma, D. Yarowsky, and K. Church. 2011. Using
large monolingual and bilingual corpora to improve
coordination disambiguation. In Proceedings of ACL,
pages 1346–1355.
</reference>
<page confidence="0.980404">
775
</page>
<reference confidence="0.999935489795919">
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and
J.C. Lai. 1992. Class-based n-gram models of natural
language. Computational linguistics, 18(4):467–479.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22–29.
M. Collins, A. Globerson, T. Koo, X. Carreras, and P.L.
Bartlett. 2008. Exponentiated gradient algorithms
for conditional random fields and max-margin markov
networks. The Journal of Machine Learning Research,
9:1775–1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1–8.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of EMNLP, pages 167–202.
D. Hindle and M. Rooth. 1993. Structural ambigu-
ity and lexical relations. Computational Linguistics,
19(1):103–120.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
ACL, pages 1077–1086.
X. Huang. 1983. Dealing with conjunctions in a machine
translation environment. In Proceedings of EACL,
pages 81–85.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA), pages 105–112.
D. Jurafsky and J.H. Martin. 2008. Speech and language
processing: an introduction to natural language pro-
cessing, computational linguistics and speech recogni-
tion. Prentice Hall.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings ofACL, pages 1–11.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
ofACL, pages 595–603.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proceedings of EMNLP,
pages 1288–1298.
D. Lin, K. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
et al. 2010. New tools for web-scale n-grams. In Pro-
ceedings of LREC.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81–88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings ofACL, pages 91–98.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(2):95–135.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404–411.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale n-grams to improve base np parsing per-
formance. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages 886–
894.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP,
pages 133–142.
L. Schwartz, T. Aikawa, and C. Quirk. 2003. Disam-
biguation of English PP attachment using multilingual
aligned data. In Proceedings of MT Summit IX.
D. Vadas and J. Curran. 2007. Adding noun phrase struc-
ture to the Penn Treebank. In ACL, pages 240–247.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Linguistic Data
Consortium, Philadelphia.
R. Weischedel, M. Palmer, M. Marcus, E. Hovy, S. Prad-
han, L. Ramshaw, N. Xue, A. Taylor, J. Kaufman,
M. Franchini, et al. 2011. Ontonotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of International Workshop of Parsing Tech-
nologies, pages 195–206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: in-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of EMNLP, pages 562–571.
G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploiting
web-derived selectional preference to improve statisti-
cal dependency parsing. In Proceedings ofACL, pages
1556–1565.
</reference>
<page confidence="0.998542">
776
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.431632">
<title confidence="0.999044">Attacking Parsing Bottlenecks with Unlabeled Data and Factorizations</title>
<author confidence="0.995728">Emily</author>
<affiliation confidence="0.9934805">Computer and Information University of</affiliation>
<address confidence="0.811292">Philadelphia, PA</address>
<email confidence="0.999686">epitler@seas.upenn.edu</email>
<abstract confidence="0.96249495">two of the largest remaining bottlenecks in parsing. Across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications. Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution. As lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem. By including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve a new state-of-the-art for English dependencies with 93.55% correct attachments on the current standard. Furthermore, conjunctions are attached with an accuracy of 90.8%, and prepositions with an accuracy of 87.4%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bansal</author>
<author>D Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>693--702</pages>
<contexts>
<context position="2286" citStr="Bansal and Klein, 2011" startWordPosition="324" endWordPosition="327">ent attachments; conjunction mistakes can cause word ordering mistakes when translating into Chinese (Huang, 1983). Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution (Jurafsky and Martin, 2008). However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance (Gildea, 2001). Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features (Koo et al., 2008) or by using association statistics from a larger corpus, such as the web (Bansal and Klein, 2011; Zhou et al., 2011), have both improved parsing. Unlabeled data has been shown to improve the accuracy of conjunctions within complex noun phrases (Pitler et al., 2010; Bergsma et al., 2011). However, it has so far been less effective within full parsing — while first-order web-scale counts noticeably improved overall parsing in Bansal and Klein (2011), the accuracy on conjunctions actually decreased when the web-scale features were added (Table 4 in that paper). In this paper we show that unlabeled data can help prepositions and conjunctions, provided that the dependency representation is co</context>
<context position="6392" citStr="Bansal and Klein (2011)" startWordPosition="962" endWordPosition="965">rom Yamada and Matsumoto (2003). Regime (2) is based on the recommendations of the two converter tools; as of the date of this writing, the Penn2Malt website says: “Penn2Malt has been superseded by the more sophisticated pennconverter, which we strongly recommend”. The pennconverter website “strongly recommends” patching the Treebank with the NP annotations of Vadas and Curran (2007). A version of pennconverter was used to prepare the data for the CoNLL Shared Tasks of 2007- 2009, so the trees produced by Regime 2 are similar (but not identical)3 to these shared tasks. As far as we are aware, Bansal and Klein (2011) is the only published work which uses both steps in Regime (2). The dependency representations produced by Regime 2 are designed to be more useful for extracting semantics (Johansson and Nugues, 2007). The parsing attachment accuracy of MALTPARSER (Nivre et al., 2007) was lower using pennconverter than Penn2Malt, but using the output of MALTPARSER under the new format parses produces a much better semantic role labeler than using its output with Penn2Malt (Johansson and Nugues, 2007). Figures 1 and 2 show how conjunctions and prepositions, respectively, are represented after the two different</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>M. Bansal and D. Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of ACL, pages 693– 702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>D Yarowsky</author>
<author>K Church</author>
</authors>
<title>Using large monolingual and bilingual corpora to improve coordination disambiguation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1346--1355</pages>
<contexts>
<context position="2477" citStr="Bergsma et al., 2011" startWordPosition="356" endWordPosition="359">ncies for correct resolution (Jurafsky and Martin, 2008). However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance (Gildea, 2001). Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features (Koo et al., 2008) or by using association statistics from a larger corpus, such as the web (Bansal and Klein, 2011; Zhou et al., 2011), have both improved parsing. Unlabeled data has been shown to improve the accuracy of conjunctions within complex noun phrases (Pitler et al., 2010; Bergsma et al., 2011). However, it has so far been less effective within full parsing — while first-order web-scale counts noticeably improved overall parsing in Bansal and Klein (2011), the accuracy on conjunctions actually decreased when the web-scale features were added (Table 4 in that paper). In this paper we show that unlabeled data can help prepositions and conjunctions, provided that the dependency representation is compatible with how the parsing problem is decomposed for learning and inference. By incorporating unlabeled data into factorizations which capture the relevant dependencies for prepositions an</context>
</contexts>
<marker>Bergsma, Yarowsky, Church, 2011</marker>
<rawString>S. Bergsma, D. Yarowsky, and K. Church. 2011. Using large monolingual and bilingual corpora to improve coordination disambiguation. In Proceedings of ACL, pages 1346–1355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V Desouza</author>
<author>R L Mercer</author>
<author>V J D Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="18800" citStr="Brown et al., 1992" startWordPosition="3015" endWordPosition="3018">torization which includes grand-parent scoring will achieve the highest performance. In this section, we investigate the impact of unlabeled data on parsing accuracy using the two conversions and using each of the factorizations described in Section 3.1-3.4. 5.1 Unlabeled Data Feature Set Clusters: We replicate the cluster-based features from Koo et al. (2008), which includes features over all edges (h, m), grand-parent triples (h, m, c), and parent sibling triples (h, m, s). The features were all derived from the publicly available clusters produced by running the Brown clustering algorithm (Brown et al., 1992) over the BLLIP corpus with the Penn Treebank sentences excluded.5 Preposition and conjunction-inspired features (motivated by Section 4) are described below: 5people.csail.mit.edu/maestro/papers/ bllip-clusters.gz S(Y ) = � S(g, h, m, s) (4) I (g, h, m, s) (g, h) E Y, (h, m) E Y, I (h, s) E Y, (m, s) E Sib(Y) 772 Web Counts: For each set of words of interest, we compute the PMI between the words, and then include binary features for whether the mutual information is undefined, if it is negative, and whether it is greater than each positive integer. For conjunctions, we only do this for triple</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and J.C. Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="16837" citStr="Church and Hanks, 1990" startWordPosition="2708" endWordPosition="2711">unts. The phrases trading and transacting versus trading and what provide an example of the difference between associations and counts. The phrase trading and what has a higher count than the phrase trading and transacting, but trading and transacting are more highly associated. In this paper, we use point-wise mutual information (PMI) to measure the strength of associations of words participating in potential conjunctions or prepositions.4 For three words h, m, c, this is calculated with: P (h .* m .*c) PMI(h,m,c) = log P(h)P(m)P(c) (5) 4PMI can be unreliable when frequency counts are small (Church and Hanks, 1990), however the data used was thresholded, so all counts used are at least 10. The probabilities are estimated using web-scale n-gram counts, which are looked up using the tools and web-scale n-grams described in Lin et al. (2010). Defining the joint probability using wildcards (rather than the exact sequence h m c) is crucially important, as determiners, adjectives, and other words may naturally intervene between the words of interest. Approaches which cluster words (i.e., Koo et al. (2008)) are also designed to identify words which are semantically related. As manually labeled parsed data is s</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K.W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>A Globerson</author>
<author>T Koo</author>
<author>X Carreras</author>
<author>P L Bartlett</author>
</authors>
<title>Exponentiated gradient algorithms for conditional random fields and max-margin markov networks.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1775</pages>
<contexts>
<context position="20958" citStr="Collins et al., 2008" startWordPosition="3381" endWordPosition="3384">’s existing feature set was the addition of binary features for the part-ofspeech tag of the child of the root node, alone and conjoined with the tags of its children. For further details about the parser, see Koo and Collins (2010). 5.3 Experimental Set-up Training was done on Section 2-21 of the Penn Treebank. Section 22 was used for development, and Section 23 for test. We use automatic partof-speech tags for both training and testing (Ratnaparkhi, 1996). The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al., 2008) as in Koo and Collins (2010). We train the full parser for 15 iterations of averaged perceptron training (Collins, 2002), choose the iteration with the best unlabeled attachment score (UAS) on the development set, and apply the model after that iteration to the test set. 6http://groups.csail.mit.edu/nlp/dpo3/ We also ran MSTParser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. Since the converted Penn Treebank now contains a few non</context>
</contexts>
<marker>Collins, Globerson, Koo, Carreras, Bartlett, 2008</marker>
<rawString>M. Collins, A. Globerson, T. Koo, X. Carreras, and P.L. Bartlett. 2008. Exponentiated gradient algorithms for conditional random fields and max-margin markov networks. The Journal of Machine Learning Research, 9:1775–1822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="21079" citStr="Collins, 2002" startWordPosition="3403" endWordPosition="3404">onjoined with the tags of its children. For further details about the parser, see Koo and Collins (2010). 5.3 Experimental Set-up Training was done on Section 2-21 of the Penn Treebank. Section 22 was used for development, and Section 23 for test. We use automatic partof-speech tags for both training and testing (Ratnaparkhi, 1996). The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al., 2008) as in Koo and Collins (2010). We train the full parser for 15 iterations of averaged perceptron training (Collins, 2002), choose the iteration with the best unlabeled attachment score (UAS) on the development set, and apply the model after that iteration to the test set. 6http://groups.csail.mit.edu/nlp/dpo3/ We also ran MSTParser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. Since the converted Penn Treebank now contains a few non-projective sentences, we ran both the projective and non-projective versions of the second order (sibling) MSTParser. Th</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>167--202</pages>
<contexts>
<context position="2068" citStr="Gildea, 2001" startWordPosition="291" endWordPosition="292">urce language sentence produce different translations. Preposition attachment mistakes are particularly bad when translating into Japanese (Schwartz et al., 2003) which uses a different postposition for different attachments; conjunction mistakes can cause word ordering mistakes when translating into Chinese (Huang, 1983). Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution (Jurafsky and Martin, 2008). However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance (Gildea, 2001). Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features (Koo et al., 2008) or by using association statistics from a larger corpus, such as the web (Bansal and Klein, 2011; Zhou et al., 2011), have both improved parsing. Unlabeled data has been shown to improve the accuracy of conjunctions within complex noun phrases (Pitler et al., 2010; Bergsma et al., 2011). However, it has so far been less effective within full parsing — while first-order web-scale counts noticeably improved overall parsing in Bansal and Klein (2011), the accuracy on conjuncti</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>D. Gildea. 2001. Corpus variation and parser performance. In Proceedings of EMNLP, pages 167–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="12177" citStr="Hindle and Rooth, 1993" startWordPosition="1926" endWordPosition="1929">, the first conversion scheme chooses yesterday as the head of the overall NP, resulting in the edge yesterday—* of, while the second conversion scheme ignores temporal phrases when finding the head, resulting in the more semantically meaningful opening—*of. Similarly, in the third example, the preposition for attaches to the pronoun whose in the first conversion scheme, while it attaches to the noun plans in the second. With edge-based scoring, the object is not accessible when scoring where the preposition should attach, and PP-attachment is known to depend on the object of the preposition (Hindle and Rooth, 1993). 3.2 Sibling Scoring Another alternative factorization is to score siblings as well as parent-child edges (McDonald and Pereira, 2006). Scores decompose as: where Sib(Y) is the set containing ordered and adjacent sibling pairs in Y : if (m, s) E Sib(Y), there must exist a shared parent h such that (h, m) E Y and (h, s) E Y , m and s must be on the same side of h, m must be closer to h than s in the linear order of the sentence, and there must not exist any other children of h in between m and s. Under this factorization, two of the three examples in Conversion 1 (and none of the examples in C</context>
<context position="19726" citStr="Hindle and Rooth (1993)" startWordPosition="3180" endWordPosition="3183">E Sib(Y) 772 Web Counts: For each set of words of interest, we compute the PMI between the words, and then include binary features for whether the mutual information is undefined, if it is negative, and whether it is greater than each positive integer. For conjunctions, we only do this for triples of both conjunct and the conjunction (and if the conjunction is and or or and the two potential conjuncts are the same coarse grained part-of-speech). For prepositions, we consider only cases in which the parent is a noun or a verb and the child is a noun (this corresponds to the cases considered by Hindle and Rooth (1993) and others). Prepositions use association features to score both the triple (parent, preposition, child) and all pairs within that triple. The counts features are not used if all the words involved are stopwords. For the scope of this paper we use only the above counts related to prepositions and conjunctions. 5.2 Parser We use the Model 1 version of dpo3, a state-of-theart third-order dependency parser (Koo and Collins, 2010))6. We augment the feature set used with the web-counts-based features relevant to prepositions and conjunctions and the cluster-based features. The only other change to</context>
<context position="28551" citStr="Hindle and Rooth (1993)" startWordPosition="4539" endWordPosition="4542">th larger factorizations, the improvement beyond edge-based is not significant for Conversion 2. One hypothesis for why Conversion 1 shows more of an improvement is that the wider scope leads to the semantic head being included; in Conversion 2, the semantic head is chosen as the parent of the preposition, so the wider scope is less necessary. 6.5 Preposition Error Analysis Prepositions are still the largest source of errors in the dpo3+Unlabeled parser. We therefore analyze the errors made on the development set to determine whether the difficult remaining cases for parsers correspond to the Hindle and Rooth (1993) style PPattachment classification task. In the PP-attachment classification task, the two choices for where the preposition attaches are the previous verb or the previous noun, and the preposition itself has a noun object. The ones that do attach to the preceeding noun or verb (not necessarily the preceeding word) and have a noun object (2323 prepositions) are attached by the dpo3+Unlabeled grandparent-scoring parser with 92.4% accuracy, while those that do not fit that categorization (1703 prepositions) have the correct parent only 82.7% of the time. Local attachments are more accurate — pre</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>D. Hindle and M. Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="5703" citStr="Huang and Sagae, 2010" startWordPosition="848" endWordPosition="851">onstituency-todependency conversion program in ways which have effects on conjunctions and prepositions. We consider two such mapping regimes here: 1. PTB trees —* Penn2Malt1 —* Dependencies 2. PTB trees patched with NP-internal annotations (Vadas and Curran, 2007) —* pennconverter2 —* Dependencies 1http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html 2Johansson and Nugues (2007) http://nlp.cs.lth. se/software/treebank_converter/ Regime (1) is very commonly done in papers which report dependency parsing experiments (e.g., (McDonald and Pereira, 2006; Nivre et al., 2007; Zhang and Clark, 2008; Huang and Sagae, 2010; Koo and Collins, 2010)). Penn2Malt uses the head finding table from Yamada and Matsumoto (2003). Regime (2) is based on the recommendations of the two converter tools; as of the date of this writing, the Penn2Malt website says: “Penn2Malt has been superseded by the more sophisticated pennconverter, which we strongly recommend”. The pennconverter website “strongly recommends” patching the Treebank with the NP annotations of Vadas and Curran (2007). A version of pennconverter was used to prepare the data for the CoNLL Shared Tasks of 2007- 2009, so the trees produced by Regime 2 are similar (b</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of ACL, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Huang</author>
</authors>
<title>Dealing with conjunctions in a machine translation environment.</title>
<date>1983</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>81--85</pages>
<contexts>
<context position="1778" citStr="Huang, 1983" startWordPosition="247" endWordPosition="248"> two categories have the lowest accuracies, and mistakes made on these have consequences for downstream applications. Machine translation is sensitive to parsing errors involving prepositions and conjunctions, because in some languages different attachment decisions in the parse of the source language sentence produce different translations. Preposition attachment mistakes are particularly bad when translating into Japanese (Schwartz et al., 2003) which uses a different postposition for different attachments; conjunction mistakes can cause word ordering mistakes when translating into Chinese (Huang, 1983). Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution (Jurafsky and Martin, 2008). However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance (Gildea, 2001). Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features (Koo et al., 2008) or by using association statistics from a larger corpus, such as the web (Bansal and Klein, 2011; Zhou et al., 2011), have both improved parsing. Unlabeled data has been shown to improve t</context>
</contexts>
<marker>Huang, 1983</marker>
<rawString>X. Huang. 1983. Dealing with conjunctions in a machine translation environment. In Proceedings of EACL, pages 81–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA),</booktitle>
<pages>105--112</pages>
<contexts>
<context position="5464" citStr="Johansson and Nugues (2007)" startWordPosition="816" endWordPosition="819"> (NPs) flat, although there have been subsequent projects which annotate the internal structure of noun phrases (Vadas and Curran, 2007; Weischedel et al., 2011). The presence or absence of these noun phrase internal annotations interacts with constituency-todependency conversion program in ways which have effects on conjunctions and prepositions. We consider two such mapping regimes here: 1. PTB trees —* Penn2Malt1 —* Dependencies 2. PTB trees patched with NP-internal annotations (Vadas and Curran, 2007) —* pennconverter2 —* Dependencies 1http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html 2Johansson and Nugues (2007) http://nlp.cs.lth. se/software/treebank_converter/ Regime (1) is very commonly done in papers which report dependency parsing experiments (e.g., (McDonald and Pereira, 2006; Nivre et al., 2007; Zhang and Clark, 2008; Huang and Sagae, 2010; Koo and Collins, 2010)). Penn2Malt uses the head finding table from Yamada and Matsumoto (2003). Regime (2) is based on the recommendations of the two converter tools; as of the date of this writing, the Penn2Malt website says: “Penn2Malt has been superseded by the more sophisticated pennconverter, which we strongly recommend”. The pennconverter website “st</context>
<context position="6881" citStr="Johansson and Nugues, 2007" startWordPosition="1041" endWordPosition="1044"> so the trees produced by Regime 2 are similar (but not identical)3 to these shared tasks. As far as we are aware, Bansal and Klein (2011) is the only published work which uses both steps in Regime (2). The dependency representations produced by Regime 2 are designed to be more useful for extracting semantics (Johansson and Nugues, 2007). The parsing attachment accuracy of MALTPARSER (Nivre et al., 2007) was lower using pennconverter than Penn2Malt, but using the output of MALTPARSER under the new format parses produces a much better semantic role labeler than using its output with Penn2Malt (Johansson and Nugues, 2007). Figures 1 and 2 show how conjunctions and prepositions, respectively, are represented after the two different conversion processes. These differences are not rare–70.7% of conjunctions and 5.2% of prepositions in the development set have a different parent under the two conversion types. These representational differences have serious implications for how well various factorizations will be able to capture these two phenomena. 3 Implications of Representations on the Scope of Factorization Parsing requires a) learning to score potential parse trees, and b) given a particular scoring function</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA), pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and language processing: an introduction to natural language processing, computational linguistics and speech recognition.</title>
<date>2008</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="1912" citStr="Jurafsky and Martin, 2008" startWordPosition="264" endWordPosition="267">chine translation is sensitive to parsing errors involving prepositions and conjunctions, because in some languages different attachment decisions in the parse of the source language sentence produce different translations. Preposition attachment mistakes are particularly bad when translating into Japanese (Schwartz et al., 2003) which uses a different postposition for different attachments; conjunction mistakes can cause word ordering mistakes when translating into Chinese (Huang, 1983). Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution (Jurafsky and Martin, 2008). However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance (Gildea, 2001). Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features (Koo et al., 2008) or by using association statistics from a larger corpus, such as the web (Bansal and Klein, 2011; Zhou et al., 2011), have both improved parsing. Unlabeled data has been shown to improve the accuracy of conjunctions within complex noun phrases (Pitler et al., 2010; Bergsma et al., 2011). However, it has so far been less </context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>D. Jurafsky and J.H. Martin. 2008. Speech and language processing: an introduction to natural language processing, computational linguistics and speech recognition. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="5727" citStr="Koo and Collins, 2010" startWordPosition="852" endWordPosition="855">y conversion program in ways which have effects on conjunctions and prepositions. We consider two such mapping regimes here: 1. PTB trees —* Penn2Malt1 —* Dependencies 2. PTB trees patched with NP-internal annotations (Vadas and Curran, 2007) —* pennconverter2 —* Dependencies 1http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html 2Johansson and Nugues (2007) http://nlp.cs.lth. se/software/treebank_converter/ Regime (1) is very commonly done in papers which report dependency parsing experiments (e.g., (McDonald and Pereira, 2006; Nivre et al., 2007; Zhang and Clark, 2008; Huang and Sagae, 2010; Koo and Collins, 2010)). Penn2Malt uses the head finding table from Yamada and Matsumoto (2003). Regime (2) is based on the recommendations of the two converter tools; as of the date of this writing, the Penn2Malt website says: “Penn2Malt has been superseded by the more sophisticated pennconverter, which we strongly recommend”. The pennconverter website “strongly recommends” patching the Treebank with the NP annotations of Vadas and Curran (2007). A version of pennconverter was used to prepare the data for the CoNLL Shared Tasks of 2007- 2009, so the trees produced by Regime 2 are similar (but not identical)3 to th</context>
<context position="15068" citStr="Koo and Collins (2010)" startWordPosition="2427" endWordPosition="2430">o this is the first factorization that has allowed the compatibility of the two arguments to affect the attachment of the preposition/conjunction. Under Conversion 1, this factorization is particularly appropriate for prepositions, but would be unlikely to help conjunctions, which have no children. S(Y ) = � S(h, m, s) (2) I (h, m, s) (h, m) E Y, (h, s) E Y, 1 (m, s) E Sib(Y) S(h, m, c) (3) 771 3.4 Grandparent-Sibling Scoring A further widening of the factorization takes grandparents and siblings simultaneously: For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for nonprojective parsing, dual decomposition was used for this factorization in Koo et al. (2010). This factorization should combine all the benefits of the sibling and grandparent factorizations described above–for Conversion 1, sibling scoring may help conjunctions and grandparent scoring may help prepositions, and for Conversion 2, grandparent scoring should help both, while sibling scoring may or may not add some additional gains. 4 Using Unlabeled Data Effectively Associations from unlabeled data have the potential to improve both conjunctions and prepositio</context>
<context position="20157" citStr="Koo and Collins, 2010" startWordPosition="3252" endWordPosition="3255"> part-of-speech). For prepositions, we consider only cases in which the parent is a noun or a verb and the child is a noun (this corresponds to the cases considered by Hindle and Rooth (1993) and others). Prepositions use association features to score both the triple (parent, preposition, child) and all pairs within that triple. The counts features are not used if all the words involved are stopwords. For the scope of this paper we use only the above counts related to prepositions and conjunctions. 5.2 Parser We use the Model 1 version of dpo3, a state-of-theart third-order dependency parser (Koo and Collins, 2010))6. We augment the feature set used with the web-counts-based features relevant to prepositions and conjunctions and the cluster-based features. The only other change to the parser’s existing feature set was the addition of binary features for the part-ofspeech tag of the child of the root node, alone and conjoined with the tags of its children. For further details about the parser, see Koo and Collins (2010). 5.3 Experimental Set-up Training was done on Section 2-21 of the Penn Treebank. Section 22 was used for development, and Section 23 for test. We use automatic partof-speech tags for both</context>
<context position="21436" citStr="Koo and Collins, 2010" startWordPosition="3454" endWordPosition="3457">ential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al., 2008) as in Koo and Collins (2010). We train the full parser for 15 iterations of averaged perceptron training (Collins, 2002), choose the iteration with the best unlabeled attachment score (UAS) on the development set, and apply the model after that iteration to the test set. 6http://groups.csail.mit.edu/nlp/dpo3/ We also ran MSTParser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. Since the converted Penn Treebank now contains a few non-projective sentences, we ran both the projective and non-projective versions of the second order (sibling) MSTParser. The Berkeley parser was trained on the constituency trees of the PTB patched with Vadas and Curran (2007), and then the predicted parses were converted using pennconverter. 6 Results and Discussion Table 1 shows the unlabeled attachment scores, complete sentence exact match accuracies, and the accuracies of conjunctions and prepositions under Conversion 2.7</context>
<context position="22699" citStr="Koo and Collins (2010)" startWordPosition="3643" endWordPosition="3646">features (clusters and web counts) into the dpo3 parser yields a significantly better parser than dpo3 alone (93.54 UAS versus 93.21)8, and is more than a 1.5% improvement over MSTParser. 6.1 Impact of Factorization In all four metrics (attachment of all nonpunctuation tokens, sentence accuracy, prepositions, and conjunctions), there is no significant difference between the version of the parser which uses the grandparent and sibling factorization (Grand+Sib) and the version which uses just the grandparent factorization (Grand). A parser which uses only grandparents (referred to as Model 0 in Koo and Collins (2010)) may therefore be preferable, as it contains far fewer parameters than a third-order parser. While the grandparent factorization and the sibling factorization (Sib) are both “second-order” parsers, scoring up to two edges (involving three words) simultaneously, their results are quite different, with the sibling factorization scoring much worse. This is particularly notable in the conjunction case, where the sibling model is over 5% absolute worse in accuracy than the grandparent model. 7As is standard for English dependency parsing, five punctuation symbols :, ,, “, ”, and. are excluded from</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proceedings ofACL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="2189" citStr="Koo et al., 2008" startWordPosition="307" endWordPosition="310">slating into Japanese (Schwartz et al., 2003) which uses a different postposition for different attachments; conjunction mistakes can cause word ordering mistakes when translating into Chinese (Huang, 1983). Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution (Jurafsky and Martin, 2008). However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance (Gildea, 2001). Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features (Koo et al., 2008) or by using association statistics from a larger corpus, such as the web (Bansal and Klein, 2011; Zhou et al., 2011), have both improved parsing. Unlabeled data has been shown to improve the accuracy of conjunctions within complex noun phrases (Pitler et al., 2010; Bergsma et al., 2011). However, it has so far been less effective within full parsing — while first-order web-scale counts noticeably improved overall parsing in Bansal and Klein (2011), the accuracy on conjunctions actually decreased when the web-scale features were added (Table 4 in that paper). In this paper we show that unlabel</context>
<context position="17331" citStr="Koo et al. (2008)" startWordPosition="2788" endWordPosition="2791">.* m .*c) PMI(h,m,c) = log P(h)P(m)P(c) (5) 4PMI can be unreliable when frequency counts are small (Church and Hanks, 1990), however the data used was thresholded, so all counts used are at least 10. The probabilities are estimated using web-scale n-gram counts, which are looked up using the tools and web-scale n-grams described in Lin et al. (2010). Defining the joint probability using wildcards (rather than the exact sequence h m c) is crucially important, as determiners, adjectives, and other words may naturally intervene between the words of interest. Approaches which cluster words (i.e., Koo et al. (2008)) are also designed to identify words which are semantically related. As manually labeled parsed data is sparse, this may help generalize across similar words. However, if edges are not connected to the semantic head, cluster-based methods may be less effective. For example, the choice of yesterday as the head of opening of trading here yesterday in Figure 2(c) or whose in 2(e) may make cluster-based features less useful than if the semantic heads were chosen (opening and plans, respectively). 5 Experiments The previous section motivated the use of unlabeled data for attaching prepositions and</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1288--1298</pages>
<contexts>
<context position="15196" citStr="Koo et al. (2010)" startWordPosition="2449" endWordPosition="2452">/conjunction. Under Conversion 1, this factorization is particularly appropriate for prepositions, but would be unlikely to help conjunctions, which have no children. S(Y ) = � S(h, m, s) (2) I (h, m, s) (h, m) E Y, (h, s) E Y, 1 (m, s) E Sib(Y) S(h, m, c) (3) 771 3.4 Grandparent-Sibling Scoring A further widening of the factorization takes grandparents and siblings simultaneously: For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for nonprojective parsing, dual decomposition was used for this factorization in Koo et al. (2010). This factorization should combine all the benefits of the sibling and grandparent factorizations described above–for Conversion 1, sibling scoring may help conjunctions and grandparent scoring may help prepositions, and for Conversion 2, grandparent scoring should help both, while sibling scoring may or may not add some additional gains. 4 Using Unlabeled Data Effectively Associations from unlabeled data have the potential to improve both conjunctions and prepositions. We predict that web counts which include both conjuncts (for conjunctions), or which include both the attachment site and th</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In Proceedings of EMNLP, pages 1288–1298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>K Church</author>
<author>H Ji</author>
<author>S Sekine</author>
<author>D Yarowsky</author>
<author>S Bergsma</author>
<author>K Patil</author>
<author>E Pitler</author>
<author>R Lathbury</author>
<author>V Rao</author>
</authors>
<title>New tools for web-scale n-grams.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="17065" citStr="Lin et al. (2010)" startWordPosition="2747" endWordPosition="2750">g and transacting are more highly associated. In this paper, we use point-wise mutual information (PMI) to measure the strength of associations of words participating in potential conjunctions or prepositions.4 For three words h, m, c, this is calculated with: P (h .* m .*c) PMI(h,m,c) = log P(h)P(m)P(c) (5) 4PMI can be unreliable when frequency counts are small (Church and Hanks, 1990), however the data used was thresholded, so all counts used are at least 10. The probabilities are estimated using web-scale n-gram counts, which are looked up using the tools and web-scale n-grams described in Lin et al. (2010). Defining the joint probability using wildcards (rather than the exact sequence h m c) is crucially important, as determiners, adjectives, and other words may naturally intervene between the words of interest. Approaches which cluster words (i.e., Koo et al. (2008)) are also designed to identify words which are semantically related. As manually labeled parsed data is sparse, this may help generalize across similar words. However, if edges are not connected to the semantic head, cluster-based methods may be less effective. For example, the choice of yesterday as the head of opening of trading </context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, Patil, Pitler, Lathbury, Rao, 2010</marker>
<rawString>D. Lin, K. Church, H. Ji, S. Sekine, D. Yarowsky, S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao, et al. 2010. New tools for web-scale n-grams. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4585" citStr="Marcus et al., 1993" startWordPosition="695" endWordPosition="698"> may be used (Section 4). We then present experiments exploring the connection between representation, factorization, and unlabeled data in Sections 5 and 6. 2 Dependency Representations A dependency tree is a rooted, directed tree (or arborescence), in which the vertices are the words in the sentence plus an artificial root node, and each edge (h, m) represents a directed dependency relation from the head h to the modifier m. Throughout this section, we will use Y to denote a particular parse tree, and (h, m) E Y to denote a particular edge in Y . The Wall Street Journal Penn Treebank (PTB) (Marcus et al., 1993) contains parsed constituency trees (where each sentence is represented as a context-free-grammar derivation). Dependency parsing requires a conversion from these constituency trees to dependency trees. The Treebank constituency trees left noun phrases (NPs) flat, although there have been subsequent projects which annotate the internal structure of noun phrases (Vadas and Curran, 2007; Weischedel et al., 2011). The presence or absence of these noun phrase internal annotations interacts with constituency-todependency conversion program in ways which have effects on conjunctions and prepositions</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="5637" citStr="McDonald and Pereira, 2006" startWordPosition="836" endWordPosition="839">e or absence of these noun phrase internal annotations interacts with constituency-todependency conversion program in ways which have effects on conjunctions and prepositions. We consider two such mapping regimes here: 1. PTB trees —* Penn2Malt1 —* Dependencies 2. PTB trees patched with NP-internal annotations (Vadas and Curran, 2007) —* pennconverter2 —* Dependencies 1http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html 2Johansson and Nugues (2007) http://nlp.cs.lth. se/software/treebank_converter/ Regime (1) is very commonly done in papers which report dependency parsing experiments (e.g., (McDonald and Pereira, 2006; Nivre et al., 2007; Zhang and Clark, 2008; Huang and Sagae, 2010; Koo and Collins, 2010)). Penn2Malt uses the head finding table from Yamada and Matsumoto (2003). Regime (2) is based on the recommendations of the two converter tools; as of the date of this writing, the Penn2Malt website says: “Penn2Malt has been superseded by the more sophisticated pennconverter, which we strongly recommend”. The pennconverter website “strongly recommends” patching the Treebank with the NP annotations of Vadas and Curran (2007). A version of pennconverter was used to prepare the data for the CoNLL Shared Tas</context>
<context position="12312" citStr="McDonald and Pereira, 2006" startWordPosition="1946" endWordPosition="1949">nd conversion scheme ignores temporal phrases when finding the head, resulting in the more semantically meaningful opening—*of. Similarly, in the third example, the preposition for attaches to the pronoun whose in the first conversion scheme, while it attaches to the noun plans in the second. With edge-based scoring, the object is not accessible when scoring where the preposition should attach, and PP-attachment is known to depend on the object of the preposition (Hindle and Rooth, 1993). 3.2 Sibling Scoring Another alternative factorization is to score siblings as well as parent-child edges (McDonald and Pereira, 2006). Scores decompose as: where Sib(Y) is the set containing ordered and adjacent sibling pairs in Y : if (m, s) E Sib(Y), there must exist a shared parent h such that (h, m) E Y and (h, s) E Y , m and s must be on the same side of h, m must be closer to h than s in the linear order of the sentence, and there must not exist any other children of h in between m and s. Under this factorization, two of the three examples in Conversion 1 (and none of the examples in Conversion 2) in Figure 1 now include the conjunction and both conjuncts in the same score (Figures 1(c) and 1(e)). The scoring for head</context>
<context position="21320" citStr="McDonald and Pereira, 2006" startWordPosition="3436" endWordPosition="3439">ection 23 for test. We use automatic partof-speech tags for both training and testing (Ratnaparkhi, 1996). The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al., 2008) as in Koo and Collins (2010). We train the full parser for 15 iterations of averaged perceptron training (Collins, 2002), choose the iteration with the best unlabeled attachment score (UAS) on the development set, and apply the model after that iteration to the test set. 6http://groups.csail.mit.edu/nlp/dpo3/ We also ran MSTParser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. Since the converted Penn Treebank now contains a few non-projective sentences, we ran both the projective and non-projective versions of the second order (sibling) MSTParser. The Berkeley parser was trained on the constituency trees of the PTB patched with Vadas and Curran (2007), and then the predicted parses were converted using pennconverter. 6 Results and Discussion Table 1 shows the unlabeled attachment scores</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="10818" citStr="McDonald et al., 2005" startWordPosition="1699" endWordPosition="1702">d-modifier relationship and a conjunction relationship. For example, compare the two natural phrases dogs and cats and really nice. dogs and cats are a good pair to conjoin, but cats is not a good modifier for dogs, so there is a tension when scoring an edge like (dogs, cats): it should get a high score 770 when actually indicating a conjunction and low otherwise. (nice, really) shows the opposite pattern– really is a good modifier for nice, but nice and really are not two words which should be conjoined. This may be partially compensated for by including features about the surrounding words (McDonald et al., 2005), but any feature templates which would be identical across the two contexts will be in tension. In Figures 1(b), 1(d) and 1(f), the conjunction participates in a directed edge with each of the conjuncts. Thus, in edge-based scoring, at least under Conversion 2 neither of the conjuncts is being ignored; however, the factorization scores each edge independently, so how compatible these two conjuncts are with each other cannot be included in the scoring of a tree. Prepositions: For all of the examples in Figure 2, there is a directed edge from the head of the phrase that the preposition modifies</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>Maltparser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K¨ubler, S. Marinov, and E. Marsi. 2007. Maltparser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="21379" citStr="Petrov and Klein, 2007" startWordPosition="3444" endWordPosition="3447">h training and testing (Ratnaparkhi, 1996). The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al., 2008) as in Koo and Collins (2010). We train the full parser for 15 iterations of averaged perceptron training (Collins, 2002), choose the iteration with the best unlabeled attachment score (UAS) on the development set, and apply the model after that iteration to the test set. 6http://groups.csail.mit.edu/nlp/dpo3/ We also ran MSTParser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. Since the converted Penn Treebank now contains a few non-projective sentences, we ran both the projective and non-projective versions of the second order (sibling) MSTParser. The Berkeley parser was trained on the constituency trees of the PTB patched with Vadas and Curran (2007), and then the predicted parses were converted using pennconverter. 6 Results and Discussion Table 1 shows the unlabeled attachment scores, complete sentence exact match accuracies, and the accurac</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>S Bergsma</author>
<author>D Lin</author>
<author>K Church</author>
</authors>
<title>Using web-scale n-grams to improve base np parsing performance.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>886--894</pages>
<contexts>
<context position="2454" citStr="Pitler et al., 2010" startWordPosition="352" endWordPosition="355">nd on lexical dependencies for correct resolution (Jurafsky and Martin, 2008). However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance (Gildea, 2001). Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features (Koo et al., 2008) or by using association statistics from a larger corpus, such as the web (Bansal and Klein, 2011; Zhou et al., 2011), have both improved parsing. Unlabeled data has been shown to improve the accuracy of conjunctions within complex noun phrases (Pitler et al., 2010; Bergsma et al., 2011). However, it has so far been less effective within full parsing — while first-order web-scale counts noticeably improved overall parsing in Bansal and Klein (2011), the accuracy on conjunctions actually decreased when the web-scale features were added (Table 4 in that paper). In this paper we show that unlabeled data can help prepositions and conjunctions, provided that the dependency representation is compatible with how the parsing problem is decomposed for learning and inference. By incorporating unlabeled data into factorizations which capture the relevant dependenc</context>
</contexts>
<marker>Pitler, Bergsma, Lin, Church, 2010</marker>
<rawString>E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Using web-scale n-grams to improve base np parsing performance. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 886– 894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="20798" citStr="Ratnaparkhi, 1996" startWordPosition="3358" endWordPosition="3360">ure set used with the web-counts-based features relevant to prepositions and conjunctions and the cluster-based features. The only other change to the parser’s existing feature set was the addition of binary features for the part-ofspeech tag of the child of the root node, alone and conjoined with the tags of its children. For further details about the parser, see Koo and Collins (2010). 5.3 Experimental Set-up Training was done on Section 2-21 of the Penn Treebank. Section 22 was used for development, and Section 23 for test. We use automatic partof-speech tags for both training and testing (Ratnaparkhi, 1996). The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al., 2008) as in Koo and Collins (2010). We train the full parser for 15 iterations of averaged perceptron training (Collins, 2002), choose the iteration with the best unlabeled attachment score (UAS) on the development set, and apply the model after that iteration to the test set. 6http://groups.csail.mit.edu/nlp/dpo3/ We also ran MSTParser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodifie</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Schwartz</author>
<author>T Aikawa</author>
<author>C Quirk</author>
</authors>
<title>Disambiguation of English PP attachment using multilingual aligned data.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX. D. Vadas</booktitle>
<pages>240--247</pages>
<contexts>
<context position="1617" citStr="Schwartz et al., 2003" startWordPosition="221" endWordPosition="224">prepositions with an accuracy of 87.4%. 1 Introduction Prepositions and conjunctions are two large remaining bottlenecks in parsing. Across various existing parsers, these two categories have the lowest accuracies, and mistakes made on these have consequences for downstream applications. Machine translation is sensitive to parsing errors involving prepositions and conjunctions, because in some languages different attachment decisions in the parse of the source language sentence produce different translations. Preposition attachment mistakes are particularly bad when translating into Japanese (Schwartz et al., 2003) which uses a different postposition for different attachments; conjunction mistakes can cause word ordering mistakes when translating into Chinese (Huang, 1983). Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution (Jurafsky and Martin, 2008). However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance (Gildea, 2001). Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features (Koo et al., 2008) or by using association sta</context>
</contexts>
<marker>Schwartz, Aikawa, Quirk, 2003</marker>
<rawString>L. Schwartz, T. Aikawa, and C. Quirk. 2003. Disambiguation of English PP attachment using multilingual aligned data. In Proceedings of MT Summit IX. D. Vadas and J. Curran. 2007. Adding noun phrase structure to the Penn Treebank. In ACL, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>A Brunstein</author>
</authors>
<title>BBN pronoun coreference and entity type corpus. Linguistic Data Consortium,</title>
<date>2005</date>
<location>Philadelphia.</location>
<contexts>
<context position="7714" citStr="Weischedel and Brunstein, 2005" startWordPosition="1167" endWordPosition="1170">positions in the development set have a different parent under the two conversion types. These representational differences have serious implications for how well various factorizations will be able to capture these two phenomena. 3 Implications of Representations on the Scope of Factorization Parsing requires a) learning to score potential parse trees, and b) given a particular scoring function, finding the highest scoring tree according to that function. The number of potential trees for a sen3The CoNLL data does not include the NP annotations; it does include annotations of named entities (Weischedel and Brunstein, 2005) so had some internal NP edges. 769 Conversion 1 Conversion 2 Conversion 2 Conversion 1 Committee (a) the House Ways and Means Committee the House Ways and plan in law (a) plan in law (b) here debt notes and other (c) yesterday opening of trading (c) opening here yesterday trading (d) of Means (b) whose plans for sell sell or 600 by merge (f) plans for whose issues (f) issues (e) Figure 2: Examples of prepositions: plan in the S&amp;L bailout law, opening of trading here yesterday, and whose plans for major rights issues. The preposition is bolded and the (semantic) head is underlined. notes and d</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>R. Weischedel and A. Brunstein. 2005. BBN pronoun coreference and entity type corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>M Palmer</author>
<author>M Marcus</author>
<author>E Hovy</author>
<author>S Pradhan</author>
<author>L Ramshaw</author>
<author>N Xue</author>
<author>A Taylor</author>
<author>J Kaufman</author>
<author>M Franchini</author>
</authors>
<date>2011</date>
<booktitle>Ontonotes release 4.0. LDC2011T03,</booktitle>
<institution>Linguistic Data Consortium.</institution>
<location>Philadelphia, Penn.:</location>
<contexts>
<context position="4998" citStr="Weischedel et al., 2011" startWordPosition="753" endWordPosition="756"> h to the modifier m. Throughout this section, we will use Y to denote a particular parse tree, and (h, m) E Y to denote a particular edge in Y . The Wall Street Journal Penn Treebank (PTB) (Marcus et al., 1993) contains parsed constituency trees (where each sentence is represented as a context-free-grammar derivation). Dependency parsing requires a conversion from these constituency trees to dependency trees. The Treebank constituency trees left noun phrases (NPs) flat, although there have been subsequent projects which annotate the internal structure of noun phrases (Vadas and Curran, 2007; Weischedel et al., 2011). The presence or absence of these noun phrase internal annotations interacts with constituency-todependency conversion program in ways which have effects on conjunctions and prepositions. We consider two such mapping regimes here: 1. PTB trees —* Penn2Malt1 —* Dependencies 2. PTB trees patched with NP-internal annotations (Vadas and Curran, 2007) —* pennconverter2 —* Dependencies 1http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html 2Johansson and Nugues (2007) http://nlp.cs.lth. se/software/treebank_converter/ Regime (1) is very commonly done in papers which report dependency parsing experim</context>
</contexts>
<marker>Weischedel, Palmer, Marcus, Hovy, Pradhan, Ramshaw, Xue, Taylor, Kaufman, Franchini, 2011</marker>
<rawString>R. Weischedel, M. Palmer, M. Marcus, E. Hovy, S. Pradhan, L. Ramshaw, N. Xue, A. Taylor, J. Kaufman, M. Franchini, et al. 2011. Ontonotes release 4.0. LDC2011T03, Philadelphia, Penn.: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of International Workshop of Parsing Technologies,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="5800" citStr="Yamada and Matsumoto (2003)" startWordPosition="863" endWordPosition="866">prepositions. We consider two such mapping regimes here: 1. PTB trees —* Penn2Malt1 —* Dependencies 2. PTB trees patched with NP-internal annotations (Vadas and Curran, 2007) —* pennconverter2 —* Dependencies 1http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html 2Johansson and Nugues (2007) http://nlp.cs.lth. se/software/treebank_converter/ Regime (1) is very commonly done in papers which report dependency parsing experiments (e.g., (McDonald and Pereira, 2006; Nivre et al., 2007; Zhang and Clark, 2008; Huang and Sagae, 2010; Koo and Collins, 2010)). Penn2Malt uses the head finding table from Yamada and Matsumoto (2003). Regime (2) is based on the recommendations of the two converter tools; as of the date of this writing, the Penn2Malt website says: “Penn2Malt has been superseded by the more sophisticated pennconverter, which we strongly recommend”. The pennconverter website “strongly recommends” patching the Treebank with the NP annotations of Vadas and Curran (2007). A version of pennconverter was used to prepare the data for the CoNLL Shared Tasks of 2007- 2009, so the trees produced by Regime 2 are similar (but not identical)3 to these shared tasks. As far as we are aware, Bansal and Klein (2011) is the </context>
<context position="23340" citStr="Yamada and Matsumoto, 2003" startWordPosition="3743" endWordPosition="3746">e be preferable, as it contains far fewer parameters than a third-order parser. While the grandparent factorization and the sibling factorization (Sib) are both “second-order” parsers, scoring up to two edges (involving three words) simultaneously, their results are quite different, with the sibling factorization scoring much worse. This is particularly notable in the conjunction case, where the sibling model is over 5% absolute worse in accuracy than the grandparent model. 7As is standard for English dependency parsing, five punctuation symbols :, ,, “, ”, and. are excluded from the results (Yamada and Matsumoto, 2003). 8If the (deprecated) Conversion 1 is used, the new features improve the UAS of dpo3 from 93.04 to 93.51. 773 UAS Exact Match Conjunctions Prepositions 91.96 38.9 84.0 84.2 91.98 38.7 83.8 84.6 90.98 36.0 85.6 84.3 93.21 44.8 89.6 86.9 93.12 43.6 85.3 87.0 93.15 43.7 85.5 86.8 93.55 46.1 90.6 87.5 93.54 46.0 90.8 87.4 93.10 45.0 90.5 87.5 93.52 45.8 89.9 87.1 Model MSTParser (proj) MSTParser (non-proj) Berkeley (converted) dpo3 (Grand+Sib) dpo3+Unlabeled (Edges) dpo3+Unlabeled (Sib) dpo3+Unlabeled (Grand) dpo3+Unlabeled (Grand+Sib) - Clusters - Prep,Conj Counts Table 1: Test set accuracies un</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of International Workshop of Parsing Technologies, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graph-based and transitionbased dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>562--571</pages>
<contexts>
<context position="5680" citStr="Zhang and Clark, 2008" startWordPosition="844" endWordPosition="847">ations interacts with constituency-todependency conversion program in ways which have effects on conjunctions and prepositions. We consider two such mapping regimes here: 1. PTB trees —* Penn2Malt1 —* Dependencies 2. PTB trees patched with NP-internal annotations (Vadas and Curran, 2007) —* pennconverter2 —* Dependencies 1http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html 2Johansson and Nugues (2007) http://nlp.cs.lth. se/software/treebank_converter/ Regime (1) is very commonly done in papers which report dependency parsing experiments (e.g., (McDonald and Pereira, 2006; Nivre et al., 2007; Zhang and Clark, 2008; Huang and Sagae, 2010; Koo and Collins, 2010)). Penn2Malt uses the head finding table from Yamada and Matsumoto (2003). Regime (2) is based on the recommendations of the two converter tools; as of the date of this writing, the Penn2Malt website says: “Penn2Malt has been superseded by the more sophisticated pennconverter, which we strongly recommend”. The pennconverter website “strongly recommends” patching the Treebank with the NP annotations of Vadas and Curran (2007). A version of pennconverter was used to prepare the data for the CoNLL Shared Tasks of 2007- 2009, so the trees produced by </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Y. Zhang and S. Clark. 2008. A tale of two parsers: investigating and combining graph-based and transitionbased dependency parsing using beam-search. In Proceedings of EMNLP, pages 562–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Zhao</author>
<author>K Liu</author>
<author>L Cai</author>
</authors>
<title>Exploiting web-derived selectional preference to improve statistical dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1556--1565</pages>
<contexts>
<context position="2306" citStr="Zhou et al., 2011" startWordPosition="328" endWordPosition="331">tion mistakes can cause word ordering mistakes when translating into Chinese (Huang, 1983). Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution (Jurafsky and Martin, 2008). However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance (Gildea, 2001). Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features (Koo et al., 2008) or by using association statistics from a larger corpus, such as the web (Bansal and Klein, 2011; Zhou et al., 2011), have both improved parsing. Unlabeled data has been shown to improve the accuracy of conjunctions within complex noun phrases (Pitler et al., 2010; Bergsma et al., 2011). However, it has so far been less effective within full parsing — while first-order web-scale counts noticeably improved overall parsing in Bansal and Klein (2011), the accuracy on conjunctions actually decreased when the web-scale features were added (Table 4 in that paper). In this paper we show that unlabeled data can help prepositions and conjunctions, provided that the dependency representation is compatible with how th</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In Proceedings ofACL, pages 1556–1565.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>