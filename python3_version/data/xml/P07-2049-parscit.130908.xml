<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009025">
<title confidence="0.9934035">
Measuring Importance and Query Relevance in Topic-focused
Multi-document Summarization
</title>
<author confidence="0.976789">
Surabhi Gupta and Ani Nenkova and Dan Jurafsky
</author>
<affiliation confidence="0.973432">
Stanford University
</affiliation>
<address confidence="0.861143">
Stanford, CA 94305
</address>
<email confidence="0.998819">
surabhi@cs.stanford.edu,{anenkova,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999596266666667">
The increasing complexity of summarization systems
makes it difficult to analyze exactly which mod-
ules make a difference in performance. We carried
out a principled comparison between the two most
commonly used schemes for assigning importance to
words in the context of query focused multi-document
summarization: raw frequency (word probability) and
log-likelihood ratio. We demonstrate that the advan-
tages of log-likelihood ratio come from its known dis-
tributional properties which allow for the identifica-
tion of a set of words that in its entirety defines the
aboutness of the input. We also find that LLR is more
suitable for query-focused summarization since, un-
like raw frequency, it is more sensitive to the integra-
tion of the information need defined by the user.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999662471698113">
Recently the task of multi-document summarization
in response to a complex user query has received
considerable attention. In generic summarization,
the summary is meant to give an overview of the
information in the documents. By contrast, when
the summary is produced in response to a user query
or topic (query-focused, topic-focused, or generally
focused summary), the topic/query determines what
information is appropriate for inclusion in the sum-
mary, making the task potentially more challenging.
In this paper we present an analytical study of two
questions regarding aspects of the topic-focused sce-
nario. First, two estimates of importance on words
have been used very successfully both in generic and
query-focused summarization: frequency (Luhn,
1958; Nenkova et al., 2006; Vanderwende et al.,
2006) and loglikelihood ratio (Lin and Hovy, 2000;
Conroy et al., 2006; Lacatusu et al., 2006). While
both schemes have proved to be suitable for sum-
marization, with generally better results from log-
likelihood ratio, no study has investigated in what
respects and by how much they differ. Second, there
are many little-understood aspects of the differences
between generic and query-focused summarization.
For example, we’d like to know if a particular word
weighting scheme is more suitable for focused sum-
marization than others. More significantly, previous
studies show that generic and focused systems per-
form very similarly to each other in query-focused
summarization (Nenkova, 2005) and it is of interest
to find out why.
To address these questions we examine the two
weighting schemes: raw frequency (or word proba-
bility estimated from the input), and log-likelihood
ratio (LLR) and two of its variants. These metrics
are used to assign importance to individual content
words in the input, as we discuss below.
Word probability R(w) = &apos;, where n is the num-
ber of times the word w appeared in the input and N
is the total number of words in the input.
Log-likelihood ratio (LLR) The likelihood ratio A
(Manning and Schutze, 1999) uses a background
corpus to estimate the importance of a word and it
is proportional to the mutual information between
a word w and the input to be summarized; A(w) is
defined as the ratio between the probability (under
a binomial distribution) of observing w in the input
and the background corpus assuming equal proba-
bility of occurrence of w in both and the probability
of the data assuming different probabilities for w in
the input and the background corpus.
LLR with cut-off (LLR(C)) A useful property
of the log-likelihood ratio is that the quantity
</bodyText>
<page confidence="0.988266">
193
</page>
<bodyText confidence="0.93044232">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 193–196,
Prague, June 2007. c�2007 Association for Computational Linguistics
−2 log(A) is asymptotically well approximated by
x2 distribution. A word appears in the input sig-
nificantly more often than in the background corpus
when −2 log(A) &gt; 10. Such words are called signa-
ture terms in Lin and Hovy (2000) who were the first
to introduce the log-likelihood weighting scheme for
summarization. Each descriptive word is assigned
an equal weight and the rest of the words have a
weight of zero:
R(w) = 1 if (−2log(A(w)) &gt; 10), 0 otherwise.
This weighting scheme has been adopted in several
recent generic and topic-focused summarizers (Con-
roy et al., 2006; Lacatusu et al., 2006).
LLR(CQ) The above three weighting schemes as-
sign a weight to words regardless of the user query
and are most appropriate for generic summarization.
When a user query is available, it should inform
the summarizer to make the summary more focused.
In Conroy et al. (2006) such query sensititivity is
achieved by augmenting LLR(C) with all content
words from the user query, each assigned a weight
of 1 equal to the weight of words defined by LLR(C)
as topic words from the input to the summarizer.
</bodyText>
<sectionHeader confidence="0.979846" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999925533333333">
We used the data from the 2005 Document Under-
standing Conference (DUC) for our experiments.
The task is to produce a 250-word summary in re-
sponse to a topic defined by a user for a total of 50
topics with approximately 25 documents for each
marked as relevant by the topic creator. In com-
puting LLR, the remaining 49 topics were used as a
background corpus as is often done by DUC partic-
ipants. A sample topic (d301) shows the complexity
of the queries:
Identify and describe types of organized crime that
crosses borders or involves more than one country. Name
the countries involved. Also identify the perpetrators in-
volved with each type of crime, including both individuals
and organizations if possible.
</bodyText>
<sectionHeader confidence="0.990745" genericHeader="method">
3 The Experiment
</sectionHeader>
<bodyText confidence="0.9999055">
In the summarizers we compare here, the various
weighting methods we describe above are used to
assign importance to individual content words in the
input. The weight or importance of a sentence 5 in
</bodyText>
<table confidence="0.996842222222222">
GENERIC FOCUSED
Frequency 0.11972 0.11795
(0.11168–0.12735) (0.11010–0.12521)
LLR 0.11223 0.11600
(0.10627–0.11873) (0.10915–0.12281)
LLR(C) 0.11949 0.12201
(0.11249–0.12724) (0.11507–0.12950)
LLR(CQ) not app 0.12546
(.11884–.13247)
</table>
<tableCaption confidence="0.865642333333333">
Table 1: SU4 ROUGE recall (and 95% confidence
intervals) for runs on the entire input (GENERIC) and
on relevant sentences (FOCUSED).
</tableCaption>
<equation confidence="0.829065333333333">
the input is defined as
WeightR(S) = � R(w) (1)
WES
</equation>
<bodyText confidence="0.999885526315789">
where R(w) assigns a weight for each word w.
For GENERIC summarization, the top scoring sen-
tences in the input are taken to form a generic extrac-
tive summary. In the computation of sentence im-
portance, only nouns, verbs, adjectives and adverbs
are considered and a short list of light verbs are ex-
cluded: “has, was, have, are, will, were, do, been,
say, said, says”. For FOCUSED summarization, we
modify this algorithm merely by running the sen-
tence selection algorithm on only those sentences
in the input that are relevent to the user query. In
some previous DUC evaluations, relevant sentences
are explicitly marked by annotators and given to sys-
tems. In our version here, a sentence in the input is
considered relevant if it contains at least one word
from the user query.
For evaluation we use ROUGE (Lin, 2004) SU4
recall metric&apos;, which was among the official auto-
matic evaluation metrics for DUC.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999385">
The results are shown in Table 1. The focused sum-
marizer using LLR(CQ) is the best, and it signif-
icantly outperforms the focused summarizer based
on frequency. Also, LLR (using log-likelihood ra-
tio to assign weights to all words) perfroms signif-
icantly worse than LLR(C). We can observe some
trends even from the results for which there is no
significance. Both LLR and LLR(C) are sensitive to
the introduction of topic relevance, producing some-
what better summaries in the FOCUSED scenario
</bodyText>
<footnote confidence="0.606262">
1-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d
</footnote>
<page confidence="0.995758">
194
</page>
<bodyText confidence="0.98865455319149">
compared to the GENERIC scenario. This is not the
case for the frequency summarizer, where using only
the relevant sentences has a negative impact.
4.1 Focused summarization: do we need query
expansion?
In the FOCUSED condition there was little (for LLR
weighting) or no (for frequency) improvement over
GENERIC. One possible explanation for the lack of
clear improvement in the FOCUSED setting is that
there are not enough relevant sentences, making it
impossible to get stable estimates of word impor-
tance. Alternatively, it could be the case that many
of the sentences are relevant, so estimates from the
relevant portion of the input are about the same as
those from the entire input.
To distinguish between these two hypotheses, we
conducted an oracle experiment. We modified the
FOCUSED condition by expanding the topic words
from the user query with all content words from any
of the human-written summaries for the topic. This
increases the number of relevant sentences for each
topic. No automatic method for query expansion can
be expected to give more accurate results, since the
content of the human summaries is a direct indica-
tion of what information in the input was important
and relevant and, moreover, the ROUGE evaluation
metric is based on direct n-gram comparison with
these human summaries.
Even under these conditions there was no signif-
icant improvement for the summarizers, each get-
ting better by 0.002: the frequency summarizer gets
R-SU4 of 0.12048 and the LLR(CQ) summarizer
achieves R-SU4 of 0.12717.
These results seem to suggest that considering the
content words in the user topic results in enough rel-
evant sentences. Indeed, Table 2 shows the mini-
mum, maximum and average percentage of relevant
sentences in the input (containing at least one con-
tent words from the user the query), both as defined
by the original query and by the oracle query ex-
pansion. It is clear from the table that, on aver-
age, over half of the input comprises sentences that
are relevant to the user topic. Oracle query expan-
sion makes the number of relevant sentences almost
equivalent to the input size and it is thus not sur-
prising that the corresponding results for content se-
lection are nearly identical to the query independent
</bodyText>
<table confidence="0.95827825">
Original query Oracle query expansion
Min 13% 52%
Average 57% 86%
Max 82% 98%
</table>
<tableCaption confidence="0.977332">
Table 2: Percentage of relevant sentences (contain-
</tableCaption>
<bodyText confidence="0.997014428571429">
ing words from the user query) in the input. The
oracle query expansion considers all content words
form human summaries of the input as query words.
runs of generic summaries for the entire input.
These numbers indictate that rather than finding
ways for query expansion, it might instead be more
important to find techniques for constraining the
query, determining which parts of the input are di-
rectly related to the user questions. Such techniques
have been described in the recent multi-strategy ap-
proach of Lacatusu et al. (2006) for example, where
one of the strategies breaks down the user topic
into smaller questions that are answered using ro-
bust question-answering techniques.
</bodyText>
<subsectionHeader confidence="0.659338">
4.2 Why is log-likelihood ratio better than
frequency?
</subsectionHeader>
<bodyText confidence="0.95334208">
Frequency and log-likelihood ratio weighting for
content words produce similar results when applied
to rank all words in the input, while the cut-off
for topicality in LLR(C) does have a positive im-
pact on content selection. A closer look at the
two weighting schemes confirms that when cut-off
is not used, similar weighting of content words is
produced. The Spearman correlation coefficient be-
tween the weights for words assigned by the two
schemes is on average 0.64. At the same time, it is
likely that the weights of sentences are dominated
by only the top most highly weighted words. In
order to see to what extent the two schemes iden-
tify the same or different words as the most impor-
tant ones, we computed the overlap between the 250
most highly weighted words according to LLR and
frequency. The average overlap across the 50 sets
was quite large, 70%.
To illustrate the degree of overlap, we list below
are the most highly weighted words according to
each weighting scheme for our sample topic con-
cerning crimes across borders.
LLR drug, cocaine, traffickers, cartel, police, crime, en-
forcement, u.s., smuggling, trafficking, arrested, government,
seized, year, drugs, organised, heroin, criminal, cartels, last,
</bodyText>
<page confidence="0.998161">
195
</page>
<bodyText confidence="0.957517689655172">
official, country, law, border, kilos, arrest, more, mexican, laun-
dering, officials, money, accounts, charges, authorities, cor-
ruption, anti-drug, international, banks, operations, seizures,
federal, italian, smugglers, dealers, narcotics, criminals, tons,
most, planes, customs
Frequency drug, cocaine, officials, police, more, last, gov-
ernment, year, cartel, traffickers, u.s., other, drugs, enforce-
ment, crime, money, country, arrested, federal, most, now, traf-
ficking, seized, law, years, new, charges, smuggling, being, of-
ficial, organised, international, former, authorities, only, crimi-
nal, border, people, countries, state, world, trade, first, mexican,
many, accounts, according, bank, heroin, cartels
It becomes clear that the advantage of likelihood
ratio as a weighting scheme does not come from
major differences in overall weights it assigns to
words compared to frequency. It is the signifi-
cance cut-off for the likelihood ratio that leads to
noticeable improvement (see Table 1). When this
weighting scheme is augmented by adding a score
of 1 for content words that appear in the user topic,
the summaries improve even further (LLR(CQ)).
Half of the improvement can be attributed to the
cut-off (LLR(C)), and the other half to focusing
the summary using the information from the user
query (LLR(CQ)). The advantage of likelihood ra-
tio comes from its providing a principled criterion
for deciding which words are truly descriptive of the
input and which are not. Raw frequency provides no
such cut-off.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999625540540541">
In this paper we examined two weighting schemes
for estimating word importance that have been suc-
cessfully used in current systems but have not to-
date been directly compared. Our analysis con-
firmed that log-likelihood ratio leads to better re-
sults, but not because it defines a more accurate as-
signment of importance than raw frequency. Rather,
its power comes from the use of a known distribution
that makes it possible to determine which words are
truly descriptive of the input. Only when such words
are viewed as equally important in defining the topic
does this weighting scheme show improved perfor-
mance. Using the significance cut-off and consider-
ing all words above it equally important is key.
Log-likelihood ratio summarizer is more sensitive
to topicality or relevance and produces summaries
that are better when it take the user request into ac-
count than when it does not. This is not the case for
a summarizer based on frequency.
At the same time it is noteworthy that the generic
summarizers perform about as well as their focused
counterparts. This may be related to our discovery
that on average 57% of the sentences in the doc-
ument are relevant and that ideal query expansion
leads to a situation in which almost all sentences
in the input become relevant. These facts could
be an unplanned side-effect from the way the test
topics were produced: annotators might have been
influenced by information in the input to be sum-
marizied when defining their topic. Such observa-
tions also suggest that a competitive generic summa-
rizer would be an appropriate baseline for the topic-
focused task in future DUCs. In addition, including
some irrelavant documents in the input might make
the task more challenging and allow more room for
advances in query expansion and other summary fo-
cusing techniques.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999375310344827">
J. Conroy, J. Schlesinger, and D. O’Leary. 2006. Topic-focused
multi-document summarization using an approximate oracle
score. In Proceedings of the COLING/ACL’06 (Poster Ses-
sion).
F. Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley, B. Rink,
P. Wang, and L. Taylor. 2006. Lcc’s gistexter at duc 2006:
Multi-strategy multi-document summarization. In Proceed-
ings ofDUC’06.
C. Lin and E. Hovy. 2000. The automated acquisition of topic
signatures for text summarization. In Proceedings of COL-
ING’00.
C. Lin. 2004. Rouge: a package for automatic evaluation of
summaries. In Proceedings of the Workshop on Text Sum-
marization Branches Out (WAS 2004).
H. P. Luhn. 1958. The automatic creation of literature abstracts.
IBMJournal of Research and Development, 2(2):159–165.
C. Manning and H. Schutze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A compositional context sensitive multi-document summa-
rizer: Exploring the factors that influence summarization. In
Proceedings ofACM SIGIR’06.
A. Nenkova. 2005. Automatic text summarization of newswire:
lessons learned from the document understanding confer-
ence. In Proceedings ofAAAI’05.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft
research at duc 2006: Task-focused summarization with sen-
tence simplification and lexical expansion. In Proceedings of
DUC’06.
</reference>
<page confidence="0.998955">
196
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939840">
<title confidence="0.9994725">Measuring Importance and Query Relevance in Topic-focused Multi-document Summarization</title>
<author confidence="0.973315">Gupta Nenkova Jurafsky</author>
<affiliation confidence="0.999977">Stanford University</affiliation>
<address confidence="0.999809">Stanford, CA 94305</address>
<abstract confidence="0.99789425">The increasing complexity of summarization systems makes it difficult to analyze exactly which modules make a difference in performance. We carried out a principled comparison between the two most commonly used schemes for assigning importance to in the context of focused multi-document raw frequency (word probability) and log-likelihood ratio. We demonstrate that the advantages of log-likelihood ratio come from its known distributional properties which allow for the identificaof set words that in its entirety defines the aboutness of the input. We also find that LLR is more suitable for query-focused summarization since, unlike raw frequency, it is more sensitive to the integration of the information need defined by the user.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Conroy</author>
<author>J Schlesinger</author>
<author>D O’Leary</author>
</authors>
<title>Topic-focused multi-document summarization using an approximate oracle score.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL’06</booktitle>
<location>(Poster Session).</location>
<marker>Conroy, Schlesinger, O’Leary, 2006</marker>
<rawString>J. Conroy, J. Schlesinger, and D. O’Leary. 2006. Topic-focused multi-document summarization using an approximate oracle score. In Proceedings of the COLING/ACL’06 (Poster Session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Lacatusu</author>
<author>A Hickl</author>
<author>K Roberts</author>
<author>Y Shi</author>
<author>J Bensley</author>
<author>B Rink</author>
<author>P Wang</author>
<author>L Taylor</author>
</authors>
<title>Lcc’s gistexter at duc 2006: Multi-strategy multi-document summarization.</title>
<date>2006</date>
<booktitle>In Proceedings ofDUC’06.</booktitle>
<contexts>
<context position="1927" citStr="Lacatusu et al., 2006" startWordPosition="280" endWordPosition="283">nse to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging. In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario. First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006). While both schemes have proved to be suitable for summarization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ. Second, there are many little-understood aspects of the differences between generic and query-focused summarization. For example, we’d like to know if a particular word weighting scheme is more suitable for focused summarization than others. More significantly, previous studies show that generic and focused systems perform very similarly to each other in query-focused summarization (Nenkova, 2005) and </context>
<context position="4369" citStr="Lacatusu et al., 2006" startWordPosition="685" endWordPosition="688">tational Linguistics −2 log(A) is asymptotically well approximated by x2 distribution. A word appears in the input significantly more often than in the background corpus when −2 log(A) &gt; 10. Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization. Each descriptive word is assigned an equal weight and the rest of the words have a weight of zero: R(w) = 1 if (−2log(A(w)) &gt; 10), 0 otherwise. This weighting scheme has been adopted in several recent generic and topic-focused summarizers (Conroy et al., 2006; Lacatusu et al., 2006). LLR(CQ) The above three weighting schemes assign a weight to words regardless of the user query and are most appropriate for generic summarization. When a user query is available, it should inform the summarizer to make the summary more focused. In Conroy et al. (2006) such query sensititivity is achieved by augmenting LLR(C) with all content words from the user query, each assigned a weight of 1 equal to the weight of words defined by LLR(C) as topic words from the input to the summarizer. 2 Data We used the data from the 2005 Document Understanding Conference (DUC) for our experiments. The</context>
<context position="10586" citStr="Lacatusu et al. (2006)" startWordPosition="1724" endWordPosition="1727"> 13% 52% Average 57% 86% Max 82% 98% Table 2: Percentage of relevant sentences (containing words from the user query) in the input. The oracle query expansion considers all content words form human summaries of the input as query words. runs of generic summaries for the entire input. These numbers indictate that rather than finding ways for query expansion, it might instead be more important to find techniques for constraining the query, determining which parts of the input are directly related to the user questions. Such techniques have been described in the recent multi-strategy approach of Lacatusu et al. (2006) for example, where one of the strategies breaks down the user topic into smaller questions that are answered using robust question-answering techniques. 4.2 Why is log-likelihood ratio better than frequency? Frequency and log-likelihood ratio weighting for content words produce similar results when applied to rank all words in the input, while the cut-off for topicality in LLR(C) does have a positive impact on content selection. A closer look at the two weighting schemes confirms that when cut-off is not used, similar weighting of content words is produced. The Spearman correlation coefficien</context>
</contexts>
<marker>Lacatusu, Hickl, Roberts, Shi, Bensley, Rink, Wang, Taylor, 2006</marker>
<rawString>F. Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley, B. Rink, P. Wang, and L. Taylor. 2006. Lcc’s gistexter at duc 2006: Multi-strategy multi-document summarization. In Proceedings ofDUC’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>E Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING’00.</booktitle>
<contexts>
<context position="1882" citStr="Lin and Hovy, 2000" startWordPosition="272" endWordPosition="275">st, when the summary is produced in response to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging. In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario. First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006). While both schemes have proved to be suitable for summarization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ. Second, there are many little-understood aspects of the differences between generic and query-focused summarization. For example, we’d like to know if a particular word weighting scheme is more suitable for focused summarization than others. More significantly, previous studies show that generic and focused systems perform very similarly to each other in que</context>
<context position="3998" citStr="Lin and Hovy (2000)" startWordPosition="623" endWordPosition="626">qual probability of occurrence of w in both and the probability of the data assuming different probabilities for w in the input and the background corpus. LLR with cut-off (LLR(C)) A useful property of the log-likelihood ratio is that the quantity 193 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 193–196, Prague, June 2007. c�2007 Association for Computational Linguistics −2 log(A) is asymptotically well approximated by x2 distribution. A word appears in the input significantly more often than in the background corpus when −2 log(A) &gt; 10. Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization. Each descriptive word is assigned an equal weight and the rest of the words have a weight of zero: R(w) = 1 if (−2log(A(w)) &gt; 10), 0 otherwise. This weighting scheme has been adopted in several recent generic and topic-focused summarizers (Conroy et al., 2006; Lacatusu et al., 2006). LLR(CQ) The above three weighting schemes assign a weight to words regardless of the user query and are most appropriate for generic summarization. When a user query is available, it should inform the summarizer to make the sum</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>C. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of COLING’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
</authors>
<title>Rouge: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out (WAS</booktitle>
<contexts>
<context position="7034" citStr="Lin, 2004" startWordPosition="1129" endWordPosition="1130"> nouns, verbs, adjectives and adverbs are considered and a short list of light verbs are excluded: “has, was, have, are, will, were, do, been, say, said, says”. For FOCUSED summarization, we modify this algorithm merely by running the sentence selection algorithm on only those sentences in the input that are relevent to the user query. In some previous DUC evaluations, relevant sentences are explicitly marked by annotators and given to systems. In our version here, a sentence in the input is considered relevant if it contains at least one word from the user query. For evaluation we use ROUGE (Lin, 2004) SU4 recall metric&apos;, which was among the official automatic evaluation metrics for DUC. 4 Results The results are shown in Table 1. The focused summarizer using LLR(CQ) is the best, and it significantly outperforms the focused summarizer based on frequency. Also, LLR (using log-likelihood ratio to assign weights to all words) perfroms significantly worse than LLR(C). We can observe some trends even from the results for which there is no significance. Both LLR and LLR(C) are sensitive to the introduction of topic relevance, producing somewhat better summaries in the FOCUSED scenario 1-n 2 -x -m</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Lin. 2004. Rouge: a package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBMJournal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="1789" citStr="Luhn, 1958" startWordPosition="259" endWordPosition="260">e summary is meant to give an overview of the information in the documents. By contrast, when the summary is produced in response to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging. In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario. First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006). While both schemes have proved to be suitable for summarization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ. Second, there are many little-understood aspects of the differences between generic and query-focused summarization. For example, we’d like to know if a particular word weighting scheme is more suitable for focused summarization than others. More significantly, previ</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBMJournal of Research and Development, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3069" citStr="Manning and Schutze, 1999" startWordPosition="467" endWordPosition="470">orm very similarly to each other in query-focused summarization (Nenkova, 2005) and it is of interest to find out why. To address these questions we examine the two weighting schemes: raw frequency (or word probability estimated from the input), and log-likelihood ratio (LLR) and two of its variants. These metrics are used to assign importance to individual content words in the input, as we discuss below. Word probability R(w) = &apos;, where n is the number of times the word w appeared in the input and N is the total number of words in the input. Log-likelihood ratio (LLR) The likelihood ratio A (Manning and Schutze, 1999) uses a background corpus to estimate the importance of a word and it is proportional to the mutual information between a word w and the input to be summarized; A(w) is defined as the ratio between the probability (under a binomial distribution) of observing w in the input and the background corpus assuming equal probability of occurrence of w in both and the probability of the data assuming different probabilities for w in the input and the background corpus. LLR with cut-off (LLR(C)) A useful property of the log-likelihood ratio is that the quantity 193 Proceedings of the ACL 2007 Demo and P</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. Manning and H. Schutze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
<author>K McKeown</author>
</authors>
<title>A compositional context sensitive multi-document summarizer: Exploring the factors that influence summarization.</title>
<date>2006</date>
<booktitle>In Proceedings ofACM SIGIR’06.</booktitle>
<contexts>
<context position="1811" citStr="Nenkova et al., 2006" startWordPosition="261" endWordPosition="264"> meant to give an overview of the information in the documents. By contrast, when the summary is produced in response to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging. In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario. First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006). While both schemes have proved to be suitable for summarization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ. Second, there are many little-understood aspects of the differences between generic and query-focused summarization. For example, we’d like to know if a particular word weighting scheme is more suitable for focused summarization than others. More significantly, previous studies show that </context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A compositional context sensitive multi-document summarizer: Exploring the factors that influence summarization. In Proceedings ofACM SIGIR’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
</authors>
<title>Automatic text summarization of newswire: lessons learned from the document understanding conference.</title>
<date>2005</date>
<booktitle>In Proceedings ofAAAI’05.</booktitle>
<contexts>
<context position="2522" citStr="Nenkova, 2005" startWordPosition="371" endWordPosition="372">atusu et al., 2006). While both schemes have proved to be suitable for summarization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ. Second, there are many little-understood aspects of the differences between generic and query-focused summarization. For example, we’d like to know if a particular word weighting scheme is more suitable for focused summarization than others. More significantly, previous studies show that generic and focused systems perform very similarly to each other in query-focused summarization (Nenkova, 2005) and it is of interest to find out why. To address these questions we examine the two weighting schemes: raw frequency (or word probability estimated from the input), and log-likelihood ratio (LLR) and two of its variants. These metrics are used to assign importance to individual content words in the input, as we discuss below. Word probability R(w) = &apos;, where n is the number of times the word w appeared in the input and N is the total number of words in the input. Log-likelihood ratio (LLR) The likelihood ratio A (Manning and Schutze, 1999) uses a background corpus to estimate the importance </context>
</contexts>
<marker>Nenkova, 2005</marker>
<rawString>A. Nenkova. 2005. Automatic text summarization of newswire: lessons learned from the document understanding conference. In Proceedings ofAAAI’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
<author>H Suzuki</author>
<author>C Brockett</author>
</authors>
<title>Microsoft research at duc 2006: Task-focused summarization with sentence simplification and lexical expansion.</title>
<date>2006</date>
<booktitle>In Proceedings of DUC’06.</booktitle>
<contexts>
<context position="1838" citStr="Vanderwende et al., 2006" startWordPosition="265" endWordPosition="268">view of the information in the documents. By contrast, when the summary is produced in response to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging. In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario. First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006). While both schemes have proved to be suitable for summarization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ. Second, there are many little-understood aspects of the differences between generic and query-focused summarization. For example, we’d like to know if a particular word weighting scheme is more suitable for focused summarization than others. More significantly, previous studies show that generic and focused systems</context>
</contexts>
<marker>Vanderwende, Suzuki, Brockett, 2006</marker>
<rawString>L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft research at duc 2006: Task-focused summarization with sentence simplification and lexical expansion. In Proceedings of DUC’06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>