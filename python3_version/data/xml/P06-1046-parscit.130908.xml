<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002199">
<title confidence="0.99227">
Scaling Distributional Similarity to Large Corpora
</title>
<author confidence="0.995094">
James Gorman and James R. Curran
</author>
<affiliation confidence="0.9863705">
School of Information Technologies
University of Sydney
</affiliation>
<note confidence="0.683219">
NSW 2006, Australia
</note>
<email confidence="0.936494">
{jgorman2,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.980525" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999495428571429">
Accurately representing synonymy using
distributional similarity requires large vol-
umes of data to reliably represent infre-
quent words. However, the naive nearest-
neighbour approach to comparing context
vectors extracted from large corpora scales
poorly (O(n2) in the vocabulary size).
In this paper, we compare several existing
approaches to approximating the nearest-
neighbour search for distributional simi-
larity. We investigate the trade-off be-
tween efficiency and accuracy, and find
that SASH (Houle and Sakuma, 2005) pro-
vides the best balance.
</bodyText>
<sectionHeader confidence="0.995156" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999655285714286">
It is a general property of Machine Learning that
increasing the volume of training data increases
the accuracy of results. This is no more evident
than in Natural Language Processing (NLP), where
massive quantities of text are required to model
rare language events. Despite the rapid increase in
computational power available for NLP systems,
the volume of raw data available still outweighs
our ability to process it. Unsupervised learning,
which does not require the expensive and time-
consuming human annotation of data, offers an
opportunity to use this wealth of data. Curran
and Moens (2002) show that synonymy extraction
for lexical semantic resources using distributional
similarity produces continuing gains in accuracy
as the volume of input data increases.
Extracting synonymy relations using distribu-
tional similarity is based on the distributional hy-
pothesis that similar words appear in similar con-
texts. Terms are described by collating informa-
tion about their occurrence in a corpus into vec-
tors. These context vectors are then compared for
similarity. Existing approaches differ primarily in
their definition of “context”, e.g. the surrounding
words or the entire document, and their choice of
distance metric for calculating similarity between
the context vectors representing each term.
Manual creation of lexical semantic resources
is open to the problems of bias, inconsistency and
limited coverage. It is difficult to account for the
needs of the many domains in which NLP tech-
niques are now being applied and for the rapid
change in language use. The assisted or auto-
matic creation and maintenance of these resources
would be of great advantage.
Finding synonyms using distributional similar-
ity requires a nearest-neighbour search over the
context vectors of each term. This is computation-
ally intensive, scaling to O(n2m) for the number
of terms n and the size of their context vectors m.
Increasing the volume of input data will increase
the size of both n and m, decreasing the efficiency
of a naive nearest-neighbour approach.
Many approaches to reduce this complexity
have been suggested. In this paper we evaluate
state-of-the-art techniques proposed to solve this
problem. We find that the Spatial Approximation
Sample Hierarchy (Houle and Sakuma, 2005) pro-
vides the best accuracy/efficiency trade-off.
</bodyText>
<sectionHeader confidence="0.977458" genericHeader="introduction">
2 Distributional Similarity
</sectionHeader>
<bodyText confidence="0.999820142857143">
Measuring distributional similarity first requires
the extraction of context information for each of
the vocabulary terms from raw text. These terms
are then compared for similarity using a nearest-
neighbour search or clustering based on distance
calculations between the statistical descriptions of
their contexts.
</bodyText>
<note confidence="0.783524333333333">
361
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 361–368,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.811986">
2.1 Extraction
</subsectionHeader>
<bodyText confidence="0.999992923076923">
A context relation is defined as a tuple (w, r, w0)
where w is a term, which occurs in some grammat-
ical relation r with another word w0 in some sen-
tence. We refer to the tuple (r, w0) as an attribute
of w. For example, (dog, direct-obj, walk) indicates
that dog was the direct object of walk in a sentence.
In our experiments context extraction begins
with a Maximum Entropy POS tagger and chun-
ker. The SEXTANT relation extractor (Grefen-
stette, 1994) produces context relations that are
then lemmatised. The relations for each term are
collected together and counted, producing a vector
of attributes and their frequencies in the corpus.
</bodyText>
<subsectionHeader confidence="0.983897">
2.2 Measures and Weights
</subsectionHeader>
<bodyText confidence="0.999988545454546">
Both nearest-neighbour and cluster analysis meth-
ods require a distance measure to calculate the
similarity between context vectors. Curran (2004)
decomposes this into measure and weight func-
tions. The measure calculates the similarity
between two weighted context vectors and the
weight calculates the informativeness of each con-
text relation from the raw frequencies.
For these experiments we use the Jaccard (1)
measure and the TTest (2) weight functions, found
by Curran (2004) to have the best performance.
</bodyText>
<equation confidence="0.9944365">
p(w, r, w0) − p(*, r, w0)p(w, *, *)(2)
VIp(*, r, w0)p(w, *, *)
</equation>
<subsectionHeader confidence="0.915366">
2.3 Nearest-neighbour Search
</subsectionHeader>
<bodyText confidence="0.9999889">
The simplest algorithm for finding synonyms is
a k-nearest-neighbour (k-NN) search, which in-
volves pair-wise vector comparison of the target
term with every term in the vocabulary. Given an
n term vocabulary and up to m attributes for each
term, the asymptotic time complexity of nearest-
neighbour search is O(n2m). This is very expen-
sive, with even a moderate vocabulary making the
use of huge datasets infeasible. Our largest exper-
iments used a vocabulary of over 184,000 words.
</bodyText>
<sectionHeader confidence="0.984514" genericHeader="method">
3 Dimensionality Reduction
</sectionHeader>
<bodyText confidence="0.99989475">
Using a cut-off to remove low frequency terms
can significantly reduce the value of n. Unfortu-
nately, reducing m by eliminating low frequency
contexts has a significant impact on the quality of
the results. There are many techniques to reduce
dimensionality while avoiding this problem. The
simplest methods use feature selection techniques,
such as information gain, to remove the attributes
that are less informative. Other techniques smooth
the data while reducing dimensionality.
Latent Semantic Analysis (LSA, Landauer and
Dumais, 1997) is a smoothing and dimensional-
ity reduction technique based on the intuition that
the true dimensionality of data is latent in the sur-
face dimensionality. Landauer and Dumais admit
that, from a pragmatic perspective, the same effect
as LSA can be generated by using large volumes
of data with very long attribute vectors. Experi-
ments with LSA typically use attribute vectors of a
dimensionality of around 1000. Our experiments
have a dimensionality of 500,000 to 1,500,000.
Decompositions on data this size are computation-
ally difficult. Dimensionality reduction is often
used before using LSA to improve its scalability.
</bodyText>
<subsectionHeader confidence="0.997766">
3.1 Heuristics
</subsectionHeader>
<bodyText confidence="0.99990468">
Another technique is to use an initial heuristic
comparison to reduce the number of full O(m)
vector comparisons that are performed. If the
heuristic comparison is sufficiently fast and a suffi-
cient number of full comparisons are avoided, the
cost of an additional check will be easily absorbed
by the savings made.
Curran and Moens (2002) introduces a vector of
canonical attributes (of bounded length k « m),
selected from the full vector, to represent the term.
These attributes are the most strongly weighted
verb attributes, chosen because they constrain the
semantics of the term more and partake in fewer
idiomatic collocations. If a pair of terms share at
least one canonical attribute then a full similarity
comparison is performed, otherwise the terms are
not compared. They show an 89% reduction in
search time, with only a 3.9% loss in accuracy.
There is a significant improvement in the com-
putational complexity. If a maximum of p posi-
tive results are returned, our complexity becomes
O(n2k + npm). When p « n, the system will
be faster as many fewer full comparisons will be
made, but at the cost of accuracy as more possibly
near results will be discarded out of hand.
</bodyText>
<sectionHeader confidence="0.989013" genericHeader="method">
4 Randomised Techniques
</sectionHeader>
<bodyText confidence="0.9944055">
Conventional dimensionality reduction techniques
can be computationally expensive: a more scal-
</bodyText>
<equation confidence="0.833157">
� ������max(w(wm, r, w0), w(w, r, w0)) (1)
� ������min(w(wm, r, w0), w(w, r, w0))
362
</equation>
<bodyText confidence="0.999621571428571">
able solution is required to handle the volumes of
data we propose to use. Randomised techniques
provide a possible solution to this.
We present two techniques that have been used
recently for distributional similarity: Random In-
dexing (Kanerva et al., 2000) and Locality Sensi-
tive Hashing (LSH, Broder, 1997).
</bodyText>
<subsectionHeader confidence="0.996613">
4.1 Random Indexing
</subsectionHeader>
<bodyText confidence="0.999751608695652">
Random Indexing (RI) is a hashing technique
based on Sparse Distributed Memory (Kanerva,
1993). Karlgren and Sahlgren (2001) showed RI
produces results similar to LSA using the Test of
English as a Foreign Language (TOEFL) evalua-
tion. Sahlgren and Karlgren (2005) showed the
technique to be successful in generating bilingual
lexicons from parallel corpora.
In RI, we first allocate a d length index vec-
tor to each unique attribute. The vectors con-
sist of a large number of 0s and small number
(E) number of randomly distributed ±1s. Context
vectors, identifying terms, are generated by sum-
ming the index vectors of the attributes for each
non-unique context in which a term appears. The
context vector for a term t appearing in contexts
c1 = [1, 0, 0, −1] and c2 = [0, 1, 0, −1] would be
[1, 1, 0, −2]. The distance between these context
vectors is then measured using the cosine measure:
vector is created by sampling a Gaussian function
m&apos; times, with a mean of 0 and a variance of 1.
For each term w we construct its bit signature
using the function
</bodyText>
<equation confidence="0.900996333333333">
�
1 : ~r.~w &gt; 0
h~r(~w) =0 : ~r.~w &lt; 0
</equation>
<bodyText confidence="0.998345666666667">
where r~ is a spherically symmetric random vector
of length d. The signature, w, is the d length bit
vector:
</bodyText>
<equation confidence="0.995802">
w� = {h~r�(~w), h~r�(~w), ... , h~rd(~w)}
</equation>
<bodyText confidence="0.994221333333333">
The cost to build all n signatures is O(nm&apos;d).
For terms u and v, Goemans and Williamson
(1995) approximate the angular similarity by
</bodyText>
<equation confidence="0.9995125">
p(h~r(~u) = h~r(~v)) = 1 − π
θ(~u, ~u) (4)
</equation>
<bodyText confidence="0.9977065">
where θ(~u, ~u) is the angle between u~ and ~u. The
angular similarity gives the cosine by
</bodyText>
<equation confidence="0.9989775">
cos(θ(~u, ~u)) = (5)
cos((1 − p(h~r(~u) = h~r(~v)))π)
</equation>
<bodyText confidence="0.90147">
The probability can be derived from the Hamming
distance:
</bodyText>
<equation confidence="0.9867516">
p(hr(u) = hr(v)) = 1 − x(�ud, v) (6)
cos(θ(u,v)) =
(3)
|~u||~v|
u· v
</equation>
<bodyText confidence="0.9991068">
This technique allows for incremental sampling,
where the index vector for an attribute is only gen-
erated when the attribute is encountered. Con-
struction complexity is O(nmd) and search com-
plexity is O(n2d).
</bodyText>
<subsectionHeader confidence="0.99822">
4.2 Locality Sensitive Hashing
</subsectionHeader>
<bodyText confidence="0.999943933333334">
LSH is a probabilistic technique that allows the
approximation of a similarity function. Broder
(1997) proposed an approximation of the Jaccard
similarity function using min-wise independent
functions. Charikar (2002) proposed an approx-
imation of the cosine measure using random hy-
perplanes Ravichandran et al. (2005) used this co-
sine variant and showed it to produce over 70%
accuracy in extracting synonyms when compared
against Pantel and Lin (2002).
Given we have n terms in an m&apos; dimensional
space, we create d « m&apos; unit random vectors also
of m&apos; dimensions, labelled {~r1, ~r2,..., ~rd}. Each
By combining equations 5 and 6 we get the fol-
lowing approximation of the cosine distance:
</bodyText>
<equation confidence="0.847042">
cos (θ(~u , ~u)) = cos((x(u,�v) ) π) (7)
</equation>
<bodyText confidence="0.9991982">
That is, the cosine of two context vectors is ap-
proximated by the cosine of the Hamming distance
between their two signatures normalised by the
size of the signatures. Search is performed using
Equation 7 and scales to O(n2d).
</bodyText>
<sectionHeader confidence="0.95966" genericHeader="method">
5 Data Structures
</sectionHeader>
<bodyText confidence="0.999994285714286">
The methods presented above fail to address the
n2 component of the search complexity. Many
data structures have been proposed that can be
used to address this problem in similarity search-
ing. We present three data structures: the vantage
point tree (VPT, Yianilos, 1993), which indexes
points in a metric space, Point Location in Equal
</bodyText>
<page confidence="0.792588">
363
</page>
<bodyText confidence="0.999778083333333">
Balls (PLEB, Indyk and Motwani, 1998), a proba-
bilistic structure that uses the bit signatures gener-
ated by LSH, and the Spatial Approximation Sam-
ple Hierarchy (SASH, Houle and Sakuma, 2005),
which approximates a k-NN search.
Another option inspired by IR is attribute index-
ing (INDEX). In this technique, in addition to each
term having a reference to its attributes, each at-
tribute has a reference to the terms referencing it.
Each term is then only compared with the terms
with which it shares attributes. We will give a the-
oretically comparison against other techniques.
</bodyText>
<subsectionHeader confidence="0.983806">
5.1 Vantage Point Tree
</subsectionHeader>
<bodyText confidence="0.999566545454545">
Metric space data structures provide a solution to
near-neighbour searches in very high dimensions.
These rely solely on the existence of a compari-
son function that satisfies the conditions of metri-
cality: non-negativity, equality, symmetry and the
triangle inequality.
VPT is typical of these structures and has been
used successfully in many applications. The VPT
is a binary tree designed for range searches. These
are searches limited to some distance from the tar-
get term but can be modified for k-NN search.
VPT is constructed recursively. Beginning with
a set of U terms, we take any term to be our van-
tage point p. This becomes our root. We now find
the median distance mp of all other terms to p:
mp = median{dist(p,u)|u E U}. Those terms
u such that dist(p, u) &lt; mp are inserted into the
left sub-tree, and the remainder into the right sub-
tree. Each sub-tree is then constructed as a new
VPT, choosing a new vantage point from within its
terms, until all terms are exhausted.
Searching a VPT is also recursive. Given a term
q and radius r, we begin by measuring the distance
to the root term p. If dist(q, p) &lt; r we enter p into
our list of near terms. If dist(q, p) − r &lt; mp we
enter the left sub-tree and if dist(q, p) + r &gt; mp
we enter the right sub-tree. Both sub-trees may be
entered. The process is repeated for each entered
subtree, taking the vantage point of the sub-tree to
be the new root term.
To perform a k-NN search we use a back-
tracking decreasing radius search (Burkhard and
Keller, 1973). The search begins with r = oo,
and terms are added to a list of the closest k terms.
When the kth closest term is found, the radius is
set to the distance between this term and the tar-
get. Each time a new, closer element is added to
the list, the radius is updated to the distance from
the target to the new kth closest term.
Construction complexity is O(n log n). Search
complexity is claimed to be O(log n) for small ra-
dius searches. This does not hold for our decreas-
ing radius search, whose worst case complexity is
O(n).
</bodyText>
<subsectionHeader confidence="0.998056">
5.2 Point Location in Equal Balls
</subsectionHeader>
<bodyText confidence="0.992891727272727">
PLEB is a randomised structure that uses the bit
signatures generated by LSH. It was used by
Ravichandran et al. (2005) to improve the effi-
ciency of distributional similarity calculations.
Having generated our d length bit signatures for
each of our n terms, we take these signatures and
randomly permute the bits. Each vector has the
same permutation applied. This is equivalent to a
column reordering in a matrix where the rows are
the terms and the columns the bits. After applying
the permutation, the list of terms is sorted lexico-
graphically based on the bit signatures. The list is
scanned sequentially, and each term is compared
to its B nearest neighbours in the list. The choice
of B will effect the accuracy/efficiency trade-off,
and need not be related to the choice of k. This is
performed q times, using a different random per-
mutation function each time. After each iteration,
the current closest k terms are stored.
For a fixed d, the complexity for the permuta-
tion step is O(qn), the sorting O(qn log n) and the
search O(qBn).
</bodyText>
<subsectionHeader confidence="0.985993">
5.3 Spatial Approximation Sample Hierarchy
</subsectionHeader>
<bodyText confidence="0.999966">
SASH approximates a k-NN search by precomput-
ing some near neighbours for each node (terms in
our case). This produces multiple paths between
terms, allowing SASH to shape itself to the data
set (Houle, 2003). The following description is
adapted from Houle and Sakuma (2005).
The SASH is a directed, edge-weighted graph
with the following properties (see Figure 1):
</bodyText>
<listItem confidence="0.9976198">
• Each term corresponds to a unique node.
• The nodes are arranged into a hierarchy of
levels, with the bottom level containing 2
nodes and the top containing a single root
node. Each level, except the top, will contain
half as many nodes as the level below.
• Edges between nodes are linked to consecu-
tive levels. Each node will have at most p
parent nodes in the level above, and c child
nodes in the level below.
</listItem>
<figure confidence="0.97819625">
364
A
1
B C D
E F G H
I J
5
K L
</figure>
<figureCaption confidence="0.90724">
Figure 1: A SASH, where p = 2, c = 3 and k = 2
</figureCaption>
<figure confidence="0.952988666666667">
2
3
4
</figure>
<listItem confidence="0.9953655">
• Every node must have at least one parent so
that all nodes are reachable from the root.
</listItem>
<bodyText confidence="0.999903617647059">
Construction begins with the nodes being ran-
domly distributed between the levels. SASH is
then constructed iteratively by each node finding
its closest p parents in the level above. The par-
ent will keep the closest c of these children, form-
ing edges in the graph, and reject the rest. Any
nodes without parents after being rejected are then
assigned as children of the nearest node in the pre-
vious level with fewer than c children.
Searching is performed by finding the k nearest
nodes at each level, which are added to a set of
near nodes. To limit the search, only those nodes
whose parents were found to be nearest at the pre-
vious level are searched. The k closest nodes from
the set of near nodes are then returned. The search
complexity is O(ck log n).
In Figure 1, the filled nodes demonstrate a
search for the near-neighbours of some node q, us-
ing k = 2. Our search begins with the root node
A. As we are using k = 2, we must find the two
nearest children of A using our similarity measure.
In this case, C and D are closer than B. We now
find the closest two children of C and D. E is not
checked as it is only a child of B. All other nodes
are checked, including F and G, which are shared
as children by B and C. From this level we chose
G and H. The final levels are considered similarly.
At this point we now have the list of near nodes
A, C, D, G, H, I, J, K and L. From this we
chose the two nodes nearest q, H and I marked in
black, which are then returned.
k can be varied at each level to force a larger
number of elements to be tested at the base of the
SASH using, for instance, the equation:
</bodyText>
<equation confidence="0.9520635">
1_ h−i 1
ki = max{ k log n , 2pc } (8)
This changes our search complexity to:
_1 + pc2 log n (9)
</equation>
<bodyText confidence="0.999722304347826">
We use this geometric function in our experiments.
Gorman and Curran (2005a; 2005b) found the
performance of SASH for distributional similarity
could be improved by replacing the initial random
ordering with a frequency based ordering. In ac-
cordance with Zipf’s law, the majority of terms
have low frequencies. Comparisons made with
these low frequency terms are unreliable (Curran
and Moens, 2002). Creating SASH with high fre-
quency terms near the root produces more reliable
initial paths, but comparisons against these terms
are more expensive.
The best accuracy/efficiency trade-off was
found when using more reliable initial paths rather
than the most reliable. This is done by folding the
data around some mean number of relations. For
each term, if its number of relations mi is greater
than some chosen number of relations M, it is
given a new ranking based on the score ��
�i . Oth-
erwise its ranking based on its number of relations.
This has the effect of pushing very high and very
low frequency terms away from the root.
</bodyText>
<sectionHeader confidence="0.995317" genericHeader="method">
6 Evaluation Measures
</sectionHeader>
<bodyText confidence="0.99995725">
The simplest method for evaluation is the direct
comparison of extracted synonyms with a manu-
ally created gold standard (Grefenstette, 1994). To
reduce the problem of limited coverage, our evalu-
ation combines three electronic thesauri: the Mac-
quarie, Roget’s and Moby thesauri.
We follow Curran (2004) and use two perfor-
mance measures: direct matches (DIRECT) and
inverse rank (INVR). DIRECT is the percentage
of returned synonyms found in the gold standard.
INVR is the sum of the inverse rank of each match-
ing synonym, e.g. matches at ranks 3, 5 and 28
</bodyText>
<figure confidence="0.985247133333333">
k1+ 1
log n
1
k
log n
365
CUT-OFF
5 100
NAIVE 1.72 1.71
HEURISTIC 1.65 1.66
RI 0.80 0.93
LSH10,000 1.26 1.31
SASH 1.73 1.71
Table 2: INVR vs frequency cut-off
CORPUS CUT-OFF
TERMS AVERAGE
RELATIONS
PER TERM
BNC 0
5
100
246,067 43
88,926 116
14,862 617
LARGE 0
5
100
541,722 97
184,494 281
35,618 1,400
</figure>
<tableCaption confidence="0.988976">
Table 1: Extracted Context Information
</tableCaption>
<bodyText confidence="0.999978666666667">
give an inverse rank score of 13 + 51 + 128. With
at most 100 matching synonyms, the maximum
INVR is 5.187. This more fine grained as it in-
corporates the both the number of matches and
their ranking. The same 300 single word nouns
were used for evaluation as used by Curran (2004)
for his large scale evaluation. These were chosen
randomly from WordNet such that they covered
a range over the following properties: frequency,
number of senses, specificity and concreteness.
For each of these terms, the closest 100 terms and
their similarity scores were extracted.
</bodyText>
<sectionHeader confidence="0.99597" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.986298054054054">
We use two corpora in our experiments: the
smaller is the non-speech portion of the British
National Corpus (BNC), 90 million words covering
a wide range of domains and formats; the larger
consists of the BNC, the Reuters Corpus Volume 1
and most of the English news holdings of the LDC
in 2003, representing over 2 billion words of text
(LARGE, Curran, 2004).
The semantic similarity system implemented by
Curran (2004) provides our baseline. This per-
forms a brute-force k-NN search (NAIVE). We
present results for the canonical attribute heuristic
(HEURISTIC), RI, LSH, PLEB, VPT and SASH.
We take the optimal canonical attribute vector
length of 30 for HEURISTIC from Curran (2004).
For SASH we take optimal values of p = 4 and c =
16 and use the folded ordering taking M = 1000
from Gorman and Curran (2005b).
For RI, LSH and PLEB we found optimal values
experimentally using the BNC. For LSH we chose
d = 3, 000 (LSH3,000) and 10, 000 (LSH10,000),
showing the effect of changing the dimensionality.
The frequency statistics were weighted using mu-
tual information, as in Ravichandran et al. (2005):
p(w r w&apos;)
The initial experiments on RI produced quite
poor results. The intuition was that this was
caused by the lack of smoothing in the algo-
rithm. Experiments were performed using the
weights given in Curran (2004). Of these, mu-
tual information (10), evaluated with an extra
log2(�(w, r, w&apos;) + 1) factor and limited to posi-
tive values, produced the best results (RIMI). The
values d = 1000 and E = 5 were found to produce
the best results.
All experiments were performed on 3.2GHz
Xeon P4 machines with 4GB of RAM.
</bodyText>
<sectionHeader confidence="0.999195" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.999967379310345">
As the accuracy of comparisons between terms in-
creases with frequency (Curran, 2004), applying a
frequency cut-off will both reduce the size of the
vocabulary (n) and increase the average accuracy
of comparisons. Table 1 shows the reduction in
vocabulary and increase in average context rela-
tions per term as cut-off increases. For LARGE,
the initial 541,722 word vocabulary is reduced by
66% when a cut-off of 5 is applied and by 86%
when the cut-off is increased to 100. The average
number of relations increases from 97 to 1400.
The work by Curran (2004) largely uses a fre-
quency cut-off of 5. When this cut-off was used
with the randomised techniques RI and LSH, it pro-
duced quite poor results. When the cut-off was
increased to 100, as used by Ravichandran et al.
(2005), the results improved significantly. Table 2
shows the INVR scores for our various techniques
using the BNC with cut-offs of 5 and 100.
Table 3 shows the results of a full thesaurus ex-
traction using the BNC and LARGE corpora using
a cut-off of 100. The average DIRECT score and
INVR are from the 300 test words. The total exe-
cution time is extrapolated from the average search
time of these test words and includes the setup
time. For LARGE, extraction using NAIVE takes
444 hours: over 18 days. If the 184,494 word vo-
cabulary were used, it would take over 7000 hours,
or nearly 300 days. This gives some indication of
</bodyText>
<table confidence="0.929576333333333">
log(p(w, *, *)p(*, r, w&apos;)) (10)
PLEB used the values q = 500 and B = 100.
366
DIRECT BNC Time DIRECT LARGE Time
INVR INVR
NAIVE 5.23 1.71 38.0hr 5.70 1.93 444.3hr
HEURISTIC 4.94 1.66 2.0hr 5.51 1.93 30.2hr
RI 2.97 0.93 0.4hr 2.42 0.85 1.9hr
RIMI 3.49 1.41 0.4hr 4.58 1.75 1.9hr
LSH3,000 2.00 0.76 0.7hr 2.92 1.07 3.6hr
LSH10,000 3.68 1.31 2.3hr 3.77 1.40 8.4hr
PLEB3,000 2.00 0.76 1.2hr 2.85 1.07 4.1hr
PLEB10,000 3.66 1.30 3.9hr 3.63 1.37 11.8hr
VPT 5.23 1.71 15.9hr 5.70 1.93 336.1hr
SASH 5.17 1.71 2.0hr 5.29 1.89 23.7hr
</table>
<tableCaption confidence="0.998984">
Table 3: Full thesaurus extraction
</tableCaption>
<bodyText confidence="0.999846325">
the scale of the problem.
The only technique to become less accurate
when the corpus size is increased is RI; it is likely
that RI is sensitive to high frequency, low informa-
tion contexts that are more prevalent in LARGE.
Weighting reduces this effect, improving accuracy.
The importance of the choice of d can be seen in
the results for LSH. While much slower, LSH10,000
is also much more accurate than LSH3,000, while
still being much faster than NAIVE. Introducing
the PLEB data structure does not improve the ef-
ficiency while incurring a small cost on accuracy.
We are not using large enough datasets to show the
improved time complexity using PLEB.
VPT is only slightly faster slightly faster than
NAIVE. This is not surprising in light of the origi-
nal design of the data structure: decreasing radius
search does not guarantee search efficiency.
A significant influence in the speed of the ran-
domised techniques, RI and LSH, is the fixed di-
mensionality. The randomised techniques use a
fixed length vector which is not influenced by the
size of m. The drawback of this is that the size of
the vector needs to be tuned to the dataset.
It would seem at first glance that HEURIS-
TIC and SASH provide very similar results, with
HEURISTIC slightly slower, but more accurate.
This misses the difference in time complexity be-
tween the methods: HEURISTIC is n2 and SASH
n log n. The improvement in execution time over
NAIVE decreases as corpus size increases and this
would be expected to continue. Further tuning of
SASH parameters may improve its accuracy.
RIMI produces similar result using LARGE to
SASH using BNC. This does not include the cost
of extracting context relations from the raw text, so
the true comparison is much worse. SASH allows
the free use of weight and measure functions, but
RI is constrained by having to transform any con-
text space into a RI space. This is important when
</bodyText>
<table confidence="0.9933718">
LARGE
CUT-OFF 0 5 100
NAIVE 541,721 184,493 35,617
SASH 10,599 8,796 6,231
INDEX 5,844 13,187 32,663
</table>
<tableCaption confidence="0.999048">
Table 4: Average number of comparisons per term
</tableCaption>
<bodyText confidence="0.999934727272727">
considering that different tasks may require differ-
ent weights and measures (Weeds and Weir, 2005).
RI also suffers n2 complexity, where as SASH is
n log n. Taking these into account, and that the im-
provements are barely significant, SASH is a better
choice.
The results for LSH are disappointing. It per-
forms consistently worse than the other methods
except VPT. This could be improved by using
larger bit vectors, but there is a limit to the size of
these as they represent a significant memory over-
head, particularly as the vocabulary increases.
Table 4 presents the theoretical analysis of at-
tribute indexing. The average number of com-
parisons made for various cut-offs of LARGE are
shown. NAIVE and INDEX are the actual values
for those techniques. The values for SASH are
worst case, where the maximum number of terms
are compared at each level. The actual number
of comparisons made will be much less. The ef-
ficiency of INDEX is sensitive to the density of
attributes and increasing the cut-off increases the
density. This is seen in the dramatic drop in per-
formance as the cut-off increases. This problem of
density will increase as volume of raw input data
increases, further reducing its effectiveness. SASH
is only dependent on the number of terms, not the
density.
Where the need for computationally efficiency
out-weighs the need for accuracy, RIMI provides
better results. SASH is the most balanced of the
techniques tested and provides the most scalable,
high quality results.
</bodyText>
<page confidence="0.781335">
367
</page>
<sectionHeader confidence="0.991488" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999982619047619">
We have evaluated several state-of-the-art tech-
niques for improving the efficiency of distribu-
tional similarity measurements. We found that,
in terms of raw efficiency, Random Indexing (RI)
was significantly faster than any other technique,
but at the cost of accuracy. Even after our mod-
ifications to the RI algorithm to significantly im-
prove its accuracy, SASH still provides a better ac-
curacy/efficiency trade-off. This is more evident
when considering the time to extract context in-
formation from the raw text. SASH, unlike RI, also
allows us to choose both the weight and the mea-
sure used. LSH and PLEB could not match either
the efficiency of RI or the accuracy of SASH.
We intend to use this knowledge to process even
larger corpora to produce more accurate results.
Having set out to improve the efficiency of dis-
tributional similarity searches while limiting any
loss in accuracy, we are producing full nearest-
neighbour searches 18 times faster, with only a 2%
loss in accuracy.
</bodyText>
<sectionHeader confidence="0.992142" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99904125">
We would like to thank our reviewers for their
helpful feedback and corrections. This work has
been supported by the Australian Research Coun-
cil under Discovery Project DP0453131.
</bodyText>
<sectionHeader confidence="0.996411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998723405063291">
Andrei Broder. 1997. On the resemblance and containment
of documents. In Proceedings of the Compression and
Complexity of Sequences, pages 21–29, Salerno, Italy.
Walter A. Burkhard and Robert M. Keller. 1973. Some ap-
proaches to best-match file searching. Communications of
the ACM, 16(4):230–236, April.
Moses S. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of the 34th
Annual ACM Symposium on Theory of Computing, pages
380–388, Montreal, Quebec, Canada, 19–21 May.
James Curran and Marc Moens. 2002. Improvements in au-
tomatic thesaurus extraction. In Proceedings of the Work-
shop of the ACL Special Interest Group on the Lexicon,
pages 59–66, Philadelphia, PA, USA, 12 July.
James Curran. 2004. From Distributional to Semantic Simi-
larity. Ph.D. thesis, University of Edinburgh.
Michel X. Goemans and David P. Williamson. 1995.
Improved approximation algorithms for maximum cut
and satisfiability problems using semidefinite program-
ming. Journal of Association for Computing Machinery,
42(6):1115–1145, November.
James Gorman and James Curran. 2005a. Approximate
searching for distributional similarity. In ACL-SIGLEX
2005 Workshop on Deep Lexical Acquisition, Ann Arbor,
MI, USA, 30 June.
James Gorman and James Curran. 2005b. Augmenting ap-
proximate similarity searching with lexical information.
In Australasian Language Technology Workshop, Sydney,
Australia, 9–11 November.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, Boston.
Michael E. Houle and Jun Sakuma. 2005. Fast approximate
similarity search in extremely high-dimensional data sets.
In Proceedings of the 21st International Conference on
Data Engineering, pages 619–630, Tokyo, Japan.
Michael E. Houle. 2003. Navigating massive data sets via
local clustering. In Proceedings of the 9th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, pages 547–552, Washington, DC, USA.
Piotr Indyk and Rajeev Motwani. 1998. Approximate near-
est neighbors: towards removing the curse of dimension-
ality. In Proceedings of the 30th annual ACM Symposium
on Theory of Computing, pages 604–613, New York, NY,
USA, 24–26 May. ACM Press.
Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000.
Random indexing of text samples for latent semantic anal-
ysis. In Proceedings of the 22nd Annual Conference of the
Cognitive Science Society, page 1036, Mahwah, NJ, USA.
Pentti Kanerva. 1993. Sparse distributed memory and re-
lated models. In M.H. Hassoun, editor, Associative Neu-
ral Memories: Theory and Implementation, pages 50–76.
Oxford University Press, New York, NY, USA.
Jussi Karlgren and Magnus Sahlgren. 2001. From words to
understanding. In Y. Uesaka, P. Kanerva, and H Asoh, ed-
itors, Foundations of Real-World Intelligence, pages 294–
308. CSLI Publications, Stanford, CA, USA.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to plato’s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211–240, April.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD-02,
pages 613–619, 23–26 July.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and NLP: Using locality
sensitive hash functions for high speed noun clustering.
In Proceedings of the 43rd Annual Meeting of the ACL,
pages 622–629, Ann Arbor, USA.
Mangus Sahlgren and Jussi Karlgren. 2005. Automatic bilin-
gual lexicon acquisition using random indexing of parallel
corpora. Journal of Natural Language Engineering, Spe-
cial Issue on Parallel Texts, 11(3), June.
Julie Weeds and David Weir. 2005. Co-occurance retrieval:
A flexible framework for lexical distributional similarity.
Computational Linguistics, 31(4):439–475, December.
Peter N. Yianilos. 1993. Data structures and algorithms for
nearest neighbor search in general metric spaces. In Pro-
ceedings of the fourth annual ACM-SIAM Symposium on
Discrete algorithms, pages 311–321, Philadelphia.
</reference>
<page confidence="0.92835">
368
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953210">
<title confidence="0.999749">Scaling Distributional Similarity to Large Corpora</title>
<author confidence="0.999988">Gorman R Curran</author>
<affiliation confidence="0.999708">School of Information Technologies University of Sydney</affiliation>
<address confidence="0.983872">NSW 2006, Australia</address>
<abstract confidence="0.997786066666667">Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words. However, the naive nearestneighbour approach to comparing context vectors extracted from large corpora scales the vocabulary size). In this paper, we compare several existing approaches to approximating the nearestneighbour search for distributional similarity. We investigate the trade-off between efficiency and accuracy, and find and Sakuma, 2005) provides the best balance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrei Broder</author>
</authors>
<title>On the resemblance and containment of documents.</title>
<date>1997</date>
<booktitle>In Proceedings of the Compression and Complexity of Sequences,</booktitle>
<pages>21--29</pages>
<location>Salerno, Italy.</location>
<contexts>
<context position="8306" citStr="Broder, 1997" startWordPosition="1288" endWordPosition="1289">l be made, but at the cost of accuracy as more possibly near results will be discarded out of hand. 4 Randomised Techniques Conventional dimensionality reduction techniques can be computationally expensive: a more scal� ������max(w(wm, r, w0), w(w, r, w0)) (1) � ������min(w(wm, r, w0), w(w, r, w0)) 362 able solution is required to handle the volumes of data we propose to use. Randomised techniques provide a possible solution to this. We present two techniques that have been used recently for distributional similarity: Random Indexing (Kanerva et al., 2000) and Locality Sensitive Hashing (LSH, Broder, 1997). 4.1 Random Indexing Random Indexing (RI) is a hashing technique based on Sparse Distributed Memory (Kanerva, 1993). Karlgren and Sahlgren (2001) showed RI produces results similar to LSA using the Test of English as a Foreign Language (TOEFL) evaluation. Sahlgren and Karlgren (2005) showed the technique to be successful in generating bilingual lexicons from parallel corpora. In RI, we first allocate a d length index vector to each unique attribute. The vectors consist of a large number of 0s and small number (E) number of randomly distributed ±1s. Context vectors, identifying terms, are gene</context>
<context position="10362" citStr="Broder (1997)" startWordPosition="1651" endWordPosition="1652">where θ(~u, ~u) is the angle between u~ and ~u. The angular similarity gives the cosine by cos(θ(~u, ~u)) = (5) cos((1 − p(h~r(~u) = h~r(~v)))π) The probability can be derived from the Hamming distance: p(hr(u) = hr(v)) = 1 − x(�ud, v) (6) cos(θ(u,v)) = (3) |~u||~v| u· v This technique allows for incremental sampling, where the index vector for an attribute is only generated when the attribute is encountered. Construction complexity is O(nmd) and search complexity is O(n2d). 4.2 Locality Sensitive Hashing LSH is a probabilistic technique that allows the approximation of a similarity function. Broder (1997) proposed an approximation of the Jaccard similarity function using min-wise independent functions. Charikar (2002) proposed an approximation of the cosine measure using random hyperplanes Ravichandran et al. (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). Given we have n terms in an m&apos; dimensional space, we create d « m&apos; unit random vectors also of m&apos; dimensions, labelled {~r1, ~r2,..., ~rd}. Each By combining equations 5 and 6 we get the following approximation of the cosine distance: cos (θ(~u , ~</context>
</contexts>
<marker>Broder, 1997</marker>
<rawString>Andrei Broder. 1997. On the resemblance and containment of documents. In Proceedings of the Compression and Complexity of Sequences, pages 21–29, Salerno, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter A Burkhard</author>
<author>Robert M Keller</author>
</authors>
<title>Some approaches to best-match file searching.</title>
<date>1973</date>
<journal>Communications of the ACM,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="13688" citStr="Burkhard and Keller, 1973" startWordPosition="2229" endWordPosition="2232">as a new VPT, choosing a new vantage point from within its terms, until all terms are exhausted. Searching a VPT is also recursive. Given a term q and radius r, we begin by measuring the distance to the root term p. If dist(q, p) &lt; r we enter p into our list of near terms. If dist(q, p) − r &lt; mp we enter the left sub-tree and if dist(q, p) + r &gt; mp we enter the right sub-tree. Both sub-trees may be entered. The process is repeated for each entered subtree, taking the vantage point of the sub-tree to be the new root term. To perform a k-NN search we use a backtracking decreasing radius search (Burkhard and Keller, 1973). The search begins with r = oo, and terms are added to a list of the closest k terms. When the kth closest term is found, the radius is set to the distance between this term and the target. Each time a new, closer element is added to the list, the radius is updated to the distance from the target to the new kth closest term. Construction complexity is O(n log n). Search complexity is claimed to be O(log n) for small radius searches. This does not hold for our decreasing radius search, whose worst case complexity is O(n). 5.2 Point Location in Equal Balls PLEB is a randomised structure that us</context>
</contexts>
<marker>Burkhard, Keller, 1973</marker>
<rawString>Walter A. Burkhard and Robert M. Keller. 1973. Some approaches to best-match file searching. Communications of the ACM, 16(4):230–236, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses S Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 34th Annual ACM Symposium on Theory of Computing,</booktitle>
<pages>380--388</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="10477" citStr="Charikar (2002)" startWordPosition="1665" endWordPosition="1666">os((1 − p(h~r(~u) = h~r(~v)))π) The probability can be derived from the Hamming distance: p(hr(u) = hr(v)) = 1 − x(�ud, v) (6) cos(θ(u,v)) = (3) |~u||~v| u· v This technique allows for incremental sampling, where the index vector for an attribute is only generated when the attribute is encountered. Construction complexity is O(nmd) and search complexity is O(n2d). 4.2 Locality Sensitive Hashing LSH is a probabilistic technique that allows the approximation of a similarity function. Broder (1997) proposed an approximation of the Jaccard similarity function using min-wise independent functions. Charikar (2002) proposed an approximation of the cosine measure using random hyperplanes Ravichandran et al. (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). Given we have n terms in an m&apos; dimensional space, we create d « m&apos; unit random vectors also of m&apos; dimensions, labelled {~r1, ~r2,..., ~rd}. Each By combining equations 5 and 6 we get the following approximation of the cosine distance: cos (θ(~u , ~u)) = cos((x(u,�v) ) π) (7) That is, the cosine of two context vectors is approximated by the cosine of the Hamming</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses S. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the 34th Annual ACM Symposium on Theory of Computing, pages 380–388, Montreal, Quebec, Canada, 19–21 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon,</booktitle>
<pages>59--66</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="1362" citStr="Curran and Moens (2002)" startWordPosition="196" endWordPosition="199"> balance. 1 Introduction It is a general property of Machine Learning that increasing the volume of training data increases the accuracy of results. This is no more evident than in Natural Language Processing (NLP), where massive quantities of text are required to model rare language events. Despite the rapid increase in computational power available for NLP systems, the volume of raw data available still outweighs our ability to process it. Unsupervised learning, which does not require the expensive and timeconsuming human annotation of data, offers an opportunity to use this wealth of data. Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. Extracting synonymy relations using distributional similarity is based on the distributional hypothesis that similar words appear in similar contexts. Terms are described by collating information about their occurrence in a corpus into vectors. These context vectors are then compared for similarity. Existing approaches differ primarily in their definition of “context”, e.g. the surrounding words or the entire document, and th</context>
<context position="6948" citStr="Curran and Moens (2002)" startWordPosition="1062" endWordPosition="1065">use attribute vectors of a dimensionality of around 1000. Our experiments have a dimensionality of 500,000 to 1,500,000. Decompositions on data this size are computationally difficult. Dimensionality reduction is often used before using LSA to improve its scalability. 3.1 Heuristics Another technique is to use an initial heuristic comparison to reduce the number of full O(m) vector comparisons that are performed. If the heuristic comparison is sufficiently fast and a sufficient number of full comparisons are avoided, the cost of an additional check will be easily absorbed by the savings made. Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k « m), selected from the full vector, to represent the term. These attributes are the most strongly weighted verb attributes, chosen because they constrain the semantics of the term more and partake in fewer idiomatic collocations. If a pair of terms share at least one canonical attribute then a full similarity comparison is performed, otherwise the terms are not compared. They show an 89% reduction in search time, with only a 3.9% loss in accuracy. There is a significant improvement in the computational complexity. If a maximum </context>
<context position="18399" citStr="Curran and Moens, 2002" startWordPosition="3117" endWordPosition="3120">ried at each level to force a larger number of elements to be tested at the base of the SASH using, for instance, the equation: 1_ h−i 1 ki = max{ k log n , 2pc } (8) This changes our search complexity to: _1 + pc2 log n (9) We use this geometric function in our experiments. Gorman and Curran (2005a; 2005b) found the performance of SASH for distributional similarity could be improved by replacing the initial random ordering with a frequency based ordering. In accordance with Zipf’s law, the majority of terms have low frequencies. Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). Creating SASH with high frequency terms near the root produces more reliable initial paths, but comparisons against these terms are more expensive. The best accuracy/efficiency trade-off was found when using more reliable initial paths rather than the most reliable. This is done by folding the data around some mean number of relations. For each term, if its number of relations mi is greater than some chosen number of relations M, it is given a new ranking based on the score �� �i . Otherwise its ranking based on its number of relations. This has the effect of pushing very high and very low f</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon, pages 59–66, Philadelphia, PA, USA, 12 July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="4465" citStr="Curran (2004)" startWordPosition="674" endWordPosition="675">bute of w. For example, (dog, direct-obj, walk) indicates that dog was the direct object of walk in a sentence. In our experiments context extraction begins with a Maximum Entropy POS tagger and chunker. The SEXTANT relation extractor (Grefenstette, 1994) produces context relations that are then lemmatised. The relations for each term are collected together and counted, producing a vector of attributes and their frequencies in the corpus. 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure to calculate the similarity between context vectors. Curran (2004) decomposes this into measure and weight functions. The measure calculates the similarity between two weighted context vectors and the weight calculates the informativeness of each context relation from the raw frequencies. For these experiments we use the Jaccard (1) measure and the TTest (2) weight functions, found by Curran (2004) to have the best performance. p(w, r, w0) − p(*, r, w0)p(w, *, *)(2) VIp(*, r, w0)p(w, *, *) 2.3 Nearest-neighbour Search The simplest algorithm for finding synonyms is a k-nearest-neighbour (k-NN) search, which involves pair-wise vector comparison of the target t</context>
<context position="19357" citStr="Curran (2004)" startWordPosition="3279" endWordPosition="3280">ach term, if its number of relations mi is greater than some chosen number of relations M, it is given a new ranking based on the score �� �i . Otherwise its ranking based on its number of relations. This has the effect of pushing very high and very low frequency terms away from the root. 6 Evaluation Measures The simplest method for evaluation is the direct comparison of extracted synonyms with a manually created gold standard (Grefenstette, 1994). To reduce the problem of limited coverage, our evaluation combines three electronic thesauri: the Macquarie, Roget’s and Moby thesauri. We follow Curran (2004) and use two performance measures: direct matches (DIRECT) and inverse rank (INVR). DIRECT is the percentage of returned synonyms found in the gold standard. INVR is the sum of the inverse rank of each matching synonym, e.g. matches at ranks 3, 5 and 28 k1+ 1 log n 1 k log n 365 CUT-OFF 5 100 NAIVE 1.72 1.71 HEURISTIC 1.65 1.66 RI 0.80 0.93 LSH10,000 1.26 1.31 SASH 1.73 1.71 Table 2: INVR vs frequency cut-off CORPUS CUT-OFF TERMS AVERAGE RELATIONS PER TERM BNC 0 5 100 246,067 43 88,926 116 14,862 617 LARGE 0 5 100 541,722 97 184,494 281 35,618 1,400 Table 1: Extracted Context Information give </context>
<context position="20890" citStr="Curran, 2004" startWordPosition="3552" endWordPosition="3553">hosen randomly from WordNet such that they covered a range over the following properties: frequency, number of senses, specificity and concreteness. For each of these terms, the closest 100 terms and their similarity scores were extracted. 7 Experiments We use two corpora in our experiments: the smaller is the non-speech portion of the British National Corpus (BNC), 90 million words covering a wide range of domains and formats; the larger consists of the BNC, the Reuters Corpus Volume 1 and most of the English news holdings of the LDC in 2003, representing over 2 billion words of text (LARGE, Curran, 2004). The semantic similarity system implemented by Curran (2004) provides our baseline. This performs a brute-force k-NN search (NAIVE). We present results for the canonical attribute heuristic (HEURISTIC), RI, LSH, PLEB, VPT and SASH. We take the optimal canonical attribute vector length of 30 for HEURISTIC from Curran (2004). For SASH we take optimal values of p = 4 and c = 16 and use the folded ordering taking M = 1000 from Gorman and Curran (2005b). For RI, LSH and PLEB we found optimal values experimentally using the BNC. For LSH we chose d = 3, 000 (LSH3,000) and 10, 000 (LSH10,000), showin</context>
<context position="22247" citStr="Curran, 2004" startWordPosition="3786" endWordPosition="3787">p(w r w&apos;) The initial experiments on RI produced quite poor results. The intuition was that this was caused by the lack of smoothing in the algorithm. Experiments were performed using the weights given in Curran (2004). Of these, mutual information (10), evaluated with an extra log2(�(w, r, w&apos;) + 1) factor and limited to positive values, produced the best results (RIMI). The values d = 1000 and E = 5 were found to produce the best results. All experiments were performed on 3.2GHz Xeon P4 machines with 4GB of RAM. 8 Results As the accuracy of comparisons between terms increases with frequency (Curran, 2004), applying a frequency cut-off will both reduce the size of the vocabulary (n) and increase the average accuracy of comparisons. Table 1 shows the reduction in vocabulary and increase in average context relations per term as cut-off increases. For LARGE, the initial 541,722 word vocabulary is reduced by 66% when a cut-off of 5 is applied and by 86% when the cut-off is increased to 100. The average number of relations increases from 97 to 1400. The work by Curran (2004) largely uses a frequency cut-off of 5. When this cut-off was used with the randomised techniques RI and LSH, it produced quite</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel X Goemans</author>
<author>David P Williamson</author>
</authors>
<title>Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming.</title>
<date>1995</date>
<journal>Journal of Association for Computing Machinery,</journal>
<volume>42</volume>
<issue>6</issue>
<contexts>
<context position="9667" citStr="Goemans and Williamson (1995)" startWordPosition="1532" endWordPosition="1535">erm t appearing in contexts c1 = [1, 0, 0, −1] and c2 = [0, 1, 0, −1] would be [1, 1, 0, −2]. The distance between these context vectors is then measured using the cosine measure: vector is created by sampling a Gaussian function m&apos; times, with a mean of 0 and a variance of 1. For each term w we construct its bit signature using the function � 1 : ~r.~w &gt; 0 h~r(~w) =0 : ~r.~w &lt; 0 where r~ is a spherically symmetric random vector of length d. The signature, w, is the d length bit vector: w� = {h~r�(~w), h~r�(~w), ... , h~rd(~w)} The cost to build all n signatures is O(nm&apos;d). For terms u and v, Goemans and Williamson (1995) approximate the angular similarity by p(h~r(~u) = h~r(~v)) = 1 − π θ(~u, ~u) (4) where θ(~u, ~u) is the angle between u~ and ~u. The angular similarity gives the cosine by cos(θ(~u, ~u)) = (5) cos((1 − p(h~r(~u) = h~r(~v)))π) The probability can be derived from the Hamming distance: p(hr(u) = hr(v)) = 1 − x(�ud, v) (6) cos(θ(u,v)) = (3) |~u||~v| u· v This technique allows for incremental sampling, where the index vector for an attribute is only generated when the attribute is encountered. Construction complexity is O(nmd) and search complexity is O(n2d). 4.2 Locality Sensitive Hashing LSH is </context>
</contexts>
<marker>Goemans, Williamson, 1995</marker>
<rawString>Michel X. Goemans and David P. Williamson. 1995. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of Association for Computing Machinery, 42(6):1115–1145, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Gorman</author>
<author>James Curran</author>
</authors>
<title>Approximate searching for distributional similarity.</title>
<date>2005</date>
<booktitle>In ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition,</booktitle>
<location>Ann Arbor, MI, USA, 30</location>
<contexts>
<context position="18075" citStr="Gorman and Curran (2005" startWordPosition="3068" endWordPosition="3071">d, including F and G, which are shared as children by B and C. From this level we chose G and H. The final levels are considered similarly. At this point we now have the list of near nodes A, C, D, G, H, I, J, K and L. From this we chose the two nodes nearest q, H and I marked in black, which are then returned. k can be varied at each level to force a larger number of elements to be tested at the base of the SASH using, for instance, the equation: 1_ h−i 1 ki = max{ k log n , 2pc } (8) This changes our search complexity to: _1 + pc2 log n (9) We use this geometric function in our experiments. Gorman and Curran (2005a; 2005b) found the performance of SASH for distributional similarity could be improved by replacing the initial random ordering with a frequency based ordering. In accordance with Zipf’s law, the majority of terms have low frequencies. Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). Creating SASH with high frequency terms near the root produces more reliable initial paths, but comparisons against these terms are more expensive. The best accuracy/efficiency trade-off was found when using more reliable initial paths rather than the most reliable. This is</context>
<context position="21341" citStr="Gorman and Curran (2005" startWordPosition="3627" endWordPosition="3630">r consists of the BNC, the Reuters Corpus Volume 1 and most of the English news holdings of the LDC in 2003, representing over 2 billion words of text (LARGE, Curran, 2004). The semantic similarity system implemented by Curran (2004) provides our baseline. This performs a brute-force k-NN search (NAIVE). We present results for the canonical attribute heuristic (HEURISTIC), RI, LSH, PLEB, VPT and SASH. We take the optimal canonical attribute vector length of 30 for HEURISTIC from Curran (2004). For SASH we take optimal values of p = 4 and c = 16 and use the folded ordering taking M = 1000 from Gorman and Curran (2005b). For RI, LSH and PLEB we found optimal values experimentally using the BNC. For LSH we chose d = 3, 000 (LSH3,000) and 10, 000 (LSH10,000), showing the effect of changing the dimensionality. The frequency statistics were weighted using mutual information, as in Ravichandran et al. (2005): p(w r w&apos;) The initial experiments on RI produced quite poor results. The intuition was that this was caused by the lack of smoothing in the algorithm. Experiments were performed using the weights given in Curran (2004). Of these, mutual information (10), evaluated with an extra log2(�(w, r, w&apos;) + 1) factor</context>
</contexts>
<marker>Gorman, Curran, 2005</marker>
<rawString>James Gorman and James Curran. 2005a. Approximate searching for distributional similarity. In ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition, Ann Arbor, MI, USA, 30 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Gorman</author>
<author>James Curran</author>
</authors>
<title>Augmenting approximate similarity searching with lexical information.</title>
<date>2005</date>
<booktitle>In Australasian Language Technology Workshop,</booktitle>
<location>Sydney, Australia, 9–11</location>
<contexts>
<context position="18075" citStr="Gorman and Curran (2005" startWordPosition="3068" endWordPosition="3071">d, including F and G, which are shared as children by B and C. From this level we chose G and H. The final levels are considered similarly. At this point we now have the list of near nodes A, C, D, G, H, I, J, K and L. From this we chose the two nodes nearest q, H and I marked in black, which are then returned. k can be varied at each level to force a larger number of elements to be tested at the base of the SASH using, for instance, the equation: 1_ h−i 1 ki = max{ k log n , 2pc } (8) This changes our search complexity to: _1 + pc2 log n (9) We use this geometric function in our experiments. Gorman and Curran (2005a; 2005b) found the performance of SASH for distributional similarity could be improved by replacing the initial random ordering with a frequency based ordering. In accordance with Zipf’s law, the majority of terms have low frequencies. Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). Creating SASH with high frequency terms near the root produces more reliable initial paths, but comparisons against these terms are more expensive. The best accuracy/efficiency trade-off was found when using more reliable initial paths rather than the most reliable. This is</context>
<context position="21341" citStr="Gorman and Curran (2005" startWordPosition="3627" endWordPosition="3630">r consists of the BNC, the Reuters Corpus Volume 1 and most of the English news holdings of the LDC in 2003, representing over 2 billion words of text (LARGE, Curran, 2004). The semantic similarity system implemented by Curran (2004) provides our baseline. This performs a brute-force k-NN search (NAIVE). We present results for the canonical attribute heuristic (HEURISTIC), RI, LSH, PLEB, VPT and SASH. We take the optimal canonical attribute vector length of 30 for HEURISTIC from Curran (2004). For SASH we take optimal values of p = 4 and c = 16 and use the folded ordering taking M = 1000 from Gorman and Curran (2005b). For RI, LSH and PLEB we found optimal values experimentally using the BNC. For LSH we chose d = 3, 000 (LSH3,000) and 10, 000 (LSH10,000), showing the effect of changing the dimensionality. The frequency statistics were weighted using mutual information, as in Ravichandran et al. (2005): p(w r w&apos;) The initial experiments on RI produced quite poor results. The intuition was that this was caused by the lack of smoothing in the algorithm. Experiments were performed using the weights given in Curran (2004). Of these, mutual information (10), evaluated with an extra log2(�(w, r, w&apos;) + 1) factor</context>
</contexts>
<marker>Gorman, Curran, 2005</marker>
<rawString>James Gorman and James Curran. 2005b. Augmenting approximate similarity searching with lexical information. In Australasian Language Technology Workshop, Sydney, Australia, 9–11 November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<contexts>
<context position="4107" citStr="Grefenstette, 1994" startWordPosition="621" endWordPosition="623">rence on Computational Linguistics and 44th Annual Meeting of the ACL, pages 361–368, Sydney, July 2006. c�2006 Association for Computational Linguistics 2.1 Extraction A context relation is defined as a tuple (w, r, w0) where w is a term, which occurs in some grammatical relation r with another word w0 in some sentence. We refer to the tuple (r, w0) as an attribute of w. For example, (dog, direct-obj, walk) indicates that dog was the direct object of walk in a sentence. In our experiments context extraction begins with a Maximum Entropy POS tagger and chunker. The SEXTANT relation extractor (Grefenstette, 1994) produces context relations that are then lemmatised. The relations for each term are collected together and counted, producing a vector of attributes and their frequencies in the corpus. 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure to calculate the similarity between context vectors. Curran (2004) decomposes this into measure and weight functions. The measure calculates the similarity between two weighted context vectors and the weight calculates the informativeness of each context relation from the raw frequencies. For these experime</context>
<context position="19196" citStr="Grefenstette, 1994" startWordPosition="3254" endWordPosition="3255"> trade-off was found when using more reliable initial paths rather than the most reliable. This is done by folding the data around some mean number of relations. For each term, if its number of relations mi is greater than some chosen number of relations M, it is given a new ranking based on the score �� �i . Otherwise its ranking based on its number of relations. This has the effect of pushing very high and very low frequency terms away from the root. 6 Evaluation Measures The simplest method for evaluation is the direct comparison of extracted synonyms with a manually created gold standard (Grefenstette, 1994). To reduce the problem of limited coverage, our evaluation combines three electronic thesauri: the Macquarie, Roget’s and Moby thesauri. We follow Curran (2004) and use two performance measures: direct matches (DIRECT) and inverse rank (INVR). DIRECT is the percentage of returned synonyms found in the gold standard. INVR is the sum of the inverse rank of each matching synonym, e.g. matches at ranks 3, 5 and 28 k1+ 1 log n 1 k log n 365 CUT-OFF 5 100 NAIVE 1.72 1.71 HEURISTIC 1.65 1.66 RI 0.80 0.93 LSH10,000 1.26 1.31 SASH 1.73 1.71 Table 2: INVR vs frequency cut-off CORPUS CUT-OFF TERMS AVERA</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Houle</author>
<author>Jun Sakuma</author>
</authors>
<title>Fast approximate similarity search in extremely high-dimensional data sets.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st International Conference on Data Engineering,</booktitle>
<pages>619--630</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="721" citStr="Houle and Sakuma, 2005" startWordPosition="94" endWordPosition="97">rmation Technologies University of Sydney NSW 2006, Australia {jgorman2,james}@it.usyd.edu.au Abstract Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words. However, the naive nearestneighbour approach to comparing context vectors extracted from large corpora scales poorly (O(n2) in the vocabulary size). In this paper, we compare several existing approaches to approximating the nearestneighbour search for distributional similarity. We investigate the trade-off between efficiency and accuracy, and find that SASH (Houle and Sakuma, 2005) provides the best balance. 1 Introduction It is a general property of Machine Learning that increasing the volume of training data increases the accuracy of results. This is no more evident than in Natural Language Processing (NLP), where massive quantities of text are required to model rare language events. Despite the rapid increase in computational power available for NLP systems, the volume of raw data available still outweighs our ability to process it. Unsupervised learning, which does not require the expensive and timeconsuming human annotation of data, offers an opportunity to use thi</context>
<context position="3048" citStr="Houle and Sakuma, 2005" startWordPosition="455" endWordPosition="458">vantage. Finding synonyms using distributional similarity requires a nearest-neighbour search over the context vectors of each term. This is computationally intensive, scaling to O(n2m) for the number of terms n and the size of their context vectors m. Increasing the volume of input data will increase the size of both n and m, decreasing the efficiency of a naive nearest-neighbour approach. Many approaches to reduce this complexity have been suggested. In this paper we evaluate state-of-the-art techniques proposed to solve this problem. We find that the Spatial Approximation Sample Hierarchy (Houle and Sakuma, 2005) provides the best accuracy/efficiency trade-off. 2 Distributional Similarity Measuring distributional similarity first requires the extraction of context information for each of the vocabulary terms from raw text. These terms are then compared for similarity using a nearestneighbour search or clustering based on distance calculations between the statistical descriptions of their contexts. 361 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 361–368, Sydney, July 2006. c�2006 Association for Computational Linguistics 2.1 Ex</context>
<context position="11765" citStr="Houle and Sakuma, 2005" startWordPosition="1883" endWordPosition="1886">the signatures. Search is performed using Equation 7 and scales to O(n2d). 5 Data Structures The methods presented above fail to address the n2 component of the search complexity. Many data structures have been proposed that can be used to address this problem in similarity searching. We present three data structures: the vantage point tree (VPT, Yianilos, 1993), which indexes points in a metric space, Point Location in Equal 363 Balls (PLEB, Indyk and Motwani, 1998), a probabilistic structure that uses the bit signatures generated by LSH, and the Spatial Approximation Sample Hierarchy (SASH, Houle and Sakuma, 2005), which approximates a k-NN search. Another option inspired by IR is attribute indexing (INDEX). In this technique, in addition to each term having a reference to its attributes, each attribute has a reference to the terms referencing it. Each term is then only compared with the terms with which it shares attributes. We will give a theoretically comparison against other techniques. 5.1 Vantage Point Tree Metric space data structures provide a solution to near-neighbour searches in very high dimensions. These rely solely on the existence of a comparison function that satisfies the conditions of</context>
<context position="15610" citStr="Houle and Sakuma (2005)" startWordPosition="2567" endWordPosition="2570">ff, and need not be related to the choice of k. This is performed q times, using a different random permutation function each time. After each iteration, the current closest k terms are stored. For a fixed d, the complexity for the permutation step is O(qn), the sorting O(qn log n) and the search O(qBn). 5.3 Spatial Approximation Sample Hierarchy SASH approximates a k-NN search by precomputing some near neighbours for each node (terms in our case). This produces multiple paths between terms, allowing SASH to shape itself to the data set (Houle, 2003). The following description is adapted from Houle and Sakuma (2005). The SASH is a directed, edge-weighted graph with the following properties (see Figure 1): • Each term corresponds to a unique node. • The nodes are arranged into a hierarchy of levels, with the bottom level containing 2 nodes and the top containing a single root node. Each level, except the top, will contain half as many nodes as the level below. • Edges between nodes are linked to consecutive levels. Each node will have at most p parent nodes in the level above, and c child nodes in the level below. 364 A 1 B C D E F G H I J 5 K L Figure 1: A SASH, where p = 2, c = 3 and k = 2 2 3 4 • Every</context>
</contexts>
<marker>Houle, Sakuma, 2005</marker>
<rawString>Michael E. Houle and Jun Sakuma. 2005. Fast approximate similarity search in extremely high-dimensional data sets. In Proceedings of the 21st International Conference on Data Engineering, pages 619–630, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Houle</author>
</authors>
<title>Navigating massive data sets via local clustering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>547--552</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="15543" citStr="Houle, 2003" startWordPosition="2559" endWordPosition="2560"> choice of B will effect the accuracy/efficiency trade-off, and need not be related to the choice of k. This is performed q times, using a different random permutation function each time. After each iteration, the current closest k terms are stored. For a fixed d, the complexity for the permutation step is O(qn), the sorting O(qn log n) and the search O(qBn). 5.3 Spatial Approximation Sample Hierarchy SASH approximates a k-NN search by precomputing some near neighbours for each node (terms in our case). This produces multiple paths between terms, allowing SASH to shape itself to the data set (Houle, 2003). The following description is adapted from Houle and Sakuma (2005). The SASH is a directed, edge-weighted graph with the following properties (see Figure 1): • Each term corresponds to a unique node. • The nodes are arranged into a hierarchy of levels, with the bottom level containing 2 nodes and the top containing a single root node. Each level, except the top, will contain half as many nodes as the level below. • Edges between nodes are linked to consecutive levels. Each node will have at most p parent nodes in the level above, and c child nodes in the level below. 364 A 1 B C D E F G H I J</context>
</contexts>
<marker>Houle, 2003</marker>
<rawString>Michael E. Houle. 2003. Navigating massive data sets via local clustering. In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 547–552, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In Proceedings of the 30th annual ACM Symposium on Theory of Computing,</booktitle>
<pages>604--613</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA, 24–26</location>
<contexts>
<context position="11613" citStr="Indyk and Motwani, 1998" startWordPosition="1858" endWordPosition="1861"> That is, the cosine of two context vectors is approximated by the cosine of the Hamming distance between their two signatures normalised by the size of the signatures. Search is performed using Equation 7 and scales to O(n2d). 5 Data Structures The methods presented above fail to address the n2 component of the search complexity. Many data structures have been proposed that can be used to address this problem in similarity searching. We present three data structures: the vantage point tree (VPT, Yianilos, 1993), which indexes points in a metric space, Point Location in Equal 363 Balls (PLEB, Indyk and Motwani, 1998), a probabilistic structure that uses the bit signatures generated by LSH, and the Spatial Approximation Sample Hierarchy (SASH, Houle and Sakuma, 2005), which approximates a k-NN search. Another option inspired by IR is attribute indexing (INDEX). In this technique, in addition to each term having a reference to its attributes, each attribute has a reference to the terms referencing it. Each term is then only compared with the terms with which it shares attributes. We will give a theoretically comparison against other techniques. 5.1 Vantage Point Tree Metric space data structures provide a s</context>
</contexts>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the 30th annual ACM Symposium on Theory of Computing, pages 604–613, New York, NY, USA, 24–26 May. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
<author>Jan Kristoferson</author>
<author>Anders Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1036</pages>
<location>Mahwah, NJ, USA.</location>
<contexts>
<context position="8255" citStr="Kanerva et al., 2000" startWordPosition="1278" endWordPosition="1281">he system will be faster as many fewer full comparisons will be made, but at the cost of accuracy as more possibly near results will be discarded out of hand. 4 Randomised Techniques Conventional dimensionality reduction techniques can be computationally expensive: a more scal� ������max(w(wm, r, w0), w(w, r, w0)) (1) � ������min(w(wm, r, w0), w(w, r, w0)) 362 able solution is required to handle the volumes of data we propose to use. Randomised techniques provide a possible solution to this. We present two techniques that have been used recently for distributional similarity: Random Indexing (Kanerva et al., 2000) and Locality Sensitive Hashing (LSH, Broder, 1997). 4.1 Random Indexing Random Indexing (RI) is a hashing technique based on Sparse Distributed Memory (Kanerva, 1993). Karlgren and Sahlgren (2001) showed RI produces results similar to LSA using the Test of English as a Foreign Language (TOEFL) evaluation. Sahlgren and Karlgren (2005) showed the technique to be successful in generating bilingual lexicons from parallel corpora. In RI, we first allocate a d length index vector to each unique attribute. The vectors consist of a large number of 0s and small number (E) number of randomly distribute</context>
</contexts>
<marker>Kanerva, Kristoferson, Holst, 2000</marker>
<rawString>Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000. Random indexing of text samples for latent semantic analysis. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, page 1036, Mahwah, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
</authors>
<title>Sparse distributed memory and related models.</title>
<date>1993</date>
<booktitle>Associative Neural Memories: Theory and Implementation,</booktitle>
<pages>50--76</pages>
<editor>In M.H. Hassoun, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8422" citStr="Kanerva, 1993" startWordPosition="1305" endWordPosition="1306">chniques Conventional dimensionality reduction techniques can be computationally expensive: a more scal� ������max(w(wm, r, w0), w(w, r, w0)) (1) � ������min(w(wm, r, w0), w(w, r, w0)) 362 able solution is required to handle the volumes of data we propose to use. Randomised techniques provide a possible solution to this. We present two techniques that have been used recently for distributional similarity: Random Indexing (Kanerva et al., 2000) and Locality Sensitive Hashing (LSH, Broder, 1997). 4.1 Random Indexing Random Indexing (RI) is a hashing technique based on Sparse Distributed Memory (Kanerva, 1993). Karlgren and Sahlgren (2001) showed RI produces results similar to LSA using the Test of English as a Foreign Language (TOEFL) evaluation. Sahlgren and Karlgren (2005) showed the technique to be successful in generating bilingual lexicons from parallel corpora. In RI, we first allocate a d length index vector to each unique attribute. The vectors consist of a large number of 0s and small number (E) number of randomly distributed ±1s. Context vectors, identifying terms, are generated by summing the index vectors of the attributes for each non-unique context in which a term appears. The contex</context>
</contexts>
<marker>Kanerva, 1993</marker>
<rawString>Pentti Kanerva. 1993. Sparse distributed memory and related models. In M.H. Hassoun, editor, Associative Neural Memories: Theory and Implementation, pages 50–76. Oxford University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jussi Karlgren</author>
<author>Magnus Sahlgren</author>
</authors>
<title>From words to understanding.</title>
<date>2001</date>
<booktitle>Foundations of Real-World Intelligence,</booktitle>
<pages>294--308</pages>
<editor>In Y. Uesaka, P. Kanerva, and H Asoh, editors,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="8452" citStr="Karlgren and Sahlgren (2001)" startWordPosition="1307" endWordPosition="1310">ional dimensionality reduction techniques can be computationally expensive: a more scal� ������max(w(wm, r, w0), w(w, r, w0)) (1) � ������min(w(wm, r, w0), w(w, r, w0)) 362 able solution is required to handle the volumes of data we propose to use. Randomised techniques provide a possible solution to this. We present two techniques that have been used recently for distributional similarity: Random Indexing (Kanerva et al., 2000) and Locality Sensitive Hashing (LSH, Broder, 1997). 4.1 Random Indexing Random Indexing (RI) is a hashing technique based on Sparse Distributed Memory (Kanerva, 1993). Karlgren and Sahlgren (2001) showed RI produces results similar to LSA using the Test of English as a Foreign Language (TOEFL) evaluation. Sahlgren and Karlgren (2005) showed the technique to be successful in generating bilingual lexicons from parallel corpora. In RI, we first allocate a d length index vector to each unique attribute. The vectors consist of a large number of 0s and small number (E) number of randomly distributed ±1s. Context vectors, identifying terms, are generated by summing the index vectors of the attributes for each non-unique context in which a term appears. The context vector for a term t appearin</context>
</contexts>
<marker>Karlgren, Sahlgren, 2001</marker>
<rawString>Jussi Karlgren and Magnus Sahlgren. 2001. From words to understanding. In Y. Uesaka, P. Kanerva, and H Asoh, editors, Foundations of Real-World Intelligence, pages 294– 308. CSLI Publications, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="5971" citStr="Landauer and Dumais, 1997" startWordPosition="907" endWordPosition="910">gest experiments used a vocabulary of over 184,000 words. 3 Dimensionality Reduction Using a cut-off to remove low frequency terms can significantly reduce the value of n. Unfortunately, reducing m by eliminating low frequency contexts has a significant impact on the quality of the results. There are many techniques to reduce dimensionality while avoiding this problem. The simplest methods use feature selection techniques, such as information gain, to remove the attributes that are less informative. Other techniques smooth the data while reducing dimensionality. Latent Semantic Analysis (LSA, Landauer and Dumais, 1997) is a smoothing and dimensionality reduction technique based on the intuition that the true dimensionality of data is latent in the surface dimensionality. Landauer and Dumais admit that, from a pragmatic perspective, the same effect as LSA can be generated by using large volumes of data with very long attribute vectors. Experiments with LSA typically use attribute vectors of a dimensionality of around 1000. Our experiments have a dimensionality of 500,000 to 1,500,000. Decompositions on data this size are computationally difficult. Dimensionality reduction is often used before using LSA to im</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD-02,</booktitle>
<pages>613--619</pages>
<contexts>
<context position="10712" citStr="Pantel and Lin (2002)" startWordPosition="1702" endWordPosition="1705"> for an attribute is only generated when the attribute is encountered. Construction complexity is O(nmd) and search complexity is O(n2d). 4.2 Locality Sensitive Hashing LSH is a probabilistic technique that allows the approximation of a similarity function. Broder (1997) proposed an approximation of the Jaccard similarity function using min-wise independent functions. Charikar (2002) proposed an approximation of the cosine measure using random hyperplanes Ravichandran et al. (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). Given we have n terms in an m&apos; dimensional space, we create d « m&apos; unit random vectors also of m&apos; dimensions, labelled {~r1, ~r2,..., ~rd}. Each By combining equations 5 and 6 we get the following approximation of the cosine distance: cos (θ(~u , ~u)) = cos((x(u,�v) ) π) (7) That is, the cosine of two context vectors is approximated by the cosine of the Hamming distance between their two signatures normalised by the size of the signatures. Search is performed using Equation 7 and scales to O(n2d). 5 Data Structures The methods presented above fail to address the n2 component of the search co</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of ACM SIGKDD-02, pages 613–619, 23–26 July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and NLP: Using locality sensitive hash functions for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>622--629</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="10577" citStr="Ravichandran et al. (2005)" startWordPosition="1679" endWordPosition="1682"> p(hr(u) = hr(v)) = 1 − x(�ud, v) (6) cos(θ(u,v)) = (3) |~u||~v| u· v This technique allows for incremental sampling, where the index vector for an attribute is only generated when the attribute is encountered. Construction complexity is O(nmd) and search complexity is O(n2d). 4.2 Locality Sensitive Hashing LSH is a probabilistic technique that allows the approximation of a similarity function. Broder (1997) proposed an approximation of the Jaccard similarity function using min-wise independent functions. Charikar (2002) proposed an approximation of the cosine measure using random hyperplanes Ravichandran et al. (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). Given we have n terms in an m&apos; dimensional space, we create d « m&apos; unit random vectors also of m&apos; dimensions, labelled {~r1, ~r2,..., ~rd}. Each By combining equations 5 and 6 we get the following approximation of the cosine distance: cos (θ(~u , ~u)) = cos((x(u,�v) ) π) (7) That is, the cosine of two context vectors is approximated by the cosine of the Hamming distance between their two signatures normalised by the size of the signatures. Search is performed</context>
<context position="14369" citStr="Ravichandran et al. (2005)" startWordPosition="2358" endWordPosition="2361"> a list of the closest k terms. When the kth closest term is found, the radius is set to the distance between this term and the target. Each time a new, closer element is added to the list, the radius is updated to the distance from the target to the new kth closest term. Construction complexity is O(n log n). Search complexity is claimed to be O(log n) for small radius searches. This does not hold for our decreasing radius search, whose worst case complexity is O(n). 5.2 Point Location in Equal Balls PLEB is a randomised structure that uses the bit signatures generated by LSH. It was used by Ravichandran et al. (2005) to improve the efficiency of distributional similarity calculations. Having generated our d length bit signatures for each of our n terms, we take these signatures and randomly permute the bits. Each vector has the same permutation applied. This is equivalent to a column reordering in a matrix where the rows are the terms and the columns the bits. After applying the permutation, the list of terms is sorted lexicographically based on the bit signatures. The list is scanned sequentially, and each term is compared to its B nearest neighbours in the list. The choice of B will effect the accuracy/</context>
<context position="21632" citStr="Ravichandran et al. (2005)" startWordPosition="3675" endWordPosition="3678">N search (NAIVE). We present results for the canonical attribute heuristic (HEURISTIC), RI, LSH, PLEB, VPT and SASH. We take the optimal canonical attribute vector length of 30 for HEURISTIC from Curran (2004). For SASH we take optimal values of p = 4 and c = 16 and use the folded ordering taking M = 1000 from Gorman and Curran (2005b). For RI, LSH and PLEB we found optimal values experimentally using the BNC. For LSH we chose d = 3, 000 (LSH3,000) and 10, 000 (LSH10,000), showing the effect of changing the dimensionality. The frequency statistics were weighted using mutual information, as in Ravichandran et al. (2005): p(w r w&apos;) The initial experiments on RI produced quite poor results. The intuition was that this was caused by the lack of smoothing in the algorithm. Experiments were performed using the weights given in Curran (2004). Of these, mutual information (10), evaluated with an extra log2(�(w, r, w&apos;) + 1) factor and limited to positive values, produced the best results (RIMI). The values d = 1000 and E = 5 were found to produce the best results. All experiments were performed on 3.2GHz Xeon P4 machines with 4GB of RAM. 8 Results As the accuracy of comparisons between terms increases with frequency</context>
<context position="22938" citStr="Ravichandran et al. (2005)" startWordPosition="3906" endWordPosition="3909">ocabulary (n) and increase the average accuracy of comparisons. Table 1 shows the reduction in vocabulary and increase in average context relations per term as cut-off increases. For LARGE, the initial 541,722 word vocabulary is reduced by 66% when a cut-off of 5 is applied and by 86% when the cut-off is increased to 100. The average number of relations increases from 97 to 1400. The work by Curran (2004) largely uses a frequency cut-off of 5. When this cut-off was used with the randomised techniques RI and LSH, it produced quite poor results. When the cut-off was increased to 100, as used by Ravichandran et al. (2005), the results improved significantly. Table 2 shows the INVR scores for our various techniques using the BNC with cut-offs of 5 and 100. Table 3 shows the results of a full thesaurus extraction using the BNC and LARGE corpora using a cut-off of 100. The average DIRECT score and INVR are from the 300 test words. The total execution time is extrapolated from the average search time of these test words and includes the setup time. For LARGE, extraction using NAIVE takes 444 hours: over 18 days. If the 184,494 word vocabulary were used, it would take over 7000 hours, or nearly 300 days. This gives</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and NLP: Using locality sensitive hash functions for high speed noun clustering. In Proceedings of the 43rd Annual Meeting of the ACL, pages 622–629, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mangus Sahlgren</author>
<author>Jussi Karlgren</author>
</authors>
<title>Automatic bilingual lexicon acquisition using random indexing of parallel corpora.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering, Special Issue on Parallel Texts,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="8591" citStr="Sahlgren and Karlgren (2005)" startWordPosition="1330" endWordPosition="1333">in(w(wm, r, w0), w(w, r, w0)) 362 able solution is required to handle the volumes of data we propose to use. Randomised techniques provide a possible solution to this. We present two techniques that have been used recently for distributional similarity: Random Indexing (Kanerva et al., 2000) and Locality Sensitive Hashing (LSH, Broder, 1997). 4.1 Random Indexing Random Indexing (RI) is a hashing technique based on Sparse Distributed Memory (Kanerva, 1993). Karlgren and Sahlgren (2001) showed RI produces results similar to LSA using the Test of English as a Foreign Language (TOEFL) evaluation. Sahlgren and Karlgren (2005) showed the technique to be successful in generating bilingual lexicons from parallel corpora. In RI, we first allocate a d length index vector to each unique attribute. The vectors consist of a large number of 0s and small number (E) number of randomly distributed ±1s. Context vectors, identifying terms, are generated by summing the index vectors of the attributes for each non-unique context in which a term appears. The context vector for a term t appearing in contexts c1 = [1, 0, 0, −1] and c2 = [0, 1, 0, −1] would be [1, 1, 0, −2]. The distance between these context vectors is then measured</context>
</contexts>
<marker>Sahlgren, Karlgren, 2005</marker>
<rawString>Mangus Sahlgren and Jussi Karlgren. 2005. Automatic bilingual lexicon acquisition using random indexing of parallel corpora. Journal of Natural Language Engineering, Special Issue on Parallel Texts, 11(3), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>Co-occurance retrieval: A flexible framework for lexical distributional similarity.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="26260" citStr="Weeds and Weir, 2005" startWordPosition="4487" endWordPosition="4490">meters may improve its accuracy. RIMI produces similar result using LARGE to SASH using BNC. This does not include the cost of extracting context relations from the raw text, so the true comparison is much worse. SASH allows the free use of weight and measure functions, but RI is constrained by having to transform any context space into a RI space. This is important when LARGE CUT-OFF 0 5 100 NAIVE 541,721 184,493 35,617 SASH 10,599 8,796 6,231 INDEX 5,844 13,187 32,663 Table 4: Average number of comparisons per term considering that different tasks may require different weights and measures (Weeds and Weir, 2005). RI also suffers n2 complexity, where as SASH is n log n. Taking these into account, and that the improvements are barely significant, SASH is a better choice. The results for LSH are disappointing. It performs consistently worse than the other methods except VPT. This could be improved by using larger bit vectors, but there is a limit to the size of these as they represent a significant memory overhead, particularly as the vocabulary increases. Table 4 presents the theoretical analysis of attribute indexing. The average number of comparisons made for various cut-offs of LARGE are shown. NAIV</context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>Julie Weeds and David Weir. 2005. Co-occurance retrieval: A flexible framework for lexical distributional similarity. Computational Linguistics, 31(4):439–475, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter N Yianilos</author>
</authors>
<title>Data structures and algorithms for nearest neighbor search in general metric spaces.</title>
<date>1993</date>
<booktitle>In Proceedings of the fourth annual ACM-SIAM Symposium on Discrete algorithms,</booktitle>
<pages>311--321</pages>
<location>Philadelphia.</location>
<contexts>
<context position="11506" citStr="Yianilos, 1993" startWordPosition="1842" endWordPosition="1843">e get the following approximation of the cosine distance: cos (θ(~u , ~u)) = cos((x(u,�v) ) π) (7) That is, the cosine of two context vectors is approximated by the cosine of the Hamming distance between their two signatures normalised by the size of the signatures. Search is performed using Equation 7 and scales to O(n2d). 5 Data Structures The methods presented above fail to address the n2 component of the search complexity. Many data structures have been proposed that can be used to address this problem in similarity searching. We present three data structures: the vantage point tree (VPT, Yianilos, 1993), which indexes points in a metric space, Point Location in Equal 363 Balls (PLEB, Indyk and Motwani, 1998), a probabilistic structure that uses the bit signatures generated by LSH, and the Spatial Approximation Sample Hierarchy (SASH, Houle and Sakuma, 2005), which approximates a k-NN search. Another option inspired by IR is attribute indexing (INDEX). In this technique, in addition to each term having a reference to its attributes, each attribute has a reference to the terms referencing it. Each term is then only compared with the terms with which it shares attributes. We will give a theoret</context>
</contexts>
<marker>Yianilos, 1993</marker>
<rawString>Peter N. Yianilos. 1993. Data structures and algorithms for nearest neighbor search in general metric spaces. In Proceedings of the fourth annual ACM-SIAM Symposium on Discrete algorithms, pages 311–321, Philadelphia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>