<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006240">
<title confidence="0.788178666666667">
Squibs and Discussions
Evaluating Discourse and Dialogue
Coding Schemes
</title>
<author confidence="0.99795">
Richard Craggs* Mary McGee Wood*
</author>
<affiliation confidence="0.998624">
University of Manchester University of Manchester
</affiliation>
<bodyText confidence="0.99865575">
Agreement statistics play an important role in the evaluation of coding schemes for discourse and
dialogue. Unfortunately there is a lack of understanding regarding appropriate agreement mea-
sures and how their results should be interpreted. In this article we describe the role of agreement
measures and argue that only chance-corrected measures that assume a common distribution of
labels for all coders are suitable for measuring agreement in reliability studies. We then provide
recommendations for how reliability should be inferred from the results of agreement statistics.
Since Jean Carletta (1996) exposed computational linguists to the desirability of using
chance-corrected agreement statistics to infer the reliability of data generated by apply-
ing coding schemes, there has been a general acceptance of their use within the field.
However, there are prevailing misunderstandings concerning agreement statistics and
the meaning of reliability.
Investigation of new dialogue types and genres has been shown to reveal new
phenomena in dialogue that are ill suited to annotation by current methods and also
new annotation schemes that are qualitatively different from those commonly used
in dialogue analysis. Previously prescribed practices for evaluating coding schemes
become less applicable as annotation schemes become more sophisticated. To compen-
sate, we need a greater understanding of reliability statistics and how they should be
interpreted. In this article we discuss the purpose of reliability testing, address certain
misunderstandings, and make recommendations regarding the way in which coding
schemes should be evaluated.
</bodyText>
<sectionHeader confidence="0.437207" genericHeader="method">
1. Agreement, Reliability, and Coding Schemes
</sectionHeader>
<bodyText confidence="0.999530777777778">
After developing schemes for annotating discourse or dialogue, it is necessary to
assess their suitability for the purpose for which they are designed. Although no
statistical test can determine whether any form of annotation is worthwhile or how
applications will benefit from it, we at least need to show that coders are capable of
performing the annotation. This often means assessing reliability based on agreement
between annotators applying the scheme. Agreement measures are discussed in detail
in section 2.
Much of the confusion regarding which agreement measures to apply and how their
results should be interpreted stems from a lack of understanding of what it means to
</bodyText>
<note confidence="0.763968">
* School of Computer Science, University of Manchester, Manchester, M13 9PL, U.K.
</note>
<email confidence="0.787459">
E-mail: richard craggs@yahoo.co.uk; mary mcgee.wood@manchester.ac.uk.
</email>
<note confidence="0.933526">
© 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 3
</note>
<bodyText confidence="0.9997984">
assess reliability. For example, the coding manual for the Switchboard DAMSL
dialogue act annotation scheme (Jurafsky, Shriberg, and Biasca 1997, page 2) states that
kappa is used to “assess labelling accuracy,” and Di Eugenio and Glass (2004) relate
reliability to “the objectivity of decisions,” whereas Carletta (1996) regards reliability as
the degree to which we understand the judgments that annotators are asked to make.
Although most researchers recognize that reporting agreement statistics is an important
part of evaluating coding schemes, there is frequently a lack of understanding of what
the figures actually mean.
The intended meaning of reliability should refer to the degree to which the data
generated by coders applying a scheme can be relied upon. If we consider the coding
process to involve mapping units of analysis onto categories, data are reliable if coders
agree on the category onto which each unit should be mapped. The further from perfect
agreement that coders stray, the less we can rely on the resulting annotation.
If data produced by applying a scheme are shown to be reliable, then we have
established two important properties of those data:
</bodyText>
<listItem confidence="0.980047">
1. The categories onto which the units are mapped are not inordinately
dependent on the idiosyncratic judgments of any individual coder.
2. There is a shared understanding of the meaning of the categories and how
data are mapped onto them.
</listItem>
<bodyText confidence="0.999579166666667">
The first of these is important for ensuring the reproducibility of the coding. To be able to
trust the analysis of annotated corpora, we need to be confident that the categorization
of the units of data is not dependent on which individual performed the annotation. The
second governs the value of data resulting from the coding process. For an annotated
corpus or the analysis thereof to be valuable, the phenomenon being annotated must
represent some notion in which we can enjoy a shared understanding.
</bodyText>
<sectionHeader confidence="0.970991" genericHeader="method">
2. Agreement Measures
</sectionHeader>
<bodyText confidence="0.997978578947368">
There are many ways in which the level of agreement between coders can be evaluated,
and the choice of which to apply in order to assess reliability is the source of much con-
fusion. An appropriate statistic for this purpose must measure agreement as a function
of the coding process and not of the coders, data, or categories. Only if the results of
a test are solely dependent on the degree to which there is a shared understanding of
how the phenomena to be described are mapped to the given categories can we infer the
reliability of the resulting data. Some agreement measures do not behave in this manner
and are therefore unsuitable for evaluating reliability.
A great deal of importance is placed on domain specificity in discourse and dialogue
studies and as such, researchers are often encouraged to evaluate schemes using corpora
from more than one domain. Concerning agreement, this encouragement is misplaced.
Since an appropriate agreement measure is a function of only the coding process, if the
original agreement test is performed in a scientifically sound manner, little more can
be proved by applying it again to different data. Any differences in the results between
corpora are a function of the variance between samples and not of the reliability of the
coding scheme.
Di Eugenio and Glass (2004) identify three general classes of agreement statistics
and suggest that all three should be used in conjunction in order to accurately evaluate
coding schemes. However, this suggestion is founded on some misunderstandings of
</bodyText>
<page confidence="0.955313">
290
</page>
<note confidence="0.648879">
Craggs and Wood Evaluating Discourse and Dialogue Coding Schemes
</note>
<bodyText confidence="0.987226">
the role of agreement measure in reliability studies. We shall now rectify these and
conclude that only one class of agreement measure is suitable.
</bodyText>
<subsectionHeader confidence="0.992569">
2.1 Percentage Agreement
</subsectionHeader>
<bodyText confidence="0.999945757575758">
The first of the recommended agreement tests, percentage agreement, measures the
proportion of agreements between coders. This is an unsuitable measure for inferring
reliability, and it was the use of this measure that prompted Carletta (1996) to recom-
mend chance-corrected measures.
Percentage agreement is inappropriate for inferring reliability because it excludes
any notion of the level of agreement that we could expect to achieve by chance. Reliabil-
ity should be inferred by locating the achieved level of agreement on a scale between the
best possible (coders agree perfectly) and the worst possible (coders do not understand
or cannot perform the mapping and behave randomly). Without any indication of the
agreement that coders would achieve by behaving randomly, any deviation from perfect
agreement is uninterpretable (Krippendorff 2004b).
The justification given for using percentage agreement is that it does not suffer from
what Di Eugenio and Glass (2004) referred to as the “prevalence problem.” Prevalence
refers to the unequal distribution of label use by coders. For example, Table 1 shows
an example taken from Di Eugenio and Glass (2004) showing the classification of the
utterance Okay as an acceptance or acknowledgment. It represents a confusion matrix
describing the number of occasions that coders used pairs of labels for a given turn. This
table shows that the two coders favored the use of accept strongly over acknowledge. They
correctly state that this skew in the distribution of categories increases the expected
chance agreement, thus lowering the overall agreement in chance-corrected tests. The
reason for this is that since one category is more popular than others, the likelihood of
coders’ agreeing by chance by choosing this category increases. We therefore require a
comparable increase in observed agreement to accommodate this.
Di Eugenio and Glass (2004) perceive this as an “unpleasant behavior” of chance-
corrected tests, one that prevents us from concluding that the example given in Table 1
shows satisfactory levels of agreement. Instead they use percentage agreement to
arrive at this conclusion. By examining the data, it is clear that this conclusion would
be false.
In Table 1, the coders agree 90 out of 100 times, but all agreements occur when both
coders choose accept. There is not a single case in which they agree on Okay’s being used
as an acknowledgment. The only conclusion one may justifiably draw is that the coders
cannot distinguish the use of Okay as an acceptance from its use as an acknowledgment.
Rather than being an unpleasant behavior, accounting for prevalence in the data is an
</bodyText>
<tableCaption confidence="0.998359">
Table 1
</tableCaption>
<table confidence="0.889981625">
Prevalence in coding.
Coder 2
Coder 1 Accept Ack
Accept 90 5 95
Acknowledge 5 0 5
95 5 100
291
Computational Linguistics Volume 31, Number 3
</table>
<bodyText confidence="0.835044666666667">
important part of accurately reporting the level of agreement. This helps us to avoid
arriving at incorrect conclusions such as believing that the data shown in Table 1 suggest
reliable coding.
</bodyText>
<subsectionHeader confidence="0.99023">
2.2 Chance-Corrected Agreement: Unequal Coder Category Distribution
</subsectionHeader>
<bodyText confidence="0.999965714285714">
The second class of agreement measure recommended in Di Eugenio and Glass (2004)
is that of chance-corrected tests that do not assume an equal distribution of categories
between coders. Chance-corrected tests compute agreement according to the ratio of
observed (dis)agreement to that which we could expect by chance, estimated from the
data. The measures differ in the way in which this expected (dis)agreement is estimated.
Those that do not assume an equal distribution between coders calculate expected
(dis)agreement based on the individual distribution of each coder.
The concern that in discourse and dialogue coding, coders will differ in the fre-
quency with which they apply labels leads Di Eugenio and Glass to conclude that
Cohen’s (1960) kappa is the best chance-corrected test to apply. To clarify, by unequal
distribution of categories, we do not refer to the disparity in the frequency with which
categories occur (e.g., verbs are more common than pronouns) but rather to the differ-
ence in proclivity between coders (e.g., coder A is more likely to label something a noun
than coder B).
Cohen’s kappa calculates expected chance agreement, based on the individual
coders’ distributions, in a manner similar to association measures, such as chi–square.
This means that its results are dependent on the preferences of the individual coders
taking part in the tests. This violates the condition set out at the beginning of this section
whereby agreement must be a function of the coding process, with coders being viewed
as interchangeable. The purpose of assessing the reliability of coding schemes is not to
judge the performance of the small number of individuals participating in the trial, but
rather to predict the performance of the schemes in general. The proposal that in most
discourse and dialogue studies, the assumption of equal distribution between coders
does not hold is, in fact, an argument against the use of Cohen’s kappa. Assessing the
agreement between coders and accounting for their idiosyncratic proclivity toward or
against certain labels tells us little about how the coding scheme will perform when ap-
plied by others. The solution is not to apply a test that panders to individual differences,
but rather to increase the number of coders so that the influence of any individual on
the final result becomes less pronounced.1
Another reason provided for using Cohen’s kappa is that its sensitivity to bias (dif-
ferences in coders’ category distribution) can be exploited to improve coding schemes.
However, there is no need to calculate kappa in order to observe bias, since it will
be evident in a contingency table of the data in question. Even if it were necessary to
compute kappa for this purpose, however, this would not justify its use as a reliability
test.
</bodyText>
<subsectionHeader confidence="0.991504">
2.3 Chance-Corrected Agreement: Assumed Equal Coder Category Distribution
</subsectionHeader>
<bodyText confidence="0.92787975">
The remaining class of agreement measure assumes an equal distribution of categories
for all coders. Once we have accepted that this assumption is necessary in order to
1 When there is a single correct label that should be used, such as part-of-speech tags used to describe the
syntactic function of a word or group of words, then training coders may mitigate coder preference.
</bodyText>
<page confidence="0.992681">
292
</page>
<note confidence="0.837955">
Craggs and Wood Evaluating Discourse and Dialogue Coding Schemes
</note>
<bodyText confidence="0.999765470588235">
predict the performance of the scheme in general, there appears to be no objection to
using this type of statistical test for assessing agreement in discourse and dialogue work.
Tests that fall into this class include Siegel and Castellan’s (1988) extension of Scott’s
(1955) pi, confusingly called kappa, and Krippendorff’s (2004a) alpha. Both of these
measures calculate expected (dis)agreement based on the frequency with which each
category is used, estimated from the overall usage by the coders.
Kappa is more frequently described in statistics textbooks and more commonly
implemented in statistical software. In circumstances in which mechanisms other than
nominal labels are used to annotate data, alpha has the benefit of being able to deal with
different degrees of disagreement between pairs of interval, ordinal, and ratio values,
among others.
Di Eugenio and Glass (2004) conclude with the proposal that these three forms of
agreement measure collectively provide better means with which to judge agreement
than any individual test. We would argue, to the contrary, that applying three different
metrics to measure the same property suggests a lack of confidence in any of them.
Percentage agreement and Cohen’s kappa do not provide an insight into a scheme’s
reliability, so reporting their results is potentially misleading.
</bodyText>
<sectionHeader confidence="0.975841" genericHeader="method">
3. Inferring Reliability
</sectionHeader>
<bodyText confidence="0.999933866666667">
To reiterate, when testing reliability we are assessing whether the data that a scheme
generates can be relied on. This may be inferred from the level of agreement between
coders applying the scheme. In section 1 we described two properties of reliable data
that are important to establish in discourse and dialogue analysis. In this section we
explain how the gap between agreement and reliability may be bridged.
When inferring reliability from agreement, a common error is to believe that there
are a number of thresholds against which agreement scores can be measured in order to
gauge whether or not a coding scheme produces reliable data. Most commonly this
is Krippendorff’s decision criterion, in which scores greater than 0.8 are considered
satisfactory and scores greater than 0.667 allow tentative conclusions to be drawn
(Krippendorff 2004a). The prevalent use of this criterion despite repeated advice that
it is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004;
Krippendorff 2004a) is probably due to a desire for a simple system that can be easily
applied to a scheme. Unfortunately, because of the diversity of both the phenomena
being coded and the applications of the results, it is impossible to prescribe a scale
against which all coding schemes can be judged.
Instead we provide discussion and some recommendations, all founded on the
premise that reliability must “correlate with the conditions under which one is willing to
rely on imperfect data” (Krippendorff 2004b, page 6). A common concern regarding the
application of standards from other fields, such as the one described above, to discourse
and dialogue research is that the subjectivity of the phenomena being coded may
mean that we never obtain the necessary agreement levels. In this context, subjectivity
describes the absence of an obvious mapping for each unit of analysis onto categories
that describe the phenomenon in question. However, the fact that we consider these
subjective phenomena worthy of study shows that we are, in fact, “willing to rely
on imperfect data,” which is fine as long as we recognize the limitations of a scheme
that delivers less-than-ideal levels of reliability and use the resulting annotated corpora
accordingly.
In order to discuss the acceptable levels of agreement for discourse and dialogue
coding, let us consider two popular uses of coded data: to train systems to perform
</bodyText>
<page confidence="0.994349">
293
</page>
<note confidence="0.595581">
Computational Linguistics Volume 31, Number 3
</note>
<bodyText confidence="0.9656965">
some automated task and to study the relationship between the coded phenomena and
some other feature of the data.
</bodyText>
<subsectionHeader confidence="0.997096">
3.1 Reliability and Training for Automatic Annotation
</subsectionHeader>
<bodyText confidence="0.999955285714286">
Considering the effort involved in manually annotating linguistic data, it is un-
surprising that attempts are often made to train a system to perform such annotation
automatically (Mast et al. 1996; Wrede and Shriberg 2003). The reliability of manually
annotated data is clearly a concern when they are used to train a system. If the level
of agreement for the annotation scheme is low, then the system is going to replicate
the inconsistent behavior of human annotators. Any deviant behavior by the system
resulting in less than 100% accuracy in comparison with the manual annotation will
compound the problem, possibly leading to meaningless data. Worse still, if a system
is to learn how to annotate from manually annotated data, it will do so based on the
patterns observed in those data. If the manual annotation is not reliable, then those
patterns may be nonexistent or misleading.
Returning to our original premise, we would suggest that if a coding scheme is to
be used to generate data from which a system will learn to perform similar coding, then
we should be “unwilling to rely on imperfect data.”
</bodyText>
<subsectionHeader confidence="0.997263">
3.2 Reliability and Corpus Analysis
</subsectionHeader>
<bodyText confidence="0.99995975">
Manually annotated corpora can also be used to infer a relationship between the phe-
nomena in question and some other facet of the data. When performing this sort of
analysis, we may be more willing to work with imperfect data and therefore accept
lower levels of agreement. However, the conclusions that are gleaned from the analysis
must be tempered according to the level of agreement achieved. For example, when it is
suggested that a correlation exists between the occurrence of one phenomenon and that
of another, less agreement observed in the sample annotation requires stronger evidence
of the correlation in order for the conclusion to be valid.
To summarize, there are no magic thresholds that, once crossed, entitle us to claim
that a coding scheme is reliable. One must decide for oneself, based on the intended use
of a scheme, whether the observed level of agreement is sufficient and conduct one’s
analysis accordingly.
</bodyText>
<sectionHeader confidence="0.997039" genericHeader="method">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999929714285714">
The application of agreement statistics has done much to improve the scientific rigor
of discourse and dialogue research. However, unless we understand what we are
attempting to prove and which tests are appropriate, the results of evaluation can be
unsatisfactory or, worse still, misleading. In this article we have encouraged researchers
to clarify their reasons for assessing agreement and have suggested that in many cases
the most suitable test for this purpose is one that corrects for expected agreement, based
on an assumed equal distribution between coders.
</bodyText>
<sectionHeader confidence="0.619425" genericHeader="method">
Acknowledgments References
</sectionHeader>
<bodyText confidence="0.99737">
The authors acknowledge the help of Klaus Carletta, Jean. 1996. Assessing agreement
Krippendorff for patiently aiding our on classification tasks: The kappa
understanding of reliability and University statistic. Computational Linguistics,
of Manchester for funding Richard Craggs. 22(2):249–254.
</bodyText>
<page confidence="0.994521">
294
</page>
<note confidence="0.899991">
Craggs and Wood Evaluating Discourse and Dialogue Coding Schemes
</note>
<reference confidence="0.994430842105263">
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Education and Psychological
Measurement, 43(6):37–46.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95–101.
Jurafsky, Daniel, Elizabeth Shriberg, and
Debra Biasca.1997. Switchboard
SWBD-DAMSL shallow-discourse-function
annotation coders manual. Technical Report
(Draft 13), University of Colorado.
Krippendorff, Klaus. 2004a. Content Analysis:
An Introduction to Its Methodology. 2nd ed.
Sage, Beverly Hills, CA.
Krippendorff, Klaus. 2004b. Reliability in
content analysis: Some common
misconceptions and recommendations.
Human Communication Research,
30(3):411–437.
Mast, Marion, Heinrich Niemann, Elmar
Noth, and Ernst Gunter
Schukat-Talamazzini. 1996. Automatic
classification of dialog acts with semantic
classification trees and polygrams. In
Learning for Natural Language Processing,
edited by Stefan Wermter, Ellen Riloff,
and Gabriele Scheler. Springer,
pages 217–229.
Scott, W. A. 1955. Reliability of content
analysis: The case of nominal scale coding.
Public Opinion Quarterly, 19:127–141.
Siegel, Sidney and John N. Castellan, Jr. 1988.
Nonparametric Statistics. 2nd ed.
McGraw-Hill.
Wrede, Britta and Elizabeth Shriberg. 2003.
Spotting “hot spots” in meetings: Human
judgments and prosodic cues. In
Proceedings of EUROSPEECH, Geneva.
</reference>
<page confidence="0.998573">
295
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.008626">
<title confidence="0.999264333333333">Squibs and Discussions Evaluating Discourse and Dialogue Coding Schemes</title>
<author confidence="0.98255">McGee</author>
<affiliation confidence="0.997059">University of Manchester University of Manchester</affiliation>
<abstract confidence="0.97899806722689">Agreement statistics play an important role in the evaluation of coding schemes for discourse and dialogue. Unfortunately there is a lack of understanding regarding appropriate agreement measures and how their results should be interpreted. In this article we describe the role of agreement measures and argue that only chance-corrected measures that assume a common distribution of labels for all coders are suitable for measuring agreement in reliability studies. We then provide recommendations for how reliability should be inferred from the results of agreement statistics. Since Jean Carletta (1996) exposed computational linguists to the desirability of using chance-corrected agreement statistics to infer the reliability of data generated by applying coding schemes, there has been a general acceptance of their use within the field. However, there are prevailing misunderstandings concerning agreement statistics and the meaning of reliability. Investigation of new dialogue types and genres has been shown to reveal new phenomena in dialogue that are ill suited to annotation by current methods and also new annotation schemes that are qualitatively different from those commonly used in dialogue analysis. Previously prescribed practices for evaluating coding schemes become less applicable as annotation schemes become more sophisticated. To compensate, we need a greater understanding of reliability statistics and how they should be interpreted. In this article we discuss the purpose of reliability testing, address certain misunderstandings, and make recommendations regarding the way in which coding schemes should be evaluated. 1. Agreement, Reliability, and Coding Schemes After developing schemes for annotating discourse or dialogue, it is necessary to assess their suitability for the purpose for which they are designed. Although no statistical test can determine whether any form of annotation is worthwhile or how applications will benefit from it, we at least need to show that coders are capable of performing the annotation. This often means assessing reliability based on agreement between annotators applying the scheme. Agreement measures are discussed in detail in section 2. Much of the confusion regarding which agreement measures to apply and how their results should be interpreted stems from a lack of understanding of what it means to of Computer Science, University of Manchester, Manchester, M13 9PL, U.K. E-mail: richard craggs@yahoo.co.uk; mary mcgee.wood@manchester.ac.uk. © 2005 Association for Computational Linguistics Computational Linguistics Volume 31, Number 3 assess reliability. For example, the coding manual for the Switchboard DAMSL dialogue act annotation scheme (Jurafsky, Shriberg, and Biasca 1997, page 2) states that kappa is used to “assess labelling accuracy,” and Di Eugenio and Glass (2004) relate reliability to “the objectivity of decisions,” whereas Carletta (1996) regards reliability as the degree to which we understand the judgments that annotators are asked to make. Although most researchers recognize that reporting agreement statistics is an important part of evaluating coding schemes, there is frequently a lack of understanding of what figures actually The intended meaning of reliability should refer to the degree to which the data generated by coders applying a scheme can be relied upon. If we consider the coding process to involve mapping units of analysis onto categories, data are reliable if coders agree on the category onto which each unit should be mapped. The further from perfect agreement that coders stray, the less we can rely on the resulting annotation. If data produced by applying a scheme are shown to be reliable, then we have established two important properties of those data: 1. The categories onto which the units are mapped are not inordinately dependent on the idiosyncratic judgments of any individual coder. 2. There is a shared understanding of the meaning of the categories and how data are mapped onto them. The first of these is important for ensuring the reproducibility of the coding. To be able to trust the analysis of annotated corpora, we need to be confident that the categorization of the units of data is not dependent on which individual performed the annotation. The second governs the value of data resulting from the coding process. For an annotated corpus or the analysis thereof to be valuable, the phenomenon being annotated must represent some notion in which we can enjoy a shared understanding. 2. Agreement Measures There are many ways in which the level of agreement between coders can be evaluated, and the choice of which to apply in order to assess reliability is the source of much confusion. An appropriate statistic for this purpose must measure agreement as a function of the coding process and not of the coders, data, or categories. Only if the results of a test are solely dependent on the degree to which there is a shared understanding of how the phenomena to be described are mapped to the given categories can we infer the reliability of the resulting data. Some agreement measures do not behave in this manner and are therefore unsuitable for evaluating reliability. A great deal of importance is placed on domain specificity in discourse and dialogue studies and as such, researchers are often encouraged to evaluate schemes using corpora from more than one domain. Concerning agreement, this encouragement is misplaced. Since an appropriate agreement measure is a function of only the coding process, if the original agreement test is performed in a scientifically sound manner, little more can be proved by applying it again to different data. Any differences in the results between corpora are a function of the variance between samples and not of the reliability of the coding scheme. Di Eugenio and Glass (2004) identify three general classes of agreement statistics and suggest that all three should be used in conjunction in order to accurately evaluate coding schemes. However, this suggestion is founded on some misunderstandings of 290 Craggs and Wood Evaluating Discourse and Dialogue Coding Schemes the role of agreement measure in reliability studies. We shall now rectify these and conclude that only one class of agreement measure is suitable. 2.1 Percentage Agreement first of the recommended agreement tests, agreement, the proportion of agreements between coders. This is an unsuitable measure for inferring reliability, and it was the use of this measure that prompted Carletta (1996) to recommend chance-corrected measures. Percentage agreement is inappropriate for inferring reliability because it excludes any notion of the level of agreement that we could expect to achieve by chance. Reliability should be inferred by locating the achieved level of agreement on a scale between the best possible (coders agree perfectly) and the worst possible (coders do not understand or cannot perform the mapping and behave randomly). Without any indication of the agreement that coders would achieve by behaving randomly, any deviation from perfect agreement is uninterpretable (Krippendorff 2004b). The justification given for using percentage agreement is that it does not suffer from Di Eugenio and Glass (2004) referred to as the “prevalence problem.” refers to the unequal distribution of label use by coders. For example, Table 1 shows an example taken from Di Eugenio and Glass (2004) showing the classification of the an acceptance or acknowledgment. It represents a confusion matrix describing the number of occasions that coders used pairs of labels for a given turn. This shows that the two coders favored the use of over They correctly state that this skew in the distribution of categories increases the expected chance agreement, thus lowering the overall agreement in chance-corrected tests. The reason for this is that since one category is more popular than others, the likelihood of coders’ agreeing by chance by choosing this category increases. We therefore require a comparable increase in observed agreement to accommodate this. Di Eugenio and Glass (2004) perceive this as an “unpleasant behavior” of chancecorrected tests, one that prevents us from concluding that the example given in Table 1 shows satisfactory levels of agreement. Instead they use percentage agreement to arrive at this conclusion. By examining the data, it is clear that this conclusion would be false. In Table 1, the coders agree 90 out of 100 times, but all agreements occur when both choose There is not a single case in which they agree on being used as an acknowledgment. The only conclusion one may justifiably draw is that the coders distinguish the use of an acceptance from its use as an acknowledgment. Rather than being an unpleasant behavior, accounting for prevalence in the data is an Table 1 Prevalence in coding.</abstract>
<note confidence="0.97960075">Coder 2 Coder 1 Accept Ack Accept 90 5 95 Acknowledge 5 0 5</note>
<phone confidence="0.550731">95 5 100</phone>
<abstract confidence="0.988575176470588">291 Computational Linguistics Volume 31, Number 3 important part of accurately reporting the level of agreement. This helps us to avoid arriving at incorrect conclusions such as believing that the data shown in Table 1 suggest reliable coding. 2.2 Chance-Corrected Agreement: Unequal Coder Category Distribution The second class of agreement measure recommended in Di Eugenio and Glass (2004) that of tests do not assume an equal distribution of categories between coders. Chance-corrected tests compute agreement according to the ratio of observed (dis)agreement to that which we could expect by chance, estimated from the data. The measures differ in the way in which this expected (dis)agreement is estimated. Those that do not assume an equal distribution between coders calculate expected (dis)agreement based on the individual distribution of each coder. The concern that in discourse and dialogue coding, coders will differ in the frequency with which they apply labels leads Di Eugenio and Glass to conclude that Cohen’s (1960) kappa is the best chance-corrected test to apply. To clarify, by unequal distribution of categories, we do not refer to the disparity in the frequency with which categories occur (e.g., verbs are more common than pronouns) but rather to the difference in proclivity between coders (e.g., coder A is more likely to label something a noun than coder B). Cohen’s kappa calculates expected chance agreement, based on the individual coders’ distributions, in a manner similar to association measures, such as chi–square. This means that its results are dependent on the preferences of the individual coders taking part in the tests. This violates the condition set out at the beginning of this section whereby agreement must be a function of the coding process, with coders being viewed as interchangeable. The purpose of assessing the reliability of coding schemes is not to judge the performance of the small number of individuals participating in the trial, but rather to predict the performance of the schemes in general. The proposal that in most discourse and dialogue studies, the assumption of equal distribution between coders not hold is, in fact, an argument use of Cohen’s kappa. Assessing the agreement between coders and accounting for their idiosyncratic proclivity toward or against certain labels tells us little about how the coding scheme will perform when applied by others. The solution is not to apply a test that panders to individual differences, but rather to increase the number of coders so that the influence of any individual on final result becomes less Another reason provided for using Cohen’s kappa is that its sensitivity to bias (differences in coders’ category distribution) can be exploited to improve coding schemes. However, there is no need to calculate kappa in order to observe bias, since it will be evident in a contingency table of the data in question. Even if it were necessary to compute kappa for this purpose, however, this would not justify its use as a reliability test. 2.3 Chance-Corrected Agreement: Assumed Equal Coder Category Distribution The remaining class of agreement measure assumes an equal distribution of categories for all coders. Once we have accepted that this assumption is necessary in order to When there is a single that should be used, such as part-of-speech tags used to describe the syntactic function of a word or group of words, then training coders may mitigate coder preference. 292 Craggs and Wood Evaluating Discourse and Dialogue Coding Schemes predict the performance of the scheme in general, there appears to be no objection to using this type of statistical test for assessing agreement in discourse and dialogue work. Tests that fall into this class include Siegel and Castellan’s (1988) extension of Scott’s (1955) pi, confusingly called kappa, and Krippendorff’s (2004a) alpha. Both of these measures calculate expected (dis)agreement based on the frequency with which each category is used, estimated from the overall usage by the coders. Kappa is more frequently described in statistics textbooks and more commonly implemented in statistical software. In circumstances in which mechanisms other than nominal labels are used to annotate data, alpha has the benefit of being able to deal with different degrees of disagreement between pairs of interval, ordinal, and ratio values, among others. Di Eugenio and Glass (2004) conclude with the proposal that these three forms of agreement measure collectively provide better means with which to judge agreement than any individual test. We would argue, to the contrary, that applying three different metrics to measure the same property suggests a lack of confidence in any of them. Percentage agreement and Cohen’s kappa do not provide an insight into a scheme’s reliability, so reporting their results is potentially misleading. 3. Inferring Reliability To reiterate, when testing reliability we are assessing whether the data that a scheme generates can be relied on. This may be inferred from the level of agreement between coders applying the scheme. In section 1 we described two properties of reliable data that are important to establish in discourse and dialogue analysis. In this section we explain how the gap between agreement and reliability may be bridged. When inferring reliability from agreement, a common error is to believe that there are a number of thresholds against which agreement scores can be measured in order to gauge whether or not a coding scheme produces reliable data. Most commonly this Krippendorff’s decision criterion, in which scores greater than are considered and scores greater than allow tentative conclusions to be drawn (Krippendorff 2004a). The prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004; Krippendorff 2004a) is probably due to a desire for a simple system that can be easily applied to a scheme. Unfortunately, because of the diversity of both the phenomena being coded and the applications of the results, it is impossible to prescribe a scale against which all coding schemes can be judged. Instead we provide discussion and some recommendations, all founded on the premise that reliability must “correlate with the conditions under which one is willing to rely on imperfect data” (Krippendorff 2004b, page 6). A common concern regarding the application of standards from other fields, such as the one described above, to discourse and dialogue research is that the subjectivity of the phenomena being coded may that we never obtain the necessary agreement levels. In this context, describes the absence of an obvious mapping for each unit of analysis onto categories that describe the phenomenon in question. However, the fact that we consider these subjective phenomena worthy of study shows that we are, in fact, “willing to rely on imperfect data,” which is fine as long as we recognize the limitations of a scheme that delivers less-than-ideal levels of reliability and use the resulting annotated corpora accordingly. In order to discuss the acceptable levels of agreement for discourse and dialogue coding, let us consider two popular uses of coded data: to train systems to perform 293 Computational Linguistics Volume 31, Number 3 some automated task and to study the relationship between the coded phenomena and some other feature of the data. 3.1 Reliability and Training for Automatic Annotation Considering the effort involved in manually annotating linguistic data, it is unsurprising that attempts are often made to train a system to perform such annotation automatically (Mast et al. 1996; Wrede and Shriberg 2003). The reliability of manually annotated data is clearly a concern when they are used to train a system. If the level of agreement for the annotation scheme is low, then the system is going to replicate the inconsistent behavior of human annotators. Any deviant behavior by the system resulting in less than 100% accuracy in comparison with the manual annotation will compound the problem, possibly leading to meaningless data. Worse still, if a system is to learn how to annotate from manually annotated data, it will do so based on the patterns observed in those data. If the manual annotation is not reliable, then those patterns may be nonexistent or misleading. Returning to our original premise, we would suggest that if a coding scheme is to be used to generate data from which a system will learn to perform similar coding, then we should be “unwilling to rely on imperfect data.” 3.2 Reliability and Corpus Analysis Manually annotated corpora can also be used to infer a relationship between the phenomena in question and some other facet of the data. When performing this sort of analysis, we may be more willing to work with imperfect data and therefore accept lower levels of agreement. However, the conclusions that are gleaned from the analysis must be tempered according to the level of agreement achieved. For example, when it is suggested that a correlation exists between the occurrence of one phenomenon and that of another, less agreement observed in the sample annotation requires stronger evidence of the correlation in order for the conclusion to be valid. To summarize, there are no magic thresholds that, once crossed, entitle us to claim that a coding scheme is reliable. One must decide for oneself, based on the intended use of a scheme, whether the observed level of agreement is sufficient and conduct one’s analysis accordingly. 4. Conclusion The application of agreement statistics has done much to improve the scientific rigor of discourse and dialogue research. However, unless we understand what we are attempting to prove and which tests are appropriate, the results of evaluation can be unsatisfactory or, worse still, misleading. In this article we have encouraged researchers to clarify their reasons for assessing agreement and have suggested that in many cases the most suitable test for this purpose is one that corrects for expected agreement, based on an assumed equal distribution between coders.</abstract>
<note confidence="0.8921870625">Acknowledgments References The authors acknowledge the help of Klaus Krippendorff for patiently aiding our understanding of reliability and University of Manchester for funding Richard Craggs. Carletta, Jean. 1996. Assessing agreement on classification tasks: The kappa 22(2):249–254. 294 Craggs and Wood Evaluating Discourse and Dialogue Coding Schemes Cohen, J. 1960. A coefficient of agreement for scales. and Psychological 43(6):37–46. Di Eugenio, Barbara and Michael Glass. 2004. The kappa statistic: A second look. 30(1):95–101. Jurafsky, Daniel, Elizabeth Shriberg, and Biasca.1997. SWBD-DAMSL shallow-discourse-function coders Technical Report (Draft 13), University of Colorado. Klaus. 2004a. Analysis:</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Education and Psychological Measurement,</booktitle>
<pages>43--6</pages>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. 1960. A coefficient of agreement for nominal scales. Education and Psychological Measurement, 43(6):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The kappa statistic: A second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Di Eugenio, Barbara and Michael Glass. 2004. The kappa statistic: A second look. Computational Linguistics, 30(1):95–101.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Debra Biasca 1997</author>
</authors>
<title>Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual.</title>
<tech>Technical Report (Draft 13),</tech>
<institution>University of Colorado.</institution>
<marker>Jurafsky, Shriberg, 1997, </marker>
<rawString>Jurafsky, Daniel, Elizabeth Shriberg, and Debra Biasca.1997. Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual. Technical Report (Draft 13), University of Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. 2nd ed. Sage,</title>
<date>2004</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="7357" citStr="Krippendorff 2004" startWordPosition="1126" endWordPosition="1127">letta (1996) to recommend chance-corrected measures. Percentage agreement is inappropriate for inferring reliability because it excludes any notion of the level of agreement that we could expect to achieve by chance. Reliability should be inferred by locating the achieved level of agreement on a scale between the best possible (coders agree perfectly) and the worst possible (coders do not understand or cannot perform the mapping and behave randomly). Without any indication of the agreement that coders would achieve by behaving randomly, any deviation from perfect agreement is uninterpretable (Krippendorff 2004b). The justification given for using percentage agreement is that it does not suffer from what Di Eugenio and Glass (2004) referred to as the “prevalence problem.” Prevalence refers to the unequal distribution of label use by coders. For example, Table 1 shows an example taken from Di Eugenio and Glass (2004) showing the classification of the utterance Okay as an acceptance or acknowledgment. It represents a confusion matrix describing the number of occasions that coders used pairs of labels for a given turn. This table shows that the two coders favored the use of accept strongly over acknowl</context>
<context position="15111" citStr="Krippendorff 2004" startWordPosition="2362" endWordPosition="2363">erties of reliable data that are important to establish in discourse and dialogue analysis. In this section we explain how the gap between agreement and reliability may be bridged. When inferring reliability from agreement, a common error is to believe that there are a number of thresholds against which agreement scores can be measured in order to gauge whether or not a coding scheme produces reliable data. Most commonly this is Krippendorff’s decision criterion, in which scores greater than 0.8 are considered satisfactory and scores greater than 0.667 allow tentative conclusions to be drawn (Krippendorff 2004a). The prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004; Krippendorff 2004a) is probably due to a desire for a simple system that can be easily applied to a scheme. Unfortunately, because of the diversity of both the phenomena being coded and the applications of the results, it is impossible to prescribe a scale against which all coding schemes can be judged. Instead we provide discussion and some recommendations, all founded on the premise that reliability must “correlate with the conditions u</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Krippendorff, Klaus. 2004a. Content Analysis: An Introduction to Its Methodology. 2nd ed. Sage, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Reliability in content analysis: Some common misconceptions and recommendations.</title>
<date>2004</date>
<journal>Human Communication Research,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="7357" citStr="Krippendorff 2004" startWordPosition="1126" endWordPosition="1127">letta (1996) to recommend chance-corrected measures. Percentage agreement is inappropriate for inferring reliability because it excludes any notion of the level of agreement that we could expect to achieve by chance. Reliability should be inferred by locating the achieved level of agreement on a scale between the best possible (coders agree perfectly) and the worst possible (coders do not understand or cannot perform the mapping and behave randomly). Without any indication of the agreement that coders would achieve by behaving randomly, any deviation from perfect agreement is uninterpretable (Krippendorff 2004b). The justification given for using percentage agreement is that it does not suffer from what Di Eugenio and Glass (2004) referred to as the “prevalence problem.” Prevalence refers to the unequal distribution of label use by coders. For example, Table 1 shows an example taken from Di Eugenio and Glass (2004) showing the classification of the utterance Okay as an acceptance or acknowledgment. It represents a confusion matrix describing the number of occasions that coders used pairs of labels for a given turn. This table shows that the two coders favored the use of accept strongly over acknowl</context>
<context position="15111" citStr="Krippendorff 2004" startWordPosition="2362" endWordPosition="2363">erties of reliable data that are important to establish in discourse and dialogue analysis. In this section we explain how the gap between agreement and reliability may be bridged. When inferring reliability from agreement, a common error is to believe that there are a number of thresholds against which agreement scores can be measured in order to gauge whether or not a coding scheme produces reliable data. Most commonly this is Krippendorff’s decision criterion, in which scores greater than 0.8 are considered satisfactory and scores greater than 0.667 allow tentative conclusions to be drawn (Krippendorff 2004a). The prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004; Krippendorff 2004a) is probably due to a desire for a simple system that can be easily applied to a scheme. Unfortunately, because of the diversity of both the phenomena being coded and the applications of the results, it is impossible to prescribe a scale against which all coding schemes can be judged. Instead we provide discussion and some recommendations, all founded on the premise that reliability must “correlate with the conditions u</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Krippendorff, Klaus. 2004b. Reliability in content analysis: Some common misconceptions and recommendations. Human Communication Research, 30(3):411–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Mast</author>
<author>Heinrich Niemann</author>
<author>Elmar Noth</author>
<author>Ernst Gunter Schukat-Talamazzini</author>
</authors>
<title>Automatic classification of dialog acts with semantic classification trees and polygrams.</title>
<date>1996</date>
<booktitle>In Learning for Natural Language Processing,</booktitle>
<pages>217--229</pages>
<publisher>Springer,</publisher>
<note>edited by</note>
<contexts>
<context position="17105" citStr="Mast et al. 1996" startWordPosition="2678" endWordPosition="2681">ting annotated corpora accordingly. In order to discuss the acceptable levels of agreement for discourse and dialogue coding, let us consider two popular uses of coded data: to train systems to perform 293 Computational Linguistics Volume 31, Number 3 some automated task and to study the relationship between the coded phenomena and some other feature of the data. 3.1 Reliability and Training for Automatic Annotation Considering the effort involved in manually annotating linguistic data, it is unsurprising that attempts are often made to train a system to perform such annotation automatically (Mast et al. 1996; Wrede and Shriberg 2003). The reliability of manually annotated data is clearly a concern when they are used to train a system. If the level of agreement for the annotation scheme is low, then the system is going to replicate the inconsistent behavior of human annotators. Any deviant behavior by the system resulting in less than 100% accuracy in comparison with the manual annotation will compound the problem, possibly leading to meaningless data. Worse still, if a system is to learn how to annotate from manually annotated data, it will do so based on the patterns observed in those data. If t</context>
</contexts>
<marker>Mast, Niemann, Noth, Schukat-Talamazzini, 1996</marker>
<rawString>Mast, Marion, Heinrich Niemann, Elmar Noth, and Ernst Gunter Schukat-Talamazzini. 1996. Automatic classification of dialog acts with semantic classification trees and polygrams. In Learning for Natural Language Processing, edited by Stefan Wermter, Ellen Riloff, and Gabriele Scheler. Springer, pages 217–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Scott</author>
</authors>
<title>Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly,</title>
<date>1955</date>
<marker>Scott, 1955</marker>
<rawString>Scott, W. A. 1955. Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 19:127–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>John N Castellan</author>
</authors>
<date>1988</date>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Siegel, Sidney and John N. Castellan, Jr. 1988. Nonparametric Statistics. 2nd ed. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Britta Wrede</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Spotting “hot spots” in meetings: Human judgments and prosodic cues.</title>
<date>2003</date>
<booktitle>In Proceedings of EUROSPEECH,</booktitle>
<location>Geneva.</location>
<contexts>
<context position="17131" citStr="Wrede and Shriberg 2003" startWordPosition="2682" endWordPosition="2685">pora accordingly. In order to discuss the acceptable levels of agreement for discourse and dialogue coding, let us consider two popular uses of coded data: to train systems to perform 293 Computational Linguistics Volume 31, Number 3 some automated task and to study the relationship between the coded phenomena and some other feature of the data. 3.1 Reliability and Training for Automatic Annotation Considering the effort involved in manually annotating linguistic data, it is unsurprising that attempts are often made to train a system to perform such annotation automatically (Mast et al. 1996; Wrede and Shriberg 2003). The reliability of manually annotated data is clearly a concern when they are used to train a system. If the level of agreement for the annotation scheme is low, then the system is going to replicate the inconsistent behavior of human annotators. Any deviant behavior by the system resulting in less than 100% accuracy in comparison with the manual annotation will compound the problem, possibly leading to meaningless data. Worse still, if a system is to learn how to annotate from manually annotated data, it will do so based on the patterns observed in those data. If the manual annotation is no</context>
</contexts>
<marker>Wrede, Shriberg, 2003</marker>
<rawString>Wrede, Britta and Elizabeth Shriberg. 2003. Spotting “hot spots” in meetings: Human judgments and prosodic cues. In Proceedings of EUROSPEECH, Geneva.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>