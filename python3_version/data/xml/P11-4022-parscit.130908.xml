<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.988606">
C-Feel-It: A Sentiment Analyzer for Micro-blogs
</title>
<author confidence="0.988673">
Aditya Joshi&apos; Balamurali A R2 Pushpak Bhattacharyya&apos; Rajat Mohanty 3
</author>
<affiliation confidence="0.89680275">
&apos;Dept. of Computer Science and Engineering, IIT Bombay, Mumbai
2 IITB-Monash Research Academy, IIT Bombay, Mumbai
3 AOL India (R&amp;D), Bangalore
India
</affiliation>
<email confidence="0.993993">
{adityaj,balamurali,pb}@cse.iitb.ac.in r.mohanty@teamaol.com
</email>
<sectionHeader confidence="0.997269" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999734083333333">
Social networking and micro-blogging sites
are stores of opinion-bearing content created
by human users. We describe C-Feel-It, a sys-
tem which can tap opinion content in posts
(called tweets) from the micro-blogging web-
site, Twitter. This web-based system catego-
rizes tweets pertaining to a search string as
positive, negative or objective and gives an ag-
gregate sentiment score that represents a senti-
ment snapshot for a search string. We present
a qualitative evaluation of this system based
on a human-annotated tweet corpus.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999251135135136">
A major contribution of Web 2.0 is the explosive rise
of user-generated content. The content has been a
by-product of a class of Internet-based applications
that allow users to interact with each other on the
web. These applications which are highly accessible
and scalable represent a class of media called social
media. Some of the currently popular social media
sites are Facebook (www.facebook.com), Myspace
(www.myspace.com), Twitter (www.Twitter.com)
etc. User-generated content on the social media rep-
resents the views of the users and hence, may be
opinion-bearing. Sales and marketing arms of busi-
ness organizations can leverage on this information
to know more about their customer base. In addi-
tion, prospective customers of a product/service can
get to know what other users have to say about the
product/service and make an informed decision.
C-Feel-It is a web-based system which
predicts sentiment in micro-blogs on
Twitter (called tweets). (Screencast at:
http://www.youtube.com/user/cfeelit/ ) C-Feel-
It uses a rule-based system to classify tweets as
positive, negative or objective using inputs from
four sentiment-based knowledge repositories. A
weighted-majority voting principle is used to predict
sentiment of a tweet. An overall sentiment score for
the search string is assigned based on the results of
predictions for the tweets fetched. This score which
is represented as a percentage value gives a live
snapshot of the sentiment of users about the topic.
The rest of the paper is organized as follows: Sec-
tion 2 gives background study of Twitter and related
work in the context of sentiment analysis for Twitter.
The system architecture is explained in section 3. A
qualitative evaluation of our system based on anno-
tated data is described in section 4. Section 5 sum-
marizes the paper and points to future work.
</bodyText>
<sectionHeader confidence="0.963404" genericHeader="introduction">
2 Background study
</sectionHeader>
<bodyText confidence="0.97407935">
Twitter is a micro-blogging website and ranks sec-
ond among the present social media websites (Prelo-
vac, 2010). A micro-blog allows users to exchange
small elements of content such as short sentences,
individual pages, or video links (Kaplan and Haen-
lein, 2010). More about Twitter can be found here 1.
In Twitter, a micro-blogging post is called a
tweet which can be upto 140 characters in length.
Since the length is constrained, the language used in
tweets is highly unstructured. Misspellings, slangs,
contractions and abbreviations are commonly used
in tweets. The following example highlights these
problems in a typical tweet:
‘Big brother doing sian massey no favours.
Let her ref. She’s good at it you know#lifesapitch’
We choose Twitter as the data source because
of the sheer quantity of data generated and its fast
reachability across masses. Additionally, Twitter al-
lows information to flow freely and instantaneously
unlike FaceBook or MySpace. These aspects of
</bodyText>
<footnote confidence="0.989442">
1http://support.twitter.com/groups/31-twitter-basics
</footnote>
<page confidence="0.89971">
127
</page>
<note confidence="0.318809">
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 127–132,
Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996374">
Twitter makes it a source for getting a live snapshot
of the things happenings on the web.
In the context of sentiment classification of tweets
Alec et al. (2009a) describes a distant supervision-
based approach for sentiment classification. The
training data for this purpose is created following a
semi-supervised approach that exploits emoticons in
tweets. In their successive work, Alec et al. (2009b)
additionally use hashtags in tweets to create train-
ing data. Topic-dependent clustering is performed
on this data and classifiers corresponding to each are
modeled. This approach is found to perform better
than a single classifier alone.
We believe that the models trained on data cre-
ated using semi-supervised approaches cannot clas-
sify all variants of tweets. Hence, we follow a rule-
based approach for predicting sentiment of a tweet.
An approach like ours provides a generic way of
solving sentiment classification problems in micro-
blogs.
</bodyText>
<sectionHeader confidence="0.982586" genericHeader="method">
3 Architecture
</sectionHeader>
<figureCaption confidence="0.971016666666667">
Figure 1: Overall Architecture
The overall architecture of C-Feel-It is shown in
Figure 1. C-Feel-It is divided into three parts: Tweet
</figureCaption>
<bodyText confidence="0.959565652173913">
Fetcher, Tweet Sentiment Predictor and Tweet
Sentiment Collaborator. All predictions are pos-
itive, negative or objective/neutral. C-Feel-It offers
two implementations of a rule-based sentiment pre-
diction system. We refer to them as version 1 and
2. The two versions differ in the Tweet Sentiment
Predictor module. This section describes different
modules of C-Feel-It and is organized as follows. In
subsections 3.1, 3.2 &amp; 3.3, we describe the three
functional blocks of C-FeeL-It. In subsection 3.4,
we explain how four lexical resources are mapped
to the desired output labels. Finally, subsection 3.5
gives implementation details of C-Feel-It.
Input to C-Feel-It is a search string and a version
number. The versions are described in detail in sub-
section 3.2.
Output given by C-Feel-It is two-level: tweet-wise
prediction and overall prediction. For tweet-wise
prediction, sentiment prediction by each of the re-
sources is returned. On the other hand, overall pre-
diction combines sentiment from all tweets to return
the percentage of positive, negative and objective
content retrieved for the search string.
</bodyText>
<subsectionHeader confidence="0.999793">
3.1 Tweet Fetcher
</subsectionHeader>
<bodyText confidence="0.997459714285714">
Tweet fetcher obtains tweets pertaining to a search
string entered by a user. To do so, we use live feeds
from Twitter using an API 2. The parameters passed
to the API ensure that system receives the latest 50
tweets about the keyword in English. This API re-
turns results in XML format which we parse using a
Java SAX parser.
</bodyText>
<subsectionHeader confidence="0.999748">
3.2 Tweet Sentiment Predictor
</subsectionHeader>
<bodyText confidence="0.954777777777778">
Tweet sentiment predictor predicts sentiment for
a single tweet. The architecture of Tweet Senti-
ment Predictor is shown in Figure 2 and can be di-
vided into three fundamental blocks: Preprocessor,
Emoticon-based Sentiment Predictor, Lexicon-based
Sentiment Predictor (refer Figure 3 &amp; 4). The first
two blocks are same for both the versions of C-Feel-
It. The two versions differ in the working of the
Lexicon-based Sentiment Predictor.
</bodyText>
<subsectionHeader confidence="0.853017">
Preprocessor
</subsectionHeader>
<bodyText confidence="0.978202636363636">
The noisy nature of tweets is a classical challenge
that any system working on tweets needs to en-
counter. Preprocessor deals with obtaining clean
tweets. We do not deploy any spelling correction
module. However, the preprocessor handles exten-
sions and contractions found in tweets as follows.
Handling extensions: Extensions like ‘besssssst’
are common in tweets. However, to look up re-
sources, it is essential that these words are normal-
ized to their dictionary equivalent. We replace con-
secutive occurrences of the same letter (if more than
</bodyText>
<footnote confidence="0.651689">
2http://search.Twitter.com/search.atom
</footnote>
<figure confidence="0.995082133333333">
C-Feel-It
keyword (s)
Tweet
fetcher
Tweet
Sentiment
Predictor
Tweet
Sentiment
Collaborator
Sentiment
score
128
Twee
Tweet
</figure>
<figureCaption confidence="0.999825">
Figure 2: Tweet Sentiment Predictor: Version 1 and 2
</figureCaption>
<bodyText confidence="0.997739923076923">
three occurrences of the same letter) with a single
letter and replace the word.
An important issue here is that extensions are in fact
strong indicators of sentiment. Hence, we replace an
extended word by two occurences of the contracted
word. This gives a higher weight to the extended
word and retains its contribution to the sentiment of
the tweet.
Chat lingo normalization: Words used in
chat/Internet language that are common in tweets are
not present in the lexical resources. We use a dictio-
nary downloaded from http://chat.reichards.net/ . A
chat word is replaced by its dictionary equivalent.
</bodyText>
<subsectionHeader confidence="0.97931">
Emoticon-based Sentiment Predictor
</subsectionHeader>
<bodyText confidence="0.999796333333333">
Emoticons are visual representations of emo-
tions frequently used in the user-generated con-
tent on the Internet. We observe that in most
cases, emoticons pinpoint the sentiment of a
tweet. We use an emoticon mapping from
http://chat.reichards.net/smiley.shtml. An emoticon
is mapped to an output label: positive or negative. A
tweet containing one of these emoticons that can be
mapped to the desired output labels directly. While
we understand that this heuristic does not work in
case of sarcastic tweets, it does provide a benefit in
most cases.
</bodyText>
<subsectionHeader confidence="0.988">
Lexicon-based Sentiment Predictor
</subsectionHeader>
<bodyText confidence="0.99208625">
For a tweet, the Lexicon-based Sentiment Predic-
tor gives a prediction each for four resources. In
addition, it returns one prediction which combines
the four predictions by weighting them on the ba-
</bodyText>
<figureCaption confidence="0.997728">
Figure 3: Lexicon-based Sentiment Predictor: C-Feel-It
Version 1
</figureCaption>
<bodyText confidence="0.946892875">
sis of their accuracies. We remove stop words 3
from the tweet and stem the words using Lovins
stemmer (Lovins, 1968). Negation in tweets is han-
dled by inverting sentiment of words after a negat-
ing word. The words ‘no’, ‘never’, ‘not’ are consid-
ered negating words and a context window of three
words after a negative words is considered for in-
version. The two versions of C-Feel-It vary in their
Lexicon-based Sentiment Predictor. Figure 3 shows
the Lexicon-based Sentiment Predictor for version
1. For each word in the tweet, it gets the predic-
tion from a lexical resource. We use the intuition
that a positive tweet has positive words outnumber-
ing other words, a negative tweet has negative words
outnumbering other words and an objective tweet
has objective words outnumbering other words.
Figure 4 shows the Lexicon-based Sentiment Predic-
tor for version 2. As opposed to the earlier version,
version 2 gets prediction from the lexical resource
for some words in the tweet. This is because certain
parts-of-speech have been found to be better indi-
cators of sentiment (Pang and Lee, 2004). A tweet
is annotated with parts-of-speech tags and the POS
bi-tags (i.e. a pattern of two consecutive POS) are
marked. The words corresponding to a set of opti-
mal POS bi-tags are retained and only these words
used for lookup. The prediction for a tweet uses
majority vote-based approach as for version 1. The
optimal POS bi-tags have been derived experimen-
tally by using top 10% features on information gain-
based-pruning classifier on polarity dataset by (Pang
and Lee, 2005). We used Stanford POS tagger(Tou,
</bodyText>
<footnote confidence="0.95099">
3http://www.ranks.nl/resources/stopwords.html
</footnote>
<page confidence="0.985463">
129
</page>
<figure confidence="0.3279642">
2000) for tagging the tweets.
Note: The dataset we use to find optimal POS
bi-tags consists of movie reviews. We understand
that POS bi-tags hence derived may not be universal
across domains.
</figure>
<figureCaption confidence="0.981011">
Figure 4: Lexicon-based Sentiment Predictor: C-Feel-It
Version 2
</figureCaption>
<subsectionHeader confidence="0.996799">
3.3 Tweet Sentiment Collaborator
</subsectionHeader>
<bodyText confidence="0.999970333333333">
Based on predictions of individual tweets, the Tweet
Sentiment Collaborator gives overall prediction
with respect to a keyword in form of percentage
of positive, negative and objective content. This
is on the basis of predictions by each resource by
weighting them according to their accuracies. These
weights have been assigned to each resource based
on experimental results. For each resource, the
following scores are determined.
</bodyText>
<equation confidence="0.9971565">
posscore[r] =
negscore[r] =
</equation>
<bodyText confidence="0.999841">
We normalize these scores to get the final positive, neg-
ative and objective pertaining to search string r. These
scores are represented in form of percentage.
</bodyText>
<subsectionHeader confidence="0.849792">
3.4 Resources
</subsectionHeader>
<bodyText confidence="0.993085833333333">
Sentiment-based lexical resources annotate
words/concepts with polarity. The completeness
of these resources individually remains a question.
To achieve greater coverage, we use four different
sentiment-based lexical resources for C-Feel-It. They are
described as follows.
</bodyText>
<listItem confidence="0.926212111111111">
1. SentiWordNet (Esuli and Sebastiani, 2006) assigns
three scores to synsets of WordNet: positive score,
negative score and objective score. When a word is
looked up, the label corresponding to maximum of
the three scores is returned. For multiple synsets of
a word, the output label returned by majority of the
synsets becomes the prediction of the resource.
2. Subjectivity lexicon (Wiebe et al., 2004) is a re-
source that annotates words with tags like parts-of-
</listItem>
<bodyText confidence="0.82652225">
speech, prior polarity, magnitude of prior polarity
(weak/strong), etc. The prior polarity can be posi-
tive, negative or neutral. For prediction using this
resource, we use this prior polarity.
</bodyText>
<listItem confidence="0.90699">
3. Inquirer (Stone et al., 1966) is a list of words
marked as positive, negative and neutral. We use
these labels to use Inquirer resource for our predic-
tion.
4. Taboada (Taboada and Grieve, 2004) is a word-list
that gives a count of collocations with positive and
negative seed words. A word closer to a positive
seed word is predicted to be positive and vice versa.
</listItem>
<subsectionHeader confidence="0.972829">
3.5 Implementation Details
</subsectionHeader>
<bodyText confidence="0.990246">
The system is implemented in JSP (JDK 1.6) using Net-
Beans IDE 6.9.1. For the purpose of tweet annotation,
an internal interface was written in PHP 5 with MySQL
5.0.51a-3ubuntu5.7 for storage.
</bodyText>
<figure confidence="0.994118315789474">
Tweet
POS tag
the
tweet
For all w
Retain
words
correspond
ing to
select POS
bi-tags
G
senti
pred
Return
la
corre
g to m
of w
</figure>
<equation confidence="0.9147781">
M
piwpi
i=1
M
niwni
i=1
M
oiwoi
i=1
objscore[r] =
</equation>
<bodyText confidence="0.967795333333333">
where
posscore[r] = Positive score for search string r
negscore[r] = Negative score for search string r
objscore[r] = Objective score for search string r
m = Number of resources used for prediction
pi, ni, oi = Positive,negative &amp; objective count of tweet
predicted respectively using resource i
wpi, wni, ooi = Weights for respective classes derived
for each resource i
</bodyText>
<sectionHeader confidence="0.977088" genericHeader="method">
4 System Analysis
</sectionHeader>
<subsectionHeader confidence="0.995161">
4.1 Evaluation Data
</subsectionHeader>
<bodyText confidence="0.998813875">
For the purpose of evaluation, a total of 7000 tweets
were downloaded by using popular trending topics of
20 domains (like books, movies, electronic gadget, etc.)
as keywords for searching tweets. In order to download
the tweets, we used the API provided by Twitter 4 that
crawls latest tweets pertaining to keywords.
Human annotators assigned to a tweet one out of 4
classes: positive, negative, objective and objective-spam.
</bodyText>
<footnote confidence="0.970888">
4http://search.twitter.com/search.atom?
</footnote>
<page confidence="0.996136">
130
</page>
<bodyText confidence="0.999944">
A tweet is assigned to objective-spam category if it con-
tains promotional links or incoherent text which was pos-
sibly not created by a human user. Apart from these nom-
inal class labels, we also assigned the positive/negative
tweets scores ranging from +2 to -2 with +2 being the
most positive and -2 being the most negative score re-
spectively. If the tweet belongs to the objective category,
a value of zero is assigned as the score.
The spam category has been included in the annotation
as a future goal of modeling a spam detection layer prior
to the sentiment detection. However, the current version
of C-Feel-It does not have a spam detection module and
hence for evaluation purpose, we use only the data be-
longing to classes other than objective-spam.
</bodyText>
<subsectionHeader confidence="0.998599">
4.2 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999945142857143">
In this section, we perform a qualitative evaluation of ac-
tual results returned by C-Feel-It. The errors described
in this section are in addition to the errors due to mis-
spellings and informal language. These erroneous results
have been obtained from both version 1 and 2. They
have been classified into eleven categories and explained
henceforth.
</bodyText>
<subsectionHeader confidence="0.700609">
4.2.1 Sarcastic Tweets
</subsectionHeader>
<bodyText confidence="0.993021222222222">
Tweet: Hoge, Jaws, and Palantonio are brilliant to-
gether talking X’s and O’s on ESPN right now.
Label by C-Feel-It: Positive
Label by human annotator: Negative
The sarcasm in the above tweet lies in the use of a pos-
itive word ’brilliant’ followed by a rather trivial action of
’talking Xs and Os’. The positive word leads to the pre-
diction by C-Feel-It where in fact, it is a negative tweet
for the human annotator.
</bodyText>
<subsectionHeader confidence="0.974626">
4.2.2 Lack of Sense Understanding
</subsectionHeader>
<bodyText confidence="0.998057769230769">
Tweet: If your tooth hurts drink some pain killers and
place a warm/hot tea bag like chamomile on your tooth
and hold it. it will relieve the pain
Label by C-Feel-It: Negative
This tweet is objective in nature. The words ’pain’,
’killers’, etc. in the tweet give an indication to C-Feel-It
that the tweet is negative. This misguided implication is
because of multiple senses of these words (for example,
’pain’ can also be used in the sentence ’symptoms of the
disease are body pain and irritation in the throat’ where
it is non-sentiment-bearing). The lack of understanding
of word senses and being unable to distinguish between
them leads to this error.
</bodyText>
<subsectionHeader confidence="0.968169">
4.2.3 Lack of Entity Specificity
</subsectionHeader>
<bodyText confidence="0.913984">
Tweet: Casablanca and a lunch comprising of rice
and fish: a good sunday
</bodyText>
<sectionHeader confidence="0.488944" genericHeader="method">
Keyword: Casablanca
</sectionHeader>
<bodyText confidence="0.996459222222222">
Label by C-Feel-It: Positive
Label by human annotator: Objective
In the above tweet, the human annotator understood
that though the tweet contains the keyword ’Casablanca’,
it is not Casablanca about which sentiment is expressed.
The system finds a positive word ’good’ and marks the
tweet as positive. This error arises because the system
cannot find out which sentence/parts of sentence is ex-
pressing opinion about the target entity.
</bodyText>
<subsectionHeader confidence="0.993596">
4.2.4 Coverage of Resources
</subsectionHeader>
<bodyText confidence="0.950774272727273">
Tweet: I’m done with this bullshit. You’re the psycho
not me.
Label by SentiWordNet: Negative
Label by Taboada/Inquirer: Objective
Label by human annotator: Negative
On manual verification, it was observed that an entry
for the emotion-bearing word ’bullshit’ is present in Sen-
tiWordNet while Inquirer and Taboada resource do not
have them. This shows that the coverage of the lexical
resource affects the performance of a system and may in-
troduce errors.
</bodyText>
<subsectionHeader confidence="0.970155">
4.2.5 Absence of Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.994404857142857">
Tweet: @user I don’t think I need to guess, but ok,
close encounters of the third kind? Lol
Entity: Close encounters of the third kind
Label by C-Feel-It: Positive
The words comprising the name of the film ’Close en-
counters of the third kind’ are also looked up. Inability to
identify the named entity leads the system into this trap.
</bodyText>
<subsectionHeader confidence="0.969557">
4.2.6 Requirement of World Knowledge
</subsectionHeader>
<bodyText confidence="0.99902575">
Tweet: The soccer world cup boasts an audience twice
that of the Summer Olympics.
Label by C-Feel-It: Negative
To judge the opinion of this tweet, one requires an un-
derstanding of the fact that larger the audience, more fa-
vorable it is for a sports tournament. This world knowl-
edge is important for a system that aims to handle tweets
like these.
</bodyText>
<subsectionHeader confidence="0.515047">
4.2.7 Mixed Emotion Tweets
</subsectionHeader>
<bodyText confidence="0.991743375">
Tweet: oh but that last kiss tells me it’s goodbye, just
like nothing happened last night. but if i had one chance,
i’d do it all over again
Label by C-Feel-It: Positive
The tweet contains emotions of positive as well as neg-
ative variety and it would in fact be difficult for a human
as well to identify the polarity. The mixed nature of the
tweet leads to this error by the system.
</bodyText>
<subsectionHeader confidence="0.921583">
4.2.8 Lack of Context
</subsectionHeader>
<bodyText confidence="0.8882445">
Tweet: I’ll have to say it’s a tie between Little Women
or To kill a Mockingbird
</bodyText>
<page confidence="0.99526">
131
</page>
<bodyText confidence="0.986404571428571">
Label by C-Feel-It: Negative
Label by human user: Positive
The tweet has a sentiment which will possibly be clear
in the context of the conversation. Going by the tweet
alone, while one understands that an comparative opinion
is being expressed, it is not possible to tag it as positive
or negative.
</bodyText>
<subsubsectionHeader confidence="0.38736">
4.2.9 Concatenated Words
</subsubsectionHeader>
<bodyText confidence="0.995849333333333">
Tweet: To Kill a Mockingbird is a #goodbook.
Label by C-Feel-It: Negative
The tweet has a hashtag containing concatenated
words ’goodbook’ which get overlooked as out-of-
dictionary words and hence, are not used for sentiment
prediction. The sentiment of ’good’ is not detected.
</bodyText>
<sectionHeader confidence="0.967535" genericHeader="method">
4.2.10 Interjections
</sectionHeader>
<bodyText confidence="0.912797571428571">
Tweet: Oooh. Apocalypse Now is on bluray now.
Label by C-Feel-It: Objective
Label by human user: Positive
The extended interjection ’Oooh’ is an indicator of
sentiment. Since it does not have a direct prior polar-
ity, it is not present in any of the resources. However, this
interjection is an important carrier of sentiment.
</bodyText>
<sectionHeader confidence="0.776194" genericHeader="method">
4.2.11 Comparatives
</sectionHeader>
<bodyText confidence="0.997406333333333">
Tweet: The more years I spend at Colbert Heights..the
more disgusted I get by the people there. I’m soooo ready
to graduate.
Label by C-Feel-It: Positive
Label by human user: Negative
The comparatives in the sentence expressed by ’..more
disgusted I get..’ have to be handled as a special case
because ’more’ is an intensification of the negative senti-
ment expressed by the word ’disgusted’.
</bodyText>
<sectionHeader confidence="0.998803" genericHeader="method">
5 Summary &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999933736842105">
In this paper, we described a system which categorizes
live tweets related to a keyword as positive, negative
and objective based on the predictions of four sentiment-
based resources. We also presented a qualitative evalua-
tion of our system pointing out the areas of improvement
for the current system.
A sentiment analyzer of this kind can be tuned to take in-
puts from different sources on the internet (for example,
wall posts on facebook). In order to improve the qual-
ity of sentiment prediction, we propose two additions.
Firstly, while we use simple heuristics to handle exten-
sions of words in tweets, a deeper study is required to
decipher the pragmatics involved. Secondly, a spam de-
tection module that eliminates promotional tweets before
performing sentiment detection may be added to the cur-
rent system. Our goal with respect to this system is to de-
ploy it for predicting share market values of firms based
on sentiment on social networks with respect to related
entitites.
</bodyText>
<sectionHeader confidence="0.974855" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.998436333333333">
We thank Akshat Malu and Subhabrata Mukherjee, IIT
Bombay for their assistance during generation of evalua-
tion data.
</bodyText>
<sectionHeader confidence="0.999359" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993050365853659">
Go Alec, Huang Lei, and Bhayani Richa. 2009a. Twit-
ter sentiment classification using distant supervision.
Technical report, Standford University.
Go Alec, Bhayani Richa, Raghunathan Karthik, and
Huang Lei. 2009b. May.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A publicly available lexical resource for opinion
mining. In Proceedings of LREC-06, Genova, Italy.
Andreas M. Kaplan and Michael Haenlein. 2010. The
early bird catches the news: Nine things you should
know about micro-blogging. Business Horizons,
54(2):05 – 113.
Julie B. Lovins. 1968. Development of a Stemming Al-
gorithm. June.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ’04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of ACL-05.
Vladimir Prelovac. 2010. Top social media sites. Web,
May.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal Automatically. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications, pages 158–161, Stan-
ford, US.
2000. Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjec-
tive language. Computional Linguistics, 30:277–308,
September.
</reference>
<page confidence="0.997678">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.408216">
<title confidence="0.999923">C-Feel-It: A Sentiment Analyzer for Micro-blogs</title>
<author confidence="0.904158">A Pushpak Mohanty</author>
<affiliation confidence="0.736844">of Computer Science and Engineering, IIT Bombay, 2IITB-Monash Research Academy, IIT Bombay, 3AOL India (R&amp;D),</affiliation>
<email confidence="0.999592">r.mohanty@teamaol.com</email>
<abstract confidence="0.999555461538462">Social networking and micro-blogging sites are stores of opinion-bearing content created human users. We describe a system which can tap opinion content in posts (called tweets) from the micro-blogging website, Twitter. This web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. We present a qualitative evaluation of this system based on a human-annotated tweet corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Go Alec</author>
<author>Huang Lei</author>
<author>Bhayani Richa</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Standford University.</institution>
<contexts>
<context position="4085" citStr="Alec et al. (2009" startWordPosition="616" endWordPosition="619">hoose Twitter as the data source because of the sheer quantity of data generated and its fast reachability across masses. Additionally, Twitter allows information to flow freely and instantaneously unlike FaceBook or MySpace. These aspects of 1http://support.twitter.com/groups/31-twitter-basics 127 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 127–132, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics Twitter makes it a source for getting a live snapshot of the things happenings on the web. In the context of sentiment classification of tweets Alec et al. (2009a) describes a distant supervisionbased approach for sentiment classification. The training data for this purpose is created following a semi-supervised approach that exploits emoticons in tweets. In their successive work, Alec et al. (2009b) additionally use hashtags in tweets to create training data. Topic-dependent clustering is performed on this data and classifiers corresponding to each are modeled. This approach is found to perform better than a single classifier alone. We believe that the models trained on data created using semi-supervised approaches cannot classify all variants of twe</context>
</contexts>
<marker>Alec, Lei, Richa, 2009</marker>
<rawString>Go Alec, Huang Lei, and Bhayani Richa. 2009a. Twitter sentiment classification using distant supervision. Technical report, Standford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Go Alec</author>
<author>Bhayani Richa</author>
<author>Raghunathan Karthik</author>
<author>Huang Lei</author>
</authors>
<date>2009</date>
<contexts>
<context position="4085" citStr="Alec et al. (2009" startWordPosition="616" endWordPosition="619">hoose Twitter as the data source because of the sheer quantity of data generated and its fast reachability across masses. Additionally, Twitter allows information to flow freely and instantaneously unlike FaceBook or MySpace. These aspects of 1http://support.twitter.com/groups/31-twitter-basics 127 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 127–132, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics Twitter makes it a source for getting a live snapshot of the things happenings on the web. In the context of sentiment classification of tweets Alec et al. (2009a) describes a distant supervisionbased approach for sentiment classification. The training data for this purpose is created following a semi-supervised approach that exploits emoticons in tweets. In their successive work, Alec et al. (2009b) additionally use hashtags in tweets to create training data. Topic-dependent clustering is performed on this data and classifiers corresponding to each are modeled. This approach is found to perform better than a single classifier alone. We believe that the models trained on data created using semi-supervised approaches cannot classify all variants of twe</context>
</contexts>
<marker>Alec, Richa, Karthik, Lei, 2009</marker>
<rawString>Go Alec, Bhayani Richa, Raghunathan Karthik, and Huang Lei. 2009b. May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-06,</booktitle>
<location>Genova, Italy.</location>
<contexts>
<context position="12085" citStr="Esuli and Sebastiani, 2006" startWordPosition="1856" endWordPosition="1859">gned to each resource based on experimental results. For each resource, the following scores are determined. posscore[r] = negscore[r] = We normalize these scores to get the final positive, negative and objective pertaining to search string r. These scores are represented in form of percentage. 3.4 Resources Sentiment-based lexical resources annotate words/concepts with polarity. The completeness of these resources individually remains a question. To achieve greater coverage, we use four different sentiment-based lexical resources for C-Feel-It. They are described as follows. 1. SentiWordNet (Esuli and Sebastiani, 2006) assigns three scores to synsets of WordNet: positive score, negative score and objective score. When a word is looked up, the label corresponding to maximum of the three scores is returned. For multiple synsets of a word, the output label returned by majority of the synsets becomes the prediction of the resource. 2. Subjectivity lexicon (Wiebe et al., 2004) is a resource that annotates words with tags like parts-ofspeech, prior polarity, magnitude of prior polarity (weak/strong), etc. The prior polarity can be positive, negative or neutral. For prediction using this resource, we use this prio</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In Proceedings of LREC-06, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas M Kaplan</author>
<author>Michael Haenlein</author>
</authors>
<title>The early bird catches the news: Nine things you should know about micro-blogging.</title>
<date>2010</date>
<journal>Business Horizons,</journal>
<volume>54</volume>
<issue>2</issue>
<pages>113</pages>
<contexts>
<context position="2996" citStr="Kaplan and Haenlein, 2010" startWordPosition="452" endWordPosition="456">f the paper is organized as follows: Section 2 gives background study of Twitter and related work in the context of sentiment analysis for Twitter. The system architecture is explained in section 3. A qualitative evaluation of our system based on annotated data is described in section 4. Section 5 summarizes the paper and points to future work. 2 Background study Twitter is a micro-blogging website and ranks second among the present social media websites (Prelovac, 2010). A micro-blog allows users to exchange small elements of content such as short sentences, individual pages, or video links (Kaplan and Haenlein, 2010). More about Twitter can be found here 1. In Twitter, a micro-blogging post is called a tweet which can be upto 140 characters in length. Since the length is constrained, the language used in tweets is highly unstructured. Misspellings, slangs, contractions and abbreviations are commonly used in tweets. The following example highlights these problems in a typical tweet: ‘Big brother doing sian massey no favours. Let her ref. She’s good at it you know#lifesapitch’ We choose Twitter as the data source because of the sheer quantity of data generated and its fast reachability across masses. Additi</context>
</contexts>
<marker>Kaplan, Haenlein, 2010</marker>
<rawString>Andreas M. Kaplan and Michael Haenlein. 2010. The early bird catches the news: Nine things you should know about micro-blogging. Business Horizons, 54(2):05 – 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie B Lovins</author>
</authors>
<title>Development of a Stemming Algorithm.</title>
<date>1968</date>
<contexts>
<context position="9308" citStr="Lovins, 1968" startWordPosition="1428" endWordPosition="1429">f these emoticons that can be mapped to the desired output labels directly. While we understand that this heuristic does not work in case of sarcastic tweets, it does provide a benefit in most cases. Lexicon-based Sentiment Predictor For a tweet, the Lexicon-based Sentiment Predictor gives a prediction each for four resources. In addition, it returns one prediction which combines the four predictions by weighting them on the baFigure 3: Lexicon-based Sentiment Predictor: C-Feel-It Version 1 sis of their accuracies. We remove stop words 3 from the tweet and stem the words using Lovins stemmer (Lovins, 1968). Negation in tweets is handled by inverting sentiment of words after a negating word. The words ‘no’, ‘never’, ‘not’ are considered negating words and a context window of three words after a negative words is considered for inversion. The two versions of C-Feel-It vary in their Lexicon-based Sentiment Predictor. Figure 3 shows the Lexicon-based Sentiment Predictor for version 1. For each word in the tweet, it gets the prediction from a lexical resource. We use the intuition that a positive tweet has positive words outnumbering other words, a negative tweet has negative words outnumbering othe</context>
</contexts>
<marker>Lovins, 1968</marker>
<rawString>Julie B. Lovins. 1968. Development of a Stemming Algorithm. June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10282" citStr="Pang and Lee, 2004" startWordPosition="1588" endWordPosition="1591">nt Predictor for version 1. For each word in the tweet, it gets the prediction from a lexical resource. We use the intuition that a positive tweet has positive words outnumbering other words, a negative tweet has negative words outnumbering other words and an objective tweet has objective words outnumbering other words. Figure 4 shows the Lexicon-based Sentiment Predictor for version 2. As opposed to the earlier version, version 2 gets prediction from the lexical resource for some words in the tweet. This is because certain parts-of-speech have been found to be better indicators of sentiment (Pang and Lee, 2004). A tweet is annotated with parts-of-speech tags and the POS bi-tags (i.e. a pattern of two consecutive POS) are marked. The words corresponding to a set of optimal POS bi-tags are retained and only these words used for lookup. The prediction for a tweet uses majority vote-based approach as for version 1. The optimal POS bi-tags have been derived experimentally by using top 10% features on information gainbased-pruning classifier on polarity dataset by (Pang and Lee, 2005). We used Stanford POS tagger(Tou, 3http://www.ranks.nl/resources/stopwords.html 129 2000) for tagging the tweets. Note: Th</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05.</booktitle>
<contexts>
<context position="10759" citStr="Pang and Lee, 2005" startWordPosition="1668" endWordPosition="1671">for some words in the tweet. This is because certain parts-of-speech have been found to be better indicators of sentiment (Pang and Lee, 2004). A tweet is annotated with parts-of-speech tags and the POS bi-tags (i.e. a pattern of two consecutive POS) are marked. The words corresponding to a set of optimal POS bi-tags are retained and only these words used for lookup. The prediction for a tweet uses majority vote-based approach as for version 1. The optimal POS bi-tags have been derived experimentally by using top 10% features on information gainbased-pruning classifier on polarity dataset by (Pang and Lee, 2005). We used Stanford POS tagger(Tou, 3http://www.ranks.nl/resources/stopwords.html 129 2000) for tagging the tweets. Note: The dataset we use to find optimal POS bi-tags consists of movie reviews. We understand that POS bi-tags hence derived may not be universal across domains. Figure 4: Lexicon-based Sentiment Predictor: C-Feel-It Version 2 3.3 Tweet Sentiment Collaborator Based on predictions of individual tweets, the Tweet Sentiment Collaborator gives overall prediction with respect to a keyword in form of percentage of positive, negative and objective content. This is on the basis of predict</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Prelovac</author>
</authors>
<title>Top social media sites.</title>
<date>2010</date>
<location>Web,</location>
<contexts>
<context position="2845" citStr="Prelovac, 2010" startWordPosition="430" endWordPosition="432">s fetched. This score which is represented as a percentage value gives a live snapshot of the sentiment of users about the topic. The rest of the paper is organized as follows: Section 2 gives background study of Twitter and related work in the context of sentiment analysis for Twitter. The system architecture is explained in section 3. A qualitative evaluation of our system based on annotated data is described in section 4. Section 5 summarizes the paper and points to future work. 2 Background study Twitter is a micro-blogging website and ranks second among the present social media websites (Prelovac, 2010). A micro-blog allows users to exchange small elements of content such as short sentences, individual pages, or video links (Kaplan and Haenlein, 2010). More about Twitter can be found here 1. In Twitter, a micro-blogging post is called a tweet which can be upto 140 characters in length. Since the length is constrained, the language used in tweets is highly unstructured. Misspellings, slangs, contractions and abbreviations are commonly used in tweets. The following example highlights these problems in a typical tweet: ‘Big brother doing sian massey no favours. Let her ref. She’s good at it you</context>
</contexts>
<marker>Prelovac, 2010</marker>
<rawString>Vladimir Prelovac. 2010. Top social media sites. Web, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12729" citStr="Stone et al., 1966" startWordPosition="1961" endWordPosition="1964">synsets of WordNet: positive score, negative score and objective score. When a word is looked up, the label corresponding to maximum of the three scores is returned. For multiple synsets of a word, the output label returned by majority of the synsets becomes the prediction of the resource. 2. Subjectivity lexicon (Wiebe et al., 2004) is a resource that annotates words with tags like parts-ofspeech, prior polarity, magnitude of prior polarity (weak/strong), etc. The prior polarity can be positive, negative or neutral. For prediction using this resource, we use this prior polarity. 3. Inquirer (Stone et al., 1966) is a list of words marked as positive, negative and neutral. We use these labels to use Inquirer resource for our prediction. 4. Taboada (Taboada and Grieve, 2004) is a word-list that gives a count of collocations with positive and negative seed words. A word closer to a positive seed word is predicted to be positive and vice versa. 3.5 Implementation Details The system is implemented in JSP (JDK 1.6) using NetBeans IDE 6.9.1. For the purpose of tweet annotation, an internal interface was written in PHP 5 with MySQL 5.0.51a-3ubuntu5.7 for storage. Tweet POS tag the tweet For all w Retain word</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Jack Grieve</author>
</authors>
<title>Analyzing Appraisal Automatically.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications,</booktitle>
<pages>158--161</pages>
<location>Stanford, US.</location>
<contexts>
<context position="12893" citStr="Taboada and Grieve, 2004" startWordPosition="1990" endWordPosition="1993">turned. For multiple synsets of a word, the output label returned by majority of the synsets becomes the prediction of the resource. 2. Subjectivity lexicon (Wiebe et al., 2004) is a resource that annotates words with tags like parts-ofspeech, prior polarity, magnitude of prior polarity (weak/strong), etc. The prior polarity can be positive, negative or neutral. For prediction using this resource, we use this prior polarity. 3. Inquirer (Stone et al., 1966) is a list of words marked as positive, negative and neutral. We use these labels to use Inquirer resource for our prediction. 4. Taboada (Taboada and Grieve, 2004) is a word-list that gives a count of collocations with positive and negative seed words. A word closer to a positive seed word is predicted to be positive and vice versa. 3.5 Implementation Details The system is implemented in JSP (JDK 1.6) using NetBeans IDE 6.9.1. For the purpose of tweet annotation, an internal interface was written in PHP 5 with MySQL 5.0.51a-3ubuntu5.7 for storage. Tweet POS tag the tweet For all w Retain words correspond ing to select POS bi-tags G senti pred Return la corre g to m of w M piwpi i=1 M niwni i=1 M oiwoi i=1 objscore[r] = where posscore[r] = Positive score</context>
</contexts>
<marker>Taboada, Grieve, 2004</marker>
<rawString>Maite Taboada and Jack Grieve. 2004. Analyzing Appraisal Automatically. In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications, pages 158–161, Stanford, US.</rawString>
</citation>
<citation valid="true">
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger,</title>
<date>2000</date>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>2000</marker>
<rawString>2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computional Linguistics,</journal>
<pages>30--277</pages>
<contexts>
<context position="12445" citStr="Wiebe et al., 2004" startWordPosition="1915" endWordPosition="1918">with polarity. The completeness of these resources individually remains a question. To achieve greater coverage, we use four different sentiment-based lexical resources for C-Feel-It. They are described as follows. 1. SentiWordNet (Esuli and Sebastiani, 2006) assigns three scores to synsets of WordNet: positive score, negative score and objective score. When a word is looked up, the label corresponding to maximum of the three scores is returned. For multiple synsets of a word, the output label returned by majority of the synsets becomes the prediction of the resource. 2. Subjectivity lexicon (Wiebe et al., 2004) is a resource that annotates words with tags like parts-ofspeech, prior polarity, magnitude of prior polarity (weak/strong), etc. The prior polarity can be positive, negative or neutral. For prediction using this resource, we use this prior polarity. 3. Inquirer (Stone et al., 1966) is a list of words marked as positive, negative and neutral. We use these labels to use Inquirer resource for our prediction. 4. Taboada (Taboada and Grieve, 2004) is a word-list that gives a count of collocations with positive and negative seed words. A word closer to a positive seed word is predicted to be posit</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computional Linguistics, 30:277–308, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>