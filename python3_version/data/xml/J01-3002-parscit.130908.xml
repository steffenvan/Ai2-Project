<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9997135">
A Statistical Model for Word Discovery
in Transcribed Speech
</title>
<author confidence="0.96601">
Anand Venkataraman*
</author>
<bodyText confidence="0.996945">
A statistical model for segmentation and word discovery in continuous speech is presented. An
incremental unsupervised learning algorithm to infer word boundaries based on this model is
described. Results are also presented of empirical tests showing that the algorithm is competitive
with other models that have been used for similar tasks.
</bodyText>
<sectionHeader confidence="0.974004" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999975423076923">
English speech lacks the acoustic analog of blank spaces that people are accustomed
to seeing between words in written text. Discovering words in continuous spoken
speech is thus an interesting problem and one that has been treated at length in the
literature. The problem of identifying word boundaries is particularly significant in
the parsing of written text in languages that do not explicitly include spaces between
words. In addition, if we assume that children start out with little or no knowledge
of the inventory of words the language possesses identification of word boundaries is
a significant problem in the domain of child language acquisition.&apos; Although speech
lacks explicit demarcation of word boundaries, it is undoubtedly the case that it never-
theless possesses significant other cues for word discovery. However, it is still a matter
of interest to see exactly how much can be achieved without the incorporation of these
other cues; that is, we are interested in the performance of a bare-bones language model.
For example, there is much evidence that stress patterns (Jusczyk, Cutler, and Redanz
1993; Cutler and Carter 1987) and phonotactics of speech (Mattys and Jusczyk 1999)
are of considerable aid in word discovery. But a bare-bones statistical model is still
useful in that it allows us to quantify precise improvements in performance upon the
integration of each specific cue into the model. We present and evaluate one such
statistical model in this paper.&apos;
The main contributions of this study are as follows: First, it demonstrates the ap-
plicability and competitiveness of a conservative, traditional approach for a task for
which nontraditional approaches have been proposed even recently (Brent 1999; Brent
and Cartwright 1996; de Marcken 1995; Elman 1990; Christiansen, Allen, and Seiden-
berg 1998). Second, although the model leads to the development of an algorithm that
learns the lexicon in an unsupervised fashion, results of partial supervision are pre-
sented, showing that its performance is consistent with results from learning theory.
Third, the study extends previous work to higher-order n-grams, specifically up to
</bodyText>
<footnote confidence="0.959292777777778">
* STAR Lab, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025. E-mail:
anand@speech.sri.com
1 See, however, work in Jusczyk and Hohne (1997) and Jusczyk (1997) that presents strong evidence in
favor of a hypothesis that children already have a reasonably powerful and accurate lexicon at their
disposal as early as 9 months of age.
2 Implementations of all the programs discussed in this paper and the input corpus are readily available
upon request from the author. The programs (totaling about 900 lines) have been written in C++ to
compile under Unix/Linux. The author will assist in porting it to other architectures or to versions of
Unix other than Linux or SunOS/Solaris if required.
</footnote>
<note confidence="0.9265375">
© 2001 Association for Computational Linguistics
Computational Linguistics Volume 27, Number 3
</note>
<bodyText confidence="0.999827909090909">
trigrams, and discusses the results in their light. Finally, results of experiments sug-
gested in Brent (1999) regarding different ways of estimating phoneme probabilities
are also reported. Wherever possible, results are averaged over 1000 repetitions of the
experiments, thus removing any potential advantages the algorithm may have had
due to ordering idiosyncrasies within the input corpus.
Section 2 briefly discusses related literature in the field and recent work on the
same topic. The model is described in Section 3. Section 4 describes an unsupervised
learning algorithm based directly on the model developed in Section 3. This section
also describes the data corpus used to test the algorithms and the methods used.
Results are presented and discussed in Section 5. Finally, the findings in this work are
summarized in Section 6.
</bodyText>
<sectionHeader confidence="0.999705" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.999949419354839">
While there exists a reasonable body of literature regarding text segmentation, espe-
cially with respect to languages such as Chinese and Japanese that do not explicitly
include spaces between words, most of the statistically based models and algorithms
tend to fall into the supervised learning category. These require the model to be trained
first on a large corpus of text before it can segment its input.&apos; It is only recently that in-
terest in unsupervised algorithms for text segmentation seems to have gained ground.
A notable exception in this regard is the work by Ando and Lee (1999) which tries
to infer word boundaries from character n-gram statistics of Japanese Kanji strings.
For example, a decision to insert a word boundary between two characters is made
solely based on whether character n-grams adjacent to the proposed boundary are
relatively more frequent than character n-grams that straddle it. This algorithm, how-
ever, is not based on a formal statistical model and is closer in spirit to approaches
based on transitional probability between phonemes or syllables in speech. One such
approach derives from experiments by Saffran, Newport, and Aslin (1996) suggesting
that young children might place word boundaries between two syllables where the
second syllable is surprising given the first. This technique is described and evaluated
in Brent (1999). Other approaches not based on explicit probability models include
those based on information theoretic criteria such as minimum description length
(Brent and Cartwright 1996; de Marcken 1995) and simple recurrent networks (Elman
1990; Christiansen, Allen, and Seidenberg 1998). The maximum likelihood approach
due to Olivier (1968) is probabilistic in the sense that it is geared toward explicitly
calculating the most probable segmentation of each block of input utterances (see also
Batchelder 1997). However, the algorithm involves heuristic steps in periodic purg-
ing of the lexicon and in the creation in the lexicon of new words. Furthermore, this
approach is again not based on a formal statistical model.
Model Based Dynamic Programming, hereafter referred to as MBDP-1 (Brent 1999),
is probably the most recent work that addresses exactly the same issue as that con-
sidered in this paper. Both the approach presented in this paper and Brent&apos;s MBDP-1
are unsupervised approaches based on explicit probability models. Here, we describe
only Brent&apos;s MBDP-1 and direct the interested reader to Brent (1999) for an excellent
review and evaluation of many of the algorithms mentioned above.
</bodyText>
<subsectionHeader confidence="0.998362">
2.1 Brent&apos;s model-based dynamic programming method
</subsectionHeader>
<bodyText confidence="0.989431">
Brent (1999) describes a model-based approach to inferring word boundaries in child-
directed speech. As the name implies, this technique uses dynamic programming to
</bodyText>
<page confidence="0.812462">
3 See, for example, Zimirt and Tseng (1993).
352
</page>
<note confidence="0.661446">
Venkataraman Word Discovery in Transcribed Speech
</note>
<bodyText confidence="0.998861833333333">
infer the best segmentation. It is assumed that the entire input corpus, consisting of a
concatenation of all utterances in sequence, is a single event in probability space and
that the best segmentation of each utterance is implied by the best segmentation of the
corpus itself. The model thus focuses on explicitly calculating probabilities for every
possible segmentation of the entire corpus, and subsequently picking the segmentation
with the maximum probability. More precisely, the model attempts to calculate
</bodyText>
<equation confidence="0.9442195">
P(Cvm) =EEEE P(CvniI n,L,f,$) • P(n, L,f, s)
nLf
</equation>
<bodyText confidence="0.998987608695652">
for each possible segmentation of the input corpus where the left-hand side is the exact
probability of that particular segmentation of the corpus into words dim = W1 W2 • • • wm;
and the sums are over all possible numbers of words n, in the lexicon, all possible
lexicons L, all possible frequencies f, of the individual words in this lexicon and all
possible orders of words s, in the segmentation. In practice, the implementation uses
an incremental approach that computes the best segmentation of the entire corpus
up to step i, where the ith step is the corpus up to and including the ith utterance.
Incremental performance is thus obtained by computing this quantity anew after each
segmentation i -1, assuming however, that segmentations of utterances up to but not
including i are fixed.
There are two problems with this approach. First, the assumption that the entire
corpus of observed speech should be treated as a single event in probability space ap-
pears rather radical. This fact is appreciated even in Brent (1999), which states &amp;quot;From a
cognitive perspective, we know that humans segment each utterance they hear with-
out waiting until the corpus of all utterances they will ever hear becomes available&amp;quot;
(p. 89). Thus, although the incremental algorithm in Brent (1999) is consistent with a
developmental model, the formal statistical model of segmentation is not.
Second, making the assumption that the corpus is a single event in probability
space significantly increases the computational complexity of the incremental algo-
rithm. The approach presented in this paper circumvents these problems through the
use of a conservative statistical model that is directly implementable as an incremental
algorithm. In the following section, we describe the model and how its 2-gram and
3-gram extensions are adapted for implementation.
</bodyText>
<sectionHeader confidence="0.842048" genericHeader="method">
3. Model Description
</sectionHeader>
<bodyText confidence="0.998755666666667">
The language model described here is a fairly standard one. The interested reader is
referred to Jelinek (1997, 57-78), where a detailed exposition can be found. Basically,
we seek
</bodyText>
<equation confidence="0.9886642">
argmax P(W) (1)
argmax P(wi wi, w1-1) (2)
w i=i
= argmin - log P(w I w1,. • • ,w-i) (3)
w i=i
</equation>
<bodyText confidence="0.99576175">
where W = w1,. , wn with wi E L denotes a particular string of n words belonging to
a lexicon L.
The usual n-gram approximation is made by grouping histories wi, ,w1_1 into
equivalence classes, allowing us to collapse contexts into histories at most n-1 words
</bodyText>
<page confidence="0.997279">
353
</page>
<note confidence="0.88186">
Computational Linguistics Volume 27, Number 3
</note>
<bodyText confidence="0.99969975">
backwards (for n-grams). Estimations of the required n-gram probabilities are then
done with relative frequencies using back-off to lower-order n-grams when a higher-
order estimate is not reliable enough (Katz 1987). Back-off is done using the Witten and
Bell (1991) technique, which allocates a probability of NON, S,) to unseen i-grams
at each stage, with the final back-off from unigrams being to an open vocabulary
where word probabilities are calculated as a normalized product of phoneme or letter
probabilities. Here, Ni is the number of distinct i-grams and S, is the sum of their
frequencies. The model can be summarized as follows:
</bodyText>
<equation confidence="0.998203333333333">
S3 C(W/-2/Wi-1,WI)
N3±S3 C(W1-1,W
I i
1\_13._ ,,,
N3 +S3 wl-1)
$2 C(ZO,_1,W,)
_= N2+S2 C(ll&apos;)
N2
N2 ±S2 k 11 p(w.)
C(w)
NiNtSi
Nidi-Si P&gt; (w)
lc;
if C(wi-2, wi-i, wi) &gt; 0
otherwise
P(w Wi-1)
P(w i I wi-i)
P(w1)
if C(wi_i, wi) &gt; 0
otherwise
if C(w) &gt; 0
otherwise
P(w) = (7)
1 - r(#)
</equation>
<bodyText confidence="0.9346575">
where CO denotes the count or frequency function, k, denotes the length of word w,,
excluding the sentinel character #, w, [j] denotes its jth phoneme, and r() denotes the
relative frequency function. The normalization by dividing using 1 - r(#) in Equa-
tion (7) is necessary because otherwise
</bodyText>
<equation confidence="0.9978585">
r(#) 1-1 r(w
1=1
00
P(w)
z=i
l_p(#)
</equation>
<bodyText confidence="0.983841">
Since we estimate P(w[j] ) by r(w[j]), dividing by 1 - r(#) will ensure that Ew P(w) = 1.
</bodyText>
<sectionHeader confidence="0.829015" genericHeader="method">
4. Method
</sectionHeader>
<bodyText confidence="0.999913230769231">
As in Brent (1999), the model described in Section 3 is presented as an incremental
learner. The only knowledge built into the system at start-up is the phoneme table,
with a uniform distribution over all phonemes, including the sentinel phoneme. The
learning algorithm considers each utterance in turn and computes the most proba-
ble segmentation of the utterance using a Viterbi search (Viterbi 1967) implemented
as a dynamic programming algorithm, as described in Section 4.2. The most likely
placement of word boundaries thus computed is committed to before the algorithm
considers the next utterance presented. Committing to a segmentation involves learn-
ing unigram, bigram, and trigram frequencies, as well as phoneme frequencies, from
the inferred words. These are used to update the respective tables.
To account for effects that any specific ordering of input utterances may have on
the segmentations that are output, the performance of the algorithm is averaged over
1000 runs, with each run receiving as input a random permutation of the input corpus.
</bodyText>
<subsectionHeader confidence="0.998595">
4.1 The input corpus
</subsectionHeader>
<bodyText confidence="0.9982035">
The corpus, which is identical to the one used by Brent (1999), consists of orthographic
transcripts made by Bernstein-Rather (1987) from the CHILDES collection (MacWhin-
</bodyText>
<page confidence="0.99853">
354
</page>
<table confidence="0.654053">
Venkataraman Word Discovery in Transcribed Speech
</table>
<tableCaption confidence="0.983994">
Table 1
</tableCaption>
<bodyText confidence="0.6044625">
Twenty randomly chosen utterances from the input corpus with their orthographic transcripts.
See the appendix for a list of the ASCII representations of the phonemes.
</bodyText>
<figure confidence="0.96842512195122">
Phonemic Transcription Orthographic English text
hQ sIli 6v mi
lUk D*z D6 b7 wIT hlz h&amp;t
9 TINk 9 si 6nADR bUk
tu
Dls wAn
r9t WEn De wOk
huz an D6 tEl6fon &amp;lIs
sIt dQn
k&amp;rt yu fid It tu D6 dOgi
D*
du yu si hIm h(
lUk
yu want It In
W* dId It go
&amp;nd WAt # Doz
h9 m6ri
oke Its 6 cIk
y&amp; lUk WAt yu dId
oke
tek It Qt
How silly of me
Look, there&apos;s the boy with his hat
I think I see another book
Two
This one
Right when they walk
Who&apos;s on the telephone, Alice?
Sit down
Can you feed it to the doggie?
There
Do you see him here?
Look
You want it in
Where did it go?
And what are those?
Hi Mary
Okay it&apos;s a chick
Yeah, look what you did
Okay
Take it out
</figure>
<bodyText confidence="0.999674105263158">
ney and Snow 1985). The speakers in this study were nine mothers speaking freely to
their children, whose ages averaged 18 months (range 13-21). Brent and his colleagues
transcribed the corpus phonemically (using the ASCII phonemic representation in the
appendix to this paper) ensuring that the number of subjective judgments in the pro-
nunciation of words was minimized by transcribing every occurrence of the same
word identically. For example, &amp;quot;look&amp;quot;, &amp;quot;drink&amp;quot;, and &amp;quot;doggie&amp;quot; were always transcribed
&amp;quot;lUk&amp;quot;, &amp;quot;drINk&amp;quot;, and &amp;quot;dOgi&amp;quot; regardless of where in the utterance they occurred and
which mother uttered them in what way. Thus transcribed, the corpus consists of a
total of 9790 such utterances and 33,399 words, and includes one space after each
word and one newline after each utterance. For purposes of illustration, Table 1 lists
the first 20 such utterances from a random permutation of this corpus.
It should be noted that the choice of this particular corpus for experimentation is
motivated purely by its use in Brent (1999). As has been pointed out by reviewers of an
earlier version of this paper, the algorithm is equally applicable to plain text in English
or other languages. The main advantage of the CHILDES corpus is that it allows
for ready comparison with results hitherto obtained and reported in the literature.
Indeed, the relative performance of all the algorithms discussed is mostly unchanged
when tested on the 1997 Switchboard telephone speech corpus with disfluency events
removed.
</bodyText>
<subsectionHeader confidence="0.865084">
4.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.9994442">
The dynamic programming algorithm finds the most probable word sequence for each
input utterance by assigning to each segmentation a score equal to its probability, and
committing to the segmentation with the highest score. In practice, the implementation
computes the negative logarithm of this score and thus commits to the segmentation
with the least negative logarithm of its probability. The algorithm for the unigram
</bodyText>
<page confidence="0.993848">
355
</page>
<figure confidence="0.503026333333333">
Computational Linguistics Volume 27, Number 3
BEGIN
Input (by ref) utterance u[0. .n] where u[i] are the characters in it.
</figure>
<equation confidence="0.982088357142857">
bestSegpoint := n;
bestScore := evalWord(u[0..n]);
for i from 0 to n-1; do
subUtterance := copy(u[0..i]);
word := copy(u[i+1..1]);
score := evalUtterance(subUtterance) + evalWord(word):
if (score &lt; bestScore); then
bestScore = score;
bestSegpoint := i;
fi
done
insertWordBoundary(u, bestSegpoint)
return bestScore;
END
</equation>
<figureCaption confidence="0.996754">
Figure 1. Algorithm: evalUtterance
</figureCaption>
<bodyText confidence="0.877401">
Recursive optimization algorithm to find the best segmentation of an input utterance using the
unigram language model described in this paper.
</bodyText>
<equation confidence="0.98397875">
BEGIN
Input (by reference) word w[0. .k] where w[i] are the phonemes in it.
score = 0;
if L.frequency(word) == 0; then {
escape = L.size()/(L.size()+L.sumFrequencies())
P_O = phonemes.relativeFrequency(&apos;#&apos;);
score = -log(escape) -log(P_O/(1-P_0));
for each w[i]; do
score -= log(phonemes.relativeFrequency(w[i]));
done
I else {
P_w = L.frequency(w)/(L.size()+L.sumFrequencies());
score = -log(P_O;
I
return score;
END
</equation>
<figureCaption confidence="0.99622">
Figure 2. Function: evalWord
</figureCaption>
<bodyText confidence="0.9925854">
The function to compute – log P(w) of an input word w. L stands for the lexicon object. If the
word is novel, then it backs off to using a distribution over the phonemes in the word.
language model is presented in recursive form in Figure 1 for readability. The actual
implementation, however, used an iterative version. The algorithm to evaluate the
back-off probability of a word is given in Figure 2. Algorithms for bigram and trigram
language models are straightforward extensions of that given for the unigram model.
Essentially, the algorithm description can be summed up semiformally as follows: For
each input utterance u, we evaluate every possible way of segmenting it as u = u&apos; + w
where u&apos; is a subutterance from the beginning of the original utterance up to some
point within it and w—the lexical difference between u and u&apos;—is treated as a word.
The subutterance u&apos; is itself evaluated recursively using the same algorithm. The base
case for recursion when the algorithm rewinds is obtained when a subutterance cannot
be split further into a smaller component subutterance and word, that is, when its
length is zero. Suppose for example, that a given utterance is abcde, where the letters
represent phonemes. If seg(x) represents the best segmentation of the utterance x and
</bodyText>
<page confidence="0.979652">
356
</page>
<note confidence="0.184137">
Venkataraman Word Discovery in Transcribed Speech
</note>
<equation confidence="0.939858166666667">
word(x) denotes that x is treated as a word, then
{word (abcde)
seg(a) + word (bcde)
seg(abcde) ----- best of seg(ab) + word(cde)
seg(abc) + word(de)
seg(abcd) + word(e)
</equation>
<bodyText confidence="0.999984846153846">
The evalUtterance algorithm in Figure 1 does precisely this. It initially assumes the
entire input utterance to be a word on its own by assuming a single segmentation point
at its right end. It then compares the log probability of this segmentation successively
to the log probabilities of segmenting it into all possible subutterance-word pairs.
The implementation maintains four separate tables internally, one each for uni-
grams, bigrams, and trigrams, and one for phonemes. When the procedure is initially
started, all the internal n-gram tables are empty. Only the phoneme table is popu-
lated with equipossible phonemes. As the program considers each utterance in turn
and commits to its best segmentation according to the evalUtterance algorithm, the
various internal n-gram tables are updated correspondingly. For example, after some
utterance abcde is segmented into a bc de, the unigram table is updated to increment the
frequencies of the three entries a, bc, and de, each by 1, the bigram table to increment
the frequencies of the adjacent bigrams a bc and bc de, and the trigram table to incre-
ment the frequency of the trigram a bc de.&apos; Furthermore, the phoneme table is updated
to increment the frequencies of each of the phonemes in the utterance, including one
sentinel for each word inferred.&apos; Of course, incrementing the frequency of a currently
unknown n-gram is equivalent to creating a new entry for it with frequency 1. Note
that the very first utterance is necessarily segmented as a single word. Since all the
n-gram tables are empty when the algorithm attempts to segment it, all probabilities
are necessarily computed from the level of phonemes up. Thus, the more words in
the segmentation of the first utterance, the more sentinel characters will be included
in the probability calculation, and so the lesser the corresponding segmentation prob-
ability will be. As the program works its way through the corpus, n-grams inferred
correctly by virtue of their relatively greater preponderance compared to noise tend to
dominate their respective n-gram distributions and thus dictate how future utterances
are segmented.
One can easily see that the running time of the program is 0(mn2) in the total
number of utterances (m) and the length of each utterance (n), assuming an efficient
implementation of a hash table allowing nearly constant lookup time is available. Since
individual utterances typically tend to be small, especially in child-directed speech as
evidenced in Table 1, the algorithm practically approximates to a linear time procedure.
A single run over the entire corpus typically completes in under 10 seconds on a 300
MHz i686-based PC running Linux 2.2.5-15.
Although the algorithm is presented as an unsupervised learner, a further exper-
iment to test the responsiveness of each algorithm to training data is also described
here: The procedure involves reserving for training increasing amounts of the input
corpus, from 0% in steps of approximately 1% (100 utterances). During the training
period, the algorithm is presented with the correct segmentation of the input utter-
ance, which it uses to update trigram, bigram, unigram, and phoneme frequencies as
</bodyText>
<footnote confidence="0.90048725">
4 Amending the algorithm to include special markers for the start and end of each utterance was not
found to make a significant difference in its performance.
5 In this context, see also Section 5.2 regarding experiments conducted to investigate different ways of
estimating phoneme probabilities.
</footnote>
<page confidence="0.985418">
357
</page>
<note confidence="0.421695">
Computational Linguistics Volume 27, Number 3
</note>
<bodyText confidence="0.9735905">
required. After the initial training segment of the input corpus has been considered,
subsequent utterances are then processed in the normal way.
</bodyText>
<subsectionHeader confidence="0.999901">
4.3 Scoring
</subsectionHeader>
<bodyText confidence="0.9999786">
In line with the results reported in Brent (1999), three scores are reported — precision,
recall, and lexicon precision. Precision is defined as the proportion of predicted words
that are actually correct. Recall is defined as the proportion of correct words that were
predicted. Lexicon precision is defined as the proportion of words in the predicted
lexicon that are correct. In addition to these, the number of correct and incorrect
words in the predicted lexicon were computed, but they are not graphed here because
lexicon precision is a good indicator of both.
Precision and recall scores were computed incrementally and cumulatively within
scoring blocks, each of which consisted of 100 utterances. These scores were computed
and averaged only for the utterances within each block scored, and thus represent the
performance of the algorithm only on the block scored, occurring in the exact context
among the other scoring blocks. Lexicon scores carried over blocks cumulatively. In
cases where the algorithm used varying amounts of training data, precision, recall, and
lexical precision scores are computed over the entire corpus. All scores are reported
as percentages.
</bodyText>
<sectionHeader confidence="0.998309" genericHeader="method">
5. Results
</sectionHeader>
<bodyText confidence="0.999475827586207">
Figures 3-5 plot the precision, recall, and lexicon precision of the proposed algorithm
for each of the unigram, bigram, and trigram models against the MBDP-1 algorithm.
Although the graphs compare the performance of the algorithm with only one pub-
lished result in the field, comparison with other related approaches is implicitly avail-
able. Brent (1999) reports results of running the algorithms due to Elman (1990) and
Olivier (1968), as well as algorithms based on mutual information and transitional
probability between pairs of phonemes, over exactly the same corpus. These are all
shown to perform significantly worse than Brent&apos;s MBDP-1. The random baseline al-
gorithm in Brent (1999), which consistently performs with under 20% precision and
recall, is not graphed for the same reason. This baseline algorithm offers an important
advantage: It knows the exact number of word boundaries, even though it does not
know their locations. Brent argued that if MBDP-1 performs as well as this random
baseline, then at the very least, it suggests that the algorithm is able to infer informa-
tion equivalent to knowing the right number of word boundaries. A second important
reason for not graphing the algorithms with worse performance was that the scale
on the vertical axis could be expanded significantly by their omission, thus allowing
distinctions between the plotted graphs to be seen more clearly.
The plots originally given in Brent (1999) are over blocks of 500 utterances. How-
ever, because they are a result of running the algorithm on a single corpus, there is no
way of telling whether the performance of the algorithm was influenced by any partic-
ular ordering of the utterances in the corpus. A further undesirable effect of reporting
results of a run on exactly one ordering of the input is that there tends to be too much
variation between the values reported for consecutive scoring blocks. To mitigate both
of these problems, we report averaged results from running the algorithms on 1000
random permutations of the input data. This has the beneficial side effect of allowing
us to plot with higher granularity, since there is much less variation in the precision
and recall scores. They are now clustered much closer to their mean values in each
block, allowing a block size of 100 to be used to score the output. These plots are thus
much more readable than those obtained without such averaging of the results.
</bodyText>
<page confidence="0.97162">
358
</page>
<figure confidence="0.9960224">
Venkataraman Word Discovery in Transcribed Speech
Average Precision %
75
70
65
60
55
50
10 20 30 40 50 60 70 80 90 100
Scoring blocks
</figure>
<figureCaption confidence="0.882772454545455">
Figure 3
Averaged precision. This is a plot of the segmentation precision over 100 utterance blocks
averaged over 1000 runs, each using a random permutation of the input corpus. Precision is
defined as the percentage of identified words that are correct, as measured against the target
data. The horizontal axis represents the number of blocks of data scored, where each block
represents 100 utterances. The plots show the performance of the 1-gram, 2-gram, 3-gram, and
MBDP-1 algorithms. The plot for MBDP-1 is not visible because it coincides almost exactly
with the plot for the 1-gram model. Discussion of this level of similarity is provided in
Section 5.5. The performance of related algorithms due to Elman (1990), Olivier (1968) and
others is implicitly available in this and the following graphs since Brent (1999) demonstrates
that these all perform significantly worse than MBDP-1.
</figureCaption>
<bodyText confidence="0.9998945625">
One may object that the original transcripts carefully preserve the order of ut-
terances directed at children by their mothers, and hence randomly permuting the
corpus would destroy the fidelity of the simulation. However, as we argued, the per-
mutation and averaging does have significant beneficial side effects, and if anything,
it only eliminates from the point of view of the algorithms the important advan-
tage that real children may be given by their mothers through a specific ordering of
the utterances. In any case, we have found no significant difference in performance
between the permuted and unpermuted cases as far as the various algorithms are
concerned.
In this context, it would be interesting to see how the algorithms would fare if
the utterances were in fact favorably ordered, that is, in order of increasing length.
Clearly, this is an important advantage for all the algorithms concerned. Section 5.3
presents the results of an experiment based on a generalization of this situation, where
instead of ordering the utterances favorably, we treat an initial portion of the corpus
as a training component, effectively giving the algorithms free word boundaries after
each word.
</bodyText>
<page confidence="0.980699">
359
</page>
<figure confidence="0.9975615">
Computational Linguistics Volume 27, Number 3
Average Recall %
65
60
55
75
70
50
10 20 30 40 50 60 70 80 90 100
Scoring blocks
</figure>
<figureCaption confidence="0.955609">
Figure 4
</figureCaption>
<table confidence="0.972296625">
Averaged recall over 1000 runs, each using a random permutation of the input corpus.
75
70
65
60
55
50
45
40
35
30
25
Average Lexicon Precision % &apos;1-gram&apos; —
&apos;2-gram&apos; ----
&apos;3-gram&apos;
&apos;MBDP&apos;
</table>
<figure confidence="0.608777">
10 20 30 40 50 60 70 80 90 100
Scoring blocks
</figure>
<figureCaption confidence="0.828422">
Figure 5
</figureCaption>
<bodyText confidence="0.53925">
Averaged lexicon precision over 1000 runs, each using a random permutation of the input
corpus.
</bodyText>
<page confidence="0.976648">
360
</page>
<table confidence="0.280416">
Venkataraman Word Discovery in Transcribed Speech
</table>
<subsectionHeader confidence="0.696788">
5.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999828114285714">
Clearly, the performance of the present model is competitive with MBDP-1 and, as a
consequence, with other algorithms evaluated in Brent (1999). However, note that the
model proposed in this paper has been developed entirely along conventional lines
and has not made the somewhat radical assumption that the entire observed corpus
is a single event in probability space. Assuming that the corpus consists of a single
event, as Brent does, requires the explicit calculation of the probability of the lexicon
in order to calculate the probability of any single segmentation. This calculation is
a nontrivial task since one has to sum over all possible orders of words in L. This
fact is recognized in Brent (1999, Appendix), where the expression for P(L) is derived
as an approximation. One can imagine then that it would be correspondingly more
difficult to extend the language model in Brent (1999) beyond the case of unigrams. In
practical terms, recalculating lexicon probabilities before each segmentation increases
the running time of an implementation of the algorithm. Although all the algorithms
discussed tend to complete within one minute on the reported corpus, MBDP-1&apos;s
running time is quadratic in the number of utterances, while the language models
presented here enable computation in almost linear time. The typical running time
of MBDP-1 on the 9790-utterance corpus averages around 40 seconds per run on a
300 MHz i686 PC while the 1-gram, 2-gram, and 3-gram models average around 7, 10,
and 14 seconds, respectively.
Furthermore, the language models presented in this paper estimate probabilities
as relative frequencies, using commonly used back-off procedures, and so they do
not assume any priors over integers. However, MBDP-1 requires the assumption of
two distributions over integers, one to pick a number for the size of the lexicon and
another to pick a frequency for each word in the lexicon. Each is assumed such that
the probability of a given integer P(i) is given by i2. We have since found some
evidence suggesting that the choice of a particular prior does not offer any significant
advantage over the choice of any other prior. For example, we have tried running
MBDP-1 using P(i) = 2&apos; and still obtained comparable results. It should be noted,
however, that no such subjective prior needs to be chosen in the model presented in
this paper.
The other important difference between MBDP-1 and the present model is that
MBDP-1 assumes a uniform distribution over all possible word orders. That is, in a
corpus that contains nk distinct words such that the frequency in the corpus of the ith
distinct word is given by fk (i), the probability of any one ordering of the words in the
corpus is
</bodyText>
<equation confidence="0.9449795">
117--k ifk(i)!
k!
</equation>
<bodyText confidence="0.999848">
because the number of unique orderings is precisely the reciprocal of the above quan-
tity Brent (1999) mentions that there may well be efficient ways of using n-gram
distributions in the MBDP-1 model. The framework presented in this paper is a for-
mal statement of a model that lends itself to such easy n-gram extendibility using the
back-off scheme proposed here. In fact, the results we present are direct extensions of
the unigram model into bigrams and trigrams.
In this context, an intriguing feature of the results is worth discussing here. Note
that while, with respect to precision, the 3-gram model is better than the 2-gram model,
which in turn is better than the 1-gram model, with respect to recall, their performance
is exactly the opposite. A possible explanation of this behavior is as follows: Since the
3-gram model places greatest emphasis on word triples, which are relatively less fre-
quent than words and word pairs, it has, of all the models, the least evidence available
</bodyText>
<page confidence="0.989554">
361
</page>
<note confidence="0.634791">
Computational Linguistics Volume 27, Number 3
</note>
<bodyText confidence="0.999901882352941">
to infer word boundaries from the observed data. Even though back-off is performed
for bigrams when a trigram is not found, there is a cost associated with such backing
off—this is the extra fractional factor N3/(N3 + S3) in the calculation of the segmen-
tation&apos;s probability. Consequently, the 3-gram model is the most conservative in its
predictions. When it does infer a word boundary, it is likely to be correct. This con-
tributes to its relatively higher precision since precision is a measure of the proportion
of inferred boundaries that were correct. More often than not, however, when the
3-gram model does not have enough evidence to infer words, it simply outputs the
default segmentation, which is a single word (the entire utterance) instead of more
than one incorrectly inferred one. This contributes to its poorer recall since recall is
an indicator of the number of words the model fails to infer. Poorer lexicon precision
is likewise explained. Because the 3-gram model is more conservative, it infers new
words only when there is strong evidence for them. As a result many utterances are
inserted as whole words into its lexicon, thereby contributing to decreased lexicon
precision. The framework presented here thus provides a systematic way of trading
off precision against recall or vice-versa. Models utilizing higher-order n-grams give
better recall at the expense of precision.
</bodyText>
<subsectionHeader confidence="0.999872">
5.2 Estimation of phoneme probabilities
</subsectionHeader>
<bodyText confidence="0.999970692307692">
Brent (1999, 101) suggests that it might be worthwhile to study whether learning
phoneme probabilities from distinct lexical entries yields better results than learning
these probabilities from the input corpus. That is, rather than inflating the probability
of the phoneme &amp;quot;th&amp;quot; in the by the preponderance of the and the-like words in actual
speech, it is better to control it by the number of such distinct words. Presented below
are an initial analysis and experimental results in this regard.
Assume the existence of some function Tx: N N that maps the size, n, of a
corpus C, onto the size of some subset X of C we may define. If this subset X = C,
then Tc is the identity function, and if X = L is the set of distinct words in C, we
have TL(n) = ILI.
Let lx be the average number of phonemes per word in X and let Ea x be the
average number of occurrences of phoneme a per word in X. Then we may estimate
the probability of an arbitrary phoneme a from X as follows.
</bodyText>
<equation confidence="0.9984862">
C(a I X)
P(a I X) =
Ea, c(ai I x)
Ea x (N)
lx (N)
</equation>
<bodyText confidence="0.9613455">
where, as before, C(a I X) is the count function that gives the frequency of phoneme a
in X. If Tx is deterministic, we can then write
</bodyText>
<equation confidence="0.989678">
P(a I X) = Eax (10)
</equation>
<bodyText confidence="0.999350333333333">
Our experiments suggest that Eat — Ea c and that IL — 1c. We are thus led to suspect
that estimates should be roughly the same regardless of whether probabilities are
estimated from L or C. This is indeed borne out by the results we present below. Of
course, this is true only if there exists, as we assumed, some deterministic function TT..
and this may not necessarily be the case. There is, however, some evidence that the
number of distinct words in a corpus can be related to the total number of words in
</bodyText>
<page confidence="0.990564">
362
</page>
<figure confidence="0.991332">
0 10 20 30 40 50 60 70 80 90 100
Number of words in corpus as percentage of total
</figure>
<figureCaption confidence="0.986876">
Figure 6
</figureCaption>
<bodyText confidence="0.960136444444444">
Plot shows the rate of growth of the lexicon with increasing corpus size as percentage of total
size. Actual is the actual number of distinct words in the input corpus. 1-gram, 2-gram, 3-gram
and MBDP plot the size of the lexicon as inferred by each of the algorithms. It is interesting
that the rates of lexicon growth are roughly similar to each other regardless of the algorithm
used to infer words and that they may all potentially be modeled by a function such as Icflq
where N is the corpus size.
the corpus in this way. In Figure 6 the rate of lexicon growth is plotted against the
proportion of the corpus size considered. The values for lexicon size were collected
using the Unix filter
cat $*1 tr &amp;quot; \\ 012 I awk {print [$0]++)? v : -F-Fv;}&apos;
and smoothed by averaging over 100 runs, each on a separate permutation of the
input corpus. The plot strongly suggests that the lexicon size can be approximated by
a deterministic function of the corpus size. It is interesting that the shape of the plot
is roughly the same regardless of the algorithm used to infer words, suggesting that
all the algorithms segment word-like units that share at least some statistical properties
with actual words.
Table 2 summarizes our empirical findings in this regard. For each model—namely,
1-gram, 2-gram, 3-gram and MBDP-1--we test all three of the following possibilities:
</bodyText>
<listItem confidence="0.96606525">
1. Always use a uniform distribution over phonemes.
2. Learn the phoneme distribution from the lexicon.
3. Learn the phoneme distribution from the corpus, that is, from all words,
whether distinct or not.
</listItem>
<figure confidence="0.995975421052632">
&apos;2-gram&apos;
&apos;3-gram&apos;
-&apos;MBDP&apos;
&apos;Actual&apos;
k*scrt(N)
1600
Venkataraman
1800
1400
1200
a)
LI 1000
co
5
a) 800
600
400
200
Word Discovery in Transcribed Speech
</figure>
<page confidence="0.925058">
363
</page>
<note confidence="0.493901">
Computational Linguistics Volume 27, Number 3
</note>
<tableCaption confidence="0.840025">
Table 2
Summary of results from each of the algorithms for each of the following cases:
Lexicon-Phoneme probabilities estimated from the lexicon, Corpus-Phoneme probabilities
estimated from input corpus and Uniform-Phoneme probabilities assumed uniform and
constant.
</tableCaption>
<table confidence="0.9963142">
Precision
1-gram 2-gram 3-gram MBDP
Lexicon 67.7 68.08 68.02 67
Corpus 66.25 66.68 68.2 66.46
Uniform 58.08 64.38 65.64 57.15
Recall
1-gram 2-gram 3-gram MBDP
Lexicon 70.18 68.56 65.07 69.39
Corpus 69.33 68.02 66.06 69.5
Uniform 65.6 69.17 67.23 65.07
Lexicon Precision
1-gram 2-gram 3-gram MBDP
Lexicon 52.85 54.45 47.32 53.56
Corpus 52.1 54.96 49.64 52.36
Uniform 41.46 52.82 50.8 40.89
</table>
<bodyText confidence="0.978828916666667">
The row labeled Lexicon lists scores on the entire corpus from a program that
learned phoneme probabilities from the lexicon. The row labeled Corpus lists scores
from a program that learned these probabilities from the input corpus, and the row
labeled Uniform lists scores from a program that just assumed uniform phoneme prob-
abilities throughout. While the performance is clearly seen to suffer when a uniform
distribution over phonemes is assumed, whether the distribution is estimated from
the lexicon or the corpus does not seem to make any significant difference. These
results lead us to believe that, from an empirical point of view, it really does not
matter whether phoneme probabilities are estimated from the corpus or the lexicon.
Intuitively, however, it seems that the right approach ought to be one that estimates
phoneme frequencies from the corpus data since frequent words ought to have a
greater influence on the phoneme distribution than infrequent ones.
</bodyText>
<subsectionHeader confidence="0.999287">
5.3 Responsiveness to training
</subsectionHeader>
<bodyText confidence="0.999989272727273">
It is interesting to compare the responsiveness of the various algorithms to the effect
of training data. Figures 7-8 plot the results (precision and recall) over the whole
input corpus, that is, blocksize = oo, as a function of the initial proportion of the
corpus reserved for training. This is done by dividing the corpus into two segments,
with an initial training segment being used by the algorithm to learn word, bigram,
trigram, and phoneme probabilities, and the second segment actually being used as
the test data. A consequence of this is that the amount of data available for testing
becomes progressively smaller as the percentage reserved for training grows. So the
significance of the test diminishes correspondingly. We can assume that the plots cease
to be meaningful and interpretable when more than about 75% (about 7500 utterances)
of the corpus is used for training. At 0%, there is no training information for any
</bodyText>
<page confidence="0.994744">
364
</page>
<figure confidence="0.967669666666667">
Word Discovery in Transcribed Speech
Venkataraman
100
90
85
80
75
0 10 20 30 40 50 60 70 80 90
Percentage of input used for training
</figure>
<figureCaption confidence="0.962384">
Figure 7
Responsiveness of the algorithm to training information. The horizontal axis represents the
initial percentage of the data corpus that was used for training the algorithm. This graph
shows the improvement in precision with training size.
</figureCaption>
<figure confidence="0.99287080952381">
100
&apos;1-gram&apos;
&apos;2-gram&apos;
&apos;3-gram&apos;
&apos;MBDP&apos;
95
100
Precision %
98
96 -
&apos;1-gram&apos;
&apos;2-gram&apos;
&apos;3-gram&apos;
_&apos;MBDP&apos;
94 -
92
90
88
86
0 10 20 30 40 50 60 70 80 90 100
Percentage of input used for training
</figure>
<figureCaption confidence="0.987594">
Figure 8
</figureCaption>
<bodyText confidence="0.831828">
Improvement in recall with training size.
</bodyText>
<page confidence="0.993854">
365
</page>
<note confidence="0.59945">
Computational Linguistics Volume 27, Number 3
</note>
<tableCaption confidence="0.990135">
Table 3
</tableCaption>
<table confidence="0.689936571428571">
Errors in the output of a fully trained 3-gram language model. Erroneous segmentations are
shown in boldface.
3-gram output Target
3482 • • • in the doghouse • • • in the dog house
5572 aclock a clock
5836 that&apos;s alright that&apos;s all right
7602 that&apos;s right it&apos;s a hairbrush that&apos;s right it&apos;s a hair brush
</table>
<bodyText confidence="0.999733">
algorithm and the scores are identical to those reported earlier. We increase the amount
of training data in steps of approximately 1% (100 utterances). For each training set
size, the results reported are averaged over 25 runs of the experiment, each over a
separate random permutation of the corpus. As before, this was done both to correct
for ordering idiosyncrasies, and to smooth the graphs to make them easier to interpret.
We interpret Figures 7 and 8 as suggesting that the performance of all algorithms
discussed here can be boosted significantly with even a small amount of training. It
is noteworthy and reassuring to see that, as one would expect from results in compu-
tational learning theory (Haussler 1988), the number of training examples required to
obtain a desired value of precision p, appears to grow with 1/(1 — p). The intriguing
reversal in the performance of the various n-gram models with respect to precision
and recall is again seen here and the explanation for this too is the same as discussed
earlier. We further note, however, that the difference in performance between the dif-
ferent models tends to narrow with increasing training size; that is, as the amount
of evidence available to infer word boundaries increases, the 3-gram model rapidly
catches up with the others in recall and lexicon precision. It is likely, therefore, that
with adequate training data, the 3-gram model might be the most suitable one to use.
The following experiment lends support to this conjecture.
</bodyText>
<subsectionHeader confidence="0.995843">
5.4 Fully trained algorithms
</subsectionHeader>
<bodyText confidence="0.9999755">
The preceding discussion raises the question of what would happen if the percentage
of input used for training was extended to the limit, that is, to 100% of the corpus. This
precise situation was tested in the following way: The entire corpus was concatenated
onto itself; the models were then trained on the first half and tested on the second
half of the corpus thus augmented. Although the unorthodox nature of this procedure
does not allow us to attach all that much significance to the outcome, we nevertheless
find the results interesting enough to warrant some mention, and we thus discuss here
the performance of each of the four algorithms on the test segment of the input corpus
(the second half). As one would expect from the results of the preceding experiments,
the trigram language model outperforms all others. It has a precision and recall of
100% on the test input, except for exactly four utterances. These four utterances are
shown in Table 3, retranscribed into plain English.
Intrigued as to why these errors occurred, we examined the corpus, only to find er-
roneous transcriptions in the input, dog house is transcribed as a single word &amp;quot;dOghQs&amp;quot;
in utterance 614, and as two words elsewhere. Likewise, o&apos;clock is transcribed &amp;quot;6klAk&amp;quot;
in utterance 5917, alright is transcribed &amp;quot;01r9t&amp;quot; in utterance 3937, and hair brush is
transcribed &amp;quot;h*brAS&amp;quot; in utterances 4838 and 7037. Elsewhere in the corpus, these are
transcribed as two words.
The erroneous segmentations in the output of the 2-gram language model are
shown in Table 4. As expected, the effect of reduced history is apparent from an in-
</bodyText>
<page confidence="0.996758">
366
</page>
<table confidence="0.611491">
Venkataraman Word Discovery in Transcribed Speech
</table>
<tableCaption confidence="0.997497">
Table 4
</tableCaption>
<table confidence="0.931203454545455">
Errors in the output of a fully trained 2-gram language model. Erroneous segmentations are
shown in boldface.
2-gram output Target
614 you want the dog house you want the doghouse
3937 thats all right that&apos;s alright
5572 a clock a clock
7327 look a hairbrush look a hair brush
7602 that&apos;s right its a hairbrush that&apos;s right its a hair brush
7681 hairbrush hair brush
7849 it&apos;s called a hairbrush it&apos;s called a hair brush
7853 hairbrush hair brush
</table>
<bodyText confidence="0.998799777777778">
crease in the total number of errors. However, it is interesting to note that while the
3-gram model incorrectly segmented an incorrect transcription (utterance 5836) that&apos;s
all right to produce that&apos;s alright, the 2-gram model incorrectly segmented a correct
transcription (utterance 3937) that&apos;s alright to produce that&apos;s all right. The reason for
this is that the bigram that&apos;s all is encountered relatively frequently in the corpus and
this biases the algorithm toward segmenting the all out of alright when it follows
that&apos;s. However, the 3-gram model is not likewise biased because, having encoun-
tered the exact 3-gram that&apos;s all right earlier, there is no back-off to try bigrams at this
stage.
Similarly, it is interesting that while the 3-gram model incorrectly segments the
incorrectly transcribed dog house into doghouse in utterance 3482, the 2-gram model
incorrectly segments the correctly transcribed doghouse into dog house in utterance 614.
In the trigram model, — log P (house the, dog) = 4.8 and — log P(dog lin, the) = 5.4,
giving a score of 10.2 to the segmentation dog house. However, due to the error in
transcription, the trigram in the doghouse is never encountered in the training data,
although the bigram the doghouse is. Backing off to bigrams, — log P(doghousel the)
is calculated as 8.1. Hence the probability that doghouse is segmented as dog house
is less than the probability that it is a single word. In the 2-gram model, however,
— log P (dog I the)P (house I dog) = 3.7 + 3.2 = 6.9 while — log P (doghouse I the) = 7.5,
whence dog house is the preferred segmentation even though the training data con-
tained instances of all three bigrams. For errors in the output of a 1-gram model, see
Table 5.
The errors in the output of Brent&apos;s fully trained MBDP-1 algorithm are not shown
here because they are identical to those produced by the 1-gram model except for one
utterance. This single difference is the segmentation of utterance 8999, &amp;quot;lItL QtlEts&amp;quot;
(little outlets), which the 1-gram model segmented correctly as &amp;quot;lItL QtlEts&amp;quot;, but MBDP-
1 segmented as &amp;quot;lItL Qt lEts&amp;quot;. In both MBDP-1 and the 1-gram model, all four words,
little, out, lets and outlets, are familiar at the time of segmenting this utterance. MBDP-1
assigns a score of 5.3+5.95 = 11.25 to the segmentation out + lets versus a score of 11.76
to outlets. As a consequence, out + lets is the preferred segmentation. In the 1-gram
language model, the segmentation out + lets scores 5.31 + 5.97 = 11.28, whereas outlets
scores 11.09. Consequently, it selects outlets as the preferred segmentation. The only
thing we could surmise from this was either that this difference must have come about
due to chance (meaning that this may not have occurred if certain parts of the corpus
had been different in any way) or else the interplay between the different elements in
the two models is too subtle to be addressed within the scope of this paper.
</bodyText>
<page confidence="0.994357">
367
</page>
<note confidence="0.602646">
Computational Linguistics Volume 27, Number 3
</note>
<tableCaption confidence="0.8052795">
Table 5
Errors in the output of a fully trained 1-gram language model.
</tableCaption>
<table confidence="0.943566386363636">
1-gram output Target
244 brush Alice &apos;s hair
503 you&apos;re in to distraction • • -
1066 you my trip it
1231 this is little doghouse
1792 stick it on to there
3056 • • • so he doesn&apos;t run in to
3094 • • • to be in the highchair
3098 • • • for this highchair
3125 • • • already • • •
3212 •• • could talk in to it
3230 can heel I down on them
3476 that&apos;s a doghouse
3482 • • • in the doghouse
3923 • • • when it&apos;s nose
3937 that&apos;s all right
4484 its about mealtime s
5328 tell him to way cup
5572 o&apos;clock
5671 where&apos;s my little hairbrush
6315 that&apos;s a nye
6968 okay mommy take seat
7327 look a hairbrush
7602 that&apos;s right its a hairbrush
7607 go along way to find it today
7676 mom put sit
7681 hairbrush
7849 its called a hairbrush
7853 hairbrush
8990 • • • in the highchair
8994 for baby&apos;s a nice highchair
8995 that&apos;s like a highchair that&apos;s right
9168 he has along tongue
9567 you wanna go in the highchair
9594 along red tongue
9674 doghouse
9688 highchair again
9689 • • • the highchari
9708 I have along tongue
brush Alice&apos;s hair
you&apos;re into distraction • • •
you might rip it
this is little dog house
stick it onto there
</table>
<listItem confidence="0.901048666666667">
• • • so he doesn&apos;t run into
• to be in the high chair
• • • for this high chair
all ready • •
• • • could talk into it
can he lie down on them
that&apos;s a dog house
• • • in the dog house
• • • when it snows
</listItem>
<figure confidence="0.939996466666667">
that&apos;s alright
its about meal times
tell him to wake up
a clock
where&apos;s my little hair brush
that&apos;s an i
okay mommy takes it
look a hair brush
that&apos;s right its a hair brush
go a long way to find it today
mom puts it
hair brush
its called a hair brush
hair brush
• • • in the high chair
</figure>
<bodyText confidence="0.903150666666667">
for baby&apos;s a nice high chair
that&apos;s like a high chair that&apos;s right
he has a long tongue
you wanna go in the high chair
a long red tongue
dog house
high chair again
• the high chair
I have a long tongue
</bodyText>
<subsectionHeader confidence="0.889905">
5.5 Similarities between MBDP-1 and the 1-gram Model
</subsectionHeader>
<bodyText confidence="0.9999855">
The similarities between the outputs of MBDP-1 and the 1-gram model are so great
that we suspect they may be capturing essentially the same nuances of the domain.
Although Brent (1999) explicitly states that probabilities are not estimated for words,
it turns out that implementations of MBDP-1 do end up having the same effect as
estimating probabilities from relative frequencies as the 1-gram model does. The relative
probability of a familiar word is given in Equation 22 of Brent (1999) as
</bodyText>
<equation confidence="0.88027">
ft(k) 04) - 2
k fk(ic)
</equation>
<page confidence="0.992087">
368
</page>
<note confidence="0.421384">
Venkataraman Word Discovery in Transcribed Speech
</note>
<bodyText confidence="0.98525293939394">
where k is the total number of words and fk(k) is the frequency at that point in seg-
mentation of the icth word. It effectively approximates to the relative frequency
fk(k)
as fk(k) grows. The 1-gram language model of this paper explicitly claims to use this
specific estimator for the unigram probabilities. From this perspective, both MBDP-
1 and the 1-gram model tend to favor the segmenting out of familiar words that
do not overlap. It is interesting, however, to see exactly how much evidence each
needs before such segmentation is carried out. In this context, the author recalls an
anecdote recounted by a British colleague who, while visiting the USA, noted that the
populace in the vicinity of his institution had grown up thinking that Damn British
was a single word, by virtue of the fact that they had never heard the latter word in
isolation. We test this particular scenario here with both algorithms. The programs are
first presented with the utterance damnbritish. Having no evidence to infer otherwise,
both programs assume that damnbritish is a single word and update their lexicons
accordingly. The interesting question now is exactly how many instances of the word
british in isolation each program would have to see before being able to successfully
segment a subsequent presentation of damnbritish correctly.
Obviously, if the word damn is also unfamiliar, there will never be enough evidence
to segment it out in favor of the familiar word damnbritish. Hence each program is
presented next with two identical utterances, damn. Unless two such utterances are
presented, the estimated probabilities of the familiar words damn and damnbritish will
be equal; and consequently, the probability of any segmentation of damnbritish that
contains the word damn will be less than the probability of damnbritish considered as
a single word.
At this stage, we present each program with increasing numbers of utterances
consisting solely of the word british followed by a repetition of the very first utterance
—damnbritish. We find that MBDP-1 needs to see the word british on its own three
times before having enough evidence to disabuse itself of the notion that damnbritish
is a single word. In comparison, the 1-gram model is more skeptical. It needs to see
the word british on its own seven times before committing to the right segmentation.
To illustrate the inherent simplicity of the model presented here, we can show that it is
easy to predict this number analytically from the 1-gram model. Let x be the number
of instances of british required. Then using the discounting scheme described, we have
</bodyText>
<equation confidence="0.9286788">
P(damnbritish) -= 1/(x + 6)
P (damn) -= 2/(x +6) and
P(british) --= x/(x + 6)
We seek an x for which P(damn)P(british) &gt; P(damnbritish). Thus, we get
2x / (x + 6)2 &gt; 1 / (x + 6) x &gt; 6
</equation>
<bodyText confidence="0.997797833333333">
The actual scores for MBDP-1 when presented with damnbritish for a second time are
– log P(damnbritish) =- 2.8 and – log P(D&amp;m) – log P(brItIS) = 1.8 + 0.9 = 2.7. For
the 1-gram model, – log P(damnbritish) = 2.6 and – log P(D&amp;m) – log P(brItIS) =
1.9 + 0.6 = 2.5. Note, however, that skepticism in this regard is not always a bad
attribute. It is desirable for the model to be skeptical in inferring new words because
a badly inferred word will adversely influence future segmentation accuracy.
</bodyText>
<page confidence="0.996158">
369
</page>
<note confidence="0.764766">
Computational Linguistics Volume 27, Number 3
</note>
<sectionHeader confidence="0.961711" genericHeader="method">
6. Summary
</sectionHeader>
<bodyText confidence="0.999983227272727">
In summary, we have presented a formal model of word discovery in continuous
speech. The main advantages of this model over that of Brent (1999) are: First, the
present model has been developed entirely by direct application of standard tech-
niques and procedures in speech processing. Second, it makes few assumptions about
the nature of the domain and remains conservative as far as possible in its develop-
ment. Finally, the model can be easily extended to incorporate more historical detail.
This is clearly evidenced by the extension of the unigram model to handle bigrams
and trigrams. Empirical results from experiments suggest that the algorithm performs
competitively with alternative unsupervised algorithms proposed for inferring words
from continous speech. We have also carried out and reported results from experiments
to determine whether particular ways of estimating phoneme (or letter) probabilities
may be more suitable than others.
Although the algorithm is originally presented as an unsupervised learner, we
have shown the effect that training data has on its performance. It appears that the
3-gram model is the most responsive to training information with regard to segmen-
tation precision, obviously by virtue of the fact that it keeps more knowledge from the
utterances presented. Indeed, we see that a fully trained 3-gram model performs with
100% accuracy on the test set. Admittedly, the test set in this case was identical to the
training set, but we should keep in mind that we were keeping only limited history—
namely 3-grams—and a significant number of utterances in the input corpus (4023
utterances) were four words or more in length. Thus, it is not completely insignificant
that the algorithm was able to perform this well.
</bodyText>
<sectionHeader confidence="0.977257" genericHeader="method">
7. Future work
</sectionHeader>
<bodyText confidence="0.9998335">
We are presently working on the incorporation into the model of more complex
phoneme distributions, such as the biphone and triphone distributions. Some pre-
liminary results we have obtained in this regard appear to be encouraging.
With regard to estimation of word probabilities, a fruitful avenue we are exploring
involves modification of the model to address the sparse data problem using interpo-
lation such that
</bodyText>
<equation confidence="0.925146">
P(Wi Wi-2, W1-1) = A3f (Wi Wi-2, Wi-1) A2f(Wi Wi-1) Alf(w1)
</equation>
<bodyText confidence="0.999984636363636">
where the positive coefficients satisfy Ai + A2 + A3 = 1 and can be derived so as to
maximize P(W).
Taking the lead from Brent (1999), attempts to model more complex distributions
for unigrams such as those based on template grammars, as well as the systematic in-
corporation of prosodic, stress, and phonotactic constraint information into the model,
are both subjects of current interest. Unpublished results already obtained suggest
that biasing the segmentation such that every word must have at least one vowel in
it dramatically increases segmentation precision from 67.7% to 81.8%, and imposing
a constraint that words can begin or end only with permitted clusters of consonants
increases precision to 80.65%. We are planning experiments to investigate models in
which these properties can be learned in the same way as n-grams.
</bodyText>
<sectionHeader confidence="0.502827" genericHeader="method">
Appendix: Inventory of Phonemes
</sectionHeader>
<bodyText confidence="0.9985445">
The following tables list the ASCII representations of the phonemes used to transcribe
the corpus into a form suitable for processing by the algorithms.
</bodyText>
<page confidence="0.984199">
370
</page>
<figure confidence="0.984227451612903">
Venkataraman Word Discovery in Transcribed Speech
Consonants Vowels Vowel + r
ASCII Example ASCII Example ASCII Example
p pan I bit
b ban E bet
m man &amp; bat
t tan A but
d dam a hot
n nap 0 law
k can U put
g go 6 her
N sing i beet
f fan e bait
✓ van u boot
T thin o boat
D than 9 buy
s sand Q bout
z zap 7 boy
S ship
Z pleasure
h hat
c chip
G gel
1 lap
✓ rap
y yet
w wall
W when
L bottle
M rhythm
— button
</figure>
<sectionHeader confidence="0.891562" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999849875">
The author thanks Michael Brent for
stimulating his interest in the area. Thanks
are also due to Koryn Grant for verifying
the results presented here. Eleanor Olds
Batchelder and Andreas Stolcke gave many
constructive comments and useful pointers
during the preparation of a revised version
of this paper. The &amp;quot;Damn British&amp;quot; anecdote
is due to Robert Linggard. Judith Lee at SRI
edited the manuscript to remove many
typographical and stylistic errors. In
addition, the author is very grateful to
anonymous reviewers of an initial version,
particularly &amp;quot;B&amp;quot; and &amp;quot;D&amp;quot;, whose
encouragement and constructive criticism
helped significantly.
</bodyText>
<sectionHeader confidence="0.983302" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.980720325581395">
Ando, R. K. and Lillian Lee. 1999.
Unsupervised statistical segmentation of
Japanese Kanji strings. Technical Report
TR99-1756, Cornell University, Ithaca, NY.
Batchelder, Eleanor Olds. 1997.
Computational evidence for the use of
frequency information in discovery of the
infant&apos;s first lexicon. Unpublished Ph.D.
3 bird
• butter
• arm
`)/0 horn
air
ear
lure
dissertation, City University of New York,
New York, NY.
Bernstein-Ratner, N. 1987. The phonology of
parent child speech. In K. Nelson and A.
van Kleeck, editors, Children&apos;s Language,
volume 6. Erlbaum, Hillsdale, NJ.
Brent, Michael R. 1999. An efficient
probabilistically sound algorithm for
segmentation and word discovery.
Machine Learning, 34:71-105.
Brent, Michael R. and Timothy A.
Cartwright. 1996. Distributional regularity
and phonotactics are useful for
segmentation. Cognition, 61:93-125.
Christiansen, M. H., J. Allen, and M.
Seidenberg. 1998. Learning to segment
speech using multiple cues: A
connectionist model. Language and
Cognitive Processes, 13:221-268.
Cutler, Anne and D. M. Carter. 1987.
Predominance of strong initial syllables in
the English vocabulary. Computer Speech
and Language, 2:133-142.
de Marcken, C. 1995. Unsupervised
acquisition of a lexicon from continuous
speech. Technical Report Al Memo No.
1558, Massachusetts Institute of
Technology, Cambridge, MA.
</reference>
<page confidence="0.961251">
371
</page>
<note confidence="0.360704">
Computational Linguistics Volume 27, Number 3
</note>
<reference confidence="0.999521867924528">
Elman, J. L. 1990. Finding structure in time.
Cognitive Science, 14:179-211.
Haussler, David. 1988. Quantifying
inductive bias: Al learning algorithms
and Valiant&apos;s learning framework.
Artificial Intelligence, 36:177-221.
Jelinek, F. 1997. Statistical Methods for Speech
Recognition. MIT Press, Cambridge, MA.
Jusczyk, Peter W. 1997. The Discovery of
Spoken Language. MIT Press, Cambridge,
MA.
Jusczyk, Peter W., Anne Cutler, and N.
Redanz. 1993. Preference for predominant
stress patterns of English words. Child
Development, 64:675-687.
Jusczyk, Peter W. and E. A. Hohne. 1997.
Infants&apos; memory for spoken words.
Science, 27:1984-1986.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing,
ASSP-35(3):400401.
MacVVhinney, Brian and C. Snow. 1985. The
child language data exchange system.
Journal of Child Language, 12:271-296.
Mattys, Sven L. and Peter W. Jusczyk. 1999.
Phonotactic and prosodic effects on word
segmentation in infants. Cognitive
Psychology, 38:465-494.
Olivier, D. C. 1968. Stochastic grammars and
language acquisition mechanisms.
Unpublished Ph.D. dissertation, Harvard
University, Cambridge, MA.
Saffran, Jennifer R., E. L. Newport, and
R. N. Aslin. 1996. Word segmentation:
The role of distributional cues. Journal of
Memory and Language, 35:606-621.
Viterbi, Andrew J. 1967. Error bounds for
convolutional codes and an
asymptotically optimal decoding
algorithm. IEEE Transactions on Information
Theory, IT-13:260-269.
Witten, Ian H. and Timothy C. Bell. 1991.
The zero-frequency problem: Estimating
the probabilities of novel events in
adaptive text compression. IEEE
Transactions on Information Theory,
37(4):1085-1091.
Zimin, W. and G. Tseng. 1993. Chinese text
segmentation for text retrieval problems
and achievements. JASIS, 44(9):532-542.
</reference>
<page confidence="0.99835">
372
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.744661">
<title confidence="0.998377">A Statistical Model for Word Discovery in Transcribed Speech</title>
<author confidence="0.858425">Anand Venkataraman</author>
<abstract confidence="0.96171275">A statistical model for segmentation and word discovery in continuous speech is presented. An incremental unsupervised learning algorithm to infer word boundaries based on this model is described. Results are also presented of empirical tests showing that the algorithm is competitive with other models that have been used for similar tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R K Ando</author>
<author>Lillian Lee</author>
</authors>
<title>Unsupervised statistical segmentation of Japanese Kanji strings.</title>
<date>1999</date>
<tech>Technical Report TR99-1756,</tech>
<institution>Cornell University,</institution>
<location>Ithaca, NY.</location>
<contexts>
<context position="4845" citStr="Ando and Lee (1999)" startWordPosition="754" endWordPosition="757">n Section 6. 2. Related Work While there exists a reasonable body of literature regarding text segmentation, especially with respect to languages such as Chinese and Japanese that do not explicitly include spaces between words, most of the statistically based models and algorithms tend to fall into the supervised learning category. These require the model to be trained first on a large corpus of text before it can segment its input.&apos; It is only recently that interest in unsupervised algorithms for text segmentation seems to have gained ground. A notable exception in this regard is the work by Ando and Lee (1999) which tries to infer word boundaries from character n-gram statistics of Japanese Kanji strings. For example, a decision to insert a word boundary between two characters is made solely based on whether character n-grams adjacent to the proposed boundary are relatively more frequent than character n-grams that straddle it. This algorithm, however, is not based on a formal statistical model and is closer in spirit to approaches based on transitional probability between phonemes or syllables in speech. One such approach derives from experiments by Saffran, Newport, and Aslin (1996) suggesting th</context>
</contexts>
<marker>Ando, Lee, 1999</marker>
<rawString>Ando, R. K. and Lillian Lee. 1999. Unsupervised statistical segmentation of Japanese Kanji strings. Technical Report TR99-1756, Cornell University, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Olds Batchelder</author>
</authors>
<title>Computational evidence for the use of frequency information in discovery of the infant&apos;s first lexicon.</title>
<date>1997</date>
<note>Unpublished Ph.D. 3 bird • butter • arm</note>
<contexts>
<context position="6130" citStr="Batchelder 1997" startWordPosition="947" endWordPosition="948">here the second syllable is surprising given the first. This technique is described and evaluated in Brent (1999). Other approaches not based on explicit probability models include those based on information theoretic criteria such as minimum description length (Brent and Cartwright 1996; de Marcken 1995) and simple recurrent networks (Elman 1990; Christiansen, Allen, and Seidenberg 1998). The maximum likelihood approach due to Olivier (1968) is probabilistic in the sense that it is geared toward explicitly calculating the most probable segmentation of each block of input utterances (see also Batchelder 1997). However, the algorithm involves heuristic steps in periodic purging of the lexicon and in the creation in the lexicon of new words. Furthermore, this approach is again not based on a formal statistical model. Model Based Dynamic Programming, hereafter referred to as MBDP-1 (Brent 1999), is probably the most recent work that addresses exactly the same issue as that considered in this paper. Both the approach presented in this paper and Brent&apos;s MBDP-1 are unsupervised approaches based on explicit probability models. Here, we describe only Brent&apos;s MBDP-1 and direct the interested reader to Bren</context>
</contexts>
<marker>Batchelder, 1997</marker>
<rawString>Batchelder, Eleanor Olds. 1997. Computational evidence for the use of frequency information in discovery of the infant&apos;s first lexicon. Unpublished Ph.D. 3 bird • butter • arm</rawString>
</citation>
<citation valid="false">
<title>0 horn air ear lure</title>
<marker></marker>
<rawString>`)/0 horn air ear lure</rawString>
</citation>
<citation valid="false">
<authors>
<author>dissertation</author>
</authors>
<institution>City University of New</institution>
<location>York, New York, NY.</location>
<marker>dissertation, </marker>
<rawString>dissertation, City University of New York, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bernstein-Ratner</author>
</authors>
<title>The phonology of parent child speech. In</title>
<date>1987</date>
<booktitle>Children&apos;s Language,</booktitle>
<volume>6</volume>
<editor>K. Nelson and A. van Kleeck, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>Bernstein-Ratner, N. 1987. The phonology of parent child speech. In K. Nelson and A. van Kleeck, editors, Children&apos;s Language, volume 6. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>An efficient probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="2173" citStr="Brent 1999" startWordPosition="335" endWordPosition="336">Cutler and Carter 1987) and phonotactics of speech (Mattys and Jusczyk 1999) are of considerable aid in word discovery. But a bare-bones statistical model is still useful in that it allows us to quantify precise improvements in performance upon the integration of each specific cue into the model. We present and evaluate one such statistical model in this paper.&apos; The main contributions of this study are as follows: First, it demonstrates the applicability and competitiveness of a conservative, traditional approach for a task for which nontraditional approaches have been proposed even recently (Brent 1999; Brent and Cartwright 1996; de Marcken 1995; Elman 1990; Christiansen, Allen, and Seidenberg 1998). Second, although the model leads to the development of an algorithm that learns the lexicon in an unsupervised fashion, results of partial supervision are presented, showing that its performance is consistent with results from learning theory. Third, the study extends previous work to higher-order n-grams, specifically up to * STAR Lab, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025. E-mail: anand@speech.sri.com 1 See, however, work in Jusczyk and Hohne (1997) and Jusczyk (1997) t</context>
<context position="3507" citStr="Brent (1999)" startWordPosition="542" endWordPosition="543">n at their disposal as early as 9 months of age. 2 Implementations of all the programs discussed in this paper and the input corpus are readily available upon request from the author. The programs (totaling about 900 lines) have been written in C++ to compile under Unix/Linux. The author will assist in porting it to other architectures or to versions of Unix other than Linux or SunOS/Solaris if required. © 2001 Association for Computational Linguistics Computational Linguistics Volume 27, Number 3 trigrams, and discusses the results in their light. Finally, results of experiments suggested in Brent (1999) regarding different ways of estimating phoneme probabilities are also reported. Wherever possible, results are averaged over 1000 repetitions of the experiments, thus removing any potential advantages the algorithm may have had due to ordering idiosyncrasies within the input corpus. Section 2 briefly discusses related literature in the field and recent work on the same topic. The model is described in Section 3. Section 4 describes an unsupervised learning algorithm based directly on the model developed in Section 3. This section also describes the data corpus used to test the algorithms and </context>
<context position="5627" citStr="Brent (1999)" startWordPosition="875" endWordPosition="876"> is made solely based on whether character n-grams adjacent to the proposed boundary are relatively more frequent than character n-grams that straddle it. This algorithm, however, is not based on a formal statistical model and is closer in spirit to approaches based on transitional probability between phonemes or syllables in speech. One such approach derives from experiments by Saffran, Newport, and Aslin (1996) suggesting that young children might place word boundaries between two syllables where the second syllable is surprising given the first. This technique is described and evaluated in Brent (1999). Other approaches not based on explicit probability models include those based on information theoretic criteria such as minimum description length (Brent and Cartwright 1996; de Marcken 1995) and simple recurrent networks (Elman 1990; Christiansen, Allen, and Seidenberg 1998). The maximum likelihood approach due to Olivier (1968) is probabilistic in the sense that it is geared toward explicitly calculating the most probable segmentation of each block of input utterances (see also Batchelder 1997). However, the algorithm involves heuristic steps in periodic purging of the lexicon and in the c</context>
<context position="6884" citStr="Brent (1999)" startWordPosition="1065" endWordPosition="1066">ore, this approach is again not based on a formal statistical model. Model Based Dynamic Programming, hereafter referred to as MBDP-1 (Brent 1999), is probably the most recent work that addresses exactly the same issue as that considered in this paper. Both the approach presented in this paper and Brent&apos;s MBDP-1 are unsupervised approaches based on explicit probability models. Here, we describe only Brent&apos;s MBDP-1 and direct the interested reader to Brent (1999) for an excellent review and evaluation of many of the algorithms mentioned above. 2.1 Brent&apos;s model-based dynamic programming method Brent (1999) describes a model-based approach to inferring word boundaries in childdirected speech. As the name implies, this technique uses dynamic programming to 3 See, for example, Zimirt and Tseng (1993). 352 Venkataraman Word Discovery in Transcribed Speech infer the best segmentation. It is assumed that the entire input corpus, consisting of a concatenation of all utterances in sequence, is a single event in probability space and that the best segmentation of each utterance is implied by the best segmentation of the corpus itself. The model thus focuses on explicitly calculating probabilities for ev</context>
<context position="8730" citStr="Brent (1999)" startWordPosition="1364" endWordPosition="1365"> implementation uses an incremental approach that computes the best segmentation of the entire corpus up to step i, where the ith step is the corpus up to and including the ith utterance. Incremental performance is thus obtained by computing this quantity anew after each segmentation i -1, assuming however, that segmentations of utterances up to but not including i are fixed. There are two problems with this approach. First, the assumption that the entire corpus of observed speech should be treated as a single event in probability space appears rather radical. This fact is appreciated even in Brent (1999), which states &amp;quot;From a cognitive perspective, we know that humans segment each utterance they hear without waiting until the corpus of all utterances they will ever hear becomes available&amp;quot; (p. 89). Thus, although the incremental algorithm in Brent (1999) is consistent with a developmental model, the formal statistical model of segmentation is not. Second, making the assumption that the corpus is a single event in probability space significantly increases the computational complexity of the incremental algorithm. The approach presented in this paper circumvents these problems through the use of</context>
<context position="11521" citStr="Brent (1999)" startWordPosition="1838" endWordPosition="1839">w) NiNtSi Nidi-Si P&gt; (w) lc; if C(wi-2, wi-i, wi) &gt; 0 otherwise P(w Wi-1) P(w i I wi-i) P(w1) if C(wi_i, wi) &gt; 0 otherwise if C(w) &gt; 0 otherwise P(w) = (7) 1 - r(#) where CO denotes the count or frequency function, k, denotes the length of word w,, excluding the sentinel character #, w, [j] denotes its jth phoneme, and r() denotes the relative frequency function. The normalization by dividing using 1 - r(#) in Equation (7) is necessary because otherwise r(#) 1-1 r(w 1=1 00 P(w) z=i l_p(#) Since we estimate P(w[j] ) by r(w[j]), dividing by 1 - r(#) will ensure that Ew P(w) = 1. 4. Method As in Brent (1999), the model described in Section 3 is presented as an incremental learner. The only knowledge built into the system at start-up is the phoneme table, with a uniform distribution over all phonemes, including the sentinel phoneme. The learning algorithm considers each utterance in turn and computes the most probable segmentation of the utterance using a Viterbi search (Viterbi 1967) implemented as a dynamic programming algorithm, as described in Section 4.2. The most likely placement of word boundaries thus computed is committed to before the algorithm considers the next utterance presented. Com</context>
<context position="14662" citStr="Brent (1999)" startWordPosition="2377" endWordPosition="2378"> identically. For example, &amp;quot;look&amp;quot;, &amp;quot;drink&amp;quot;, and &amp;quot;doggie&amp;quot; were always transcribed &amp;quot;lUk&amp;quot;, &amp;quot;drINk&amp;quot;, and &amp;quot;dOgi&amp;quot; regardless of where in the utterance they occurred and which mother uttered them in what way. Thus transcribed, the corpus consists of a total of 9790 such utterances and 33,399 words, and includes one space after each word and one newline after each utterance. For purposes of illustration, Table 1 lists the first 20 such utterances from a random permutation of this corpus. It should be noted that the choice of this particular corpus for experimentation is motivated purely by its use in Brent (1999). As has been pointed out by reviewers of an earlier version of this paper, the algorithm is equally applicable to plain text in English or other languages. The main advantage of the CHILDES corpus is that it allows for ready comparison with results hitherto obtained and reported in the literature. Indeed, the relative performance of all the algorithms discussed is mostly unchanged when tested on the 1997 Switchboard telephone speech corpus with disfluency events removed. 4.2 Algorithm The dynamic programming algorithm finds the most probable word sequence for each input utterance by assigning</context>
<context position="21910" citStr="Brent (1999)" startWordPosition="3512" endWordPosition="3513">ate trigram, bigram, unigram, and phoneme frequencies as 4 Amending the algorithm to include special markers for the start and end of each utterance was not found to make a significant difference in its performance. 5 In this context, see also Section 5.2 regarding experiments conducted to investigate different ways of estimating phoneme probabilities. 357 Computational Linguistics Volume 27, Number 3 required. After the initial training segment of the input corpus has been considered, subsequent utterances are then processed in the normal way. 4.3 Scoring In line with the results reported in Brent (1999), three scores are reported — precision, recall, and lexicon precision. Precision is defined as the proportion of predicted words that are actually correct. Recall is defined as the proportion of correct words that were predicted. Lexicon precision is defined as the proportion of words in the predicted lexicon that are correct. In addition to these, the number of correct and incorrect words in the predicted lexicon were computed, but they are not graphed here because lexicon precision is a good indicator of both. Precision and recall scores were computed incrementally and cumulatively within s</context>
<context position="23406" citStr="Brent (1999)" startWordPosition="3743" endWordPosition="3744">cks. Lexicon scores carried over blocks cumulatively. In cases where the algorithm used varying amounts of training data, precision, recall, and lexical precision scores are computed over the entire corpus. All scores are reported as percentages. 5. Results Figures 3-5 plot the precision, recall, and lexicon precision of the proposed algorithm for each of the unigram, bigram, and trigram models against the MBDP-1 algorithm. Although the graphs compare the performance of the algorithm with only one published result in the field, comparison with other related approaches is implicitly available. Brent (1999) reports results of running the algorithms due to Elman (1990) and Olivier (1968), as well as algorithms based on mutual information and transitional probability between pairs of phonemes, over exactly the same corpus. These are all shown to perform significantly worse than Brent&apos;s MBDP-1. The random baseline algorithm in Brent (1999), which consistently performs with under 20% precision and recall, is not graphed for the same reason. This baseline algorithm offers an important advantage: It knows the exact number of word boundaries, even though it does not know their locations. Brent argued t</context>
<context position="26443" citStr="Brent (1999)" startWordPosition="4247" endWordPosition="4248">ntage of identified words that are correct, as measured against the target data. The horizontal axis represents the number of blocks of data scored, where each block represents 100 utterances. The plots show the performance of the 1-gram, 2-gram, 3-gram, and MBDP-1 algorithms. The plot for MBDP-1 is not visible because it coincides almost exactly with the plot for the 1-gram model. Discussion of this level of similarity is provided in Section 5.5. The performance of related algorithms due to Elman (1990), Olivier (1968) and others is implicitly available in this and the following graphs since Brent (1999) demonstrates that these all perform significantly worse than MBDP-1. One may object that the original transcripts carefully preserve the order of utterances directed at children by their mothers, and hence randomly permuting the corpus would destroy the fidelity of the simulation. However, as we argued, the permutation and averaging does have significant beneficial side effects, and if anything, it only eliminates from the point of view of the algorithms the important advantage that real children may be given by their mothers through a specific ordering of the utterances. In any case, we have</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Brent, Michael R. 1999. An efficient probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Timothy A Cartwright</author>
</authors>
<title>Distributional regularity and phonotactics are useful for segmentation.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--93</pages>
<marker>Brent, Cartwright, 1996</marker>
<rawString>Brent, Michael R. and Timothy A. Cartwright. 1996. Distributional regularity and phonotactics are useful for segmentation. Cognition, 61:93-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Christiansen</author>
<author>J Allen</author>
<author>M Seidenberg</author>
</authors>
<title>Learning to segment speech using multiple cues: A connectionist model.</title>
<date>1998</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>13--221</pages>
<marker>Christiansen, Allen, Seidenberg, 1998</marker>
<rawString>Christiansen, M. H., J. Allen, and M. Seidenberg. 1998. Learning to segment speech using multiple cues: A connectionist model. Language and Cognitive Processes, 13:221-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Cutler</author>
<author>D M Carter</author>
</authors>
<title>Predominance of strong initial syllables in the English vocabulary.</title>
<date>1987</date>
<journal>Computer Speech and Language,</journal>
<pages>2--133</pages>
<contexts>
<context position="1586" citStr="Cutler and Carter 1987" startWordPosition="242" endWordPosition="245">of words the language possesses identification of word boundaries is a significant problem in the domain of child language acquisition.&apos; Although speech lacks explicit demarcation of word boundaries, it is undoubtedly the case that it nevertheless possesses significant other cues for word discovery. However, it is still a matter of interest to see exactly how much can be achieved without the incorporation of these other cues; that is, we are interested in the performance of a bare-bones language model. For example, there is much evidence that stress patterns (Jusczyk, Cutler, and Redanz 1993; Cutler and Carter 1987) and phonotactics of speech (Mattys and Jusczyk 1999) are of considerable aid in word discovery. But a bare-bones statistical model is still useful in that it allows us to quantify precise improvements in performance upon the integration of each specific cue into the model. We present and evaluate one such statistical model in this paper.&apos; The main contributions of this study are as follows: First, it demonstrates the applicability and competitiveness of a conservative, traditional approach for a task for which nontraditional approaches have been proposed even recently (Brent 1999; Brent and C</context>
</contexts>
<marker>Cutler, Carter, 1987</marker>
<rawString>Cutler, Anne and D. M. Carter. 1987. Predominance of strong initial syllables in the English vocabulary. Computer Speech and Language, 2:133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de Marcken</author>
</authors>
<title>Unsupervised acquisition of a lexicon from continuous speech.</title>
<date>1995</date>
<tech>Technical Report Al Memo No. 1558,</tech>
<institution>Massachusetts Institute of Technology,</institution>
<location>Cambridge, MA.</location>
<marker>de Marcken, 1995</marker>
<rawString>de Marcken, C. 1995. Unsupervised acquisition of a lexicon from continuous speech. Technical Report Al Memo No. 1558, Massachusetts Institute of Technology, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<pages>14--179</pages>
<contexts>
<context position="2229" citStr="Elman 1990" startWordPosition="344" endWordPosition="345">ys and Jusczyk 1999) are of considerable aid in word discovery. But a bare-bones statistical model is still useful in that it allows us to quantify precise improvements in performance upon the integration of each specific cue into the model. We present and evaluate one such statistical model in this paper.&apos; The main contributions of this study are as follows: First, it demonstrates the applicability and competitiveness of a conservative, traditional approach for a task for which nontraditional approaches have been proposed even recently (Brent 1999; Brent and Cartwright 1996; de Marcken 1995; Elman 1990; Christiansen, Allen, and Seidenberg 1998). Second, although the model leads to the development of an algorithm that learns the lexicon in an unsupervised fashion, results of partial supervision are presented, showing that its performance is consistent with results from learning theory. Third, the study extends previous work to higher-order n-grams, specifically up to * STAR Lab, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025. E-mail: anand@speech.sri.com 1 See, however, work in Jusczyk and Hohne (1997) and Jusczyk (1997) that presents strong evidence in favor of a hypothesis th</context>
<context position="5862" citStr="Elman 1990" startWordPosition="908" endWordPosition="909">in spirit to approaches based on transitional probability between phonemes or syllables in speech. One such approach derives from experiments by Saffran, Newport, and Aslin (1996) suggesting that young children might place word boundaries between two syllables where the second syllable is surprising given the first. This technique is described and evaluated in Brent (1999). Other approaches not based on explicit probability models include those based on information theoretic criteria such as minimum description length (Brent and Cartwright 1996; de Marcken 1995) and simple recurrent networks (Elman 1990; Christiansen, Allen, and Seidenberg 1998). The maximum likelihood approach due to Olivier (1968) is probabilistic in the sense that it is geared toward explicitly calculating the most probable segmentation of each block of input utterances (see also Batchelder 1997). However, the algorithm involves heuristic steps in periodic purging of the lexicon and in the creation in the lexicon of new words. Furthermore, this approach is again not based on a formal statistical model. Model Based Dynamic Programming, hereafter referred to as MBDP-1 (Brent 1999), is probably the most recent work that addr</context>
<context position="23468" citStr="Elman (1990)" startWordPosition="3753" endWordPosition="3754"> where the algorithm used varying amounts of training data, precision, recall, and lexical precision scores are computed over the entire corpus. All scores are reported as percentages. 5. Results Figures 3-5 plot the precision, recall, and lexicon precision of the proposed algorithm for each of the unigram, bigram, and trigram models against the MBDP-1 algorithm. Although the graphs compare the performance of the algorithm with only one published result in the field, comparison with other related approaches is implicitly available. Brent (1999) reports results of running the algorithms due to Elman (1990) and Olivier (1968), as well as algorithms based on mutual information and transitional probability between pairs of phonemes, over exactly the same corpus. These are all shown to perform significantly worse than Brent&apos;s MBDP-1. The random baseline algorithm in Brent (1999), which consistently performs with under 20% precision and recall, is not graphed for the same reason. This baseline algorithm offers an important advantage: It knows the exact number of word boundaries, even though it does not know their locations. Brent argued that if MBDP-1 performs as well as this random baseline, then a</context>
<context position="26340" citStr="Elman (1990)" startWordPosition="4231" endWordPosition="4232"> over 1000 runs, each using a random permutation of the input corpus. Precision is defined as the percentage of identified words that are correct, as measured against the target data. The horizontal axis represents the number of blocks of data scored, where each block represents 100 utterances. The plots show the performance of the 1-gram, 2-gram, 3-gram, and MBDP-1 algorithms. The plot for MBDP-1 is not visible because it coincides almost exactly with the plot for the 1-gram model. Discussion of this level of similarity is provided in Section 5.5. The performance of related algorithms due to Elman (1990), Olivier (1968) and others is implicitly available in this and the following graphs since Brent (1999) demonstrates that these all perform significantly worse than MBDP-1. One may object that the original transcripts carefully preserve the order of utterances directed at children by their mothers, and hence randomly permuting the corpus would destroy the fidelity of the simulation. However, as we argued, the permutation and averaging does have significant beneficial side effects, and if anything, it only eliminates from the point of view of the algorithms the important advantage that real chi</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Elman, J. L. 1990. Finding structure in time. Cognitive Science, 14:179-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Quantifying inductive bias: Al learning algorithms and Valiant&apos;s learning framework.</title>
<date>1988</date>
<journal>Artificial Intelligence,</journal>
<pages>36--177</pages>
<contexts>
<context position="41337" citStr="Haussler 1988" startWordPosition="6759" endWordPosition="6760">n steps of approximately 1% (100 utterances). For each training set size, the results reported are averaged over 25 runs of the experiment, each over a separate random permutation of the corpus. As before, this was done both to correct for ordering idiosyncrasies, and to smooth the graphs to make them easier to interpret. We interpret Figures 7 and 8 as suggesting that the performance of all algorithms discussed here can be boosted significantly with even a small amount of training. It is noteworthy and reassuring to see that, as one would expect from results in computational learning theory (Haussler 1988), the number of training examples required to obtain a desired value of precision p, appears to grow with 1/(1 — p). The intriguing reversal in the performance of the various n-gram models with respect to precision and recall is again seen here and the explanation for this too is the same as discussed earlier. We further note, however, that the difference in performance between the different models tends to narrow with increasing training size; that is, as the amount of evidence available to infer word boundaries increases, the 3-gram model rapidly catches up with the others in recall and lexi</context>
</contexts>
<marker>Haussler, 1988</marker>
<rawString>Haussler, David. 1988. Quantifying inductive bias: Al learning algorithms and Valiant&apos;s learning framework. Artificial Intelligence, 36:177-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9676" citStr="Jelinek (1997" startWordPosition="1507" endWordPosition="1508">entation is not. Second, making the assumption that the corpus is a single event in probability space significantly increases the computational complexity of the incremental algorithm. The approach presented in this paper circumvents these problems through the use of a conservative statistical model that is directly implementable as an incremental algorithm. In the following section, we describe the model and how its 2-gram and 3-gram extensions are adapted for implementation. 3. Model Description The language model described here is a fairly standard one. The interested reader is referred to Jelinek (1997, 57-78), where a detailed exposition can be found. Basically, we seek argmax P(W) (1) argmax P(wi wi, w1-1) (2) w i=i = argmin - log P(w I w1,. • • ,w-i) (3) w i=i where W = w1,. , wn with wi E L denotes a particular string of n words belonging to a lexicon L. The usual n-gram approximation is made by grouping histories wi, ,w1_1 into equivalence classes, allowing us to collapse contexts into histories at most n-1 words 353 Computational Linguistics Volume 27, Number 3 backwards (for n-grams). Estimations of the required n-gram probabilities are then done with relative frequencies using back-</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Jelinek, F. 1997. Statistical Methods for Speech Recognition. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Jusczyk</author>
</authors>
<title>The Discovery of Spoken Language.</title>
<date>1997</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2771" citStr="Jusczyk (1997)" startWordPosition="425" endWordPosition="426">ntly (Brent 1999; Brent and Cartwright 1996; de Marcken 1995; Elman 1990; Christiansen, Allen, and Seidenberg 1998). Second, although the model leads to the development of an algorithm that learns the lexicon in an unsupervised fashion, results of partial supervision are presented, showing that its performance is consistent with results from learning theory. Third, the study extends previous work to higher-order n-grams, specifically up to * STAR Lab, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025. E-mail: anand@speech.sri.com 1 See, however, work in Jusczyk and Hohne (1997) and Jusczyk (1997) that presents strong evidence in favor of a hypothesis that children already have a reasonably powerful and accurate lexicon at their disposal as early as 9 months of age. 2 Implementations of all the programs discussed in this paper and the input corpus are readily available upon request from the author. The programs (totaling about 900 lines) have been written in C++ to compile under Unix/Linux. The author will assist in porting it to other architectures or to versions of Unix other than Linux or SunOS/Solaris if required. © 2001 Association for Computational Linguistics Computational Lingu</context>
</contexts>
<marker>Jusczyk, 1997</marker>
<rawString>Jusczyk, Peter W. 1997. The Discovery of Spoken Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Jusczyk</author>
<author>Anne Cutler</author>
<author>N Redanz</author>
</authors>
<title>Preference for predominant stress patterns of English words.</title>
<date>1993</date>
<journal>Child Development,</journal>
<pages>64--675</pages>
<marker>Jusczyk, Cutler, Redanz, 1993</marker>
<rawString>Jusczyk, Peter W., Anne Cutler, and N. Redanz. 1993. Preference for predominant stress patterns of English words. Child Development, 64:675-687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Jusczyk</author>
<author>E A Hohne</author>
</authors>
<title>Infants&apos; memory for spoken words.</title>
<date>1997</date>
<journal>Science,</journal>
<pages>27--1984</pages>
<contexts>
<context position="2752" citStr="Jusczyk and Hohne (1997)" startWordPosition="420" endWordPosition="423"> have been proposed even recently (Brent 1999; Brent and Cartwright 1996; de Marcken 1995; Elman 1990; Christiansen, Allen, and Seidenberg 1998). Second, although the model leads to the development of an algorithm that learns the lexicon in an unsupervised fashion, results of partial supervision are presented, showing that its performance is consistent with results from learning theory. Third, the study extends previous work to higher-order n-grams, specifically up to * STAR Lab, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025. E-mail: anand@speech.sri.com 1 See, however, work in Jusczyk and Hohne (1997) and Jusczyk (1997) that presents strong evidence in favor of a hypothesis that children already have a reasonably powerful and accurate lexicon at their disposal as early as 9 months of age. 2 Implementations of all the programs discussed in this paper and the input corpus are readily available upon request from the author. The programs (totaling about 900 lines) have been written in C++ to compile under Unix/Linux. The author will assist in porting it to other architectures or to versions of Unix other than Linux or SunOS/Solaris if required. © 2001 Association for Computational Linguistics </context>
</contexts>
<marker>Jusczyk, Hohne, 1997</marker>
<rawString>Jusczyk, Peter W. and E. A. Hohne. 1997. Infants&apos; memory for spoken words. Science, 27:1984-1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="10365" citStr="Katz 1987" startWordPosition="1626" endWordPosition="1627">(W) (1) argmax P(wi wi, w1-1) (2) w i=i = argmin - log P(w I w1,. • • ,w-i) (3) w i=i where W = w1,. , wn with wi E L denotes a particular string of n words belonging to a lexicon L. The usual n-gram approximation is made by grouping histories wi, ,w1_1 into equivalence classes, allowing us to collapse contexts into histories at most n-1 words 353 Computational Linguistics Volume 27, Number 3 backwards (for n-grams). Estimations of the required n-gram probabilities are then done with relative frequencies using back-off to lower-order n-grams when a higherorder estimate is not reliable enough (Katz 1987). Back-off is done using the Witten and Bell (1991) technique, which allocates a probability of NON, S,) to unseen i-grams at each stage, with the final back-off from unigrams being to an open vocabulary where word probabilities are calculated as a normalized product of phoneme or letter probabilities. Here, Ni is the number of distinct i-grams and S, is the sum of their frequencies. The model can be summarized as follows: S3 C(W/-2/Wi-1,WI) N3±S3 C(W1-1,W I i 1\_13._ ,,, N3 +S3 wl-1) $2 C(ZO,_1,W,) _= N2+S2 C(ll&apos;) N2 N2 ±S2 k 11 p(w.) C(w) NiNtSi Nidi-Si P&gt; (w) lc; if C(wi-2, wi-i, wi) &gt; 0 ot</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35(3):400401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacVVhinney</author>
<author>C Snow</author>
</authors>
<title>The child language data exchange system.</title>
<date>1985</date>
<journal>Journal of Child Language,</journal>
<pages>12--271</pages>
<marker>MacVVhinney, Snow, 1985</marker>
<rawString>MacVVhinney, Brian and C. Snow. 1985. The child language data exchange system. Journal of Child Language, 12:271-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven L Mattys</author>
<author>Peter W Jusczyk</author>
</authors>
<title>Phonotactic and prosodic effects on word segmentation in infants.</title>
<date>1999</date>
<journal>Cognitive Psychology,</journal>
<pages>38--465</pages>
<contexts>
<context position="1639" citStr="Mattys and Jusczyk 1999" startWordPosition="250" endWordPosition="253">rd boundaries is a significant problem in the domain of child language acquisition.&apos; Although speech lacks explicit demarcation of word boundaries, it is undoubtedly the case that it nevertheless possesses significant other cues for word discovery. However, it is still a matter of interest to see exactly how much can be achieved without the incorporation of these other cues; that is, we are interested in the performance of a bare-bones language model. For example, there is much evidence that stress patterns (Jusczyk, Cutler, and Redanz 1993; Cutler and Carter 1987) and phonotactics of speech (Mattys and Jusczyk 1999) are of considerable aid in word discovery. But a bare-bones statistical model is still useful in that it allows us to quantify precise improvements in performance upon the integration of each specific cue into the model. We present and evaluate one such statistical model in this paper.&apos; The main contributions of this study are as follows: First, it demonstrates the applicability and competitiveness of a conservative, traditional approach for a task for which nontraditional approaches have been proposed even recently (Brent 1999; Brent and Cartwright 1996; de Marcken 1995; Elman 1990; Christia</context>
</contexts>
<marker>Mattys, Jusczyk, 1999</marker>
<rawString>Mattys, Sven L. and Peter W. Jusczyk. 1999. Phonotactic and prosodic effects on word segmentation in infants. Cognitive Psychology, 38:465-494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Olivier</author>
</authors>
<title>Stochastic grammars and language acquisition mechanisms. Unpublished Ph.D. dissertation,</title>
<date>1968</date>
<institution>Harvard University,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="5960" citStr="Olivier (1968)" startWordPosition="921" endWordPosition="922">ech. One such approach derives from experiments by Saffran, Newport, and Aslin (1996) suggesting that young children might place word boundaries between two syllables where the second syllable is surprising given the first. This technique is described and evaluated in Brent (1999). Other approaches not based on explicit probability models include those based on information theoretic criteria such as minimum description length (Brent and Cartwright 1996; de Marcken 1995) and simple recurrent networks (Elman 1990; Christiansen, Allen, and Seidenberg 1998). The maximum likelihood approach due to Olivier (1968) is probabilistic in the sense that it is geared toward explicitly calculating the most probable segmentation of each block of input utterances (see also Batchelder 1997). However, the algorithm involves heuristic steps in periodic purging of the lexicon and in the creation in the lexicon of new words. Furthermore, this approach is again not based on a formal statistical model. Model Based Dynamic Programming, hereafter referred to as MBDP-1 (Brent 1999), is probably the most recent work that addresses exactly the same issue as that considered in this paper. Both the approach presented in this</context>
<context position="23487" citStr="Olivier (1968)" startWordPosition="3756" endWordPosition="3757">thm used varying amounts of training data, precision, recall, and lexical precision scores are computed over the entire corpus. All scores are reported as percentages. 5. Results Figures 3-5 plot the precision, recall, and lexicon precision of the proposed algorithm for each of the unigram, bigram, and trigram models against the MBDP-1 algorithm. Although the graphs compare the performance of the algorithm with only one published result in the field, comparison with other related approaches is implicitly available. Brent (1999) reports results of running the algorithms due to Elman (1990) and Olivier (1968), as well as algorithms based on mutual information and transitional probability between pairs of phonemes, over exactly the same corpus. These are all shown to perform significantly worse than Brent&apos;s MBDP-1. The random baseline algorithm in Brent (1999), which consistently performs with under 20% precision and recall, is not graphed for the same reason. This baseline algorithm offers an important advantage: It knows the exact number of word boundaries, even though it does not know their locations. Brent argued that if MBDP-1 performs as well as this random baseline, then at the very least, i</context>
<context position="26356" citStr="Olivier (1968)" startWordPosition="4233" endWordPosition="4234">s, each using a random permutation of the input corpus. Precision is defined as the percentage of identified words that are correct, as measured against the target data. The horizontal axis represents the number of blocks of data scored, where each block represents 100 utterances. The plots show the performance of the 1-gram, 2-gram, 3-gram, and MBDP-1 algorithms. The plot for MBDP-1 is not visible because it coincides almost exactly with the plot for the 1-gram model. Discussion of this level of similarity is provided in Section 5.5. The performance of related algorithms due to Elman (1990), Olivier (1968) and others is implicitly available in this and the following graphs since Brent (1999) demonstrates that these all perform significantly worse than MBDP-1. One may object that the original transcripts carefully preserve the order of utterances directed at children by their mothers, and hence randomly permuting the corpus would destroy the fidelity of the simulation. However, as we argued, the permutation and averaging does have significant beneficial side effects, and if anything, it only eliminates from the point of view of the algorithms the important advantage that real children may be giv</context>
</contexts>
<marker>Olivier, 1968</marker>
<rawString>Olivier, D. C. 1968. Stochastic grammars and language acquisition mechanisms. Unpublished Ph.D. dissertation, Harvard University, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer R Saffran</author>
<author>E L Newport</author>
<author>R N Aslin</author>
</authors>
<title>Word segmentation: The role of distributional cues.</title>
<date>1996</date>
<journal>Journal of Memory and Language,</journal>
<pages>35--606</pages>
<marker>Saffran, Newport, Aslin, 1996</marker>
<rawString>Saffran, Jennifer R., E. L. Newport, and R. N. Aslin. 1996. Word segmentation: The role of distributional cues. Journal of Memory and Language, 35:606-621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimal decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>13--260</pages>
<contexts>
<context position="11904" citStr="Viterbi 1967" startWordPosition="1898" endWordPosition="1899">n by dividing using 1 - r(#) in Equation (7) is necessary because otherwise r(#) 1-1 r(w 1=1 00 P(w) z=i l_p(#) Since we estimate P(w[j] ) by r(w[j]), dividing by 1 - r(#) will ensure that Ew P(w) = 1. 4. Method As in Brent (1999), the model described in Section 3 is presented as an incremental learner. The only knowledge built into the system at start-up is the phoneme table, with a uniform distribution over all phonemes, including the sentinel phoneme. The learning algorithm considers each utterance in turn and computes the most probable segmentation of the utterance using a Viterbi search (Viterbi 1967) implemented as a dynamic programming algorithm, as described in Section 4.2. The most likely placement of word boundaries thus computed is committed to before the algorithm considers the next utterance presented. Committing to a segmentation involves learning unigram, bigram, and trigram frequencies, as well as phoneme frequencies, from the inferred words. These are used to update the respective tables. To account for effects that any specific ordering of input utterances may have on the segmentations that are output, the performance of the algorithm is averaged over 1000 runs, with each run </context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi, Andrew J. 1967. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, IT-13:260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>37--4</pages>
<contexts>
<context position="10416" citStr="Witten and Bell (1991)" startWordPosition="1633" endWordPosition="1636"> = argmin - log P(w I w1,. • • ,w-i) (3) w i=i where W = w1,. , wn with wi E L denotes a particular string of n words belonging to a lexicon L. The usual n-gram approximation is made by grouping histories wi, ,w1_1 into equivalence classes, allowing us to collapse contexts into histories at most n-1 words 353 Computational Linguistics Volume 27, Number 3 backwards (for n-grams). Estimations of the required n-gram probabilities are then done with relative frequencies using back-off to lower-order n-grams when a higherorder estimate is not reliable enough (Katz 1987). Back-off is done using the Witten and Bell (1991) technique, which allocates a probability of NON, S,) to unseen i-grams at each stage, with the final back-off from unigrams being to an open vocabulary where word probabilities are calculated as a normalized product of phoneme or letter probabilities. Here, Ni is the number of distinct i-grams and S, is the sum of their frequencies. The model can be summarized as follows: S3 C(W/-2/Wi-1,WI) N3±S3 C(W1-1,W I i 1\_13._ ,,, N3 +S3 wl-1) $2 C(ZO,_1,W,) _= N2+S2 C(ll&apos;) N2 N2 ±S2 k 11 p(w.) C(w) NiNtSi Nidi-Si P&gt; (w) lc; if C(wi-2, wi-i, wi) &gt; 0 otherwise P(w Wi-1) P(w i I wi-i) P(w1) if C(wi_i, wi</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Witten, Ian H. and Timothy C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4):1085-1091.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zimin</author>
<author>G Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval problems and achievements.</title>
<date>1993</date>
<journal>JASIS,</journal>
<pages>44--9</pages>
<marker>Zimin, Tseng, 1993</marker>
<rawString>Zimin, W. and G. Tseng. 1993. Chinese text segmentation for text retrieval problems and achievements. JASIS, 44(9):532-542.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>