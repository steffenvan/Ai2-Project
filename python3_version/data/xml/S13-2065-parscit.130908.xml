<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.754861">
KLUE: Simple and robust methods for polarity classification
</title>
<author confidence="0.752065">
Thomas Proisl and Paul Greiner and Stefan Evert and Besim Kabashi
</author>
<affiliation confidence="0.61501">
Friedrich-Alexander-Universität Erlangen-Nürnberg
</affiliation>
<address confidence="0.68465875">
Department Germanistik und Komparatistik
Professur für Korpuslinguistik
Bismarckstr. 6
91054 Erlangen, Germany
</address>
<email confidence="0.996939">
{thomas.proisl,paul.greiner,stefan.evert,besim.kabashi}@fau.de
</email>
<sectionHeader confidence="0.995597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99984475">
This paper describes our approach to the
SemEval-2013 task on “Sentiment Analysis
in Twitter”. We use simple bag-of-words mod-
els, a freely available sentiment dictionary auto-
matically extended with distributionally similar
terms, as well as lists of emoticons and inter-
net slang abbreviations in conjunction with fast
and robust machine learning algorithms. The
resulting system is resource-lean, making it rel-
atively independent of a specific language. De-
spite its simplicity, the system achieves compet-
itive accuracies of 0.70–0.72 in detecting the
sentiment of text messages. We also apply our
approach to the task of detecting the context-
dependent sentiment of individual words and
phrases within a message.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960269230769">
The SemEval-2013 task on “Sentiment Analysis in
Twitter” (Wilson et al., 2013) focuses on polarity clas-
sification, i. e. the problem of determining whether
a textual unit, e. g. a document, paragraph, sentence
or phrase, expresses a positive, negative or neutral
sentiment (for a review of research topics and re-
cent developments in the field of sentiment analysis
see Liu (2012)). There are two subtasks: in task B,
“Message Polarity Classification”, whole messages
have to be classified as being of positive, negative
or neutral sentiment; in task A, “Contextual Polarity
Disambiguation”, a marked instance of a word or
phrase has to be classified in the context of a whole
message.
The training data for task B consist of approxi-
mately 10 200 manually annotated Twitter messages,
the training data for task A of approximately 9 500
marked instances in approximately 6 300 Twitter mes-
sages.1 The test data consist of in-domain Twit-
ter messages (3 813 messages for task B and 4435
marked instances in 2 826 messages for task A) and
out-of-domain SMS text messages (2 094 messages
for task B, 2 334 marked instances in 1437 messages
for task A). The distribution of messages and marked
instances over sentiment categories in the training
and test sets is shown in Tab. 1.
</bodyText>
<table confidence="0.994488142857143">
pos neg neu total
3 783 1600 4 832 10 215
1572 601 1640 3 813
492 394 1208 2094
5 862 3166 463 9 491
2734 1541 160 4435
1071 1104 159 2 334
</table>
<tableCaption confidence="0.999915">
Table 1: The data sets for both tasks
</tableCaption>
<bodyText confidence="0.9998755">
The main focus of the current paper lies on experi-
menting with resource-lean and robust methods for
task B, the classification of whole messages. We do,
however, apply our approach also to task A.
</bodyText>
<sectionHeader confidence="0.731574" genericHeader="method">
2 Features used for polarity classification
</sectionHeader>
<bodyText confidence="0.997528">
Our general approach is quite simple: we extract
feature vectors from the training data (based on the
</bodyText>
<footnote confidence="0.817528">
1These figures indicate the amount of training data we were
actually able to use. Due to Twitter’s licensing conditions, the
training data could only be made available as a collection of IDs.
Even when using the official Twitter API for collecting the actual
messages rather than the screen-scraping approach suggested by
the task organizers, ca. 10% of the data were not (or no longer)
available.
</footnote>
<equation confidence="0.952529333333333">
train-B
test-B Twitter
test-B SMS
train-A
test-A Twitter
test-A SMS
</equation>
<page confidence="0.936527">
395
</page>
<bodyText confidence="0.988345846153846">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 395–401, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
original messages and a small number of additional
resources) and feed them into fast and robust super-
vised machine learning algorithms implemented in
the Python machine learning library scikit-learn (Pe-
dregosa et al., 2011). For task B, the features are
computed on the basis of the whole message; for task
A, we use essentially the same features, but compute
them once for the marked word or phrase and once
for the rest of the message. All the features we use
are described in some more detail in the following
subsections.
</bodyText>
<subsectionHeader confidence="0.999803">
2.1 Bag of words
</subsectionHeader>
<bodyText confidence="0.999929952380952">
We experimented with three different sets of bag-of-
words features: unigrams, unigrams and bigrams, and
an extended unigram model that includes a simple
treatment of negation. For all three models we simply
use the word frequencies as feature weights.
Our preprocessing pipeline starts with a simple
preliminary tokenization step (lowercasing the whole
message and splitting it on whitespace). In the re-
sulting list of tokens, all user IDs and web URLs are
replaced with placeholders.2 Any remaining punctu-
ation is stripped from the tokens and empty tokens
are deleted. In the extended unigram model, up to
three tokens following a negation marker are then
prefixed with not_ (fewer tokens if another negation
marker or the end of the message is reached). Finally
all words are stemmed using the Snowball stemmer.3
For a token unigram or bigram to be included in
the bag of words models, it has to occur in at least
five messages.
As an additional feature we include the total num-
ber of tokens per message.
</bodyText>
<subsectionHeader confidence="0.99815">
2.2 Features based on a sentiment dictionary
</subsectionHeader>
<bodyText confidence="0.999143857142857">
Widely-used algorithms such as SentiStrength (Thel-
wall et al., 2010) rely heavily on dictionaries contain-
ing sentiment ratings of words and/or phrases. We
use features based on an extended version of AFINN-
111 (Nielsen, 2011).4
The AFINN sentiment dictionary contains senti-
ment ratings ranging from −5 (very negative) to 5
</bodyText>
<footnote confidence="0.9965912">
2The regular expression for matching web URLs has
been taken from http://daringfireball.net/2010/07/
improved_regex_for_matching_urls.
3http://snowball.tartarus.org/
4http://www2.imm.dtu.dk/pubdb/p.php?6010
</footnote>
<bodyText confidence="0.999958461538461">
(very positive) for 2476 word forms. In order to ob-
tain a better coverage, we extended the dictionary
with distributionally similar words. For this pur-
pose, large-vocabulary distributional semantic mod-
els (DSM) were constructed from a version of the
English Wikipedia5 and the Google Web 1T 5-Grams
database (Brants and Franz, 2006). The Wikipedia
DSM consists of 122 281 case-folded word forms
as target terms and 30 484 mid-frequency content
words (lemmatised) as feature terms; the Web1T5
DSM of 241583 case-folded word forms as target
terms and 100 063 case-folded word forms as fea-
ture terms. Both DSMs use a context window of two
words to the left and right, and were reduced to 300
latent dimensions using randomized singular value
decomposition (Halko et al., 2009).
For each AFINN entry, the 30 nearest neighbours
according to each DSM were considered as exten-
sion candidates. Sentiment ratings for the new candi-
dates were computed by averaging over the 30 near-
est neighbours of the respective candidate term (with
scores set to 0 for all neighbours not listed in AFINN),
and rescaling to the range [−5,5].6 After some ini-
tial experiments, only candidates with a computed
rating &lt; −2.5 or &gt; 2.5 were retained, resulting in an
extended dictionary of 2 820 word forms.
As with the bag of words model, we make use of
a simple heuristic treatment of negation: following a
negation marker, the polarity of the next sentiment-
carrying token up to a distance of at most four tokens
is multiplied by −1.
The sentiment dictionary is used to extract four
features: I) the number of tokens that express a posi-
tive sentiment, II) the number of tokens that express
a negative sentiment, III) the total number of tokens
that express a sentiment according to our sentiment
dictionary and IV) the arithmetic mean of all the sen-
timent scores from the sentiment dictionary in the
message.
</bodyText>
<footnote confidence="0.9459524">
5We used the pre-processed and linguistically annotated
Wackypedia corpus available from http://wacky.sslmit.
unibo.it/.
6Scaling coefficients were determined by regression on ex-
tension candidates that were already listed in AFINN.
</footnote>
<page confidence="0.996592">
396
</page>
<subsectionHeader confidence="0.770444">
2.3 Features based on emoticons and internet
slang abbreviations
</subsectionHeader>
<bodyText confidence="0.999980454545455">
In addition to the sentiment dictionary we use a list
of 212 emoticons and 95 internet slang abbreviations
from Wikipedia. We manually classified these 307
emotion markers as negative (−1), neutral (0) or pos-
itive (1).
The extracted features based on this list are similar
to the ones based on the sentiment dictionary. We use
I) the number of positive emotion markers, II) the
number of negative emotion markers, III) the total
number of emotion markers and IV) the arithmetic
mean of all the emotion markers in the message.
</bodyText>
<sectionHeader confidence="0.99957" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998961125">
In this section we evaluate different classifiers (multi-
nomial Naive Bayes,7 Linear SVM8 and Maximum
Entropy9) and various combinations of features on
the gold test sets. We vary the bag-of-words model
(bow), the use of AFINN (sent), our extensions to
the sentiment dictionary (ext) and the list of emotion
markers (emo). To present as clear a picture of the
classifiers’ performances as possible, we report F-
scores for each of the three classes, the weighted av-
erage of all three F-scores (Fw), the (unweighted) av-
erage of the positive and negative F-scores (Fpos+neg;
this is the value shown in the official task results and
used for ranking systems), as well as accuracy.
Results for submitted systems are typeset in italics,
the best results in each column are typeset in bold
font.
</bodyText>
<subsectionHeader confidence="0.997042">
3.1 Task B: Message Polarity Classification
</subsectionHeader>
<bodyText confidence="0.999958285714286">
Experiments with just a simple unigram bag-of-
words model show that for both the Twitter (Tab. 3)
and the SMS data (Tab. 4) the Maximum Entropy
classifier outperforms multinomial Naive Bayes and
Linear SVM by a considerable margin. For compar-
ison, we also include some weak baselines (Tab. 2).
The random baselines classify messages randomly,10
</bodyText>
<footnote confidence="0.996018375">
7We always use the default setting alpha = 1.0.
8In all experiments, we use the following parameters:
penalty = ‘l1’, dual = False, C = 1.0.
9We use the following parameter settings in our experiments:
penalty = ‘l1’, C = 1.0.
10randomuniform assumes a uniform probability distribution
(all categories have equal probabilities), randomweighted has
learned the probability distribution from the training data,
</footnote>
<bodyText confidence="0.999612344827586">
the majority baselines simply assign all messages
to the most frequent category in the training data.11
As one would expect, all three learning algorithms
are vastly superior to those baselines. Using both
unigrams and bigrams in the bag-of-words model
improves classifier peformance; so does the extended
unigram model with negations.
For the Twitter data, adding the sentiment dictio-
nary, the dictionary extensions and the list of emo-
tion markers further improves classifier performance,
with the best results being achieved by a combina-
tion of all these features with a uni- and bigram bag-
of-words model. The best combination of features
would have been the fourth best system out of 35
constrained systems (sixth best out of all 51 systems),
one rank higher than our task submission.12
For the SMS data, adding the sentiment dictio-
nary and the dictionary extensions seems to improve
the official score Fpos+neg, but slightly decreases
weighted average F-score and accuracy. This might
be due to the greater orthographical variation in SMS
texts. Emotion markers seem to be a much better
sentiment indicator in the SMS data. But while just
combining the list of emotion markers with the ex-
tended unigram bag-of-words model leads to the best
weighted average F-score and accuracy, Fpos+neg is
best when a combination of all features is used. This
is also the system we submitted, being the third best
system (out of 44) for that task.
</bodyText>
<subsectionHeader confidence="0.6699375">
3.2 Task A: Contextual Polarity
Disambiguation
</subsectionHeader>
<bodyText confidence="0.942051090909091">
The results for task A are similar to those for task
B in that Maximum Entropy is the best classifier for
the unigram bag-of-words model for both the Twitter
(Tab. 5) and the SMS data (Tab. 6). Adding negation
treatment to the bag-of-words model increases classi-
fier performance, as do the inclusion of AFINN and
the use of emotion markers. Interestingly, extend-
ing the sentiment dictionary based on distributional
similarity leads to slightly worse results. Therefore,
randomweighted,binary uses the same probability distribution but
classifies messages only as either positive or negative.
</bodyText>
<footnote confidence="0.737857166666667">
11majority classifies all messages as neutral, as this is the most
frequent category in the training data, majoritybinary does binary
classification and thus classifies all messages as positive.
12Evaluation results for all SemEval-2013 tasks are avail-
able online: http://www.cs.york.ac.uk/semeval-2013/
index.php?id=evaluation-results.
</footnote>
<page confidence="0.990537">
397
</page>
<table confidence="0.997881166666667">
classifier Fpos Fneg Fneu Fw Fpos+neg Acc
randomuniform 0.3666 0.2128 0.3745 0.3458 0.2897 0.3318
randomweighted 0.3912 0.1681 0.4521 0.3820 0.2796 0.3835
randomweighted,binary 0.5186 0.2042 0.000 0.2460 0.3614 0.3349
majority 0.0000 0.0000 0.6015 0.2587 0.0000 0.4301
majoritybinary 0.5838 0.0000 0.0000 0.2407 0.2919 0.4123
</table>
<tableCaption confidence="0.998895">
Table 2: Some weak baselines for task B, Twitter test set
</tableCaption>
<figure confidence="0.991295357142857">
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.6355 0.5093 0.6898 0.6390 0.5724 0.6423
LinearSVM uni - - - 0.6412 0.4884 0.6876 0.6371 0.5648 0.6418
MaxEnt uni - - - 0.6705 0.5109 0.7212 0.6671 0.5907 0.6761
MaxEnt uni+bi - - - 0.6845 0.5192 0.7257 0.6762 0.6019 0.6845
MaxEnt unineg - - - 0.6797 0.5284 0.7242 0.6750 0.6041 0.6824
MaxEnt unineg + - - 0.6860 0.5661 0.7284 0.6854 0.6261 0.6911
MaxEnt unineg - - + 0.6807 0.5393 0.7229 0.6766 0.6100 0.6835
MaxEnt unineg + + - 0.6841 0.5529 0.7258 0.6814 0.6185 0.6874
MaxEnt unineg + + + 0.6963 0.5650 0.7325 0.6912 0.6306 0.6968
MaxEnt unineg + - + 0.6952 0.5753 0.7338 0.6929 0.6353 0.6984
MaxEnt uni+bi + - + 0.7034 0.5706 0.7358 0.6964 0.6370 0.7018
MaxEnt uni+bi + + + 0.7052 0.5720 0.7371 0.6979 0.6386 0.7031
MaxEnt - + + + 0.6920 0.3532 0.6533 0.6220 0.5226 0.6370
</figure>
<tableCaption confidence="0.951992">
Table 3: Evaluation results for task B on the Twitter test set
</tableCaption>
<figure confidence="0.653842857142857">
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.4918 0.4773 0.5541 0.5250 0.4845 0.5153
LinearSVM uni - - - 0.5833 0.5046 0.7229 0.6490 0.5440 0.6442
MaxEnt uni - - - 0.6260 0.5015 0.7903 0.6974 0.5638 0.7015
MaxEnt uni+bi - - - 0.6003 0.5380 0.7658 0.6840 0.5692 0.6829
MaxEnt unineg - - - 0.6528 0.5412 0.7884 0.7100 0.5970 0.7125
MaxEnt unineg + - - 0.6399 0.5955 0.7744 0.7092 0.6177 0.7073
MaxEnt unineg - - + 0.6596 0.5507 0.8033 0.7220 0.6052 0.7259
MaxEnt unineg + + - 0.6374 0.5905 0.7731 0.7068 0.6140 0.7049
MaxEnt unineg + + + 0.6506 0.5900 0.7903 0.7198 0.6203 0.7197
MaxEnt unineg + - + 0.6556 0.5833 0.7908 0.7200 0.6195 0.7202
MaxEnt uni+bi + - + 0.6318 0.5896 0.7750 0.7064 0.6107 0.7044
MaxEnt uni+bi + + + 0.6341 0.5783 0.7746 0.7047 0.6062 0.7030
MaxEnt - + + + 0.5961 0.3421 0.7179 0.6186 0.4691 0.6342
</figure>
<tableCaption confidence="0.997924">
Table 4: Evaluation results for task B on the SMS test set
</tableCaption>
<page confidence="0.995445">
398
</page>
<bodyText confidence="0.789343">
classifier bow sent ext emo Fpos Fneg Fneu Fw
</bodyText>
<equation confidence="0.874580222222222">
Multin. NB uni - - - 0.7799 0.6164 0.0498 0.6967
LinearSVM uni - - - 0.7759 0.6046 0.0576 0.6905
MaxEnt uni - - - 0.7974 0.6155 0.0110 0.7059
MaxEnt uni+bi - - - 0.8071 0.6320 0.0222 0.7179
MaxEnt unineg - - - 0.8058 0.6380 0.0110 0.7188
MaxEnt unineg + - - 0.8160 0.6610 0.0317 0.7339
MaxEnt unineg + + - 0.8153 0.6583 0.0316 0.7325
MaxEnt unineg + + + 0.8141 0.6608 0.0330 0.7326
MaxEnt unineg + - + 0.8153 0.6664 0.0331 0.7353
</equation>
<table confidence="0.9945055">
Fpos+neg Acc
0.6981 0.7067
0.6902 0.6949
0.7065 0.7218
0.7195 0.7335
0.7219 0.7342
0.7385 0.7479
0.7368 0.7466
0.7374 0.7468
0.7409 0.7493
</table>
<tableCaption confidence="0.999747">
Table 5: Evaluation results for task A on the Twitter test set
</tableCaption>
<bodyText confidence="0.666027">
classifier bow sent ext emo Fpos Fneg Fneu Fw
</bodyText>
<equation confidence="0.944028888888889">
Multin. NB uni - - - 0.6766 0.6657 0.0213 0.6268
LinearSVM uni - - - 0.6628 0.6533 0.0365 0.6157
MaxEnt uni - - - 0.6829 0.6630 0.0117 0.6277
MaxEnt uni+bi - - - 0.6825 0.6504 0.0230 0.6224
MaxEnt unineg - - - 0.7008 0.6770 0.0120 0.6427
MaxEnt unineg + - - 0.7127 0.6962 0.0238 0.6579
MaxEnt unineg + + - 0.7108 0.6954 0.0238 0.6568
MaxEnt unineg + + + 0.7090 0.7017 0.0237 0.6589
MaxEnt unineg + - + 0.7114 0.7034 0.0238 0.6608
</equation>
<table confidence="0.9964386">
Fpos+neg Acc
0.6712 0.6452
0.6581 0.6290
0.6729 0.6491
0.6665 0.6435
0.6889 0.6654
0.7044 0.6804
0.7031 0.6791
0.7054 0.6808
0.7074 0.6829
</table>
<tableCaption confidence="0.999075">
Table 6: Evaluation results for task A on the SMS test set
</tableCaption>
<bodyText confidence="0.977256833333333">
gold
we could have improved upon our task submission
by excluding the sentiment dictionary extensions –
however, the gains are very small and the system’s
ranks would still be the same (17/28 for the Twitter
data, 16/26 for the SMS data).
</bodyText>
<sectionHeader confidence="0.999776" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.981603">
4.1 Error analysis
4.1.1 Task B: Message Polarity Classification
</subsectionHeader>
<bodyText confidence="0.994538833333333">
The most prominent problem, according to the con-
fusion matrix in Tab. 7, is that a lot of negative mes-
sages are classified as neutral; the same problem
exists to a lesser extent for positive messages.
A qualitative analysis of mis-classified messages
for which the MaxEnt classifier indicated high con-
fidence suggests that the human annotators did not
clearly distinguish between sentiment expressed by
the authors of messages and their own response to
message content. For example, the messages shown
predicted
pos neg neu
</bodyText>
<table confidence="0.534191333333333">
pos 979 352 70 40 523 100
neg 70 47 287 213 244 134
neu 191 191 58 75 1391 942
</table>
<tableCaption confidence="0.99744">
Table 7: Task B, confusion matrix for tweets/SMS
</tableCaption>
<bodyText confidence="0.9916878">
in (1) and (2) report a negative and positive event,
respectively, in a neutral way and should therefore
be annotated with neutral sentiment. However, in the
test data they are labelled as negative and positive by
the human annotators.
</bodyText>
<listItem confidence="0.554192">
(1) MT @LccSy #Syria, Deir Ezzor  |Marba’eh:
</listItem>
<bodyText confidence="0.923342666666667">
Aerial shelling dropped explosive barrels on
residential buildings in the town. Tue, 23
October.
</bodyText>
<page confidence="0.996252">
399
</page>
<figure confidence="0.574407857142857">
able.13 In fact, the learning curve for our system
(Fig. 1) suggests that even as few as 3 000–3 500
labelled messages might be sufficient. The similar
(2) European Exchanges open with a slight
rise: (AGI) Rome, October 24 - Euro-
pean Exchanges opened with a slight ris...
http://t.co/mAljf6eT
</figure>
<bodyText confidence="0.999885083333333">
This problem is probably a major factor in the mis-
classification of many negative and positive messages
as neutral. In order to better reproduce the human
annotations, the system would additionally have to
decide whether a reported event is of a negative, pos-
itive or neutral nature per se – a quite different task
that would require external training data and world
knowledge.
An analysis of mis-classified positive messages
further suggests that certain punctuation marks, espe-
cially multiple exclamation marks, might be useful
as additional features.
</bodyText>
<sectionHeader confidence="0.980101" genericHeader="method">
4.1.2 Task A: Contextual Polarity
Disambiguation
</sectionHeader>
<bodyText confidence="0.922612916666667">
The confusion matrix in Tab. 8 shows that mes-
sages marked as negative in the test data often mis-
classified as positive and vice versa, while neutral
instances are overwhelmingly classified as positive
or negative. This suggests that for the classifiers we
use, there might be too few neutral instances in the
training data (cf. Tab. 1).
predicted
pos neg neu
pos 2329 826 397 239 8 6
neg 550 341 980 761 11 2
neu 109 92 48 65 3 2
</bodyText>
<tableCaption confidence="0.988322">
Table 8: Task A, confusion matrix for tweets/SMS
</tableCaption>
<subsectionHeader confidence="0.962329">
4.2 Conclusion and future work
</subsectionHeader>
<bodyText confidence="0.9998544">
We use a resource-lean approach, relying only on
three external resources: a stemmer, a relatively
small sentiment dictionary and an even smaller list
of emotion markers. Stemmers are already avail-
able for many languages and both kinds of lexical
resources can be gathered relatively easily for other
languages. The list of emotion markers should apply
to most languages. This makes our whole system rel-
atively language-independent, provided that a similar
amount of manually labelled training data is avail-
</bodyText>
<figure confidence="0.663295">
Amount of training data
</figure>
<figureCaption confidence="0.966459">
Figure 1: Learning curve of our system for the “Message
Polarity Classification” task, evaluated on the Twitter data
</figureCaption>
<bodyText confidence="0.99955337037037">
evaluation results for the Twitter and the SMS data
show that not relying on Twitter-specific features like
hashtags pays off: by making our system as generic
as possible, it is robust, not overfitted to the training
data, and generalizes well to other types of data. The
methods discussed in the current paper are particu-
larly well suited to the “Message Polarity Classifica-
tion” task, our system ranking amongst the best. It
turns out, however, that simply applying the same ap-
proach to the “Contextual Polarity Disambiguation”
task yields only mediocre results.
In the future, we would like to experiment with a
couple of additional features. Determining the near-
est neighbors of a message based on Latent Semantic
Analysis might be a useful addition, as might be the
use of part-of-speech tags created by an in-domain
POS tagger (Gimpel et al., 2011)14. We would also
like to find out whether a heuristic treatment of inten-
sifiers and detensifiers, the normalization of character
repetitions, or the inclusion of some punctuation-
based features could further improve classifier per-
formance.
13For task B, even the extended unigram bag-of-words model
by itself, without any additional resources, would have per-
formed quite well as the 9th best constrained system on the
Twitter test set (13th best system overall) and the 5th best system
on the SMS test set.
</bodyText>
<figure confidence="0.941459142857143">
14http://www.ark.cs.cmu.edu/TweetNLP/
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
Score
0.0 0.2 0.4 0.6 0.8 1.0
Fpos+neg
Accuracy
gold
</figure>
<page confidence="0.98882">
400
</page>
<sectionHeader confidence="0.99498" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999851095238095">
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium, Philadelphia,
PA.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Di-
panjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 42–47, Portland,
Oregon. Association for Computational Linguistics.
N. Halko, P. G. Martinsson, and J. A. Tropp. 2009. Find-
ing structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions.
Technical Report 2009-05, ACM, California Institute
of Technology, September.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technologies.
Morgan &amp; Claypool.
Finn Årup Nielsen. 2011. A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ‘Making
Sense of Microposts’: Big things come in small pack-
ages, number 718 in CEUR Workshop Proceedings,
pages 93–98, Heraklion.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in short
strength detection informal text. Journal of the Amer-
ican Society for Information Science and Technology,
61(12):2544–2558.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
SemEval-2013 task 2: Sentiment analysis in Twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013). Association for
Computational Linguistics.
</reference>
<page confidence="0.998641">
401
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.050347">
<title confidence="0.994547">KLUE: Simple and robust methods for polarity classification</title>
<author confidence="0.72007">Proisl Greiner Evert</author>
<email confidence="0.433286">Friedrich-Alexander-Universität</email>
<affiliation confidence="0.350074">Department Germanistik und Professur für</affiliation>
<address confidence="0.614959">Bismarckstr. 91054 Erlangen,</address>
<email confidence="0.998461">thomas.proisl@fau.de</email>
<email confidence="0.998461">paul.greiner@fau.de</email>
<email confidence="0.998461">stefan.evert@fau.de</email>
<email confidence="0.998461">besim.kabashi@fau.de</email>
<abstract confidence="0.998581117647059">This paper describes our approach to the SemEval-2013 task on “Sentiment Analysis in Twitter”. We use simple bag-of-words models, a freely available sentiment dictionary automatically extended with distributionally similar terms, as well as lists of emoticons and internet slang abbreviations in conjunction with fast and robust machine learning algorithms. The resulting system is resource-lean, making it relatively independent of a specific language. Despite its simplicity, the system achieves competitive accuracies of 0.70–0.72 in detecting the sentiment of text messages. We also apply our approach to the task of detecting the contextdependent sentiment of individual words and phrases within a message.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6056" citStr="Brants and Franz, 2006" startWordPosition="946" endWordPosition="949">entiment dictionary contains sentiment ratings ranging from −5 (very negative) to 5 2The regular expression for matching web URLs has been taken from http://daringfireball.net/2010/07/ improved_regex_for_matching_urls. 3http://snowball.tartarus.org/ 4http://www2.imm.dtu.dk/pubdb/p.php?6010 (very positive) for 2476 word forms. In order to obtain a better coverage, we extended the dictionary with distributionally similar words. For this purpose, large-vocabulary distributional semantic models (DSM) were constructed from a version of the English Wikipedia5 and the Google Web 1T 5-Grams database (Brants and Franz, 2006). The Wikipedia DSM consists of 122 281 case-folded word forms as target terms and 30 484 mid-frequency content words (lemmatised) as feature terms; the Web1T5 DSM of 241583 case-folded word forms as target terms and 100 063 case-folded word forms as feature terms. Both DSMs use a context window of two words to the left and right, and were reduced to 300 latent dimensions using randomized singular value decomposition (Halko et al., 2009). For each AFINN entry, the 30 nearest neighbours according to each DSM were considered as extension candidates. Sentiment ratings for the new candidates were </context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 42–47, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Halko</author>
<author>P G Martinsson</author>
<author>J A Tropp</author>
</authors>
<title>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions.</title>
<date>2009</date>
<tech>Technical Report 2009-05,</tech>
<institution>ACM, California Institute of Technology,</institution>
<contexts>
<context position="6497" citStr="Halko et al., 2009" startWordPosition="1020" endWordPosition="1023">e, large-vocabulary distributional semantic models (DSM) were constructed from a version of the English Wikipedia5 and the Google Web 1T 5-Grams database (Brants and Franz, 2006). The Wikipedia DSM consists of 122 281 case-folded word forms as target terms and 30 484 mid-frequency content words (lemmatised) as feature terms; the Web1T5 DSM of 241583 case-folded word forms as target terms and 100 063 case-folded word forms as feature terms. Both DSMs use a context window of two words to the left and right, and were reduced to 300 latent dimensions using randomized singular value decomposition (Halko et al., 2009). For each AFINN entry, the 30 nearest neighbours according to each DSM were considered as extension candidates. Sentiment ratings for the new candidates were computed by averaging over the 30 nearest neighbours of the respective candidate term (with scores set to 0 for all neighbours not listed in AFINN), and rescaling to the range [−5,5].6 After some initial experiments, only candidates with a computed rating &lt; −2.5 or &gt; 2.5 were retained, resulting in an extended dictionary of 2 820 word forms. As with the bag of words model, we make use of a simple heuristic treatment of negation: followin</context>
</contexts>
<marker>Halko, Martinsson, Tropp, 2009</marker>
<rawString>N. Halko, P. G. Martinsson, and J. A. Tropp. 2009. Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions. Technical Report 2009-05, ACM, California Institute of Technology, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool.</publisher>
<contexts>
<context position="1465" citStr="Liu (2012)" startWordPosition="203" endWordPosition="204">s competitive accuracies of 0.70–0.72 in detecting the sentiment of text messages. We also apply our approach to the task of detecting the contextdependent sentiment of individual words and phrases within a message. 1 Introduction The SemEval-2013 task on “Sentiment Analysis in Twitter” (Wilson et al., 2013) focuses on polarity classification, i. e. the problem of determining whether a textual unit, e. g. a document, paragraph, sentence or phrase, expresses a positive, negative or neutral sentiment (for a review of research topics and recent developments in the field of sentiment analysis see Liu (2012)). There are two subtasks: in task B, “Message Polarity Classification”, whole messages have to be classified as being of positive, negative or neutral sentiment; in task A, “Contextual Polarity Disambiguation”, a marked instance of a word or phrase has to be classified in the context of a whole message. The training data for task B consist of approximately 10 200 manually annotated Twitter messages, the training data for task A of approximately 9 500 marked instances in approximately 6 300 Twitter messages.1 The test data consist of in-domain Twitter messages (3 813 messages for task B and 44</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn Årup Nielsen</author>
</authors>
<title>A new ANEW: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things come in small packages, number 718 in CEUR Workshop Proceedings,</booktitle>
<pages>93--98</pages>
<contexts>
<context position="5419" citStr="Nielsen, 2011" startWordPosition="866" endWordPosition="867">xed with not_ (fewer tokens if another negation marker or the end of the message is reached). Finally all words are stemmed using the Snowball stemmer.3 For a token unigram or bigram to be included in the bag of words models, it has to occur in at least five messages. As an additional feature we include the total number of tokens per message. 2.2 Features based on a sentiment dictionary Widely-used algorithms such as SentiStrength (Thelwall et al., 2010) rely heavily on dictionaries containing sentiment ratings of words and/or phrases. We use features based on an extended version of AFINN111 (Nielsen, 2011).4 The AFINN sentiment dictionary contains sentiment ratings ranging from −5 (very negative) to 5 2The regular expression for matching web URLs has been taken from http://daringfireball.net/2010/07/ improved_regex_for_matching_urls. 3http://snowball.tartarus.org/ 4http://www2.imm.dtu.dk/pubdb/p.php?6010 (very positive) for 2476 word forms. In order to obtain a better coverage, we extended the dictionary with distributionally similar words. For this purpose, large-vocabulary distributional semantic models (DSM) were constructed from a version of the English Wikipedia5 and the Google Web 1T 5-Gr</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn Årup Nielsen. 2011. A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things come in small packages, number 718 in CEUR Workshop Proceedings, pages 93–98, Heraklion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="3824" citStr="Pedregosa et al., 2011" startWordPosition="593" endWordPosition="597">the task organizers, ca. 10% of the data were not (or no longer) available. train-B test-B Twitter test-B SMS train-A test-A Twitter test-A SMS 395 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 395–401, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics original messages and a small number of additional resources) and feed them into fast and robust supervised machine learning algorithms implemented in the Python machine learning library scikit-learn (Pedregosa et al., 2011). For task B, the features are computed on the basis of the whole message; for task A, we use essentially the same features, but compute them once for the marked word or phrase and once for the rest of the message. All the features we use are described in some more detail in the following subsections. 2.1 Bag of words We experimented with three different sets of bag-ofwords features: unigrams, unigrams and bigrams, and an extended unigram model that includes a simple treatment of negation. For all three models we simply use the word frequencies as feature weights. Our preprocessing pipeline st</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
<author>Di Cai</author>
<author>Arvid Kappas</author>
</authors>
<title>Sentiment in short strength detection informal text.</title>
<date>2010</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>61</volume>
<issue>12</issue>
<marker>Thelwall, Buckley, Paltoglou, Di Cai, Kappas, 2010</marker>
<rawString>Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas. 2010. Sentiment in short strength detection informal text. Journal of the American Society for Information Science and Technology, 61(12):2544–2558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1164" citStr="Wilson et al., 2013" startWordPosition="152" endWordPosition="155">xtended with distributionally similar terms, as well as lists of emoticons and internet slang abbreviations in conjunction with fast and robust machine learning algorithms. The resulting system is resource-lean, making it relatively independent of a specific language. Despite its simplicity, the system achieves competitive accuracies of 0.70–0.72 in detecting the sentiment of text messages. We also apply our approach to the task of detecting the contextdependent sentiment of individual words and phrases within a message. 1 Introduction The SemEval-2013 task on “Sentiment Analysis in Twitter” (Wilson et al., 2013) focuses on polarity classification, i. e. the problem of determining whether a textual unit, e. g. a document, paragraph, sentence or phrase, expresses a positive, negative or neutral sentiment (for a review of research topics and recent developments in the field of sentiment analysis see Liu (2012)). There are two subtasks: in task B, “Message Polarity Classification”, whole messages have to be classified as being of positive, negative or neutral sentiment; in task A, “Contextual Polarity Disambiguation”, a marked instance of a word or phrase has to be classified in the context of a whole me</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013. SemEval-2013 task 2: Sentiment analysis in Twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013). Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>