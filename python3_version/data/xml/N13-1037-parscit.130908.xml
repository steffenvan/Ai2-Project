<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002188">
<title confidence="0.904279">
What to do about bad language on the internet
</title>
<author confidence="0.933795">
Jacob Eisenstein
</author>
<email confidence="0.816391">
jacobe@gatech.edu
</email>
<affiliation confidence="0.9901735">
School of Interactive Computing
Georgia Institute of Technology
</affiliation>
<sectionHeader confidence="0.987995" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999462642857143">
The rise of social media has brought compu-
tational linguistics in ever-closer contact with
bad language: text that defies our expecta-
tions about vocabulary, spelling, and syntax.
This paper surveys the landscape of bad lan-
guage, and offers a critical review of the NLP
community’s response, which has largely fol-
lowed two paths: normalization and domain
adaptation. Each approach is evaluated in the
context of theoretical and empirical work on
computer-mediated communication. In addi-
tion, the paper presents a quantitative analy-
sis of the lexical diversity of social media text,
and its relationship to other corpora.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9988012">
As social media becomes an increasingly important
application domain for natural language processing,
we encounter language that is substantially different
from many benchmark corpora. The following ex-
amples are all from the social media service Twitter:
</bodyText>
<listItem confidence="0.996425363636364">
• Work on farm Fri. Burning piles of brush
WindyFire got out of control. Thank God for
good naber He help get undr control Pants-
BurnLegWound. (Senator Charles Grassley)
• Boom! Ya ur website suxx bro
(Sarah Silverman)
• ...dats why pluto is pluto it can neva b a star
(Shaquille O’Neil)
• michelle obama great. job. and. whit all my.
respect she. look. great. congrats. to. her.
(Ozzie Guillen)
</listItem>
<bodyText confidence="0.999851529411765">
These examples are selected from celebrities (for
privacy reasons), but they contain linguistic chal-
lenges that are endemic to the medium, including
non-standard punctuation, capitalization, spelling,
vocabulary, and syntax. The consequences for lan-
guage technology are dire: a series of papers has
detailed how state-of-the-art natural language pro-
cessing (NLP) systems perform significantly worse
on social media text. In part-of-speech tagging, the
accuracy of the Stanford tagger (Toutanova et al.,
2003) falls from 97% on Wall Street Journal text to
85% accuracy on Twitter (Gimpel et al., 2011). In
named entity recognition, the CoNLL-trained Stan-
ford recognizer achieves 44% F-measure (Ritter et
al., 2011), down from 86% on the CoNLL test
set (Finkel et al., 2005). In parsing, Foster et al.
(2011) report double-digit decreases in accuracy for
four different state-of-the-art parsers when applied
to social media text.
The application of language technology to so-
cial media is potentially transformative, leveraging
the knowledge and perspectives of millions of peo-
ple. But to deliver on this potential, the problems
at the core of the NLP pipeline must be addressed.
A growing thread of research takes up this chal-
lenge, including a shared task and workshop on
“parsing the web,” with new corpora which appear
to sit somewhere between the Wall Street Journal
and Twitter on the spectrum of bad language (Petrov
and McDonald, 2012). But perhaps surprisingly,
very little of this research has considered why social
media language is so different. This review paper
attempts to shed some light on this question, sur-
veying a strong tradition of empirical and theoreti-
</bodyText>
<page confidence="0.98612">
359
</page>
<note confidence="0.471812">
Proceedings of NAACL-HLT 2013, pages 359–369,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999714342105263">
cal research on computer-mediated communication
(CMC). I argue that the two main computational ap-
proaches to dealing with bad language — normaliza-
tion and domain adaptation — are based on theories
of social media language that are not descriptively
accurate. I have worked and continue to work in
both of these areas, so I make this argument not as
a criticism of others, but in a spirit of self-reflection.
It is hoped that a greater engagement with sociolin-
guistic and CMC research will lead to new, nuanced
approaches to the challenge of bad language.
Why so much Twitter? Most of the examples
in this paper will focus on Twitter, a microblog-
ging service. Munro and Manning (2012) argue
that Twitter has unfairly dominated recent research,
at the expense of email and SMS text messages,
which they found to be both linguistically distinct
from Twitter and significantly more prevalent (in
2010). This matches earlier research arguing that
email contained relatively little “neography,” com-
pared with text messages and chat (Anis, 2007).
A crucial advantage for Twitter is that it is public
by default, while SMS and email are private. This
makes Twitter data less problematic from a privacy
standpoint,1 far easier to obtain, and more amenable
to target applications such as large-scale mining of
events (Sakaki et al., 2010; Benson et al., 2011) and
opinions (Sauper et al., 2011). Similar argument
could be made on behalf of other public social me-
dia, such as blog comments (Ali-Hasan and Adamic,
2007), forums, and chatrooms (Paolillo, 2001). The
main advantage of Twitter over these media is con-
venience in gathering large datasets through a sin-
gle streaming interface. More comparative evalu-
ation is needed to determine linguistic similarities
and differences between Twitter and these other me-
dia; Section 4 presents an evaluation of the lexical
similarity between Twitter and political blogs.
</bodyText>
<sectionHeader confidence="0.865246" genericHeader="method">
2 A tour of bad language
</sectionHeader>
<bodyText confidence="0.9369442">
While many NLP researchers and engineers have
wrestled with the difficulties imposed by bad lan-
guage, there has been relatively little considera-
tion of why language in social media is so differ-
ent from our other corpora. A survey of laypeo-
1boyd and Crawford (2012) note that “public by default”
data still raises important ethical considerations.
ple found that more than half of the respondents
agreed with the following partial explanations for
non-standard spelling on the internet: “people are
unsure of the correct spellings,” “it’s faster,” “it’s be-
come the norm,” and “people want to represent their
own dialects and/or accents” (Jones, 2010). Let us
now consider the evidence for these and other poten-
tial explanations.
</bodyText>
<subsectionHeader confidence="0.940873">
2.1 Illiteracy
</subsectionHeader>
<bodyText confidence="0.999981894736842">
Some commentators have fixated on the proposal
that the authors of non-standard language in social
media are simply unaware or incapable of using
more standard language (Thurlow, 2006). But em-
pirical research suggests that many users of bad lan-
guage are capable of using more traditional forms.
Drouin and Davis (2009) find no significant differ-
ences in the literacy scores of individuals who do
or do not use non-standard vocabulary in text mes-
sages. Tagliamonte and Denis (2008) review traces
of instant messaging conversations among students,
arguing that they “pick and choose ... from the en-
tire stylistic repertoire of the language” in a way that
would be impossible without skilled command of
both formal and informal registers. While news text
is usually more carefully composed and edited than
much of the language in social media, there is little
evidence that bad language results from an inability
to speak anything else.
</bodyText>
<subsectionHeader confidence="0.999229">
2.2 Length limits
</subsectionHeader>
<bodyText confidence="0.999935588235294">
In the case of Twitter, the limit of 140 characters for
each message is frequently cited as an explanation
for bad language (Finin et al., 2010). Does Twitter’s
character limit cause users to prefer shorter words,
such as u instead of you? If so, one might expect
shortening to be used most frequently in messages
that are near the 140-character limit. Using a dataset
of one million English-language tweets (Bamman et
al., 2012), I have computed the average length of
messages containing both standard words and their
non-standard alternatives, focusing on the top five
non-standard shortenings identified by the automatic
method of Gouws et al. (2011a). The shortening ur
can substitute for both your and you’re. While wit
and bout are also spellings for standard words, man-
ual examination of one hundred randomly selected
examples for each surface form revealed only one
</bodyText>
<page confidence="0.996235">
360
</page>
<table confidence="0.999757428571429">
standard length alternative length
your 85.1 f 0.4 ur 81.9 f 0.6
you’re 90.0 f 0.1 wit 78.8 f 0.7
with 87.9 f 0.3 goin 72.2 f 1.0
going 82.7 f 0.5 kno 78.4 f 1.0
know 86.1 f 0.4 bout 74.5 f 0.7
about 88.9 f 0.4
</table>
<tableCaption confidence="0.9851655">
Table 1: Average length of messages containing standard
forms and their shortenings
</tableCaption>
<bodyText confidence="0.999847">
case in which the standard meaning was intended for
wit, and none for bout.
The average message lengths are shown in Ta-
ble 1. In all five cases, the non-standard form tends
to be used in shorter messages — not in long mes-
sages near the 140 character limit. Moreover, this
difference is substantially greater than the saving of
one or two characters offered by shortened form.
This is not consistent with the explanation that Twit-
ter’s character limit is the primary factor driving the
use of shortened forms. It is still possible that Twit-
ter’s length limitations might indirectly cause word
shortenings: for example, by legitimizing shortened
forms or causing authors to develop a habit of pre-
ferring them. But factors other than the length limit
must be recruited to explain why such conventions
or habits apply only to some messages and not oth-
ers.
</bodyText>
<subsectionHeader confidence="0.997981">
2.3 Text input affordances
</subsectionHeader>
<bodyText confidence="0.999935052631579">
Text input affordances — whether standard key-
boards or predictive entry on mobile devices — play
a role in computer-mediated communication that is
perhaps under-appreciated. Gouws et al. (2011b) in-
vestigate orthographic variation on Twitter, and find
differences across devices: for example, that mes-
sages from iPhones include more contractions than
messages from Blackberries, and that tweets sent
from the web browser are more likely to drop vow-
els. While each affordance facilitates some writ-
ing styles and inhibits others, the affordances them-
selves are unevenly distributed across users. For ex-
ample, older people may prefer standard keyboards,
and wealthier people may be more likely to own
iPhones. Affordances are a moving target: new de-
vices and software are constantly becoming avail-
able, the software itself may adapt to the user’s in-
put, and the user may adapt to the software and de-
vice.
</bodyText>
<subsectionHeader confidence="0.993008">
2.4 Pragmatics
</subsectionHeader>
<bodyText confidence="0.999875523809524">
Emoticons are frequently thought of as introduc-
ing an expressive, non-verbal component into writ-
ten language, mirroring the role played by facial ex-
pressions in speech (Walther and D’Addario, 2001),
but they can also be seen as playing a pragmatic
function: marking an utterance as facetious, or
demonstrating a non-confrontational, less invested
stance (Dresner and Herring, 2010). In many cases,
phrasal abbreviations like lol (laugh out loud),
lmao (laughing my ass off), smh (shake my head),
and ikr (i know, right?) play a similar role: yea she
dnt like me lol; lmao I’m playin son. A key differ-
ence from emoticons is that abbreviations can act
as constituents, as in smh at your ignorance. An-
other form of non-standard language is expressive
lengthening (e.g., coooolllllll), found by Brody and
Diakopoulos (2011) to indicate subjectivity and sen-
timent. In running dialogues — such as in online
multiplayer games — the symbols * and ˆ can play
an explicit pragmatic function (Collister, 2011; Col-
lister, 2012).
</bodyText>
<subsectionHeader confidence="0.989436">
2.5 Social variables
</subsectionHeader>
<bodyText confidence="0.999956428571429">
A series of papers has documented the interac-
tions between social media text and social vari-
ables such as age (Burger and Henderson, 2006;
Argamon et al., 2007; Rosenthal and McKeown,
2011), gender (Burger et al., 2011; Rao et al., 2010),
race (Eisenstein et al., 2011), and location (Eisen-
stein et al., 2010; Wing and Baldridge, 2011). From
this literature, it is clear that many of the features
that characterize bad language have strong associa-
tions with specific social variables. In some cases,
these associations mirror linguistic variables known
from speech — such as geographically-associated
lexical items like hella, or transcriptions of phono-
logical variables like “g-dropping” (Eisenstein et al.,
2010). But in other cases, apparently new lexical
items, such as the abbreviations ctfu, lls, and af,
acquire surprisingly strong associations with geo-
graphical areas and demographic groups (Eisenstein
et al., 2011).
A robust finding from the sociolinguistics litera-
ture is that non-standard forms that mark social vari-
</bodyText>
<page confidence="0.99296">
361
</page>
<bodyText confidence="0.999975323943662">
ables, such as regional dialects, are often inhibited in
formal registers (Labov, 1972). For example, while
the Pittsburgh spoken dialect sometimes features the
address term yinz (Johnstone et al., 2006), one would
not expect to find many examples in financial re-
ports. Other investigators have found that much of
the content in Twitter concerns social events and self
presentation (Ramage et al., 2010), which may en-
courage the use of less formal registers in which
socially-marketed language is uninhibited.
The use of non-standard language is often seen
as a form of identity work, signaling authentic-
ity, solidarity, or resistance to norms imposed from
above (Bucholtz and Hall, 2005). In spoken lan-
guage, many of the linguistic variables that perform
identity work are phonological — for example, Eck-
ert (2000) showed how the northern cities vowel
shift was used by a subset of suburban teenagers to
index affiliation with Detroit. The emergence of new
linguistic variables in social media suggests that this
identity work is as necessary in social media as it
is in spoken language. Some of these new variables
are transcriptions of existing spoken language vari-
ables: like finna, which transcribes fixing to. Oth-
ers — abbreviations like ctfu and emoticons — seem
to be linguistic inventions created to meet the needs
of social communication in a new medium. In an
early study of variation in social media, Paolillo
(1999) notes that code-switching between English
and Hindi also performs this type of identity work.
Finally, it is an uncomfortable fact that the text
in many of our most frequently-used corpora was
written and edited predominantly by working-age
white men. The Penn Treebank is composed of
professionally-written news text from 1989, when
minorities comprised 7.5% of the print journalism
workforce; the proportion of women in the journal-
ism workforce was first recorded in 1999, when it
was 37% (American Society of Newspaper Editors,
1999). In contrast, Twitter users in the USA con-
tain an equal proportion of men and women, and
a higher proportion of young adults and minorities
than in the population as a whole (Smith and Brewer,
2012). Such demographic differences are very likely
to lead to differences in language (Green, 2002;
Labov, 2001; Eckert and McConnell-Ginet, 2003).
Overall, the reasons for language diversity in so-
cial media are manifold, though some of the most
frequently cited explanations (illiteracy and length
restrictions) do not hold up to scrutiny. The in-
creasing prevalence of emoticons, phrasal abbrevi-
ations (lol, ctfu), and expressive lengthening may
reflect the increasing use of written language for
ephemeral social interaction, with the concomitant
need for multiple channels through which to express
multiple types of meaning. The fact many such neol-
ogisms are closely circumscribed in geography and
demographics may reflect diffusion through social
networks that are assortative on exactly these dimen-
sions (Backstrom et al., 2010; Thelwall, 2009). But
an additional consideration is that non-standard lan-
guage is deliberately deployed in the performance of
identity work and stancetaking. This seems a partic-
ularly salient explanation for the use of lexical vari-
ables that originate in spoken language (jawn, hella),
and for the orthographic transcription of phonolog-
ical variation (Eisenstein, 2013). Determining the
role and relative importance of social network diffu-
sion and identity work as factors in the diversifica-
tion of social media language is an exciting direction
for future research.
</bodyText>
<sectionHeader confidence="0.875796" genericHeader="method">
3 What can we do about it?
</sectionHeader>
<bodyText confidence="0.99990275">
Having surveyed the landscape of bad language and
its possible causes, let us now turn to the responses
offered by the language technology research com-
munity.
</bodyText>
<subsectionHeader confidence="0.995789">
3.1 Normalization
</subsectionHeader>
<bodyText confidence="0.9991988125">
One approach to dealing with bad language is to
turn it good: “normalizing” social media or SMS
messages to better conform to the sort of language
that our technology expects. Approaches to normal-
ization include the noisy-channel model (Cook and
Stevenson, 2009), string and distributional similar-
ity (Han and Baldwin, 2011; Han et al., 2012), se-
quence labeling (Choudhury et al., 2007; Liu et al.,
2011a), and machine translation (Aw et al., 2006).
As this task has been the focus of substantial atten-
tion in recent years, labeled datasets have become
available and accuracies have climbed.
That said, it is surprisingly difficult to find a
precise definition of the normalization task. Writ-
ing before social media was a significant focus for
NLP, Sproat et al. (2001) proposed to replace non-
</bodyText>
<page confidence="0.991655">
362
</page>
<bodyText confidence="0.99997772">
standard words with “the contextually appropriate
word or sequence of words.” In some cases, this
seems clear enough: we can rewrite dats why pluto
is pluto with that’s why... But it is not difficult to find
cases that are less clear, putting would-be normaliz-
ers in a difficult position. The labeled dataset of Han
and Baldwin (2011) addresses a more tractable sub-
set of the normalization problem, annotating only
token-to-token normalizations. Thus, imma — a
transcription of I’m gonna, which in turn transcribes
I’m going to — is not normalized in this dataset. Ab-
breviations like LOL and WTF are also not normal-
ized, even when they are used to abbreviate syntac-
tic constituents, as in wtf is the matter with you? Nor
are words like hella and jawn normalized, since they
have no obvious one-word transcription in standard
English. These decisions no doubt help to solidify
the reliability of the annotations, but they provide an
overly optimistic impression of the ability of string
edit distance and related similarity-based techniques
to normalize bad language. The resulting gold stan-
dard annotations seem little more amenable to au-
tomated parsing and information extraction than the
original text.
But if we critique normalization for not going
far enough, we must also ask whether it goes too
far. The logic of normalization presupposes that the
“norm” can be identified unambiguously, and that
there is a direct mapping from non-standard words
to the elements in this normal set. On closer exami-
nation, the norm reveals itself to be slippery. Whose
norm are we targeting? Should we normalize flvr to
flavor or flavour? Where does the normal end and
the abnormal begin? For example, Han and Baldwin
normalize ain to ain’t, but not all the way to isn’t.
While ain’t is certainly well-known to speakers of
Standard American English, it does not appear in the
Penn Treebank and probably could not be used in the
Wall Street Journal, except in quotation.
Normalization is often impossible without chang-
ing the meaning of the text. Should we normalize the
final word of ya ur website suxx bro to brother? At
the very least, this adds semantic ambiguity where
there was none before (is she talking to her biolog-
ical brother? or possibly to a monk?). Language
variation does not arise from passing standard text
through a noisy channel; it often serves a pragmatic
and/or stancetaking (Du Bois, 2007) function. Elim-
inating variation would strip those additional lay-
ers of meaning from whatever propositional content
might survive the normalization process. Sarah Sil-
verman’s ya ur website suxx bro can only be under-
stood as a critique from a caricatured persona — the
type of person who ends sentences with bro. Sim-
ilarly, we can assume that Shaquille O’Neil is ca-
pable of writing that’s why Pluto is Pluto, but that
to do so would convey an undesirably didactic and
authoritative stance towards the audience and topic.
This is not to deny that there is great poten-
tial value in research aimed at understanding or-
thographic variation through a combination of lo-
cal context, string similarity, and related finite-state
machinery. Given the productivity of orthographic
substitutions in social media text, it is clear that lan-
guage technology must be made more robust. Nor-
malization may point the way towards such robust-
ness, even if we do not build an explicit normaliza-
tion component directly into the language process-
ing pipeline. Another potential benefit of this re-
search is to better understand the underlying ortho-
graphic processes that lead to the diversity of lan-
guage in social media, how these processes diffuse
over social networks, and how they impact compre-
hensibility for both the target and non-target audi-
ences.
</bodyText>
<subsectionHeader confidence="0.998173">
3.2 Domain adaptation
</subsectionHeader>
<bodyText confidence="0.999968909090909">
Rather than adapting text to fit our tools, we may
instead adapt our tools to fit the text. A series of
papers has followed the mold of “NLP for Twit-
ter,” including part-of-speech tagging (Gimpel et al.,
2011; Owoputi et al., 2013), named entity recogni-
tion (Finin et al., 2010; Ritter et al., 2011; Liu et al.,
2011b), parsing (Foster et al., 2011), dialogue mod-
eling (Ritter et al., 2010) and summarization (Sharifi
et al., 2010). These papers adapt various parts of the
natural language processing pipeline for social me-
dia text, and make use of a range of techniques:
</bodyText>
<listItem confidence="0.984584857142857">
• preprocessing to normalize expressive length-
ening, and eliminate or group all hashtags,
usernames, and URLs (Gimpel et al., 2011;
Foster et al., 2011)
• new labeled data, enabling the application of
semi-supervised learning (Finin et al., 2010;
Gimpel et al., 2011; Ritter et al., 2011)
</listItem>
<page confidence="0.914715">
363
</page>
<listItem confidence="0.998630083333333">
• new annotation schemes specifically cus-
tomized for social media text (Gimpel et al.,
2011)
• self-training on unlabeled social media
text (Foster et al., 2011)
• distributional features to address the sparsity
of bag-of-words features (Gimpel et al., 2011;
Owoputi et al., 2013; Ritter et al., 2011)
• joint normalization, incorporated directly into
downstream application (Liu et al., 2012)
• distant supervision, using named entity on-
tologies and topic models (Ritter et al., 2011)
</listItem>
<bodyText confidence="0.993529666666667">
Only a few of these techniques (normalization and
new annotation systems) are specific to social me-
dia; the rest can found in other domain adaptation
settings. Is domain adaptation appropriate for social
media? Darling et al. (2012) argue that social me-
dia is not a coherent domain at all, and that a POS
tagger for Twitter will not necessarily generalize to
other social media. One can go further: Twitter it-
self is not a unified genre, it is composed of many
different styles and registers, with widely varying
expectations for the degree of standardness and di-
mensions of variation (Androutsopoulos, 2011). I
am the co-author on a paper entitled “Part-of-speech
tagging for Twitter,” but if we take this title literally,
it is impossible on a trivial level: Twitter contains
text in dozens or hundreds of languages, including
many for which no POS tagger exists. Even within
a single language — setting aside issues of code-
switching (Paolillo, 1996) — Twitter and other so-
cial media can contain registers ranging from hash-
tag wordplay (Naaman et al., 2011) to the official
pronouncements of the British Monarchy. And even
if all good language is alike, bad language can be
bad in many different ways — as Androutsopoulos
(2011) notes when contrasting the types of variation
encountered when “visiting a gamer forum” versus
“joining the Twitter profile of a rap star.”
</bodyText>
<sectionHeader confidence="0.930663" genericHeader="method">
4 The lexical coherence of social media
</sectionHeader>
<bodyText confidence="0.997459">
The internal coherence of social media — and its
relationship to other types of text — can be quan-
tified in terms of the similarity of distributions over
bigrams. While there are many techniques for com-
paring word distributions, I apply the relatively sim-
ple method of counting out-of-vocabulary (OOV) bi-
grams. The relationship between OOV rate and do-
main adaptation has been explored by McClosky et
al. (2010), who use it as a feature to predict how well
a parser will perform when applied across domains.2
Specifically, the datasets A and B are compared
by counting the number of bigram tokens in A that
are unseen in B. The following corpora are com-
pared:
</bodyText>
<listItem confidence="0.995367782608696">
• Twitter-month: randomly selected tweets
from each month between January 2010 to Oc-
tober 2012 (Eisenstein et al., 2012).
• Twitter-hour: randomly selected tweets from
each hour of the day, randomly sampled during
the period from January 2010 to October 2012.
• Twitter-#: tweets in which the first token is a
hashtag. The hashtag itself is not included in
the bigram counts; see below for more details
on which bigrams are included.
• Twitter-@: tweets in which the first token is a
username. The username itself is not included
in the bigram counts.
• Penn Treebank: sections 2-21
• Infinite Jest: the text of the 1996 novel by
David Foster Wallace (Wallace, 2012). Con-
sists of only 482,558 tokens.
• Blog articles: A randomly-sampled subset
of the American political blog posts gathered
by Yano et al. (2009).
• Blog comments: A randomly-selected subset
of comments associated with the blog posts de-
scribed above.
</listItem>
<bodyText confidence="0.672486666666667">
In all corpora, only fully alphabetic tokens are
counted; thus, all hashtags and usernames are dis-
carded. The Twitter text is tokenized using Tweet-
</bodyText>
<footnote confidence="0.998683166666667">
2A very recent study compares Twitter with other corpora,
using a number of alternative metrics, such as the use of high
and low frequency words, pronouns, and intensifiers (Hu et al.,
2013). This is complementary to the present study, which fo-
cuses on the degree of difference in the lexical distributions of
corpora gathered from various media.
</footnote>
<page confidence="0.997142">
364
</page>
<figureCaption confidence="0.995377">
Figure 1: Lexical mismatch increases over time, as social
media language evolves.
Figure 2: Different times of day have unique lexical sig-
natures, reflecting differing topics and authors.
</figureCaption>
<figure confidence="0.997337037037037">
5 10 15 20
month gap
with NEs
without NEs
1.035
relative proportion of OOV bigrams
1.030
1.025
1.020
1.015
1.010
1.005
1.000
2 4 6 8 10 12
hour gap
with NEs
without NEs
1.040
relative proportion of OOV bigrams
1.035
1.030
1.025
1.020
1.015
1.010
1.005
1.000
</figure>
<bodyText confidence="0.963827727272728">
motif;3 the Penn Treebank data uses the gold stan-
dard tokenization; Infinite Jest and the blog data are
tokenized using NLTK (Bird et al., 2009). All to-
kens are downcased, and sequences of three or more
consecutive identical characters are reduced to three
characters (e.g., coooool —* coool). All Twitter cor-
pora are subject to the following filters: messages
must be from the United States and should be written
in English,4 they may not include hyperlinks (elim-
inating most marketing messages), they may not be
retweets, and the author must not have more than
1,000 followers or follow more than 1,000 people.
These criteria serve to eliminate text from celebri-
ties, businesses, or automated bots.
Twitter over time Figure 1 shows how the pro-
portion of out-of-vocabulary bigrams increases over
time. It is possible that the core features of language
are constant but the set of named entities that are
mentioned changes over time. To control for this,
the CMU Twitter Part-of-Speech tagger (Owoputi et
al., 2013) was used to identify named entity men-
tions, and they were replaced with a special token.
</bodyText>
<footnote confidence="0.4772">
3https://github.com/brendano/tweetmotif
4Approximate language detection was performed as follows.
</footnote>
<bodyText confidence="0.99878841025641">
We first identify the 1000 most common words, then sort all au-
thors by the proportion of these types that they used, and elim-
inate the bottom 10%. This filtering mechanism eliminates in-
dividuals who never write in English, but a small amount of
foreign language still enters the dataset via code-switching au-
thors. The effect of more advanced language detection meth-
ods (Bergsma et al., 2012) on these results may be considered
in future work.
The OOV rate is standardized with respect to a one-
month time gap, where it is 24.4% when named en-
tities are included, and 21.3% when they are not.
These rates reach maxima at 25.2% and 22.0% re-
spectively, with dips at 12 and 24 months indicat-
ing cyclic yearly effects. While the proportion of
OOV tokens is smaller when named entities are not
included, the rate of growth is similar in each case.
The steadily increasingly rate of OOV bigrams sug-
gests that we cannot annotate our way out of the bad
language problem. An NLP system trained from
data gathered in January 2010 will be increasingly
outdated as time passes and social media language
continues to evolve.
One need not wait months to see language change
on Twitter: marked changes can be observed over
the course of a single day (Golder and Macy, 2011).
A quantitative comparison is shown in Figure 2.
Here the OOV rate is standardized with respect to
a one-hour gap, where it is 24.2% when named en-
tities are included, and 21.1% when they are not.
These rates rise monotonically as the time gap in-
creases, peaking at 25.1% and 21.9% respectively.
Such diurnal changes may reflect the diverse lan-
guage of the different types of authors who post
throughout the day.
Types of usage The Twitter-# and Twitter-0 cor-
pora are designed to capture the diversity of ways
in which social media is used to communicate.
Twitter-# contains tweets that begin with hashtags,
and are thus more likely to be part of running jokes
</bodyText>
<page confidence="0.997808">
365
</page>
<bodyText confidence="0.999905536585366">
or trending topics (Naaman et al., 2011). Twitter-
@ contains tweets that begin with usernames — an
addressing mechanism that is used to maintain dia-
logue threads on the site. These datasets are com-
pared with a set of randomly selected tweets from
June 2011, and with several other corpora: Penn
Treebank, the novel Infinite Jest, and text and com-
ments from political blogs. There was no attempt to
remove named entities from any of these corpora, as
such a comparison would merely reflect the different
accuracy levels of NER in each corpus.
The results are shown in Table 2. A few observa-
tions stand out. First, the Penn Treebank is the clear
outlier: a PTB dictionary has by far the most OOV
tokens for all three Twitter domains and Infinite Jest,
although it is a better match for the blog corpora
than Infinite Jest is. Second, the social media are
fairly internally coherent: the Twitter datasets bet-
ter match each other than any other corpus, with a
maximum OOV rate of 33.4 for Twitter-# against
Twitter-@, though this is significantly higher than
the OOV rate of 27.8 between two separate generic
Twitter samples drawn from the same month. Fi-
nally, the OOV rate increase between Twitter and
blogs — also social media — is substantial. Con-
trary to expectations, the Blog-body corpus was no
closer to the PTB standard than Blog-comment.
These results suggest that the Penn Treebank cor-
pus is so distant from social media that there are in-
deed substantial gains to be reaped by adapting from
news text towards generic Twitter or Blog target do-
mains. The internal differences within these social
media — at least as measured by the distinctions
drawn in Table 2 — are much smaller than the dif-
ferences between these corpora and the PTB stan-
dard. However, in the long run, the effectiveness
of this approach will be limited, as it is clear from
Figure 1 that social media is a moving target. Any
static system that we build today, whether by man-
ual annotation or automated adaptation, will see its
performance decay over time.
</bodyText>
<sectionHeader confidence="0.87431" genericHeader="method">
5 What to do next
</sectionHeader>
<bodyText confidence="0.99998525">
Language is shaped by a constant negotiation be-
tween processes that encourage change and linguis-
tic diversity, and countervailing processes that en-
force existing norms. The decision of the NLP com-
munity to focus so much effort on news text is em-
inently justified on practical grounds, but has unin-
tended consequences not just for technology but for
language itself. By developing software that works
best for standard linguistic forms, we throw the
weight of language technology behind those forms,
and against variants that are preferred by disempow-
ered groups. By adopting a model of “normaliza-
tion,” we declare one version of language to be the
norm, and all others to be outside that norm. By
adopting a model of “domain adaptation,” we con-
fuse a medium with a coherent domain. Adapting
language technology towards the median Tweet can
improve accuracy on average, but it is certain to
leave many forms of language out.
Much of the current research on the relationship
between social media language and metadata has the
goal of using language to predict the metadata —
revealing who is a woman or a man, who is from
Oklahoma or New Jersey, and so on. This perspec-
tive on social variables and personal identity ignores
the local categories that are often more linguisti-
cally salient (Eckert, 2008); worse, it strips individ-
uals of any agency in using language as a resource
to create and shape their identity (Coupland, 2007),
and conceals the role that language plays in creating
and perpetuating categories like gender (Bucholtz
and Hall, 2005). An alternative possibility is to re-
verse the relationship between language and meta-
data, using metadata to achieve a more flexible and
heterogeneous domain adaptation that is sensitive to
the social factors that shape variation. Such a re-
versal would help language technology to move be-
yond false dichotomies between normal and abnor-
mal text, source and target domains, and good and
bad language.
</bodyText>
<sectionHeader confidence="0.984418" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998298222222222">
This paper benefitted from discussions with David
Bamman, Natalia Cecire, Micha Elsner, Sharon
Goldwater, Scott Kiesling, Brendan O’Connor,
Tyler Schnoebelen, and Yi Yang. Many thanks to
Brendan O’Connor and David Bamman for provid-
ing Twitter datasets, Tae Yano for the blog com-
ment dataset, and Byron Wallace for the Infinite Jest
dataset. Thanks also to the anonymous reviewers for
their helpful feedback.
</bodyText>
<page confidence="0.996463">
366
</page>
<table confidence="0.9998955">
Tw-June Tw-0 Tw-# Blog-body Blog-comment Infinite-Jest PTB
Tw-June 28.7 29.3 47.1 48.6 54.0 63.9
Tw-0 25.9 29.7 47.8 49.9 56.3 66.4
Tw-# 29.8 33.4 49.6 51.0 54.7 66.2
Blog-body 41.9 44.1 43.8 27.2 49.1 48.0
Blog-comment 47.4 49.6 49.2 30.2 53.0 48.4
Infinite-Jest 49.4 51.1 49.9 48.3 47.4 55.5
PTB 72.2 73.1 72.7 64.5 61.9 71.9
</table>
<tableCaption confidence="0.9149035">
Table 2: Percent OOV bigram tokens across corpora. Rows are the dataset providing the tokens, columns are the
dataset providing the dictionary.
</tableCaption>
<sectionHeader confidence="0.975709" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996577487179487">
Noor Ali-Hasan and Lada Adamic. 2007. Expressing so-
cial relationships on the blog through links and com-
ments. In Proceedings of ICWSM.
American Society of Newspaper Editors. 1999. 1999
Newsroom Census: Minority Employment Inches up in
Daily Newspapers. American Society of Newspaper
Editors, Reston, VA.
Jannis Androutsopoulos. 2011. Language change and
digital media: a review of conceptions and evidence.
In Nikolas Coupland and Tore Kristiansen, editors,
Standard Languages and Language Standards in a
Changing Europe. Novus, Oslo.
Jacques Anis. 2007. Neography: Unconventional
spelling in French SMS text messages. In Brenda
Danet and Susan C. Herring, editors, The multilingual
internet: Language, culture, and communication on-
line, pages 87 – 115. Oxford University Press.
Shlomo Argamon, Moshe Koppel, James W. Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
age, gender, and the varieties of self-expression. First
Monday, 12(9).
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of ACL, pages 33–40.
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical prediction
with social and spatial proximity. In Proceedings of
WWW, pages 61–70.
David Bamman, Jacob Eisenstein, and Tyler Schnoebe-
len. 2012. Gender in twitter: Styles, stances, and
social networks. Technical Report 1210.4567, arXiv,
October.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of ACL.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65–74, June.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural language processing with Python. O’Reilly Me-
dia, Incorporated.
danah boyd and Kate Crawford. 2012. Critical questions
for big data. Information, Communication &amp; Society,
15(5):662–679, May.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP.
Mary Bucholtz and Kira Hall. 2005. Identity and inter-
action: A sociocultural linguistic approach. Discourse
studies, 7(4-5):585–614.
John D. Burger and John C. Henderson. 2006. An explo-
ration of observable features related to blogger age. In
AAAI Spring Symposium: Computational Approaches
to Analyzing Weblogs.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on twit-
ter. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157–174.
Lauren B. Collister. 2011. *-repair in online discourse.
Journal of Pragmatics, 43(3):918–921, February.
Lauren B. Collister. 2012. The discourse deictics ˆ and
&lt;-- in a world of warcraft community. Discourse,
Context &amp; Media, 1(1):9–19, March.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text message normalization. In Pro-
ceedings of the NAACL-HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71–
78.
Nikolas Coupland. 2007. Style (Key Topics in Sociolin-
guistics). Cambridge University Press, July.
</reference>
<page confidence="0.992324">
367
</page>
<reference confidence="0.998302076190476">
William M. Darling, Michael J. Paul, and Fei Song.
2012. Unsupervised part-of-speech tagging in
noisy and esoteric domains with a syntactic-semantic
bayesian hmm. In Proceedings of EACL Workshop on
Semantic Analysis in Social Media.
Eli Dresner and Susan C. Herring. 2010. Functions of
the non-verbal in cmc: Emoticons and illocutionary
force. Communication Theory, 20(3):249–268.
Michelle Drouin and Claire Davis. 2009. R u txting? is
the use of text speak hurting your literacy? Journal of
Literacy Research, 41(1):46–67.
John W. Du Bois. 2007. The stance triangle. In Robert
Engelbretson, editor, Stancetaking in discourse, pages
139–182. John Benjamins Publishing Company, Ams-
terdam/Philadelphia.
Penelope Eckert and Sally McConnell-Ginet. 2003. Lan-
guage and Gender. Cambridge University Press, New
York.
Penelope Eckert. 2000. Linguistic variation as social
practice. Blackwell.
Penelope Eckert. 2008. Variation and the indexical field.
Journal of Sociolinguistics, 12(4):453–476.
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Proceedings of EMNLP.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of ACL.
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2012. Mapping the geographical
diffusion of new words. Technical Report 1210.5268,
arXiv.
Jacob Eisenstein. 2013. Phonological factors in social
media writing. In Proceedings of the NAACL Work-
shop on Language Analysis in Social Media.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
Proceedings of ACL, pages 363–370.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011. From news to comment:
Resources and benchmarks for parsing the language
of web 2.0. In Proceedings of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In Pro-
ceedings of ACL.
Scott A. Golder and Michael W. Macy. 2011. Di-
urnal and seasonal mood vary with work, sleep,
and daylength across diverse cultures. Science,
333(6051):1878–1881, September.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011a.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, pages 82–90, July.
Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011b. Contextual bearing on linguistic
variation in social media. In Proceedings of the ACL
Workshop on Language in Social Media.
Lisa Green. 2002. African American English. Cam-
bridge University Press.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a# twitter. In
Proceedings of ACL, volume 1.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Auto-
matically constructing a normalisation dictionary for
microblogs. In Proceedings of EMNLP.
Yuheng Hu, Kartik Talamadupula, and Subbarao Kamb-
hampati. 2013. Dude, srsly?: The surprisingly for-
mal nature of twitter’s language. In Proceedings of
ICWSM.
Barbara Johnstone, Jennifer Andrus, and Andrew E
Danielson. 2006. Mobility, indexicality, and the en-
registerment of pittsburghese. Journal of English Lin-
guistics, 34(2):77–104.
Lucy Jones. 2010. The changing face of spelling on the
internet. Technical report, The English Spelling Soci-
ety.
William Labov. 1972. Sociolinguistic patterns.
Philadelphia: University of Pennsylvania Press.
William Labov. 2001. Principles of linguistic change.
Vol.2 : Social factors. Blackwell Publishers, Oxford.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011a. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of ACL, pages 71–76.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In Proceedings of ACL.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang
Fu, and Furu Wei. 2012. Joint inference of named en-
tity recognition and normalization for tweets. In Pro-
ceedings of ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
Proceedings of NAACL, pages 28–36, June.
</reference>
<page confidence="0.983172">
368
</page>
<reference confidence="0.999870267441861">
Robert Munro and Christopher D. Manning. 2012.
Short message communications: users, topics, and in-
language processing. In Proceedings of the 2nd ACM
Symposium on Computing for Development.
Mor Naaman, Hila Becker, and Luis Gravano. 2011. Hip
and trendy: Characterizing emerging trends on twit-
ter. Journal of the American Society for Information
Science and Technology, 62(5):902–918.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL.
John C. Paolillo. 1996. Language choice on
soc.culture.punjab. Electronic Journal of Communica-
tion/La Revue Electronique de Communication, 6(3).
John C. Paolillo. 1999. The virtual speech community:
Social network and language variation on irc. Journal
of Computer-Mediated Communication, 4(4):0.
John C. Paolillo. 2001. Language variation on internet
relay chat: A social network approach. Journal of So-
ciolinguistics, 5(2):180–213.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Daniel Ramage, Sue Dumais, and D. Liebling. 2010.
Characterizing microblogs with topic models. In Pro-
ceedings of ICWSM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of Workshop on
Search and mining user-generated contents.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of NAACL.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an experi-
mental study. In Proceedings of EMNLP.
Sara Rosenthal and Kathleen McKeown. 2011. Age pre-
diction in blogs: A study of style, content, and online
behavior in pre- and Post-Social media generations. In
Proceedings of ACL.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time event
detection by social sensors. In Proceedings of WWW,
pages 851–860.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2011. Content models with attitude. In Proceedings
of ACL.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proceedings of NAACL.
Aaron Smith and Joanna Brewer. 2012. Twitter use
2012. Technical report, Pew Research Center, May.
Richard Sproat, Alan W Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech &amp; Language, 15(3):287–333.
Sali A. Tagliamonte and Derek Denis. 2008. Linguistic
ruin? lol! instant messaging and teen language. Amer-
ican Speech, 83(1):3–34, March.
Mike Thelwall. 2009. Homophily in MySpace. J. Am.
Soc. Inf. Sci., 60(2):219–231.
Crispin Thurlow. 2006. From statistical panic to moral
panic: The metadiscursive construction and popular
exaggeration of new media language in the print me-
dia. J. Computer-Mediated Communication, pages
667–701.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
Byron Wallace. 2012. Multiple narrative disentangle-
ment: Unraveling infinite jest. In Proceedings of
NAACL.
Joseph B. Walther and Kyle P. D’Addario. 2001. The
impacts of emoticons on message interpretation in
computer-mediated communication. Social Science
Computer Review, 19(3):324–347.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proceedings of ACL.
Tae Yano, William W. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of NAACL.
</reference>
<page confidence="0.999161">
369
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955294">
<title confidence="0.991649">What to do about bad language on the internet</title>
<author confidence="0.998509">Jacob</author>
<affiliation confidence="0.999934">School of Interactive Georgia Institute of Technology</affiliation>
<abstract confidence="0.997536866666667">The rise of social media has brought computational linguistics in ever-closer contact with text that defies our expectations about vocabulary, spelling, and syntax. This paper surveys the landscape of bad language, and offers a critical review of the NLP community’s response, which has largely followed two paths: normalization and domain adaptation. Each approach is evaluated in the context of theoretical and empirical work on computer-mediated communication. In addition, the paper presents a quantitative analysis of the lexical diversity of social media text, and its relationship to other corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Noor Ali-Hasan</author>
<author>Lada Adamic</author>
</authors>
<title>Expressing social relationships on the blog through links and comments.</title>
<date>2007</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="4752" citStr="Ali-Hasan and Adamic, 2007" startWordPosition="747" endWordPosition="750">evalent (in 2010). This matches earlier research arguing that email contained relatively little “neography,” compared with text messages and chat (Anis, 2007). A crucial advantage for Twitter is that it is public by default, while SMS and email are private. This makes Twitter data less problematic from a privacy standpoint,1 far easier to obtain, and more amenable to target applications such as large-scale mining of events (Sakaki et al., 2010; Benson et al., 2011) and opinions (Sauper et al., 2011). Similar argument could be made on behalf of other public social media, such as blog comments (Ali-Hasan and Adamic, 2007), forums, and chatrooms (Paolillo, 2001). The main advantage of Twitter over these media is convenience in gathering large datasets through a single streaming interface. More comparative evaluation is needed to determine linguistic similarities and differences between Twitter and these other media; Section 4 presents an evaluation of the lexical similarity between Twitter and political blogs. 2 A tour of bad language While many NLP researchers and engineers have wrestled with the difficulties imposed by bad language, there has been relatively little consideration of why language in social medi</context>
</contexts>
<marker>Ali-Hasan, Adamic, 2007</marker>
<rawString>Noor Ali-Hasan and Lada Adamic. 2007. Expressing social relationships on the blog through links and comments. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<title>of Newspaper Editors.</title>
<date>1999</date>
<publisher>American Society</publisher>
<location>Reston, VA.</location>
<contexts>
<context position="13334" citStr="(1999)" startWordPosition="2143" endWordPosition="2143"> how the northern cities vowel shift was used by a subset of suburban teenagers to index affiliation with Detroit. The emergence of new linguistic variables in social media suggests that this identity work is as necessary in social media as it is in spoken language. Some of these new variables are transcriptions of existing spoken language variables: like finna, which transcribes fixing to. Others — abbreviations like ctfu and emoticons — seem to be linguistic inventions created to meet the needs of social communication in a new medium. In an early study of variation in social media, Paolillo (1999) notes that code-switching between English and Hindi also performs this type of identity work. Finally, it is an uncomfortable fact that the text in many of our most frequently-used corpora was written and edited predominantly by working-age white men. The Penn Treebank is composed of professionally-written news text from 1989, when minorities comprised 7.5% of the print journalism workforce; the proportion of women in the journalism workforce was first recorded in 1999, when it was 37% (American Society of Newspaper Editors, 1999). In contrast, Twitter users in the USA contain an equal propor</context>
</contexts>
<marker>1999</marker>
<rawString>American Society of Newspaper Editors. 1999. 1999 Newsroom Census: Minority Employment Inches up in Daily Newspapers. American Society of Newspaper Editors, Reston, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jannis Androutsopoulos</author>
</authors>
<title>Language change and digital media: a review of conceptions and evidence.</title>
<date>2011</date>
<booktitle>In Nikolas Coupland and Tore Kristiansen, editors, Standard Languages and Language Standards in a Changing Europe. Novus,</booktitle>
<location>Oslo.</location>
<contexts>
<context position="22177" citStr="Androutsopoulos, 2011" startWordPosition="3578" endWordPosition="3579">et al., 2011) Only a few of these techniques (normalization and new annotation systems) are specific to social media; the rest can found in other domain adaptation settings. Is domain adaptation appropriate for social media? Darling et al. (2012) argue that social media is not a coherent domain at all, and that a POS tagger for Twitter will not necessarily generalize to other social media. One can go further: Twitter itself is not a unified genre, it is composed of many different styles and registers, with widely varying expectations for the degree of standardness and dimensions of variation (Androutsopoulos, 2011). I am the co-author on a paper entitled “Part-of-speech tagging for Twitter,” but if we take this title literally, it is impossible on a trivial level: Twitter contains text in dozens or hundreds of languages, including many for which no POS tagger exists. Even within a single language — setting aside issues of codeswitching (Paolillo, 1996) — Twitter and other social media can contain registers ranging from hashtag wordplay (Naaman et al., 2011) to the official pronouncements of the British Monarchy. And even if all good language is alike, bad language can be bad in many different ways — as </context>
</contexts>
<marker>Androutsopoulos, 2011</marker>
<rawString>Jannis Androutsopoulos. 2011. Language change and digital media: a review of conceptions and evidence. In Nikolas Coupland and Tore Kristiansen, editors, Standard Languages and Language Standards in a Changing Europe. Novus, Oslo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Anis</author>
</authors>
<title>Neography: Unconventional spelling in French SMS text messages.</title>
<date>2007</date>
<booktitle>The multilingual internet: Language, culture, and communication online,</booktitle>
<pages>87--115</pages>
<editor>In Brenda Danet and Susan C. Herring, editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="4283" citStr="Anis, 2007" startWordPosition="670" endWordPosition="671">greater engagement with sociolinguistic and CMC research will lead to new, nuanced approaches to the challenge of bad language. Why so much Twitter? Most of the examples in this paper will focus on Twitter, a microblogging service. Munro and Manning (2012) argue that Twitter has unfairly dominated recent research, at the expense of email and SMS text messages, which they found to be both linguistically distinct from Twitter and significantly more prevalent (in 2010). This matches earlier research arguing that email contained relatively little “neography,” compared with text messages and chat (Anis, 2007). A crucial advantage for Twitter is that it is public by default, while SMS and email are private. This makes Twitter data less problematic from a privacy standpoint,1 far easier to obtain, and more amenable to target applications such as large-scale mining of events (Sakaki et al., 2010; Benson et al., 2011) and opinions (Sauper et al., 2011). Similar argument could be made on behalf of other public social media, such as blog comments (Ali-Hasan and Adamic, 2007), forums, and chatrooms (Paolillo, 2001). The main advantage of Twitter over these media is convenience in gathering large datasets</context>
</contexts>
<marker>Anis, 2007</marker>
<rawString>Jacques Anis. 2007. Neography: Unconventional spelling in French SMS text messages. In Brenda Danet and Susan C. Herring, editors, The multilingual internet: Language, culture, and communication online, pages 87 – 115. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Moshe Koppel</author>
<author>James W Pennebaker</author>
<author>Jonathan Schler</author>
</authors>
<title>Mining the blogosphere: age, gender, and the varieties of self-expression.</title>
<date>2007</date>
<journal>First Monday,</journal>
<volume>12</volume>
<issue>9</issue>
<contexts>
<context position="11033" citStr="Argamon et al., 2007" startWordPosition="1778" endWordPosition="1781">layin son. A key difference from emoticons is that abbreviations can act as constituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many of the features that characterize bad language have strong associations with specific social variables. In some cases, these associations mirror linguistic variables known from speech — such as geographically-associated lexical items like hella, or transcriptions of phonological variables like “g-dropping” (Eisenstein et al., 2010). But in other cases, apparently new lexical </context>
</contexts>
<marker>Argamon, Koppel, Pennebaker, Schler, 2007</marker>
<rawString>Shlomo Argamon, Moshe Koppel, James W. Pennebaker, and Jonathan Schler. 2007. Mining the blogosphere: age, gender, and the varieties of self-expression. First Monday, 12(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for SMS text normalization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="16119" citStr="Aw et al., 2006" startWordPosition="2574" endWordPosition="2577">rveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace non362 standard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewrite dats why pluto is pluto with that’s why... But it is not difficult to find cases that are less clear, putting would-be</context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for SMS text normalization. In Proceedings of ACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Backstrom</author>
<author>Eric Sun</author>
<author>Cameron Marlow</author>
</authors>
<title>Find me if you can: improving geographical prediction with social and spatial proximity.</title>
<date>2010</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>61--70</pages>
<contexts>
<context position="14898" citStr="Backstrom et al., 2010" startWordPosition="2381" endWordPosition="2384">e manifold, though some of the most frequently cited explanations (illiteracy and length restrictions) do not hold up to scrutiny. The increasing prevalence of emoticons, phrasal abbreviations (lol, ctfu), and expressive lengthening may reflect the increasing use of written language for ephemeral social interaction, with the concomitant need for multiple channels through which to express multiple types of meaning. The fact many such neologisms are closely circumscribed in geography and demographics may reflect diffusion through social networks that are assortative on exactly these dimensions (Backstrom et al., 2010; Thelwall, 2009). But an additional consideration is that non-standard language is deliberately deployed in the performance of identity work and stancetaking. This seems a particularly salient explanation for the use of lexical variables that originate in spoken language (jawn, hella), and for the orthographic transcription of phonological variation (Eisenstein, 2013). Determining the role and relative importance of social network diffusion and identity work as factors in the diversification of social media language is an exciting direction for future research. 3 What can we do about it? Havi</context>
</contexts>
<marker>Backstrom, Sun, Marlow, 2010</marker>
<rawString>Lars Backstrom, Eric Sun, and Cameron Marlow. 2010. Find me if you can: improving geographical prediction with social and spatial proximity. In Proceedings of WWW, pages 61–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Jacob Eisenstein</author>
<author>Tyler Schnoebelen</author>
</authors>
<title>Gender in twitter: Styles, stances, and social networks.</title>
<date>2012</date>
<tech>Technical Report 1210.4567, arXiv,</tech>
<contexts>
<context position="7297" citStr="Bamman et al., 2012" startWordPosition="1157" endWordPosition="1160">y more carefully composed and edited than much of the language in social media, there is little evidence that bad language results from an inability to speak anything else. 2.2 Length limits In the case of Twitter, the limit of 140 characters for each message is frequently cited as an explanation for bad language (Finin et al., 2010). Does Twitter’s character limit cause users to prefer shorter words, such as u instead of you? If so, one might expect shortening to be used most frequently in messages that are near the 140-character limit. Using a dataset of one million English-language tweets (Bamman et al., 2012), I have computed the average length of messages containing both standard words and their non-standard alternatives, focusing on the top five non-standard shortenings identified by the automatic method of Gouws et al. (2011a). The shortening ur can substitute for both your and you’re. While wit and bout are also spellings for standard words, manual examination of one hundred randomly selected examples for each surface form revealed only one 360 standard length alternative length your 85.1 f 0.4 ur 81.9 f 0.6 you’re 90.0 f 0.1 wit 78.8 f 0.7 with 87.9 f 0.3 goin 72.2 f 1.0 going 82.7 f 0.5 kno </context>
</contexts>
<marker>Bamman, Eisenstein, Schnoebelen, 2012</marker>
<rawString>David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. 2012. Gender in twitter: Styles, stances, and social networks. Technical Report 1210.4567, arXiv, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Benson</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Event discovery in social media feeds.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4594" citStr="Benson et al., 2011" startWordPosition="720" endWordPosition="723">ent research, at the expense of email and SMS text messages, which they found to be both linguistically distinct from Twitter and significantly more prevalent (in 2010). This matches earlier research arguing that email contained relatively little “neography,” compared with text messages and chat (Anis, 2007). A crucial advantage for Twitter is that it is public by default, while SMS and email are private. This makes Twitter data less problematic from a privacy standpoint,1 far easier to obtain, and more amenable to target applications such as large-scale mining of events (Sakaki et al., 2010; Benson et al., 2011) and opinions (Sauper et al., 2011). Similar argument could be made on behalf of other public social media, such as blog comments (Ali-Hasan and Adamic, 2007), forums, and chatrooms (Paolillo, 2001). The main advantage of Twitter over these media is convenience in gathering large datasets through a single streaming interface. More comparative evaluation is needed to determine linguistic similarities and differences between Twitter and these other media; Section 4 presents an evaluation of the lexical similarity between Twitter and political blogs. 2 A tour of bad language While many NLP resear</context>
</contexts>
<marker>Benson, Haghighi, Barzilay, 2011</marker>
<rawString>Edward Benson, Aria Haghighi, and Regina Barzilay. 2011. Event discovery in social media feeds. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Paul McNamee</author>
<author>Mossaab Bagdouri</author>
<author>Clayton Fink</author>
<author>Theresa Wilson</author>
</authors>
<title>Language identification for creating language-specific twitter collections.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media,</booktitle>
<pages>65--74</pages>
<contexts>
<context position="27095" citStr="Bergsma et al., 2012" startWordPosition="4395" endWordPosition="4398">t-of-Speech tagger (Owoputi et al., 2013) was used to identify named entity mentions, and they were replaced with a special token. 3https://github.com/brendano/tweetmotif 4Approximate language detection was performed as follows. We first identify the 1000 most common words, then sort all authors by the proportion of these types that they used, and eliminate the bottom 10%. This filtering mechanism eliminates individuals who never write in English, but a small amount of foreign language still enters the dataset via code-switching authors. The effect of more advanced language detection methods (Bergsma et al., 2012) on these results may be considered in future work. The OOV rate is standardized with respect to a onemonth time gap, where it is 24.4% when named entities are included, and 21.3% when they are not. These rates reach maxima at 25.2% and 22.0% respectively, with dips at 12 and 24 months indicating cyclic yearly effects. While the proportion of OOV tokens is smaller when named entities are not included, the rate of growth is similar in each case. The steadily increasingly rate of OOV bigrams suggests that we cannot annotate our way out of the bad language problem. An NLP system trained from data</context>
</contexts>
<marker>Bergsma, McNamee, Bagdouri, Fink, Wilson, 2012</marker>
<rawString>Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identification for creating language-specific twitter collections. In Proceedings of the Second Workshop on Language in Social Media, pages 65–74, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural language processing with Python. O’Reilly</title>
<date>2009</date>
<location>Media, Incorporated.</location>
<contexts>
<context position="25644" citStr="Bird et al., 2009" startWordPosition="4162" endWordPosition="4165">red from various media. 364 Figure 1: Lexical mismatch increases over time, as social media language evolves. Figure 2: Different times of day have unique lexical signatures, reflecting differing topics and authors. 5 10 15 20 month gap with NEs without NEs 1.035 relative proportion of OOV bigrams 1.030 1.025 1.020 1.015 1.010 1.005 1.000 2 4 6 8 10 12 hour gap with NEs without NEs 1.040 relative proportion of OOV bigrams 1.035 1.030 1.025 1.020 1.015 1.010 1.005 1.000 motif;3 the Penn Treebank data uses the gold standard tokenization; Infinite Jest and the blog data are tokenized using NLTK (Bird et al., 2009). All tokens are downcased, and sequences of three or more consecutive identical characters are reduced to three characters (e.g., coooool —* coool). All Twitter corpora are subject to the following filters: messages must be from the United States and should be written in English,4 they may not include hyperlinks (eliminating most marketing messages), they may not be retweets, and the author must not have more than 1,000 followers or follow more than 1,000 people. These criteria serve to eliminate text from celebrities, businesses, or automated bots. Twitter over time Figure 1 shows how the pr</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python. O’Reilly Media, Incorporated.</rawString>
</citation>
<citation valid="true">
<authors>
<author>danah boyd</author>
<author>Kate Crawford</author>
</authors>
<title>Critical questions for big data.</title>
<date>2012</date>
<journal>Information, Communication &amp; Society,</journal>
<volume>15</volume>
<issue>5</issue>
<contexts>
<context position="5437" citStr="boyd and Crawford (2012)" startWordPosition="856" endWordPosition="860">of Twitter over these media is convenience in gathering large datasets through a single streaming interface. More comparative evaluation is needed to determine linguistic similarities and differences between Twitter and these other media; Section 4 presents an evaluation of the lexical similarity between Twitter and political blogs. 2 A tour of bad language While many NLP researchers and engineers have wrestled with the difficulties imposed by bad language, there has been relatively little consideration of why language in social media is so different from our other corpora. A survey of laypeo1boyd and Crawford (2012) note that “public by default” data still raises important ethical considerations. ple found that more than half of the respondents agreed with the following partial explanations for non-standard spelling on the internet: “people are unsure of the correct spellings,” “it’s faster,” “it’s become the norm,” and “people want to represent their own dialects and/or accents” (Jones, 2010). Let us now consider the evidence for these and other potential explanations. 2.1 Illiteracy Some commentators have fixated on the proposal that the authors of non-standard language in social media are simply unawa</context>
</contexts>
<marker>boyd, Crawford, 2012</marker>
<rawString>danah boyd and Kate Crawford. 2012. Critical questions for big data. Information, Communication &amp; Society, 15(5):662–679, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Nicholas Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="10655" citStr="Brody and Diakopoulos (2011)" startWordPosition="1714" endWordPosition="1717"> 2001), but they can also be seen as playing a pragmatic function: marking an utterance as facetious, or demonstrating a non-confrontational, less invested stance (Dresner and Herring, 2010). In many cases, phrasal abbreviations like lol (laugh out loud), lmao (laughing my ass off), smh (shake my head), and ikr (i know, right?) play a similar role: yea she dnt like me lol; lmao I’m playin son. A key difference from emoticons is that abbreviations can act as constituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many </context>
</contexts>
<marker>Brody, Diakopoulos, 2011</marker>
<rawString>Samuel Brody and Nicholas Diakopoulos. 2011. Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Bucholtz</author>
<author>Kira Hall</author>
</authors>
<title>Identity and interaction: A sociocultural linguistic approach.</title>
<date>2005</date>
<booktitle>Discourse studies,</booktitle>
<pages>7--4</pages>
<contexts>
<context position="12594" citStr="Bucholtz and Hall, 2005" startWordPosition="2017" endWordPosition="2020">registers (Labov, 1972). For example, while the Pittsburgh spoken dialect sometimes features the address term yinz (Johnstone et al., 2006), one would not expect to find many examples in financial reports. Other investigators have found that much of the content in Twitter concerns social events and self presentation (Ramage et al., 2010), which may encourage the use of less formal registers in which socially-marketed language is uninhibited. The use of non-standard language is often seen as a form of identity work, signaling authenticity, solidarity, or resistance to norms imposed from above (Bucholtz and Hall, 2005). In spoken language, many of the linguistic variables that perform identity work are phonological — for example, Eckert (2000) showed how the northern cities vowel shift was used by a subset of suburban teenagers to index affiliation with Detroit. The emergence of new linguistic variables in social media suggests that this identity work is as necessary in social media as it is in spoken language. Some of these new variables are transcriptions of existing spoken language variables: like finna, which transcribes fixing to. Others — abbreviations like ctfu and emoticons — seem to be linguistic i</context>
<context position="32220" citStr="Bucholtz and Hall, 2005" startWordPosition="5284" endWordPosition="5287">uch of the current research on the relationship between social media language and metadata has the goal of using language to predict the metadata — revealing who is a woman or a man, who is from Oklahoma or New Jersey, and so on. This perspective on social variables and personal identity ignores the local categories that are often more linguistically salient (Eckert, 2008); worse, it strips individuals of any agency in using language as a resource to create and shape their identity (Coupland, 2007), and conceals the role that language plays in creating and perpetuating categories like gender (Bucholtz and Hall, 2005). An alternative possibility is to reverse the relationship between language and metadata, using metadata to achieve a more flexible and heterogeneous domain adaptation that is sensitive to the social factors that shape variation. Such a reversal would help language technology to move beyond false dichotomies between normal and abnormal text, source and target domains, and good and bad language. Acknowledgments This paper benefitted from discussions with David Bamman, Natalia Cecire, Micha Elsner, Sharon Goldwater, Scott Kiesling, Brendan O’Connor, Tyler Schnoebelen, and Yi Yang. Many thanks t</context>
</contexts>
<marker>Bucholtz, Hall, 2005</marker>
<rawString>Mary Bucholtz and Kira Hall. 2005. Identity and interaction: A sociocultural linguistic approach. Discourse studies, 7(4-5):585–614.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John C Henderson</author>
</authors>
<title>An exploration of observable features related to blogger age. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs.</title>
<date>2006</date>
<contexts>
<context position="11011" citStr="Burger and Henderson, 2006" startWordPosition="1774" endWordPosition="1777"> dnt like me lol; lmao I’m playin son. A key difference from emoticons is that abbreviations can act as constituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many of the features that characterize bad language have strong associations with specific social variables. In some cases, these associations mirror linguistic variables known from speech — such as geographically-associated lexical items like hella, or transcriptions of phonological variables like “g-dropping” (Eisenstein et al., 2010). But in other cases, a</context>
</contexts>
<marker>Burger, Henderson, 2006</marker>
<rawString>John D. Burger and John C. Henderson. 2006. An exploration of observable features related to blogger age. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John C Henderson</author>
<author>George Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating gender on twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11092" citStr="Burger et al., 2011" startWordPosition="1787" endWordPosition="1790">ions can act as constituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many of the features that characterize bad language have strong associations with specific social variables. In some cases, these associations mirror linguistic variables known from speech — such as geographically-associated lexical items like hella, or transcriptions of phonological variables like “g-dropping” (Eisenstein et al., 2010). But in other cases, apparently new lexical items, such as the abbreviations ctfu, lls, and af, acquire</context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>John D. Burger, John C. Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="16056" citStr="Choudhury et al., 2007" startWordPosition="2563" endWordPosition="2566">g direction for future research. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace non362 standard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewrite dats why pluto is pluto with that’s why... But it is no</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. International Journal on Document Analysis and Recognition, 10(3):157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauren B Collister</author>
</authors>
<title>repair in online discourse.</title>
<date>2011</date>
<journal>Journal of Pragmatics,</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="10833" citStr="Collister, 2011" startWordPosition="1746" endWordPosition="1747"> In many cases, phrasal abbreviations like lol (laugh out loud), lmao (laughing my ass off), smh (shake my head), and ikr (i know, right?) play a similar role: yea she dnt like me lol; lmao I’m playin son. A key difference from emoticons is that abbreviations can act as constituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many of the features that characterize bad language have strong associations with specific social variables. In some cases, these associations mirror linguistic variables known from s</context>
</contexts>
<marker>Collister, 2011</marker>
<rawString>Lauren B. Collister. 2011. *-repair in online discourse. Journal of Pragmatics, 43(3):918–921, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauren B Collister</author>
</authors>
<title>The discourse deictics ˆ and &lt;-- in a world of warcraft community.</title>
<date>2012</date>
<journal>Discourse, Context &amp; Media,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="10851" citStr="Collister, 2012" startWordPosition="1748" endWordPosition="1750">hrasal abbreviations like lol (laugh out loud), lmao (laughing my ass off), smh (shake my head), and ikr (i know, right?) play a similar role: yea she dnt like me lol; lmao I’m playin son. A key difference from emoticons is that abbreviations can act as constituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many of the features that characterize bad language have strong associations with specific social variables. In some cases, these associations mirror linguistic variables known from speech — such as ge</context>
</contexts>
<marker>Collister, 2012</marker>
<rawString>Lauren B. Collister. 2012. The discourse deictics ˆ and &lt;-- in a world of warcraft community. Discourse, Context &amp; Media, 1(1):9–19, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on Computational Approaches to Linguistic Creativity,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="15933" citStr="Cook and Stevenson, 2009" startWordPosition="2543" endWordPosition="2546">ortance of social network diffusion and identity work as factors in the diversification of social media language is an exciting direction for future research. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace non362 standard words with “the contextually appropriate word or sequenc</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In Proceedings of the NAACL-HLT Workshop on Computational Approaches to Linguistic Creativity, pages 71– 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolas Coupland</author>
</authors>
<title>Style (Key Topics in Sociolinguistics).</title>
<date>2007</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="32099" citStr="Coupland, 2007" startWordPosition="5268" endWordPosition="5269">wards the median Tweet can improve accuracy on average, but it is certain to leave many forms of language out. Much of the current research on the relationship between social media language and metadata has the goal of using language to predict the metadata — revealing who is a woman or a man, who is from Oklahoma or New Jersey, and so on. This perspective on social variables and personal identity ignores the local categories that are often more linguistically salient (Eckert, 2008); worse, it strips individuals of any agency in using language as a resource to create and shape their identity (Coupland, 2007), and conceals the role that language plays in creating and perpetuating categories like gender (Bucholtz and Hall, 2005). An alternative possibility is to reverse the relationship between language and metadata, using metadata to achieve a more flexible and heterogeneous domain adaptation that is sensitive to the social factors that shape variation. Such a reversal would help language technology to move beyond false dichotomies between normal and abnormal text, source and target domains, and good and bad language. Acknowledgments This paper benefitted from discussions with David Bamman, Natali</context>
</contexts>
<marker>Coupland, 2007</marker>
<rawString>Nikolas Coupland. 2007. Style (Key Topics in Sociolinguistics). Cambridge University Press, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Darling</author>
<author>Michael J Paul</author>
<author>Fei Song</author>
</authors>
<title>Unsupervised part-of-speech tagging in noisy and esoteric domains with a syntactic-semantic bayesian hmm.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL Workshop on Semantic Analysis in Social Media.</booktitle>
<contexts>
<context position="21801" citStr="Darling et al. (2012)" startWordPosition="3512" endWordPosition="3515">lf-training on unlabeled social media text (Foster et al., 2011) • distributional features to address the sparsity of bag-of-words features (Gimpel et al., 2011; Owoputi et al., 2013; Ritter et al., 2011) • joint normalization, incorporated directly into downstream application (Liu et al., 2012) • distant supervision, using named entity ontologies and topic models (Ritter et al., 2011) Only a few of these techniques (normalization and new annotation systems) are specific to social media; the rest can found in other domain adaptation settings. Is domain adaptation appropriate for social media? Darling et al. (2012) argue that social media is not a coherent domain at all, and that a POS tagger for Twitter will not necessarily generalize to other social media. One can go further: Twitter itself is not a unified genre, it is composed of many different styles and registers, with widely varying expectations for the degree of standardness and dimensions of variation (Androutsopoulos, 2011). I am the co-author on a paper entitled “Part-of-speech tagging for Twitter,” but if we take this title literally, it is impossible on a trivial level: Twitter contains text in dozens or hundreds of languages, including man</context>
</contexts>
<marker>Darling, Paul, Song, 2012</marker>
<rawString>William M. Darling, Michael J. Paul, and Fei Song. 2012. Unsupervised part-of-speech tagging in noisy and esoteric domains with a syntactic-semantic bayesian hmm. In Proceedings of EACL Workshop on Semantic Analysis in Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eli Dresner</author>
<author>Susan C Herring</author>
</authors>
<title>Functions of the non-verbal in cmc: Emoticons and illocutionary force.</title>
<date>2010</date>
<journal>Communication Theory,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="10217" citStr="Dresner and Herring, 2010" startWordPosition="1640" endWordPosition="1643">may be more likely to own iPhones. Affordances are a moving target: new devices and software are constantly becoming available, the software itself may adapt to the user’s input, and the user may adapt to the software and device. 2.4 Pragmatics Emoticons are frequently thought of as introducing an expressive, non-verbal component into written language, mirroring the role played by facial expressions in speech (Walther and D’Addario, 2001), but they can also be seen as playing a pragmatic function: marking an utterance as facetious, or demonstrating a non-confrontational, less invested stance (Dresner and Herring, 2010). In many cases, phrasal abbreviations like lol (laugh out loud), lmao (laughing my ass off), smh (shake my head), and ikr (i know, right?) play a similar role: yea she dnt like me lol; lmao I’m playin son. A key difference from emoticons is that abbreviations can act as constituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function </context>
</contexts>
<marker>Dresner, Herring, 2010</marker>
<rawString>Eli Dresner and Susan C. Herring. 2010. Functions of the non-verbal in cmc: Emoticons and illocutionary force. Communication Theory, 20(3):249–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Drouin</author>
<author>Claire Davis</author>
</authors>
<title>R u txting? is the use of text speak hurting your literacy?</title>
<date>2009</date>
<journal>Journal of Literacy Research,</journal>
<volume>41</volume>
<issue>1</issue>
<contexts>
<context position="6234" citStr="Drouin and Davis (2009)" startWordPosition="981" endWordPosition="984">ions for non-standard spelling on the internet: “people are unsure of the correct spellings,” “it’s faster,” “it’s become the norm,” and “people want to represent their own dialects and/or accents” (Jones, 2010). Let us now consider the evidence for these and other potential explanations. 2.1 Illiteracy Some commentators have fixated on the proposal that the authors of non-standard language in social media are simply unaware or incapable of using more standard language (Thurlow, 2006). But empirical research suggests that many users of bad language are capable of using more traditional forms. Drouin and Davis (2009) find no significant differences in the literacy scores of individuals who do or do not use non-standard vocabulary in text messages. Tagliamonte and Denis (2008) review traces of instant messaging conversations among students, arguing that they “pick and choose ... from the entire stylistic repertoire of the language” in a way that would be impossible without skilled command of both formal and informal registers. While news text is usually more carefully composed and edited than much of the language in social media, there is little evidence that bad language results from an inability to speak</context>
</contexts>
<marker>Drouin, Davis, 2009</marker>
<rawString>Michelle Drouin and Claire Davis. 2009. R u txting? is the use of text speak hurting your literacy? Journal of Literacy Research, 41(1):46–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John W Du Bois</author>
</authors>
<title>The stance triangle.</title>
<date>2007</date>
<booktitle>Stancetaking in discourse,</booktitle>
<pages>139--182</pages>
<editor>In Robert Engelbretson, editor,</editor>
<publisher>John Benjamins Publishing Company, Amsterdam/Philadelphia.</publisher>
<contexts>
<context position="18869" citStr="Bois, 2007" startWordPosition="3034" endWordPosition="3035">-known to speakers of Standard American English, it does not appear in the Penn Treebank and probably could not be used in the Wall Street Journal, except in quotation. Normalization is often impossible without changing the meaning of the text. Should we normalize the final word of ya ur website suxx bro to brother? At the very least, this adds semantic ambiguity where there was none before (is she talking to her biological brother? or possibly to a monk?). Language variation does not arise from passing standard text through a noisy channel; it often serves a pragmatic and/or stancetaking (Du Bois, 2007) function. Eliminating variation would strip those additional layers of meaning from whatever propositional content might survive the normalization process. Sarah Silverman’s ya ur website suxx bro can only be understood as a critique from a caricatured persona — the type of person who ends sentences with bro. Similarly, we can assume that Shaquille O’Neil is capable of writing that’s why Pluto is Pluto, but that to do so would convey an undesirably didactic and authoritative stance towards the audience and topic. This is not to deny that there is great potential value in research aimed at und</context>
</contexts>
<marker>Bois, 2007</marker>
<rawString>John W. Du Bois. 2007. The stance triangle. In Robert Engelbretson, editor, Stancetaking in discourse, pages 139–182. John Benjamins Publishing Company, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Eckert</author>
<author>Sally McConnell-Ginet</author>
</authors>
<title>Language and Gender.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="14212" citStr="Eckert and McConnell-Ginet, 2003" startWordPosition="2279" endWordPosition="2282">te men. The Penn Treebank is composed of professionally-written news text from 1989, when minorities comprised 7.5% of the print journalism workforce; the proportion of women in the journalism workforce was first recorded in 1999, when it was 37% (American Society of Newspaper Editors, 1999). In contrast, Twitter users in the USA contain an equal proportion of men and women, and a higher proportion of young adults and minorities than in the population as a whole (Smith and Brewer, 2012). Such demographic differences are very likely to lead to differences in language (Green, 2002; Labov, 2001; Eckert and McConnell-Ginet, 2003). Overall, the reasons for language diversity in social media are manifold, though some of the most frequently cited explanations (illiteracy and length restrictions) do not hold up to scrutiny. The increasing prevalence of emoticons, phrasal abbreviations (lol, ctfu), and expressive lengthening may reflect the increasing use of written language for ephemeral social interaction, with the concomitant need for multiple channels through which to express multiple types of meaning. The fact many such neologisms are closely circumscribed in geography and demographics may reflect diffusion through so</context>
</contexts>
<marker>Eckert, McConnell-Ginet, 2003</marker>
<rawString>Penelope Eckert and Sally McConnell-Ginet. 2003. Language and Gender. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Eckert</author>
</authors>
<title>Linguistic variation as social practice.</title>
<date>2000</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="12721" citStr="Eckert (2000)" startWordPosition="2039" endWordPosition="2041">), one would not expect to find many examples in financial reports. Other investigators have found that much of the content in Twitter concerns social events and self presentation (Ramage et al., 2010), which may encourage the use of less formal registers in which socially-marketed language is uninhibited. The use of non-standard language is often seen as a form of identity work, signaling authenticity, solidarity, or resistance to norms imposed from above (Bucholtz and Hall, 2005). In spoken language, many of the linguistic variables that perform identity work are phonological — for example, Eckert (2000) showed how the northern cities vowel shift was used by a subset of suburban teenagers to index affiliation with Detroit. The emergence of new linguistic variables in social media suggests that this identity work is as necessary in social media as it is in spoken language. Some of these new variables are transcriptions of existing spoken language variables: like finna, which transcribes fixing to. Others — abbreviations like ctfu and emoticons — seem to be linguistic inventions created to meet the needs of social communication in a new medium. In an early study of variation in social media, Pa</context>
</contexts>
<marker>Eckert, 2000</marker>
<rawString>Penelope Eckert. 2000. Linguistic variation as social practice. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Eckert</author>
</authors>
<title>Variation and the indexical field.</title>
<date>2008</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="31971" citStr="Eckert, 2008" startWordPosition="5246" endWordPosition="5247"> norm. By adopting a model of “domain adaptation,” we confuse a medium with a coherent domain. Adapting language technology towards the median Tweet can improve accuracy on average, but it is certain to leave many forms of language out. Much of the current research on the relationship between social media language and metadata has the goal of using language to predict the metadata — revealing who is a woman or a man, who is from Oklahoma or New Jersey, and so on. This perspective on social variables and personal identity ignores the local categories that are often more linguistically salient (Eckert, 2008); worse, it strips individuals of any agency in using language as a resource to create and shape their identity (Coupland, 2007), and conceals the role that language plays in creating and perpetuating categories like gender (Bucholtz and Hall, 2005). An alternative possibility is to reverse the relationship between language and metadata, using metadata to achieve a more flexible and heterogeneous domain adaptation that is sensitive to the social factors that shape variation. Such a reversal would help language technology to move beyond false dichotomies between normal and abnormal text, source</context>
</contexts>
<marker>Eckert, 2008</marker>
<rawString>Penelope Eckert. 2008. Variation and the indexical field. Journal of Sociolinguistics, 12(4):453–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2010. A latent variable model for geographic lexical variation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="11143" citStr="Eisenstein et al., 2011" startWordPosition="1796" endWordPosition="1799">r ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many of the features that characterize bad language have strong associations with specific social variables. In some cases, these associations mirror linguistic variables known from speech — such as geographically-associated lexical items like hella, or transcriptions of phonological variables like “g-dropping” (Eisenstein et al., 2010). But in other cases, apparently new lexical items, such as the abbreviations ctfu, lls, and af, acquire surprisingly strong associations with geographical</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011. Discovering sociolinguistic associations with structured sparsity. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Mapping the geographical diffusion of new words.</title>
<date>2012</date>
<tech>Technical Report 1210.5268, arXiv.</tech>
<marker>Eisenstein, O’Connor, Smith, Xing, 2012</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2012. Mapping the geographical diffusion of new words. Technical Report 1210.5268, arXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>Phonological factors in social media writing.</title>
<date>2013</date>
<booktitle>In Proceedings of the NAACL Workshop on Language Analysis in Social Media.</booktitle>
<contexts>
<context position="15269" citStr="Eisenstein, 2013" startWordPosition="2437" endWordPosition="2438">ich to express multiple types of meaning. The fact many such neologisms are closely circumscribed in geography and demographics may reflect diffusion through social networks that are assortative on exactly these dimensions (Backstrom et al., 2010; Thelwall, 2009). But an additional consideration is that non-standard language is deliberately deployed in the performance of identity work and stancetaking. This seems a particularly salient explanation for the use of lexical variables that originate in spoken language (jawn, hella), and for the orthographic transcription of phonological variation (Eisenstein, 2013). Determining the role and relative importance of social network diffusion and identity work as factors in the diversification of social media language is an exciting direction for future research. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normaliz</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013. Phonological factors in social media writing. In Proceedings of the NAACL Workshop on Language Analysis in Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>Will Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating named entities in twitter data with crowdsourcing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="7012" citStr="Finin et al., 2010" startWordPosition="1110" endWordPosition="1113">08) review traces of instant messaging conversations among students, arguing that they “pick and choose ... from the entire stylistic repertoire of the language” in a way that would be impossible without skilled command of both formal and informal registers. While news text is usually more carefully composed and edited than much of the language in social media, there is little evidence that bad language results from an inability to speak anything else. 2.2 Length limits In the case of Twitter, the limit of 140 characters for each message is frequently cited as an explanation for bad language (Finin et al., 2010). Does Twitter’s character limit cause users to prefer shorter words, such as u instead of you? If so, one might expect shortening to be used most frequently in messages that are near the 140-character limit. Using a dataset of one million English-language tweets (Bamman et al., 2012), I have computed the average length of messages containing both standard words and their non-standard alternatives, focusing on the top five non-standard shortenings identified by the automatic method of Gouws et al. (2011a). The shortening ur can substitute for both your and you’re. While wit and bout are also s</context>
<context position="20495" citStr="Finin et al., 2010" startWordPosition="3303" endWordPosition="3306">ge processing pipeline. Another potential benefit of this research is to better understand the underlying orthographic processes that lead to the diversity of language in social media, how these processes diffuse over social networks, and how they impact comprehensibility for both the target and non-target audiences. 3.2 Domain adaptation Rather than adapting text to fit our tools, we may instead adapt our tools to fit the text. A series of papers has followed the mold of “NLP for Twitter,” including part-of-speech tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Liu et al., 2011b), parsing (Foster et al., 2011), dialogue modeling (Ritter et al., 2010) and summarization (Sharifi et al., 2010). These papers adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques: • preprocessing to normalize expressive lengthening, and eliminate or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) 363 • new annota</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in twitter data with crowdsourcing. In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="2211" citStr="Finkel et al., 2005" startWordPosition="337" endWordPosition="340">-standard punctuation, capitalization, spelling, vocabulary, and syntax. The consequences for language technology are dire: a series of papers has detailed how state-of-the-art natural language processing (NLP) systems perform significantly worse on social media text. In part-of-speech tagging, the accuracy of the Stanford tagger (Toutanova et al., 2003) falls from 97% on Wall Street Journal text to 85% accuracy on Twitter (Gimpel et al., 2011). In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure (Ritter et al., 2011), down from 86% on the CoNLL test set (Finkel et al., 2005). In parsing, Foster et al. (2011) report double-digit decreases in accuracy for four different state-of-the-art parsers when applied to social media text. The application of language technology to social media is potentially transformative, leveraging the knowledge and perspectives of millions of people. But to deliver on this potential, the problems at the core of the NLP pipeline must be addressed. A growing thread of research takes up this challenge, including a shared task and workshop on “parsing the web,” with new corpora which appear to sit somewhere between the Wall Street Journal and</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny R. Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>From news to comment: Resources and benchmarks for parsing the language of web 2.0.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011. From news to comment: Resources and benchmarks for parsing the language of web 2.0. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
</authors>
<location>and</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, </marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Smith, 2011</marker>
<rawString>Noah A. Smith. 2011. Part-of-speech tagging for twitter: annotation, features, and experiments. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott A Golder</author>
<author>Michael W Macy</author>
</authors>
<title>Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures.</title>
<date>2011</date>
<journal>Science,</journal>
<volume>333</volume>
<issue>6051</issue>
<contexts>
<context position="27959" citStr="Golder and Macy, 2011" startWordPosition="4550" endWordPosition="4553"> respectively, with dips at 12 and 24 months indicating cyclic yearly effects. While the proportion of OOV tokens is smaller when named entities are not included, the rate of growth is similar in each case. The steadily increasingly rate of OOV bigrams suggests that we cannot annotate our way out of the bad language problem. An NLP system trained from data gathered in January 2010 will be increasingly outdated as time passes and social media language continues to evolve. One need not wait months to see language change on Twitter: marked changes can be observed over the course of a single day (Golder and Macy, 2011). A quantitative comparison is shown in Figure 2. Here the OOV rate is standardized with respect to a one-hour gap, where it is 24.2% when named entities are included, and 21.1% when they are not. These rates rise monotonically as the time gap increases, peaking at 25.1% and 21.9% respectively. Such diurnal changes may reflect the diverse language of the different types of authors who post throughout the day. Types of usage The Twitter-# and Twitter-0 corpora are designed to capture the diversity of ways in which social media is used to communicate. Twitter-# contains tweets that begin with ha</context>
</contexts>
<marker>Golder, Macy, 2011</marker>
<rawString>Scott A. Golder and Michael W. Macy. 2011. Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures. Science, 333(6051):1878–1881, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Dirk Hovy</author>
<author>Donald Metzler</author>
</authors>
<title>Unsupervised mining of lexical variants from noisy text.</title>
<date>2011</date>
<booktitle>In Proceedings of the First workshop on Unsupervised Learning in NLP,</booktitle>
<pages>82--90</pages>
<contexts>
<context position="7520" citStr="Gouws et al. (2011" startWordPosition="1190" endWordPosition="1193">of 140 characters for each message is frequently cited as an explanation for bad language (Finin et al., 2010). Does Twitter’s character limit cause users to prefer shorter words, such as u instead of you? If so, one might expect shortening to be used most frequently in messages that are near the 140-character limit. Using a dataset of one million English-language tweets (Bamman et al., 2012), I have computed the average length of messages containing both standard words and their non-standard alternatives, focusing on the top five non-standard shortenings identified by the automatic method of Gouws et al. (2011a). The shortening ur can substitute for both your and you’re. While wit and bout are also spellings for standard words, manual examination of one hundred randomly selected examples for each surface form revealed only one 360 standard length alternative length your 85.1 f 0.4 ur 81.9 f 0.6 you’re 90.0 f 0.1 wit 78.8 f 0.7 with 87.9 f 0.3 goin 72.2 f 1.0 going 82.7 f 0.5 kno 78.4 f 1.0 know 86.1 f 0.4 bout 74.5 f 0.7 about 88.9 f 0.4 Table 1: Average length of messages containing standard forms and their shortenings case in which the standard meaning was intended for wit, and none for bout. The</context>
<context position="9112" citStr="Gouws et al. (2011" startWordPosition="1466" endWordPosition="1469"> the primary factor driving the use of shortened forms. It is still possible that Twitter’s length limitations might indirectly cause word shortenings: for example, by legitimizing shortened forms or causing authors to develop a habit of preferring them. But factors other than the length limit must be recruited to explain why such conventions or habits apply only to some messages and not others. 2.3 Text input affordances Text input affordances — whether standard keyboards or predictive entry on mobile devices — play a role in computer-mediated communication that is perhaps under-appreciated. Gouws et al. (2011b) investigate orthographic variation on Twitter, and find differences across devices: for example, that messages from iPhones include more contractions than messages from Blackberries, and that tweets sent from the web browser are more likely to drop vowels. While each affordance facilitates some writing styles and inhibits others, the affordances themselves are unevenly distributed across users. For example, older people may prefer standard keyboards, and wealthier people may be more likely to own iPhones. Affordances are a moving target: new devices and software are constantly becoming avai</context>
</contexts>
<marker>Gouws, Hovy, Metzler, 2011</marker>
<rawString>Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011a. Unsupervised mining of lexical variants from noisy text. In Proceedings of the First workshop on Unsupervised Learning in NLP, pages 82–90, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Donald Metzler</author>
<author>Congxing Cai</author>
<author>Eduard Hovy</author>
</authors>
<title>Contextual bearing on linguistic variation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Language in Social Media.</booktitle>
<contexts>
<context position="7520" citStr="Gouws et al. (2011" startWordPosition="1190" endWordPosition="1193">of 140 characters for each message is frequently cited as an explanation for bad language (Finin et al., 2010). Does Twitter’s character limit cause users to prefer shorter words, such as u instead of you? If so, one might expect shortening to be used most frequently in messages that are near the 140-character limit. Using a dataset of one million English-language tweets (Bamman et al., 2012), I have computed the average length of messages containing both standard words and their non-standard alternatives, focusing on the top five non-standard shortenings identified by the automatic method of Gouws et al. (2011a). The shortening ur can substitute for both your and you’re. While wit and bout are also spellings for standard words, manual examination of one hundred randomly selected examples for each surface form revealed only one 360 standard length alternative length your 85.1 f 0.4 ur 81.9 f 0.6 you’re 90.0 f 0.1 wit 78.8 f 0.7 with 87.9 f 0.3 goin 72.2 f 1.0 going 82.7 f 0.5 kno 78.4 f 1.0 know 86.1 f 0.4 bout 74.5 f 0.7 about 88.9 f 0.4 Table 1: Average length of messages containing standard forms and their shortenings case in which the standard meaning was intended for wit, and none for bout. The</context>
<context position="9112" citStr="Gouws et al. (2011" startWordPosition="1466" endWordPosition="1469"> the primary factor driving the use of shortened forms. It is still possible that Twitter’s length limitations might indirectly cause word shortenings: for example, by legitimizing shortened forms or causing authors to develop a habit of preferring them. But factors other than the length limit must be recruited to explain why such conventions or habits apply only to some messages and not others. 2.3 Text input affordances Text input affordances — whether standard keyboards or predictive entry on mobile devices — play a role in computer-mediated communication that is perhaps under-appreciated. Gouws et al. (2011b) investigate orthographic variation on Twitter, and find differences across devices: for example, that messages from iPhones include more contractions than messages from Blackberries, and that tweets sent from the web browser are more likely to drop vowels. While each affordance facilitates some writing styles and inhibits others, the affordances themselves are unevenly distributed across users. For example, older people may prefer standard keyboards, and wealthier people may be more likely to own iPhones. Affordances are a moving target: new devices and software are constantly becoming avai</context>
</contexts>
<marker>Gouws, Metzler, Cai, Hovy, 2011</marker>
<rawString>Stephan Gouws, Donald Metzler, Congxing Cai, and Eduard Hovy. 2011b. Contextual bearing on linguistic variation in social media. In Proceedings of the ACL Workshop on Language in Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Green</author>
</authors>
<title>African American English.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="14164" citStr="Green, 2002" startWordPosition="2275" endWordPosition="2276">inantly by working-age white men. The Penn Treebank is composed of professionally-written news text from 1989, when minorities comprised 7.5% of the print journalism workforce; the proportion of women in the journalism workforce was first recorded in 1999, when it was 37% (American Society of Newspaper Editors, 1999). In contrast, Twitter users in the USA contain an equal proportion of men and women, and a higher proportion of young adults and minorities than in the population as a whole (Smith and Brewer, 2012). Such demographic differences are very likely to lead to differences in language (Green, 2002; Labov, 2001; Eckert and McConnell-Ginet, 2003). Overall, the reasons for language diversity in social media are manifold, though some of the most frequently cited explanations (illiteracy and length restrictions) do not hold up to scrutiny. The increasing prevalence of emoticons, phrasal abbreviations (lol, ctfu), and expressive lengthening may reflect the increasing use of written language for ephemeral social interaction, with the concomitant need for multiple channels through which to express multiple types of meaning. The fact many such neologisms are closely circumscribed in geography a</context>
</contexts>
<marker>Green, 2002</marker>
<rawString>Lisa Green. 2002. African American English. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a# twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>1</volume>
<contexts>
<context position="15994" citStr="Han and Baldwin, 2011" startWordPosition="2552" endWordPosition="2555">in the diversification of social media language is an exciting direction for future research. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace non362 standard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can r</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a# twitter. In Proceedings of ACL, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="16013" citStr="Han et al., 2012" startWordPosition="2556" endWordPosition="2559">of social media language is an exciting direction for future research. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace non362 standard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewrite dats why plu</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuheng Hu</author>
<author>Kartik Talamadupula</author>
<author>Subbarao Kambhampati</author>
</authors>
<title>Dude, srsly?: The surprisingly formal nature of twitter’s language.</title>
<date>2013</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="24893" citStr="Hu et al., 2013" startWordPosition="4034" endWordPosition="4037">ster Wallace (Wallace, 2012). Consists of only 482,558 tokens. • Blog articles: A randomly-sampled subset of the American political blog posts gathered by Yano et al. (2009). • Blog comments: A randomly-selected subset of comments associated with the blog posts described above. In all corpora, only fully alphabetic tokens are counted; thus, all hashtags and usernames are discarded. The Twitter text is tokenized using Tweet2A very recent study compares Twitter with other corpora, using a number of alternative metrics, such as the use of high and low frequency words, pronouns, and intensifiers (Hu et al., 2013). This is complementary to the present study, which focuses on the degree of difference in the lexical distributions of corpora gathered from various media. 364 Figure 1: Lexical mismatch increases over time, as social media language evolves. Figure 2: Different times of day have unique lexical signatures, reflecting differing topics and authors. 5 10 15 20 month gap with NEs without NEs 1.035 relative proportion of OOV bigrams 1.030 1.025 1.020 1.015 1.010 1.005 1.000 2 4 6 8 10 12 hour gap with NEs without NEs 1.040 relative proportion of OOV bigrams 1.035 1.030 1.025 1.020 1.015 1.010 1.005</context>
</contexts>
<marker>Hu, Talamadupula, Kambhampati, 2013</marker>
<rawString>Yuheng Hu, Kartik Talamadupula, and Subbarao Kambhampati. 2013. Dude, srsly?: The surprisingly formal nature of twitter’s language. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Johnstone</author>
<author>Jennifer Andrus</author>
<author>Andrew E Danielson</author>
</authors>
<title>Mobility, indexicality, and the enregisterment of pittsburghese.</title>
<date>2006</date>
<journal>Journal of English Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="12109" citStr="Johnstone et al., 2006" startWordPosition="1939" endWordPosition="1942">ms like hella, or transcriptions of phonological variables like “g-dropping” (Eisenstein et al., 2010). But in other cases, apparently new lexical items, such as the abbreviations ctfu, lls, and af, acquire surprisingly strong associations with geographical areas and demographic groups (Eisenstein et al., 2011). A robust finding from the sociolinguistics literature is that non-standard forms that mark social vari361 ables, such as regional dialects, are often inhibited in formal registers (Labov, 1972). For example, while the Pittsburgh spoken dialect sometimes features the address term yinz (Johnstone et al., 2006), one would not expect to find many examples in financial reports. Other investigators have found that much of the content in Twitter concerns social events and self presentation (Ramage et al., 2010), which may encourage the use of less formal registers in which socially-marketed language is uninhibited. The use of non-standard language is often seen as a form of identity work, signaling authenticity, solidarity, or resistance to norms imposed from above (Bucholtz and Hall, 2005). In spoken language, many of the linguistic variables that perform identity work are phonological — for example, E</context>
</contexts>
<marker>Johnstone, Andrus, Danielson, 2006</marker>
<rawString>Barbara Johnstone, Jennifer Andrus, and Andrew E Danielson. 2006. Mobility, indexicality, and the enregisterment of pittsburghese. Journal of English Linguistics, 34(2):77–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Jones</author>
</authors>
<title>The changing face of spelling on the internet. Technical report, The English Spelling Society.</title>
<date>2010</date>
<contexts>
<context position="5822" citStr="Jones, 2010" startWordPosition="917" endWordPosition="918">ers have wrestled with the difficulties imposed by bad language, there has been relatively little consideration of why language in social media is so different from our other corpora. A survey of laypeo1boyd and Crawford (2012) note that “public by default” data still raises important ethical considerations. ple found that more than half of the respondents agreed with the following partial explanations for non-standard spelling on the internet: “people are unsure of the correct spellings,” “it’s faster,” “it’s become the norm,” and “people want to represent their own dialects and/or accents” (Jones, 2010). Let us now consider the evidence for these and other potential explanations. 2.1 Illiteracy Some commentators have fixated on the proposal that the authors of non-standard language in social media are simply unaware or incapable of using more standard language (Thurlow, 2006). But empirical research suggests that many users of bad language are capable of using more traditional forms. Drouin and Davis (2009) find no significant differences in the literacy scores of individuals who do or do not use non-standard vocabulary in text messages. Tagliamonte and Denis (2008) review traces of instant </context>
</contexts>
<marker>Jones, 2010</marker>
<rawString>Lucy Jones. 2010. The changing face of spelling on the internet. Technical report, The English Spelling Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
</authors>
<title>Sociolinguistic patterns.</title>
<date>1972</date>
<publisher>University of Pennsylvania Press.</publisher>
<location>Philadelphia:</location>
<contexts>
<context position="11993" citStr="Labov, 1972" startWordPosition="1924" endWordPosition="1925">ssociations mirror linguistic variables known from speech — such as geographically-associated lexical items like hella, or transcriptions of phonological variables like “g-dropping” (Eisenstein et al., 2010). But in other cases, apparently new lexical items, such as the abbreviations ctfu, lls, and af, acquire surprisingly strong associations with geographical areas and demographic groups (Eisenstein et al., 2011). A robust finding from the sociolinguistics literature is that non-standard forms that mark social vari361 ables, such as regional dialects, are often inhibited in formal registers (Labov, 1972). For example, while the Pittsburgh spoken dialect sometimes features the address term yinz (Johnstone et al., 2006), one would not expect to find many examples in financial reports. Other investigators have found that much of the content in Twitter concerns social events and self presentation (Ramage et al., 2010), which may encourage the use of less formal registers in which socially-marketed language is uninhibited. The use of non-standard language is often seen as a form of identity work, signaling authenticity, solidarity, or resistance to norms imposed from above (Bucholtz and Hall, 2005</context>
</contexts>
<marker>Labov, 1972</marker>
<rawString>William Labov. 1972. Sociolinguistic patterns. Philadelphia: University of Pennsylvania Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
</authors>
<title>Principles of linguistic change. Vol.2 : Social factors.</title>
<date>2001</date>
<publisher>Blackwell Publishers,</publisher>
<location>Oxford.</location>
<contexts>
<context position="14177" citStr="Labov, 2001" startWordPosition="2277" endWordPosition="2278">rking-age white men. The Penn Treebank is composed of professionally-written news text from 1989, when minorities comprised 7.5% of the print journalism workforce; the proportion of women in the journalism workforce was first recorded in 1999, when it was 37% (American Society of Newspaper Editors, 1999). In contrast, Twitter users in the USA contain an equal proportion of men and women, and a higher proportion of young adults and minorities than in the population as a whole (Smith and Brewer, 2012). Such demographic differences are very likely to lead to differences in language (Green, 2002; Labov, 2001; Eckert and McConnell-Ginet, 2003). Overall, the reasons for language diversity in social media are manifold, though some of the most frequently cited explanations (illiteracy and length restrictions) do not hold up to scrutiny. The increasing prevalence of emoticons, phrasal abbreviations (lol, ctfu), and expressive lengthening may reflect the increasing use of written language for ephemeral social interaction, with the concomitant need for multiple channels through which to express multiple types of meaning. The fact many such neologisms are closely circumscribed in geography and demographi</context>
</contexts>
<marker>Labov, 2001</marker>
<rawString>William Labov. 2001. Principles of linguistic change. Vol.2 : Social factors. Blackwell Publishers, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Bingqing Wang</author>
<author>Yang Liu</author>
</authors>
<title>Insertion, deletion, or substitution? normalizing text messages without pre-categorization nor supervision.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>71--76</pages>
<contexts>
<context position="16074" citStr="Liu et al., 2011" startWordPosition="2567" endWordPosition="2570">esearch. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace non362 standard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewrite dats why pluto is pluto with that’s why... But it is not difficult to fin</context>
<context position="20534" citStr="Liu et al., 2011" startWordPosition="3311" endWordPosition="3314"> benefit of this research is to better understand the underlying orthographic processes that lead to the diversity of language in social media, how these processes diffuse over social networks, and how they impact comprehensibility for both the target and non-target audiences. 3.2 Domain adaptation Rather than adapting text to fit our tools, we may instead adapt our tools to fit the text. A series of papers has followed the mold of “NLP for Twitter,” including part-of-speech tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Liu et al., 2011b), parsing (Foster et al., 2011), dialogue modeling (Ritter et al., 2010) and summarization (Sharifi et al., 2010). These papers adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques: • preprocessing to normalize expressive lengthening, and eliminate or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) 363 • new annotation schemes specifically customized fo</context>
</contexts>
<marker>Liu, Weng, Wang, Liu, 2011</marker>
<rawString>Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011a. Insertion, deletion, or substitution? normalizing text messages without pre-categorization nor supervision. In Proceedings of ACL, pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Shaodian Zhang</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
</authors>
<title>Recognizing named entities in tweets.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16074" citStr="Liu et al., 2011" startWordPosition="2567" endWordPosition="2570">esearch. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace non362 standard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewrite dats why pluto is pluto with that’s why... But it is not difficult to fin</context>
<context position="20534" citStr="Liu et al., 2011" startWordPosition="3311" endWordPosition="3314"> benefit of this research is to better understand the underlying orthographic processes that lead to the diversity of language in social media, how these processes diffuse over social networks, and how they impact comprehensibility for both the target and non-target audiences. 3.2 Domain adaptation Rather than adapting text to fit our tools, we may instead adapt our tools to fit the text. A series of papers has followed the mold of “NLP for Twitter,” including part-of-speech tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Liu et al., 2011b), parsing (Foster et al., 2011), dialogue modeling (Ritter et al., 2010) and summarization (Sharifi et al., 2010). These papers adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques: • preprocessing to normalize expressive lengthening, and eliminate or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) 363 • new annotation schemes specifically customized fo</context>
</contexts>
<marker>Liu, Zhang, Wei, Zhou, 2011</marker>
<rawString>Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011b. Recognizing named entities in tweets. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Xiangyang Zhou</author>
<author>Zhongyang Fu</author>
<author>Furu Wei</author>
</authors>
<title>Joint inference of named entity recognition and normalization for tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="21476" citStr="Liu et al., 2012" startWordPosition="3460" endWordPosition="3463"> group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) 363 • new annotation schemes specifically customized for social media text (Gimpel et al., 2011) • self-training on unlabeled social media text (Foster et al., 2011) • distributional features to address the sparsity of bag-of-words features (Gimpel et al., 2011; Owoputi et al., 2013; Ritter et al., 2011) • joint normalization, incorporated directly into downstream application (Liu et al., 2012) • distant supervision, using named entity ontologies and topic models (Ritter et al., 2011) Only a few of these techniques (normalization and new annotation systems) are specific to social media; the rest can found in other domain adaptation settings. Is domain adaptation appropriate for social media? Darling et al. (2012) argue that social media is not a coherent domain at all, and that a POS tagger for Twitter will not necessarily generalize to other social media. One can go further: Twitter itself is not a unified genre, it is composed of many different styles and registers, with widely va</context>
</contexts>
<marker>Liu, Zhou, Zhou, Fu, Wei, 2012</marker>
<rawString>Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang Fu, and Furu Wei. 2012. Joint inference of named entity recognition and normalization for tweets. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Automatic domain adaptation for parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="23390" citStr="McClosky et al. (2010)" startWordPosition="3779" endWordPosition="3782">ays — as Androutsopoulos (2011) notes when contrasting the types of variation encountered when “visiting a gamer forum” versus “joining the Twitter profile of a rap star.” 4 The lexical coherence of social media The internal coherence of social media — and its relationship to other types of text — can be quantified in terms of the similarity of distributions over bigrams. While there are many techniques for comparing word distributions, I apply the relatively simple method of counting out-of-vocabulary (OOV) bigrams. The relationship between OOV rate and domain adaptation has been explored by McClosky et al. (2010), who use it as a feature to predict how well a parser will perform when applied across domains.2 Specifically, the datasets A and B are compared by counting the number of bigram tokens in A that are unseen in B. The following corpora are compared: • Twitter-month: randomly selected tweets from each month between January 2010 to October 2012 (Eisenstein et al., 2012). • Twitter-hour: randomly selected tweets from each hour of the day, randomly sampled during the period from January 2010 to October 2012. • Twitter-#: tweets in which the first token is a hashtag. The hashtag itself is not includ</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In Proceedings of NAACL, pages 28–36, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Munro</author>
<author>Christopher D Manning</author>
</authors>
<title>Short message communications: users, topics, and inlanguage processing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2nd ACM Symposium on Computing for Development.</booktitle>
<contexts>
<context position="3928" citStr="Munro and Manning (2012)" startWordPosition="615" endWordPosition="618">gue that the two main computational approaches to dealing with bad language — normalization and domain adaptation — are based on theories of social media language that are not descriptively accurate. I have worked and continue to work in both of these areas, so I make this argument not as a criticism of others, but in a spirit of self-reflection. It is hoped that a greater engagement with sociolinguistic and CMC research will lead to new, nuanced approaches to the challenge of bad language. Why so much Twitter? Most of the examples in this paper will focus on Twitter, a microblogging service. Munro and Manning (2012) argue that Twitter has unfairly dominated recent research, at the expense of email and SMS text messages, which they found to be both linguistically distinct from Twitter and significantly more prevalent (in 2010). This matches earlier research arguing that email contained relatively little “neography,” compared with text messages and chat (Anis, 2007). A crucial advantage for Twitter is that it is public by default, while SMS and email are private. This makes Twitter data less problematic from a privacy standpoint,1 far easier to obtain, and more amenable to target applications such as large</context>
</contexts>
<marker>Munro, Manning, 2012</marker>
<rawString>Robert Munro and Christopher D. Manning. 2012. Short message communications: users, topics, and inlanguage processing. In Proceedings of the 2nd ACM Symposium on Computing for Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mor Naaman</author>
<author>Hila Becker</author>
<author>Luis Gravano</author>
</authors>
<title>Hip and trendy: Characterizing emerging trends on twitter.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>5</issue>
<contexts>
<context position="22628" citStr="Naaman et al., 2011" startWordPosition="3652" endWordPosition="3655">t is composed of many different styles and registers, with widely varying expectations for the degree of standardness and dimensions of variation (Androutsopoulos, 2011). I am the co-author on a paper entitled “Part-of-speech tagging for Twitter,” but if we take this title literally, it is impossible on a trivial level: Twitter contains text in dozens or hundreds of languages, including many for which no POS tagger exists. Even within a single language — setting aside issues of codeswitching (Paolillo, 1996) — Twitter and other social media can contain registers ranging from hashtag wordplay (Naaman et al., 2011) to the official pronouncements of the British Monarchy. And even if all good language is alike, bad language can be bad in many different ways — as Androutsopoulos (2011) notes when contrasting the types of variation encountered when “visiting a gamer forum” versus “joining the Twitter profile of a rap star.” 4 The lexical coherence of social media The internal coherence of social media — and its relationship to other types of text — can be quantified in terms of the similarity of distributions over bigrams. While there are many techniques for comparing word distributions, I apply the relativ</context>
<context position="28664" citStr="Naaman et al., 2011" startWordPosition="4673" endWordPosition="4676">ith respect to a one-hour gap, where it is 24.2% when named entities are included, and 21.1% when they are not. These rates rise monotonically as the time gap increases, peaking at 25.1% and 21.9% respectively. Such diurnal changes may reflect the diverse language of the different types of authors who post throughout the day. Types of usage The Twitter-# and Twitter-0 corpora are designed to capture the diversity of ways in which social media is used to communicate. Twitter-# contains tweets that begin with hashtags, and are thus more likely to be part of running jokes 365 or trending topics (Naaman et al., 2011). Twitter@ contains tweets that begin with usernames — an addressing mechanism that is used to maintain dialogue threads on the site. These datasets are compared with a set of randomly selected tweets from June 2011, and with several other corpora: Penn Treebank, the novel Infinite Jest, and text and comments from political blogs. There was no attempt to remove named entities from any of these corpora, as such a comparison would merely reflect the different accuracy levels of NER in each corpus. The results are shown in Table 2. A few observations stand out. First, the Penn Treebank is the cle</context>
</contexts>
<marker>Naaman, Becker, Gravano, 2011</marker>
<rawString>Mor Naaman, Hila Becker, and Luis Gravano. 2011. Hip and trendy: Characterizing emerging trends on twitter. Journal of the American Society for Information Science and Technology, 62(5):902–918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Paolillo</author>
</authors>
<title>Language choice on soc.culture.punjab.</title>
<date>1996</date>
<journal>Electronic Journal of Communication/La Revue Electronique de Communication,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="22521" citStr="Paolillo, 1996" startWordPosition="3635" endWordPosition="3636">ssarily generalize to other social media. One can go further: Twitter itself is not a unified genre, it is composed of many different styles and registers, with widely varying expectations for the degree of standardness and dimensions of variation (Androutsopoulos, 2011). I am the co-author on a paper entitled “Part-of-speech tagging for Twitter,” but if we take this title literally, it is impossible on a trivial level: Twitter contains text in dozens or hundreds of languages, including many for which no POS tagger exists. Even within a single language — setting aside issues of codeswitching (Paolillo, 1996) — Twitter and other social media can contain registers ranging from hashtag wordplay (Naaman et al., 2011) to the official pronouncements of the British Monarchy. And even if all good language is alike, bad language can be bad in many different ways — as Androutsopoulos (2011) notes when contrasting the types of variation encountered when “visiting a gamer forum” versus “joining the Twitter profile of a rap star.” 4 The lexical coherence of social media The internal coherence of social media — and its relationship to other types of text — can be quantified in terms of the similarity of distri</context>
</contexts>
<marker>Paolillo, 1996</marker>
<rawString>John C. Paolillo. 1996. Language choice on soc.culture.punjab. Electronic Journal of Communication/La Revue Electronique de Communication, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Paolillo</author>
</authors>
<title>The virtual speech community: Social network and language variation on irc.</title>
<date>1999</date>
<journal>Journal of Computer-Mediated Communication,</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="13334" citStr="Paolillo (1999)" startWordPosition="2142" endWordPosition="2143">0) showed how the northern cities vowel shift was used by a subset of suburban teenagers to index affiliation with Detroit. The emergence of new linguistic variables in social media suggests that this identity work is as necessary in social media as it is in spoken language. Some of these new variables are transcriptions of existing spoken language variables: like finna, which transcribes fixing to. Others — abbreviations like ctfu and emoticons — seem to be linguistic inventions created to meet the needs of social communication in a new medium. In an early study of variation in social media, Paolillo (1999) notes that code-switching between English and Hindi also performs this type of identity work. Finally, it is an uncomfortable fact that the text in many of our most frequently-used corpora was written and edited predominantly by working-age white men. The Penn Treebank is composed of professionally-written news text from 1989, when minorities comprised 7.5% of the print journalism workforce; the proportion of women in the journalism workforce was first recorded in 1999, when it was 37% (American Society of Newspaper Editors, 1999). In contrast, Twitter users in the USA contain an equal propor</context>
</contexts>
<marker>Paolillo, 1999</marker>
<rawString>John C. Paolillo. 1999. The virtual speech community: Social network and language variation on irc. Journal of Computer-Mediated Communication, 4(4):0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Paolillo</author>
</authors>
<title>Language variation on internet relay chat: A social network approach.</title>
<date>2001</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="4792" citStr="Paolillo, 2001" startWordPosition="754" endWordPosition="755">uing that email contained relatively little “neography,” compared with text messages and chat (Anis, 2007). A crucial advantage for Twitter is that it is public by default, while SMS and email are private. This makes Twitter data less problematic from a privacy standpoint,1 far easier to obtain, and more amenable to target applications such as large-scale mining of events (Sakaki et al., 2010; Benson et al., 2011) and opinions (Sauper et al., 2011). Similar argument could be made on behalf of other public social media, such as blog comments (Ali-Hasan and Adamic, 2007), forums, and chatrooms (Paolillo, 2001). The main advantage of Twitter over these media is convenience in gathering large datasets through a single streaming interface. More comparative evaluation is needed to determine linguistic similarities and differences between Twitter and these other media; Section 4 presents an evaluation of the lexical similarity between Twitter and political blogs. 2 A tour of bad language While many NLP researchers and engineers have wrestled with the difficulties imposed by bad language, there has been relatively little consideration of why language in social media is so different from our other corpora</context>
</contexts>
<marker>Paolillo, 2001</marker>
<rawString>John C. Paolillo. 2001. Language variation on internet relay chat: A social network approach. Journal of Sociolinguistics, 5(2):180–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</booktitle>
<contexts>
<context position="2879" citStr="Petrov and McDonald, 2012" startWordPosition="444" endWordPosition="447">double-digit decreases in accuracy for four different state-of-the-art parsers when applied to social media text. The application of language technology to social media is potentially transformative, leveraging the knowledge and perspectives of millions of people. But to deliver on this potential, the problems at the core of the NLP pipeline must be addressed. A growing thread of research takes up this challenge, including a shared task and workshop on “parsing the web,” with new corpora which appear to sit somewhere between the Wall Street Journal and Twitter on the spectrum of bad language (Petrov and McDonald, 2012). But perhaps surprisingly, very little of this research has considered why social media language is so different. This review paper attempts to shed some light on this question, surveying a strong tradition of empirical and theoreti359 Proceedings of NAACL-HLT 2013, pages 359–369, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics cal research on computer-mediated communication (CMC). I argue that the two main computational approaches to dealing with bad language — normalization and domain adaptation — are based on theories of social media language that are not</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Sue Dumais</author>
<author>D Liebling</author>
</authors>
<title>Characterizing microblogs with topic models.</title>
<date>2010</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="12309" citStr="Ramage et al., 2010" startWordPosition="1972" endWordPosition="1975">ire surprisingly strong associations with geographical areas and demographic groups (Eisenstein et al., 2011). A robust finding from the sociolinguistics literature is that non-standard forms that mark social vari361 ables, such as regional dialects, are often inhibited in formal registers (Labov, 1972). For example, while the Pittsburgh spoken dialect sometimes features the address term yinz (Johnstone et al., 2006), one would not expect to find many examples in financial reports. Other investigators have found that much of the content in Twitter concerns social events and self presentation (Ramage et al., 2010), which may encourage the use of less formal registers in which socially-marketed language is uninhibited. The use of non-standard language is often seen as a form of identity work, signaling authenticity, solidarity, or resistance to norms imposed from above (Bucholtz and Hall, 2005). In spoken language, many of the linguistic variables that perform identity work are phonological — for example, Eckert (2000) showed how the northern cities vowel shift was used by a subset of suburban teenagers to index affiliation with Detroit. The emergence of new linguistic variables in social media suggests</context>
</contexts>
<marker>Ramage, Dumais, Liebling, 2010</marker>
<rawString>Daniel Ramage, Sue Dumais, and D. Liebling. 2010. Characterizing microblogs with topic models. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying latent user attributes in twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of Workshop on Search and mining user-generated contents.</booktitle>
<contexts>
<context position="11111" citStr="Rao et al., 2010" startWordPosition="1791" endWordPosition="1794">ituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many of the features that characterize bad language have strong associations with specific social variables. In some cases, these associations mirror linguistic variables known from speech — such as geographically-associated lexical items like hella, or transcriptions of phonological variables like “g-dropping” (Eisenstein et al., 2010). But in other cases, apparently new lexical items, such as the abbreviations ctfu, lls, and af, acquire surprisingly stron</context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes in twitter. In Proceedings of Workshop on Search and mining user-generated contents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="20608" citStr="Ritter et al., 2010" startWordPosition="3323" endWordPosition="3326">graphic processes that lead to the diversity of language in social media, how these processes diffuse over social networks, and how they impact comprehensibility for both the target and non-target audiences. 3.2 Domain adaptation Rather than adapting text to fit our tools, we may instead adapt our tools to fit the text. A series of papers has followed the mold of “NLP for Twitter,” including part-of-speech tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Liu et al., 2011b), parsing (Foster et al., 2011), dialogue modeling (Ritter et al., 2010) and summarization (Sharifi et al., 2010). These papers adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques: • preprocessing to normalize expressive lengthening, and eliminate or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) 363 • new annotation schemes specifically customized for social media text (Gimpel et al., 2011) • self-training on unlabeled soc</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2152" citStr="Ritter et al., 2011" startWordPosition="325" endWordPosition="328">ic challenges that are endemic to the medium, including non-standard punctuation, capitalization, spelling, vocabulary, and syntax. The consequences for language technology are dire: a series of papers has detailed how state-of-the-art natural language processing (NLP) systems perform significantly worse on social media text. In part-of-speech tagging, the accuracy of the Stanford tagger (Toutanova et al., 2003) falls from 97% on Wall Street Journal text to 85% accuracy on Twitter (Gimpel et al., 2011). In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure (Ritter et al., 2011), down from 86% on the CoNLL test set (Finkel et al., 2005). In parsing, Foster et al. (2011) report double-digit decreases in accuracy for four different state-of-the-art parsers when applied to social media text. The application of language technology to social media is potentially transformative, leveraging the knowledge and perspectives of millions of people. But to deliver on this potential, the problems at the core of the NLP pipeline must be addressed. A growing thread of research takes up this challenge, including a shared task and workshop on “parsing the web,” with new corpora which </context>
<context position="20516" citStr="Ritter et al., 2011" startWordPosition="3307" endWordPosition="3310">ne. Another potential benefit of this research is to better understand the underlying orthographic processes that lead to the diversity of language in social media, how these processes diffuse over social networks, and how they impact comprehensibility for both the target and non-target audiences. 3.2 Domain adaptation Rather than adapting text to fit our tools, we may instead adapt our tools to fit the text. A series of papers has followed the mold of “NLP for Twitter,” including part-of-speech tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Liu et al., 2011b), parsing (Foster et al., 2011), dialogue modeling (Ritter et al., 2010) and summarization (Sharifi et al., 2010). These papers adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques: • preprocessing to normalize expressive lengthening, and eliminate or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) 363 • new annotation schemes specific</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Kathleen McKeown</author>
</authors>
<title>Age prediction in blogs: A study of style, content, and online behavior in pre- and Post-Social media generations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="11063" citStr="Rosenthal and McKeown, 2011" startWordPosition="1782" endWordPosition="1785">rence from emoticons is that abbreviations can act as constituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many of the features that characterize bad language have strong associations with specific social variables. In some cases, these associations mirror linguistic variables known from speech — such as geographically-associated lexical items like hella, or transcriptions of phonological variables like “g-dropping” (Eisenstein et al., 2010). But in other cases, apparently new lexical items, such as the abbreviatio</context>
</contexts>
<marker>Rosenthal, McKeown, 2011</marker>
<rawString>Sara Rosenthal and Kathleen McKeown. 2011. Age prediction in blogs: A study of style, content, and online behavior in pre- and Post-Social media generations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>851--860</pages>
<contexts>
<context position="4572" citStr="Sakaki et al., 2010" startWordPosition="716" endWordPosition="719">nfairly dominated recent research, at the expense of email and SMS text messages, which they found to be both linguistically distinct from Twitter and significantly more prevalent (in 2010). This matches earlier research arguing that email contained relatively little “neography,” compared with text messages and chat (Anis, 2007). A crucial advantage for Twitter is that it is public by default, while SMS and email are private. This makes Twitter data less problematic from a privacy standpoint,1 far easier to obtain, and more amenable to target applications such as large-scale mining of events (Sakaki et al., 2010; Benson et al., 2011) and opinions (Sauper et al., 2011). Similar argument could be made on behalf of other public social media, such as blog comments (Ali-Hasan and Adamic, 2007), forums, and chatrooms (Paolillo, 2001). The main advantage of Twitter over these media is convenience in gathering large datasets through a single streaming interface. More comparative evaluation is needed to determine linguistic similarities and differences between Twitter and these other media; Section 4 presents an evaluation of the lexical similarity between Twitter and political blogs. 2 A tour of bad language</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time event detection by social sensors. In Proceedings of WWW, pages 851–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Content models with attitude.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4629" citStr="Sauper et al., 2011" startWordPosition="726" endWordPosition="729">il and SMS text messages, which they found to be both linguistically distinct from Twitter and significantly more prevalent (in 2010). This matches earlier research arguing that email contained relatively little “neography,” compared with text messages and chat (Anis, 2007). A crucial advantage for Twitter is that it is public by default, while SMS and email are private. This makes Twitter data less problematic from a privacy standpoint,1 far easier to obtain, and more amenable to target applications such as large-scale mining of events (Sakaki et al., 2010; Benson et al., 2011) and opinions (Sauper et al., 2011). Similar argument could be made on behalf of other public social media, such as blog comments (Ali-Hasan and Adamic, 2007), forums, and chatrooms (Paolillo, 2001). The main advantage of Twitter over these media is convenience in gathering large datasets through a single streaming interface. More comparative evaluation is needed to determine linguistic similarities and differences between Twitter and these other media; Section 4 presents an evaluation of the lexical similarity between Twitter and political blogs. 2 A tour of bad language While many NLP researchers and engineers have wrestled w</context>
</contexts>
<marker>Sauper, Haghighi, Barzilay, 2011</marker>
<rawString>Christina Sauper, Aria Haghighi, and Regina Barzilay. 2011. Content models with attitude. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beaux Sharifi</author>
<author>Mark-Anthony Hutton</author>
<author>Jugal Kalita</author>
</authors>
<title>Summarizing microblogs automatically.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="20649" citStr="Sharifi et al., 2010" startWordPosition="3329" endWordPosition="3332">sity of language in social media, how these processes diffuse over social networks, and how they impact comprehensibility for both the target and non-target audiences. 3.2 Domain adaptation Rather than adapting text to fit our tools, we may instead adapt our tools to fit the text. A series of papers has followed the mold of “NLP for Twitter,” including part-of-speech tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Liu et al., 2011b), parsing (Foster et al., 2011), dialogue modeling (Ritter et al., 2010) and summarization (Sharifi et al., 2010). These papers adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques: • preprocessing to normalize expressive lengthening, and eliminate or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) 363 • new annotation schemes specifically customized for social media text (Gimpel et al., 2011) • self-training on unlabeled social media text (Foster et al., 2011) • di</context>
</contexts>
<marker>Sharifi, Hutton, Kalita, 2010</marker>
<rawString>Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita. 2010. Summarizing microblogs automatically. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Smith</author>
<author>Joanna Brewer</author>
</authors>
<title>Twitter use</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>Pew Research Center,</institution>
<contexts>
<context position="14070" citStr="Smith and Brewer, 2012" startWordPosition="2259" endWordPosition="2262">uncomfortable fact that the text in many of our most frequently-used corpora was written and edited predominantly by working-age white men. The Penn Treebank is composed of professionally-written news text from 1989, when minorities comprised 7.5% of the print journalism workforce; the proportion of women in the journalism workforce was first recorded in 1999, when it was 37% (American Society of Newspaper Editors, 1999). In contrast, Twitter users in the USA contain an equal proportion of men and women, and a higher proportion of young adults and minorities than in the population as a whole (Smith and Brewer, 2012). Such demographic differences are very likely to lead to differences in language (Green, 2002; Labov, 2001; Eckert and McConnell-Ginet, 2003). Overall, the reasons for language diversity in social media are manifold, though some of the most frequently cited explanations (illiteracy and length restrictions) do not hold up to scrutiny. The increasing prevalence of emoticons, phrasal abbreviations (lol, ctfu), and expressive lengthening may reflect the increasing use of written language for ephemeral social interaction, with the concomitant need for multiple channels through which to express mul</context>
</contexts>
<marker>Smith, Brewer, 2012</marker>
<rawString>Aaron Smith and Joanna Brewer. 2012. Twitter use 2012. Technical report, Pew Research Center, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Alan W Black</author>
<author>Stanley Chen</author>
<author>Shankar Kumar</author>
<author>Mari Ostendorf</author>
<author>Christopher Richards</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="16440" citStr="Sproat et al. (2001)" startWordPosition="2627" endWordPosition="2630">at our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace non362 standard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewrite dats why pluto is pluto with that’s why... But it is not difficult to find cases that are less clear, putting would-be normalizers in a difficult position. The labeled dataset of Han and Baldwin (2011) addresses a more tractable subset of the normalization problem, annotating only token-to-token normalizations. Thus, imma — a transcription of I’m gonna, which in turn transcribes I’m going to — is not normalized in this dataset. Abbrevi</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Richard Sproat, Alan W Black, Stanley Chen, Shankar Kumar, Mari Ostendorf, and Christopher Richards. 2001. Normalization of non-standard words. Computer Speech &amp; Language, 15(3):287–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sali A Tagliamonte</author>
<author>Derek Denis</author>
</authors>
<title>Linguistic ruin? lol! instant messaging and teen language.</title>
<date>2008</date>
<journal>American Speech,</journal>
<volume>83</volume>
<issue>1</issue>
<contexts>
<context position="6396" citStr="Tagliamonte and Denis (2008)" startWordPosition="1008" endWordPosition="1011">esent their own dialects and/or accents” (Jones, 2010). Let us now consider the evidence for these and other potential explanations. 2.1 Illiteracy Some commentators have fixated on the proposal that the authors of non-standard language in social media are simply unaware or incapable of using more standard language (Thurlow, 2006). But empirical research suggests that many users of bad language are capable of using more traditional forms. Drouin and Davis (2009) find no significant differences in the literacy scores of individuals who do or do not use non-standard vocabulary in text messages. Tagliamonte and Denis (2008) review traces of instant messaging conversations among students, arguing that they “pick and choose ... from the entire stylistic repertoire of the language” in a way that would be impossible without skilled command of both formal and informal registers. While news text is usually more carefully composed and edited than much of the language in social media, there is little evidence that bad language results from an inability to speak anything else. 2.2 Length limits In the case of Twitter, the limit of 140 characters for each message is frequently cited as an explanation for bad language (Fin</context>
</contexts>
<marker>Tagliamonte, Denis, 2008</marker>
<rawString>Sali A. Tagliamonte and Derek Denis. 2008. Linguistic ruin? lol! instant messaging and teen language. American Speech, 83(1):3–34, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
</authors>
<title>Homophily in MySpace.</title>
<date>2009</date>
<journal>J. Am. Soc. Inf. Sci.,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="14915" citStr="Thelwall, 2009" startWordPosition="2385" endWordPosition="2386">of the most frequently cited explanations (illiteracy and length restrictions) do not hold up to scrutiny. The increasing prevalence of emoticons, phrasal abbreviations (lol, ctfu), and expressive lengthening may reflect the increasing use of written language for ephemeral social interaction, with the concomitant need for multiple channels through which to express multiple types of meaning. The fact many such neologisms are closely circumscribed in geography and demographics may reflect diffusion through social networks that are assortative on exactly these dimensions (Backstrom et al., 2010; Thelwall, 2009). But an additional consideration is that non-standard language is deliberately deployed in the performance of identity work and stancetaking. This seems a particularly salient explanation for the use of lexical variables that originate in spoken language (jawn, hella), and for the orthographic transcription of phonological variation (Eisenstein, 2013). Determining the role and relative importance of social network diffusion and identity work as factors in the diversification of social media language is an exciting direction for future research. 3 What can we do about it? Having surveyed the l</context>
</contexts>
<marker>Thelwall, 2009</marker>
<rawString>Mike Thelwall. 2009. Homophily in MySpace. J. Am. Soc. Inf. Sci., 60(2):219–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Crispin Thurlow</author>
</authors>
<title>From statistical panic to moral panic: The metadiscursive construction and popular exaggeration of new media language in the print media.</title>
<date>2006</date>
<journal>J. Computer-Mediated Communication,</journal>
<pages>667--701</pages>
<contexts>
<context position="6100" citStr="Thurlow, 2006" startWordPosition="960" endWordPosition="961">important ethical considerations. ple found that more than half of the respondents agreed with the following partial explanations for non-standard spelling on the internet: “people are unsure of the correct spellings,” “it’s faster,” “it’s become the norm,” and “people want to represent their own dialects and/or accents” (Jones, 2010). Let us now consider the evidence for these and other potential explanations. 2.1 Illiteracy Some commentators have fixated on the proposal that the authors of non-standard language in social media are simply unaware or incapable of using more standard language (Thurlow, 2006). But empirical research suggests that many users of bad language are capable of using more traditional forms. Drouin and Davis (2009) find no significant differences in the literacy scores of individuals who do or do not use non-standard vocabulary in text messages. Tagliamonte and Denis (2008) review traces of instant messaging conversations among students, arguing that they “pick and choose ... from the entire stylistic repertoire of the language” in a way that would be impossible without skilled command of both formal and informal registers. While news text is usually more carefully compos</context>
</contexts>
<marker>Thurlow, 2006</marker>
<rawString>Crispin Thurlow. 2006. From statistical panic to moral panic: The metadiscursive construction and popular exaggeration of new media language in the print media. J. Computer-Mediated Communication, pages 667–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1947" citStr="Toutanova et al., 2003" startWordPosition="292" endWordPosition="295">’Neil) • michelle obama great. job. and. whit all my. respect she. look. great. congrats. to. her. (Ozzie Guillen) These examples are selected from celebrities (for privacy reasons), but they contain linguistic challenges that are endemic to the medium, including non-standard punctuation, capitalization, spelling, vocabulary, and syntax. The consequences for language technology are dire: a series of papers has detailed how state-of-the-art natural language processing (NLP) systems perform significantly worse on social media text. In part-of-speech tagging, the accuracy of the Stanford tagger (Toutanova et al., 2003) falls from 97% on Wall Street Journal text to 85% accuracy on Twitter (Gimpel et al., 2011). In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure (Ritter et al., 2011), down from 86% on the CoNLL test set (Finkel et al., 2005). In parsing, Foster et al. (2011) report double-digit decreases in accuracy for four different state-of-the-art parsers when applied to social media text. The application of language technology to social media is potentially transformative, leveraging the knowledge and perspectives of millions of people. But to deliver on this potent</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byron Wallace</author>
</authors>
<title>Multiple narrative disentanglement: Unraveling infinite jest.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="24305" citStr="Wallace, 2012" startWordPosition="3940" endWordPosition="3941">month between January 2010 to October 2012 (Eisenstein et al., 2012). • Twitter-hour: randomly selected tweets from each hour of the day, randomly sampled during the period from January 2010 to October 2012. • Twitter-#: tweets in which the first token is a hashtag. The hashtag itself is not included in the bigram counts; see below for more details on which bigrams are included. • Twitter-@: tweets in which the first token is a username. The username itself is not included in the bigram counts. • Penn Treebank: sections 2-21 • Infinite Jest: the text of the 1996 novel by David Foster Wallace (Wallace, 2012). Consists of only 482,558 tokens. • Blog articles: A randomly-sampled subset of the American political blog posts gathered by Yano et al. (2009). • Blog comments: A randomly-selected subset of comments associated with the blog posts described above. In all corpora, only fully alphabetic tokens are counted; thus, all hashtags and usernames are discarded. The Twitter text is tokenized using Tweet2A very recent study compares Twitter with other corpora, using a number of alternative metrics, such as the use of high and low frequency words, pronouns, and intensifiers (Hu et al., 2013). This is co</context>
</contexts>
<marker>Wallace, 2012</marker>
<rawString>Byron Wallace. 2012. Multiple narrative disentanglement: Unraveling infinite jest. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph B Walther</author>
<author>Kyle P D’Addario</author>
</authors>
<title>The impacts of emoticons on message interpretation in computer-mediated communication.</title>
<date>2001</date>
<journal>Social Science Computer Review,</journal>
<volume>19</volume>
<issue>3</issue>
<marker>Walther, D’Addario, 2001</marker>
<rawString>Joseph B. Walther and Kyle P. D’Addario. 2001. The impacts of emoticons on message interpretation in computer-mediated communication. Social Science Computer Review, 19(3):324–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wing</author>
<author>Jason Baldridge</author>
</authors>
<title>Simple supervised document geolocation with geodesic grids.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="11209" citStr="Wing and Baldridge, 2011" startWordPosition="1807" endWordPosition="1810">lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many of the features that characterize bad language have strong associations with specific social variables. In some cases, these associations mirror linguistic variables known from speech — such as geographically-associated lexical items like hella, or transcriptions of phonological variables like “g-dropping” (Eisenstein et al., 2010). But in other cases, apparently new lexical items, such as the abbreviations ctfu, lls, and af, acquire surprisingly strong associations with geographical areas and demographic groups (Eisenstein et al., 2011). A robust </context>
</contexts>
<marker>Wing, Baldridge, 2011</marker>
<rawString>Benjamin Wing and Jason Baldridge. 2011. Simple supervised document geolocation with geodesic grids. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tae Yano</author>
<author>William W Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting response to political blog posts with topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="24450" citStr="Yano et al. (2009)" startWordPosition="3962" endWordPosition="3965">domly sampled during the period from January 2010 to October 2012. • Twitter-#: tweets in which the first token is a hashtag. The hashtag itself is not included in the bigram counts; see below for more details on which bigrams are included. • Twitter-@: tweets in which the first token is a username. The username itself is not included in the bigram counts. • Penn Treebank: sections 2-21 • Infinite Jest: the text of the 1996 novel by David Foster Wallace (Wallace, 2012). Consists of only 482,558 tokens. • Blog articles: A randomly-sampled subset of the American political blog posts gathered by Yano et al. (2009). • Blog comments: A randomly-selected subset of comments associated with the blog posts described above. In all corpora, only fully alphabetic tokens are counted; thus, all hashtags and usernames are discarded. The Twitter text is tokenized using Tweet2A very recent study compares Twitter with other corpora, using a number of alternative metrics, such as the use of high and low frequency words, pronouns, and intensifiers (Hu et al., 2013). This is complementary to the present study, which focuses on the degree of difference in the lexical distributions of corpora gathered from various media. </context>
</contexts>
<marker>Yano, Cohen, Smith, 2009</marker>
<rawString>Tae Yano, William W. Cohen, and Noah A. Smith. 2009. Predicting response to political blog posts with topic models. In Proceedings of NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>