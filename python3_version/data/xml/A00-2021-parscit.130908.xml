<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003309">
<title confidence="0.9641585">
Exploiting auxiliary distributions in stochastic unification-based
grammars
</title>
<author confidence="0.852446">
Mark Johnson* Stefan Riezler
</author>
<affiliation confidence="0.7261195">
Cognitive and Linguistic Sciences Institut fiir Maschinelle Sprachverarbeitung
Brown University Universitat Stuttgart
</affiliation>
<email confidence="0.991956">
Markiohnson@Brown.edu riezler@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.993697" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889714285714">
This paper describes a method for estimat-
ing conditional probability distributions over
the parses of &amp;quot;unification-based&amp;quot; grammars
which can utilize auxiliary distributions that
are estimated by other means. We show how
this can be used to incorporate information
about lexical selectional preferences gathered
from other sources into Stochastic &amp;quot;Unification-
based&amp;quot; Grammars (SUBGs). While we ap-
ply this estimator to a Stochastic Lexical-
Functional Grammar, the method is general,
and should be applicable to stochastic versions
of HPSGs, categorial grammars and transfor-
mational grammars.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997572">
&amp;quot;Unification-based&amp;quot; Grammars (UBGs) can
capture a wide variety of linguistically impor-
tant syntactic and semantic constraints. How-
ever, because these constraints can be non-local
or context-sensitive, developing stochastic ver-
sions of UBGs and associated estimation pro-
cedures is not as straight-forward as it is for,
e.g., PCFGs. Recent work has shown how to
define probability distributions over the parses
of UBGs (Abney, 1997) and efficiently estimate
and use conditional probabilities for parsing
(Johnson et al., 1999). Like most other practical
stochastic grammar estimation procedures, this
latter estimation procedure requires a parsed
training corpus.
Unfortunately, large parsed UBG corpora are
not yet available. This restricts the kinds of
models one can realistically expect to be able
to estimate. For example, a model incorporat-
ing lexical selectional preferences of the kind
</bodyText>
<note confidence="0.407194">
* This research was supported by NSF awards 9720368,
9870676 and 9812169.
</note>
<bodyText confidence="0.99970075">
described below might have tens or hundreds
of thousands of parameters, which one could
not reasonably attempt to estimate from a cor-
pus with on the order of a thousand clauses.
However, statistical models of lexical selec-
tional preferences can be estimated from very
large corpora based on simpler syntactic struc-
tures, e.g., those produced by a shallow parser.
While there is undoubtedly disagreement be-
tween these simple syntactic structures and the
syntactic structures produced by the UBG, one
might hope that they are close enough for lexical
information gathered from the simpler syntactic
structures to be of use in defining a probability
distribution over the UBG&apos;s structures.
In the estimation procedure described here,
we call the probability distribution estimated
from the larger, simpler corpus an auxiliary dis-
tribution. Our treatment of auxiliary distribu-
tions is inspired by the treatment of reference
distributions in Jelinek&apos;s (1997) presentation of
Maximum Entropy estimation, but in our es-
timation procedure we simply regard the loga-
rithm of each auxiliary distribution as another
(real-valued) feature. Despite its simplicity, our
approach seems to offer several advantages over
the reference distribution approach. First, it
is straight-forward to utilize several auxiliary
distributions simultaneously: each is treated as
a distinct feature. Second, each auxiliary dis-
tribution is associated with a parameter which
scales its contribution to the final distribution.
In applications such as ours where the auxiliary
distribution may be of questionable relevance
to the distribution we are trying to estimate, it
seems reasonable to permit the estimation pro-
cedure to discount or even ignore the auxiliary
distribution. Finally, note that neither Jelinek&apos;s
nor our estimation procedures require that an
auxiliary or reference distribution Q be a prob-
</bodyText>
<page confidence="0.999363">
154
</page>
<bodyText confidence="0.984777">
ability distribution; i.e., it is not necessary that
Q(12) = 1, where 12 is the set of well-formed
linguistic structures.
The rest of this paper is structured as fol-
lows. Section 2 reviews how exponential mod-
els can be defined over the parses of UBGs,
gives a brief description of Stochastic Lexical-
Functional Grammar, and reviews why maxi-
mum pseudo-likelihood estimation is both feasi-
ble and sufficient of parsing purposes. Section 3
presents our new estimator, and shows how it
is related to the minimization of the Kullback-
Leibler divergence between the conditional es-
timated and auxiliary distributions. Section 4
describes the auxiliary distribution used in our
experiments, and section 5 presents the results
of those experiments.
</bodyText>
<sectionHeader confidence="0.982719" genericHeader="method">
2 Stochastic Unification-based
Grammars
</sectionHeader>
<bodyText confidence="0.9822986">
Most of the classes of probabilistic language
models used in computational linguistic are ex-
ponential families. That is, the probability P(w)
of a well-formed syntactic structure cv E SI is de-
fined by a function of the form
</bodyText>
<equation confidence="0.998350333333333">
QIcv) eA..f(w) (1)
P AP) =
4A
</equation>
<bodyText confidence="0.999209096774194">
where f (w) E Rin is a vector of feature values,
A E Rin is a vector of adjustable feature param-
eters, Q is a function of u.; (which Jelinek (1997)
calls a reference distribution when it is not an in-
dicator function), and ZA = fr? Q(co)eAâ€¢f(&apos;)dw is
a normalization factor called the partition func-
tion. (Note that a feature here is just a real-
valued function of a syntactic structure cv; to
avoid confusion we use the term &amp;quot;attribute&amp;quot; to
refer to a feature in a feature structure). If
Q(co) = 1 then the class of exponential dis-
tributions is precisely the class of distributions
with maximum entropy satisfying the constraint
that the expected values of the features is a cer-
tain specified value (e.g., a value estimated from
training data), so exponential models are some-
times also called &amp;quot;Maximum Entropy&amp;quot; models.
For example, the class of distributions ob-
tained by varying the parameters of a PCFG
is an exponential family. In a PCFG each rule
or production is associated with a feature, so m
is the number of rules and the jth feature value
1, (w) is the number of times the j rule is used
in the derivation of the tree co E ft. Simple ma-
nipulations show that PA (w) is equivalent to the
PCFG distribution if Ai = log pi, where pi is the
rule emission probability, and Q(w) = ZA = 1.
If the features satisfy suitable Markovian in-
dependence constraints, estimation from fully
observed training data is straight-forward. For
example, because the rule features of a PCFG
meet &amp;quot;context-free&amp;quot; Markovian independence
conditions, the well-known &amp;quot;relative frequency&amp;quot;
estimator for PCFGs both maximizes the likeli-
hood of the training data (and hence is asymp-
totically consistent and efficient) and minimizes
the Kullback-Leibler divergence between train-
ing and estimated distributions.
However, the situation changes dramatically
if we enforce non-local or context-sensitive con-
straints on linguistic structures of the kind that
can be expressed by a UBG. As Abney (1997)
showed, under these circumstances the relative
frequency estimator is in general inconsistent,
even if one restricts attention to rule features.
Consequently, maximum likelihood estimation
is much more complicated, as discussed in sec-
tion 2.2. Moreover, while rule features are natu-
ral for PCFGs given their context-free indepen-
dence properties, there is no particular reason
to use only rule features in Stochastic UBGs
(SUBGs). Thus an SUBG is a triple (G, f, A),
where G is a UBG which generates a set of well-
formed linguistic structures f2, and f and A are
vectors of feature functions and feature param-
eters as above. The probability of a structure
w E Srl is given by (1) with Q(w) = 1. Given a
base UBG, there are usually infinitely many dif-
ferent ways of selecting the features f to make
a SUBG, and each of these makes an empirical
claim about the class of possible distributions
of structures.
</bodyText>
<subsectionHeader confidence="0.9875195">
2.1 Stochastic Lexical Functional
Grammar
</subsectionHeader>
<bodyText confidence="0.9981546">
Stochastic Lexical-Functional Grammar
(SLFG) is a stochastic extension of Lexical-
Functional Grammar (LFG), a UBG formalism
developed by Kaplan and Bresnan (1982).
Given a base LFG, an SLFG is constructed
by defining features which identify salient
constructions in a linguistic structure (in LFG
this is a c-structure/f-structure pair and its
associated mapping; see Kaplan (1995)). Apart
from the auxiliary distributions, we based our
</bodyText>
<page confidence="0.995243">
155
</page>
<bodyText confidence="0.999952333333333">
features on those used in Johnson et al. (1999),
which should be consulted for further details.
Most of these feature values range over the
natural numbers, counting the number of times
that a particular construction appears in a
linguistic structure. For example, adjunct and
argument features count the number of adjunct
and argument attachments, permitting SLFG
to capture a general argument attachment pref-
erence, while more specialized features count
the number of attachments to each grammatical
function (e.g., SUBJ, OBJ, COMP, etc.).
The flexibility of features in stochastic UBGs
permits us to include features for relatively
complex constructions, such as date expres-
sions (it seems that date interpretations, if
possible, are usually preferred), right-branching
constituent structures (usually preferred) and
non-parallel coordinate structures (usually
dispreferred). Johnson et al. remark that they
would have liked to have included features for
lexical selectional preferences. While such fea-
tures are perfectly acceptable in a SLFG, they
felt that their corpora were so small that the
large number of lexical dependency parameters
could not be accurately estimated. The present
paper proposes a method to address this by
using an auxiliary distribution estimated from
a corpus large enough to (hopefully) provide
reliable estimates for these parameters.
</bodyText>
<subsectionHeader confidence="0.995234">
2.2 Estimating stochastic
unification-based grammars
</subsectionHeader>
<bodyText confidence="0.9975935">
Suppose = wi, ,w, is a corpus of n syn-
tactic structures. Letting fj() = .6 (wi)
and assuming each wi E Q, the likelihood of the
corpus LA() is:
</bodyText>
<equation confidence="0.820324">
LAP) = PA(wi)
a = i=i
eA.f(a&apos;) Z&apos;
A
MD) â€” nEA(fi )
log LA
0Aj P)
</equation>
<bodyText confidence="0.960511666666667">
where Ex(f3) is the expected value of h un-
der the distribution PA. The maximum likeli-
hood estimates are the A which maximize (2), or
equivalently, which make (3) zero, but as John-
son et al. (1999) explain, there seems to be no
practical way of computing these for realistic
SUBGs since evaluating (2) and its derivatives
(3) involves integrating over all syntactic struc-
tures Q.
However, Johnson et al. observe that parsing
applications require only the conditional prob-
ability distribution PA (wly), where y is the ter-
minal string or yield being parsed, and that this
can be estimated by maximizing the pseudo-
likelihood of the corpus PLA (CD):
</bodyText>
<equation confidence="0.982497">
PLA() = II PA (wi fyi )
i=1
eAlP) &amp;quot;
Z1(y) (4)
i=1
In (4), yi is the yield of wi and
ZA(yi) = fY) eA&amp;quot;&amp;quot;dw,
cl(.
</equation>
<bodyText confidence="0.959564857142857">
where I(y) is the set of all syntactic structures
in Q with yield yi (i.e., all parses of yi gener-
ated by the base UBG). It turns out that cal-
culating the pseudo-likelihood of a corpus only
involves integrations over the sets of parses of
its yields Q(yi), which is feasible for many inter-
esting UBGs. Moreover, the maximum pseudo-
likelihood estimator is asymptotically consistent
for the conditional distribution P(wly). For the
reasons explained in Johnson et al. (1999) we ac-
tually estimate A by maximizing a regularized
version of the log pseudo-likelihood (5), where
is 7 times the maximum value of fi found in
the training corpus:
</bodyText>
<equation confidence="0.7786115">
m A? (5)
log PLA (&amp;) â€” E
?
j=1 2a
</equation>
<bodyText confidence="0.8828078">
See Johnson et al. (1999) for details of the calcu-
lation of this quantity and its derivatives, and
the conjugate gradient routine used to calcu-
late the A which maximize the regularized log
pseudo-likelihood of the training corpus.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="method">
3 Auxiliary distributions
</sectionHeader>
<bodyText confidence="0.999946571428571">
We modify the estimation problem presented in
section 2.2 by assuming that in addition to the
corpusc.D and the m feature functions f we are
given k auxiliary distributions Qi, , Qk whose
support includes Q that we suspect may be re-
lated to the joint distribution P(w) or condi-
tional distribution P(wly) that we wish to esti-
</bodyText>
<page confidence="0.994921">
156
</page>
<bodyText confidence="0.999681166666667">
mate. We do not require that the Q3 be proba-
bility distributions, i.e., it is not necessary that
Qi (w)dw = 1, but we do require that they
are strictly positive (i.e., Q3(w) &gt; 0, Vw e Cl).
We define k new features fni+i , , fm,Â±k where
fm+3(w) = log Q3(w), which we call auxiliary
features. The m + k parameters associated with
the resulting m+k features can be estimated us-
ing any method for estimating the parameters
of an exponential family with real-valued fea-
tures (in our experiments we used the pseudo-
likelihood estimation procedure reviewed in sec-
tion 2.2). Such a procedure estimates parame-
ters Am+i, , Ani+ k associated with the auxil-
iary features, so the estimated distributions take
the form (6) (for simplicity we only discuss joint
distributions here, but the treatment of condi-
tional distributions is parallel).
</bodyText>
<equation confidence="0.9937985">
fl=i Q2(w)m+,
eEim=i Ai /J(w) .(6)
</equation>
<bodyText confidence="0.997391684210526">
shallow parses (compared to the LFG parses)
for the 117 million word British National Cor-
pus (Carroll and Rooth, 1998). We based our
auxiliary distribution on 3.7 million (g, r, a) tu-
ples (belonging to 600,000 types) we extracted
these parses, where g is a lexical governor (for
the shallow parses, g is either a verb or a prepo-
sition), a is the head of one of its NP arguments
and r is the the grammatical relationship be-
tween the governor and argument (in the shal-
low parses r is always OBJ for prepositional gov-
ernors, and r is either SUBJ or OBJ for verbal
governors).
In order to avoid sparse data problems we
smoothed this distribution over tuples as de-
scribed in (Rooth et al., 1999). We assume that
governor-relation pairs (g, r) and arguments a
are independently generated from 25 hidden
classes C, i.e.:
</bodyText>
<equation confidence="0.929303">
P((g, r, a)) = E Pe((g, r)(c)13, (a(c)P, (c)
cEc
</equation>
<bodyText confidence="0.998868111111111">
Note that the auxiliary distributions Qi are
treated as fixed distributions for the purposes
of this estimation, even though each Qi may it-
self be a complex model obtained via a previous
estimation process. Comparing (6) with (1) on
page 2, we see that the two equations become
identical if the reference distribution Q in (1) is
replaced by a geometric mixture of the auxiliary
distributions Qi, i.e., if:
</bodyText>
<equation confidence="0.9692825">
(W) = 11 Qj (w)
j= 1
</equation>
<bodyText confidence="0.9999593">
The parameter associated with an auxiliary fea-
ture represents the weight of that feature in the
mixture. If a parameter Ani+3 = 1 then the
corresponding auxiliary feature Qi is equivalent
to a reference distribution in Jelinek&apos;s sense,
while if Ani+3 = 0 then Qi is effectively ig-
nored. Thus our approach can be regarded as
a smoothed version Jelinek&apos;s reference distribu-
tion approach, generalized to permit multiple
auxiliary distributions.
</bodyText>
<sectionHeader confidence="0.939556" genericHeader="method">
4 Lexical selectional preferences
</sectionHeader>
<bodyText confidence="0.999987764705883">
The auxiliary distribution we used here is based
on the probabilistic model of lexical selectional
preferences described in Rooth et al. (1999). An
existing broad-coverage parser was used to find
where the distributions Pe are estimated from
the training tuples using the Expectation-
Maximization algorithm. While the hidden
classes are not given any prior interpretation
they often cluster semantically coherent pred-
icates and arguments, as shown in Figure 1.
The smoothing power of a clustering model such
as this can be calculated explicitly as the per-
centage of possible tuples which are assigned a
non-zero probability. For the 25-class model
we get a smoothing power of 99%, compared
to only 1.7% using the empirical distribution of
the training data.
</bodyText>
<sectionHeader confidence="0.988581" genericHeader="method">
5 Empirical evaluation
</sectionHeader>
<bodyText confidence="0.999689928571429">
Hadar Shemtov and Ron Kaplan at Xerox PARC
provided us with two LFG parsed corpora called
the Verbmobil corpus and the Homecentre cor-
pus. These contain parse forests for each sen-
tence (packed according to scheme described in
Maxwell and Kaplan (1995)), together with a
manual annotation as to which parse is cor-
rect. The Verbmobil corpus contains 540 sen-
tences relating to appointment planning, while
the Homecentre corpus contains 980 sentences
from Xerox documentation on their &amp;quot;homecen-
tre&amp;quot; multifunction devices. Xerox did not pro-
vide us with the base LFGs for intellectual prop-
erty reasons, but from inspection of the parses
</bodyText>
<page confidence="0.994102">
157
</page>
<table confidence="0.998808717391304">
Class 16 CO â€¢-â–  â€¢-â€¢ 0) 10 CO â– O â– 0 CI 0 0 CO t- I&amp;quot;- CO TJ cl â– O â€¢-â€¢ â€¢-â€¢ cr: Ch CO CO 1.-- tO CO CO â– 0 â– 0
PROS 0.0340 ., C,1 A I&amp;quot;. t- to to to LO tO Tr I c c O V â€¢ V c CO CO O CO CO 10 10 n I CO
E; 8 8288882888888888888888888888
6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 &lt;6 6 6 6 6 6 6 6 &lt;6
. u0 ).. 0
aa 71 46 g2g 0e E20,
Ev3t
.a &amp;quot;... F. , e.&apos; t
- , 12 t zt-aa&apos; t.8 gg,
,.. &amp;quot;73 . , , â€¢
. .. 5,22 8z
. .
bo c n. EE .0 7,-.., EE .- â€¢- 47 Â§ &apos;3 4 6 co .4 .... 0- 0 I,
Eâ€˜,..E...82..2:-E1 &apos;E 22 0.9,1002.?r,1.Z-5zET,,Ecat a.
0.3183 says
0.0405 saro
â€¢ â€¢ â€¢ â€¢ â€¢
0.0345 asks
0.0276 tell:s
0.0214 bets â€¢
0.0193 know:s â€¢ 0000000 â€¢ 00000 â€¢ â€¢ â€¢ â€¢ â€¢ 00000
0.0147 hovels OOOOOOOOOOOOOOO â€¢â€¢â€¢â€¢ OOOOO
0.0144 nod:s â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢
0.0137 think:s
0.0130 shake:s â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢
0.0128 take:s â€¢
0.0104 reply:s â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢
0.0096 smile:s â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢
0.0094 dols
0.0094 laugh:s â€¢ â€¢ â€¢ â€¢ 00000 â€¢ â€¢ â€¢ â€¢ â€¢ â€¢
0.0089 tell:o
0.0084 saws
0.0082 add:s â€¢ 00000 â€¢ â€¢ â€¢â€¢ â€¢ 00000 â€¢ â€¢â€¢â€¢â€¢ â€¢
â€¢ â€¢ oo
0.0078 feels
0.0071 make:s â€¢
0.0070 gives
0.0067 ask:o â€¢
0.0066 shrug:s â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢
0.0061 explain:s 00000 â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ 0000000000000 â€¢
0.0051 like:s â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ 000
0.0050 look:s â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ oo
0.0050 sigh:s â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢
0.0049 watch:s 0000000000 â€¢ â€¢ â€¢ â€¢ â€¢ 00000 â€¢ â€¢ â€¢ â€¢
0.0049 hear:s 0000000 â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ 00000 â€¢ â€¢ â€¢ â€¢
0.0047 answers oo â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢
</table>
<figureCaption confidence="0.848056">
Figure 1: A depiction of the highest probability predicates and arguments in Class 16. The class
matrix shows at the top the 30 most probable nouns in the Pe(a116) distribution and their probabil-
ities, and at the left the 30 most probable verbs and prepositions listed according to Pre((g,r)116)
and their probabilities. Dots in the matrix indicate that the respective pair was seen in the training
data. Predicates with suffix : s indicate the subject slot of an intransitive or transitive verb; the
suffix : o specifies the nouns in the corresponding row as objects of verbs or prepositions.
</figureCaption>
<bodyText confidence="0.999725914285714">
it seems that slightly different grammars were
used with each corpus, so we did not merge the
corpora. We chose the features of our SLFG
based solely on the basis of the Verbmobil cor-
pus, so the Homecentre corpus can be regarded
as a held-out evaluation corpus.
We discarded the unambiguous sentences in
each corpus for both training and testing (as
explained in Johnson et al. (1999), pseudo-
likelihood estimation ignores unambiguous sen-
tences), leaving us with a corpus of 324 am-
biguous sentences in the Verbmobil corpus and
481 sentences in the Homecentre corpus; these
sentences had a total of 3,245 and 3,169 parses
respectively.
The (non-auxiliary) features used in were
based on those described by Johnson et
al. (1999). Different numbers of features
were used with the two corpora because
some of the features were generated semi-
automatically (e.g., we introduced a feature for
every attribute-value pair found in any feature
structure), and &amp;quot;pseudo-constant&amp;quot; features (i.e.,
features whose values never differ on the parses
of the same sentence) are discarded. We used
172 features in the SLFG for the Verbmobil cor-
pus and 186 features in the SLFG for the Home-
centre corpus.
We used three additional auxiliary features
derived from the lexical selectional preference
model described in section 4. These were de-
fined in the following way. For each governing
predicate g, grammatical relation r and argu-
ment a, let n(g,,,a)(w) be the number of times
that the f-structure:
</bodyText>
<equation confidence="0.9937645">
FRED = g
r = [FRED = a] I
</equation>
<bodyText confidence="0.803372">
appears as a subgraph of the f-structure of
w, i.e., the number of times that a fills the
</bodyText>
<page confidence="0.996146">
158
</page>
<bodyText confidence="0.999428145454545">
grammatical role r of g. We used the lexical
model described in the last section to estimate
P(alg, r), and defined our first auxiliary feature
as:
smaller score corresponds to better performance
here. The correct parses measure is most closely
related to parser performance, but the pseudo-
likelihood measure is more closely related to the
quantity we are optimizing and may be more
relevant to applications where the parser has to
return a certainty factor associated with each
parse.
Table 1 also provides the number of indistin-
guishable sentences under each model. A sen-
tence y is indistinguishable with respect to fea-
tures f iff f (wc) = f (0, where w, is the correct
parse of y and we cot E S2(y), i.e., the feature
values of correct parse of y are identical to the
feature values of some other parse of y. If a
sentence is indistinguishable it is not possible
to assign its correct parse a (conditional) prob-
ability higher than the (conditional) probability
assigned to other parses, so all else being equal
we would expect a SUBG with with fewer indis-
tinguishable sentences to perform better than
one with more.
Adding auxiliary features reduced the already
low number of indistinguishable sentences in the
Verbmobil corpus by only 11%, while it reduced
the number of indistinguishable sentences in the
Homecentre corpus by 24%. This probably re-
flects the fact that the feature set was designed
by inspecting only the Verbmobil corpus.
We must admit disappointment with these
results. Adding auxiliary lexical features im-
proves the correct parses measure only slightly,
and degrades rather than improves performance
on the pseudo-likelihood measure. Perhaps this
is due to the fact that adding auxiliary features
increases the dimensionality of the feature vec-
tor f, so the pseudo-likelihood scores with dif-
ferent numbers of features are not strictly com-
parable.
The small improvement in the correct parses
measure is typical of the improvement we might
expect to achieve by adding a &amp;quot;good&amp;quot; non-
auxiliary feature, but given the importance usu-
ally placed on lexical dependencies in statistical
models one might have expected more improve-
ment. Probably the poor performance is due
in part to the fairly large differences between
the parses from which the lexical dependencies
were estimated and the parses produced by the
LFG. LFG parses are very detailed, and many
ambiguities depend on the precise grammatical
</bodyText>
<equation confidence="0.853742">
(w) log P(go) + E n(9,r,a)(w) log P(alg, r)
(g,r,a)
</equation>
<bodyText confidence="0.999821636363636">
where go is the predicate of the root feature
structure. The justification for this feature is
that if f-structures were in fact a tree, ft(w)
would be the (logarithm of) a probability dis-
tribution over them. The auxiliary feature ft
is defective in many ways. Because LFG f-
structures are DAGs with reentrancies rather
than trees we double count certain arguments,
so ft is certainly not the logarithm of a prob-
ability distribution (which is why we stressed
that our approach does not require an auxiliary
distribution to be a distribution).
The number of governor-argument tuples
found in different parses of the same sentence
can vary markedly. Since the conditional prob-
abilities P(afg, r) are usually very small, we
found that fi(w) was strongly related to the
number of tuples found in w, so the parse with
the smaller number of tuples usually obtains the
higher h score. We tried to address this by
adding two additional features. We set MA)) to
be the number of tuples in w, i.e.:
</bodyText>
<equation confidence="0.4874525">
Ma)
(g,r,0
</equation>
<bodyText confidence="0.999582421052632">
Then we set fn(w) = fi(w)/fc(w), i.e., fn(w) is
the average log probability of a lexical depen-
dency tuple under the auxiliary lexical distribu-
tion. We performed our experiments with ft as
the sole auxiliary distribution, and with fj, A
and fn as three auxiliary distributions.
Because our corpora were so small, we trained
and tested these models using a 10-fold cross-
validation paradigm; the cumulative results are
shown in Table 1. On each fold we evaluated
each model in two ways. The correct parses
measure simply counts the number of test sen-
tences for which the estimated model assigns
its maximum parse probability to the correct
parse, with ties broken randomly. The pseudo-
likelihood measure is the pseudo-likelihood of
test set parses; i.e., the conditional probability
of the test parses given their yields. We actu-
ally report the negative log of this measure, so a
</bodyText>
<page confidence="0.996662">
159
</page>
<table confidence="0.9941561">
Verbmobil corpus (324 sentences, 172 non-auxiliary features)
Auxiliary features used Indistinguishable Correct - log PL
(none) 9 180 401.3
ft 8 183 401.6
Ii, fe, In 8 180.5 404.0
Homecentre corpus (481 sentences, 186 non-auxiliary features)
Auxiliary features used Indistinguishable Correct - log PL
(none) 45 283.25 580.6
ft 34 284 580.6
ft, ic, .f. 34 285 582.2
</table>
<tableCaption confidence="0.8236835">
Table 1: The effect of adding auxiliary lexical dependency features to a SLFG. The auxiliary
features are described in the text. The column labelled &amp;quot;indistinguishable&amp;quot; gives the number of
</tableCaption>
<bodyText confidence="0.985065105263158">
indistinguishable sentences with respect to each feature set, while &amp;quot;correct&amp;quot; and &amp;quot;- log PL&amp;quot; give
the correct parses and pseudo-likelihood measures respectively.
relationship holding between a predicate and its
argument. It could also be that better perfor-
mance could be achieved if the lexical dependen-
cies were estimated from a corpus more closely
related to the actual test corpus. For example,
the verb feed in the Homecentre corpus is used in
the sense of &amp;quot;insert (paper into printer)&amp;quot;, which
hardly seems to be a prototypical usage.
Note that overall system performance is quite
good; taking the unambiguous sentences into
account the combined LFG parser and statisti-
cal model finds the correct parse for 73% of the
Verbmobil test sentences and 80% of the Home-
centre test sentences. On just the ambiguous
sentences, our system selects the correct parse
for 56% of the Verbmobil test sentences and 59%
of the Homecentre test sentences.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991173913043">
This paper has presented a method for incorpo-
rating auxiliary distributional information gath-
ered by other means possibly from other corpora
into a Stochastic &amp;quot;Unification-based&amp;quot; Grammar
(SUBG). This permits one to incorporate de-
pendencies into a SUBG which probably can-
not be estimated directly from the small UBG
parsed corpora available today. It has the virtue
that it can incorporate several auxiliary dis-
tributions simultaneously, and because it asso-
ciates each auxiliary distribution with its own
&amp;quot;weight&amp;quot; parameter, it can scale the contribu-
tions of each auxiliary distribution toward the
final estimated distribution, or even ignore it
entirely. We have applied this to incorporate
lexical selectional preference information into
a Stochastic Lexical-Functional Grammar, but
the technique generalizes to stochastic versions
of HPSGs, categorial grammars and transfor-
mational grammars. An obvious extension of
this work, which we hope will be persued in the
future, is to apply these techniques in broad-
coverage feature-based TAG parsers.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999767136363636">
Steven P. Abney. 1997. Stochastic Attribute-
Value Grammars. Computational Linguis-
tics, 23(4):597-617.
Glenn Carroll and Mats Rooth. 1998. Valence
induction with a head-lexicalized PCFG. In
Proceedings of EMNLP-3, Granada.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition. The MIT Press, Cam-
bridge, Massachusetts.
Mark Johnson, Stuart Geman, Stephen Canon,
Zhiyi Chi, and Stefan Riezler. 1999. Estima-
tors for stochastic &amp;quot;unification-based&amp;quot; gram-
mars. In The Proceedings of the 37th Annual
Conference of the Association for Computa-
tional Linguistics, pages 535-541, San Fran-
cisco. Morgan Kaufmann.
Ronald M. Kaplan and Joan Bresnan. 1982.
Lexical-Functional Grammar: A formal sys-
tem for grammatical representation. In Joan
Bresnan, editor, The Mental Representation
of Grammatical Relations, chapter 4, pages
173-281. The MIT Press.
</reference>
<page confidence="0.96846">
160
</page>
<reference confidence="0.999679681818182">
Ronald M. Kaplan. 1995. The formal architec-
ture of LFG. In Mary Dalrymple, Ronald M.
Kaplan, John T. Maxwell III, and Annie
Zaenen, editors, Formal Issues in Lexical-
Functional Grammar, number 47 in CSLI
Lecture Notes Series, chapter 1, pages 7-28.
CSLI Publications.
John T. Maxwell III and Ronald M. Kaplan.
1995. A method for disjunctive constraint
satisfaction. In Mary Dalrymple, Ronald M.
Kaplan, John T. Maxwell III, and Annie
Zaenen, editors, Formal Issues in Lexical-
Functional Grammar, number 47 in CSLI
Lecture Notes Series, chapter 14, pages 381-
481. CSLI Publications.
Mats Rooth, Stefan Riezler, Detlef Prescher,
Glenn Carrollâ€ž and Franz Beil. 1999. Induc-
ing a semantically annotated lexicon via EM-
based clustering. In Proceedings of the 37th
Annual Meeting of the Association for Com-
putational Linguistics, San Francisco. Mor-
gan Kaufmann.
</reference>
<page confidence="0.998232">
161
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.524750">
<title confidence="0.993914">Exploiting auxiliary distributions in stochastic unification-based grammars</title>
<author confidence="0.998949">Mark Johnson Stefan Riezler</author>
<affiliation confidence="0.8770095">Cognitive and Linguistic Sciences Institut fiir Maschinelle Sprachverarbeitung Brown University Universitat Stuttgart</affiliation>
<email confidence="0.788386">Markiohnson@Brown.eduriezler@ims.uni-stuttgart.de</email>
<abstract confidence="0.9926254">This paper describes a method for estimating conditional probability distributions over the parses of &amp;quot;unification-based&amp;quot; grammars which can utilize auxiliary distributions that are estimated by other means. We show how this can be used to incorporate information about lexical selectional preferences gathered from other sources into Stochastic &amp;quot;Unificationbased&amp;quot; Grammars (SUBGs). While we apply this estimator to a Stochastic Lexical- Functional Grammar, the method is general, and should be applicable to stochastic versions of HPSGs, categorial grammars and transformational grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic AttributeValue Grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--4</pages>
<contexts>
<context position="1316" citStr="Abney, 1997" startWordPosition="172" endWordPosition="173">chastic LexicalFunctional Grammar, the method is general, and should be applicable to stochastic versions of HPSGs, categorial grammars and transformational grammars. 1 Introduction &amp;quot;Unification-based&amp;quot; Grammars (UBGs) can capture a wide variety of linguistically important syntactic and semantic constraints. However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, e.g., PCFGs. Recent work has shown how to define probability distributions over the parses of UBGs (Abney, 1997) and efficiently estimate and use conditional probabilities for parsing (Johnson et al., 1999). Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus. Unfortunately, large parsed UBG corpora are not yet available. This restricts the kinds of models one can realistically expect to be able to estimate. For example, a model incorporating lexical selectional preferences of the kind * This research was supported by NSF awards 9720368, 9870676 and 9812169. described below might have tens or hundreds of thousands of para</context>
<context position="6726" citStr="Abney (1997)" startWordPosition="1031" endWordPosition="1032">s, estimation from fully observed training data is straight-forward. For example, because the rule features of a PCFG meet &amp;quot;context-free&amp;quot; Markovian independence conditions, the well-known &amp;quot;relative frequency&amp;quot; estimator for PCFGs both maximizes the likelihood of the training data (and hence is asymptotically consistent and efficient) and minimizes the Kullback-Leibler divergence between training and estimated distributions. However, the situation changes dramatically if we enforce non-local or context-sensitive constraints on linguistic structures of the kind that can be expressed by a UBG. As Abney (1997) showed, under these circumstances the relative frequency estimator is in general inconsistent, even if one restricts attention to rule features. Consequently, maximum likelihood estimation is much more complicated, as discussed in section 2.2. Moreover, while rule features are natural for PCFGs given their context-free independence properties, there is no particular reason to use only rule features in Stochastic UBGs (SUBGs). Thus an SUBG is a triple (G, f, A), where G is a UBG which generates a set of wellformed linguistic structures f2, and f and A are vectors of feature functions and featu</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven P. Abney. 1997. Stochastic AttributeValue Grammars. Computational Linguistics, 23(4):597-617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of EMNLP-3,</booktitle>
<location>Granada.</location>
<contexts>
<context position="12733" citStr="Carroll and Rooth, 1998" startWordPosition="2030" endWordPosition="2033">an be estimated using any method for estimating the parameters of an exponential family with real-valued features (in our experiments we used the pseudolikelihood estimation procedure reviewed in section 2.2). Such a procedure estimates parameters Am+i, , Ani+ k associated with the auxiliary features, so the estimated distributions take the form (6) (for simplicity we only discuss joint distributions here, but the treatment of conditional distributions is parallel). fl=i Q2(w)m+, eEim=i Ai /J(w) .(6) shallow parses (compared to the LFG parses) for the 117 million word British National Corpus (Carroll and Rooth, 1998). We based our auxiliary distribution on 3.7 million (g, r, a) tuples (belonging to 600,000 types) we extracted these parses, where g is a lexical governor (for the shallow parses, g is either a verb or a preposition), a is the head of one of its NP arguments and r is the the grammatical relationship between the governor and argument (in the shallow parses r is always OBJ for prepositional governors, and r is either SUBJ or OBJ for verbal governors). In order to avoid sparse data problems we smoothed this distribution over tuples as described in (Rooth et al., 1999). We assume that governor-re</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of EMNLP-3, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="4907" citStr="Jelinek (1997)" startWordPosition="728" endWordPosition="729">onal estimated and auxiliary distributions. Section 4 describes the auxiliary distribution used in our experiments, and section 5 presents the results of those experiments. 2 Stochastic Unification-based Grammars Most of the classes of probabilistic language models used in computational linguistic are exponential families. That is, the probability P(w) of a well-formed syntactic structure cv E SI is defined by a function of the form QIcv) eA..f(w) (1) P AP) = 4A where f (w) E Rin is a vector of feature values, A E Rin is a vector of adjustable feature parameters, Q is a function of u.; (which Jelinek (1997) calls a reference distribution when it is not an indicator function), and ZA = fr? Q(co)eAâ€¢f(&apos;)dw is a normalization factor called the partition function. (Note that a feature here is just a realvalued function of a syntactic structure cv; to avoid confusion we use the term &amp;quot;attribute&amp;quot; to refer to a feature in a feature structure). If Q(co) = 1 then the class of exponential distributions is precisely the class of distributions with maximum entropy satisfying the constraint that the expected values of the features is a certain specified value (e.g., a value estimated from training data), so ex</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic &amp;quot;unification-based&amp;quot; grammars.</title>
<date>1999</date>
<booktitle>In The Proceedings of the 37th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>535--541</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="1410" citStr="Johnson et al., 1999" startWordPosition="183" endWordPosition="186"> stochastic versions of HPSGs, categorial grammars and transformational grammars. 1 Introduction &amp;quot;Unification-based&amp;quot; Grammars (UBGs) can capture a wide variety of linguistically important syntactic and semantic constraints. However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, e.g., PCFGs. Recent work has shown how to define probability distributions over the parses of UBGs (Abney, 1997) and efficiently estimate and use conditional probabilities for parsing (Johnson et al., 1999). Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus. Unfortunately, large parsed UBG corpora are not yet available. This restricts the kinds of models one can realistically expect to be able to estimate. For example, a model incorporating lexical selectional preferences of the kind * This research was supported by NSF awards 9720368, 9870676 and 9812169. described below might have tens or hundreds of thousands of parameters, which one could not reasonably attempt to estimate from a corpus with on the order of </context>
<context position="8160" citStr="Johnson et al. (1999)" startWordPosition="1262" endWordPosition="1265"> of these makes an empirical claim about the class of possible distributions of structures. 2.1 Stochastic Lexical Functional Grammar Stochastic Lexical-Functional Grammar (SLFG) is a stochastic extension of LexicalFunctional Grammar (LFG), a UBG formalism developed by Kaplan and Bresnan (1982). Given a base LFG, an SLFG is constructed by defining features which identify salient constructions in a linguistic structure (in LFG this is a c-structure/f-structure pair and its associated mapping; see Kaplan (1995)). Apart from the auxiliary distributions, we based our 155 features on those used in Johnson et al. (1999), which should be consulted for further details. Most of these feature values range over the natural numbers, counting the number of times that a particular construction appears in a linguistic structure. For example, adjunct and argument features count the number of adjunct and argument attachments, permitting SLFG to capture a general argument attachment preference, while more specialized features count the number of attachments to each grammatical function (e.g., SUBJ, OBJ, COMP, etc.). The flexibility of features in stochastic UBGs permits us to include features for relatively complex cons</context>
<context position="9939" citStr="Johnson et al. (1999)" startWordPosition="1541" endWordPosition="1545">s a method to address this by using an auxiliary distribution estimated from a corpus large enough to (hopefully) provide reliable estimates for these parameters. 2.2 Estimating stochastic unification-based grammars Suppose = wi, ,w, is a corpus of n syntactic structures. Letting fj() = .6 (wi) and assuming each wi E Q, the likelihood of the corpus LA() is: LAP) = PA(wi) a = i=i eA.f(a&apos;) Z&apos; A MD) â€” nEA(fi ) log LA 0Aj P) where Ex(f3) is the expected value of h under the distribution PA. The maximum likelihood estimates are the A which maximize (2), or equivalently, which make (3) zero, but as Johnson et al. (1999) explain, there seems to be no practical way of computing these for realistic SUBGs since evaluating (2) and its derivatives (3) involves integrating over all syntactic structures Q. However, Johnson et al. observe that parsing applications require only the conditional probability distribution PA (wly), where y is the terminal string or yield being parsed, and that this can be estimated by maximizing the pseudolikelihood of the corpus PLA (CD): PLA() = II PA (wi fyi ) i=1 eAlP) &amp;quot; Z1(y) (4) i=1 In (4), yi is the yield of wi and ZA(yi) = fY) eA&amp;quot;&amp;quot;dw, cl(. where I(y) is the set of all syntactic st</context>
<context position="11193" citStr="Johnson et al. (1999)" startWordPosition="1766" endWordPosition="1769">, all parses of yi generated by the base UBG). It turns out that calculating the pseudo-likelihood of a corpus only involves integrations over the sets of parses of its yields Q(yi), which is feasible for many interesting UBGs. Moreover, the maximum pseudolikelihood estimator is asymptotically consistent for the conditional distribution P(wly). For the reasons explained in Johnson et al. (1999) we actually estimate A by maximizing a regularized version of the log pseudo-likelihood (5), where is 7 times the maximum value of fi found in the training corpus: m A? (5) log PLA (&amp;) â€” E ? j=1 2a See Johnson et al. (1999) for details of the calculation of this quantity and its derivatives, and the conjugate gradient routine used to calculate the A which maximize the regularized log pseudo-likelihood of the training corpus. 3 Auxiliary distributions We modify the estimation problem presented in section 2.2 by assuming that in addition to the corpusc.D and the m feature functions f we are given k auxiliary distributions Qi, , Qk whose support includes Q that we suspect may be related to the joint distribution P(w) or conditional distribution P(wly) that we wish to esti156 mate. We do not require that the Q3 be p</context>
<context position="18169" citStr="Johnson et al. (1999)" startWordPosition="3108" endWordPosition="3111"> respective pair was seen in the training data. Predicates with suffix : s indicate the subject slot of an intransitive or transitive verb; the suffix : o specifies the nouns in the corresponding row as objects of verbs or prepositions. it seems that slightly different grammars were used with each corpus, so we did not merge the corpora. We chose the features of our SLFG based solely on the basis of the Verbmobil corpus, so the Homecentre corpus can be regarded as a held-out evaluation corpus. We discarded the unambiguous sentences in each corpus for both training and testing (as explained in Johnson et al. (1999), pseudolikelihood estimation ignores unambiguous sentences), leaving us with a corpus of 324 ambiguous sentences in the Verbmobil corpus and 481 sentences in the Homecentre corpus; these sentences had a total of 3,245 and 3,169 parses respectively. The (non-auxiliary) features used in were based on those described by Johnson et al. (1999). Different numbers of features were used with the two corpora because some of the features were generated semiautomatically (e.g., we introduced a feature for every attribute-value pair found in any feature structure), and &amp;quot;pseudo-constant&amp;quot; features (i.e., f</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic &amp;quot;unification-based&amp;quot; grammars. In The Proceedings of the 37th Annual Conference of the Association for Computational Linguistics, pages 535-541, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: A formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations, chapter 4,</booktitle>
<pages>173--281</pages>
<editor>In Joan Bresnan, editor,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="7834" citStr="Kaplan and Bresnan (1982)" startWordPosition="1211" endWordPosition="1214"> which generates a set of wellformed linguistic structures f2, and f and A are vectors of feature functions and feature parameters as above. The probability of a structure w E Srl is given by (1) with Q(w) = 1. Given a base UBG, there are usually infinitely many different ways of selecting the features f to make a SUBG, and each of these makes an empirical claim about the class of possible distributions of structures. 2.1 Stochastic Lexical Functional Grammar Stochastic Lexical-Functional Grammar (SLFG) is a stochastic extension of LexicalFunctional Grammar (LFG), a UBG formalism developed by Kaplan and Bresnan (1982). Given a base LFG, an SLFG is constructed by defining features which identify salient constructions in a linguistic structure (in LFG this is a c-structure/f-structure pair and its associated mapping; see Kaplan (1995)). Apart from the auxiliary distributions, we based our 155 features on those used in Johnson et al. (1999), which should be consulted for further details. Most of these feature values range over the natural numbers, counting the number of times that a particular construction appears in a linguistic structure. For example, adjunct and argument features count the number of adjunc</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-Functional Grammar: A formal system for grammatical representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, chapter 4, pages 173-281. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
</authors>
<title>The formal architecture of LFG.</title>
<date>1995</date>
<booktitle>Formal Issues in LexicalFunctional Grammar, number 47 in CSLI Lecture Notes Series, chapter 1,</booktitle>
<pages>7--28</pages>
<editor>In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="8053" citStr="Kaplan (1995)" startWordPosition="1246" endWordPosition="1247">ere are usually infinitely many different ways of selecting the features f to make a SUBG, and each of these makes an empirical claim about the class of possible distributions of structures. 2.1 Stochastic Lexical Functional Grammar Stochastic Lexical-Functional Grammar (SLFG) is a stochastic extension of LexicalFunctional Grammar (LFG), a UBG formalism developed by Kaplan and Bresnan (1982). Given a base LFG, an SLFG is constructed by defining features which identify salient constructions in a linguistic structure (in LFG this is a c-structure/f-structure pair and its associated mapping; see Kaplan (1995)). Apart from the auxiliary distributions, we based our 155 features on those used in Johnson et al. (1999), which should be consulted for further details. Most of these feature values range over the natural numbers, counting the number of times that a particular construction appears in a linguistic structure. For example, adjunct and argument features count the number of adjunct and argument attachments, permitting SLFG to capture a general argument attachment preference, while more specialized features count the number of attachments to each grammatical function (e.g., SUBJ, OBJ, COMP, etc.)</context>
<context position="15412" citStr="Kaplan (1995)" startWordPosition="2482" endWordPosition="2483">cates and arguments, as shown in Figure 1. The smoothing power of a clustering model such as this can be calculated explicitly as the percentage of possible tuples which are assigned a non-zero probability. For the 25-class model we get a smoothing power of 99%, compared to only 1.7% using the empirical distribution of the training data. 5 Empirical evaluation Hadar Shemtov and Ron Kaplan at Xerox PARC provided us with two LFG parsed corpora called the Verbmobil corpus and the Homecentre corpus. These contain parse forests for each sentence (packed according to scheme described in Maxwell and Kaplan (1995)), together with a manual annotation as to which parse is correct. The Verbmobil corpus contains 540 sentences relating to appointment planning, while the Homecentre corpus contains 980 sentences from Xerox documentation on their &amp;quot;homecentre&amp;quot; multifunction devices. Xerox did not provide us with the base LFGs for intellectual property reasons, but from inspection of the parses 157 Class 16 CO â€¢-â–  â€¢-â€¢ 0) 10 CO â– O â– 0 CI 0 0 CO t- I&amp;quot;- CO TJ cl â– O â€¢-â€¢ â€¢-â€¢ cr: Ch CO CO 1.-- tO CO CO â– 0 â– 0 PROS 0.0340 ., C,1 A I&amp;quot;. t- to to to LO tO Tr I c c O V â€¢ V c CO CO O CO CO 10 10 n I CO E; 8 828888288888888888</context>
</contexts>
<marker>Kaplan, 1995</marker>
<rawString>Ronald M. Kaplan. 1995. The formal architecture of LFG. In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors, Formal Issues in LexicalFunctional Grammar, number 47 in CSLI Lecture Notes Series, chapter 1, pages 7-28. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John T Maxwell</author>
<author>Ronald M Kaplan</author>
</authors>
<title>A method for disjunctive constraint satisfaction.</title>
<date>1995</date>
<booktitle>Formal Issues in LexicalFunctional Grammar, number 47 in CSLI Lecture Notes Series, chapter 14,</booktitle>
<pages>381--481</pages>
<editor>In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="15412" citStr="Maxwell and Kaplan (1995)" startWordPosition="2480" endWordPosition="2483">herent predicates and arguments, as shown in Figure 1. The smoothing power of a clustering model such as this can be calculated explicitly as the percentage of possible tuples which are assigned a non-zero probability. For the 25-class model we get a smoothing power of 99%, compared to only 1.7% using the empirical distribution of the training data. 5 Empirical evaluation Hadar Shemtov and Ron Kaplan at Xerox PARC provided us with two LFG parsed corpora called the Verbmobil corpus and the Homecentre corpus. These contain parse forests for each sentence (packed according to scheme described in Maxwell and Kaplan (1995)), together with a manual annotation as to which parse is correct. The Verbmobil corpus contains 540 sentences relating to appointment planning, while the Homecentre corpus contains 980 sentences from Xerox documentation on their &amp;quot;homecentre&amp;quot; multifunction devices. Xerox did not provide us with the base LFGs for intellectual property reasons, but from inspection of the parses 157 Class 16 CO â€¢-â–  â€¢-â€¢ 0) 10 CO â– O â– 0 CI 0 0 CO t- I&amp;quot;- CO TJ cl â– O â€¢-â€¢ â€¢-â€¢ cr: Ch CO CO 1.-- tO CO CO â– 0 â– 0 PROS 0.0340 ., C,1 A I&amp;quot;. t- to to to LO tO Tr I c c O V â€¢ V c CO CO O CO CO 10 10 n I CO E; 8 828888288888888888</context>
</contexts>
<marker>Maxwell, Kaplan, 1995</marker>
<rawString>John T. Maxwell III and Ronald M. Kaplan. 1995. A method for disjunctive constraint satisfaction. In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors, Formal Issues in LexicalFunctional Grammar, number 47 in CSLI Lecture Notes Series, chapter 14, pages 381-481. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carrollâ€ž</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EMbased clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<marker>Rooth, Riezler, Prescher, Carrollâ€ž, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carrollâ€ž and Franz Beil. 1999. Inducing a semantically annotated lexicon via EMbased clustering. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, San Francisco. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>