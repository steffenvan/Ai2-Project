<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001341">
<title confidence="0.989512">
SemEval-2014 Task 6:
Supervised Semantic Parsing of Robotic Spatial Commands
</title>
<author confidence="0.995545">
Kais Dukes
</author>
<affiliation confidence="0.999805">
School of Computing, University of Leeds
</affiliation>
<address confidence="0.976555">
Leeds LS2 9JT, United Kingdom
</address>
<email confidence="0.998304">
sckd@leeds.ac.uk
</email>
<sectionHeader confidence="0.993751" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998463944444444">
SemEval-2014 Task 6 aims to advance
semantic parsing research by providing a
high-quality annotated dataset to com-
pare and evaluate approaches. The task
focuses on contextual parsing of robotic
commands, in which the additional con-
text of spatial scenes can be used to guide
a parser to control a robot arm. Six teams
submitted systems using both rule-based
and statistical methods. The best per-
forming (hybrid) system scored 92.5%
and 90.5% for parsing with and without
spatial context. However, the best per-
forming statistical system scored 87.35%
and 60.84% respectively, indicating that
generalized understanding of commands
given to a robot remains challenging, de-
spite the fixed domain used for the task.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999429882352941">
Semantic parsers analyze sentences to produce
formal meaning representations that are used for
the computational understanding of natural lan-
guage. Recently, state-of-the-art semantic pars-
ing methods have used for a variety of applica-
tions, including question answering (Kwiat-
kowski et al., 2013; Krishnamurthy and Mitchell,
2012), dialog systems (Artzi and Zettlemoyer,
2011), entity relation extraction (Kate and
Mooney, 2010) and robotic control (Tellex,
2011; Kim and Mooney, 2012).
Different parsers can be distinguished by the
level of supervision they require during training.
Fully supervised training typically requires an
annotated dataset that maps natural language
(NL) to a formal meaning representation such as
logical form. However, because annotated data is
</bodyText>
<footnote confidence="0.482426666666667">
This work is licensed under a Creative Commons Attribution
4.0 International License. License details:
http://creativecommons.org/licenses/by/4.0
</footnote>
<bodyText confidence="0.999649565217391">
often not available, a recent trend in semantic
parsing research has been to eschew supervised
training in favour of either unsupervised or
weakly-supervised methods that utilize addi-
tional information. For example, Berant and Li-
ang (2014) use a dataset of 5,810 question-
answer pairs without annotated logical forms to
induce a parser for a question-answering system.
In comparison, Poon (2013) converts NL ques-
tions into formal queries via indirect supervision
through database interaction.
In contrast to previous work, the shared task
described in this paper uses the Robot Com-
mands Treebank (Dukes, 2013a), a new dataset
made available for supervised semantic parsing.
The chosen domain is robotic control, in which
NL commands are given to a robot arm used to
manipulate shapes on an 8 x 8 game board. De-
spite the fixed domain, the task is challenging as
correctly parsing commands requires understand-
ing spatial context. For example, the command in
Figure 1 may have several plausible interpreta-
tions, given different board configurations.
</bodyText>
<note confidence="0.60101">
‘Move the pyramid on the blue cube on the gray one.’
</note>
<figureCaption confidence="0.987974">
Figure 1: Example scene with a contextual spatial
command from the Robot Commands Treebank.
</figureCaption>
<page confidence="0.989916">
45
</page>
<note confidence="0.980625">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 45–53,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999836">
The task is inspired by the classic AI system
SHRLDU, which responded to NL commands to
control a robot for a similar game board (Wino-
grad, 1972), although that system is reported to
not have generalized well (Dreyfus, 2009; Mit-
kov, 1999). More recent research in command
understanding has focused on parsing jointly
with grounding, the process of mapping NL de-
scriptions of entities within an environment to a
semantic representation. Previous work includes
Tellex et al. (2011), who develop a small corpus
of commands for a simulated fork lift robot, with
grounding performed using a factor graph. Simi-
larly, Kim and Mooney (2012) perform joint
parsing and grounding using a corpus of naviga-
tion commands. In contrast, this paper focuses on
parsing using additional situational context for
disambiguation and by using a larger NL dataset,
in comparison to previous robotics research.
In the remainder of this paper, we describe the
task, the dataset and the metrics used for evalua-
tion. We then compare the approaches used by
participant systems and conclude with suggested
improvements for future work.
</bodyText>
<sectionHeader confidence="0.988399" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.977080958333333">
The long term research goal encouraged by the
task is to develop a system that will robustly
execute NL robotic commands. In general, this is
a highly complex problem involving computa-
tional processing of language, spatial reasoning,
contextual awareness and knowledge representa-
tion. To simplify the problem, participants were
provided with additional tools and resources,
allowing them to focus on developing a semantic
parser for a fixed domain that would fit into an
existing component architecture. Figure 2 shows
how these components interact.
Semantic parser: Systems submitted by partici-
pants are semantic parsers that accept an NL
command as input, mapping this to a formal Ro-
bot Control Language (RCL), described further
in section 3.3. The Robot Commands Treebank
used for the both training and evaluation is an
annotated corpus that pairs NL commands with
contextual RCL statements.
Spatial planner: A spatial planner is provided
as an open Java API1. Commands in the treebank
are specified in the context of spatial scenes. By
interfacing with the planner, participant systems
</bodyText>
<footnote confidence="0.821903">
1 https://github.com/kaisdukes/train-robots
</footnote>
<subsectionHeader confidence="0.373296">
Robotic simulator
</subsectionHeader>
<figureCaption confidence="0.996916">
Figure 2: Integrated command understanding system.
</figureCaption>
<bodyText confidence="0.9999246">
have access to this additional information. For
example, given an RCL fragment for the expres-
sion ‘the red cube on the blue block’, the planner
will ground the entity, returning a list of zero or
more board coordinates corresponding to possi-
ble matches. The planner also validates com-
mands to determine if they are compatible with
spatial context. It can therefore be used to con-
strain the search space of possible parses, as well
as enabling early resolution of attachment ambi-
guity during parsing.
Robotic simulator: The simulated environment
consists of an 8 x 8 board that can hold prisms
and cubes which occur in eight different colors.
The robot’s gripper can move to any discrete po-
sition within an 8 x 8 x 8 space above the board.
The planner uses the simulator to enforce physi-
cal laws within the game. For example, a block
cannot remain unsupported in empty space due
to gravity. Similarly, prisms cannot lie below
other block types. In the integrated system, the
parser uses the planner for context, then provides
the final RCL statement to the simulator which
executes the command by moving the robot arm
to update the board.
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="method">
3 Data
</sectionHeader>
<subsectionHeader confidence="0.999844">
3.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.999891363636364">
For the shared task, 3,409 sentences were se-
lected from the treebank. This data size compares
with related corpora used for semantic parsing
such as the ATIS (Zettlemoyer and Collins,
2007), GeoQuery (Kate et al., 2005), Jobs (Tang
and Mooney, 2001) and RoboCup (Kuhlmann et
al., 2004) datasets, consisting of 4,978; 880; 640
and 300 sentences respectively.
The treebank was developed via a game with a
purpose (www.TrainRobots.com), in which play-
ers were shown before and after configurations
</bodyText>
<figure confidence="0.995171333333333">
Spatial planner
spatial context
NL command
Semantic parser
RCL
parsing
</figure>
<page confidence="0.979811">
46
</page>
<figureCaption confidence="0.999356">
Figure 3: Semantic tree from the treebank with an elliptical anaphoric node and its annotated antecedent.
</figureCaption>
<bodyText confidence="0.99995">
and asked to give a corresponding command to a
hypothetical robot arm. To make the game more
competitive and to promote data quality, players
rated each other’s sentences and were rewarded
with points for accurate entries (Dukes, 2013b).
</bodyText>
<subsectionHeader confidence="0.998526">
3.2 Annotation
</subsectionHeader>
<bodyText confidence="0.999995">
In total, over 10,000 commands were collected
through the game. During an offline annotation
phase, sentences were manually mapped to RCL.
However, due to the nature of the game, players
were free to enter arbitrarily complex sentences
to describe moves, not all of which could be rep-
resented by RCL. In addition, some commands
were syntactically well-formed, but not compati-
ble with the corresponding scenes. The 3,409
commands selected for the task had RCL state-
ments that were both understood by the planner
</bodyText>
<figure confidence="0.255276625">
(sequence:
(event:
(action: take)
(entity:
(id: 1)
(color: cyan)
(type: prism)
(spatial-relation:
(relation: above)
(entity:
(color: white)
(type: cube)))))
(event:
(action: drop)
(entity:
(type: reference)
(reference-id: 1))
(destination:
(spatial-relation:
(relation: above)
(entity:
(color: blue)
(color: green)
(type: stack))))))
</figure>
<figureCaption confidence="0.998885">
Figure 4: RCL representation with co-referencing.
</figureCaption>
<bodyText confidence="0.999914">
and when given to the robotic simulator resulted
in the expected move being made between before
and after board configurations. Due to this extra
validation step, all RCL statements provided for
the task were contextually well-formed.
</bodyText>
<subsectionHeader confidence="0.985231">
3.3 Robot Control Language
</subsectionHeader>
<bodyText confidence="0.999886363636364">
RCL is a novel linguistically-oriented semantic
representation. An RCL statement is a semantic
tree (Figure 3) where leaf nodes generally align
to words in the corresponding sentence, and non-
leaves are tagged using a pre-defined set of cate-
gories. RCL is designed to annotate rich linguis-
tic structure, including ellipsis (such as ‘place [it]
on’), anaphoric references (‘it’ and ‘one’), multi-
word spatial expressions (‘on top of’) and lexical
disambiguation (‘one’ and ‘place’). Due to ellip-
sis, unaligned words and multi-word expressions,
a leaf node may align to zero, one or more words
in a sentence. Figure 4 shows the RCL syntax for
the tree in Figure 3, as accepted by the spatial
planner and the simulator. As these components
do not require NL word alignment data, this ad-
ditional information was made available to task
participants for training via a separate Java API.
The tagset used to annotate RCL nodes can be
divided into general tags (that are arguably ap-
plicable to other domains) and specific tags that
were customized for the domain in the task (Ta-
bles 1 and 2 overleaf, respectively). The general
elements are typed entities (labelled with seman-
tic features) that are connected using relations
and events. This universal formalism is not do-
main-specific, and is inspired by semantic frames
(Fillmore and Baker, 2001), a practical represen-
tation used for NL understanding systems (Dzik-
ovska, 2004; UzZaman and Allen, 2010; Coyne
et al., 2010; Dukes, 2009).
In the remainder of this section we summarize
aspects of RCL that are relevant to the task; a
</bodyText>
<page confidence="0.996043">
47
</page>
<bodyText confidence="0.969302413793103">
more detailed description is provided by Dukes
(2013a; 2014). In an RCL statement such as Fig-
ure 4, a preterminal node together with its child
leaf node correspond to a feature-value pair
(such as the feature color and the constant blue).
Two special features which are distinguished by
the planner are id and reference-id, which are
used for co-referencing such as for annotating
anaphora and their antecedents. The remaining
features model the simulated robotic domain. For
RCL Element Description
action Aligned to a verbal group in NL,
e.g. ‘drop’ or ‘pick up’.
cardinal Number (e.g. 2 or ‘three’).
color Colored attribute of an entity.
destination A spatial destination.
entity Entity within the domain.
event Specification of a command.
id Id for anaphoric references.
indicator Spatial attribute of an entity.
measure Used for distance metrics.
reference-id A resolved reference.
relation Relation type (e.g. ‘above’).
sequence Used to specify a sequence of
events or statements.
spatial-relation Used to specify a spatial relation
between two entities or to de-
scribe a location.
type Used to specify an entity type.
</bodyText>
<tableCaption confidence="0.996222">
Table 1: Universal semantic elements in RCL.
</tableCaption>
<bodyText confidence="0.860203208333333">
Category Values
Actions move, take, drop
Relations left, right, above, below,
forward, backward, adjacent,
within, between, nearest, near,
furthest, far, part
Indicators left, leftmost, right, rightmost,
top, highest, bottom, lowest,
front, back, individual, furthest,
nearest, center
entity types cube, prism, corner, board stack,
row, column, edge, tile, robot,
region, reference, type-reference
Colors blue, cyan, red, yellow,
green, magenta, gray, white
example, the values of the action feature are the
moves used to control the robotic arm, while
values of the type and relation features are the
entity and relation types understood by the spa-
tial planner (Table 2). As well as qualitative rela-
tions (such as ‘below’ or ‘above’), the planner
also accepts spatial relations that include quanti-
tative measurements, such as in ‘two squares left
of the red prism’ (Figure 5).
</bodyText>
<figureCaption confidence="0.993409">
Fig.ure 5: A quantitative relation with a landmark.
</figureCaption>
<bodyText confidence="0.999987444444445">
RCL distinguishes between relations which
relate entities and indicators, which are attributes
of entities (such as ‘left’ in ‘the left cube’). For
the task, participants are asked to map NL sen-
tences to well-formed RCL by identifying spatial
relations and indicators, then parsing higher-level
entities and events. Finally, a well-formed RCL
tree with an event (or sequence of events) at top-
level is given the simulator for execution.
</bodyText>
<sectionHeader confidence="0.998412" genericHeader="method">
4 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.999585428571429">
Out of the 3,400 sentences annotated for the task,
2,500 sentences were provided to participants for
system training. During evaluation, trained sys-
tems were presented with 909 previously unseen
sentences and asked to generate corresponding
RCL statements, with access to the spatial plan-
ner for additional context. To keep the evaluation
process as simple as possible, each parser’s out-
put for a sentence was scored as correct if it ex-
actly matched the expected RCL statement in the
treebank. Participants were asked to calculate
two metrics, P and IP, which are the proportion
of exact matches with and without using the spa-
tial planner respectively:
</bodyText>
<tableCaption confidence="0.991609">
Table 2: Semantic categories customized for the task.
</tableCaption>
<page confidence="0.97243">
48
</page>
<note confidence="0.730328714285714">
System Authors Statistical? Strategy P IP IP - P
UW-MRS Packard Hybrid Rule-based ERG + Berkeley parser 92.50 90.50 -2.00
AT&amp;T Labs Stoyanchev et al. Statistical Statistical maximum entropy parser 87.35 60.84 -26.51
RoBox Evang and Bos Statistical CCG parser + structured perceptron 86.80 79.21 -7.59
Shrdlite Ljunglöf Rule-based Hand crafted domain-specific grammar 86.10 51.50 -34.60
KUL-Eval Mattelaer et al. Statistical CCG parser 71.29 57.76 -13.53
UWM Kate Statistical KRISP parser N/A 45.98 N/A
</note>
<tableCaption confidence="0.999357">
Table 3: System results for supervised semantic parsing of the Robot Commands Treebank
</tableCaption>
<equation confidence="0.6183065">
(P = parsing with integrated spatial planning, NP = parsing without integrated spatial planning,
NP - P = drop in performance without integrated spatial planning, N/A = performance not available).
</equation>
<bodyText confidence="0.999681">
These metrics contrast with measures for par-
tially correct parsed structures, such as Parseval
(Black et al., 1991) or the leaf-ancestor metric
(Sampson and Babarczy, 2003). The rationale for
using a strict match is that in the integrated sys-
tem, a command will only be executed if it is
completely understood, as both the spatial plan-
ner and the simulator require well-formed RCL.
</bodyText>
<sectionHeader confidence="0.985757" genericHeader="method">
5 Systems and Results
</sectionHeader>
<bodyText confidence="0.999917126760563">
Six teams participated in the shared task using a
variety of strategies (Table 3). The last measure
in the table gives the performance drop without
spatial context. The value IP - P = -2 for the
best performing system suggests this as an upper
bound for the task. The different values of this
measure indicate the sensitivity to (or possibly
reliance on) context to guide the parsing process.
In the remainder of this section we compare the
approaches and results of the six systems.
UW-MRS: Packard (2014) achieved the best
score for parsing both with and without spatial
context, at 92.5% and 90.5%, respectively, using
a hybrid system that combines a rule-based
grammar with the Berkeley parser (Petrov et al.,
2006). The rule-based component uses the Eng-
lish Resource Grammar, a broad coverage hand-
written HPSG grammar for English. The ERG
produces a ranked list of Minimal Recursion
Semantics (MRS) structures that encode predi-
cate argument relations (Copestake et al., 2005).
Approximately 80 rules were then used to con-
vert MRS to RCL. The highest ranked result that
is validated by the spatial planner was selected as
the output of the rule-based system. Using this
approach, Packard reports scores of P = 82.4%
and IP = 80.3% for parsing the evaluation data.
To further boost performance, the Berkeley
parser was used for back-off. To train the parser,
the RCL treebank was converted to phrase struc-
ture by removing non-aligned nodes and insert-
ing additional nodes to ensure one-to-one align-
ment with words in NL sentences. Performance
of the Berkeley parser alone was IP = 81.5% (no
P-measure was available as spatial planning was
not integrated).
To combine components, the ERG was used
initially, with fall back to the Berkeley parser
when no contextually compatible RCL statement
was produced. The hybrid approach improved
accuracy considerably, with P = 92.5% and IP =
90.5%. Interestingly, Packard also performs pre-
cision and recall analysis, and reports that the
rule-based component had higher precision,
while the statistical component had higher recall,
with the combined system outperforming each
separate component in both precision and recall.
AT&amp;T Labs Research: The system by Stoy-
anchev et al. (2014) scored second best for con-
textual parsing and third best for parsing without
using the spatial planner (P = 87.35% and IP =
60.84%). In contrast to Packard’s UW-MRS
submission, the AT&amp;T system is a combination
of three statistical models for tagging, parsing
and reference resolution. During the tagging
phase, a two-stage sequence tagger first assigns a
part-of-speech tag to each word in a sentence,
followed by an RCL feature-value pair such as
(type: cube) or (color: blue), with unaligned
words tagged as ‘O’. For parsing, a constituency
parser was trained using non-lexical RCL trees.
Finally, anaphoric references were resolved us-
ing a maximum entropy feature model. When
combined, the three components generate a list
of weighted RCL trees, which are filtered by the
spatial planner. Without integrated planning, the
most-probable parse tree is selected.
In their evaluation, Stoyanchev et al. report
accuracy scores for the separate phases as well as
for the combined system. For the tagger, they
report an accuracy score of 95.2%, using the
</bodyText>
<page confidence="0.998755">
49
</page>
<bodyText confidence="0.999925009345795">
standard split of 2,500 sentences for training and
909 for evaluation. To separately measure the
joint accuracy of the parser together with refer-
ence resolution, gold-standard tags were used
resulting in a performance of P = 94.83% and IP
= 67.55%. However, using predicted tags, the
system’s final performance dropped to P =
87.35% and IP = 60.84%. To measure the effect
of less supervision, the models were additionally
trained on only 500 sentences. In this scenario,
the tagging model degraded significantly, while
the parsing and reference resolution models per-
formed nearly as well.
RoBox: Using Combinatory Categorial Grammar
(CCG) as a semantic parsing framework has
been previously shown to be suitable for translat-
ing NL into logical form. Inspired by previous
work using a CCG parser in combination with a
structured perceptron (Zettlemoyer and Collins,
2007), RoBox (Evang and Bos, 2014) was the
best performing CCG system in the shared task
scoring P = 86.8% and IP = 79.21%.
Using a similar approach to UW-MRS for its
statistical component, RCL trees were interpreted
as phrase-structure and converted to CCG deriva-
tions for training. During decoding, RCL state-
ments were generated directly by the CCG
parser. However, in contrast to the approach used
by the AT&amp;T system, RoBox interfaces with the
planner during parsing instead of performing
spatial validation a post-processing step. This
enables early resolution of attachment ambiguity
and helps constrain the search space. However,
the planner is only used to validate entity ele-
ments, so that event and sequence elements were
not validated. As a further difference to the
AT&amp;T system, anaphora resolution was not per-
formed using a statistical model. Instead, multi-
ple RCL trees were generated with different can-
didate anaphoric references, which were filtered
out contextually using the spatial planner.
RoBox suffered only a 7.59% absolute drop in
performance without using spatial planning, sec-
ond only to UW-MRS at 2%. Evang and Bos
perform error analysis on RoBox and report that
most errors relate to ellipsis, the ambiguous word
one, anaphora or attachment ambiguity. They
suggest that the system could be improved with
better feature selection or by integrating the CCG
parser more closely with the spatial planner.
Shrdlite: The Shrdlite system by Ljunglöf
(2014), inspired by the Classic SHRDLU system
by Winograd (1972), is a purely rule-based sys-
tem that was shown to be effective for the task.
Scoring P = 86.1% and IP = 51.5%, Shrdlite
ranked fourth for parsing with integrated plan-
ning, and fifth without using spatial context.
However, it suffered the largest absolute drop in
performance without planning (34.6 points), in-
dicating that integration with the planner is es-
sential for the system’s reported accuracy.
Shrdlite uses a hand-written compact unifica-
tion grammar for the fragment of English appear-
ing in the training data. The grammar is small,
consisting of only 25 grammatical rules and 60
lexical rules implemented as a recursive-descent
parser in Prolog. The lexicon consists of 150
words (and multi-word expressions) divided into
23 lexical categories, based on the RCL pre-
terminal nodes found in the treebank. In a post-
processing phase, the resulting parse trees are
normalized to ensure that they are well-formed
by using a small set of supplementary rules.
However, the grammar is highly ambiguous
resulting in multiple parses for a given input sen-
tence. These are filtered by the spatial planner. If
multiple parse trees were found to be compatible
with spatial context (or when not using the plan-
ner), the tree with the smallest number of nodes
was selected as the parser’s final output. Addi-
tionally, because both the training and evaluation
data were collected via crowdsourcing, sentences
occasionally contain spelling errors, which were
intentionally included in the task. To handle mis-
spelt words, Shrdlite uses Levenshtein edit dis-
tance with a penalty to reparse sentences when
the parser initially fails to produce any analysis.
KUL-Eval: The CCG system by Mattelaer et al.
(2014) uses a different approach to the RoBox
system described previously. KUL-Eval scored P
= 71.29% and IP = 57.76% in comparison to the
RoBox scores of P = 86.8% and IP = 79.21%.
During training, the RCL treebank was con-
verted to X-expressions. This process is fully re-
versible, so that no information in an RCL tree is
lost during conversion. In contrast to RoBox, but
in common with the AT&amp;T parser, KUL-Eval
performs spatial validation as a post-processing
step and does not integrate the planner directly
into the parsing process. A probabilistic CCG is
used for parsing, so that multiple X-expressions
are returned (each with an associated confidence
measure) that are translated into RCL. Finally, in
the validation step, the spatial planner is used to
discard RCL statements that are incompatible
with spatial context and the remaining most-
probable parse is returned as the system’s output.
</bodyText>
<page confidence="0.990072">
50
</page>
<bodyText confidence="0.999071888888889">
Mattelaer et al. note that in several cases the
parser produced partially correct statements but
that these outputs did not contribute to the final
score, given the strictly matching measures used
for the P and IP metrics. However, well-formed
RCL statements are required by the spatial plan-
ner and robotic simulator for the integrated sys-
tem to robustly execute the specified NL com-
mand. Partially correct structures included state-
ments which almost matched the expected RCL
tree with the exception of incorrect feature-
values, or the addition or deletion of nodes. The
most common errors were feature-values with
incorrect entity types (such as ‘edge’ and ‘re-
gion’) and mismatched spatial relations (such as
confusing ‘above’ and ‘within’ and confusing
‘right’, ‘left’ and ‘front’).
UWM: The UWM system submitted by Kate
(2014) uses an existing semantic parser, KRISP,
for the shared task. KRISP (Kernel-based Robust
Interpretation for Semantic Parsing) is a trainable
semantic parser (Kate and Mooney, 2006) that
uses Support Vector Machines (SVMs) as the
machine learning method with a string subse-
quence kernel. As well as training data consisting
of RCL paired with NL commands, KRISP re-
quired a context-free grammar for RCL, which
was hand-written for UWM. During training, id
nodes were removed from the RCL trees. These
were recovered after parsing in a post-processing
phase to resolve anaphora by matching to the
nearest preceding antecedent.
In contrast to other systems submitted for the
task, UWM does not interface with the spatial
planner and parses purely non-contextually. Be-
cause the planner was not used, the system’s ac-
curacy was negatively impacted by simple issues
that may have been easily resolved using spatial
context. For example, in RCL, the verb ‘place’
can map to either drop or move actions, depend-
ing on whether or not a block is held in the grip-
per in the corresponding spatial scene. Without
using spatial context, it is hard to distinguish be-
tween these cases during parsing.
The system scored a non-contextual measure
of IP = 45.98%, with Kate reporting a 51.18%
best F-measure (at 72.67% precision and 39.49%
recall). No P-measure was reported as the spatial
planner was not used. Due to memory constraints
when training the SVM classifiers, only 1,500
out of 2,500 possible sentences were used from
the treebank to build the parsing model. How-
ever, it may be possible to increasing the size of
training data in future work through sampling.
</bodyText>
<sectionHeader confidence="0.997494" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99999120754717">
The six systems evaluated for the task employed
a variety of semantic parsing strategies. With the
exception of one submission, all systems inter-
faced with the spatial planner, either in a post-
processing phase, or directly during parsing to
enable early disambiguation and to help con-
strain the search space. An open question that
remains following the task is how applicable
these methods would be to other domains. Sys-
tems that relied heavily on the planner to guide
the parsing process could only be adapted to do-
mains for a which a planner could conceivably
exist. For example, nearly all robotic tasks such
as such as navigation, object manipulation and
task execution involve aspects of planning. NL
question-answering interfaces to databases or
knowledge stores are also good candidates for
this approach, since parsing NL questions into a
semantic representation within the context of a
database schema or an ontology could be guided
by a query planner.
However, approaches with a more attractive
IP - P measure (such as UW-MRS and RoBox)
are arguably more easily generalized to other
domains, as they are less reliant on a planner.
Additionally, the usual arguments for rule-based
systems verses supervised statistical systems ap-
ply to any discussion on domain adaptation: rule-
based systems require human manual effort,
while supervised statistical systems required an-
notated data for the new domain.
In comparing the best two statistical systems
(AT&amp;T and RoBox) it is interesting to note that
these performed similarly with integrated plan-
ning (P = 87.35% and 86.80%, respectively), but
differed considerably without planning (IP =
60.84% and 79.21%). As these two systems em-
ployed different parsers (a constituency parser
and a CCG parser), it is difficult to perform a
direct comparison to understand why the AT&amp;T
system is more reliant on spatial context. It
would also be interesting to understand, in fur-
ther work, why the two CCG-based systems dif-
fered considerably in their P and IP scores.
It is also surprising that the best performing
system, UW-MRS, suffered only a 2% drop in
performance without using the planner, demon-
strating clearly that in the majority of sentences
in the evaluation data, spatial context is not actu-
ally required to perform semantic parsing. Al-
though as shown by the IP - P scores, spatial
context can dramatically boost performance of
certain approaches for the task when used.
</bodyText>
<page confidence="0.997963">
51
</page>
<sectionHeader confidence="0.979047" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999993526315789">
This paper described a new task for SemEval:
Supervised Semantic Parsing of Robotic Spatial
Commands. Despite its novel nature, the task
attracted high-quality submissions from six
teams, using a variety of semantic parsing strate-
gies.
It is hoped that this task will reappear at Se-
mEval. Several lessons were learnt from this first
version of the shared task which can be used to
improve the task in future. One issue which sev-
eral participants noted was the way in which the
treebank was split into training and evaluation
datasets. Out of the 3,409 sentences in the tree-
bank, the first 2,500 sequential sentences were
chosen for training. Because this data was not
randomized, certain syntactic structures were
only found during evaluation and were not pre-
sent in the training data. Although this may have
affected results, all participants evaluated their
systems against the same datasets. Based on par-
ticipant feedback, in addition to reporting P and
NP-measures, it would also be illuminating to
include a metric such as Parseval F1-scores to
measure partial accuracy. An improved version
of the task could also feature a better dataset by
expanding the treebank, not only in terms of size
but also in terms of linguistic structure. Many
commands captured in the annotation game are
not yet represented in RCL due to linguistic phe-
nomena such as negation and conditional state-
ments.
Looking forward, a more promising approach
to improving the spatial planner could be prob-
abilistic planning, so that semantic parsers could
interface with probabilistic facts with confidence
measures. This approach is particularly suitable
for robotics, where sensors often supply noisy
signals about the robot’s environment.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999655">
The author would like to thank the numerous
volunteer annotators who helped develop the
dataset used for the task using crowdsourcing, by
participating in the online game-with-a-purpose.
</bodyText>
<sectionHeader confidence="0.998832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998929879310345">
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping Semantic Parsers from Conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP (pp.
421–432).
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of the
Conference of the Association for Computational
Linguistics, ACL (pp. 1415–1425).
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, et al. 1991. A Procedure for Quantita-
tively Comparing the Syntactic Coverage of Eng-
lish Grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop (pp. 306-
311). San Mateo, California.
Ann Copestake, et al. 2005. Minimal Recursion Se-
mantics: An Introduction. Research on Language
and Computation, 3(2) (pp. 281-332).
Bob Coyne, Owen Rambow, et al. 2010. Frame Se-
mantics in Text-to-Scene Generation. Knowledge-
Based and Intelligent Information and Engineering
Systems (pp. 375-384). Springer, Berlin.
Hubert Dreyfus and Stuart Dreyfus. 2009. Why Com-
puters May Never Think Like People. Readings in
the Philosophy of Technology.
Kais Dukes. 2009. LOGICON: A System for Extract-
ing Semantic Structure using Partial Parsing. In In-
ternational Conference on Recent Advances in
Natural Language Processing, RANLP (pp. 18-
22). Borovets, Bulgaria.
Kais Dukes. 2013a. Semantic Annotation of Robotic
Spatial Commands. In Proceedings of the Lan-
guage and Technology Conference, LTC.
Kais Dukes. 2013b. Train Robots: A Dataset for
Natural Language Human-Robot Spatial Interac-
tion through Verbal Commands. In International
Conference on Social Robotics. Embodied Com-
munication of Goals and Intentions Workshop.
Kais Dukes. 2014. Contextual Semantic Parsing using
Crowdsourced Spatial Descriptions. Computation
and Language, arXiv:1405.0145 [cs.CL]
Myroslava Dzikovska 2004. A Practical Semantic
Representation For Natural Language Parsing. PhD
Thesis. University of Rochester.
Kilian Evang and Johan Bos. 2014. RoBox: CCG
with Structured Perceptron for Supervised Seman-
tic Parsing of Robotic Spatial Commands. In Pro-
ceedings of the International Workshop on Seman-
tic Evaluation, SemEval.
Charles Fillmore and Collin Baker. 2001. Frame se-
mantics for Text Understanding. In Proceedings of
WordNet and Other Lexical Resources Workshop.
Rohit Kate and Ray Mooney. 2006. Using String
Kernels for Learning Semantic Parsers. In Pro-
ceedings of the International Conference on Com-
putational Linguistics and Annual Meeting of the
Association for Computational Linguistics, COL-
ING-ACL (pp. 913–920).
</reference>
<page confidence="0.973787">
52
</page>
<reference confidence="0.999837488888888">
Rohit Kate and Raymond Mooney. 2010. Joint Entity
and Relation Extraction using Card-Pyramid Pars-
ing. In Proceedings of the Conference on Compu-
tational Natural Language Learning, CoNLL (pp.
203-212).
Rohit Kate, Yuk Wah Wong and Raymond Mooney.
2005. Learning to Transform Natural to Formal
Languages. In Proceedings of the National Confer-
ence on Artificial Intelligence (pp. 1062-1068).
Rohit Kate. 2014. UWM: Applying an Existing
Trainable Semantic Parser to Parse Robotic Spatial
Commands. In Proceedings of the International
Workshop on Semantic Evaluation, SemEval.
Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised PCFG Induction for Grounded Language
Learning with Highly Ambiguous Supervision. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning,
EMNLP-CoNLL (pp. 433-444).
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly Supervised Training of Semantic Parsers.
In Proceedings of the Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning,
EMNLP-CoNLL.
Gregory Kuhlmann et al. 2004. Guiding a Reinforce-
ment Learner with Natural Language Advice: Ini-
tial Results in RoboCup Soccer. In Proceedings of
the AAAI Workshop on Supervisory Control of
Learning and Adaptive Systems.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi and
Luke Zettlemoyer. 2013. Scaling Semantic Parsers
with On-the-fly Ontology Matching. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP.
Peter Ljunglöf. 2014. Shrdlite: Semantic Parsing us-
ing a Handmade Grammar. In Proceedings of the
International Workshop on Semantic Evaluation,
SemEval.
Willem Mattelaer, Mathias Verbeke and Davide Nitti.
2014. KUL-Eval: A Combinatory Categorial
Grammar Approach for Improving Semantic Pars-
ing of Robot Commands using Spatial Context. In
Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval.
Ruslan Mitkov. 1999. Anaphora Resolution: The
State of the Art. Technical Report. University of
Wolverhampton.
Woodley Packard. 2014. UW-MRS: Leveraging a
Deep Grammar for Robotic Spatial Commands. In
Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval.
Slav Petrov, et al. 2006. Learning Accurate, Compact,
and Interpretable Tree Annotation. In Proceedings
of the International Conference on Computational
Linguistics and the Annual Meeting of the Associa-
tion for Computational Linguistics, COLING-ACL
(pp. 433-440).
Hoifung Poon. 2013. Grounded Unsupervised Seman-
tic Parsing. In Proceedings of the Conference of the
Association for Computational Linguistics, ACL
(pp. 466-477).
Geoffrey Sampson and Anna Babarczy. 2003. A Test
of the Leaf-Ancestor Metric for Parse Accuracy.
Natural Language Engineering, 9.4 (pp. 365-380).
Svetlana Stoyanchev, et al. 2014. AT&amp;T Labs Re-
search: Tag&amp;Parse Approach to Semantic Parsing
of Robot Spatial Commands. In Proceedings of the
International Workshop on Semantic Evaluation,
SemEval.
Lappoon Tang and Raymond Mooney. 2001. Using
Multiple Clause Constructors in Inductive Logic
Programming for Semantic Parsing. Machine
Learning, ECML.
Stefanie Tellax, et al. 2011. Approaching the Symbol
Grounding Problem with Probabilistic Graphical
Models. AI Magazine, 32:4 (pp. 64-76).
Naushad UzZaman and James Allen. 2010. TRIPS
and TRIOS System for TempEval-2. In Proceed-
ings of the International Workshop on Semantic
Evaluation, SemEval (pp. 276-283).
Terry Winograd. 1972. Understanding Natural Lan-
guage. Cognitive Psychology, 3:1 (pp. 1-191).
Luke Zettlemoyer and Michael Collins. 2007. Online
Learning of Relaxed CCG Grammars for Parsing to
Logical Form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, EMNLP-CoNLL (pp. 878-887).
</reference>
<page confidence="0.999346">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.749083">
<title confidence="0.9547905">SemEval-2014 Task 6: Supervised Semantic Parsing of Robotic Spatial Commands</title>
<author confidence="0.886836">Kais</author>
<affiliation confidence="0.984107">School of Computing, University of</affiliation>
<address confidence="0.894809">Leeds LS2 9JT, United</address>
<email confidence="0.990109">sckd@leeds.ac.uk</email>
<abstract confidence="0.999176631578947">SemEval-2014 Task 6 aims to advance semantic parsing research by providing a high-quality annotated dataset to compare and evaluate approaches. The task focuses on contextual parsing of robotic commands, in which the additional context of spatial scenes can be used to guide a parser to control a robot arm. Six teams submitted systems using both rule-based and statistical methods. The best performing (hybrid) system scored 92.5% and 90.5% for parsing with and without spatial context. However, the best performing statistical system scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Bootstrapping Semantic Parsers from Conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>421--432</pages>
<contexts>
<context position="1287" citStr="Artzi and Zettlemoyer, 2011" startWordPosition="186" endWordPosition="189">tial context. However, the best performing statistical system scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task. 1 Introduction Semantic parsers analyze sentences to produce formal meaning representations that are used for the computational understanding of natural language. Recently, state-of-the-art semantic parsing methods have used for a variety of applications, including question answering (Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012), dialog systems (Artzi and Zettlemoyer, 2011), entity relation extraction (Kate and Mooney, 2010) and robotic control (Tellex, 2011; Kim and Mooney, 2012). Different parsers can be distinguished by the level of supervision they require during training. Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form. However, because annotated data is This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0 often not available, a recent trend in semantic parsing </context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping Semantic Parsers from Conversations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP (pp. 421–432).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics, ACL</booktitle>
<pages>1415--1425</pages>
<contexts>
<context position="2070" citStr="Berant and Liang (2014)" startWordPosition="293" endWordPosition="297">el of supervision they require during training. Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form. However, because annotated data is This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0 often not available, a recent trend in semantic parsing research has been to eschew supervised training in favour of either unsupervised or weakly-supervised methods that utilize additional information. For example, Berant and Liang (2014) use a dataset of 5,810 questionanswer pairs without annotated logical forms to induce a parser for a question-answering system. In comparison, Poon (2013) converts NL questions into formal queries via indirect supervision through database interaction. In contrast to previous work, the shared task described in this paper uses the Robot Commands Treebank (Dukes, 2013a), a new dataset made available for supervised semantic parsing. The chosen domain is robotic control, in which NL commands are given to a robot arm used to manipulate shapes on an 8 x 8 game board. Despite the fixed domain, the ta</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of the Conference of the Association for Computational Linguistics, ACL (pp. 1415–1425).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Steven Abney</author>
<author>Dan Flickinger</author>
<author>Claudia Gdaniec</author>
</authors>
<title>A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop</booktitle>
<pages>306--311</pages>
<location>San Mateo, California.</location>
<contexts>
<context position="14478" citStr="Black et al., 1991" startWordPosition="2242" endWordPosition="2245"> 86.80 79.21 -7.59 Shrdlite Ljunglöf Rule-based Hand crafted domain-specific grammar 86.10 51.50 -34.60 KUL-Eval Mattelaer et al. Statistical CCG parser 71.29 57.76 -13.53 UWM Kate Statistical KRISP parser N/A 45.98 N/A Table 3: System results for supervised semantic parsing of the Robot Commands Treebank (P = parsing with integrated spatial planning, NP = parsing without integrated spatial planning, NP - P = drop in performance without integrated spatial planning, N/A = performance not available). These metrics contrast with measures for partially correct parsed structures, such as Parseval (Black et al., 1991) or the leaf-ancestor metric (Sampson and Babarczy, 2003). The rationale for using a strict match is that in the integrated system, a command will only be executed if it is completely understood, as both the spatial planner and the simulator require well-formed RCL. 5 Systems and Results Six teams participated in the shared task using a variety of strategies (Table 3). The last measure in the table gives the performance drop without spatial context. The value IP - P = -2 for the best performing system suggests this as an upper bound for the task. The different values of this measure indicate t</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, 1991</marker>
<rawString>Ezra Black, Steven Abney, Dan Flickinger, Claudia Gdaniec, et al. 1991. A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars. In Proceedings of the DARPA Speech and Natural Language Workshop (pp. 306-311). San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Minimal Recursion Semantics: An Introduction.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>281--332</pages>
<marker>Copestake, 2005</marker>
<rawString>Ann Copestake, et al. 2005. Minimal Recursion Semantics: An Introduction. Research on Language and Computation, 3(2) (pp. 281-332).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Owen Rambow</author>
</authors>
<date>2010</date>
<booktitle>Frame Semantics in Text-to-Scene Generation. KnowledgeBased and Intelligent Information and Engineering Systems</booktitle>
<pages>375--384</pages>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<marker>Coyne, Rambow, 2010</marker>
<rawString>Bob Coyne, Owen Rambow, et al. 2010. Frame Semantics in Text-to-Scene Generation. KnowledgeBased and Intelligent Information and Engineering Systems (pp. 375-384). Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hubert Dreyfus</author>
<author>Stuart Dreyfus</author>
</authors>
<date>2009</date>
<booktitle>Why Computers May Never Think Like People. Readings in the Philosophy of Technology.</booktitle>
<marker>Dreyfus, Dreyfus, 2009</marker>
<rawString>Hubert Dreyfus and Stuart Dreyfus. 2009. Why Computers May Never Think Like People. Readings in the Philosophy of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>LOGICON: A System for Extracting Semantic Structure using Partial Parsing.</title>
<date>2009</date>
<booktitle>In International Conference on Recent Advances in Natural Language Processing, RANLP</booktitle>
<pages>18--22</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="10212" citStr="Dukes, 2009" startWordPosition="1582" endWordPosition="1583">parate Java API. The tagset used to annotate RCL nodes can be divided into general tags (that are arguably applicable to other domains) and specific tags that were customized for the domain in the task (Tables 1 and 2 overleaf, respectively). The general elements are typed entities (labelled with semantic features) that are connected using relations and events. This universal formalism is not domain-specific, and is inspired by semantic frames (Fillmore and Baker, 2001), a practical representation used for NL understanding systems (Dzikovska, 2004; UzZaman and Allen, 2010; Coyne et al., 2010; Dukes, 2009). In the remainder of this section we summarize aspects of RCL that are relevant to the task; a 47 more detailed description is provided by Dukes (2013a; 2014). In an RCL statement such as Figure 4, a preterminal node together with its child leaf node correspond to a feature-value pair (such as the feature color and the constant blue). Two special features which are distinguished by the planner are id and reference-id, which are used for co-referencing such as for annotating anaphora and their antecedents. The remaining features model the simulated robotic domain. For RCL Element Description a</context>
</contexts>
<marker>Dukes, 2009</marker>
<rawString>Kais Dukes. 2009. LOGICON: A System for Extracting Semantic Structure using Partial Parsing. In International Conference on Recent Advances in Natural Language Processing, RANLP (pp. 18-22). Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Semantic Annotation of Robotic Spatial Commands.</title>
<date>2013</date>
<booktitle>In Proceedings of the Language and Technology Conference, LTC.</booktitle>
<contexts>
<context position="2438" citStr="Dukes, 2013" startWordPosition="353" endWordPosition="354">4.0 often not available, a recent trend in semantic parsing research has been to eschew supervised training in favour of either unsupervised or weakly-supervised methods that utilize additional information. For example, Berant and Liang (2014) use a dataset of 5,810 questionanswer pairs without annotated logical forms to induce a parser for a question-answering system. In comparison, Poon (2013) converts NL questions into formal queries via indirect supervision through database interaction. In contrast to previous work, the shared task described in this paper uses the Robot Commands Treebank (Dukes, 2013a), a new dataset made available for supervised semantic parsing. The chosen domain is robotic control, in which NL commands are given to a robot arm used to manipulate shapes on an 8 x 8 game board. Despite the fixed domain, the task is challenging as correctly parsing commands requires understanding spatial context. For example, the command in Figure 1 may have several plausible interpretations, given different board configurations. ‘Move the pyramid on the blue cube on the gray one.’ Figure 1: Example scene with a contextual spatial command from the Robot Commands Treebank. 45 Proceedings o</context>
<context position="7561" citStr="Dukes, 2013" startWordPosition="1172" endWordPosition="1173">onsisting of 4,978; 880; 640 and 300 sentences respectively. The treebank was developed via a game with a purpose (www.TrainRobots.com), in which players were shown before and after configurations Spatial planner spatial context NL command Semantic parser RCL parsing 46 Figure 3: Semantic tree from the treebank with an elliptical anaphoric node and its annotated antecedent. and asked to give a corresponding command to a hypothetical robot arm. To make the game more competitive and to promote data quality, players rated each other’s sentences and were rewarded with points for accurate entries (Dukes, 2013b). 3.2 Annotation In total, over 10,000 commands were collected through the game. During an offline annotation phase, sentences were manually mapped to RCL. However, due to the nature of the game, players were free to enter arbitrarily complex sentences to describe moves, not all of which could be represented by RCL. In addition, some commands were syntactically well-formed, but not compatible with the corresponding scenes. The 3,409 commands selected for the task had RCL statements that were both understood by the planner (sequence: (event: (action: take) (entity: (id: 1) (color: cyan) (type</context>
<context position="10363" citStr="Dukes (2013" startWordPosition="1609" endWordPosition="1610">s that were customized for the domain in the task (Tables 1 and 2 overleaf, respectively). The general elements are typed entities (labelled with semantic features) that are connected using relations and events. This universal formalism is not domain-specific, and is inspired by semantic frames (Fillmore and Baker, 2001), a practical representation used for NL understanding systems (Dzikovska, 2004; UzZaman and Allen, 2010; Coyne et al., 2010; Dukes, 2009). In the remainder of this section we summarize aspects of RCL that are relevant to the task; a 47 more detailed description is provided by Dukes (2013a; 2014). In an RCL statement such as Figure 4, a preterminal node together with its child leaf node correspond to a feature-value pair (such as the feature color and the constant blue). Two special features which are distinguished by the planner are id and reference-id, which are used for co-referencing such as for annotating anaphora and their antecedents. The remaining features model the simulated robotic domain. For RCL Element Description action Aligned to a verbal group in NL, e.g. ‘drop’ or ‘pick up’. cardinal Number (e.g. 2 or ‘three’). color Colored attribute of an entity. destination</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Kais Dukes. 2013a. Semantic Annotation of Robotic Spatial Commands. In Proceedings of the Language and Technology Conference, LTC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Train Robots: A Dataset for Natural Language Human-Robot Spatial Interaction through Verbal Commands.</title>
<date>2013</date>
<booktitle>In International Conference on Social Robotics. Embodied Communication of Goals and Intentions Workshop.</booktitle>
<contexts>
<context position="2438" citStr="Dukes, 2013" startWordPosition="353" endWordPosition="354">4.0 often not available, a recent trend in semantic parsing research has been to eschew supervised training in favour of either unsupervised or weakly-supervised methods that utilize additional information. For example, Berant and Liang (2014) use a dataset of 5,810 questionanswer pairs without annotated logical forms to induce a parser for a question-answering system. In comparison, Poon (2013) converts NL questions into formal queries via indirect supervision through database interaction. In contrast to previous work, the shared task described in this paper uses the Robot Commands Treebank (Dukes, 2013a), a new dataset made available for supervised semantic parsing. The chosen domain is robotic control, in which NL commands are given to a robot arm used to manipulate shapes on an 8 x 8 game board. Despite the fixed domain, the task is challenging as correctly parsing commands requires understanding spatial context. For example, the command in Figure 1 may have several plausible interpretations, given different board configurations. ‘Move the pyramid on the blue cube on the gray one.’ Figure 1: Example scene with a contextual spatial command from the Robot Commands Treebank. 45 Proceedings o</context>
<context position="7561" citStr="Dukes, 2013" startWordPosition="1172" endWordPosition="1173">onsisting of 4,978; 880; 640 and 300 sentences respectively. The treebank was developed via a game with a purpose (www.TrainRobots.com), in which players were shown before and after configurations Spatial planner spatial context NL command Semantic parser RCL parsing 46 Figure 3: Semantic tree from the treebank with an elliptical anaphoric node and its annotated antecedent. and asked to give a corresponding command to a hypothetical robot arm. To make the game more competitive and to promote data quality, players rated each other’s sentences and were rewarded with points for accurate entries (Dukes, 2013b). 3.2 Annotation In total, over 10,000 commands were collected through the game. During an offline annotation phase, sentences were manually mapped to RCL. However, due to the nature of the game, players were free to enter arbitrarily complex sentences to describe moves, not all of which could be represented by RCL. In addition, some commands were syntactically well-formed, but not compatible with the corresponding scenes. The 3,409 commands selected for the task had RCL statements that were both understood by the planner (sequence: (event: (action: take) (entity: (id: 1) (color: cyan) (type</context>
<context position="10363" citStr="Dukes (2013" startWordPosition="1609" endWordPosition="1610">s that were customized for the domain in the task (Tables 1 and 2 overleaf, respectively). The general elements are typed entities (labelled with semantic features) that are connected using relations and events. This universal formalism is not domain-specific, and is inspired by semantic frames (Fillmore and Baker, 2001), a practical representation used for NL understanding systems (Dzikovska, 2004; UzZaman and Allen, 2010; Coyne et al., 2010; Dukes, 2009). In the remainder of this section we summarize aspects of RCL that are relevant to the task; a 47 more detailed description is provided by Dukes (2013a; 2014). In an RCL statement such as Figure 4, a preterminal node together with its child leaf node correspond to a feature-value pair (such as the feature color and the constant blue). Two special features which are distinguished by the planner are id and reference-id, which are used for co-referencing such as for annotating anaphora and their antecedents. The remaining features model the simulated robotic domain. For RCL Element Description action Aligned to a verbal group in NL, e.g. ‘drop’ or ‘pick up’. cardinal Number (e.g. 2 or ‘three’). color Colored attribute of an entity. destination</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Kais Dukes. 2013b. Train Robots: A Dataset for Natural Language Human-Robot Spatial Interaction through Verbal Commands. In International Conference on Social Robotics. Embodied Communication of Goals and Intentions Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Contextual Semantic Parsing using Crowdsourced Spatial Descriptions. Computation and Language,</title>
<date>2014</date>
<note>arXiv:1405.0145 [cs.CL]</note>
<marker>Dukes, 2014</marker>
<rawString>Kais Dukes. 2014. Contextual Semantic Parsing using Crowdsourced Spatial Descriptions. Computation and Language, arXiv:1405.0145 [cs.CL]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava Dzikovska</author>
</authors>
<title>A Practical Semantic Representation For Natural Language Parsing. PhD Thesis.</title>
<date>2004</date>
<institution>University of Rochester.</institution>
<contexts>
<context position="10153" citStr="Dzikovska, 2004" startWordPosition="1571" endWordPosition="1573"> was made available to task participants for training via a separate Java API. The tagset used to annotate RCL nodes can be divided into general tags (that are arguably applicable to other domains) and specific tags that were customized for the domain in the task (Tables 1 and 2 overleaf, respectively). The general elements are typed entities (labelled with semantic features) that are connected using relations and events. This universal formalism is not domain-specific, and is inspired by semantic frames (Fillmore and Baker, 2001), a practical representation used for NL understanding systems (Dzikovska, 2004; UzZaman and Allen, 2010; Coyne et al., 2010; Dukes, 2009). In the remainder of this section we summarize aspects of RCL that are relevant to the task; a 47 more detailed description is provided by Dukes (2013a; 2014). In an RCL statement such as Figure 4, a preterminal node together with its child leaf node correspond to a feature-value pair (such as the feature color and the constant blue). Two special features which are distinguished by the planner are id and reference-id, which are used for co-referencing such as for annotating anaphora and their antecedents. The remaining features model </context>
</contexts>
<marker>Dzikovska, 2004</marker>
<rawString>Myroslava Dzikovska 2004. A Practical Semantic Representation For Natural Language Parsing. PhD Thesis. University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Evang</author>
<author>Johan Bos</author>
</authors>
<title>RoBox: CCG with Structured Perceptron for Supervised Semantic Parsing of Robotic Spatial Commands.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</booktitle>
<contexts>
<context position="18945" citStr="Evang and Bos, 2014" startWordPosition="2962" endWordPosition="2965"> the system’s final performance dropped to P = 87.35% and IP = 60.84%. To measure the effect of less supervision, the models were additionally trained on only 500 sentences. In this scenario, the tagging model degraded significantly, while the parsing and reference resolution models performed nearly as well. RoBox: Using Combinatory Categorial Grammar (CCG) as a semantic parsing framework has been previously shown to be suitable for translating NL into logical form. Inspired by previous work using a CCG parser in combination with a structured perceptron (Zettlemoyer and Collins, 2007), RoBox (Evang and Bos, 2014) was the best performing CCG system in the shared task scoring P = 86.8% and IP = 79.21%. Using a similar approach to UW-MRS for its statistical component, RCL trees were interpreted as phrase-structure and converted to CCG derivations for training. During decoding, RCL statements were generated directly by the CCG parser. However, in contrast to the approach used by the AT&amp;T system, RoBox interfaces with the planner during parsing instead of performing spatial validation a post-processing step. This enables early resolution of attachment ambiguity and helps constrain the search space. However</context>
</contexts>
<marker>Evang, Bos, 2014</marker>
<rawString>Kilian Evang and Johan Bos. 2014. RoBox: CCG with Structured Perceptron for Supervised Semantic Parsing of Robotic Spatial Commands. In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
<author>Collin Baker</author>
</authors>
<title>Frame semantics for Text Understanding.</title>
<date>2001</date>
<booktitle>In Proceedings of WordNet and Other Lexical Resources Workshop.</booktitle>
<contexts>
<context position="10074" citStr="Fillmore and Baker, 2001" startWordPosition="1558" endWordPosition="1561">r. As these components do not require NL word alignment data, this additional information was made available to task participants for training via a separate Java API. The tagset used to annotate RCL nodes can be divided into general tags (that are arguably applicable to other domains) and specific tags that were customized for the domain in the task (Tables 1 and 2 overleaf, respectively). The general elements are typed entities (labelled with semantic features) that are connected using relations and events. This universal formalism is not domain-specific, and is inspired by semantic frames (Fillmore and Baker, 2001), a practical representation used for NL understanding systems (Dzikovska, 2004; UzZaman and Allen, 2010; Coyne et al., 2010; Dukes, 2009). In the remainder of this section we summarize aspects of RCL that are relevant to the task; a 47 more detailed description is provided by Dukes (2013a; 2014). In an RCL statement such as Figure 4, a preterminal node together with its child leaf node correspond to a feature-value pair (such as the feature color and the constant blue). Two special features which are distinguished by the planner are id and reference-id, which are used for co-referencing such </context>
</contexts>
<marker>Fillmore, Baker, 2001</marker>
<rawString>Charles Fillmore and Collin Baker. 2001. Frame semantics for Text Understanding. In Proceedings of WordNet and Other Lexical Resources Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Kate</author>
<author>Ray Mooney</author>
</authors>
<title>Using String Kernels for Learning Semantic Parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics, COLING-ACL</booktitle>
<pages>913--920</pages>
<contexts>
<context position="24044" citStr="Kate and Mooney, 2006" startWordPosition="3777" endWordPosition="3780">. Partially correct structures included statements which almost matched the expected RCL tree with the exception of incorrect featurevalues, or the addition or deletion of nodes. The most common errors were feature-values with incorrect entity types (such as ‘edge’ and ‘region’) and mismatched spatial relations (such as confusing ‘above’ and ‘within’ and confusing ‘right’, ‘left’ and ‘front’). UWM: The UWM system submitted by Kate (2014) uses an existing semantic parser, KRISP, for the shared task. KRISP (Kernel-based Robust Interpretation for Semantic Parsing) is a trainable semantic parser (Kate and Mooney, 2006) that uses Support Vector Machines (SVMs) as the machine learning method with a string subsequence kernel. As well as training data consisting of RCL paired with NL commands, KRISP required a context-free grammar for RCL, which was hand-written for UWM. During training, id nodes were removed from the RCL trees. These were recovered after parsing in a post-processing phase to resolve anaphora by matching to the nearest preceding antecedent. In contrast to other systems submitted for the task, UWM does not interface with the spatial planner and parses purely non-contextually. Because the planner</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit Kate and Ray Mooney. 2006. Using String Kernels for Learning Semantic Parsers. In Proceedings of the International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics, COLING-ACL (pp. 913–920).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Kate</author>
<author>Raymond Mooney</author>
</authors>
<title>Joint Entity and Relation Extraction using Card-Pyramid Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning, CoNLL</booktitle>
<pages>203--212</pages>
<contexts>
<context position="1339" citStr="Kate and Mooney, 2010" startWordPosition="193" endWordPosition="196">tem scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task. 1 Introduction Semantic parsers analyze sentences to produce formal meaning representations that are used for the computational understanding of natural language. Recently, state-of-the-art semantic parsing methods have used for a variety of applications, including question answering (Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012), dialog systems (Artzi and Zettlemoyer, 2011), entity relation extraction (Kate and Mooney, 2010) and robotic control (Tellex, 2011; Kim and Mooney, 2012). Different parsers can be distinguished by the level of supervision they require during training. Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form. However, because annotated data is This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0 often not available, a recent trend in semantic parsing research has been to eschew supervised training in f</context>
</contexts>
<marker>Kate, Mooney, 2010</marker>
<rawString>Rohit Kate and Raymond Mooney. 2010. Joint Entity and Relation Extraction using Card-Pyramid Parsing. In Proceedings of the Conference on Computational Natural Language Learning, CoNLL (pp. 203-212).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Kate</author>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to Transform Natural to Formal Languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence</booktitle>
<pages>1062--1068</pages>
<contexts>
<context position="6872" citStr="Kate et al., 2005" startWordPosition="1063" endWordPosition="1066">tor to enforce physical laws within the game. For example, a block cannot remain unsupported in empty space due to gravity. Similarly, prisms cannot lie below other block types. In the integrated system, the parser uses the planner for context, then provides the final RCL statement to the simulator which executes the command by moving the robot arm to update the board. 3 Data 3.1 Data Collection For the shared task, 3,409 sentences were selected from the treebank. This data size compares with related corpora used for semantic parsing such as the ATIS (Zettlemoyer and Collins, 2007), GeoQuery (Kate et al., 2005), Jobs (Tang and Mooney, 2001) and RoboCup (Kuhlmann et al., 2004) datasets, consisting of 4,978; 880; 640 and 300 sentences respectively. The treebank was developed via a game with a purpose (www.TrainRobots.com), in which players were shown before and after configurations Spatial planner spatial context NL command Semantic parser RCL parsing 46 Figure 3: Semantic tree from the treebank with an elliptical anaphoric node and its annotated antecedent. and asked to give a corresponding command to a hypothetical robot arm. To make the game more competitive and to promote data quality, players rat</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Rohit Kate, Yuk Wah Wong and Raymond Mooney. 2005. Learning to Transform Natural to Formal Languages. In Proceedings of the National Conference on Artificial Intelligence (pp. 1062-1068).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Kate</author>
</authors>
<title>UWM: Applying an Existing Trainable Semantic Parser to Parse Robotic Spatial Commands.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</booktitle>
<contexts>
<context position="23863" citStr="Kate (2014)" startWordPosition="3753" endWordPosition="3754">rics. However, well-formed RCL statements are required by the spatial planner and robotic simulator for the integrated system to robustly execute the specified NL command. Partially correct structures included statements which almost matched the expected RCL tree with the exception of incorrect featurevalues, or the addition or deletion of nodes. The most common errors were feature-values with incorrect entity types (such as ‘edge’ and ‘region’) and mismatched spatial relations (such as confusing ‘above’ and ‘within’ and confusing ‘right’, ‘left’ and ‘front’). UWM: The UWM system submitted by Kate (2014) uses an existing semantic parser, KRISP, for the shared task. KRISP (Kernel-based Robust Interpretation for Semantic Parsing) is a trainable semantic parser (Kate and Mooney, 2006) that uses Support Vector Machines (SVMs) as the machine learning method with a string subsequence kernel. As well as training data consisting of RCL paired with NL commands, KRISP required a context-free grammar for RCL, which was hand-written for UWM. During training, id nodes were removed from the RCL trees. These were recovered after parsing in a post-processing phase to resolve anaphora by matching to the neare</context>
</contexts>
<marker>Kate, 2014</marker>
<rawString>Rohit Kate. 2014. UWM: Applying an Existing Trainable Semantic Parser to Parse Robotic Spatial Commands. In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohyun Kim</author>
<author>Raymond Mooney</author>
</authors>
<title>Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL</booktitle>
<pages>433--444</pages>
<contexts>
<context position="1396" citStr="Kim and Mooney, 2012" startWordPosition="202" endWordPosition="205"> generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task. 1 Introduction Semantic parsers analyze sentences to produce formal meaning representations that are used for the computational understanding of natural language. Recently, state-of-the-art semantic parsing methods have used for a variety of applications, including question answering (Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012), dialog systems (Artzi and Zettlemoyer, 2011), entity relation extraction (Kate and Mooney, 2010) and robotic control (Tellex, 2011; Kim and Mooney, 2012). Different parsers can be distinguished by the level of supervision they require during training. Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form. However, because annotated data is This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0 often not available, a recent trend in semantic parsing research has been to eschew supervised training in favour of either unsupervised or weakly-supervised methods</context>
<context position="3792" citStr="Kim and Mooney (2012)" startWordPosition="570" endWordPosition="573"> inspired by the classic AI system SHRLDU, which responded to NL commands to control a robot for a similar game board (Winograd, 1972), although that system is reported to not have generalized well (Dreyfus, 2009; Mitkov, 1999). More recent research in command understanding has focused on parsing jointly with grounding, the process of mapping NL descriptions of entities within an environment to a semantic representation. Previous work includes Tellex et al. (2011), who develop a small corpus of commands for a simulated fork lift robot, with grounding performed using a factor graph. Similarly, Kim and Mooney (2012) perform joint parsing and grounding using a corpus of navigation commands. In contrast, this paper focuses on parsing using additional situational context for disambiguation and by using a larger NL dataset, in comparison to previous robotics research. In the remainder of this paper, we describe the task, the dataset and the metrics used for evaluation. We then compare the approaches used by participant systems and conclude with suggested improvements for future work. 2 Task Description The long term research goal encouraged by the task is to develop a system that will robustly execute NL rob</context>
</contexts>
<marker>Kim, Mooney, 2012</marker>
<rawString>Joohyun Kim and Raymond Mooney. 2012. Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL (pp. 433-444).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom Mitchell</author>
</authors>
<title>Weakly Supervised Training of Semantic Parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1241" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="180" endWordPosition="183">ed 92.5% and 90.5% for parsing with and without spatial context. However, the best performing statistical system scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task. 1 Introduction Semantic parsers analyze sentences to produce formal meaning representations that are used for the computational understanding of natural language. Recently, state-of-the-art semantic parsing methods have used for a variety of applications, including question answering (Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012), dialog systems (Artzi and Zettlemoyer, 2011), entity relation extraction (Kate and Mooney, 2010) and robotic control (Tellex, 2011; Kim and Mooney, 2012). Different parsers can be distinguished by the level of supervision they require during training. Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form. However, because annotated data is This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0 often not </context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom Mitchell. 2012. Weakly Supervised Training of Semantic Parsers. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Kuhlmann</author>
</authors>
<title>Guiding a Reinforcement Learner with Natural Language Advice: Initial Results in RoboCup Soccer.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Workshop on Supervisory Control of Learning and Adaptive Systems.</booktitle>
<marker>Kuhlmann, 2004</marker>
<rawString>Gregory Kuhlmann et al. 2004. Guiding a Reinforcement Learner with Natural Language Advice: Initial Results in RoboCup Soccer. In Proceedings of the AAAI Workshop on Supervisory Control of Learning and Adaptive Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling Semantic Parsers with On-the-fly Ontology Matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP.</booktitle>
<contexts>
<context position="1206" citStr="Kwiatkowski et al., 2013" startWordPosition="175" endWordPosition="179">rming (hybrid) system scored 92.5% and 90.5% for parsing with and without spatial context. However, the best performing statistical system scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task. 1 Introduction Semantic parsers analyze sentences to produce formal meaning representations that are used for the computational understanding of natural language. Recently, state-of-the-art semantic parsing methods have used for a variety of applications, including question answering (Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012), dialog systems (Artzi and Zettlemoyer, 2011), entity relation extraction (Kate and Mooney, 2010) and robotic control (Tellex, 2011; Kim and Mooney, 2012). Different parsers can be distinguished by the level of supervision they require during training. Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form. However, because annotated data is This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecom</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi and Luke Zettlemoyer. 2013. Scaling Semantic Parsers with On-the-fly Ontology Matching. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Ljunglöf</author>
</authors>
<title>Shrdlite: Semantic Parsing using a Handmade Grammar.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</booktitle>
<contexts>
<context position="20387" citStr="Ljunglöf (2014)" startWordPosition="3192" endWordPosition="3193">d, multiple RCL trees were generated with different candidate anaphoric references, which were filtered out contextually using the spatial planner. RoBox suffered only a 7.59% absolute drop in performance without using spatial planning, second only to UW-MRS at 2%. Evang and Bos perform error analysis on RoBox and report that most errors relate to ellipsis, the ambiguous word one, anaphora or attachment ambiguity. They suggest that the system could be improved with better feature selection or by integrating the CCG parser more closely with the spatial planner. Shrdlite: The Shrdlite system by Ljunglöf (2014), inspired by the Classic SHRDLU system by Winograd (1972), is a purely rule-based system that was shown to be effective for the task. Scoring P = 86.1% and IP = 51.5%, Shrdlite ranked fourth for parsing with integrated planning, and fifth without using spatial context. However, it suffered the largest absolute drop in performance without planning (34.6 points), indicating that integration with the planner is essential for the system’s reported accuracy. Shrdlite uses a hand-written compact unification grammar for the fragment of English appearing in the training data. The grammar is small, co</context>
</contexts>
<marker>Ljunglöf, 2014</marker>
<rawString>Peter Ljunglöf. 2014. Shrdlite: Semantic Parsing using a Handmade Grammar. In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem Mattelaer</author>
<author>Mathias Verbeke</author>
<author>Davide Nitti</author>
</authors>
<title>KUL-Eval: A Combinatory Categorial Grammar Approach for Improving Semantic Parsing of Robot Commands using Spatial Context.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</booktitle>
<contexts>
<context position="22137" citStr="Mattelaer et al. (2014)" startWordPosition="3472" endWordPosition="3475">iltered by the spatial planner. If multiple parse trees were found to be compatible with spatial context (or when not using the planner), the tree with the smallest number of nodes was selected as the parser’s final output. Additionally, because both the training and evaluation data were collected via crowdsourcing, sentences occasionally contain spelling errors, which were intentionally included in the task. To handle misspelt words, Shrdlite uses Levenshtein edit distance with a penalty to reparse sentences when the parser initially fails to produce any analysis. KUL-Eval: The CCG system by Mattelaer et al. (2014) uses a different approach to the RoBox system described previously. KUL-Eval scored P = 71.29% and IP = 57.76% in comparison to the RoBox scores of P = 86.8% and IP = 79.21%. During training, the RCL treebank was converted to X-expressions. This process is fully reversible, so that no information in an RCL tree is lost during conversion. In contrast to RoBox, but in common with the AT&amp;T parser, KUL-Eval performs spatial validation as a post-processing step and does not integrate the planner directly into the parsing process. A probabilistic CCG is used for parsing, so that multiple X-expressi</context>
</contexts>
<marker>Mattelaer, Verbeke, Nitti, 2014</marker>
<rawString>Willem Mattelaer, Mathias Verbeke and Davide Nitti. 2014. KUL-Eval: A Combinatory Categorial Grammar Approach for Improving Semantic Parsing of Robot Commands using Spatial Context. In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Anaphora Resolution: The State of the Art. Technical Report.</title>
<date>1999</date>
<institution>University of Wolverhampton.</institution>
<contexts>
<context position="3398" citStr="Mitkov, 1999" startWordPosition="509" endWordPosition="511"> Figure 1 may have several plausible interpretations, given different board configurations. ‘Move the pyramid on the blue cube on the gray one.’ Figure 1: Example scene with a contextual spatial command from the Robot Commands Treebank. 45 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 45–53, Dublin, Ireland, August 23-24, 2014. The task is inspired by the classic AI system SHRLDU, which responded to NL commands to control a robot for a similar game board (Winograd, 1972), although that system is reported to not have generalized well (Dreyfus, 2009; Mitkov, 1999). More recent research in command understanding has focused on parsing jointly with grounding, the process of mapping NL descriptions of entities within an environment to a semantic representation. Previous work includes Tellex et al. (2011), who develop a small corpus of commands for a simulated fork lift robot, with grounding performed using a factor graph. Similarly, Kim and Mooney (2012) perform joint parsing and grounding using a corpus of navigation commands. In contrast, this paper focuses on parsing using additional situational context for disambiguation and by using a larger NL datase</context>
</contexts>
<marker>Mitkov, 1999</marker>
<rawString>Ruslan Mitkov. 1999. Anaphora Resolution: The State of the Art. Technical Report. University of Wolverhampton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Woodley Packard</author>
</authors>
<title>UW-MRS: Leveraging a Deep Grammar for Robotic Spatial Commands.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</booktitle>
<contexts>
<context position="15273" citStr="Packard (2014)" startWordPosition="2380" endWordPosition="2381">ly understood, as both the spatial planner and the simulator require well-formed RCL. 5 Systems and Results Six teams participated in the shared task using a variety of strategies (Table 3). The last measure in the table gives the performance drop without spatial context. The value IP - P = -2 for the best performing system suggests this as an upper bound for the task. The different values of this measure indicate the sensitivity to (or possibly reliance on) context to guide the parsing process. In the remainder of this section we compare the approaches and results of the six systems. UW-MRS: Packard (2014) achieved the best score for parsing both with and without spatial context, at 92.5% and 90.5%, respectively, using a hybrid system that combines a rule-based grammar with the Berkeley parser (Petrov et al., 2006). The rule-based component uses the English Resource Grammar, a broad coverage handwritten HPSG grammar for English. The ERG produces a ranked list of Minimal Recursion Semantics (MRS) structures that encode predicate argument relations (Copestake et al., 2005). Approximately 80 rules were then used to convert MRS to RCL. The highest ranked result that is validated by the spatial plan</context>
</contexts>
<marker>Packard, 2014</marker>
<rawString>Woodley Packard. 2014. UW-MRS: Leveraging a Deep Grammar for Robotic Spatial Commands. In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics, COLING-ACL</booktitle>
<pages>433--440</pages>
<marker>Petrov, 2006</marker>
<rawString>Slav Petrov, et al. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics, COLING-ACL (pp. 433-440).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
</authors>
<title>Grounded Unsupervised Semantic Parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics, ACL</booktitle>
<pages>466--477</pages>
<contexts>
<context position="2225" citStr="Poon (2013)" startWordPosition="320" endWordPosition="321">epresentation such as logical form. However, because annotated data is This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0 often not available, a recent trend in semantic parsing research has been to eschew supervised training in favour of either unsupervised or weakly-supervised methods that utilize additional information. For example, Berant and Liang (2014) use a dataset of 5,810 questionanswer pairs without annotated logical forms to induce a parser for a question-answering system. In comparison, Poon (2013) converts NL questions into formal queries via indirect supervision through database interaction. In contrast to previous work, the shared task described in this paper uses the Robot Commands Treebank (Dukes, 2013a), a new dataset made available for supervised semantic parsing. The chosen domain is robotic control, in which NL commands are given to a robot arm used to manipulate shapes on an 8 x 8 game board. Despite the fixed domain, the task is challenging as correctly parsing commands requires understanding spatial context. For example, the command in Figure 1 may have several plausible int</context>
</contexts>
<marker>Poon, 2013</marker>
<rawString>Hoifung Poon. 2013. Grounded Unsupervised Semantic Parsing. In Proceedings of the Conference of the Association for Computational Linguistics, ACL (pp. 466-477).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
<author>Anna Babarczy</author>
</authors>
<title>A Test of the Leaf-Ancestor Metric for Parse Accuracy.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<pages>365--380</pages>
<contexts>
<context position="14535" citStr="Sampson and Babarczy, 2003" startWordPosition="2250" endWordPosition="2253">and crafted domain-specific grammar 86.10 51.50 -34.60 KUL-Eval Mattelaer et al. Statistical CCG parser 71.29 57.76 -13.53 UWM Kate Statistical KRISP parser N/A 45.98 N/A Table 3: System results for supervised semantic parsing of the Robot Commands Treebank (P = parsing with integrated spatial planning, NP = parsing without integrated spatial planning, NP - P = drop in performance without integrated spatial planning, N/A = performance not available). These metrics contrast with measures for partially correct parsed structures, such as Parseval (Black et al., 1991) or the leaf-ancestor metric (Sampson and Babarczy, 2003). The rationale for using a strict match is that in the integrated system, a command will only be executed if it is completely understood, as both the spatial planner and the simulator require well-formed RCL. 5 Systems and Results Six teams participated in the shared task using a variety of strategies (Table 3). The last measure in the table gives the performance drop without spatial context. The value IP - P = -2 for the best performing system suggests this as an upper bound for the task. The different values of this measure indicate the sensitivity to (or possibly reliance on) context to gu</context>
</contexts>
<marker>Sampson, Babarczy, 2003</marker>
<rawString>Geoffrey Sampson and Anna Babarczy. 2003. A Test of the Leaf-Ancestor Metric for Parse Accuracy. Natural Language Engineering, 9.4 (pp. 365-380).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Stoyanchev</author>
</authors>
<title>AT&amp;T Labs Research: Tag&amp;Parse Approach to Semantic Parsing of Robot Spatial Commands.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</booktitle>
<marker>Stoyanchev, 2014</marker>
<rawString>Svetlana Stoyanchev, et al. 2014. AT&amp;T Labs Research: Tag&amp;Parse Approach to Semantic Parsing of Robot Spatial Commands. In Proceedings of the International Workshop on Semantic Evaluation, SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon Tang</author>
<author>Raymond Mooney</author>
</authors>
<title>Using Multiple Clause Constructors</title>
<date>2001</date>
<booktitle>in Inductive Logic Programming for Semantic Parsing. Machine Learning, ECML.</booktitle>
<contexts>
<context position="6902" citStr="Tang and Mooney, 2001" startWordPosition="1068" endWordPosition="1071">ws within the game. For example, a block cannot remain unsupported in empty space due to gravity. Similarly, prisms cannot lie below other block types. In the integrated system, the parser uses the planner for context, then provides the final RCL statement to the simulator which executes the command by moving the robot arm to update the board. 3 Data 3.1 Data Collection For the shared task, 3,409 sentences were selected from the treebank. This data size compares with related corpora used for semantic parsing such as the ATIS (Zettlemoyer and Collins, 2007), GeoQuery (Kate et al., 2005), Jobs (Tang and Mooney, 2001) and RoboCup (Kuhlmann et al., 2004) datasets, consisting of 4,978; 880; 640 and 300 sentences respectively. The treebank was developed via a game with a purpose (www.TrainRobots.com), in which players were shown before and after configurations Spatial planner spatial context NL command Semantic parser RCL parsing 46 Figure 3: Semantic tree from the treebank with an elliptical anaphoric node and its annotated antecedent. and asked to give a corresponding command to a hypothetical robot arm. To make the game more competitive and to promote data quality, players rated each other’s sentences and </context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>Lappoon Tang and Raymond Mooney. 2001. Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing. Machine Learning, ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellax</author>
</authors>
<title>Approaching the Symbol Grounding Problem with Probabilistic Graphical Models.</title>
<date>2011</date>
<journal>AI Magazine,</journal>
<volume>32</volume>
<pages>64--76</pages>
<marker>Tellax, 2011</marker>
<rawString>Stefanie Tellax, et al. 2011. Approaching the Symbol Grounding Problem with Probabilistic Graphical Models. AI Magazine, 32:4 (pp. 64-76).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>James Allen</author>
</authors>
<title>TRIPS and TRIOS System for TempEval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval</booktitle>
<pages>276--283</pages>
<contexts>
<context position="10178" citStr="UzZaman and Allen, 2010" startWordPosition="1574" endWordPosition="1577">le to task participants for training via a separate Java API. The tagset used to annotate RCL nodes can be divided into general tags (that are arguably applicable to other domains) and specific tags that were customized for the domain in the task (Tables 1 and 2 overleaf, respectively). The general elements are typed entities (labelled with semantic features) that are connected using relations and events. This universal formalism is not domain-specific, and is inspired by semantic frames (Fillmore and Baker, 2001), a practical representation used for NL understanding systems (Dzikovska, 2004; UzZaman and Allen, 2010; Coyne et al., 2010; Dukes, 2009). In the remainder of this section we summarize aspects of RCL that are relevant to the task; a 47 more detailed description is provided by Dukes (2013a; 2014). In an RCL statement such as Figure 4, a preterminal node together with its child leaf node correspond to a feature-value pair (such as the feature color and the constant blue). Two special features which are distinguished by the planner are id and reference-id, which are used for co-referencing such as for annotating anaphora and their antecedents. The remaining features model the simulated robotic dom</context>
</contexts>
<marker>UzZaman, Allen, 2010</marker>
<rawString>Naushad UzZaman and James Allen. 2010. TRIPS and TRIOS System for TempEval-2. In Proceedings of the International Workshop on Semantic Evaluation, SemEval (pp. 276-283).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<pages>1--191</pages>
<contexts>
<context position="3305" citStr="Winograd, 1972" startWordPosition="494" endWordPosition="496"> correctly parsing commands requires understanding spatial context. For example, the command in Figure 1 may have several plausible interpretations, given different board configurations. ‘Move the pyramid on the blue cube on the gray one.’ Figure 1: Example scene with a contextual spatial command from the Robot Commands Treebank. 45 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 45–53, Dublin, Ireland, August 23-24, 2014. The task is inspired by the classic AI system SHRLDU, which responded to NL commands to control a robot for a similar game board (Winograd, 1972), although that system is reported to not have generalized well (Dreyfus, 2009; Mitkov, 1999). More recent research in command understanding has focused on parsing jointly with grounding, the process of mapping NL descriptions of entities within an environment to a semantic representation. Previous work includes Tellex et al. (2011), who develop a small corpus of commands for a simulated fork lift robot, with grounding performed using a factor graph. Similarly, Kim and Mooney (2012) perform joint parsing and grounding using a corpus of navigation commands. In contrast, this paper focuses on pa</context>
<context position="20445" citStr="Winograd (1972)" startWordPosition="3201" endWordPosition="3202">ate anaphoric references, which were filtered out contextually using the spatial planner. RoBox suffered only a 7.59% absolute drop in performance without using spatial planning, second only to UW-MRS at 2%. Evang and Bos perform error analysis on RoBox and report that most errors relate to ellipsis, the ambiguous word one, anaphora or attachment ambiguity. They suggest that the system could be improved with better feature selection or by integrating the CCG parser more closely with the spatial planner. Shrdlite: The Shrdlite system by Ljunglöf (2014), inspired by the Classic SHRDLU system by Winograd (1972), is a purely rule-based system that was shown to be effective for the task. Scoring P = 86.1% and IP = 51.5%, Shrdlite ranked fourth for parsing with integrated planning, and fifth without using spatial context. However, it suffered the largest absolute drop in performance without planning (34.6 points), indicating that integration with the planner is essential for the system’s reported accuracy. Shrdlite uses a hand-written compact unification grammar for the fragment of English appearing in the training data. The grammar is small, consisting of only 25 grammatical rules and 60 lexical rules</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding Natural Language. Cognitive Psychology, 3:1 (pp. 1-191).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online Learning of Relaxed CCG Grammars for Parsing to Logical Form.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL</booktitle>
<pages>878--887</pages>
<contexts>
<context position="6842" citStr="Zettlemoyer and Collins, 2007" startWordPosition="1058" endWordPosition="1061">ove the board. The planner uses the simulator to enforce physical laws within the game. For example, a block cannot remain unsupported in empty space due to gravity. Similarly, prisms cannot lie below other block types. In the integrated system, the parser uses the planner for context, then provides the final RCL statement to the simulator which executes the command by moving the robot arm to update the board. 3 Data 3.1 Data Collection For the shared task, 3,409 sentences were selected from the treebank. This data size compares with related corpora used for semantic parsing such as the ATIS (Zettlemoyer and Collins, 2007), GeoQuery (Kate et al., 2005), Jobs (Tang and Mooney, 2001) and RoboCup (Kuhlmann et al., 2004) datasets, consisting of 4,978; 880; 640 and 300 sentences respectively. The treebank was developed via a game with a purpose (www.TrainRobots.com), in which players were shown before and after configurations Spatial planner spatial context NL command Semantic parser RCL parsing 46 Figure 3: Semantic tree from the treebank with an elliptical anaphoric node and its annotated antecedent. and asked to give a corresponding command to a hypothetical robot arm. To make the game more competitive and to pro</context>
<context position="18916" citStr="Zettlemoyer and Collins, 2007" startWordPosition="2957" endWordPosition="2960"> 67.55%. However, using predicted tags, the system’s final performance dropped to P = 87.35% and IP = 60.84%. To measure the effect of less supervision, the models were additionally trained on only 500 sentences. In this scenario, the tagging model degraded significantly, while the parsing and reference resolution models performed nearly as well. RoBox: Using Combinatory Categorial Grammar (CCG) as a semantic parsing framework has been previously shown to be suitable for translating NL into logical form. Inspired by previous work using a CCG parser in combination with a structured perceptron (Zettlemoyer and Collins, 2007), RoBox (Evang and Bos, 2014) was the best performing CCG system in the shared task scoring P = 86.8% and IP = 79.21%. Using a similar approach to UW-MRS for its statistical component, RCL trees were interpreted as phrase-structure and converted to CCG derivations for training. During decoding, RCL statements were generated directly by the CCG parser. However, in contrast to the approach used by the AT&amp;T system, RoBox interfaces with the planner during parsing instead of performing spatial validation a post-processing step. This enables early resolution of attachment ambiguity and helps constr</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2007. Online Learning of Relaxed CCG Grammars for Parsing to Logical Form. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL (pp. 878-887).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>