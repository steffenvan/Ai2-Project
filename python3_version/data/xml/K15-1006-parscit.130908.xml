<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.997654">
An Iterative Similarity based Adaptation Technique for Cross Domain
Text Classification
</title>
<author confidence="0.953474">
Himanshu S. Bhatt Deepali Semwal Shourya Roy
</author>
<affiliation confidence="0.951191">
Xerox Research Center India, Bengaluru, INDIA
</affiliation>
<email confidence="0.989971">
{Himanshu.Bhatt,Deepali.Semwal,Shourya.Roy}@xerox.com
</email>
<sectionHeader confidence="0.997282" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999610047619048">
Supervised machine learning classifica-
tion algorithms assume both train and test
data are sampled from the same domain
or distribution. However, performance
of the algorithms degrade for test data
from different domain. Such cross do-
main classification is arduous as features
in the test domain may be different and
absence of labeled data could further ex-
acerbate the problem. This paper proposes
an algorithm to adapt classification model
by iteratively learning domain specific fea-
tures from the unlabeled test data. More-
over, this adaptation transpires in a simi-
larity aware manner by integrating similar-
ity between domains in the adaptation set-
ting. Cross-domain classification exper-
iments on different datasets, including a
real world dataset, demonstrate efficacy of
the proposed algorithm over state-of-the-
art.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915">
A fundamental assumption in supervised statis-
tical learning is that training and test data are
independently and identically distributed (i.i.d.)
samples drawn from a distribution. Otherwise,
good performance on test data cannot be guar-
anteed even if the training error is low. In real
life applications such as business process automa-
tion, this assumption is often violated. While re-
searchers develop new techniques and models for
machine learning based automation of one or a
handful business processes, large scale adoption is
hindered owing to poor generalized performance.
In our interactions with analytics software devel-
opment teams, we noticed such pervasive diver-
sity of learning tasks and associated inefficiency.
Novel predictive analytics techniques on standard
datasets (or limited client data) did not general-
ize across different domains ( new products &amp; ser-
vices) and has limited applicability. Training mod-
els from scratch for every new domain requires hu-
man annotated labeled data which is expensive and
time consuming, hence, not pragmatic.
On the other hand, transfer learning techniques
allow domains, tasks, and distributions used in
training and testing to be different, but related. It
works in contrast to traditional supervised tech-
niques on the principle of transferring learned
knowledge across domains. While transfer learn-
ing has generally proved useful in reducing the
labelled data requirement, brute force techniques
suffer from the problem of negative transfer (Pan
and Yang, 2010a). One cannot use transfer learn-
ing as the proverbial hammer, but needs to gauge
when to transfer and also how much to transfer.
To address these issues, this paper proposes
a domain adaptation technique for cross-domain
text classification. In our setting for cross-domain
classification, a classifier trained on one domain
with sufficient labelled training data is applied to
a different test domain with no labelled data. As
shown in Figure 1, this paper proposes an iterative
similarity based adaptation algorithm which starts
with a shared feature representation of source and
target domains. To adapt, it iteratively learns do-
main specific features from the unlabeled target
domain data. In this process, similarity between
two domains is incorporated in the adaptation set-
ting for similarity-aware transfer. The major con-
tributions of this research are:
</bodyText>
<listItem confidence="0.898810714285714">
• An iterative algorithm for learning domain
specific discriminative features from unla-
beled data in the target domain starting with
an initial shared feature representation.
• Facilitating similarity-aware domain adapta-
tion by seamlessly integrating similarity be-
tween two domains in the adaptation settings.
</listItem>
<page confidence="0.982617">
52
</page>
<note confidence="0.992705">
Proceedings of the 19th Conference on Computational Language Learning, pages 52–61,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.980832">
Figure 1: Outlines different stages of the proposed
algorithm i.e. shared feature representation, do-
main similarity, and the iterative learning process.
</figureCaption>
<bodyText confidence="0.9997603">
To the best of our knowledge, this is the first-of-
its-kind approach in cross-domain text classifica-
tion which integrates similarity between domains
in the adaptation setting to learn domain specific
features in an iterative manner. The rest of the
paper is organized as follows: Section 2 summa-
rizes the related work, Section 3 presents details
about the proposed algorithm. Section 4 presents
databases, experimental protocol, and results. Fi-
nally, Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999932038461539">
Transfer learning in text analysis (domain adapta-
tion) has shown promising results in recent years
(Pan and Yang, 2010a). Prior work on domain
adaptation for text classification can be broadly
classified into instance re-weighing and feature-
representation based adaptation approaches.
Instance re-weighing approaches address the
difference between the joint distributions of ob-
served instances and class labels in source do-
main with that of target domain. Towards this di-
rection, Liao et al. (2005) learned mismatch be-
tween two domains and used active learning to
select instances from the source domain to en-
hance adaptability of the classifier. Jiang and Zhai
(2007) proposed instance weighing scheme for do-
main adaptation in NLP tasks which exploit inde-
pendence between feature mapping and instance
weighing approaches. Saha et al. (2011) lever-
aged knowledge from source domain to actively
select the most informative samples from the tar-
get domain. Xia et al. (2013) proposed a hybrid
method for sentiment classification task that also
addresses the challenge of mutually opposite ori-
entation words.
A number of domain adaptation techniques are
based on learning common feature representation
(Pan and Yang, 2010b; Blitzer et al., 2006; Ji et
al., 2011; Daum´e III, 2009) for text classification.
The basic idea being identifying a suitable fea-
ture space where projected source and target do-
main data follow similar distributions and hence,
a standard supervised learning algorithm can be
trained on the former to predict instances from
the latter. Among them, Structural Correspon-
dence Learning (SCL) (Blitzer et al., 2007) is the
most representative one, explained later. Daum´e
(2009) proposed a heuristic based non-linear map-
ping of source and target data to a high dimen-
sional space. Pan et al. (2008) proposed a di-
mensionality reduction method Maximum Mean
Discrepancy Embedding to identify a latent space.
Subsequently, Pan et al. (2010) proposed to map
domain specific words into unified clusters using
spectral clustering algorithm. In another follow
up work, Pan et al. (2011) proposed a novel fea-
ture representation to perform domain adaptation
via Reproducing Kernel Hilbert Space using Max-
imum Mean Discrepancy. A similar approach,
based on co-clustering (Dhillon et al., 2003), was
proposed in Dai et al. (2007) to leverage common
words as bridge between two domains. Bollegala
et al. (2011) used sentiment sensitive thesaurus to
expand features for cross-domain sentiment clas-
sification. In a comprehensive evaluation study, it
was observed that their approach tends to increase
the adaptation performance when multiple source
domains were used (Bollegala et al., 2013).
Domain adaptation based on iterative learning
has been explored by Chen et al. (2011) and
Garcia-Fernandez et al. (2014) and are similar to
the philosophy of the proposed approach in ap-
pending pseudo-labeled test data to the training
set. The first approach uses an expensive fea-
ture split to co-train two classifiers while the for-
mer presents a single classifier self-training based
setting. However, the proposed algorithm offers
novel contributions in terms of 1) leveraging two
independent feature representations capturing the
shared and target specific representations, 2) an
ensemble of classifiers that uses labelled source
domain and pseudo labelled target domain in-
stances carefully moderated based on similarity
between two domains. Ensemble based domain
adaptation for text classification was first pro-
posed by Aue and Gammon (2005) though their
approach could not achieve significant improve-
ments over baseline. Later, Zhao et al. (2010)
proposed online transfer learning (OTL) frame-
</bodyText>
<page confidence="0.997813">
53
</page>
<bodyText confidence="0.999984142857143">
work which forms the basis of our ensemble based
domain adaptation. However, the proposed algo-
rithm differs in the following ways: 1) an unsuper-
vised approach that transforms unlabeled data into
pseudo labeled data unlike OTL which is super-
vised, and 2) incorporates similarity in the adapta-
tion setting for gradual transfer.
</bodyText>
<sectionHeader confidence="0.99536" genericHeader="method">
3 Iterative Similarity based Adaptation
</sectionHeader>
<bodyText confidence="0.99171487912088">
The philosophy of our algorithm is gradual trans-
fer of knowledge from the source to the target do-
main while being cognizant of similarity between
two domains. To accomplish this, we have devel-
oped a technique based on ensemble of two classi-
fiers. Transfer occurs within the ensemble where
a classifier learned on shared representation trans-
forms unlabeled test data into pseudo labeled data
to learn domain specific classifier. Before explain-
ing the algorithm, we highlight its salient features:
Common Feature Space Representation: Our
objective is to find a good feature representation
which minimizes divergence between the source
and target domains as well as the classification
error. There have been several works towards
feature-representation-transfer approach such as
(Blitzer et al., 2007; Ji et al., 2011) which derives a
transformation matrix Q that gives a shared repre-
sentation between the source and target domains.
One of the widely used approaches is Structural
Correspondence Learning (SCL) (Blitzer et al.,
2006) which aims to learn the co-occurrence be-
tween features expressing similar meaning in dif-
ferent domains. Top k Eigenvectors of matrix, W,
represent the principal predictors for weight space,
Q. Features from both domains are projected on
this principal predictor space, Q, to obtain a shared
representation. Source domain classifier in our ap-
proach is based on this SCL representation. In
Section 4, we empirically show how our algorithm
generalizes to different shared representations.
Iterative Building of Target Domain Labeled
Data: If we have enough labeled data from the
target domain then a classifier can be trained with-
out the need for adaptation. Hence, we wanted
to explore if and how (pseudo) labeled data for
the target domain can be created. Our hypothe-
sis is that certain target domain instances are more
similar to source domain instances than the rest.
Hence a classifier trained on (a suitably chosen
transformed representation of) source domain in-
stances will be able to categorize similar target do-
main instances confidently. Such confidently pre-
dicted instances can be considered as pseudo la-
beled data which are then used to initialize a clas-
sifier in target domain.
Only handful of instances in the target domain
can be confidently predicted using the shared rep-
resentation, therefore, we further iterate to create
pseudo labeled instances in target domain. In the
next round of iterations, remaining unlabeled tar-
get domain instances are passed through both the
classifiers and their output are suitably combined.
Again, confidently labeled instances are added to
the pool of pseudo labeled data and the classi-
fier in the target domain is updated. This pro-
cess is repeated till all unlabeled data is labeled
or certain maximum number of iterations is per-
formed. This way we gradually adapt the target
domain classifier on pseudo labeled data using the
knowledge transferred from source domain. In
Section 4, we empirically demonstrate effective-
ness of this technique compared to one-shot adap-
tation approaches.
Domain Similarity-based Aggregation: Perfor-
mance of domain adaptation is often constrained
by the dissimilarity between the source and target
domains (Luo et al., 2012; Rosenstein et al., 2005;
Chin, 2013; Blitzer et al., 2007). If the two do-
mains are largely similar, the knowledge learned in
the source domain can be aggressively transferred
to the target domain. On the other hand, if the two
domains are less similar, knowledge learned in the
source domain should be transferred in a conserva-
tive manner so as to mitigate the effects of negative
transfer. Therefore, it is imperative for domain
adaptation techniques to account for similarity be-
tween domains and transfer knowledge in a simi-
larity aware manner. While this may sound obvi-
ous, we do not see many works in domain adapta-
tion literature that leverage inter-domain similar-
ity for transfer of knowledge. In this work, we use
the cosine similarity measure to compute similar-
ity between two domains and based on that gradu-
ally transfer knowledge from the source to the tar-
get domain. While it would be interesting to com-
pare how different similarity measures compare
towards preventing negative transfer but that is not
the focus of this work. In Section 4, we empiri-
cally show marginal gains of transferring knowl-
edge in a similarity aware manner.
</bodyText>
<page confidence="0.99847">
54
</page>
<tableCaption confidence="0.999323">
Table 1: Notations used in this research.
</tableCaption>
<table confidence="0.984577833333333">
Symbol Description
{xs i , ys i }i=1:ns ; xs Labeled source domain instances
i ∈
Rd; ys
i ∈ {−1, +1}
{xti}i=1:nt; ˆyi ∈ Unlabeled target domain instances and pre-
{−1, +1} dicted label for target domain
Q Co-occurrence based projection matrix
Pu, Ps Pool of unlabeled and pseudo-labeled target
domain instances respectively
Cs, Ct ; function from Classifier Cs is trained on {(Qxsi , ysi )};
Rd → {−1, +1} classifier Ct is trained on {xti, ˆyti } where
xti ∈ Ps and yˆ is the pseudo label
predicted labels by Ensemble E
α confidence of prediction
E Weighted ensemble of Cs and Ct
01, 02 confidence threshold for Cs and ensemble E
ws, wt Weights for Cs and Ct respectively
</table>
<subsectionHeader confidence="0.9956">
3.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.999605">
Table 1 lists the notations used in this research. In-
puts to the algorithm are labeled source domain in-
stances {xsi , ysi }i=1:ns and a pool of unlabeled tar-
get domain instances {xti}i=1:nt, denoted by Pu.
As shown in Figure 2, the steps of the algorithm
are as follows:
</bodyText>
<listItem confidence="0.994407636363636">
1. Learn Q, a shared representation projection
matrix from the source and target domains,
using any of the existing techniques. SCL is
used in this research.
2. Learn Cs on SCL-based representation of la-
beled source domain instances {Qxsi, ysi }.
3. Use Cs to predict labels, ˆyi, for instances
in Pu using the SCL-based representation
Qxti. Instances which are predicted with con-
fidence greater than a pre-defined threshold,
01, are moved from Pu to Ps with pseudo la-
bel, ˆy.
4. Learn Ct from instances in Ps ∈ {xti, ˆyti} to
incorporate target specific features. Ps only
contains instances added in step-3 and will
be growing iteratively (hence the training set
here is small).
5. Cs and Ct are combined in an ensemble, E,
as a weighted combination with weights as
ws and wt which are both initialized to 0.5.
6. Ensemble E is applied to all remaining in-
stances in Pu to obtain the label ˆyi as:
</listItem>
<equation confidence="0.982753">
E(xti) → ˆyi → wsCs(Qxti) + wtCt(xti) (1)
</equation>
<bodyText confidence="0.9768965">
(a) If the ensemble classifies an instance
with confidence greater than the thresh-
old 02, then it is moved from Pu to Ps
along with pseudo label ˆyi.
</bodyText>
<figureCaption confidence="0.977128333333333">
Figure 2: Illustrates learning of the initial classi-
fiers and iterative learning process of the proposed
similarity-aware domain adaptation algorithm.
</figureCaption>
<figure confidence="0.566492">
(b) Repeat step-6 for all xti ∈ Pu.
</figure>
<bodyText confidence="0.9781875">
7. Weights ws and wt are updated as shown in
Eqs. 2 and 3. This update facilitates knowl-
edge transfer within the ensemble guided by
the similarity between domains.
</bodyText>
<equation confidence="0.996851666666667">
(sign, ∗ wsl ∗ I(Cs))
(sign, ∗ wsl ∗ I(Cs) + (1 − sign,) ∗ wtl ∗ I(Ct))
(2)
((1 − sign,) ∗ wtl ∗ I(Ct))
(sign, ∗ wsl ∗ I(Cs) + (1 − sign,) ∗ wtl ∗ I(Ct))
(3)
</equation>
<bodyText confidence="0.999782333333333">
where, l is the iteration, sim is the similarity
score between domains computed using co-
sine similarity metric as shown in Eq. 4
</bodyText>
<equation confidence="0.5203585">
sign, = a · b (4)
||a |b||
</equation>
<bodyText confidence="0.99996975">
where a &amp; b are normalized vector represen-
tations for the two domains. I(·) is the loss
function to measure the errors of individual
classifiers in each iteration:
</bodyText>
<equation confidence="0.994471">
I(·) = exp{−ηl(C, Y )} (5)
</equation>
<bodyText confidence="0.99994775">
where, η is learning rate set to 0.1, l(y, ˆy) =
(y − ˆy)2 is the square loss function, y is the
label predicted by the classifier and yˆ is the
label predicted by the ensemble.
</bodyText>
<listItem confidence="0.986356666666667">
8. Re-train classifier Ct on Ps.
9. Repeat step 6 − 8 until Pu is empty or maxi-
mum number of iterations is reached.
</listItem>
<equation confidence="0.999857333333333">
ws(l+1) =
t
w(l+1) =
</equation>
<page confidence="0.963967">
55
</page>
<bodyText confidence="0.999985714285714">
In this iterative manner, the proposed algorithm
transforms unlabeled data in the test domain into
pseudo labeled data and progressively learns clas-
sifier Ct. Confidence of prediction, αz for ith in-
stance, is measured as the distance from the de-
cision boundary (Hsu et al., 2003) which is com-
puted as shown in Eq. 6.
</bodyText>
<equation confidence="0.985392">
α = |R |(6)
</equation>
<bodyText confidence="0.97246705">
where R is the un-normalized output from the
support vector machine (SVM) classifier, v is the
weight vector for support vectors and |v |= vT v.
Weights of individual classifiers in the ensem-
ble are updated with each iteration that gradu-
ally shifts emphasis from the classifier learned on
shared representation to the classifier learned on
target domain. Algorithm 1 illustrates the pro-
posed iterative learning algorithm.
Algorithm 1 Iterative Learning Algorithm
Input: C3 trained on shared co-occurrence
based representation Qx, Ct initiated on TFIDF
representation from P3, P,, remaining unlabeled
target domain instances.
Iterate: l = 0 : till P,, = {φ} or l &lt; iterMax
Process: Construct ensemble E as weighted
combination of C3 and Ct with initials weights
wi and wt� as 0.5 and sim = similarity between
domains.
for i = 1 to n (size of P,,) do
</bodyText>
<listItem confidence="0.911602125">
Predict labels: E(Qxz, xz) — ˆyz; calculate αz
if αz &gt; θ2 then
Remove ith instance from P,, and add to
P3 with pseudo label ˆyz.
end if.
end for. Retrain Ct on P3 and update wi and
wt�.
end iterate.
</listItem>
<bodyText confidence="0.60033">
Output: Updated Ct, wi and wt .
</bodyText>
<sectionHeader confidence="0.998098" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999985333333333">
The efficacy of the proposed algorithm is eval-
uated on different datasets for cross-domain text
classification (Blitzer et al., 2007), (Dai et al.,
2007). In our experiments, performance is eval-
uated on two-class classification task and reported
in terms of classification accuracy.
</bodyText>
<subsectionHeader confidence="0.878028">
4.1 Datasets &amp; Experimental Protocol
</subsectionHeader>
<bodyText confidence="0.999735346153846">
The first dataset is the Amazon review dataset
(Blitzer et al., 2007) which has four different
domains, Books, DVDs, Kitchen appliances and
Electronics. Each domain comprises 1000 pos-
itive and 1000 negative reviews. In all experi-
ments, 1600 labeled reviews from the source and
1600 unlabeled reviews from the target domains
are used in training and performance is reported
on the non-overlapping 400 reviews from the tar-
get domain.
The second dataset is the 20 Newsgroups
dataset (Lang, 1995) which is a text collection
of approximately 20, 000 documents evenly par-
titioned across 20 newsgroups. For cross-domain
text classification on the 20 Newsgroups dataset,
we followed the protocol of Dai et al. (2007)
where it is divided into six different datasets and
the top two categories in each are picked as the two
classes. The data is further segregated based on
sub-categories, where each sub-category is con-
sidered as a different domain. Table 2 lists how
different sub-categories are combined to represent
the source and target domains. In our experiments,
4/5th of the source and target data is used to learn
shared feature representation and results are re-
ported on the remaining 1/5th of the target data.
</bodyText>
<tableCaption confidence="0.633308666666667">
Table 2: Elaborates data segregation on the 20
Newsgroups dataset for cross-domain classifica-
tion.
</tableCaption>
<table confidence="0.992543178571429">
dataset Ds Dt
comp vs rec comp.graphics comp.os.ms-windows.misc
comp.sys.ibm.pc.hardware comp.windows.x
comp.sys.mac.hardware rec.autos
rec.motorcycles rec.sport.baseball
rec.sport.hockey
comp vs sci comp.graphics comp.sys.ibm.pc.hardware
comp.os.ms-windows.misc comp.sys.mac.hardware
sci.crypt comp.windows.x
sci.electronics sci.med
sci.space
comp vs talk comp.graphics comp.os.ms-windows.miscnewline
comp.sys.mac.hardware comp.sys.ibm.pc.hardware
comp.windows.x talk.politics.guns
talk.politics.mideast talk.politics.misc
talk.religion.misc
rec vs sci rec.autos rec.motorcycles
rec.sport.baseball rec.sport.hockey
sci.med sci.crypt
sci.space sci.electronics
rec vs talk rec.autos rec.sport.baseball
rec.motorcycles rec.sport.hockey
talk.politics.guns talk.politics.mideast
talk.politics.misc talk.religion.misc
sci vs talk sci.electronics sci.crypt
sci.med sci.space
talk.politics.misc talk.politics.guns
talk.religion.misc talk.politics.mideast
</table>
<bodyText confidence="0.99448675">
The third dataset is a real world dataset com-
prising tweets about the products and services
in different domains. The dataset comprises
tweets/posts from three collections, Coll1 about
gaming, Coll2 about Microsoft products and
Coll3 about mobile support. Each collection has
218 positive and negative tweets. These tweets
are collected based on user-defined keywords cap-
</bodyText>
<page confidence="0.992978">
56
</page>
<bodyText confidence="0.999981875">
tured in a listening engine which then crawls the
social media and fetches comments matching the
keywords. This dataset being noisy and compris-
ing short-text is more challenging than the previ-
ous two datasets.
All datasets are pre-processed by converting to
lowercase followed by stemming. Feature selec-
tion based on document frequency (DF = 5)
reduces the number of features as well as speed
up the classification task. For Amazon review
dataset, TF is used for feature weighing whereas
TFIDF is used for feature weighing in other two
datasets. In all our experiments, constituent clas-
sifiers used in the ensemble are support vector ma-
chines (SVMs) with radial basis function kernel.
Performance of the proposed algorithm for cross-
domain classification task is compared with dif-
ferent techniques1including 1) in-domain classi-
fier trained and tested on the same domain data, 2)
baseline classifier which is trained on the source
and directly tested on the target domain, 3) SCL2,
a widely used domain adaptation technique for
cross-domain text classification, 4) ‘Proposed w/o
sim’, removing similarity from Eqs. 2 &amp; 3.
</bodyText>
<subsectionHeader confidence="0.78105">
4.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.9980837">
For cross-domain classification, the performance
degrades mainly due to 1) feature divergence and
2) negative transfer owing to largely dissimilar do-
mains. Table 3 shows the accuracy of individ-
ual classifiers and the ensemble for cross-domain
classification on the Amazon review dataset. The
ensemble has better accuracy than the individual
classifiers, therefore, in our experiments the fi-
nal reported performance is the accuracy of the
ensemble. The combination weights in the en-
semble represent the contributions of individual
classifiers toward classification accuracy. In our
experiments, the maximum number of iterations
(iterMax) is set to 30. It is observed that at the
end of the iterative learning process, the target spe-
cific classifier is assigned more weight mass as
compared to the classifier trained on the shared
representation. On average, the weights for the
two classifiers converge to ws = 0.22 and wt =
0.78 at the end of the iterative learning process.
</bodyText>
<footnote confidence="0.631218714285714">
1We also compared our performance with sentiment sen-
sitive thesaurus (SST) proposed by (Bollegala et al., 2013)
and our algorithm outperformed on our protocol. However,
we did not include comparative results because of difference
in experimental protocol as SST is tailored for using multiple
source domains and our protocol uses single source domain.
2Our implementation of SCL is used in this paper.
</footnote>
<tableCaption confidence="0.8359944">
Table 3: Comparing the performance of individual
classifiers and the ensemble for training on Books
domain and test across different domains. Cs and
Ct are applied on the test domain data before per-
forming the iterating learning process.
</tableCaption>
<table confidence="0.92357575">
SD → TD Cs Ct Ensemble
B → D 63.1 34.8 72.1
B → E 64.5 39.1 75.8
B → K 68.4 42.3 76.2
</table>
<tableCaption confidence="0.971729333333333">
Table 4: List some examples of domain specific
discriminative features learned by the proposed al-
gorithm on the Amazon review dataset.
</tableCaption>
<table confidence="0.9948624">
Domain Domain specific features
Books pictures illustrations, more detail, to read
DvDs Definite buy, delivery prompt
Kitchen invaluable resource, rust, delicious
Electronics Bargain, Energy saving, actually use
</table>
<bodyText confidence="0.997724714285714">
This further validates our assertion that the tar-
get specific features are more discriminative than
the shared features in classifying target domain in-
stances, which are efficiently captured by the pro-
posed algorithm. Key observations and analysis
from the experiments on different datasets is sum-
marized below.
</bodyText>
<subsubsectionHeader confidence="0.792037">
4.2.1 Results on the Amazon Review dataset
</subsubsectionHeader>
<bodyText confidence="0.9766575">
To study the effects of different components of the
proposed algorithm, comprehensive experiments
are performed on the Amazon review dataset3.
1) Effect of learning target specific features: Re-
sults in Figure 3 show that iteratively learning tar-
get specific feature representation (slow transfer as
opposed to one-shot transfer) yields better perfor-
mance across different cross-domain classification
tasks as compared to SCL, SFA (Pan et al., 2010)4
and the baseline. Unlike SCL and SFA, the pro-
posed approach uses shared and target specific fea-
ture representations for the cross-domain classifi-
cation task. Table 4 illustrates some examples of
the target specific discriminative features learned
by the proposed algorithm that leads to enhanced
performance. At 95% confidence, parametric t-
test suggests that the proposed algorithm and SCL
are significantly (statistically) different.
</bodyText>
<listItem confidence="0.8232445">
2) Effect of similarity on performance: It is ob-
served that existing domain adaptation techniques
enhance the accuracy for cross-domain classifica-
tion, though, negative transfer exists in camou-
</listItem>
<footnote confidence="0.9962108">
3Due to space restrictions, we show this analysis only on
one dataset; however similar conclusions were drawn from
other datasets as well.
4We directly compared our results with the performance
reported in (Pan et al., 2010).
</footnote>
<page confidence="0.99796">
57
</page>
<figureCaption confidence="0.999693">
Figure 3: Comparing the performance of the proposed approach with existing techniques for cross-
</figureCaption>
<bodyText confidence="0.981720090909091">
domain classification on Amazon review dataset.
flage. Results in Figure 3(b) (for the case K → B)
describes an evident scenario for negative trans-
fer where the adaptation performance with SCL
descends lower than the baseline. However, the
proposed algorithm still sustains the performance
by transferring knowledge proportionate to simi-
larity between the two domains. To further an-
alyze the effect of similarity, we segregated the
12 cross-domain classification cases into two cat-
egories based on similarity between two the par-
ticipating domains i.e. 1) &gt; 0.5 and 2) &lt; 0.5.
Table 5 shows that for 6 out of 12 cases that fall
in the first category, the average accuracy gain is
10.8% as compared to the baseline. While for
the remaining 6 cases that fall in the second cat-
egory, the average accuracy gain is 15.4% as com-
pared to the baseline. This strongly elucidates that
the proposed similarity-based iterative algorithm
not only adapts well when the domain similarity
is high but also yields gain in the accuracy when
the domains are largely dissimilar. Figure 4 also
shows how weight for the target domain classi-
fier wt varies with the number of iterations. It
further strengthens our assertion that if domains
are similar, algorithm can readily adapt and con-
verges in a few iterations. On the other hand for
dissimilar domains, slow iterative transfer, as op-
posed to one-shot transfer, can achieve similar per-
formance; however, it may take more iterations
to converge.While the effect of similarity on do-
main adaptation performance is evident, this work
opens possibilities for further investigations.
3) Effect of varying threshold 01 &amp; 02: Figure
5(a) explains the effect of varying 01 on the final
classification accuracy. If 01 is low, Ct may get
trained on incorrectly predicted pseudo labeled in-
stances; whereas, if 01 is high, Ct may be defi-
cient of instances to learn a good decision bound-
ary. On the other hand, 02 influences the number
of iterations required by the algorithm to reach the
Table 5: Effect of similarity on accuracy gain for
cross-domain classification on the Amazon review
dataset.
</bodyText>
<table confidence="0.998801142857143">
Category SD → TD Sim Gain Avg. (SD)
E → K 0.78 13.1 10.8 (4.9)
K → E 0.78 10.6
&gt; 0.5 0.54 8.0
B → K
K → B 0.54 2.9
B → E 0.52 13.1
E → B 0.52 17.2
&lt; 0.5 K → D 0.34 8.9 15.4 (4.4)
D → K 0.34 21.6
E→D 0.33 14.5
D → E 0.33 14.5
B → D 0.29 14.1
D → B 0.29 19.1
</table>
<figureCaption confidence="0.828857666666667">
Figure 4: Illustrates how the weight (wt) for tar-
get domain classifiers varies for the most and least
similar domains with number of iterations.
</figureCaption>
<bodyText confidence="0.999915933333333">
stopping criteria. If this threshold is low, the algo-
rithm converges aggressively (in a few iterations)
and does not benefit from the iterative nature of
learning the target specific features. Whereas a
high threshold tends to make the algorithm con-
servative. It hampers the accuracy because of the
unavailability of sufficient instances to update the
classifier after each iteration which also leads to
large number of iterations to converge (may not
even converge).
01 and 02 are set empirically on a held-out
set, with values ranging from zero to distance of
farthest classified instance from the SVM hyper-
plane (Hsu et al., 2003). The knee-shaped curve
on the graphs in Figure 5 shows that there exists
</bodyText>
<page confidence="0.998117">
58
</page>
<figureCaption confidence="0.9382256">
Figure 5: Bar plot shows % of data that crosses
confidence threshold, lower and upper part of the
bar represents % correctly and wrongly predicted
pseudo labels. The black line shows how the final
classification accuracy is effected with threshold.
</figureCaption>
<bodyText confidence="0.993488655172414">
an optimal value for 01 and 02 which yields the
best accuracy. We observed that the best accuracy
is obtained when the thresholds are set to the dis-
tance between the hyper plane and the farthest sup-
port vector in each class.
4) Effect of using different shared represen-
tations in ensemble: To study the generaliza-
tion ability of the proposed algorithm to differ-
ent shared representations, experiments are per-
formed using three different shared representa-
tions on the Amazon review dataset. Apart from
using the SCL representation, the accuracy is
compared with the proposed algorithm using two
other representations, 1) common features be-
tween the two domains (“common”) and 2) multi-
view principal component analysis based repre-
sentation (“MVPCA”) (Ji et al., 2011) as they are
previously used for cross-domain sentiment clas-
sification on the same dataset. Table 6 shows that
the proposed algorithm yields significant gains in
cross-domain classification accuracy with all three
representations and is not restricted to any spe-
cific representation. The final accuracy depends
on the initial classifier trained on the shared repre-
sentation; therefore, if a shared representation suf-
ficiently captures the characteristics of both source
and target domains, the proposed algorithm can
be built on any such representation for enhanced
cross-domain classification accuracy.
</bodyText>
<subsubsectionHeader confidence="0.831704">
4.2.2 Results on 20 Newsgroups data
</subsubsectionHeader>
<bodyText confidence="0.998064833333333">
Results in Figure 6 compares the accuracy of pro-
posed algorithm with existing approaches on the
20 Newsgroups dataset. Since different domain
are crafted out from the sub-categories of the
same dataset, domains are exceedingly similar and
therefore, the baseline accuracy is relatively better
</bodyText>
<tableCaption confidence="0.672767">
Table 6: Comparing the accuracy of proposed al-
gorithm built on different shared representations.
</tableCaption>
<table confidence="0.998278230769231">
SD → TD Common MVPCA SCL
B → D 66.8 76.4 78.2
B → E 69.0 79.2 80.6
B → K 71.4 79.2 79.8
D → B 64.5 78.4 79.3
D → E 62.8 76.4 76.2
D → K 64.3 80.9 82.4
E → B 68.9 77.8 78.5
E → D 65.7 77.0 77.3
E → K 75.1 85.4 86.2
K → B 71.3 71.0 71.1
K → D 70.4 75.0 76.1
K → E 76.7 85.7 86.4
</table>
<figureCaption confidence="0.808411">
Figure 6: Results comparing the accuracy of pro-
posed approach with existing techniques for cross
domain categorization on 20 Newsgroups dataset.
</figureCaption>
<bodyText confidence="0.999779384615385">
than that on the other two datasets. The proposed
algorithm still yields an improvement of at least
10.8% over the baseline accuracy. As compared to
other existing domain adaptation approaches like
SCL(Blitzer et al., 2007) and CoCC (Dai et al.,
2007), the proposed algorithm outperforms by at
least 4% and 1.9% respectively. This also vali-
dates our assertion that generally domain adapta-
tion techniques accomplishes well when the par-
ticipating domains are largely similar; however,
the similarity aggregation and the iterative learn-
ing offer the proposed algorithm an edge over one-
shot adaptation algorithms.
</bodyText>
<subsectionHeader confidence="0.561006">
4.2.3 Results on real world data
</subsectionHeader>
<bodyText confidence="0.999862333333333">
Results in Figure 7 exhibit challenges associated
with real world dataset. The baseline accuracy
for cross-domain classification task is severely af-
fected for this dataset. SCL based domain adap-
tation does not yields generous improvements as
selecting the pivot features and computing the co-
occurrence statistics with noisy short text is ardu-
ous and inept. On the other hand, the proposed
algorithm iteratively learns discriminative target
specific features from such perplexing data and
translates it to an improvement of at least 6.4%
and 3.5% over the baseline and the SCL respec-
</bodyText>
<page confidence="0.998331">
59
</page>
<figureCaption confidence="0.97022175">
Figure 7: Results comparing the accuracy of the
proposed approach with existing techniques for
cross domain categorization on the real world
dataset.
</figureCaption>
<bodyText confidence="0.943121">
tively.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999977157894737">
The paper presents an iterative similarity-aware
domain adaptation algorithm that progressively
learns domain specific features from the unlabeled
test domain data starting with a shared feature rep-
resentation. In each iteration, the proposed algo-
rithm assigns pseudo labels to the unlabeled data
which are then used to update the constituent clas-
sifiers and their weights in the ensemble. Updating
the target specific classifier in each iteration helps
better learn the domain specific features and thus,
results in enhanced cross-domain classification ac-
curacy. Similarity between the two domains is ag-
gregated while updating weights of the constituent
classifiers which facilitates gradual shift of knowl-
edge from the source to the target domain. Finally,
experimental results for cross-domain classifica-
tion on different datasets show the efficacy of the
proposed algorithm as compared to other existing
approaches.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999006014084507">
A. Aue and M. Gamon. 2005. Customizing sentiment clas-
sifiers to new domains: A case study. Technical report,
Microsoft Research.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain adap-
tation with structural correspondence learning. In Pro-
ceedings of Conference on Empirical Methods in Natural
Language Processing, pages 120–128.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boomboxes and blenders: Domain adaptation
for sentiment classification. In Proceedings ofAssociation
for Computational Linguistics, pages 187–205.
D. Bollegala, D. Weir, and J. Carroll. 2011. Using multi-
ple sources to construct a sentiment sensitive thesaurus for
cross-domain sentiment classification. In Proceedings of
Association for Computational Linguistics: Human Lan-
guage Technologies, pages 132–141.
D. Bollegala, D. Weir, and J. Carroll. 2013. Cross-domain
sentiment classification using a sentiment sensitive the-
saurus. IEEE Transactions on Knowledge and Data En-
gineering, 25(8):1719–1731.
M. Chen, K. Q Weinberger, and J. Blitzer. 2011. Co-
training for domain adaptation. In Proceedings of Ad-
vances in Neural Information Processing Systems, pages
2456–2464.
Si-Chi Chin. 2013. Knowledge transfer: what, how, and
why.
W Dai, G-R Xue, Q Yang, and Y Yu. 2007. Co-clustering
based classification for out-of-domain documents. In Pro-
ceedings of International Conference on Knowledge Dis-
covery and Data Mining, pages 210–219.
Hal Daum´e III. 2009. Frustratingly easy domain adaptation.
arXiv preprint arXiv:0907.1815.
I. S. Dhillon, S. Mallela, and D. S Modha. 2003.
Information-theoretic co-clustering. In Proceedings of
International Conference on Knowledge Discovery and
Data Mining, pages 89–98.
A. Garcia-Fernandez, O. Ferret, and M. Dinarelli. 2014.
Evaluation of different strategies for domain adaptation in
opinion mining. In Proceedings of the International Con-
ference on Language Resources and Evaluation, pages
26–31.
C-W. Hsu, C.-C. Chang, and C.-J. Lin. 2003. A practical
guide to support vector classification. Technical report,
Department of Computer Science, National Taiwan Uni-
versity.
Y.-S. Ji, J.-J. Chen, G. Niu, L. Shang, and X.-Y. Dai.
2011. Transfer learning via multi-view principal compo-
nent analysis. Journal of Computer Science and Technol-
ogy, 26(1):81–98.
J. Jiang and C. Zhai. 2007. Instance weighting for do-
main adaptation in NLP. In Proceedings of Association
for Computational Linguistics, volume 7, pages 264–271.
K Lang. 1995. Newsweeder: Learning to filter netnews.
In Proceedings of International Conference on Machine
Learning.
X. Liao, Y. Xue, and L. Carin. 2005. Logistic regression with
an auxiliary data source. In Proceedings ofInternational
Conference on Machine Learning, pages 505–512.
C. Luo, Y. Ji, X. Dai, and J. Chen. 2012. Active learning with
transfer learning. In Proceedings ofAssociation for Com-
putational Linguistics Student Research Workshop, pages
13–18. Association for Computational Linguistics.
S. J. Pan and Q. Yang. 2010a. A survey on transfer learning.
IEEE Transactions on Knowledge and Data Engineering,
22(10):1345–1359.
Sinno Jialin Pan and Qiang Yang. 2010b. A survey on trans-
fer learning. Knowledge and Data Engineering, IEEE
Transactions on, 22(10):1345–1359.
Sinno Jialin Pan, James T Kwok, and Qiang Yang. 2008.
Transfer learning via dimensionality reduction. In AAAI,
volume 8, pages 677–682.
</reference>
<page confidence="0.967044">
60
</page>
<reference confidence="0.999708217391304">
S. J. Pan, X. Ni, J-T Sun, Q. Yang, and Z. Chen. 2010. Cross-
domain sentiment classification via spectral feature align-
ment. In Proceedings International Conference on World
Wide Web, pages 751–760. ACM.
S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. 2011. Do-
main adaptation via transfer component analysis. IEEE
Transactions on Neural Networks, 22(2):199–210.
M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. Di-
etterich. 2005. To transfer or not to transfer. In Pro-
ceedings of Advances in Neural Information Processing
Systems Workshop, Inductive Transfer: 10 Years Later.
A. Saha, P. Rai, H. Daum´e, S. Venkatasubramanian, and S. L.
DuVall. 2011. Active supervised domain adaptation. In
Proceedings of European Conference on Machine Learn-
ing and Knowledge Discovery in Databases, pages 97–
112.
R. Xia, C. Zong, X. Hu, and E. Cambria. 2013. Feature en-
semble plus sample selection: domain adaptation for sen-
timent classification. IEEE Intelligent Systems, 28(3):10–
18.
P. Zhao and S. C. H. Hoi. 2010. OTL: A Framework of
Online Transfer Learning. In Proceeding ofInternational
Conference on Machine Learning.
</reference>
<page confidence="0.99927">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.521268">
<title confidence="0.996751">An Iterative Similarity based Adaptation Technique for Cross Text Classification</title>
<author confidence="0.984773">Himanshu S Bhatt Deepali Semwal Shourya Roy</author>
<affiliation confidence="0.664786">Xerox Research Center India, Bengaluru,</affiliation>
<abstract confidence="0.990525727272727">Supervised machine learning classification algorithms assume both train and test data are sampled from the same domain or distribution. However, performance of the algorithms degrade for test data from different domain. Such cross domain classification is arduous as features in the test domain may be different and absence of labeled data could further exacerbate the problem. This paper proposes an algorithm to adapt classification model by iteratively learning domain specific features from the unlabeled test data. Moreover, this adaptation transpires in a similarity aware manner by integrating similarity between domains in the adaptation setting. Cross-domain classification experiments on different datasets, including a real world dataset, demonstrate efficacy of the proposed algorithm over state-of-theart.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aue</author>
<author>M Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: A case study.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research.</tech>
<marker>Aue, Gamon, 2005</marker>
<rawString>A. Aue and M. Gamon. 2005. Customizing sentiment classifiers to new domains: A case study. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="5819" citStr="Blitzer et al., 2006" startWordPosition="860" endWordPosition="863">tability of the classifier. Jiang and Zhai (2007) proposed instance weighing scheme for domain adaptation in NLP tasks which exploit independence between feature mapping and instance weighing approaches. Saha et al. (2011) leveraged knowledge from source domain to actively select the most informative samples from the target domain. Xia et al. (2013) proposed a hybrid method for sentiment classification task that also addresses the challenge of mutually opposite orientation words. A number of domain adaptation techniques are based on learning common feature representation (Pan and Yang, 2010b; Blitzer et al., 2006; Ji et al., 2011; Daum´e III, 2009) for text classification. The basic idea being identifying a suitable feature space where projected source and target domain data follow similar distributions and hence, a standard supervised learning algorithm can be trained on the former to predict instances from the latter. Among them, Structural Correspondence Learning (SCL) (Blitzer et al., 2007) is the most representative one, explained later. Daum´e (2009) proposed a heuristic based non-linear mapping of source and target data to a high dimensional space. Pan et al. (2008) proposed a dimensionality re</context>
<context position="9666" citStr="Blitzer et al., 2006" startWordPosition="1448" endWordPosition="1451">cific classifier. Before explaining the algorithm, we highlight its salient features: Common Feature Space Representation: Our objective is to find a good feature representation which minimizes divergence between the source and target domains as well as the classification error. There have been several works towards feature-representation-transfer approach such as (Blitzer et al., 2007; Ji et al., 2011) which derives a transformation matrix Q that gives a shared representation between the source and target domains. One of the widely used approaches is Structural Correspondence Learning (SCL) (Blitzer et al., 2006) which aims to learn the co-occurrence between features expressing similar meaning in different domains. Top k Eigenvectors of matrix, W, represent the principal predictors for weight space, Q. Features from both domains are projected on this principal predictor space, Q, to obtain a shared representation. Source domain classifier in our approach is based on this SCL representation. In Section 4, we empirically show how our algorithm generalizes to different shared representations. Iterative Building of Target Domain Labeled Data: If we have enough labeled data from the target domain then a cl</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings ofAssociation for Computational Linguistics,</booktitle>
<pages>187--205</pages>
<contexts>
<context position="6208" citStr="Blitzer et al., 2007" startWordPosition="921" endWordPosition="924">nt classification task that also addresses the challenge of mutually opposite orientation words. A number of domain adaptation techniques are based on learning common feature representation (Pan and Yang, 2010b; Blitzer et al., 2006; Ji et al., 2011; Daum´e III, 2009) for text classification. The basic idea being identifying a suitable feature space where projected source and target domain data follow similar distributions and hence, a standard supervised learning algorithm can be trained on the former to predict instances from the latter. Among them, Structural Correspondence Learning (SCL) (Blitzer et al., 2007) is the most representative one, explained later. Daum´e (2009) proposed a heuristic based non-linear mapping of source and target data to a high dimensional space. Pan et al. (2008) proposed a dimensionality reduction method Maximum Mean Discrepancy Embedding to identify a latent space. Subsequently, Pan et al. (2010) proposed to map domain specific words into unified clusters using spectral clustering algorithm. In another follow up work, Pan et al. (2011) proposed a novel feature representation to perform domain adaptation via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy.</context>
<context position="9433" citStr="Blitzer et al., 2007" startWordPosition="1411" endWordPosition="1414"> we have developed a technique based on ensemble of two classifiers. Transfer occurs within the ensemble where a classifier learned on shared representation transforms unlabeled test data into pseudo labeled data to learn domain specific classifier. Before explaining the algorithm, we highlight its salient features: Common Feature Space Representation: Our objective is to find a good feature representation which minimizes divergence between the source and target domains as well as the classification error. There have been several works towards feature-representation-transfer approach such as (Blitzer et al., 2007; Ji et al., 2011) which derives a transformation matrix Q that gives a shared representation between the source and target domains. One of the widely used approaches is Structural Correspondence Learning (SCL) (Blitzer et al., 2006) which aims to learn the co-occurrence between features expressing similar meaning in different domains. Top k Eigenvectors of matrix, W, represent the principal predictors for weight space, Q. Features from both domains are projected on this principal predictor space, Q, to obtain a shared representation. Source domain classifier in our approach is based on this S</context>
<context position="11928" citStr="Blitzer et al., 2007" startWordPosition="1806" endWordPosition="1809"> the target domain is updated. This process is repeated till all unlabeled data is labeled or certain maximum number of iterations is performed. This way we gradually adapt the target domain classifier on pseudo labeled data using the knowledge transferred from source domain. In Section 4, we empirically demonstrate effectiveness of this technique compared to one-shot adaptation approaches. Domain Similarity-based Aggregation: Performance of domain adaptation is often constrained by the dissimilarity between the source and target domains (Luo et al., 2012; Rosenstein et al., 2005; Chin, 2013; Blitzer et al., 2007). If the two domains are largely similar, the knowledge learned in the source domain can be aggressively transferred to the target domain. On the other hand, if the two domains are less similar, knowledge learned in the source domain should be transferred in a conservative manner so as to mitigate the effects of negative transfer. Therefore, it is imperative for domain adaptation techniques to account for similarity between domains and transfer knowledge in a similarity aware manner. While this may sound obvious, we do not see many works in domain adaptation literature that leverage inter-doma</context>
<context position="17841" citStr="Blitzer et al., 2007" startWordPosition="2867" endWordPosition="2870">nstances. Iterate: l = 0 : till P,, = {φ} or l &lt; iterMax Process: Construct ensemble E as weighted combination of C3 and Ct with initials weights wi and wt� as 0.5 and sim = similarity between domains. for i = 1 to n (size of P,,) do Predict labels: E(Qxz, xz) — ˆyz; calculate αz if αz &gt; θ2 then Remove ith instance from P,, and add to P3 with pseudo label ˆyz. end if. end for. Retrain Ct on P3 and update wi and wt�. end iterate. Output: Updated Ct, wi and wt . 4 Experimental Results The efficacy of the proposed algorithm is evaluated on different datasets for cross-domain text classification (Blitzer et al., 2007), (Dai et al., 2007). In our experiments, performance is evaluated on two-class classification task and reported in terms of classification accuracy. 4.1 Datasets &amp; Experimental Protocol The first dataset is the Amazon review dataset (Blitzer et al., 2007) which has four different domains, Books, DVDs, Kitchen appliances and Electronics. Each domain comprises 1000 positive and 1000 negative reviews. In all experiments, 1600 labeled reviews from the source and 1600 unlabeled reviews from the target domains are used in training and performance is reported on the non-overlapping 400 reviews from </context>
<context position="31524" citStr="Blitzer et al., 2007" startWordPosition="5014" endWordPosition="5017"> B → D 66.8 76.4 78.2 B → E 69.0 79.2 80.6 B → K 71.4 79.2 79.8 D → B 64.5 78.4 79.3 D → E 62.8 76.4 76.2 D → K 64.3 80.9 82.4 E → B 68.9 77.8 78.5 E → D 65.7 77.0 77.3 E → K 75.1 85.4 86.2 K → B 71.3 71.0 71.1 K → D 70.4 75.0 76.1 K → E 76.7 85.7 86.4 Figure 6: Results comparing the accuracy of proposed approach with existing techniques for cross domain categorization on 20 Newsgroups dataset. than that on the other two datasets. The proposed algorithm still yields an improvement of at least 10.8% over the baseline accuracy. As compared to other existing domain adaptation approaches like SCL(Blitzer et al., 2007) and CoCC (Dai et al., 2007), the proposed algorithm outperforms by at least 4% and 1.9% respectively. This also validates our assertion that generally domain adaptation techniques accomplishes well when the participating domains are largely similar; however, the similarity aggregation and the iterative learning offer the proposed algorithm an edge over oneshot adaptation algorithms. 4.2.3 Results on real world data Results in Figure 7 exhibit challenges associated with real world dataset. The baseline accuracy for cross-domain classification task is severely affected for this dataset. SCL bas</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification. In Proceedings ofAssociation for Computational Linguistics, pages 187–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>D Weir</author>
<author>J Carroll</author>
</authors>
<title>Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>132--141</pages>
<contexts>
<context position="6989" citStr="Bollegala et al. (2011)" startWordPosition="1044" endWordPosition="1047">space. Pan et al. (2008) proposed a dimensionality reduction method Maximum Mean Discrepancy Embedding to identify a latent space. Subsequently, Pan et al. (2010) proposed to map domain specific words into unified clusters using spectral clustering algorithm. In another follow up work, Pan et al. (2011) proposed a novel feature representation to perform domain adaptation via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy. A similar approach, based on co-clustering (Dhillon et al., 2003), was proposed in Dai et al. (2007) to leverage common words as bridge between two domains. Bollegala et al. (2011) used sentiment sensitive thesaurus to expand features for cross-domain sentiment classification. In a comprehensive evaluation study, it was observed that their approach tends to increase the adaptation performance when multiple source domains were used (Bollegala et al., 2013). Domain adaptation based on iterative learning has been explored by Chen et al. (2011) and Garcia-Fernandez et al. (2014) and are similar to the philosophy of the proposed approach in appending pseudo-labeled test data to the training set. The first approach uses an expensive feature split to co-train two classifiers w</context>
</contexts>
<marker>Bollegala, Weir, Carroll, 2011</marker>
<rawString>D. Bollegala, D. Weir, and J. Carroll. 2011. Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification. In Proceedings of Association for Computational Linguistics: Human Language Technologies, pages 132–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>D Weir</author>
<author>J Carroll</author>
</authors>
<title>Cross-domain sentiment classification using a sentiment sensitive thesaurus.</title>
<date>2013</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>25</volume>
<issue>8</issue>
<contexts>
<context position="7268" citStr="Bollegala et al., 2013" startWordPosition="1083" endWordPosition="1086">p work, Pan et al. (2011) proposed a novel feature representation to perform domain adaptation via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy. A similar approach, based on co-clustering (Dhillon et al., 2003), was proposed in Dai et al. (2007) to leverage common words as bridge between two domains. Bollegala et al. (2011) used sentiment sensitive thesaurus to expand features for cross-domain sentiment classification. In a comprehensive evaluation study, it was observed that their approach tends to increase the adaptation performance when multiple source domains were used (Bollegala et al., 2013). Domain adaptation based on iterative learning has been explored by Chen et al. (2011) and Garcia-Fernandez et al. (2014) and are similar to the philosophy of the proposed approach in appending pseudo-labeled test data to the training set. The first approach uses an expensive feature split to co-train two classifiers while the former presents a single classifier self-training based setting. However, the proposed algorithm offers novel contributions in terms of 1) leveraging two independent feature representations capturing the shared and target specific representations, 2) an ensemble of clas</context>
<context position="22891" citStr="Bollegala et al., 2013" startWordPosition="3584" endWordPosition="3587"> weights in the ensemble represent the contributions of individual classifiers toward classification accuracy. In our experiments, the maximum number of iterations (iterMax) is set to 30. It is observed that at the end of the iterative learning process, the target specific classifier is assigned more weight mass as compared to the classifier trained on the shared representation. On average, the weights for the two classifiers converge to ws = 0.22 and wt = 0.78 at the end of the iterative learning process. 1We also compared our performance with sentiment sensitive thesaurus (SST) proposed by (Bollegala et al., 2013) and our algorithm outperformed on our protocol. However, we did not include comparative results because of difference in experimental protocol as SST is tailored for using multiple source domains and our protocol uses single source domain. 2Our implementation of SCL is used in this paper. Table 3: Comparing the performance of individual classifiers and the ensemble for training on Books domain and test across different domains. Cs and Ct are applied on the test domain data before performing the iterating learning process. SD → TD Cs Ct Ensemble B → D 63.1 34.8 72.1 B → E 64.5 39.1 75.8 B → K </context>
</contexts>
<marker>Bollegala, Weir, Carroll, 2013</marker>
<rawString>D. Bollegala, D. Weir, and J. Carroll. 2013. Cross-domain sentiment classification using a sentiment sensitive thesaurus. IEEE Transactions on Knowledge and Data Engineering, 25(8):1719–1731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chen</author>
<author>K Q Weinberger</author>
<author>J Blitzer</author>
</authors>
<title>Cotraining for domain adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems,</booktitle>
<pages>2456--2464</pages>
<contexts>
<context position="7355" citStr="Chen et al. (2011)" startWordPosition="1097" endWordPosition="1100">on via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy. A similar approach, based on co-clustering (Dhillon et al., 2003), was proposed in Dai et al. (2007) to leverage common words as bridge between two domains. Bollegala et al. (2011) used sentiment sensitive thesaurus to expand features for cross-domain sentiment classification. In a comprehensive evaluation study, it was observed that their approach tends to increase the adaptation performance when multiple source domains were used (Bollegala et al., 2013). Domain adaptation based on iterative learning has been explored by Chen et al. (2011) and Garcia-Fernandez et al. (2014) and are similar to the philosophy of the proposed approach in appending pseudo-labeled test data to the training set. The first approach uses an expensive feature split to co-train two classifiers while the former presents a single classifier self-training based setting. However, the proposed algorithm offers novel contributions in terms of 1) leveraging two independent feature representations capturing the shared and target specific representations, 2) an ensemble of classifiers that uses labelled source domain and pseudo labelled target domain instances ca</context>
</contexts>
<marker>Chen, Weinberger, Blitzer, 2011</marker>
<rawString>M. Chen, K. Q Weinberger, and J. Blitzer. 2011. Cotraining for domain adaptation. In Proceedings of Advances in Neural Information Processing Systems, pages 2456–2464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Si-Chi Chin</author>
</authors>
<date>2013</date>
<note>Knowledge transfer: what, how, and why.</note>
<contexts>
<context position="11905" citStr="Chin, 2013" startWordPosition="1804" endWordPosition="1805">lassifier in the target domain is updated. This process is repeated till all unlabeled data is labeled or certain maximum number of iterations is performed. This way we gradually adapt the target domain classifier on pseudo labeled data using the knowledge transferred from source domain. In Section 4, we empirically demonstrate effectiveness of this technique compared to one-shot adaptation approaches. Domain Similarity-based Aggregation: Performance of domain adaptation is often constrained by the dissimilarity between the source and target domains (Luo et al., 2012; Rosenstein et al., 2005; Chin, 2013; Blitzer et al., 2007). If the two domains are largely similar, the knowledge learned in the source domain can be aggressively transferred to the target domain. On the other hand, if the two domains are less similar, knowledge learned in the source domain should be transferred in a conservative manner so as to mitigate the effects of negative transfer. Therefore, it is imperative for domain adaptation techniques to account for similarity between domains and transfer knowledge in a similarity aware manner. While this may sound obvious, we do not see many works in domain adaptation literature t</context>
</contexts>
<marker>Chin, 2013</marker>
<rawString>Si-Chi Chin. 2013. Knowledge transfer: what, how, and why.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dai</author>
<author>G-R Xue</author>
<author>Q Yang</author>
<author>Y Yu</author>
</authors>
<title>Co-clustering based classification for out-of-domain documents.</title>
<date>2007</date>
<booktitle>In Proceedings of International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>210--219</pages>
<contexts>
<context position="6909" citStr="Dai et al. (2007)" startWordPosition="1031" endWordPosition="1034"> based non-linear mapping of source and target data to a high dimensional space. Pan et al. (2008) proposed a dimensionality reduction method Maximum Mean Discrepancy Embedding to identify a latent space. Subsequently, Pan et al. (2010) proposed to map domain specific words into unified clusters using spectral clustering algorithm. In another follow up work, Pan et al. (2011) proposed a novel feature representation to perform domain adaptation via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy. A similar approach, based on co-clustering (Dhillon et al., 2003), was proposed in Dai et al. (2007) to leverage common words as bridge between two domains. Bollegala et al. (2011) used sentiment sensitive thesaurus to expand features for cross-domain sentiment classification. In a comprehensive evaluation study, it was observed that their approach tends to increase the adaptation performance when multiple source domains were used (Bollegala et al., 2013). Domain adaptation based on iterative learning has been explored by Chen et al. (2011) and Garcia-Fernandez et al. (2014) and are similar to the philosophy of the proposed approach in appending pseudo-labeled test data to the training set. </context>
<context position="17861" citStr="Dai et al., 2007" startWordPosition="2871" endWordPosition="2874"> : till P,, = {φ} or l &lt; iterMax Process: Construct ensemble E as weighted combination of C3 and Ct with initials weights wi and wt� as 0.5 and sim = similarity between domains. for i = 1 to n (size of P,,) do Predict labels: E(Qxz, xz) — ˆyz; calculate αz if αz &gt; θ2 then Remove ith instance from P,, and add to P3 with pseudo label ˆyz. end if. end for. Retrain Ct on P3 and update wi and wt�. end iterate. Output: Updated Ct, wi and wt . 4 Experimental Results The efficacy of the proposed algorithm is evaluated on different datasets for cross-domain text classification (Blitzer et al., 2007), (Dai et al., 2007). In our experiments, performance is evaluated on two-class classification task and reported in terms of classification accuracy. 4.1 Datasets &amp; Experimental Protocol The first dataset is the Amazon review dataset (Blitzer et al., 2007) which has four different domains, Books, DVDs, Kitchen appliances and Electronics. Each domain comprises 1000 positive and 1000 negative reviews. In all experiments, 1600 labeled reviews from the source and 1600 unlabeled reviews from the target domains are used in training and performance is reported on the non-overlapping 400 reviews from the target domain. T</context>
<context position="31552" citStr="Dai et al., 2007" startWordPosition="5020" endWordPosition="5023"> 79.2 80.6 B → K 71.4 79.2 79.8 D → B 64.5 78.4 79.3 D → E 62.8 76.4 76.2 D → K 64.3 80.9 82.4 E → B 68.9 77.8 78.5 E → D 65.7 77.0 77.3 E → K 75.1 85.4 86.2 K → B 71.3 71.0 71.1 K → D 70.4 75.0 76.1 K → E 76.7 85.7 86.4 Figure 6: Results comparing the accuracy of proposed approach with existing techniques for cross domain categorization on 20 Newsgroups dataset. than that on the other two datasets. The proposed algorithm still yields an improvement of at least 10.8% over the baseline accuracy. As compared to other existing domain adaptation approaches like SCL(Blitzer et al., 2007) and CoCC (Dai et al., 2007), the proposed algorithm outperforms by at least 4% and 1.9% respectively. This also validates our assertion that generally domain adaptation techniques accomplishes well when the participating domains are largely similar; however, the similarity aggregation and the iterative learning offer the proposed algorithm an edge over oneshot adaptation algorithms. 4.2.3 Results on real world data Results in Figure 7 exhibit challenges associated with real world dataset. The baseline accuracy for cross-domain classification task is severely affected for this dataset. SCL based domain adaptation does no</context>
</contexts>
<marker>Dai, Xue, Yang, Yu, 2007</marker>
<rawString>W Dai, G-R Xue, Q Yang, and Y Yu. 2007. Co-clustering based classification for out-of-domain documents. In Proceedings of International Conference on Knowledge Discovery and Data Mining, pages 210–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815.</title>
<date>2009</date>
<marker>Daum´e, 2009</marker>
<rawString>Hal Daum´e III. 2009. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S Dhillon</author>
<author>S Mallela</author>
<author>D S Modha</author>
</authors>
<title>Information-theoretic co-clustering.</title>
<date>2003</date>
<booktitle>In Proceedings of International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>89--98</pages>
<contexts>
<context position="6874" citStr="Dhillon et al., 2003" startWordPosition="1024" endWordPosition="1027">ter. Daum´e (2009) proposed a heuristic based non-linear mapping of source and target data to a high dimensional space. Pan et al. (2008) proposed a dimensionality reduction method Maximum Mean Discrepancy Embedding to identify a latent space. Subsequently, Pan et al. (2010) proposed to map domain specific words into unified clusters using spectral clustering algorithm. In another follow up work, Pan et al. (2011) proposed a novel feature representation to perform domain adaptation via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy. A similar approach, based on co-clustering (Dhillon et al., 2003), was proposed in Dai et al. (2007) to leverage common words as bridge between two domains. Bollegala et al. (2011) used sentiment sensitive thesaurus to expand features for cross-domain sentiment classification. In a comprehensive evaluation study, it was observed that their approach tends to increase the adaptation performance when multiple source domains were used (Bollegala et al., 2013). Domain adaptation based on iterative learning has been explored by Chen et al. (2011) and Garcia-Fernandez et al. (2014) and are similar to the philosophy of the proposed approach in appending pseudo-labe</context>
</contexts>
<marker>Dhillon, Mallela, Modha, 2003</marker>
<rawString>I. S. Dhillon, S. Mallela, and D. S Modha. 2003. Information-theoretic co-clustering. In Proceedings of International Conference on Knowledge Discovery and Data Mining, pages 89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Garcia-Fernandez</author>
<author>O Ferret</author>
<author>M Dinarelli</author>
</authors>
<title>Evaluation of different strategies for domain adaptation in opinion mining.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<pages>26--31</pages>
<contexts>
<context position="7390" citStr="Garcia-Fernandez et al. (2014)" startWordPosition="1102" endWordPosition="1105">el Hilbert Space using Maximum Mean Discrepancy. A similar approach, based on co-clustering (Dhillon et al., 2003), was proposed in Dai et al. (2007) to leverage common words as bridge between two domains. Bollegala et al. (2011) used sentiment sensitive thesaurus to expand features for cross-domain sentiment classification. In a comprehensive evaluation study, it was observed that their approach tends to increase the adaptation performance when multiple source domains were used (Bollegala et al., 2013). Domain adaptation based on iterative learning has been explored by Chen et al. (2011) and Garcia-Fernandez et al. (2014) and are similar to the philosophy of the proposed approach in appending pseudo-labeled test data to the training set. The first approach uses an expensive feature split to co-train two classifiers while the former presents a single classifier self-training based setting. However, the proposed algorithm offers novel contributions in terms of 1) leveraging two independent feature representations capturing the shared and target specific representations, 2) an ensemble of classifiers that uses labelled source domain and pseudo labelled target domain instances carefully moderated based on similari</context>
</contexts>
<marker>Garcia-Fernandez, Ferret, Dinarelli, 2014</marker>
<rawString>A. Garcia-Fernandez, O. Ferret, and M. Dinarelli. 2014. Evaluation of different strategies for domain adaptation in opinion mining. In Proceedings of the International Conference on Language Resources and Evaluation, pages 26–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-W Hsu</author>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>A practical guide to support vector classification.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, National Taiwan University.</institution>
<contexts>
<context position="16555" citStr="Hsu et al., 2003" startWordPosition="2641" endWordPosition="2644">exp{−ηl(C, Y )} (5) where, η is learning rate set to 0.1, l(y, ˆy) = (y − ˆy)2 is the square loss function, y is the label predicted by the classifier and yˆ is the label predicted by the ensemble. 8. Re-train classifier Ct on Ps. 9. Repeat step 6 − 8 until Pu is empty or maximum number of iterations is reached. ws(l+1) = t w(l+1) = 55 In this iterative manner, the proposed algorithm transforms unlabeled data in the test domain into pseudo labeled data and progressively learns classifier Ct. Confidence of prediction, αz for ith instance, is measured as the distance from the decision boundary (Hsu et al., 2003) which is computed as shown in Eq. 6. α = |R |(6) where R is the un-normalized output from the support vector machine (SVM) classifier, v is the weight vector for support vectors and |v |= vT v. Weights of individual classifiers in the ensemble are updated with each iteration that gradually shifts emphasis from the classifier learned on shared representation to the classifier learned on target domain. Algorithm 1 illustrates the proposed iterative learning algorithm. Algorithm 1 Iterative Learning Algorithm Input: C3 trained on shared co-occurrence based representation Qx, Ct initiated on TFID</context>
<context position="28758" citStr="Hsu et al., 2003" startWordPosition="4552" endWordPosition="4555">a. If this threshold is low, the algorithm converges aggressively (in a few iterations) and does not benefit from the iterative nature of learning the target specific features. Whereas a high threshold tends to make the algorithm conservative. It hampers the accuracy because of the unavailability of sufficient instances to update the classifier after each iteration which also leads to large number of iterations to converge (may not even converge). 01 and 02 are set empirically on a held-out set, with values ranging from zero to distance of farthest classified instance from the SVM hyperplane (Hsu et al., 2003). The knee-shaped curve on the graphs in Figure 5 shows that there exists 58 Figure 5: Bar plot shows % of data that crosses confidence threshold, lower and upper part of the bar represents % correctly and wrongly predicted pseudo labels. The black line shows how the final classification accuracy is effected with threshold. an optimal value for 01 and 02 which yields the best accuracy. We observed that the best accuracy is obtained when the thresholds are set to the distance between the hyper plane and the farthest support vector in each class. 4) Effect of using different shared representatio</context>
</contexts>
<marker>Hsu, Chang, Lin, 2003</marker>
<rawString>C-W. Hsu, C.-C. Chang, and C.-J. Lin. 2003. A practical guide to support vector classification. Technical report, Department of Computer Science, National Taiwan University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-S Ji</author>
<author>J-J Chen</author>
<author>G Niu</author>
<author>L Shang</author>
<author>X-Y Dai</author>
</authors>
<title>Transfer learning via multi-view principal component analysis.</title>
<date>2011</date>
<journal>Journal of Computer Science and Technology,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="5836" citStr="Ji et al., 2011" startWordPosition="864" endWordPosition="867">fier. Jiang and Zhai (2007) proposed instance weighing scheme for domain adaptation in NLP tasks which exploit independence between feature mapping and instance weighing approaches. Saha et al. (2011) leveraged knowledge from source domain to actively select the most informative samples from the target domain. Xia et al. (2013) proposed a hybrid method for sentiment classification task that also addresses the challenge of mutually opposite orientation words. A number of domain adaptation techniques are based on learning common feature representation (Pan and Yang, 2010b; Blitzer et al., 2006; Ji et al., 2011; Daum´e III, 2009) for text classification. The basic idea being identifying a suitable feature space where projected source and target domain data follow similar distributions and hence, a standard supervised learning algorithm can be trained on the former to predict instances from the latter. Among them, Structural Correspondence Learning (SCL) (Blitzer et al., 2007) is the most representative one, explained later. Daum´e (2009) proposed a heuristic based non-linear mapping of source and target data to a high dimensional space. Pan et al. (2008) proposed a dimensionality reduction method Ma</context>
<context position="9451" citStr="Ji et al., 2011" startWordPosition="1415" endWordPosition="1418">echnique based on ensemble of two classifiers. Transfer occurs within the ensemble where a classifier learned on shared representation transforms unlabeled test data into pseudo labeled data to learn domain specific classifier. Before explaining the algorithm, we highlight its salient features: Common Feature Space Representation: Our objective is to find a good feature representation which minimizes divergence between the source and target domains as well as the classification error. There have been several works towards feature-representation-transfer approach such as (Blitzer et al., 2007; Ji et al., 2011) which derives a transformation matrix Q that gives a shared representation between the source and target domains. One of the widely used approaches is Structural Correspondence Learning (SCL) (Blitzer et al., 2006) which aims to learn the co-occurrence between features expressing similar meaning in different domains. Top k Eigenvectors of matrix, W, represent the principal predictors for weight space, Q. Features from both domains are projected on this principal predictor space, Q, to obtain a shared representation. Source domain classifier in our approach is based on this SCL representation.</context>
<context position="29849" citStr="Ji et al., 2011" startWordPosition="4730" endWordPosition="4733">ance between the hyper plane and the farthest support vector in each class. 4) Effect of using different shared representations in ensemble: To study the generalization ability of the proposed algorithm to different shared representations, experiments are performed using three different shared representations on the Amazon review dataset. Apart from using the SCL representation, the accuracy is compared with the proposed algorithm using two other representations, 1) common features between the two domains (“common”) and 2) multiview principal component analysis based representation (“MVPCA”) (Ji et al., 2011) as they are previously used for cross-domain sentiment classification on the same dataset. Table 6 shows that the proposed algorithm yields significant gains in cross-domain classification accuracy with all three representations and is not restricted to any specific representation. The final accuracy depends on the initial classifier trained on the shared representation; therefore, if a shared representation sufficiently captures the characteristics of both source and target domains, the proposed algorithm can be built on any such representation for enhanced cross-domain classification accura</context>
</contexts>
<marker>Ji, Chen, Niu, Shang, Dai, 2011</marker>
<rawString>Y.-S. Ji, J.-J. Chen, G. Niu, L. Shang, and X.-Y. Dai. 2011. Transfer learning via multi-view principal component analysis. Journal of Computer Science and Technology, 26(1):81–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>C Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Proceedings of Association for Computational Linguistics,</booktitle>
<volume>7</volume>
<pages>264--271</pages>
<contexts>
<context position="5248" citStr="Jiang and Zhai (2007)" startWordPosition="772" endWordPosition="775">tation) has shown promising results in recent years (Pan and Yang, 2010a). Prior work on domain adaptation for text classification can be broadly classified into instance re-weighing and featurerepresentation based adaptation approaches. Instance re-weighing approaches address the difference between the joint distributions of observed instances and class labels in source domain with that of target domain. Towards this direction, Liao et al. (2005) learned mismatch between two domains and used active learning to select instances from the source domain to enhance adaptability of the classifier. Jiang and Zhai (2007) proposed instance weighing scheme for domain adaptation in NLP tasks which exploit independence between feature mapping and instance weighing approaches. Saha et al. (2011) leveraged knowledge from source domain to actively select the most informative samples from the target domain. Xia et al. (2013) proposed a hybrid method for sentiment classification task that also addresses the challenge of mutually opposite orientation words. A number of domain adaptation techniques are based on learning common feature representation (Pan and Yang, 2010b; Blitzer et al., 2006; Ji et al., 2011; Daum´e III</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>J. Jiang and C. Zhai. 2007. Instance weighting for domain adaptation in NLP. In Proceedings of Association for Computational Linguistics, volume 7, pages 264–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lang</author>
</authors>
<title>Newsweeder: Learning to filter netnews.</title>
<date>1995</date>
<booktitle>In Proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="18520" citStr="Lang, 1995" startWordPosition="2975" endWordPosition="2976">n two-class classification task and reported in terms of classification accuracy. 4.1 Datasets &amp; Experimental Protocol The first dataset is the Amazon review dataset (Blitzer et al., 2007) which has four different domains, Books, DVDs, Kitchen appliances and Electronics. Each domain comprises 1000 positive and 1000 negative reviews. In all experiments, 1600 labeled reviews from the source and 1600 unlabeled reviews from the target domains are used in training and performance is reported on the non-overlapping 400 reviews from the target domain. The second dataset is the 20 Newsgroups dataset (Lang, 1995) which is a text collection of approximately 20, 000 documents evenly partitioned across 20 newsgroups. For cross-domain text classification on the 20 Newsgroups dataset, we followed the protocol of Dai et al. (2007) where it is divided into six different datasets and the top two categories in each are picked as the two classes. The data is further segregated based on sub-categories, where each sub-category is considered as a different domain. Table 2 lists how different sub-categories are combined to represent the source and target domains. In our experiments, 4/5th of the source and target d</context>
</contexts>
<marker>Lang, 1995</marker>
<rawString>K Lang. 1995. Newsweeder: Learning to filter netnews. In Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Liao</author>
<author>Y Xue</author>
<author>L Carin</author>
</authors>
<title>Logistic regression with an auxiliary data source.</title>
<date>2005</date>
<booktitle>In Proceedings ofInternational Conference on Machine Learning,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="5078" citStr="Liao et al. (2005)" startWordPosition="744" endWordPosition="747">ection 4 presents databases, experimental protocol, and results. Finally, Section 5 concludes the paper. 2 Related Work Transfer learning in text analysis (domain adaptation) has shown promising results in recent years (Pan and Yang, 2010a). Prior work on domain adaptation for text classification can be broadly classified into instance re-weighing and featurerepresentation based adaptation approaches. Instance re-weighing approaches address the difference between the joint distributions of observed instances and class labels in source domain with that of target domain. Towards this direction, Liao et al. (2005) learned mismatch between two domains and used active learning to select instances from the source domain to enhance adaptability of the classifier. Jiang and Zhai (2007) proposed instance weighing scheme for domain adaptation in NLP tasks which exploit independence between feature mapping and instance weighing approaches. Saha et al. (2011) leveraged knowledge from source domain to actively select the most informative samples from the target domain. Xia et al. (2013) proposed a hybrid method for sentiment classification task that also addresses the challenge of mutually opposite orientation w</context>
</contexts>
<marker>Liao, Xue, Carin, 2005</marker>
<rawString>X. Liao, Y. Xue, and L. Carin. 2005. Logistic regression with an auxiliary data source. In Proceedings ofInternational Conference on Machine Learning, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Luo</author>
<author>Y Ji</author>
<author>X Dai</author>
<author>J Chen</author>
</authors>
<title>Active learning with transfer learning.</title>
<date>2012</date>
<booktitle>In Proceedings ofAssociation for Computational Linguistics Student Research Workshop,</booktitle>
<pages>13--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11868" citStr="Luo et al., 2012" startWordPosition="1796" endWordPosition="1799">o the pool of pseudo labeled data and the classifier in the target domain is updated. This process is repeated till all unlabeled data is labeled or certain maximum number of iterations is performed. This way we gradually adapt the target domain classifier on pseudo labeled data using the knowledge transferred from source domain. In Section 4, we empirically demonstrate effectiveness of this technique compared to one-shot adaptation approaches. Domain Similarity-based Aggregation: Performance of domain adaptation is often constrained by the dissimilarity between the source and target domains (Luo et al., 2012; Rosenstein et al., 2005; Chin, 2013; Blitzer et al., 2007). If the two domains are largely similar, the knowledge learned in the source domain can be aggressively transferred to the target domain. On the other hand, if the two domains are less similar, knowledge learned in the source domain should be transferred in a conservative manner so as to mitigate the effects of negative transfer. Therefore, it is imperative for domain adaptation techniques to account for similarity between domains and transfer knowledge in a similarity aware manner. While this may sound obvious, we do not see many wo</context>
</contexts>
<marker>Luo, Ji, Dai, Chen, 2012</marker>
<rawString>C. Luo, Y. Ji, X. Dai, and J. Chen. 2012. Active learning with transfer learning. In Proceedings ofAssociation for Computational Linguistics Student Research Workshop, pages 13–18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Pan</author>
<author>Q Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="2589" citStr="Pan and Yang, 2010" startWordPosition="375" endWordPosition="378">applicability. Training models from scratch for every new domain requires human annotated labeled data which is expensive and time consuming, hence, not pragmatic. On the other hand, transfer learning techniques allow domains, tasks, and distributions used in training and testing to be different, but related. It works in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains. While transfer learning has generally proved useful in reducing the labelled data requirement, brute force techniques suffer from the problem of negative transfer (Pan and Yang, 2010a). One cannot use transfer learning as the proverbial hammer, but needs to gauge when to transfer and also how much to transfer. To address these issues, this paper proposes a domain adaptation technique for cross-domain text classification. In our setting for cross-domain classification, a classifier trained on one domain with sufficient labelled training data is applied to a different test domain with no labelled data. As shown in Figure 1, this paper proposes an iterative similarity based adaptation algorithm which starts with a shared feature representation of source and target domains. T</context>
<context position="4698" citStr="Pan and Yang, 2010" startWordPosition="689" endWordPosition="692">ss. To the best of our knowledge, this is the first-ofits-kind approach in cross-domain text classification which integrates similarity between domains in the adaptation setting to learn domain specific features in an iterative manner. The rest of the paper is organized as follows: Section 2 summarizes the related work, Section 3 presents details about the proposed algorithm. Section 4 presents databases, experimental protocol, and results. Finally, Section 5 concludes the paper. 2 Related Work Transfer learning in text analysis (domain adaptation) has shown promising results in recent years (Pan and Yang, 2010a). Prior work on domain adaptation for text classification can be broadly classified into instance re-weighing and featurerepresentation based adaptation approaches. Instance re-weighing approaches address the difference between the joint distributions of observed instances and class labels in source domain with that of target domain. Towards this direction, Liao et al. (2005) learned mismatch between two domains and used active learning to select instances from the source domain to enhance adaptability of the classifier. Jiang and Zhai (2007) proposed instance weighing scheme for domain adap</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>S. J. Pan and Q. Yang. 2010a. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345–1359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="2589" citStr="Pan and Yang, 2010" startWordPosition="375" endWordPosition="378">applicability. Training models from scratch for every new domain requires human annotated labeled data which is expensive and time consuming, hence, not pragmatic. On the other hand, transfer learning techniques allow domains, tasks, and distributions used in training and testing to be different, but related. It works in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains. While transfer learning has generally proved useful in reducing the labelled data requirement, brute force techniques suffer from the problem of negative transfer (Pan and Yang, 2010a). One cannot use transfer learning as the proverbial hammer, but needs to gauge when to transfer and also how much to transfer. To address these issues, this paper proposes a domain adaptation technique for cross-domain text classification. In our setting for cross-domain classification, a classifier trained on one domain with sufficient labelled training data is applied to a different test domain with no labelled data. As shown in Figure 1, this paper proposes an iterative similarity based adaptation algorithm which starts with a shared feature representation of source and target domains. T</context>
<context position="4698" citStr="Pan and Yang, 2010" startWordPosition="689" endWordPosition="692">ss. To the best of our knowledge, this is the first-ofits-kind approach in cross-domain text classification which integrates similarity between domains in the adaptation setting to learn domain specific features in an iterative manner. The rest of the paper is organized as follows: Section 2 summarizes the related work, Section 3 presents details about the proposed algorithm. Section 4 presents databases, experimental protocol, and results. Finally, Section 5 concludes the paper. 2 Related Work Transfer learning in text analysis (domain adaptation) has shown promising results in recent years (Pan and Yang, 2010a). Prior work on domain adaptation for text classification can be broadly classified into instance re-weighing and featurerepresentation based adaptation approaches. Instance re-weighing approaches address the difference between the joint distributions of observed instances and class labels in source domain with that of target domain. Towards this direction, Liao et al. (2005) learned mismatch between two domains and used active learning to select instances from the source domain to enhance adaptability of the classifier. Jiang and Zhai (2007) proposed instance weighing scheme for domain adap</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang. 2010b. A survey on transfer learning. Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345–1359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>James T Kwok</author>
<author>Qiang Yang</author>
</authors>
<title>Transfer learning via dimensionality reduction.</title>
<date>2008</date>
<booktitle>In AAAI,</booktitle>
<volume>8</volume>
<pages>677--682</pages>
<contexts>
<context position="6390" citStr="Pan et al. (2008)" startWordPosition="952" endWordPosition="955">ion (Pan and Yang, 2010b; Blitzer et al., 2006; Ji et al., 2011; Daum´e III, 2009) for text classification. The basic idea being identifying a suitable feature space where projected source and target domain data follow similar distributions and hence, a standard supervised learning algorithm can be trained on the former to predict instances from the latter. Among them, Structural Correspondence Learning (SCL) (Blitzer et al., 2007) is the most representative one, explained later. Daum´e (2009) proposed a heuristic based non-linear mapping of source and target data to a high dimensional space. Pan et al. (2008) proposed a dimensionality reduction method Maximum Mean Discrepancy Embedding to identify a latent space. Subsequently, Pan et al. (2010) proposed to map domain specific words into unified clusters using spectral clustering algorithm. In another follow up work, Pan et al. (2011) proposed a novel feature representation to perform domain adaptation via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy. A similar approach, based on co-clustering (Dhillon et al., 2003), was proposed in Dai et al. (2007) to leverage common words as bridge between two domains. Bollegala et al. (2011) </context>
</contexts>
<marker>Pan, Kwok, Yang, 2008</marker>
<rawString>Sinno Jialin Pan, James T Kwok, and Qiang Yang. 2008. Transfer learning via dimensionality reduction. In AAAI, volume 8, pages 677–682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Pan</author>
<author>X Ni</author>
<author>J-T Sun</author>
<author>Q Yang</author>
<author>Z Chen</author>
</authors>
<title>Crossdomain sentiment classification via spectral feature alignment.</title>
<date>2010</date>
<booktitle>In Proceedings International Conference on World Wide Web,</booktitle>
<pages>751--760</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6528" citStr="Pan et al. (2010)" startWordPosition="972" endWordPosition="975">ng a suitable feature space where projected source and target domain data follow similar distributions and hence, a standard supervised learning algorithm can be trained on the former to predict instances from the latter. Among them, Structural Correspondence Learning (SCL) (Blitzer et al., 2007) is the most representative one, explained later. Daum´e (2009) proposed a heuristic based non-linear mapping of source and target data to a high dimensional space. Pan et al. (2008) proposed a dimensionality reduction method Maximum Mean Discrepancy Embedding to identify a latent space. Subsequently, Pan et al. (2010) proposed to map domain specific words into unified clusters using spectral clustering algorithm. In another follow up work, Pan et al. (2011) proposed a novel feature representation to perform domain adaptation via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy. A similar approach, based on co-clustering (Dhillon et al., 2003), was proposed in Dai et al. (2007) to leverage common words as bridge between two domains. Bollegala et al. (2011) used sentiment sensitive thesaurus to expand features for cross-domain sentiment classification. In a comprehensive evaluation study, it w</context>
<context position="24656" citStr="Pan et al., 2010" startWordPosition="3859" endWordPosition="3862">ured by the proposed algorithm. Key observations and analysis from the experiments on different datasets is summarized below. 4.2.1 Results on the Amazon Review dataset To study the effects of different components of the proposed algorithm, comprehensive experiments are performed on the Amazon review dataset3. 1) Effect of learning target specific features: Results in Figure 3 show that iteratively learning target specific feature representation (slow transfer as opposed to one-shot transfer) yields better performance across different cross-domain classification tasks as compared to SCL, SFA (Pan et al., 2010)4 and the baseline. Unlike SCL and SFA, the proposed approach uses shared and target specific feature representations for the cross-domain classification task. Table 4 illustrates some examples of the target specific discriminative features learned by the proposed algorithm that leads to enhanced performance. At 95% confidence, parametric ttest suggests that the proposed algorithm and SCL are significantly (statistically) different. 2) Effect of similarity on performance: It is observed that existing domain adaptation techniques enhance the accuracy for cross-domain classification, though, neg</context>
</contexts>
<marker>Pan, Ni, Sun, Yang, Chen, 2010</marker>
<rawString>S. J. Pan, X. Ni, J-T Sun, Q. Yang, and Z. Chen. 2010. Crossdomain sentiment classification via spectral feature alignment. In Proceedings International Conference on World Wide Web, pages 751–760. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Pan</author>
<author>I W Tsang</author>
<author>J T Kwok</author>
<author>Q Yang</author>
</authors>
<title>Domain adaptation via transfer component analysis.</title>
<date>2011</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="6670" citStr="Pan et al. (2011)" startWordPosition="994" endWordPosition="997">ng algorithm can be trained on the former to predict instances from the latter. Among them, Structural Correspondence Learning (SCL) (Blitzer et al., 2007) is the most representative one, explained later. Daum´e (2009) proposed a heuristic based non-linear mapping of source and target data to a high dimensional space. Pan et al. (2008) proposed a dimensionality reduction method Maximum Mean Discrepancy Embedding to identify a latent space. Subsequently, Pan et al. (2010) proposed to map domain specific words into unified clusters using spectral clustering algorithm. In another follow up work, Pan et al. (2011) proposed a novel feature representation to perform domain adaptation via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy. A similar approach, based on co-clustering (Dhillon et al., 2003), was proposed in Dai et al. (2007) to leverage common words as bridge between two domains. Bollegala et al. (2011) used sentiment sensitive thesaurus to expand features for cross-domain sentiment classification. In a comprehensive evaluation study, it was observed that their approach tends to increase the adaptation performance when multiple source domains were used (Bollegala et al., 2013). </context>
</contexts>
<marker>Pan, Tsang, Kwok, Yang, 2011</marker>
<rawString>S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. 2011. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Rosenstein</author>
<author>Z Marx</author>
<author>L P Kaelbling</author>
<author>T G Dietterich</author>
</authors>
<title>To transfer or not to transfer.</title>
<date>2005</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems Workshop, Inductive Transfer: 10 Years Later.</booktitle>
<contexts>
<context position="11893" citStr="Rosenstein et al., 2005" startWordPosition="1800" endWordPosition="1803">do labeled data and the classifier in the target domain is updated. This process is repeated till all unlabeled data is labeled or certain maximum number of iterations is performed. This way we gradually adapt the target domain classifier on pseudo labeled data using the knowledge transferred from source domain. In Section 4, we empirically demonstrate effectiveness of this technique compared to one-shot adaptation approaches. Domain Similarity-based Aggregation: Performance of domain adaptation is often constrained by the dissimilarity between the source and target domains (Luo et al., 2012; Rosenstein et al., 2005; Chin, 2013; Blitzer et al., 2007). If the two domains are largely similar, the knowledge learned in the source domain can be aggressively transferred to the target domain. On the other hand, if the two domains are less similar, knowledge learned in the source domain should be transferred in a conservative manner so as to mitigate the effects of negative transfer. Therefore, it is imperative for domain adaptation techniques to account for similarity between domains and transfer knowledge in a similarity aware manner. While this may sound obvious, we do not see many works in domain adaptation </context>
</contexts>
<marker>Rosenstein, Marx, Kaelbling, Dietterich, 2005</marker>
<rawString>M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. Dietterich. 2005. To transfer or not to transfer. In Proceedings of Advances in Neural Information Processing Systems Workshop, Inductive Transfer: 10 Years Later.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Saha</author>
<author>P Rai</author>
<author>H Daum´e</author>
<author>S Venkatasubramanian</author>
<author>S L DuVall</author>
</authors>
<title>Active supervised domain adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of European Conference on Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>97--112</pages>
<marker>Saha, Rai, Daum´e, Venkatasubramanian, DuVall, 2011</marker>
<rawString>A. Saha, P. Rai, H. Daum´e, S. Venkatasubramanian, and S. L. DuVall. 2011. Active supervised domain adaptation. In Proceedings of European Conference on Machine Learning and Knowledge Discovery in Databases, pages 97– 112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Xia</author>
<author>C Zong</author>
<author>X Hu</author>
<author>E Cambria</author>
</authors>
<title>Feature ensemble plus sample selection: domain adaptation for sentiment classification.</title>
<date>2013</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>18</pages>
<contexts>
<context position="5550" citStr="Xia et al. (2013)" startWordPosition="820" endWordPosition="823">oint distributions of observed instances and class labels in source domain with that of target domain. Towards this direction, Liao et al. (2005) learned mismatch between two domains and used active learning to select instances from the source domain to enhance adaptability of the classifier. Jiang and Zhai (2007) proposed instance weighing scheme for domain adaptation in NLP tasks which exploit independence between feature mapping and instance weighing approaches. Saha et al. (2011) leveraged knowledge from source domain to actively select the most informative samples from the target domain. Xia et al. (2013) proposed a hybrid method for sentiment classification task that also addresses the challenge of mutually opposite orientation words. A number of domain adaptation techniques are based on learning common feature representation (Pan and Yang, 2010b; Blitzer et al., 2006; Ji et al., 2011; Daum´e III, 2009) for text classification. The basic idea being identifying a suitable feature space where projected source and target domain data follow similar distributions and hence, a standard supervised learning algorithm can be trained on the former to predict instances from the latter. Among them, Struc</context>
</contexts>
<marker>Xia, Zong, Hu, Cambria, 2013</marker>
<rawString>R. Xia, C. Zong, X. Hu, and E. Cambria. 2013. Feature ensemble plus sample selection: domain adaptation for sentiment classification. IEEE Intelligent Systems, 28(3):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Zhao</author>
<author>S C H Hoi</author>
</authors>
<title>OTL: A Framework of Online Transfer Learning.</title>
<date>2010</date>
<booktitle>In Proceeding ofInternational Conference on Machine Learning.</booktitle>
<marker>Zhao, Hoi, 2010</marker>
<rawString>P. Zhao and S. C. H. Hoi. 2010. OTL: A Framework of Online Transfer Learning. In Proceeding ofInternational Conference on Machine Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>