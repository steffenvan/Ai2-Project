<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017448">
<title confidence="0.982092">
Translation with Cascaded Finite State Transducers
</title>
<author confidence="0.985554">
Stephan Vogel and Hermann Ney
</author>
<affiliation confidence="0.9385235">
Lehrstuhl fiir Informatik VI, Computer Science Department
RWTH Aachen - University of Technology
</affiliation>
<address confidence="0.616412">
D-52056 Aachen, Germany
</address>
<email confidence="0.997212">
vogel@informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.995609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999450846153846">
In this paper we discuss the use
of cascaded finite state transducers
for machine translation. A num-
ber of small, dedicated transducers
is applied to convert sentence pairs
from a bilingual corpus into gener-
alized translation patterns. These
patterns, together with the trans-
ducers are then used as a hierarchi-
cal translation memory for fully au-
tomatic translation. Results on the
German—English VERBMOBIL cor-
pus are given.
</bodyText>
<sectionHeader confidence="0.998524" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955119047619">
Corpus based approaches to automatic trans-
lation come in a number of different flavors.
In the simplest form, translations are stored
and reused for the translation of new input.
This approach, known as translation memory,
example-based or case-based translation, can
work on the word level as well as on structured
examples as they are generated during anal-
ysis and generation in more grammar-based
translation paradigms (Kitano, 1993; Brown,
1996).
Finite state transducers, which can be
learned from bilingual corpora, have been
proposed for automatic translation (Amen-
gual et al., 2000), as have been bilingual
stochastic grammars (Wu, 1996). Statistical
approaches (Wang and Waibel, 1997; Och et
al., 1999) also fall into the category of corpus
based approaches.
In this paper, a translation method is pro-
posed which is based on the very same princi-
ples as the aforementioned approaches. One
difference is, that not a fully automatic train-
ing of the translation model is performed.
Rather, a number of special purpose trans-
ducers are hand-crafted and used then at two
points. First, to convert the bilingual train-
ing corpus into a translation memory contain-
ing translation patterns rather than merely
sentence pairs, and which is itself used as a
transducer in the translation process. Second,
when new sentences are to be translated, the
transducers are applied to transform the in-
put sentence into one or many possible target
sentences the best of which, according to some
scoring scheme, is selected as the translation.
In the next section, the construction of the
transducers and the translation memory is
outlined. Then, the application of the trans-
ducers for the translation of new sentences is
described. In the last section the results of
some translation experiments are given.
</bodyText>
<sectionHeader confidence="0.990169" genericHeader="method">
2 The Transducers
</sectionHeader>
<subsectionHeader confidence="0.655416">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999879088235294">
A finite state transducer (FST) is a finite
state device which reads symbols from one
channel and outputs a stream of symbols to a
second channel. So, a FST can be depicted as
a transition net with edges and nodes, where
the nodes represent the states and the edges
the possible state transitions. The edges are
labelled with an input symbol and an output
string, which may be the empty words of the
two vocabularies. The final states can pro-
duce additional output.
We want to construct transducers for auto-
matic machine translation from a given bilin-
gual corpus. In fact, a collection of sentence
pairs can be viewed as a trivial transducer,
where each sentence pair is represented by a
distinct line of nodes connected by edges la-
beled with the source sentence words and the
target sentence emitted from the final state.
This can be easily transformed into a tree
transducer by building a prefix tree over the
source sentences.
In (Amengual et al., 2000) a method is
given to propagate prefixes of the transla-
tions towards the root of such a tree trans-
ducer and to coalesce states to gain gener-
alization power. We choose here a different
route to generalization by using an approach
similar to the one used for chunk parsing,
where a cascade of FST is applied (Abney,
1997). Each transducer, defined by a set of
regular-expression patterns, reads part of the
input sentence and writes a stream of cate-
gory labels, which form, together with the un-
analyzed parts of the sentence, the input to
the next transducer in the cascade.
Our approach differs from the aforemen-
tioned chunk parsing in that an analyzed se-
quence of words is not replaced by the cat-
egory label but is kept as a parallel option
for transducers applied at a later stage. How
this leads to the construction of a translation
graph will be explained in Section 3.
For translation, not only the analysis of the
source sentence is required but also the gen-
eration of the target sentence. This can be
achieved if the transducers write category la-
bels as well as translations to the output chan-
nel.
We allow for more than one translation for
a given input sequence. This raises the ques-
tion of how to select one translation over the
others. Some kind of scoring is required, a
point we will return to in section 2.3.
To summarize: each transducer is given as
a set of quadruples of the form: [label #
source pattern # target pattern # score]. At
runtime these patterns are stored in a pre-
fix tree with respect to the source patterns.
We write the labels at first position as these
translations patterns can be used in the re-
verse direction, i.e. from target language to
source language. In section 2.4 this property
is used to convert a bilingual corpus into a
set of translation patterns which are formu-
lated in terms of words and category labels.
It also shows the structural identity to bilin-
gual grammars as used in (Wu, 1996).
</bodyText>
<subsectionHeader confidence="0.999146">
2.2 Construction of the Transducers
</subsectionHeader>
<bodyText confidence="0.998404857142857">
Most of the transducers are customized to-
wards the domain for which the translation
system is developed. In the VERBMOBIL
Corpus, which is used for the experiments,
time and date expressions are very promi-
nent. To translate those expressions, a small
grammar has been developed and coded as
finite state transducer. Actually, two trans-
ducers are used. On the first level, words
are replaced by labels, like DAYOFWEEK
= {Montag/Monday, Dienstag/Tuesday, ...I.
On the second level, these labels together with
labeled numbers (ordinal, cardinal, fractions)
from the number transducer are used to form
complex time and date expressions. Some ex-
amples are given in Table 1.
All in all we use currently seven of
those dedicated transducers: names (persons,
towns, places, events, etc), spelling sequences
(e.g. &apos;D A double L&apos;), numbers (ordinal, car-
dinal, fractions, etc.), simple time and date
expressions, compound time and date expres-
sions, part-of-speech tagging, grammar (noun
phrases, verb phrases). The relationship be-
tween these different transducers is depicted
in figure 1. The arrows indicate that category
labels introduces by one transducer are used
by another transducer.
The division into these transducers is
mainly a conceptual one. The five base level
transducers could be coalesced into one trans-
ducer. Actually, this is done at runtime for
efficiency. However, to keep them appart at
construction time gives more flexibility. For
example, while for a closed vocabulary in a
speech translation task these transducers boil
down to simple substitution list an open vo-
cabulary task will require a more elaborate
approach to proper name spotting or handling
of numbers.
The part-of-speech transducer has been
constructed semi-automatically. A tagger was
</bodyText>
<figure confidence="0.997208555555555">
POS-Tags
Number
Simple
Date
Spell
Name
Compound
Date
Grammar
</figure>
<tableCaption confidence="0.895158">
Table 2: Compound translation patterns.
</tableCaption>
<table confidence="0.88533975">
CTP # DATE ginge es wieder # DATE it is possible again # -4.6
CTP # NAME SURNAME am Apparat # this is NAME SURNAME speaking # -4.6
CTP # NP dauert DATE # NP takes DATE # -3.3
CTP # nehmen PPER NP DATE # let PPER take NP DATE # -4.6
</table>
<bodyText confidence="0.999810771428572">
graph. That is to say, the sentence to be
translated is viewed as a graph which is tra-
versed from left to right. For each matching
source pattern, as stored in the transducers,
a new edge is added to the graph. The edge is
labeled with the category label of the transla-
tion pattern. The translation and the trans-
lation score are attached to the edge. In this
way a translation graph is constructed. In
those cases, where a source pattern has sev-
eral translations, one edge for each translation
is added to the graph. One advantage of this
approach is that it can be applied to perform
translation on word lattices as generated by
speech recognition systems without any mod-
ifications.
The left—right traversal of the graph is or-
ganized in such a way that all paths are tra-
versed in parallel and the patterns stored in
the transducer are matched synchronously.
For each node n and each edge e leading to
that node all patterns in the transducer start-
ing with the word or category label of e are
attached to n. This gives a number of hy-
potheses describing partially matching pat-
terns. Already started hypotheses are ex-
panded with the label of the edge running
from the previous node to the current node.
As an example, the translation graph for
the sentence `Samstag und Februar sind gut,
aber der vierte ware besser&apos; is shown in Fig-
ure 2. Actually, the graph is much bigger. In
the figure, only those edges are shown which
contributed to the construction of the best
path.
</bodyText>
<subsectionHeader confidence="0.935618">
3.1 Error Tolerant Match
</subsectionHeader>
<bodyText confidence="0.999907606060606">
To improve the coverage on unseen test data,
it may be advantageous to allow for only ap-
proximative matching with the segments in
the translation memory. The idea is to apply
longer segments for syntactically better trans-
lations without losing too much as far as the
content of the sentences is concerned. We use
a weighted edit distance, i.e. each error (inser-
tion, deletion, substitution) is associated with
a score. Thereby, the deletion or insertion of
typical filler words can be allowed, whereas
the deletion or insertion of content words is
avoided.
Hypotheses with to high a matching er-
ror score are discarded. A threshold propor-
tional to the number of covered positions is
used. Thus, longer translation patterns can
be matched with more insertions, deletions
and substitutions. A drawback of this is, how-
ever, that for long patterns mismatches on
content words may occur.
Each transducer has its own list of inser-
tion, deletion and substiution scores. Ac-
tually, only for those transducers where the
translation patterns cover longer sequences
of words and labels do we use error tolerant
matching.
Error-tolerant matching may also help to
compensate for speech recognition errors in
the case of speech translations. In that case
the confusion matrix obtained by comparing
the recognizer output for the training speech
data with the transliteration can be used.
</bodyText>
<subsectionHeader confidence="0.999963">
3.2 Using a Language Model
</subsectionHeader>
<bodyText confidence="0.9996644">
The application of the transducers to a given
source sentence yield a large number of tar-
get sentences. These are scored according to
the cumulative scores of the applied transla-
tion patterns. As an independent and direct
model of the likelihood of the target sentences
a language model is applied. We use a word-
based trigram language model (Sawaf et al.,
2000). The logarithm of the language model
probabilities is added to the transducer scores
</bodyText>
<figure confidence="0.994849285714286">
C_PHRASE
the fourth would be better
-7.4
DATE
Saturday and February
-4,2
DATE
the fourth
-4.1
DATE
Saturday
-0.6
DATEDAY
the fourth
-4.0
DATE
February
-0.6
DAYWEEK
Saturday
-0.5
NUM_ORD
fourth
-2.0
ADV
but
-0.1
S_PHRASE
are good
-2.1
MONTH
February
-0.5
Samstag und Februar sind gut aber
der vierte waere besser
</figure>
<bodyText confidence="0.99980775">
are evaluated by a human examiner using a
scale ranging from 0 to 10. The average of
these values is linearly transformed to give the
sentence error rate in percent.
</bodyText>
<subsectionHeader confidence="0.999225">
4.1 Effect of Grammar
</subsectionHeader>
<bodyText confidence="0.999828444444445">
A simple translation memory without any
categorization gives insufficient coverage on
unseen test data. With the part-of-speech
transducer we get one or more translations
for each word in the vocabulary. But only
by applying transducers which handle longer
translation patterns is word reordering possi-
ble.
In Table 6 the results are given for differ-
ent combinations of transducers. The baseline
(T) is the combination of all special purpose
transducers (name, spell, number, date, word
tags) plus the simple translations patterns.
Then the grammar was added and finally the
compound translation patterns. The trigram
language model for the target language was
applied in selecting the best translation, but
no error tolerant matching was allowed.
</bodyText>
<tableCaption confidence="0.692125">
Table 6: Effect of bilingual grammar on trans-
lation quality: T = POS-tagging, G = gram-
mar, C = compound translation patterns.
</tableCaption>
<table confidence="0.992283">
Transducer mWER[%] SSER[%]
T 41.2 25.8
TG 39.7 22.5
TGC 38.8 22.1
</table>
<bodyText confidence="0.999890133333333">
We observe a clear effect in word error rate
and subjective sentence error rate. The use of
the bilingual grammar, also very restricted,
improves translation quality. Applying the
compound translation patterns gives an ad-
ditional small improvement.
In Table 5 a simple and a more involved
example for the reordering effect of the bilin-
gual grammar are given. The first translation
pattern operates solely on the level of POS
tags whereas the second example generates a
hierarchical structure. We are not concerned
whether the source sentence parses are cor-
rect, good translations is what we are looking
for.
</bodyText>
<subsectionHeader confidence="0.998308">
4.2 Effect of Language Model
</subsectionHeader>
<bodyText confidence="0.997645">
The next experiment shows the effect of ap-
plying a language model for the target lan-
guage. A word-based trigram language model
was interpolated with the scores from the
transducers. In Table 7 the effect of the scal-
ing between the two models is shown.
There is a clear drop in the WER when
switching on the language model. This is due
to the fact, that several translation hypothe-
ses have the same score from the transducers.
So, it is rather by chance if the best transla-
tion for a given word is chosen. The language
model for the target language helps in doing
this.
</bodyText>
<tableCaption confidence="0.9570245">
Table 7: Effect of language model on word
error rate and subjective sentence error rate.
</tableCaption>
<table confidence="0.999235666666667">
LM Scale mWER[%] SSER[%]
0.0 49.3 31.8
0.2 38.8 23.5
0.5 38.8 22.1
1.0 39.4 23.8
5.0 42.6 27.4
</table>
<bodyText confidence="0.9997805">
There is a second benefit gained from the
language model: sometimes the source sen-
tence can be covered with only very short
source patterns. That is to say, word con-
text is hardly taken into account. With a
language model context is brought into play
again. If the language model scaling factor is
increased too much translation quality dete-
riorates again. So, a good balance between
both knowledge sources is necessary.
In Table 8 some examples which show the
effect of the language model are given. The
first translation is without language model,
the second is the translation obtained when
the language model score is added using a
scaling factor of 0.5.
</bodyText>
<subsectionHeader confidence="0.999217">
4.3 Effect of Error Tolerant Matching
</subsectionHeader>
<bodyText confidence="0.99629925">
Finally, the effect of error tolerant match-
ing has been investigated. Only for the sim-
ple and compound translation patterns errors
have been allowed in matching parts of the in-
</bodyText>
<tableCaption confidence="0.850565">
Table 5: Example for the application of the bilingual grammar.
</tableCaption>
<table confidence="0.781632636363636">
VP # PPER VMFIN PP VVINF # PPER VMFIN VVINF PP
VP { PPER { ich # I # -0.1 }
VMFIN { m&amp;quot;ochte # want # -0.1 }
PP { APPR { mit # with # -0.1 }
{ PPER { Ihnen # you # -0.1 }
NP { ART { einen # a # 0.01 }
{ NN { Termin # date # -0.1 }
# a date # -2.09 }
# a date with you # -6.29 1
VVINF { vereinbaren # to arrange # -0.1 }
# I want to arrange a date with you # -12.59 1
</table>
<tableCaption confidence="0.980203">
Table 8: Examples for the effect of the language model.
</tableCaption>
<bodyText confidence="0.986343363636363">
ja, wunderbar. machen wir das so, und dann treffen wir uns dann in Hamburg.
no LM yes, nice. will we do which right, after all we meet us after all in Hamburg.
with LM fine, let us do it like that, and then we will meet then in Hamburg.
erst wieder ab dem sechzehnten.
no LM
starting from the sixteenth only again.
with LM
only starting from the sixteenth.
put sentences to stored translation patterns.
The effect of increasing the error threshold is
given in Table 9.
</bodyText>
<tableCaption confidence="0.997623">
Table 9: Effect of error tolerant matching.
</tableCaption>
<table confidence="0.997523">
Errors per word mWER[%] SSER[%]
0.0 38.8 22.1
0.2 38.3 20.3
0.4 37.0 21.0
0.6 39.6 24.2
</table>
<bodyText confidence="0.9999445">
We see a considerable improvement when
allowing for a small number of errors in
matching the translation patterns to the in-
put sentence. However, if the match gets
too sloppy serious errors occur which alter
the meaning of the sentence. For longer se-
quences of words the number of errors allowed
becomes higher than the default score for sub-
stitutions. In such a case content words can
be substituted.
An example of how the same source sen-
tence gets different translations when more
matching errors are allowed is given in Ta-
ble 10.
</bodyText>
<sectionHeader confidence="0.838858" genericHeader="conclusions">
5 Summary and future work
</sectionHeader>
<bodyText confidence="0.999760944444444">
In this paper a translation approach based
on cascaded finite state transducers has been
presented. A small number of simple trans-
ducers is hand-crafted and then used to con-
vert a bilingual corpus into a translation
memory consisting of source pattern — target
pattern pairs, which include category labels.
Translation is then performed by applying the
complete cascade of transducers.
With the simple heuristic for the transla-
tion scores a language model for the target
language is paramount to select good trans-
lations. Error-tolerant matching improves
translation quality.
Experiments have shown the potential of
this approach for machine translation. Good
coverage on unseen test data could be ob-
tained. A major advantage of this translation
</bodyText>
<tableCaption confidence="0.409939">
Table 10: Examples for the effect of error tolerant matching.
</tableCaption>
<note confidence="0.606049">
ja , wunderbar. . machen wir das so , und dann treffen wir uns dann in Hamburg.
0.0 fine, let us do it like that , and then we will meet then in Hamburg.
0.2 fine, let us do that , and then we will meet in Hamburg.
0.4 fine, let us do it like that , and then we will meet in Hamburg.
0.6 fine . let us do it like that , and then we will meet in your office .
</note>
<bodyText confidence="0.997964473684211">
method is that it breaks the middle ground
between direct translation methods like sim-
ple translation memory or word-based statis-
tical translation and transfer based methods
involving deep linguistic analysis of the input.
In fact, the cascaded transducer approach al-
lows for building quickly a first version and
improving translation quality by gradually
adding more linguistic and domain specific
knowledge.
We expect further improvement by assign-
ing translation scores according to corpus
statistics. This will be the main focus for fu-
ture work.
Acknowledgement. This work was
partly supported by the German Federal Min-
istry of Education, Science, Research and
Technology under the Contract Number 01
IV 701 T4 (VERBmoBIL).
</bodyText>
<sectionHeader confidence="0.99826" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981285483871">
S. Abney. 1997. Part-of-speech tagging and par-
tial parsing. In S. Young and G. Bloothooft,
(Eds.), Corpus-based Methods in Language and
Speech Processing, pages 118-136. Kluwer Aca-
demic Publishers, Dordrecht, Boston, London.
J. C. Amengual, J. M. Benedi, F. Casacuberta,
A. Castano, A. Castellanos, V. M. Jimenez,
D. Llorens, A. Marzal, M. Pastor, F. Prat,
E. Vidal, and J. M. Vilar. 2000. The Eutrans-I
speech translation system. Machine Transla-
tion, Special Issue, forthcoming.
R. D. Brown. 1996. Example-based machine
translation in the pangloss system. In Proc.
of COLING &apos;96: The 16th Int. Conf. on Com-
putational Linguistics, pages 169-174, Copen-
hagen, Denmark, August.
H. Kitano. 1993. A comprehensive and practi-
cal model of memory-based machine transla-
tion. In R. Bajcsy, editor, Proc. of the 13th
Int. Joint Conf. on Artificial Intelligence, pages
1276-1282. Morgan Kaufmann.
S. Nief3en, F. J. Och, G. Leusch, and H. Ney.
2000. An evaluation tool for machine transla-
tion: Fast evaluation for MT research. In 2nd
Int. Conf. on Language Resources and Evalua-
tion, pages 39-45, Athens, Greece, May.
F. J. Och, C. Tillmann, and H. Ney. 1999.
Improved alignment models for statistical ma-
chine translation. In Proc. of the Joint SIG-
DAT Conf. on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 20-28, University of Maryland, College
Park, MD, USA, June.
H. Sawaf, K. Schutz, and H. Ney. 2000. On the
use of grammar based language models for sta-
tistical machine translation. In 6th Int. Work-
shop on Parsing Technologies, pages 231-241,
Trento, Italy, February.
A. Schiller, S. Teufel, and C. Thielen. 1995.
Guidelines fiir das Tagging deutscher Text-
korpora mit STTS. Technical report, Uni-
versitat Stuttgart and Universitat Tubingen.
http://www.sfs.npphil.uni-tuebingen.de/
Elwis/stts/stts.html.
S. Vogel and H. Ney. 2000. Construction of
a hierarchical translation memory. In Proc.
of COLING 2000: The 18th Int. Conf. on
Computational Linguistics, pages 1131-1135,
Saarbriicken, Germany, July.
Wahlster, W. (Ed.) 2000. Verbmobil: Founda-
tions of Speech-to-Speech Translation. Springer-
Verlag Heidelberg.
Y.-Y. Wang and A. Waibel. 1997. Decoding algo-
rithm in statistical translation. In Proc. of the
35th Annual Conf. of the Association for Com-
putational Linguistics, pages 366-372, Madrid,
Spain, July.
D. Wu. 1996. A Polynomial-Time Algorithm for
Statistical Machine Translation. In Proc. of the
34th Annual Conf. of the Association for Com-
putational Linguistics, Santa Cruz, CA, pages
152-158, June.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961581">
<title confidence="0.999874">Translation with Cascaded Finite State Transducers</title>
<author confidence="0.999964">Stephan Vogel</author>
<author confidence="0.999964">Hermann Ney</author>
<affiliation confidence="0.99392">Lehrstuhl fiir Informatik VI, Computer Science Department RWTH Aachen - University of Technology</affiliation>
<address confidence="0.999787">D-52056 Aachen, Germany</address>
<abstract confidence="0.997289428571429">In this paper we discuss the use of cascaded finite state transducers for machine translation. A number of small, dedicated transducers is applied to convert sentence pairs from a bilingual corpus into generalized translation patterns. These patterns, together with the transducers are then used as a hierarchical translation memory for fully automatic translation. Results on the German—English VERBMOBIL corpus are given.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Part-of-speech tagging and partial parsing. In</title>
<date>1997</date>
<booktitle>Corpus-based Methods in Language and Speech Processing,</booktitle>
<pages>118--136</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, Boston, London.</location>
<contexts>
<context position="3780" citStr="Abney, 1997" startWordPosition="611" endWordPosition="612">pair is represented by a distinct line of nodes connected by edges labeled with the source sentence words and the target sentence emitted from the final state. This can be easily transformed into a tree transducer by building a prefix tree over the source sentences. In (Amengual et al., 2000) a method is given to propagate prefixes of the translations towards the root of such a tree transducer and to coalesce states to gain generalization power. We choose here a different route to generalization by using an approach similar to the one used for chunk parsing, where a cascade of FST is applied (Abney, 1997). Each transducer, defined by a set of regular-expression patterns, reads part of the input sentence and writes a stream of category labels, which form, together with the unanalyzed parts of the sentence, the input to the next transducer in the cascade. Our approach differs from the aforementioned chunk parsing in that an analyzed sequence of words is not replaced by the category label but is kept as a parallel option for transducers applied at a later stage. How this leads to the construction of a translation graph will be explained in Section 3. For translation, not only the analysis of the </context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>S. Abney. 1997. Part-of-speech tagging and partial parsing. In S. Young and G. Bloothooft, (Eds.), Corpus-based Methods in Language and Speech Processing, pages 118-136. Kluwer Academic Publishers, Dordrecht, Boston, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Amengual</author>
<author>J M Benedi</author>
<author>F Casacuberta</author>
<author>A Castano</author>
<author>A Castellanos</author>
<author>V M Jimenez</author>
<author>D Llorens</author>
<author>A Marzal</author>
<author>M Pastor</author>
<author>F Prat</author>
<author>E Vidal</author>
<author>J M Vilar</author>
</authors>
<title>The Eutrans-I speech translation system.</title>
<date>2000</date>
<journal>Machine Translation, Special Issue, forthcoming.</journal>
<contexts>
<context position="1272" citStr="Amengual et al., 2000" startWordPosition="183" endWordPosition="187">orpus are given. 1 Introduction Corpus based approaches to automatic translation come in a number of different flavors. In the simplest form, translations are stored and reused for the translation of new input. This approach, known as translation memory, example-based or case-based translation, can work on the word level as well as on structured examples as they are generated during analysis and generation in more grammar-based translation paradigms (Kitano, 1993; Brown, 1996). Finite state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation (Amengual et al., 2000), as have been bilingual stochastic grammars (Wu, 1996). Statistical approaches (Wang and Waibel, 1997; Och et al., 1999) also fall into the category of corpus based approaches. In this paper, a translation method is proposed which is based on the very same principles as the aforementioned approaches. One difference is, that not a fully automatic training of the translation model is performed. Rather, a number of special purpose transducers are hand-crafted and used then at two points. First, to convert the bilingual training corpus into a translation memory containing translation patterns rat</context>
<context position="3461" citStr="Amengual et al., 2000" startWordPosition="551" endWordPosition="554"> symbol and an output string, which may be the empty words of the two vocabularies. The final states can produce additional output. We want to construct transducers for automatic machine translation from a given bilingual corpus. In fact, a collection of sentence pairs can be viewed as a trivial transducer, where each sentence pair is represented by a distinct line of nodes connected by edges labeled with the source sentence words and the target sentence emitted from the final state. This can be easily transformed into a tree transducer by building a prefix tree over the source sentences. In (Amengual et al., 2000) a method is given to propagate prefixes of the translations towards the root of such a tree transducer and to coalesce states to gain generalization power. We choose here a different route to generalization by using an approach similar to the one used for chunk parsing, where a cascade of FST is applied (Abney, 1997). Each transducer, defined by a set of regular-expression patterns, reads part of the input sentence and writes a stream of category labels, which form, together with the unanalyzed parts of the sentence, the input to the next transducer in the cascade. Our approach differs from t</context>
</contexts>
<marker>Amengual, Benedi, Casacuberta, Castano, Castellanos, Jimenez, Llorens, Marzal, Pastor, Prat, Vidal, Vilar, 2000</marker>
<rawString>J. C. Amengual, J. M. Benedi, F. Casacuberta, A. Castano, A. Castellanos, V. M. Jimenez, D. Llorens, A. Marzal, M. Pastor, F. Prat, E. Vidal, and J. M. Vilar. 2000. The Eutrans-I speech translation system. Machine Translation, Special Issue, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Brown</author>
</authors>
<title>Example-based machine translation in the pangloss system.</title>
<date>1996</date>
<booktitle>In Proc. of COLING &apos;96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>169--174</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="1131" citStr="Brown, 1996" startWordPosition="165" endWordPosition="166">ucers are then used as a hierarchical translation memory for fully automatic translation. Results on the German—English VERBMOBIL corpus are given. 1 Introduction Corpus based approaches to automatic translation come in a number of different flavors. In the simplest form, translations are stored and reused for the translation of new input. This approach, known as translation memory, example-based or case-based translation, can work on the word level as well as on structured examples as they are generated during analysis and generation in more grammar-based translation paradigms (Kitano, 1993; Brown, 1996). Finite state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation (Amengual et al., 2000), as have been bilingual stochastic grammars (Wu, 1996). Statistical approaches (Wang and Waibel, 1997; Och et al., 1999) also fall into the category of corpus based approaches. In this paper, a translation method is proposed which is based on the very same principles as the aforementioned approaches. One difference is, that not a fully automatic training of the translation model is performed. Rather, a number of special purpose transducers are hand-craft</context>
</contexts>
<marker>Brown, 1996</marker>
<rawString>R. D. Brown. 1996. Example-based machine translation in the pangloss system. In Proc. of COLING &apos;96: The 16th Int. Conf. on Computational Linguistics, pages 169-174, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kitano</author>
</authors>
<title>A comprehensive and practical model of memory-based machine translation. In</title>
<date>1993</date>
<booktitle>Proc. of the 13th Int. Joint Conf. on Artificial Intelligence,</booktitle>
<pages>1276--1282</pages>
<editor>R. Bajcsy, editor,</editor>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="1117" citStr="Kitano, 1993" startWordPosition="163" endWordPosition="164">ith the transducers are then used as a hierarchical translation memory for fully automatic translation. Results on the German—English VERBMOBIL corpus are given. 1 Introduction Corpus based approaches to automatic translation come in a number of different flavors. In the simplest form, translations are stored and reused for the translation of new input. This approach, known as translation memory, example-based or case-based translation, can work on the word level as well as on structured examples as they are generated during analysis and generation in more grammar-based translation paradigms (Kitano, 1993; Brown, 1996). Finite state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation (Amengual et al., 2000), as have been bilingual stochastic grammars (Wu, 1996). Statistical approaches (Wang and Waibel, 1997; Och et al., 1999) also fall into the category of corpus based approaches. In this paper, a translation method is proposed which is based on the very same principles as the aforementioned approaches. One difference is, that not a fully automatic training of the translation model is performed. Rather, a number of special purpose transducers </context>
</contexts>
<marker>Kitano, 1993</marker>
<rawString>H. Kitano. 1993. A comprehensive and practical model of memory-based machine translation. In R. Bajcsy, editor, Proc. of the 13th Int. Joint Conf. on Artificial Intelligence, pages 1276-1282. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nief3en</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In 2nd Int. Conf. on Language Resources and Evaluation,</booktitle>
<pages>39--45</pages>
<location>Athens, Greece,</location>
<marker>Nief3en, Och, Leusch, Ney, 2000</marker>
<rawString>S. Nief3en, F. J. Och, G. Leusch, and H. Ney. 2000. An evaluation tool for machine translation: Fast evaluation for MT research. In 2nd Int. Conf. on Language Resources and Evaluation, pages 39-45, Athens, Greece, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>College Park, MD, USA,</location>
<contexts>
<context position="1393" citStr="Och et al., 1999" startWordPosition="202" endWordPosition="205"> simplest form, translations are stored and reused for the translation of new input. This approach, known as translation memory, example-based or case-based translation, can work on the word level as well as on structured examples as they are generated during analysis and generation in more grammar-based translation paradigms (Kitano, 1993; Brown, 1996). Finite state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation (Amengual et al., 2000), as have been bilingual stochastic grammars (Wu, 1996). Statistical approaches (Wang and Waibel, 1997; Och et al., 1999) also fall into the category of corpus based approaches. In this paper, a translation method is proposed which is based on the very same principles as the aforementioned approaches. One difference is, that not a fully automatic training of the translation model is performed. Rather, a number of special purpose transducers are hand-crafted and used then at two points. First, to convert the bilingual training corpus into a translation memory containing translation patterns rather than merely sentence pairs, and which is itself used as a transducer in the translation process. Second, when new sen</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F. J. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20-28, University of Maryland, College Park, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sawaf</author>
<author>K Schutz</author>
<author>H Ney</author>
</authors>
<title>On the use of grammar based language models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In 6th Int. Workshop on Parsing Technologies,</booktitle>
<pages>231--241</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="10785" citStr="Sawaf et al., 2000" startWordPosition="1799" endWordPosition="1802"> help to compensate for speech recognition errors in the case of speech translations. In that case the confusion matrix obtained by comparing the recognizer output for the training speech data with the transliteration can be used. 3.2 Using a Language Model The application of the transducers to a given source sentence yield a large number of target sentences. These are scored according to the cumulative scores of the applied translation patterns. As an independent and direct model of the likelihood of the target sentences a language model is applied. We use a wordbased trigram language model (Sawaf et al., 2000). The logarithm of the language model probabilities is added to the transducer scores C_PHRASE the fourth would be better -7.4 DATE Saturday and February -4,2 DATE the fourth -4.1 DATE Saturday -0.6 DATEDAY the fourth -4.0 DATE February -0.6 DAYWEEK Saturday -0.5 NUM_ORD fourth -2.0 ADV but -0.1 S_PHRASE are good -2.1 MONTH February -0.5 Samstag und Februar sind gut aber der vierte waere besser are evaluated by a human examiner using a scale ranging from 0 to 10. The average of these values is linearly transformed to give the sentence error rate in percent. 4.1 Effect of Grammar A simple trans</context>
</contexts>
<marker>Sawaf, Schutz, Ney, 2000</marker>
<rawString>H. Sawaf, K. Schutz, and H. Ney. 2000. On the use of grammar based language models for statistical machine translation. In 6th Int. Workshop on Parsing Technologies, pages 231-241, Trento, Italy, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Schiller</author>
<author>S Teufel</author>
<author>C Thielen</author>
</authors>
<title>Guidelines fiir das Tagging deutscher Textkorpora mit STTS.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>Universitat Stuttgart and Universitat Tubingen.</institution>
<note>http://www.sfs.npphil.uni-tuebingen.de/ Elwis/stts/stts.html.</note>
<marker>Schiller, Teufel, Thielen, 1995</marker>
<rawString>A. Schiller, S. Teufel, and C. Thielen. 1995. Guidelines fiir das Tagging deutscher Textkorpora mit STTS. Technical report, Universitat Stuttgart and Universitat Tubingen. http://www.sfs.npphil.uni-tuebingen.de/ Elwis/stts/stts.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
</authors>
<title>Construction of a hierarchical translation memory.</title>
<date>2000</date>
<booktitle>In Proc. of COLING 2000: The 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>1131--1135</pages>
<location>Saarbriicken, Germany,</location>
<marker>Vogel, Ney, 2000</marker>
<rawString>S. Vogel and H. Ney. 2000. Construction of a hierarchical translation memory. In Proc. of COLING 2000: The 18th Int. Conf. on Computational Linguistics, pages 1131-1135, Saarbriicken, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>Verbmobil: Foundations of Speech-to-Speech Translation.</title>
<date>2000</date>
<publisher>SpringerVerlag</publisher>
<location>Heidelberg.</location>
<marker>Wahlster, 2000</marker>
<rawString>Wahlster, W. (Ed.) 2000. Verbmobil: Foundations of Speech-to-Speech Translation. SpringerVerlag Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-Y Wang</author>
<author>A Waibel</author>
</authors>
<title>Decoding algorithm in statistical translation.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Conf. of the Association for Computational Linguistics,</booktitle>
<pages>366--372</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="1374" citStr="Wang and Waibel, 1997" startWordPosition="198" endWordPosition="201">fferent flavors. In the simplest form, translations are stored and reused for the translation of new input. This approach, known as translation memory, example-based or case-based translation, can work on the word level as well as on structured examples as they are generated during analysis and generation in more grammar-based translation paradigms (Kitano, 1993; Brown, 1996). Finite state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation (Amengual et al., 2000), as have been bilingual stochastic grammars (Wu, 1996). Statistical approaches (Wang and Waibel, 1997; Och et al., 1999) also fall into the category of corpus based approaches. In this paper, a translation method is proposed which is based on the very same principles as the aforementioned approaches. One difference is, that not a fully automatic training of the translation model is performed. Rather, a number of special purpose transducers are hand-crafted and used then at two points. First, to convert the bilingual training corpus into a translation memory containing translation patterns rather than merely sentence pairs, and which is itself used as a transducer in the translation process. S</context>
</contexts>
<marker>Wang, Waibel, 1997</marker>
<rawString>Y.-Y. Wang and A. Waibel. 1997. Decoding algorithm in statistical translation. In Proc. of the 35th Annual Conf. of the Association for Computational Linguistics, pages 366-372, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>A Polynomial-Time Algorithm for Statistical Machine Translation.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Conf. of the Association for Computational Linguistics,</booktitle>
<pages>152--158</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="1327" citStr="Wu, 1996" startWordPosition="194" endWordPosition="195"> translation come in a number of different flavors. In the simplest form, translations are stored and reused for the translation of new input. This approach, known as translation memory, example-based or case-based translation, can work on the word level as well as on structured examples as they are generated during analysis and generation in more grammar-based translation paradigms (Kitano, 1993; Brown, 1996). Finite state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation (Amengual et al., 2000), as have been bilingual stochastic grammars (Wu, 1996). Statistical approaches (Wang and Waibel, 1997; Och et al., 1999) also fall into the category of corpus based approaches. In this paper, a translation method is proposed which is based on the very same principles as the aforementioned approaches. One difference is, that not a fully automatic training of the translation model is performed. Rather, a number of special purpose transducers are hand-crafted and used then at two points. First, to convert the bilingual training corpus into a translation memory containing translation patterns rather than merely sentence pairs, and which is itself use</context>
<context position="5396" citStr="Wu, 1996" startWordPosition="901" endWordPosition="902">ummarize: each transducer is given as a set of quadruples of the form: [label # source pattern # target pattern # score]. At runtime these patterns are stored in a prefix tree with respect to the source patterns. We write the labels at first position as these translations patterns can be used in the reverse direction, i.e. from target language to source language. In section 2.4 this property is used to convert a bilingual corpus into a set of translation patterns which are formulated in terms of words and category labels. It also shows the structural identity to bilingual grammars as used in (Wu, 1996). 2.2 Construction of the Transducers Most of the transducers are customized towards the domain for which the translation system is developed. In the VERBMOBIL Corpus, which is used for the experiments, time and date expressions are very prominent. To translate those expressions, a small grammar has been developed and coded as finite state transducer. Actually, two transducers are used. On the first level, words are replaced by labels, like DAYOFWEEK = {Montag/Monday, Dienstag/Tuesday, ...I. On the second level, these labels together with labeled numbers (ordinal, cardinal, fractions) from the</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>D. Wu. 1996. A Polynomial-Time Algorithm for Statistical Machine Translation. In Proc. of the 34th Annual Conf. of the Association for Computational Linguistics, Santa Cruz, CA, pages 152-158, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>