<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001145">
<title confidence="0.996638">
The Modulation of Cooperation and Emotion in Dialogue:
The REC Corpus
</title>
<author confidence="0.918713">
Federica Cavicchio
</author>
<affiliation confidence="0.7100205">
Mind and Brain Center/ Corso Bettini 31,
38068 Rovereto (Tn) Italy
</affiliation>
<email confidence="0.983634">
federica.cavicchio@unitn.it
</email>
<sectionHeader confidence="0.994744" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997158607142857">
In this paper we describe the Rovereto Emotive
Corpus (REC) which we collected to investigate
the relationship between emotion and coopera-
tion in dialogue tasks. It is an area where still
many unsolved questions are present. *ne of the
main open issues is the annotation of the so-
called &amp;quot;blended&amp;quot; emotions and their recognition.
Usually, there is a low agreement among raters
in annotating emotions and, surprisingly, emo-
tion recognition is higher in a condition of mod-
ality deprivation (i. e. only acoustic or only visu-
al modality vs. bimodal display of emotion). Be-
cause of these previous results, we collected a
corpus in which &amp;quot;emotive&amp;quot; tokens are pointed
out during the recordings by psychophysiologi-
cal indexes (ElectroCardioGram, and Galvanic
Skin Conductance). From the output values of
these indexes a general recognition of each emo-
tion arousal is allowed. After this selection we
will annotate emotive interactions with our mul-
timodal annotation scheme, performing a kappa
statistic on annotation results to validate our
coding scheme. In the near future, a logistic re-
gression on annotated data will be performed to
find out correlations between cooperation and
negative emotions. A final step will be an fMRI
experiment on emotion recognition of blended
emotions from face displays.
</bodyText>
<sectionHeader confidence="0.998096" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929456140351">
In the last years many multimodal corpora have
been collected. These corpora have been recorded
in several languages and have being elicited with
different methodologies: acted (such as for emo-
tion corpora, see for example Goeleven, 2008),
task oriented corpora, multiparty dialogs, corpora
elicited with scripts or storytelling and ecological
corpora. Among the goals of collection and analy-
sis of corpora there is shading light on crucial as-
pects of speech production. Some of the main re-
search questions are how language and gesture
correlate with each other (Kipp et al., 2006) and
how emotion expression modifies speech (Magno
Caldognetto et al., 2004) and gesture (Poggi,
2007). Moreover, great efforts have been done to
analyze multimodal aspects of irony, persuasion
or motivation.
Multimodal coding schemes are mainly focused
on dialogue acts, topic segmentation and the so
called &amp;quot;emotional area&amp;quot;. The collection of mul-
timodal data has raised the question of coding
scheme reliability. The aim of testing coding
scheme reliability is to assess whether a scheme
is able to capture observable reality and allows
some generalizations. From mid Nineties, the
kappa statistic has begun to be applied to vali-
date coding scheme reliability. Basically, the
kappa statistic is a statistical method to assess
agreement among a group of observers. Kappa
has been used to validate some multimodal cod-
ing schemes too. However, up to now many mul-
timodal coding schemes have a very low kappa
score (Carletta, 2007, Douglas-Cowie et al.,
2005; Pianesi et al., 2005, Reidsma et al., 2008).
This could be due to the nature of multimodal
data. In fact, annotation of mental and emotional
states of mind is a very demanding task. The low
annotation agreement which affects multimodal
corpora validation could also be due to the nature
of the kappa statistics. In fact, the assumption
underlining the use of kappa as reliability meas-
ure is that coding scheme categories are mutually
exclusive and equally distinct one another. This
is clearly difficult to be obtained in multimodal
corpora annotation, as communication channels
(i.e. voice, face movements, gestures and post-
ure) are deeply interconnected one another.
To overcome these limits we are collecting a
new corpus, Rovereto Emotive Corpus (REC), a
task oriented corpus with psychophysiological
data registered and aligned with audiovisual da-
ta. In our opinion this corpus will allow to clear-
ly identify emotions and, as a result, having a
clearer idea of facial expression of emotions in
dialogue. In fact, REC is created to shade light
on the relationship between cooperation and
emotions in dialogues. This resource is the first
</bodyText>
<page confidence="0.698828">
81
</page>
<note confidence="0.8938365">
Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 81–87,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.932676">
up to now with audiovisual and psychophysio-
logical data recorded together.
</bodyText>
<sectionHeader confidence="0.974458" genericHeader="method">
2 The REC Corpus
</sectionHeader>
<bodyText confidence="0.999106838709677">
REC (Rovereto Emotive Corpus) is an audiovi-
sual and psychophysiological corpus of dialo-
gues elicited with a modified Map Task. The
Map Task is a cooperative task involving two
participants. It was used for the first time by the
HCRC group at Edinburg University (Anderson
et al., 1991). In this task two speakers sit oppo-
site one another and each of them has a map.
They cannot see each other&apos;s map because the
they are separated by a short barrier. *ne speak-
er, designated the Instruction Giver, has a route
marked on her map; the other speaker, the In-
struction Follower, has no route. The speakers
are told that their goal is to reproduce the In-
struction Giver&apos;s route on the Instruction Follow-
er&apos;s map. To the speakers are told explicitly that
the maps are not identical at the beginning of the
dialogue session. However, it is up to them to
discover how the two maps differ.
*ur map task is modified with respect to the
original one. In our Map Task the two participants
are sitting one in front of the other and are
separated by a short barrier or a full screen. They
both have a map with some objects. Some of them
are in the same position and with the same name,
but most of them are in different positions or have
names that sound similar to each other (e. g. Maso
Michelini vs. Maso Nichelini, see Fig. 1). *ne
participant (the giver) must drive the other
participant (the follower) from a starting point
(the bus station) to the finish (the Castle).
</bodyText>
<figureCaption confidence="0.997514">
Figure 1: Maps used in the recording of REC corpus
</figureCaption>
<bodyText confidence="0.999961767857143">
Giver and follower are both native Italian speak-
ers. In the instructions it was told them that they
will have no more than 20 minutes to accomplish
the task. The interaction has two conditions:
screen and no screen. In screen condition a barrier
was present between the two speakers. In no
screen condition a short barrier, as in the original
map task, was placed allowing giver and follower
to see each other&apos;s face. With these two condi-
tions we want to test whether seeing the speakers
face during interactions influences facial emotion
display and cooperation (see Kendon, 1967; Ar-
gyle and Cook 1976; for the relationship between
gaze/no gaze and facial displays; for the influence
of gaze on cooperation and coordination see
Brennan et al., 2008). A further condition, emo-
tion elicitation, was added. In &amp;quot;emotion&amp;quot; condi-
tion the follower or the giver can alternatively be
a confederate, with the aim of getting the other
participant angry. In this condition the psycho-
physiological state of the confederate is not rec-
orded. In fact, as it is an acted behavior, it is not
interesting for research purpose. All the partici-
pants had given informed consent and the experi-
mental protocol has been approved by the Human
Research Ethics Committee of Trento University.
REC is by now made up of 17 dyadic interac-
tions, 9 with confederate, for a total of 204 min-
utes of audiovisual and psychophysiological re-
cordings (electrocardiogram and derived heart
rate value, and skin conductance). *ur goal is
reaching 12 recordings in the confederate condi-
tion. During each dialogue, the psychophysiologi-
cal state of non-confederate giver or follower is
recorded and synchronized with video and audio
recordings. So far, REC corpus is the only multi-
modal corpus which has psychophysiological data
to assess emotive states.
The psychophysiological state of each partici-
pant has been recorded with a BI*PAC MP150
system. In particular, Electrocardiogram (ECG)
was recorded by Ag AgC1 surface electrodes
fixed on participant&apos;s wrists, low pass filter 100
Hz, at a 200 samples/second rate. Heart Rate
(HR) has been automatic calculated as number of
heart beats per minute. Galvanic Skin Conduc-
tance (SK) was recorded with Ag AgC1 elec-
trodes attached to the palmar surface of the
second and third fingers of the non dominant
hand, and recorded at a rate of
200samples/second. Artefacts due to hand move-
ments have been removed with proper algorithms.
Audiovisual interactions are recorded with 2 Ca-
non Digital Cameras and 2 free field Sennheiser
half-cardioid microphones with permanently pola-
rized condenser, placed in front of each speaker
</bodyText>
<page confidence="0.658841">
82
</page>
<bodyText confidence="0.962975285714286">
The recording procedure of REC is the follow-
ing. Before starting the task, we record baseline
condition that is to say we record participants&apos;
psychophysiological outputs for 5 minutes with-
out challenging them. Then the task started and
we recorded the psychophysiological outputs dur-
ing the interaction which we called task condition.
Then the confederate started challenging the
speaker with the aim of getting him/her angry. To
do so, the confederate at minutes 4, 9 and 13 of
the interaction plays a script (negative emotion
elicitation in giver; Anderson et al., 2005):
·You driving me in the wrong direction, try to be
more accurate!&amp;quot;;
</bodyText>
<listItem confidence="0.852915666666667">
·&amp;quot;It&apos;s still wrong, this can&apos;t be your best, try
harder! So, again, from where you stop &amp;quot;I-
;
e
• &amp;quot;You&apos;re obviously not good enough in giving
instruction&amp;quot;.
</listItem>
<bodyText confidence="0.999755416666667">
In Fig. 2 we show the results of a 1x5 AN*VA
executed in confederate condition. Heart rate
(HR) is confronted over the five times of interest
(baseline, task, after 4 minutes, after 9 minutes,
after 13 minutes). The times of interest are base-
line, task, and after 4, 9 and 13 minutes, that is to
say just after emotion elicitation with the script.
We find that HR is significantly different in the
five conditions, which means that the procedure
to elicit emotions is incremental and allows
recognition of different psychophysiological
states, which in turns are linked to emotive states.
Mean HR values are in line with the ones showed
by Anderson et al. (2005). Moreover, from the
inspection of skin conductance values (Fig. 3)
there is a linear increase of the number of peaks
of conductance over time. This can be due to two
factors: emotion elicitation but also an increasing
of task difficulty leading to higher stress and
therefore to an increasing number of skin
conductance peaks.
As Cacioppo et al. (2000) pointed out, it is not
possible to assess the emotion typology from
psychophysiological data alone. In fact, HR and
skin conductance are signals of arousal which in
turns can be due both to high arousal emotions
such as happiness or anger. Therefore, we asked
participants after the conclusion of the task to
report on a 8 points rank scale the valence of the
emotions felt towards the interlocutor during the
task (from extremely positive to extremely
negative). *n 10 participants, 50% of them rated
the experience as quite negative, 30% rated the
experience as almost negative, 10% of
participants rated it as negative and 10% as
neutral.
</bodyText>
<figureCaption confidence="0.987639">
Figure 2: 1x5 AN*VA on heart rate (HR) over time in
emotion elicitation condition in 9 partecipants
</figureCaption>
<bodyText confidence="0.997942">
Participants who have reported a neutral or
positive experience were discarded from the
corpus.
</bodyText>
<figureCaption confidence="0.97698">
Figure 3: Number of skin conductance positive peaks
over time in emotion elicitation condition in 9 parte ci-
</figureCaption>
<bodyText confidence="0.837685">
pants
</bodyText>
<sectionHeader confidence="0.696691" genericHeader="method">
3 Annotation Method and Coding Scheme
</sectionHeader>
<bodyText confidence="0.999622333333333">
The emotion annotation coding scheme used to
analyze our map task is quite far from the emotion
annotation schemes proposed in Computational
Linguistic literature. Craggs and Woods (2005)
proposed to annotate emotions with a scheme
where emotions are expressed at different blend-
ing levels (i. e. blending of different emotion and
emotive levels). In Craggs and Woods opinions&apos;
annotators must label the given emotion with a
main emotive term (e. g. anger, sadness, joy etc.)
correcting the emotional state with a score rang-
ing from 1 (low) to 5 (very high). Martin et al.
(2006) used a three steps rank scale of emotion
valence (positive, neutral and negative) to anno-
tate their corpus recorded from TV interviews.
</bodyText>
<figure confidence="0.98814740625">
�������� ���������
������
������
������
�������
�������
����
�
�
�
�
�
����
�����
�����
����
����
����
�������
�������
������
������
������
�������
�������
������
������
������
��� ���������� ��������
��� ����� ����� ����� ����� �����
PeaksMme
83
</figure>
<bodyText confidence="0.994709545454546">
But both these methods had quite poor results in
terms of annotation agreement among coders.
Several studies on emotions have shown how
emotional words and their connected concepts
influence emotion judgments and their labeling
(for a review, see Feldman Barrett et al., 2007).
Thus, labeling an emotive display (e. g. a voice or
a face) with a single emotive term could be not
the best solution to recognize an emotion. Moreo-
ver researchers on emotion recognition from face
displays find that some emotions as anger or fear
are discriminated only by mouth or eyes configu-
rations. Face seems to be evolved to transmit or-
thogonal signals, with a lower correlation each
other. Then, these signals are deconstructed by the
&amp;quot;human filtering functions&amp;quot;, i. e. the brain, as op-
timized inputs (Smith et al., 2005). The Facial
Action Units (FACS, Ekman and Friesen, 1978) is
a good scheme to annotate face expressions start-
ing from movement of muscular units, called ac-
tion units. Even if accurate, it is a little problemat-
ic to annotate facial expression, especially the
mouth ones, when the subject to be annotated is
speaking, as the muscular movements for speech
production overlaps with the emotional configura-
tion.
*n the basis of such findings, an ongoing de-
bate is whether the perception of a face and, spe-
cifically, of a face displaying emotions, is based
on holistic perception or perception of parts. Al-
though many efforts are ongoing in neuroscience
to determine the basis of emotion perception and
decoding, little is still known on how brains and
computer might learn part of an object such as a
face. Most of the research in this field is based on
PCA-alike algorithms which learn holistic repre-
sentations. *n the contrary other methods such as
non Negative Matrix Factorization are based on
only positive constrains leading to part based ad-
ditive representations. Keeping this in mind, we
decide not to label emotions directly but to
attribute valence and activation to nonverbal sig-
nals, &amp;quot;deconstructing&amp;quot; them in simpler elements.
These elements have implicit emotive dimen-
sions, as for example mouth shape. Thus, in our
coding scheme a smile would be annotate as &amp;quot;)&amp;quot;
and a large smile as &amp;quot;+)&amp;quot;. The latter means a
higher valence and arousal than the previous sig-
nal, as when the speaker is laughing.
In the following, we describe the modalities
and the annotation features of our multimodal
annotation scheme. As an example, the analysis of
emotive labial movements implemented in our
annotation scheme is based on a little amount of
signs similar to emoticons. We sign two levels
of activation using the plus and minus signs. So,
annotation values for mouth shape are:
·o open lips when the mouth is open;
·- closed lips when the mouth is closed;
· ) corners up e.g. when smiling; +) open
smile;
·( corners down; +( corners very down
·1cornerup for asymmetric smile;
·O protruded, when the lips are rounded.
Similar signals are used to annotate eyebrows
shape.
</bodyText>
<subsectionHeader confidence="0.999669">
3.1 Cooperation Analysis
</subsectionHeader>
<bodyText confidence="0.908318766666667">
The approach we have used to analyze coopera-
tion in dialogue task is mainly based on Bethan
Davies model (Bethan Davies, 2006). The basic
coded unit is the &amp;quot;move&amp;quot;, which means individual
linguistic choices to successfully fulfill Map Task.
The idea of evaluating utterance choices in rela-
tion to task success can be traced back to Ander-
son and Boyle (1994) who linked utterance choic-
es to the accuracy of the route performed on the
map. Bethan Davies extended the meaning of
&amp;quot;move&amp;quot; to the goal evaluation, from a narrow set
of indicators to a sort of data-driven set. In partic-
ular, Bethan Davies stressed some useful points
for the computation of collaboration between two
communicative partners:
·social needs of dialogue: there is a mini-
mum &amp;quot;effort&amp;quot; needed to keep the conversa-
tion going. It includes minimal answers like
&amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot; and feedbacks. These brief
utterances are classified by Bethan Davies
(following Traum, 1994) as low effort, as
they do not require much planning to the
overall dialogue and to the joint task;
·responsibility of supplying the needs of the
communication partner: to keep an utter-
ance going, one of the speakers can provide
follow-ups which take more consideration
of the partner&apos;s intentions and goals in the
task performance. This involves longer ut-
terances, and of course a larger effort;
</bodyText>
<listItem confidence="0.855329">
·responsibility of maintaining a known
track of communication or starting a new
one: there is an effort in considering the ac-
tions of a speaker within the context of a
particular goal: that is, they mainly deal
with situations where a speaker is reacting
to the instruction or question offered by the
other participant, rather than moving the
discourse on another goal. In fact the latter
</listItem>
<page confidence="0.848512">
84
</page>
<bodyText confidence="0.989058118644068">
is perceived as a great effort as it involves
reasoning about the task as a whole, beside
planning and producing a particular utter-
ance.
Following Traum (1994), speakers tend to engage
in lower effort behaviors than higher ones. Thus,
if you do not answer to a question, the
conversation will end, but you can choose
whether or not to query an instruction or offer a
suggestion about what to do next. This is reflected
in a weighting system where behaviors account
for the effort invested and provides a basis for the
empirical testing of dialogue principles. The use
of this system provides a positive and negative
score for each dialogue move. We slightly
simplified the Bethan Davies&apos; weighting system
and propose a system giving positive and negative
weights in an ordinal scale from +2 to -2. We also
attribute a weight of 0 for actions which are in the
area of &amp;quot;minimum social needs&amp;quot; of dialogue. In
Table 1 we report some of the dialogue moves,
called cooperation type, and the corresponding
cooperation weighting level. There is also a
description of different type of moves in terms of
Grice&apos;s conversational rules breaking or
following. Due to the nature of the map task,
where giver and a follower have different
dialogue roles, we have two slightly different
versions of the cooperation annotation scheme.
For example &amp;quot;giving instruction&amp;quot; is present only
when annotating the giver cooperation. *n the
other hand &amp;quot;feedback&amp;quot; is present in both
annotation schemes. *ther communicative
collaboration indexes we codify in our coding
scheme are the presence or absence of eye contact
through gaze direction (to the interlocutor, to the
map, unfocused), even in full screen condition,
where the two speakers can&apos;t see each other.
Dialogue turns management (turn giving, turn
offering, turn taking, turn yielding, turn
concluding, and feedback) has been annotated as
well. Video clips have been orthographically
transcribed. To do so, we adopted a subset of the
conventions applied to the transcription of the
speech corpus of the LUNA project corpus
annotation (see Rodriguez et al., 2007).
3. 2 Coding Procedure and Kappa Scores
Up to now we have annotated 9 emotive tokens of
an average length of 100 seconds each. They have
been annotated with the coding scheme previous-
ly described by 6 annotators. *ur coding scheme
has been implemented into ANVIL software
(Kipp, 2001). A Fleiss&apos; kappa statistic (Fleiss,
1971) has been performed on the annotations. We
choose Fleiss&apos; kappa as it is the suitable statistics
when chance agreement is calculated on more
than two coders. In this case the agreement is ex-
pected on the basis of a single distribution reflect-
ing the combined judgments of all coders.
</bodyText>
<table confidence="0.984666904761905">
Cooperation Cooperation type
level
-2 No response to answer: breaks the maxims of quality,
quantity and relevance
-2 No information add when required: breaks the maxims of
quality, quantity and manner
-2 No turn giving, no check: breaks the maxims of quality,
quantity and relevance
-1 Inappropriate reply (no giving info7: breaks the maxims of
quantity and relevance
0 Giving instruction: cooperation baseline, task demands
1 Question answering y/n: applies the maxims of quality and
relevance
1 Repeating instruction: applies the maxims of quantity and
manner
2 Question answering y/n + adding info: applies the maxims
of quantity, quality and relevance
2 Checking the other understands (ra sei? Capkoi7: applies
the m of quantity, quality and manner
2 Spontaneous info/description adding: applies the maxims of
quantity, quality and manner
</table>
<tableCaption confidence="0.9323875">
Table 1: Computing cooperation in our coding scheme
(from Bethan Davies, 2006 adapted)
</tableCaption>
<bodyText confidence="0.999863142857143">
Thus, expected agreement is measured as the
overall proportion of items assigned to a category
k by all coders n.
Cooperation annotation for giver has a Fleiss&apos;
kappa score of 0.835 (p&lt;0.001), while for follow-
er cooperation annotation is 0.829 (p&lt;0.001).
Turn management has a Fleiss kappa score of
0.784 (p&lt;0.001). As regard gaze, Fleiss kappa
score is 0.788 (p&lt;0.001). Mouth shape annotation
has a Fleiss kappa score of 0.816 (p&lt;0.001) and
eyebrows shape annotation has a Fleiss kappa of
0.855 (p&lt;0.001). In the last years a large debate
on the interpretation of kappa scores has wide-
spread. There is a general lack of consensus on
how to interpret those values. Some authors (All-
wood et al., 2006) consider as reliable for multi-
modal annotation kappa values between 0.67 and
0.8. *ther authors accept as reliable only scoring
rates over 0.8 (Krippendorff, 2004) to allow some
generalizations. What is clear is that it seems in-
appropriate to propose a general cut off point,
especially for multimodal annotation where very
little literature on kappa agreement has been re-
ported. In this field it seems more necessary that
researches report clearly the method they apply
(e. g. the number of coders, if they code indepen-
dently or not, if their coding relies only manual-
ly).
</bodyText>
<page confidence="0.95728">
85
</page>
<bodyText confidence="0.99997806060606">
*ur kappa scores are very high if compared
with other multimodal annotation results. This is
because we analyze cooperation and emotion with
an unambiguous coding scheme. In particular, we
do not refer to emotive terms directly. In fact
every annotator has his/her own representation of
a particular emotion, which could be pretty differ-
ent from the one of another coder. This represen-
tation will represent a problem especially for an-
notation of blended emotions, which are ambi-
guous and mixed by nature. As some authors have
argued (Colletta et al., 2008) annotation of mental
and emotional states is a very demanding task.
The analysis of non verbal features requires a dif-
ferent approach if compared with other linguistics
tasks as multimodal communication is multi chan-
nel (e.g. audiovisual) and has multiple semantic
levels (e.g. a facial expression can deeply modify
the sense of a sentence, such as in humor or iro-
ny).
The final goal of this research is performing a
logistic regression on cooperation and emotion
display. We will also investigate speakers&apos; role
(giver or follower) and screen/no screen condi-
tions role with respect to cooperation. *ur pre-
dictions are that in case of full screen condition
(i. e. the two speakers can&apos;t see each other) the
cooperation will be lower with respect to short
screen condition (i. e. the two speakers can see
each other&apos;s face) while emotion display will be
wider and more intense for full screen condition
with respect to short barrier condition. No predic-
tions are made on the speaker role.
</bodyText>
<sectionHeader confidence="0.99829" genericHeader="conclusions">
4 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999957136363636">
Cooperative behavior and its relationship with
emotions is a topic of great interest in the field of
dialogue annotation. Usually emotions achieve a
low agreement among raters (see Douglas-Cowie
et al., 2005) and surprisingly emotion recognition
is higher in a condition of modality deprivation
(only acoustic or only visual vs. bimodal).
Neuroscience research on emotion shows that
emotion recognition is a process performed firstly
by sight, but the awareness of the emotion ex-
pressed is mediated by the prefrontal cortex.
Moreover a predefined set of emotion labels can
influence the perception of facial expression.
Therefore we decide to deconstruct each signal
without attributing directly an emotive label. We
consider promising the implementation in compu-
tational coding schemes of neuroscience evi-
dences on transmitting and decoding of emotions.
Further researches will implement an experiment
on coders&apos; brain activation of to understand if
emotion recognition from face is a whole or a part
based process.
</bodyText>
<sectionHeader confidence="0.995939" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996087046153846">
Allwood J., Cerrato L., Jokinen K., Navarretta C., and
Paggio P. 2006. A Coding Scheme for the Annota-
tion of Feedback, Turn Management and Sequenc-
ing Phenomena. In Martin, J.-C., Kuhnlein, P.,
Paggio, P., Stiefelhagen, R., Pianesi, F. (Eds.) Mul-
timodal Corpora: From Multimodal Behavior Theo-
ries to Usable Models: 38-42.
Anderson A., Bader M., Bard E., Boyle E., Doherty G.
M., Garrod S., Isard S., Kowtko J., McAllister J.,
Miller J., Sotillo C., Thompson H. S. and Weinert
R. 1991. The HCRC Map Task Corpus. Language
and Speech, 34:351-366
Anderson A. H., and Boyle E. A. 1994. Forms of in-
troduction in dialogues: Their discourse contexts
and communicative consequences. Language and
Cognitive Process , 9(1):101 - 122
Anderson J. C., Linden W., and Habra M. E. 2005. The
importance of examining blood pressure reactivity
and recovery in anger provocation research. Interna-
tional Journal of Psychophysiology 57(3): 159-163
Argyle M. and Cook M. 1976 Gaze and mutual gaze,
Cambridge: Cambridge University Press
Bethan Davies L. 2006. Testing Dialogue Principles in
Task-*riented Dialogues: An Exploration of Coop-
eration, Collaboration, Effort and Risk. In Universi-
ty ofLeeds papers
Brennan S. E., Chen X., Dickinson C. A., Neider M.
A. and Zelinsky J. C. 2008. Coordinating cognition:
The costs and benefits of shared gaze during colla-
borative search. Cognition 106(3): 1465-1477
Ekman P. and Friesen WV. 1978. FACS Facial Action
Codind Scheme. A technique for the measurement of
facial action, Palo Alto, CA: Consulting Press
Carletta, J. 2007. Unleashing the killer corpus: expe-
riences in creating the multi-everything AMI Meet-
ing Corpus, Language Resources and Evaluation,
41: 181-190
Colletta, J.-M., Kunene, R., Venouil, and A. Tcherkas-
sof, A. 2008. Double Level Analysis of the Multi-
modal Expressions of Emotions in Human-machine
Interaction. In Martin, J.-C., Patrizia, P., Kipp, M.,
Heylen, D., (Eds.) Multimodal Corpora: From Mod-
els of Natural Interaction to Systems and Applica-
tions, 5-11
Craggs R., and Wood M. 2004. A Categorical Annota-
tion Scheme for Emotion in the Linguistic Content
of Dialogue. In Affective Dialogue Systems, Elsevi-
er, 89-100
86
Douglas-Cowie E., Devillers L., Martin J.-C., Cowi R.,
Savvidou S., Abrilian S., and Cox C. 2005. Multi-
modal Databases of Everyday Emotion: Facing up
to Complexity. In 9th European Conference on
Speech Communication and Technology (Inters-
peech&apos;2005) Lisbon, Portugal, September 4-8, 813-
816
Feldman Barrett L., Lindquist K. A., and Gendron M.
2007. Language as Context for the Perception of
Emotion. Trends in Cognitive Sciences, 11(8): 327-
332.
Fleiss J. L. 1971. Measuring Nominal Scale Agree-
ment among Multiple Coders Psychological Bulletin
11(4): 23-34.
Goeleven E., De Raedt R., Leyman L., and Ver-
schuere, B. 2008. The Karolinska Directed Emo-
tional Faces: A validation study, Cognition and
Emotion, 22:1094 -1118
Kendon A. 1967. Some Functions of Gaze Directions
in Social Interaction, Acta Psychologica 26(1):1-47
Kipp M., Neff M., and Albrecht I. 2006. An Annota-
tion Scheme for Conversational Gestures: How to
economically capture timing and form. In Martin,
J.-C., Kizhnlein, P., Paggio, P., Stiefelhagen, R.,
Pianesi, F. (Eds.) Multimodal Corpora: From Mul-
timodal Behavior Theories to Usable Models, 24-28
Kipp M. 2001. ANVIL - A Generic Annotation Tool
for Multimodal Dialogue. In Eurospeech 2001
Scandinavia 7th European Conference on Speech
Communication and Technology
Krippendorff K. 2004. Reliability in content analysis:
Some common misconceptions and recommenda-
tions. Human Communication Research, 30:411-
433
Magno Caldognetto E., Poggi I., Cosi P., Cavicchio F.
and Merola G. 2004. Multimodal Score: an Anvil
Based Annotation Scheme for Multimodal Audio-
Video Analysis. In Martin, J.-C., *s, E.D.,
Kizhnlein, P., Boves, L., Paggio, P., Catizone, R.
(eds.) Proceedings of Workshop Multimodal Corpo-
ra: Models Of Human Behavior For The Specifica-
tion And Evaluation Of Multimodal Input And Out-
put Interfaces. 29-33
Martin J.-C., Caridakis G., Devillers L., Karpouzis K.
and Abrilian S. 2006. Manual Annotation and Au-
tomatic Image Processing of Multimodal Emotional
Behaviors: Validating the Annotation of TV Inter-
views. In Fifth international conference on Lan-
guage Resources and Evaluation (LREC 2006), Ge-
noa, Italy
Pianesi F., Leonardi C., and Zancanaro M. 2006. Mul-
timodal Annotated Corpora of Consensus Decision
Making Meetings. In Martin, J.-C., Kizhnlein, P.,
Paggio, P., Stiefelhagen, R., Pianesi, F. (Eds.) Mul-
timodal Corpora: From Multimodal Behavior Theo-
ries to Usable Models, 6--9
Poggi I., 2007. Mind, hands, face and body. A goal and
belief view of multimodal communication, Berlin:
Weidler Buchverlag
Reidsma D. Heylen D., and *p den Akker R. 2008. *n
the Contextual Analysis of Agreement Scores. In
Martin, J.-C., Patrizia, P., Kipp, M., Heylen, D.,
(Eds.) Multimodal Corpora: From Models of Natu-
ral Interaction to Systems and Applications, 52--55
Rodriguez K., Stefan K. J., Dipper S., GStze M., Poe-
sio M., Riccardi G., and Raymond C., and Wis-
niewska J., 2007. Standoff Coordination for Multi-
Tool Annotation in a Dialogue Corpus. In Proceed-
ings of the Linguistic Annotation Workshop at the
ACL&apos;07 (LAW-07), Prague, Czech Republic.
Smith M. L., Cottrell G. W., Gosselin F., and Schyns
P. G. 2005. Transmitting and Decoding Facial Ex-
pressions. Psychological Science 16(3):184-189
Tassinary L. G. and Cacioppo J. T. 2000. The skeleto-
motor system: Surface electromyography. In LG
Tassinary, GG Berntson, JT Cacioppo (eds) Hand-
book of psychophysiology, New York: Cambridge
University Press, 263-299
Traum D. R. 1994. A Computational Theory of
Grounding in Natural Language Conversation, PhD
Dissertation. urresearch.rochester.edu
</reference>
<page confidence="0.962929">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.414155">
<title confidence="0.999623">The Modulation of Cooperation and Emotion in Dialogue: The REC Corpus</title>
<author confidence="0.7107265">Federica Cavicchio Mind</author>
<author confidence="0.7107265">Brain Center Corso Bettini</author>
<address confidence="0.992153">38068 Rovereto (Tn) Italy</address>
<email confidence="0.999437">federica.cavicchio@unitn.it</email>
<abstract confidence="0.998385620689655">In this paper we describe the Rovereto Emotive Corpus (REC) which we collected to investigate the relationship between emotion and cooperation in dialogue tasks. It is an area where still many unsolved questions are present. *ne of the main open issues is the annotation of the socalled &amp;quot;blended&amp;quot; emotions and their recognition. Usually, there is a low agreement among raters in annotating emotions and, surprisingly, emotion recognition is higher in a condition of modality deprivation (i. e. only acoustic or only visual modality vs. bimodal display of emotion). Because of these previous results, we collected a corpus in which &amp;quot;emotive&amp;quot; tokens are pointed out during the recordings by psychophysiological indexes (ElectroCardioGram, and Galvanic Skin Conductance). From the output values of these indexes a general recognition of each emotion arousal is allowed. After this selection we will annotate emotive interactions with our multimodal annotation scheme, performing a kappa statistic on annotation results to validate our coding scheme. In the near future, a logistic regression on annotated data will be performed to find out correlations between cooperation and negative emotions. A final step will be an fMRI experiment on emotion recognition of blended emotions from face displays.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allwood</author>
<author>L Cerrato</author>
<author>K Jokinen</author>
<author>C Navarretta</author>
<author>P Paggio</author>
</authors>
<title>A Coding Scheme for the Annotation of Feedback, Turn Management and Sequencing Phenomena. In</title>
<date>2006</date>
<pages>38--42</pages>
<contexts>
<context position="21362" citStr="Allwood et al., 2006" startWordPosition="3509" endWordPosition="3513">d to a category k by all coders n. Cooperation annotation for giver has a Fleiss&apos; kappa score of 0.835 (p&lt;0.001), while for follower cooperation annotation is 0.829 (p&lt;0.001). Turn management has a Fleiss kappa score of 0.784 (p&lt;0.001). As regard gaze, Fleiss kappa score is 0.788 (p&lt;0.001). Mouth shape annotation has a Fleiss kappa score of 0.816 (p&lt;0.001) and eyebrows shape annotation has a Fleiss kappa of 0.855 (p&lt;0.001). In the last years a large debate on the interpretation of kappa scores has widespread. There is a general lack of consensus on how to interpret those values. Some authors (Allwood et al., 2006) consider as reliable for multimodal annotation kappa values between 0.67 and 0.8. *ther authors accept as reliable only scoring rates over 0.8 (Krippendorff, 2004) to allow some generalizations. What is clear is that it seems inappropriate to propose a general cut off point, especially for multimodal annotation where very little literature on kappa agreement has been reported. In this field it seems more necessary that researches report clearly the method they apply (e. g. the number of coders, if they code independently or not, if their coding relies only manually). 85 *ur kappa scores are v</context>
</contexts>
<marker>Allwood, Cerrato, Jokinen, Navarretta, Paggio, 2006</marker>
<rawString>Allwood J., Cerrato L., Jokinen K., Navarretta C., and Paggio P. 2006. A Coding Scheme for the Annotation of Feedback, Turn Management and Sequencing Phenomena. In Martin, J.-C., Kuhnlein, P., Paggio, P., Stiefelhagen, R., Pianesi, F. (Eds.) Multimodal Corpora: From Multimodal Behavior Theories to Usable Models: 38-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Anderson</author>
<author>M Bader</author>
<author>E Bard</author>
<author>E Boyle</author>
<author>G M Doherty</author>
<author>S Garrod</author>
<author>S Isard</author>
<author>J Kowtko</author>
<author>J McAllister</author>
<author>J Miller</author>
<author>C Sotillo</author>
<author>H S Thompson</author>
<author>R Weinert</author>
</authors>
<title>The HCRC Map Task Corpus. Language and Speech,</title>
<date>1991</date>
<pages>34--351</pages>
<contexts>
<context position="4694" citStr="Anderson et al., 1991" startWordPosition="737" endWordPosition="740">ed to shade light on the relationship between cooperation and emotions in dialogues. This resource is the first 81 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 81–87, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP up to now with audiovisual and psychophysiological data recorded together. 2 The REC Corpus REC (Rovereto Emotive Corpus) is an audiovisual and psychophysiological corpus of dialogues elicited with a modified Map Task. The Map Task is a cooperative task involving two participants. It was used for the first time by the HCRC group at Edinburg University (Anderson et al., 1991). In this task two speakers sit opposite one another and each of them has a map. They cannot see each other&apos;s map because the they are separated by a short barrier. *ne speaker, designated the Instruction Giver, has a route marked on her map; the other speaker, the Instruction Follower, has no route. The speakers are told that their goal is to reproduce the Instruction Giver&apos;s route on the Instruction Follower&apos;s map. To the speakers are told explicitly that the maps are not identical at the beginning of the dialogue session. However, it is up to them to discover how the two maps differ. *ur ma</context>
</contexts>
<marker>Anderson, Bader, Bard, Boyle, Doherty, Garrod, Isard, Kowtko, McAllister, Miller, Sotillo, Thompson, Weinert, 1991</marker>
<rawString>Anderson A., Bader M., Bard E., Boyle E., Doherty G. M., Garrod S., Isard S., Kowtko J., McAllister J., Miller J., Sotillo C., Thompson H. S. and Weinert R. 1991. The HCRC Map Task Corpus. Language and Speech, 34:351-366</rawString>
</citation>
<citation valid="true">
<authors>
<author>A H Anderson</author>
<author>E A Boyle</author>
</authors>
<title>Forms of introduction in dialogues: Their discourse contexts and communicative consequences.</title>
<date>1994</date>
<booktitle>Language and Cognitive Process ,</booktitle>
<pages>9--1</pages>
<contexts>
<context position="15662" citStr="Anderson and Boyle (1994)" startWordPosition="2577" endWordPosition="2581">ips when the mouth is closed; · ) corners up e.g. when smiling; +) open smile; ·( corners down; +( corners very down ·1cornerup for asymmetric smile; ·O protruded, when the lips are rounded. Similar signals are used to annotate eyebrows shape. 3.1 Cooperation Analysis The approach we have used to analyze cooperation in dialogue task is mainly based on Bethan Davies model (Bethan Davies, 2006). The basic coded unit is the &amp;quot;move&amp;quot;, which means individual linguistic choices to successfully fulfill Map Task. The idea of evaluating utterance choices in relation to task success can be traced back to Anderson and Boyle (1994) who linked utterance choices to the accuracy of the route performed on the map. Bethan Davies extended the meaning of &amp;quot;move&amp;quot; to the goal evaluation, from a narrow set of indicators to a sort of data-driven set. In particular, Bethan Davies stressed some useful points for the computation of collaboration between two communicative partners: ·social needs of dialogue: there is a minimum &amp;quot;effort&amp;quot; needed to keep the conversation going. It includes minimal answers like &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot; and feedbacks. These brief utterances are classified by Bethan Davies (following Traum, 1994) as low effort, as they </context>
</contexts>
<marker>Anderson, Boyle, 1994</marker>
<rawString>Anderson A. H., and Boyle E. A. 1994. Forms of introduction in dialogues: Their discourse contexts and communicative consequences. Language and Cognitive Process , 9(1):101 - 122</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Anderson</author>
<author>W Linden</author>
<author>M E Habra</author>
</authors>
<title>The importance of examining blood pressure reactivity and recovery in anger provocation research.</title>
<date>2005</date>
<journal>International Journal of Psychophysiology</journal>
<volume>57</volume>
<issue>3</issue>
<pages>159--163</pages>
<contexts>
<context position="9118" citStr="Anderson et al., 2005" startWordPosition="1483" endWordPosition="1486">ed condenser, placed in front of each speaker 82 The recording procedure of REC is the following. Before starting the task, we record baseline condition that is to say we record participants&apos; psychophysiological outputs for 5 minutes without challenging them. Then the task started and we recorded the psychophysiological outputs during the interaction which we called task condition. Then the confederate started challenging the speaker with the aim of getting him/her angry. To do so, the confederate at minutes 4, 9 and 13 of the interaction plays a script (negative emotion elicitation in giver; Anderson et al., 2005): ·You driving me in the wrong direction, try to be more accurate!&amp;quot;; ·&amp;quot;It&apos;s still wrong, this can&apos;t be your best, try harder! So, again, from where you stop &amp;quot;I; e • &amp;quot;You&apos;re obviously not good enough in giving instruction&amp;quot;. In Fig. 2 we show the results of a 1x5 AN*VA executed in confederate condition. Heart rate (HR) is confronted over the five times of interest (baseline, task, after 4 minutes, after 9 minutes, after 13 minutes). The times of interest are baseline, task, and after 4, 9 and 13 minutes, that is to say just after emotion elicitation with the script. We find that HR is significan</context>
</contexts>
<marker>Anderson, Linden, Habra, 2005</marker>
<rawString>Anderson J. C., Linden W., and Habra M. E. 2005. The importance of examining blood pressure reactivity and recovery in anger provocation research. International Journal of Psychophysiology 57(3): 159-163</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Argyle</author>
<author>M Cook</author>
</authors>
<title>Gaze and mutual gaze, Cambridge:</title>
<date>1976</date>
<publisher>Cambridge University Press</publisher>
<contexts>
<context position="6522" citStr="Argyle and Cook 1976" startWordPosition="1066" endWordPosition="1070">ing of REC corpus Giver and follower are both native Italian speakers. In the instructions it was told them that they will have no more than 20 minutes to accomplish the task. The interaction has two conditions: screen and no screen. In screen condition a barrier was present between the two speakers. In no screen condition a short barrier, as in the original map task, was placed allowing giver and follower to see each other&apos;s face. With these two conditions we want to test whether seeing the speakers face during interactions influences facial emotion display and cooperation (see Kendon, 1967; Argyle and Cook 1976; for the relationship between gaze/no gaze and facial displays; for the influence of gaze on cooperation and coordination see Brennan et al., 2008). A further condition, emotion elicitation, was added. In &amp;quot;emotion&amp;quot; condition the follower or the giver can alternatively be a confederate, with the aim of getting the other participant angry. In this condition the psychophysiological state of the confederate is not recorded. In fact, as it is an acted behavior, it is not interesting for research purpose. All the participants had given informed consent and the experimental protocol has been approve</context>
</contexts>
<marker>Argyle, Cook, 1976</marker>
<rawString>Argyle M. and Cook M. 1976 Gaze and mutual gaze, Cambridge: Cambridge University Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bethan Davies L</author>
</authors>
<title>Testing Dialogue Principles in Task-*riented Dialogues: An Exploration of Cooperation, Collaboration, Effort and Risk.</title>
<date>2006</date>
<booktitle>In University ofLeeds papers</booktitle>
<marker>L, 2006</marker>
<rawString>Bethan Davies L. 2006. Testing Dialogue Principles in Task-*riented Dialogues: An Exploration of Cooperation, Collaboration, Effort and Risk. In University ofLeeds papers</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
<author>X Chen</author>
<author>C A Dickinson</author>
<author>M A Neider</author>
<author>J C Zelinsky</author>
</authors>
<title>Coordinating cognition: The costs and benefits of shared gaze during collaborative search.</title>
<date>2008</date>
<journal>Cognition</journal>
<volume>106</volume>
<issue>3</issue>
<pages>1465--1477</pages>
<contexts>
<context position="6670" citStr="Brennan et al., 2008" startWordPosition="1090" endWordPosition="1093">utes to accomplish the task. The interaction has two conditions: screen and no screen. In screen condition a barrier was present between the two speakers. In no screen condition a short barrier, as in the original map task, was placed allowing giver and follower to see each other&apos;s face. With these two conditions we want to test whether seeing the speakers face during interactions influences facial emotion display and cooperation (see Kendon, 1967; Argyle and Cook 1976; for the relationship between gaze/no gaze and facial displays; for the influence of gaze on cooperation and coordination see Brennan et al., 2008). A further condition, emotion elicitation, was added. In &amp;quot;emotion&amp;quot; condition the follower or the giver can alternatively be a confederate, with the aim of getting the other participant angry. In this condition the psychophysiological state of the confederate is not recorded. In fact, as it is an acted behavior, it is not interesting for research purpose. All the participants had given informed consent and the experimental protocol has been approved by the Human Research Ethics Committee of Trento University. REC is by now made up of 17 dyadic interactions, 9 with confederate, for a total of 2</context>
</contexts>
<marker>Brennan, Chen, Dickinson, Neider, Zelinsky, 2008</marker>
<rawString>Brennan S. E., Chen X., Dickinson C. A., Neider M. A. and Zelinsky J. C. 2008. Coordinating cognition: The costs and benefits of shared gaze during collaborative search. Cognition 106(3): 1465-1477</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>Friesen WV</author>
</authors>
<title>FACS Facial Action Codind Scheme. A technique for the measurement of facial action,</title>
<date>1978</date>
<publisher>Consulting Press</publisher>
<location>Palo Alto, CA:</location>
<marker>Ekman, WV, 1978</marker>
<rawString>Ekman P. and Friesen WV. 1978. FACS Facial Action Codind Scheme. A technique for the measurement of facial action, Palo Alto, CA: Consulting Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Unleashing the killer corpus: experiences in creating the multi-everything</title>
<date>2007</date>
<journal>AMI Meeting Corpus, Language Resources and Evaluation,</journal>
<volume>41</volume>
<pages>181--190</pages>
<contexts>
<context position="3012" citStr="Carletta, 2007" startWordPosition="469" endWordPosition="470">tional area&amp;quot;. The collection of multimodal data has raised the question of coding scheme reliability. The aim of testing coding scheme reliability is to assess whether a scheme is able to capture observable reality and allows some generalizations. From mid Nineties, the kappa statistic has begun to be applied to validate coding scheme reliability. Basically, the kappa statistic is a statistical method to assess agreement among a group of observers. Kappa has been used to validate some multimodal coding schemes too. However, up to now many multimodal coding schemes have a very low kappa score (Carletta, 2007, Douglas-Cowie et al., 2005; Pianesi et al., 2005, Reidsma et al., 2008). This could be due to the nature of multimodal data. In fact, annotation of mental and emotional states of mind is a very demanding task. The low annotation agreement which affects multimodal corpora validation could also be due to the nature of the kappa statistics. In fact, the assumption underlining the use of kappa as reliability measure is that coding scheme categories are mutually exclusive and equally distinct one another. This is clearly difficult to be obtained in multimodal corpora annotation, as communication </context>
</contexts>
<marker>Carletta, 2007</marker>
<rawString>Carletta, J. 2007. Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus, Language Resources and Evaluation, 41: 181-190</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-M Colletta</author>
<author>R Kunene</author>
<author>Venouil</author>
<author>A Tcherkassof</author>
<author>A</author>
</authors>
<title>Double Level Analysis of the Multimodal Expressions of Emotions in Human-machine Interaction. In</title>
<date>2008</date>
<pages>5--11</pages>
<contexts>
<context position="22496" citStr="Colletta et al., 2008" startWordPosition="3697" endWordPosition="3700">e independently or not, if their coding relies only manually). 85 *ur kappa scores are very high if compared with other multimodal annotation results. This is because we analyze cooperation and emotion with an unambiguous coding scheme. In particular, we do not refer to emotive terms directly. In fact every annotator has his/her own representation of a particular emotion, which could be pretty different from the one of another coder. This representation will represent a problem especially for annotation of blended emotions, which are ambiguous and mixed by nature. As some authors have argued (Colletta et al., 2008) annotation of mental and emotional states is a very demanding task. The analysis of non verbal features requires a different approach if compared with other linguistics tasks as multimodal communication is multi channel (e.g. audiovisual) and has multiple semantic levels (e.g. a facial expression can deeply modify the sense of a sentence, such as in humor or irony). The final goal of this research is performing a logistic regression on cooperation and emotion display. We will also investigate speakers&apos; role (giver or follower) and screen/no screen conditions role with respect to cooperation. </context>
</contexts>
<marker>Colletta, Kunene, Venouil, Tcherkassof, A, 2008</marker>
<rawString>Colletta, J.-M., Kunene, R., Venouil, and A. Tcherkassof, A. 2008. Double Level Analysis of the Multimodal Expressions of Emotions in Human-machine Interaction. In Martin, J.-C., Patrizia, P., Kipp, M., Heylen, D., (Eds.) Multimodal Corpora: From Models of Natural Interaction to Systems and Applications, 5-11</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Craggs</author>
<author>M Wood</author>
</authors>
<title>A Categorical Annotation Scheme for Emotion in the Linguistic Content of Dialogue.</title>
<date>2004</date>
<booktitle>In Affective Dialogue Systems, Elsevier,</booktitle>
<pages>89--100</pages>
<marker>Craggs, Wood, 2004</marker>
<rawString>Craggs R., and Wood M. 2004. A Categorical Annotation Scheme for Emotion in the Linguistic Content of Dialogue. In Affective Dialogue Systems, Elsevier, 89-100</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Douglas-Cowie</author>
<author>L Devillers</author>
<author>J-C Martin</author>
<author>R Cowi</author>
<author>S Savvidou</author>
<author>S Abrilian</author>
<author>C Cox</author>
</authors>
<title>Multimodal Databases of Everyday Emotion: Facing up to Complexity. In</title>
<date>2005</date>
<booktitle>9th European Conference on Speech Communication and Technology (Interspeech&apos;2005)</booktitle>
<pages>813--816</pages>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="3040" citStr="Douglas-Cowie et al., 2005" startWordPosition="471" endWordPosition="474">e collection of multimodal data has raised the question of coding scheme reliability. The aim of testing coding scheme reliability is to assess whether a scheme is able to capture observable reality and allows some generalizations. From mid Nineties, the kappa statistic has begun to be applied to validate coding scheme reliability. Basically, the kappa statistic is a statistical method to assess agreement among a group of observers. Kappa has been used to validate some multimodal coding schemes too. However, up to now many multimodal coding schemes have a very low kappa score (Carletta, 2007, Douglas-Cowie et al., 2005; Pianesi et al., 2005, Reidsma et al., 2008). This could be due to the nature of multimodal data. In fact, annotation of mental and emotional states of mind is a very demanding task. The low annotation agreement which affects multimodal corpora validation could also be due to the nature of the kappa statistics. In fact, the assumption underlining the use of kappa as reliability measure is that coding scheme categories are mutually exclusive and equally distinct one another. This is clearly difficult to be obtained in multimodal corpora annotation, as communication channels (i.e. voice, face m</context>
<context position="23729" citStr="Douglas-Cowie et al., 2005" startWordPosition="3899" endWordPosition="3902">edictions are that in case of full screen condition (i. e. the two speakers can&apos;t see each other) the cooperation will be lower with respect to short screen condition (i. e. the two speakers can see each other&apos;s face) while emotion display will be wider and more intense for full screen condition with respect to short barrier condition. No predictions are made on the speaker role. 4 Conclusions and Future Directions Cooperative behavior and its relationship with emotions is a topic of great interest in the field of dialogue annotation. Usually emotions achieve a low agreement among raters (see Douglas-Cowie et al., 2005) and surprisingly emotion recognition is higher in a condition of modality deprivation (only acoustic or only visual vs. bimodal). Neuroscience research on emotion shows that emotion recognition is a process performed firstly by sight, but the awareness of the emotion expressed is mediated by the prefrontal cortex. Moreover a predefined set of emotion labels can influence the perception of facial expression. Therefore we decide to deconstruct each signal without attributing directly an emotive label. We consider promising the implementation in computational coding schemes of neuroscience evide</context>
</contexts>
<marker>Douglas-Cowie, Devillers, Martin, Cowi, Savvidou, Abrilian, Cox, 2005</marker>
<rawString>Douglas-Cowie E., Devillers L., Martin J.-C., Cowi R., Savvidou S., Abrilian S., and Cox C. 2005. Multimodal Databases of Everyday Emotion: Facing up to Complexity. In 9th European Conference on Speech Communication and Technology (Interspeech&apos;2005) Lisbon, Portugal, September 4-8, 813-816</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feldman Barrett L</author>
<author>K A Lindquist</author>
<author>M Gendron</author>
</authors>
<title>Language as Context for the Perception of Emotion. Trends in Cognitive Sciences,</title>
<date>2007</date>
<volume>11</volume>
<issue>8</issue>
<pages>327--332</pages>
<marker>L, Lindquist, Gendron, 2007</marker>
<rawString>Feldman Barrett L., Lindquist K. A., and Gendron M. 2007. Language as Context for the Perception of Emotion. Trends in Cognitive Sciences, 11(8): 327-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Measuring Nominal Scale Agreement among</title>
<date>1971</date>
<journal>Multiple Coders Psychological Bulletin</journal>
<volume>11</volume>
<issue>4</issue>
<pages>23--34</pages>
<contexts>
<context position="19448" citStr="Fleiss, 1971" startWordPosition="3203" endWordPosition="3204">yielding, turn concluding, and feedback) has been annotated as well. Video clips have been orthographically transcribed. To do so, we adopted a subset of the conventions applied to the transcription of the speech corpus of the LUNA project corpus annotation (see Rodriguez et al., 2007). 3. 2 Coding Procedure and Kappa Scores Up to now we have annotated 9 emotive tokens of an average length of 100 seconds each. They have been annotated with the coding scheme previously described by 6 annotators. *ur coding scheme has been implemented into ANVIL software (Kipp, 2001). A Fleiss&apos; kappa statistic (Fleiss, 1971) has been performed on the annotations. We choose Fleiss&apos; kappa as it is the suitable statistics when chance agreement is calculated on more than two coders. In this case the agreement is expected on the basis of a single distribution reflecting the combined judgments of all coders. Cooperation Cooperation type level -2 No response to answer: breaks the maxims of quality, quantity and relevance -2 No information add when required: breaks the maxims of quality, quantity and manner -2 No turn giving, no check: breaks the maxims of quality, quantity and relevance -1 Inappropriate reply (no giving</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Fleiss J. L. 1971. Measuring Nominal Scale Agreement among Multiple Coders Psychological Bulletin 11(4): 23-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Goeleven</author>
<author>R De Raedt</author>
<author>L Leyman</author>
<author>B Verschuere</author>
</authors>
<title>The Karolinska Directed Emotional Faces: A validation study, Cognition and Emotion,</title>
<date>2008</date>
<volume>22</volume>
<pages>1118</pages>
<marker>Goeleven, De Raedt, Leyman, Verschuere, 2008</marker>
<rawString>Goeleven E., De Raedt R., Leyman L., and Verschuere, B. 2008. The Karolinska Directed Emotional Faces: A validation study, Cognition and Emotion, 22:1094 -1118</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Some Functions of Gaze Directions</title>
<date>1967</date>
<journal>in Social Interaction, Acta Psychologica</journal>
<pages>26--1</pages>
<contexts>
<context position="6500" citStr="Kendon, 1967" startWordPosition="1064" endWordPosition="1065"> in the recording of REC corpus Giver and follower are both native Italian speakers. In the instructions it was told them that they will have no more than 20 minutes to accomplish the task. The interaction has two conditions: screen and no screen. In screen condition a barrier was present between the two speakers. In no screen condition a short barrier, as in the original map task, was placed allowing giver and follower to see each other&apos;s face. With these two conditions we want to test whether seeing the speakers face during interactions influences facial emotion display and cooperation (see Kendon, 1967; Argyle and Cook 1976; for the relationship between gaze/no gaze and facial displays; for the influence of gaze on cooperation and coordination see Brennan et al., 2008). A further condition, emotion elicitation, was added. In &amp;quot;emotion&amp;quot; condition the follower or the giver can alternatively be a confederate, with the aim of getting the other participant angry. In this condition the psychophysiological state of the confederate is not recorded. In fact, as it is an acted behavior, it is not interesting for research purpose. All the participants had given informed consent and the experimental pro</context>
</contexts>
<marker>Kendon, 1967</marker>
<rawString>Kendon A. 1967. Some Functions of Gaze Directions in Social Interaction, Acta Psychologica 26(1):1-47</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
<author>M Neff</author>
<author>I Albrecht</author>
</authors>
<title>An Annotation Scheme for Conversational Gestures: How to economically capture timing and form. In</title>
<date>2006</date>
<pages>24--28</pages>
<contexts>
<context position="2085" citStr="Kipp et al., 2006" startWordPosition="321" endWordPosition="324">s from face displays. 1 Introduction In the last years many multimodal corpora have been collected. These corpora have been recorded in several languages and have being elicited with different methodologies: acted (such as for emotion corpora, see for example Goeleven, 2008), task oriented corpora, multiparty dialogs, corpora elicited with scripts or storytelling and ecological corpora. Among the goals of collection and analysis of corpora there is shading light on crucial aspects of speech production. Some of the main research questions are how language and gesture correlate with each other (Kipp et al., 2006) and how emotion expression modifies speech (Magno Caldognetto et al., 2004) and gesture (Poggi, 2007). Moreover, great efforts have been done to analyze multimodal aspects of irony, persuasion or motivation. Multimodal coding schemes are mainly focused on dialogue acts, topic segmentation and the so called &amp;quot;emotional area&amp;quot;. The collection of multimodal data has raised the question of coding scheme reliability. The aim of testing coding scheme reliability is to assess whether a scheme is able to capture observable reality and allows some generalizations. From mid Nineties, the kappa statistic </context>
</contexts>
<marker>Kipp, Neff, Albrecht, 2006</marker>
<rawString>Kipp M., Neff M., and Albrecht I. 2006. An Annotation Scheme for Conversational Gestures: How to economically capture timing and form. In Martin, J.-C., Kizhnlein, P., Paggio, P., Stiefelhagen, R., Pianesi, F. (Eds.) Multimodal Corpora: From Multimodal Behavior Theories to Usable Models, 24-28</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>ANVIL - A Generic Annotation Tool for Multimodal Dialogue.</title>
<date>2001</date>
<booktitle>In Eurospeech 2001 Scandinavia 7th European Conference on Speech Communication and Technology</booktitle>
<contexts>
<context position="19406" citStr="Kipp, 2001" startWordPosition="3197" endWordPosition="3198">iving, turn offering, turn taking, turn yielding, turn concluding, and feedback) has been annotated as well. Video clips have been orthographically transcribed. To do so, we adopted a subset of the conventions applied to the transcription of the speech corpus of the LUNA project corpus annotation (see Rodriguez et al., 2007). 3. 2 Coding Procedure and Kappa Scores Up to now we have annotated 9 emotive tokens of an average length of 100 seconds each. They have been annotated with the coding scheme previously described by 6 annotators. *ur coding scheme has been implemented into ANVIL software (Kipp, 2001). A Fleiss&apos; kappa statistic (Fleiss, 1971) has been performed on the annotations. We choose Fleiss&apos; kappa as it is the suitable statistics when chance agreement is calculated on more than two coders. In this case the agreement is expected on the basis of a single distribution reflecting the combined judgments of all coders. Cooperation Cooperation type level -2 No response to answer: breaks the maxims of quality, quantity and relevance -2 No information add when required: breaks the maxims of quality, quantity and manner -2 No turn giving, no check: breaks the maxims of quality, quantity and r</context>
</contexts>
<marker>Kipp, 2001</marker>
<rawString>Kipp M. 2001. ANVIL - A Generic Annotation Tool for Multimodal Dialogue. In Eurospeech 2001 Scandinavia 7th European Conference on Speech Communication and Technology</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Reliability in content analysis: Some common misconceptions and recommendations.</title>
<date>2004</date>
<journal>Human Communication Research,</journal>
<pages>30--411</pages>
<contexts>
<context position="21526" citStr="Krippendorff, 2004" startWordPosition="3537" endWordPosition="3538">p&lt;0.001). Turn management has a Fleiss kappa score of 0.784 (p&lt;0.001). As regard gaze, Fleiss kappa score is 0.788 (p&lt;0.001). Mouth shape annotation has a Fleiss kappa score of 0.816 (p&lt;0.001) and eyebrows shape annotation has a Fleiss kappa of 0.855 (p&lt;0.001). In the last years a large debate on the interpretation of kappa scores has widespread. There is a general lack of consensus on how to interpret those values. Some authors (Allwood et al., 2006) consider as reliable for multimodal annotation kappa values between 0.67 and 0.8. *ther authors accept as reliable only scoring rates over 0.8 (Krippendorff, 2004) to allow some generalizations. What is clear is that it seems inappropriate to propose a general cut off point, especially for multimodal annotation where very little literature on kappa agreement has been reported. In this field it seems more necessary that researches report clearly the method they apply (e. g. the number of coders, if they code independently or not, if their coding relies only manually). 85 *ur kappa scores are very high if compared with other multimodal annotation results. This is because we analyze cooperation and emotion with an unambiguous coding scheme. In particular, </context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Krippendorff K. 2004. Reliability in content analysis: Some common misconceptions and recommendations. Human Communication Research, 30:411-433</rawString>
</citation>
<citation valid="false">
<authors>
<author>Magno Caldognetto E</author>
<author>I Poggi</author>
<author>P Cosi</author>
<author>F Cavicchio</author>
<author>G Merola</author>
</authors>
<title>Multimodal Score: an Anvil Based Annotation Scheme for Multimodal AudioVideo Analysis.</title>
<date>2004</date>
<booktitle>Proceedings of Workshop Multimodal Corpora: Models Of Human Behavior For The Specification And Evaluation Of Multimodal Input And Output Interfaces.</booktitle>
<pages>29--33</pages>
<editor>In Martin, J.-C., *s, E.D., Kizhnlein, P., Boves, L., Paggio, P., Catizone, R. (eds.)</editor>
<marker>E, Poggi, Cosi, Cavicchio, Merola, 2004</marker>
<rawString>Magno Caldognetto E., Poggi I., Cosi P., Cavicchio F. and Merola G. 2004. Multimodal Score: an Anvil Based Annotation Scheme for Multimodal AudioVideo Analysis. In Martin, J.-C., *s, E.D., Kizhnlein, P., Boves, L., Paggio, P., Catizone, R. (eds.) Proceedings of Workshop Multimodal Corpora: Models Of Human Behavior For The Specification And Evaluation Of Multimodal Input And Output Interfaces. 29-33</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Martin</author>
<author>G Caridakis</author>
<author>L Devillers</author>
<author>K Karpouzis</author>
<author>S Abrilian</author>
</authors>
<title>Manual Annotation and Automatic Image Processing of Multimodal Emotional Behaviors: Validating the Annotation of TV Interviews.</title>
<date>2006</date>
<booktitle>In Fifth international conference on Language Resources and Evaluation (LREC</booktitle>
<location>Genoa, Italy</location>
<contexts>
<context position="11926" citStr="Martin et al. (2006)" startWordPosition="1954" endWordPosition="1957">ts 3 Annotation Method and Coding Scheme The emotion annotation coding scheme used to analyze our map task is quite far from the emotion annotation schemes proposed in Computational Linguistic literature. Craggs and Woods (2005) proposed to annotate emotions with a scheme where emotions are expressed at different blending levels (i. e. blending of different emotion and emotive levels). In Craggs and Woods opinions&apos; annotators must label the given emotion with a main emotive term (e. g. anger, sadness, joy etc.) correcting the emotional state with a score ranging from 1 (low) to 5 (very high). Martin et al. (2006) used a three steps rank scale of emotion valence (positive, neutral and negative) to annotate their corpus recorded from TV interviews. �������� ��������� ������ ������ ������ ������� ������� ���� � � � � � ���� ����� ����� ���� ���� ���� ������� ������� ������ ������ ������ ������� ������� ������ ������ ������ ��� ���������� �������� ��� ����� ����� ����� ����� ����� PeaksMme 83 But both these methods had quite poor results in terms of annotation agreement among coders. Several studies on emotions have shown how emotional words and their connected concepts influence emotion judgments and the</context>
</contexts>
<marker>Martin, Caridakis, Devillers, Karpouzis, Abrilian, 2006</marker>
<rawString>Martin J.-C., Caridakis G., Devillers L., Karpouzis K. and Abrilian S. 2006. Manual Annotation and Automatic Image Processing of Multimodal Emotional Behaviors: Validating the Annotation of TV Interviews. In Fifth international conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pianesi</author>
<author>C Leonardi</author>
<author>M Zancanaro</author>
</authors>
<title>Multimodal Annotated Corpora of Consensus Decision Making Meetings. In</title>
<date>2006</date>
<pages>6--9</pages>
<marker>Pianesi, Leonardi, Zancanaro, 2006</marker>
<rawString>Pianesi F., Leonardi C., and Zancanaro M. 2006. Multimodal Annotated Corpora of Consensus Decision Making Meetings. In Martin, J.-C., Kizhnlein, P., Paggio, P., Stiefelhagen, R., Pianesi, F. (Eds.) Multimodal Corpora: From Multimodal Behavior Theories to Usable Models, 6--9</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Poggi</author>
</authors>
<title>Mind, hands, face and body. A goal and belief view of multimodal communication,</title>
<date>2007</date>
<location>Berlin: Weidler Buchverlag</location>
<contexts>
<context position="2187" citStr="Poggi, 2007" startWordPosition="338" endWordPosition="339">rpora have been recorded in several languages and have being elicited with different methodologies: acted (such as for emotion corpora, see for example Goeleven, 2008), task oriented corpora, multiparty dialogs, corpora elicited with scripts or storytelling and ecological corpora. Among the goals of collection and analysis of corpora there is shading light on crucial aspects of speech production. Some of the main research questions are how language and gesture correlate with each other (Kipp et al., 2006) and how emotion expression modifies speech (Magno Caldognetto et al., 2004) and gesture (Poggi, 2007). Moreover, great efforts have been done to analyze multimodal aspects of irony, persuasion or motivation. Multimodal coding schemes are mainly focused on dialogue acts, topic segmentation and the so called &amp;quot;emotional area&amp;quot;. The collection of multimodal data has raised the question of coding scheme reliability. The aim of testing coding scheme reliability is to assess whether a scheme is able to capture observable reality and allows some generalizations. From mid Nineties, the kappa statistic has begun to be applied to validate coding scheme reliability. Basically, the kappa statistic is a sta</context>
</contexts>
<marker>Poggi, 2007</marker>
<rawString>Poggi I., 2007. Mind, hands, face and body. A goal and belief view of multimodal communication, Berlin: Weidler Buchverlag</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reidsma D Heylen D</author>
<author>p den Akker R</author>
</authors>
<title>n the Contextual Analysis of Agreement Scores. In</title>
<date>2008</date>
<pages>52--55</pages>
<marker>D, R, 2008</marker>
<rawString>Reidsma D. Heylen D., and *p den Akker R. 2008. *n the Contextual Analysis of Agreement Scores. In Martin, J.-C., Patrizia, P., Kipp, M., Heylen, D., (Eds.) Multimodal Corpora: From Models of Natural Interaction to Systems and Applications, 52--55</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rodriguez</author>
<author>K J Stefan</author>
<author>S Dipper</author>
<author>M GStze</author>
<author>M Poesio</author>
<author>G Riccardi</author>
<author>C Raymond</author>
<author>J Wisniewska</author>
</authors>
<title>Standoff Coordination for MultiTool Annotation in a Dialogue Corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop at the ACL&apos;07 (LAW-07),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="19121" citStr="Rodriguez et al., 2007" startWordPosition="3146" endWordPosition="3149">s. *ther communicative collaboration indexes we codify in our coding scheme are the presence or absence of eye contact through gaze direction (to the interlocutor, to the map, unfocused), even in full screen condition, where the two speakers can&apos;t see each other. Dialogue turns management (turn giving, turn offering, turn taking, turn yielding, turn concluding, and feedback) has been annotated as well. Video clips have been orthographically transcribed. To do so, we adopted a subset of the conventions applied to the transcription of the speech corpus of the LUNA project corpus annotation (see Rodriguez et al., 2007). 3. 2 Coding Procedure and Kappa Scores Up to now we have annotated 9 emotive tokens of an average length of 100 seconds each. They have been annotated with the coding scheme previously described by 6 annotators. *ur coding scheme has been implemented into ANVIL software (Kipp, 2001). A Fleiss&apos; kappa statistic (Fleiss, 1971) has been performed on the annotations. We choose Fleiss&apos; kappa as it is the suitable statistics when chance agreement is calculated on more than two coders. In this case the agreement is expected on the basis of a single distribution reflecting the combined judgments of a</context>
</contexts>
<marker>Rodriguez, Stefan, Dipper, GStze, Poesio, Riccardi, Raymond, Wisniewska, 2007</marker>
<rawString>Rodriguez K., Stefan K. J., Dipper S., GStze M., Poesio M., Riccardi G., and Raymond C., and Wisniewska J., 2007. Standoff Coordination for MultiTool Annotation in a Dialogue Corpus. In Proceedings of the Linguistic Annotation Workshop at the ACL&apos;07 (LAW-07), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Smith</author>
<author>G W Cottrell</author>
<author>F Gosselin</author>
<author>P G Schyns</author>
</authors>
<title>Transmitting and Decoding Facial Expressions.</title>
<date>2005</date>
<pages>16--3</pages>
<publisher>Psychological Science</publisher>
<contexts>
<context position="13116" citStr="Smith et al., 2005" startWordPosition="2150" endWordPosition="2153">nce emotion judgments and their labeling (for a review, see Feldman Barrett et al., 2007). Thus, labeling an emotive display (e. g. a voice or a face) with a single emotive term could be not the best solution to recognize an emotion. Moreover researchers on emotion recognition from face displays find that some emotions as anger or fear are discriminated only by mouth or eyes configurations. Face seems to be evolved to transmit orthogonal signals, with a lower correlation each other. Then, these signals are deconstructed by the &amp;quot;human filtering functions&amp;quot;, i. e. the brain, as optimized inputs (Smith et al., 2005). The Facial Action Units (FACS, Ekman and Friesen, 1978) is a good scheme to annotate face expressions starting from movement of muscular units, called action units. Even if accurate, it is a little problematic to annotate facial expression, especially the mouth ones, when the subject to be annotated is speaking, as the muscular movements for speech production overlaps with the emotional configuration. *n the basis of such findings, an ongoing debate is whether the perception of a face and, specifically, of a face displaying emotions, is based on holistic perception or perception of parts. Al</context>
</contexts>
<marker>Smith, Cottrell, Gosselin, Schyns, 2005</marker>
<rawString>Smith M. L., Cottrell G. W., Gosselin F., and Schyns P. G. 2005. Transmitting and Decoding Facial Expressions. Psychological Science 16(3):184-189</rawString>
</citation>
<citation valid="true">
<authors>
<author>L G Tassinary</author>
<author>J T Cacioppo</author>
</authors>
<title>The skeletomotor system: Surface electromyography.</title>
<date>2000</date>
<booktitle>In LG Tassinary, GG Berntson, JT Cacioppo (eds) Handbook of psychophysiology,</booktitle>
<pages>263--299</pages>
<publisher>Cambridge University Press,</publisher>
<location>New York:</location>
<marker>Tassinary, Cacioppo, 2000</marker>
<rawString>Tassinary L. G. and Cacioppo J. T. 2000. The skeletomotor system: Surface electromyography. In LG Tassinary, GG Berntson, JT Cacioppo (eds) Handbook of psychophysiology, New York: Cambridge University Press, 263-299</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Traum</author>
</authors>
<title>A Computational Theory of Grounding in Natural Language Conversation, PhD Dissertation. urresearch.rochester.edu</title>
<date>1994</date>
<contexts>
<context position="16238" citStr="Traum, 1994" startWordPosition="2674" endWordPosition="2675">ed back to Anderson and Boyle (1994) who linked utterance choices to the accuracy of the route performed on the map. Bethan Davies extended the meaning of &amp;quot;move&amp;quot; to the goal evaluation, from a narrow set of indicators to a sort of data-driven set. In particular, Bethan Davies stressed some useful points for the computation of collaboration between two communicative partners: ·social needs of dialogue: there is a minimum &amp;quot;effort&amp;quot; needed to keep the conversation going. It includes minimal answers like &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot; and feedbacks. These brief utterances are classified by Bethan Davies (following Traum, 1994) as low effort, as they do not require much planning to the overall dialogue and to the joint task; ·responsibility of supplying the needs of the communication partner: to keep an utterance going, one of the speakers can provide follow-ups which take more consideration of the partner&apos;s intentions and goals in the task performance. This involves longer utterances, and of course a larger effort; ·responsibility of maintaining a known track of communication or starting a new one: there is an effort in considering the actions of a speaker within the context of a particular goal: that is, they main</context>
</contexts>
<marker>Traum, 1994</marker>
<rawString>Traum D. R. 1994. A Computational Theory of Grounding in Natural Language Conversation, PhD Dissertation. urresearch.rochester.edu</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>