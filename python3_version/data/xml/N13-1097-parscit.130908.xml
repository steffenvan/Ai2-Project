<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002544">
<title confidence="0.951817">
Separating Fact from Fear: Tracking Flu Infections on Twitter
</title>
<author confidence="0.99978">
Alex Lamb, Michael J. Paul, Mark Dredze
</author>
<affiliation confidence="0.949487333333333">
Human Language Technology Center of Excellence
Department of Computer Science
Johns Hopkins University
</affiliation>
<address confidence="0.95969">
Baltimore, MD 21218
</address>
<email confidence="0.999718">
{alamb3,mpaul19,mdredze}@jhu.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998920363636364">
Twitter has been shown to be a fast and reli-
able method for disease surveillance of com-
mon illnesses like influenza. However, previ-
ous work has relied on simple content anal-
ysis, which conflates flu tweets that report
infection with those that express concerned
awareness of the flu. By discriminating these
categories, as well as tweets about the authors
versus about others, we demonstrate signifi-
cant improvements on influenza surveillance
using Twitter.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998726">
Twitter is a fantastic data resource for many tasks:
measuring political (O’Connor et al., 2010; Tumas-
jan et al., 2010), and general sentiment (Bollen et
al., 2011), studying linguistic variation (Eisenstein
et al., 2010) and detecting earthquakes (Sakaki et
al., 2010). Similarly, Twitter has proven useful for
public health applications (Dredze, 2012), primar-
ily disease surveillance (Collier, 2012; Signorini et
al., 2011), whereby public health officials track in-
fection rates of common diseases. Standard govern-
ment data sources take weeks while Twitter provides
an immediate population measure.
Strategies for Twitter influenza surveillance in-
clude supervised classification (Culotta, 2010b; Cu-
lotta, 2010a; Eiji Aramaki and Morita, 2011), un-
supervised models for disease discovery (Paul and
Dredze, 2011), keyword counting1, tracking geo-
graphic illness propagation (Sadilek et al., 2012b),
and combining tweet contents with the social net-
work (Sadilek et al., 2012a) and location informa-
</bodyText>
<footnote confidence="0.985007">
1The DHHS competition relied solely on keyword counting.
http://www.nowtrendingchallenge.com/
</footnote>
<bodyText confidence="0.9980144">
tion (Asta and Shalizi, 2012). All of these methods
rely on a relatively simple NLP approach to analyz-
ing the tweet content, i.e. n-gram models for classi-
fying related or not related to the flu. Yet examining
flu tweets yields a more complex picture:
</bodyText>
<listItem confidence="0.993271666666667">
• going over to a friends house to check on her son.
he has the flu and i am worried about him
• Starting to get worried about swine flu...
</listItem>
<bodyText confidence="0.999932923076923">
Both are related to the flu and express worry, but
tell a different story. The first reports an infec-
tion of another person, while the second expresses
the author’s concerned awareness. While infection
tweets indicate a rise in infection rate, awareness
tweets may not. Automatically making these dis-
tinctions may improve influenza surveillance, yet re-
quires more than keywords.
We present an approach for differentiating be-
tween flu infection and concerned awareness tweets,
as well as self vs other, by relying on a deeper analy-
sis of the tweet. We present our features and demon-
strate improvements in influenza surveillance.
</bodyText>
<sectionHeader confidence="0.67922" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.999766">
Much of the early work on web-based influenza
surveillance relied on query logs and click-through
data from search engines (Eysenbach, 2006), most
famously Google’s Flu Trends service (Ginsberg et
al., 2008; Cook et al., 2011). Other sources of in-
formation include articles from the news media and
online mailing lists (Brownstein et al., 2010).
</bodyText>
<sectionHeader confidence="0.98594" genericHeader="method">
2 Capturing Nuanced Trends
</sectionHeader>
<bodyText confidence="0.999292">
Previous work has classified messages as being re-
lated or not related to influenza, with promising
surveillance results, but has ignored nuanced differ-
ences between flu tweets. Tweets that are related to
</bodyText>
<page confidence="0.978021">
789
</page>
<subsectionHeader confidence="0.293235">
Proceedings of NAACL-HLT 2013, pages 789–795,
</subsectionHeader>
<bodyText confidence="0.981203666666667">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
flu but do not report an infection can corrupt infec-
tion tracking.
Concerned Awareness vs. Infection (A/I) Many
flu tweets express a concerned awareness as opposed
to infection, including fear of getting the flu, an
awareness of increased infections, beliefs related to
flu infection, and preventative flu measures (e.g. flu
shots.) Critically, these people do not seem to have
the flu, whereas infection tweets report having the
flu. This distinction is similar to modality (Prab-
hakaran et al., 2012a). Conflating these tweets can
hurt surveillance, as around half of our annotated
flu messages were awareness. Identifying awareness
tweets may be of use in-and-of itself, such as for
characterizing fear of illness (Epstein et al., 2008;
Epstein, 2009), public perception, and discerning
sentiment (e.g. flu is negative, flu shots may be pos-
itive.) We focus on surveillance improvements.2
Self vs. Other (S/O) Tweets for both awareness
and infection can describe the author (self) or oth-
ers. It may be that self infection reporting is more
informative. We test this hypothesis by classifying
tweets as self vs. other.
Finding Flu Related Tweets (R/U) We must first
identify messages that are flu related. We construct
a classifier for flu related vs. unrelated.
</bodyText>
<sectionHeader confidence="0.999699" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.996221666666667">
Token sequences (n-grams) are an insufficient fea-
ture set, since our classes share common vocabular-
ies. Consider,
</bodyText>
<listItem confidence="0.999944">
• A little worried about the swine flu epidemic!
• Robbie might have swine flu. I’m worried.
</listItem>
<bodyText confidence="0.9986648">
Both tweets mention flu and worried, which distin-
guish them as flu related but not specifically aware-
ness or infection, nor self or other. Motivated by
Bergsma et al. (2012), we complement 3-grams with
additional features that capture longer spans of text
and generalize using part of speech tags. We begin
by processing each tweet using the ARK POS tag-
ger (Gimpel et al., 2011) and find phrase segmen-
tations using punctuation tags.3 Most phrases were
two (31.2%) or three (26.6%) tokens long.
</bodyText>
<footnote confidence="0.989717">
2While tweets can both show awareness and report an in-
fection, we formulate a binary task for simplicity since only a
small percentage of tweets were so labeled.
3We used whitespace for tokenization, which did about the
same as Jerboa (Van Durme, 2012).
</footnote>
<table confidence="0.998411736842105">
Class Name Words in Class
Infection getting, got, recovered, have, hav-
ing, had, has, catching, catch, cured,
infected
Possession bird, the flu, flu, sick, epidemic
Concern afraid, worried, scared, fear, worry,
nervous, dread, dreaded, terrified
Vaccination vaccine, vaccines, shot, shots, mist,
tamiflu, jab, nasal spray
Past Tense was, did, had, got, were, or verb with
the suffix “ed”
Present Tense is, am, are, have, has, or verb with
the suffix “ing”
Self I, I’ve, I’d, I’m, im, my
Others your, everyone, you, it, its, u, her,
he, she, he’s, she’s, she, they, you’re,
she’ll, he’ll, husband, wife, brother,
sister, your, people, kid, kids, chil-
dren, son, daughter
</table>
<tableCaption confidence="0.999956">
Table 1: Our manually created set of word class features.
</tableCaption>
<bodyText confidence="0.934744928571429">
Word Classes For our task, many word types can
behave similarly with regard to the label. We create
word lists for possessive words, flu related words,
fear related words, “self” words, “other” words, and
fear words (Table 1). A word’s presence triggers a
count-based feature corresponding to each list.
Stylometry We include Twitter-specific style fea-
tures. A feature is included for retweet, hashtags,
and mentions of other users. We include a feature
for emoticons (based on the emoticon part-of-speech
tag). We include a more specific feature for positive
emoticons (:) :D :)). We also include a feature
for negative emoticons (:( :/). Additionally, we
include a feature for links to URLs.
Part of Speech Templates We include features
based on a number of templates matching specific
sequences of words, word classes, and part of speech
tags. Where any word included in the template
matches a word in one of the word classes, an ad-
ditional feature is included indicating that the word
class was included in that template.
• Tuples of (subject,verb,object) and pairs of (sub-
ject, verb), (subject, object), and (verb, object). We
use a simple rule to construct these tuples: the first
noun or pronoun is taken as the subject, and the first
verb appearing after the subject is taken as the verb.
The object is taken as any noun or pronoun that ap-
pears before a verb or at the end of a phrase.
</bodyText>
<page confidence="0.94146">
790
</page>
<listItem confidence="0.937519023809524">
• A pairing of the first pronoun with last noun.
These are useful for S/O, e.g. I am worried that my
son has the flu to recognize the difference between
the author (I) and someone else.
• Phrases that begin with a verb (pro-drop). This is
helpful for S/O, e.g. getting the flu! which can indi-
cate self even without a self-related pronoun. An ad-
ditional feature is included if this verb is past-tense.
• Numeric references. These often indicate aware-
ness (number of people with the flu) and are gen-
erally not detected by an n-gram model. We add a
separate feature if the word following has the root
“died”, e.g. So many people dying from the flu, I’m
scared!
• Pair of first pronoun/noun with last verb in a
phrase. Many phrases have multiple verbs, but the
last verb is critical, e.g. I had feared the flu. Ad-
ditional features are added if the noun/pronoun is in
the “self” or “other” word class, and if the verb is in
the “possessive” word class.
• Flu appears as a noun before first verb in a phrase.
This indicates when flu is a subject, which is more
likely to be about awareness.
• Pair of verb and following noun. This indicates the
verbs object, which can change the focus of A/T,
e.g., I am getting a serious case of the flu vs. I am
getting a flu shot. Additional features are added if
the verb is past tense (based on word list and suffix
“-ed”.)
• Whether a flu related word appears as a noun or
an adjective. When flu is used as an adjective, it
may indicate a more general discussion of the flu,
as opposed to an actual infection I hate this flu vs. I
hate this flu hype.
• If a proper noun is followed by a possessive verb.
This may indicate others for the S/O task Looks like
Denmark has the flu. An additional feature fires for
any verb that follows a proper noun and any past
tense verb that follows a proper noun.
• Pair each noun with “?”. While infection tweets
are often statements and awareness questions, the
subject matters, e.g. Do you think that swine flu
</listItem>
<bodyText confidence="0.9013705">
is coming to America? as awareness. An equivalent
feature is included for phrases ending with “!”.
While many of our features can be extracted using
a syntactic parser (Foster et al., 2011), tweets are
very short, so our simple rules and over-generating
features captures the desired effects without parsing.
</bodyText>
<table confidence="0.9978715">
Self Other Total
Awareness 23.15% 24.07% 47.22%
Infection 37.21% 15.57% 52.78%
Total 60.36% 39.64%
</table>
<tableCaption confidence="0.997574333333333">
Table 2: The distribution over labels of the data set. In-
fection tweets are more likely to be about the author (self)
than those expressing awareness.
</tableCaption>
<subsectionHeader confidence="0.99892">
3.1 Learning
</subsectionHeader>
<bodyText confidence="0.999965333333333">
We used a log-linear model from Mallet (McCal-
lum, 2002) with L2 regularization. For each task, we
first labeled tweets as related/not-related and then
classified the related tweets as awareness/infection
and self/others. We found this two phase approach
worked better than multi-class.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="method">
4 Data Collection
</sectionHeader>
<bodyText confidence="0.99996328">
We used two Twitter data sets: a collection of 2
billion tweets from May 2009 and October 2010
(O’Connor et al., 2010)4 and 1.8 billion tweets col-
lected from August 2011 to November 2012. To
obtain labeled data, we first filtered the data sets
for messages containing words related to concern
and influenza,5 and used Amazon Mechanical Turk
(Callison-Burch and Dredze, 2010) to label tweets
as concerned awareness, infection, media and un-
related. We allowed multiple categories per tweet.
Annotators also labeled awareness/infection tweets
as self, other or both. We included tweets we anno-
tated to measure Turker quality and obtained three
annotations per tweet. More details can be found in
Lamb et al. (2012).
To construct a labeled data set we removed low
quality annotators (below 80% accuracy on gold
tweets.) This seemed like a difficult task for anno-
tators as a fifth of the data had no annotations after
this step. We used the majority label as truth and ties
were broken using the remaining low quality anno-
tators. We then hand-corrected all tweets, changing
13.5% of the labels. The resulting data set contained
11,990 tweets (Table 2), 5,990 from 2011-2012 for
training and the remaining from 2009-2010 as test.6
</bodyText>
<footnote confidence="0.990234">
4This coincided with the second and larger H1N1 (swine
flu) outbreak of 2009; swine flu is mentioned in 39.6% of the
annotated awareness or infection tweets.
5e.g. “flu”, “worried”, “worry”, “scared”, “scare”, etc.
6All development was done using cross-validation on train-
ing data, reserving test data for the final experiments.
</footnote>
<page confidence="0.96324">
791
</page>
<table confidence="0.999923181818182">
Feature Removed A/I S/O
n-grams 0.6701 0.8440
Word Classes 0.7735 0.8549
Stylometry 0.8011 0.8522
Pronoun/Last Noun 0.7976 0.8534
Pro-Drop 0.7989 0.8523
Numeric Reference 0.7988 0.8530
Pronoun/Verb 0.7987 0.8530
Flu Noun Before Verb 0.7987 0.8526
Noun in Question 0.8004 0.8534
Subject,Object,Verb 0.8005 0.8541
</table>
<tableCaption confidence="0.999017">
Table 3: F1 scores after feature ablation.
</tableCaption>
<sectionHeader confidence="0.99691" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99998915">
We begin by evaluating the accuracy on the bi-
nary classification tasks and then measure the re-
sults from the classifiers for influenza surveillance.
We created precision recall curves on the test data
(Figure 1), and measured the highest F1, for the
three binary classifiers. For A/I and S/O, our addi-
tional features improved over the n-gram baselines.
We performed feature ablation experiments (Table
3) and found that for A/I, the word class features
helped the most by a large margin, while for S/O
the stylometry and pro-drop features were the most
important after n-grams. Interestingly, S/O does
equally well removing just n-gram features, sug-
gesting that the S/O task depends on a few words
captured by our features.
Since live data will have classifiers run in stages
– to filter out not-related tweets – we evaluated
the performance of two-staged classification. F1
dropped to 0.7250 for A/I and S/O dropped to
0.8028.
</bodyText>
<subsectionHeader confidence="0.964321">
5.1 Influenza surveillance using Twitter
</subsectionHeader>
<bodyText confidence="0.9939191">
We demonstrate how our classifiers can improve in-
fluenza surveillance using Twitter. Our hypothesis
is that by isolating infection tweets we can improve
correlations against government influenza data. We
include several baseline methods:
Google Flu Trends: Trends from search queries.7
Keywords: Tweets that contained keywords from
the DHHS Twitter surveillance competition.
ATAM: We obtained 1.6 million tweets that were
automatically labeled as influenza/other by ATAM
</bodyText>
<footnote confidence="0.951425">
7http://www.google.org/flutrends/
</footnote>
<table confidence="0.999849285714286">
Data System 2009 2011
Google Flu Trends 0.9929 0.8829
ATAM 0.9698 0.5131
Keywords 0.9771 0.6597
Twitter All Flu 0.9833 0.7247
Infection 0.9897 0.7987
Infection+Self 0.9752 0.6662
</table>
<tableCaption confidence="0.9479445">
Table 4: Correlations against CDC ILI data: Aug 2009-
Aug 2010, Dec 2011 to Aug 2012.
</tableCaption>
<bodyText confidence="0.999674214285714">
(Paul and Dredze, 2011). We trained a binary classi-
fier with n-grams and marked tweets as flu infection.
We evaluated three trends using our three binary
classifiers trained with a reduced feature set close to
the n-gram features:8
All Flu: Tweets marked as flu by Keywords or
ATAM were then classified as related/unrelated.9
This trend used all flu-related tweets.
Infection: Related tweets were classified as either
awareness or infection. This used infection tweets.
Infection+Self: Infection were then labeled as self
or other. This trend used self tweets.
All five of these trends were correlated against
data from the Centers for Disease Control and Pre-
vention (CDC) weekly estimates of influenza-like
illness (ILI) in the U.S., with Pearson correlations
computed separately for 2009 and 2011 (Table 4).10
Previous work has shown high correlations for 2009
data, but since swine flu had so dominated social me-
dia, we expect weaker correlations for 2011.
Results are show in Table 4 and Figure 2 shows
two classifiers against the CDC ILI data. We see
that in 2009 the Infection curve fits the CDC curve
very closely, while the All Flu curve appears to
substantially overestimate the flu rate at the peak.
While 2009 is clearly easier, and all trends have
similar correlations, our Infection classifier beats the
other Twitter methods. All trends do much worse in
</bodyText>
<footnote confidence="0.858685833333333">
8Classifiers trained on 2011 data and thresholds selected to
maximize F1 on held out 2009 data.
9Since our data set to train related or unrelated focused on
tweets that appeared to mention the flu, we first filtered out ob-
vious non-flu tweets by running ATAM and Keywords.
10While the 2009 data is a 10% sample of Twitter, we used a
different approach for 2011. To increase the amount of data, we
collected Tweets mentioning health keywords and then normal-
ized by the public stream counts. For our analysis, we excluded
days that were missing data. Additionally, we used a geolocator
based on user provided locations to exclude non-US messages.
See (Dredze et al., 2013) for details and code for the geolocator.
</footnote>
<page confidence="0.992795">
792
</page>
<figureCaption confidence="0.987911333333333">
Figure 1: Left to right: Precision-recall curves for related vs. not related, awareness vs. infection and self vs. others.
Figure 2: The Twitter flu rate for two years alongside the ILI rates provided by the CDC. The y-axes are not comparable
between the two years due to differences in data collection, but we note that the 2011-12 season was much milder.
</figureCaption>
<figure confidence="0.997752568181818">
100
80
Recall
60
40
20
100
80
Recall
60
40
20
055 60 65 70 75 80 85
Precision
80
F1 = 0.7891
60
40
20
040 50 60 70 80 90 100
Precision
F1 = 0.8550
F1 = 0.8499
N-Grams
All Features
070 75 80 85 90 95
Precision
100
Recall
N-Grams
All Features
F1 = 0.7985
F1 = 0.7562
F1 = 0.7665
N-Grams
All Features
2009-2010 2011-2012
08/30/09 11/08/09 01/17/10 03/28/10 06/06/10 08/15/10 11/27/11 01/15/12 03/04/12 04/22/12 06/10/12 07/29/12
Date Date
Flu Rate
CDC
Twitter (All Flu)
Twitter (Infection Only)
Flu Rate
</figure>
<bodyText confidence="0.99866476">
the 2011 season, which was much milder and thus
harder to detect. Of the Twitter methods, those us-
ing our system were dramatically higher, with the
Infection curve doing the best by a significant mar-
gin. Separating out infection from awareness (A/T)
led to significant improvements, while the S/O clas-
sifier did not, for unknown reasons.
The best result using Twitter reported to date has
been by Doan et al. (2012), whose best system had
a correlation of 0.9846 during the weeks beginning
8/30/09–05/02/10. Our Infection system had a cor-
relation of 0.9887 during the same period. While
Google does better than any of the Twitter systems,
we note that Google has access to much more (pro-
prietary) data, and their system is trained to predict
CDC trends, whereas our Twitter system is intrinsi-
cally trained only on the tweets themselves.
Finally, we are also interested in daily trends in
addition to weekly, but there is no available evalu-
ation data on this scale. Instead, we computed the
stability of each curve, by measuring the day-to-day
changes. In the 2009 season, the relative increase
or decrease from the previous day had a variance of
3.0% under the Infection curve, compared to 4.1%
under ATAM and 6.7% under Keywords.
</bodyText>
<sectionHeader confidence="0.998827" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999964833333333">
Previous papers have implicitly assumed that flu-
related tweets mimick the infection rate. While this
was plausible on 2009 data that focused on the swine
flu epidemic, it is clearly false for more typical flu
seasons. Our results show that by differentiating be-
tween types of flu tweets to isolate reports of infec-
tion, we can recover reasonable surveillance. This
result delivers a promising message for the NLP
community: deeper content analysis of tweets mat-
ters. We believe this conclusion is applicable to nu-
merous Twitter trend tasks, and we encourage others
to investigate richer content analyses for these tasks.
In particular, the community interested in modeling
author beliefs and influence (Diab et al., 2009; Prab-
hakaran et al., 2012b; Biran and Rambow, 2011)
may find our task and data of interest. Finally, be-
yond surveillance, our methods can be used to study
disease awareness and sentiment, which has impli-
cations for how public health officials respond to
outbreaks. We conclude with an example of this dis-
tinction. On June 11th, 2009, the World Health Or-
ganization declared that the swine flu had become a
global flu pandemic. On that day, flu awareness in-
creased 282%, while infections increased only 53%.
</bodyText>
<page confidence="0.998052">
793
</page>
<sectionHeader confidence="0.99626" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999748660194175">
Dena Asta and Cosma Shalizi. 2012. Identifying in-
fluenza trends via Twitter. In NIPS Workshop on So-
cial Network and Social Media Analysis: Methods,
Models and Applications.
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Proc.
NAACL-HLT, pages 327–337.
O. Biran and O. Rambow. 2011. Identifying justifi-
cations in written dialogs. In Semantic Computing
(ICSC), 2011 Fifth IEEE International Conference on,
pages 162–168. IEEE.
J. Bollen, A. Pepe, and H. Mao. 2011. Modeling pub-
lic mood and emotion: Twitter sentiment and socio-
economic phenomena. In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media, pages 450–453.
John S. Brownstein, Clark C. Freifeld, Emily H. Chan,
Mikaela Keller, Amy L. Sonricker, Sumiko R. Mekaru,
and David L. Buckeridge. 2010. Information tech-
nology and global surveillance of cases of 2009
h1n1 influenza. New England Journal of Medicine,
362(18):1731–1735.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon’s Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Mechanical Turk.
N. Collier. 2012. Uncovering text mining: A survey
of current work on web-based epidemic intelligence.
Global Public Health, 7(7):731–749.
Samantha Cook, Corrie Conrad, Ashley L. Fowlkes, and
Matthew H. Mohebbi. 2011. Assessing google flu
trends performance in the united states during the
2009 influenza virus a (h1n1) pandemic. PLOS ONE,
6(8):e23610.
A. Culotta. 2010a. Towards detecting influenza epi-
demics by analyzing Twitter messages. In ACM Work-
shop on Soc.Med. Analytics.
Aron Culotta. 2010b. Detecting influenza epidemics
by analyzing Twitter messages. arXiv:1007.4748v1
[cs.IR], July.
Mona T. Diab, Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009. Committed belief annotation and tagging. In
ACL Third Linguistic Annotation Workshop.
S. Doan, L. Ohno-Machado, and N. Collier. 2012. En-
hancing Twitter data analysis with simple semantic fil-
tering: Example in tracking influenza-like illnesses.
arXiv preprint arXiv:1210.0848.
Mark Dredze, Michael J. Paul, Shane Bergsma, and Hieu
Tran. 2013. A Twitter geolocation system with appli-
cations to public health. Working paper.
Mark Dredze. 2012. How social media will change pub-
lic health. IEEE Intelligent Systems, 27(4):81–84.
Sachiko Maskawa Eiji Aramaki and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza epi-
demics using Twitter. In Empirical Natural Language
Processing Conference (EMNLP).
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Empirical Natural Lan-
guage Processing Conference (EMNLP).
Joshua Epstein, Jon Parker, Derek Cummings, and Ross
Hammond. 2008. Coupled contagion dynamics of
fear and disease: Mathematical and computational ex-
plorations. PLoS ONE, 3(12).
J.M. Epstein. 2009. Modelling to contain pandemics.
Nature, 460(7256):687–687.
G. Eysenbach. 2006. Infodemiology: tracking flu-
related searches on the web for syndromic surveil-
lance. In AMIA Annual Symposium, pages 244–248.
AMIA.
J. Foster, ¨O. C¸etinoglu, J. Wagner, J. Le Roux, S. Hogan,
J. Nivre, D. Hogan, J. Van Genabith, et al. 2011. #
hardtoparse: Pos tagging and parsing the Twitterverse.
In proceedings of the Workshop On Analyzing Micro-
text (AAAI 2011), pages 20–25.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In As-
sociation for Computational Linguistics (ACL).
J. Ginsberg, M.H. Mohebbi, R.S. Patel, L. Brammer,
M.S. Smolinski, and L. Brilliant. 2008. Detecting
influenza epidemics using search engine query data.
Nature, 457(7232):1012–1014.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2012.
Investigating Twitter as a source for studying behav-
ioral responses to epidemics. In AAAI Fall Symposium
on Information Retrieval and Knowledge Discovery in
Biomedical Text.
A.K. McCallum. 2002. MALLET: A machine learning
for language toolkit.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
Tweets to polls: Linking text sentiment to public
opinion time series. In ICWSM.
Michael J. Paul and Mark Dredze. 2011. You are what
you Tweet: Analyzing Twitter for public health. In
ICWSM.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko,
Owen Rambow, and Benjamin Van Durme. 2012a.
</reference>
<page confidence="0.98273">
794
</page>
<reference confidence="0.998258090909091">
Statistical modality tagging from rule-based annota-
tions and crowdsourcing. In Extra-Propositional As-
pects of Meaning in Computational Linguistics (Ex-
ProM 2012).
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012b. Predicting overt display of power in
written dialogs. In North American Chapter of the As-
sociation for Computational Linguistics (NAACL).
Adam Sadilek, Henry Kautz, and Vincent Silenzio.
2012a. Modeling spread of disease from social inter-
actions. In Sixth AAAI International Conference on
Weblogs and Social Media (ICWSM).
Adam Sadilek, Henry Kautz, and Vincent Silenzio.
2012b. Predicting disease transmission from geo-
tagged micro-blog data. In Twenty-Sixth AAAI Con-
ference on Artificial Intelligence.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In WWW, New York,
NY, USA.
A. Signorini, A.M. Segre, and P.M. Polgreen. 2011. The
use of Twitter to track levels of disease activity and
public concern in the US during the influenza a H1N1
pandemic. PLoS One, 6(5):e19467.
A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M.
Welpe. 2010. Predicting elections with twitter: What
140 characters reveal about political sentiment. In
Proceedings of the fourth international aaai confer-
ence on weblogs and social media, pages 178–185.
B. Van Durme. 2012. Jerboa: A toolkit for randomized
and streaming algorithms. Technical report, Techni-
cal Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
</reference>
<page confidence="0.998584">
795
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416652">
<title confidence="0.999656">Separating Fact from Fear: Tracking Flu Infections on Twitter</title>
<author confidence="0.999497">Alex Lamb</author>
<author confidence="0.999497">Michael J Paul</author>
<author confidence="0.999497">Mark</author>
<affiliation confidence="0.793883">Human Language Technology Center of Department of Computer Johns Hopkins</affiliation>
<address confidence="0.973998">Baltimore, MD</address>
<abstract confidence="0.994035583333333">Twitter has been shown to be a fast and reliable method for disease surveillance of common illnesses like influenza. However, previous work has relied on simple content analysis, which conflates flu tweets that report infection with those that express concerned awareness of the flu. By discriminating these categories, as well as tweets about the authors versus about others, we demonstrate significant improvements on influenza surveillance using Twitter.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dena Asta</author>
<author>Cosma Shalizi</author>
</authors>
<title>Identifying influenza trends via Twitter. In</title>
<date>2012</date>
<booktitle>NIPS Workshop on Social Network</booktitle>
<contexts>
<context position="1857" citStr="Asta and Shalizi, 2012" startWordPosition="261" endWordPosition="264">eases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1The DHHS competition relied solely on keyword counting. http://www.nowtrendingchallenge.com/ tion (Asta and Shalizi, 2012). All of these methods rely on a relatively simple NLP approach to analyzing the tweet content, i.e. n-gram models for classifying related or not related to the flu. Yet examining flu tweets yields a more complex picture: • going over to a friends house to check on her son. he has the flu and i am worried about him • Starting to get worried about swine flu... Both are related to the flu and express worry, but tell a different story. The first reports an infection of another person, while the second expresses the author’s concerned awareness. While infection tweets indicate a rise in infection </context>
</contexts>
<marker>Asta, Shalizi, 2012</marker>
<rawString>Dena Asta and Cosma Shalizi. 2012. Identifying influenza trends via Twitter. In NIPS Workshop on Social Network and Social Media Analysis: Methods, Models and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Matt Post</author>
<author>David Yarowsky</author>
</authors>
<title>Stylometric analysis of scientific articles.</title>
<date>2012</date>
<booktitle>In Proc. NAACL-HLT,</booktitle>
<pages>327--337</pages>
<contexts>
<context position="5228" citStr="Bergsma et al. (2012)" startWordPosition="805" endWordPosition="808">is more informative. We test this hypothesis by classifying tweets as self vs. other. Finding Flu Related Tweets (R/U) We must first identify messages that are flu related. We construct a classifier for flu related vs. unrelated. 3 Features Token sequences (n-grams) are an insufficient feature set, since our classes share common vocabularies. Consider, • A little worried about the swine flu epidemic! • Robbie might have swine flu. I’m worried. Both tweets mention flu and worried, which distinguish them as flu related but not specifically awareness or infection, nor self or other. Motivated by Bergsma et al. (2012), we complement 3-grams with additional features that capture longer spans of text and generalize using part of speech tags. We begin by processing each tweet using the ARK POS tagger (Gimpel et al., 2011) and find phrase segmentations using punctuation tags.3 Most phrases were two (31.2%) or three (26.6%) tokens long. 2While tweets can both show awareness and report an infection, we formulate a binary task for simplicity since only a small percentage of tweets were so labeled. 3We used whitespace for tokenization, which did about the same as Jerboa (Van Durme, 2012). Class Name Words in Class</context>
</contexts>
<marker>Bergsma, Post, Yarowsky, 2012</marker>
<rawString>Shane Bergsma, Matt Post, and David Yarowsky. 2012. Stylometric analysis of scientific articles. In Proc. NAACL-HLT, pages 327–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Biran</author>
<author>O Rambow</author>
</authors>
<title>Identifying justifications in written dialogs.</title>
<date>2011</date>
<booktitle>In Semantic Computing (ICSC), 2011 Fifth IEEE International Conference on,</booktitle>
<pages>162--168</pages>
<publisher>IEEE.</publisher>
<marker>Biran, Rambow, 2011</marker>
<rawString>O. Biran and O. Rambow. 2011. Identifying justifications in written dialogs. In Semantic Computing (ICSC), 2011 Fifth IEEE International Conference on, pages 162–168. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bollen</author>
<author>A Pepe</author>
<author>H Mao</author>
</authors>
<title>Modeling public mood and emotion: Twitter sentiment and socioeconomic phenomena.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>450--453</pages>
<contexts>
<context position="904" citStr="Bollen et al., 2011" startWordPosition="131" endWordPosition="134">shown to be a fast and reliable method for disease surveillance of common illnesses like influenza. However, previous work has relied on simple content analysis, which conflates flu tweets that report infection with those that express concerned awareness of the flu. By discriminating these categories, as well as tweets about the authors versus about others, we demonstrate significant improvements on influenza surveillance using Twitter. 1 Introduction Twitter is a fantastic data resource for many tasks: measuring political (O’Connor et al., 2010; Tumasjan et al., 2010), and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised model</context>
</contexts>
<marker>Bollen, Pepe, Mao, 2011</marker>
<rawString>J. Bollen, A. Pepe, and H. Mao. 2011. Modeling public mood and emotion: Twitter sentiment and socioeconomic phenomena. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media, pages 450–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Brownstein</author>
<author>Clark C Freifeld</author>
<author>Emily H Chan</author>
<author>Mikaela Keller</author>
<author>Amy L Sonricker</author>
<author>Sumiko R Mekaru</author>
<author>David L Buckeridge</author>
</authors>
<title>Information technology and global surveillance of cases of 2009 h1n1 influenza.</title>
<date>2010</date>
<journal>New England Journal of Medicine,</journal>
<volume>362</volume>
<issue>18</issue>
<contexts>
<context position="3208" citStr="Brownstein et al., 2010" startWordPosition="487" endWordPosition="490">han keywords. We present an approach for differentiating between flu infection and concerned awareness tweets, as well as self vs other, by relying on a deeper analysis of the tweet. We present our features and demonstrate improvements in influenza surveillance. 1.1 Related Work Much of the early work on web-based influenza surveillance relied on query logs and click-through data from search engines (Eysenbach, 2006), most famously Google’s Flu Trends service (Ginsberg et al., 2008; Cook et al., 2011). Other sources of information include articles from the news media and online mailing lists (Brownstein et al., 2010). 2 Capturing Nuanced Trends Previous work has classified messages as being related or not related to influenza, with promising surveillance results, but has ignored nuanced differences between flu tweets. Tweets that are related to 789 Proceedings of NAACL-HLT 2013, pages 789–795, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics flu but do not report an infection can corrupt infection tracking. Concerned Awareness vs. Infection (A/I) Many flu tweets express a concerned awareness as opposed to infection, including fear of getting the flu, an awareness of incre</context>
</contexts>
<marker>Brownstein, Freifeld, Chan, Keller, Sonricker, Mekaru, Buckeridge, 2010</marker>
<rawString>John S. Brownstein, Clark C. Freifeld, Emily H. Chan, Mikaela Keller, Amy L. Sonricker, Sumiko R. Mekaru, and David L. Buckeridge. 2010. Information technology and global surveillance of cases of 2009 h1n1 influenza. New England Journal of Medicine, 362(18):1731–1735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Mechanical Turk.</booktitle>
<contexts>
<context position="11159" citStr="Callison-Burch and Dredze, 2010" startWordPosition="1827" endWordPosition="1830">um, 2002) with L2 regularization. For each task, we first labeled tweets as related/not-related and then classified the related tweets as awareness/infection and self/others. We found this two phase approach worked better than multi-class. 4 Data Collection We used two Twitter data sets: a collection of 2 billion tweets from May 2009 and October 2010 (O’Connor et al., 2010)4 and 1.8 billion tweets collected from August 2011 to November 2012. To obtain labeled data, we first filtered the data sets for messages containing words related to concern and influenza,5 and used Amazon Mechanical Turk (Callison-Burch and Dredze, 2010) to label tweets as concerned awareness, infection, media and unrelated. We allowed multiple categories per tweet. Annotators also labeled awareness/infection tweets as self, other or both. We included tweets we annotated to measure Turker quality and obtained three annotations per tweet. More details can be found in Lamb et al. (2012). To construct a labeled data set we removed low quality annotators (below 80% accuracy on gold tweets.) This seemed like a difficult task for annotators as a fifth of the data had no annotations after this step. We used the majority label as truth and ties were </context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
</authors>
<title>Uncovering text mining: A survey of current work on web-based epidemic intelligence.</title>
<date>2012</date>
<journal>Global Public Health,</journal>
<volume>7</volume>
<issue>7</issue>
<contexts>
<context position="1140" citStr="Collier, 2012" startWordPosition="164" endWordPosition="165">d awareness of the flu. By discriminating these categories, as well as tweets about the authors versus about others, we demonstrate significant improvements on influenza surveillance using Twitter. 1 Introduction Twitter is a fantastic data resource for many tasks: measuring political (O’Connor et al., 2010; Tumasjan et al., 2010), and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1The D</context>
</contexts>
<marker>Collier, 2012</marker>
<rawString>N. Collier. 2012. Uncovering text mining: A survey of current work on web-based epidemic intelligence. Global Public Health, 7(7):731–749.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samantha Cook</author>
<author>Corrie Conrad</author>
<author>Ashley L Fowlkes</author>
<author>Matthew H Mohebbi</author>
</authors>
<title>Assessing google flu trends performance in the united states during the 2009 influenza virus a (h1n1) pandemic.</title>
<date>2011</date>
<journal>PLOS ONE,</journal>
<volume>6</volume>
<issue>8</issue>
<contexts>
<context position="3090" citStr="Cook et al., 2011" startWordPosition="468" endWordPosition="471"> tweets may not. Automatically making these distinctions may improve influenza surveillance, yet requires more than keywords. We present an approach for differentiating between flu infection and concerned awareness tweets, as well as self vs other, by relying on a deeper analysis of the tweet. We present our features and demonstrate improvements in influenza surveillance. 1.1 Related Work Much of the early work on web-based influenza surveillance relied on query logs and click-through data from search engines (Eysenbach, 2006), most famously Google’s Flu Trends service (Ginsberg et al., 2008; Cook et al., 2011). Other sources of information include articles from the news media and online mailing lists (Brownstein et al., 2010). 2 Capturing Nuanced Trends Previous work has classified messages as being related or not related to influenza, with promising surveillance results, but has ignored nuanced differences between flu tweets. Tweets that are related to 789 Proceedings of NAACL-HLT 2013, pages 789–795, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics flu but do not report an infection can corrupt infection tracking. Concerned Awareness vs. Infection (A/I) Many flu </context>
</contexts>
<marker>Cook, Conrad, Fowlkes, Mohebbi, 2011</marker>
<rawString>Samantha Cook, Corrie Conrad, Ashley L. Fowlkes, and Matthew H. Mohebbi. 2011. Assessing google flu trends performance in the united states during the 2009 influenza virus a (h1n1) pandemic. PLOS ONE, 6(8):e23610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
</authors>
<title>Towards detecting influenza epidemics by analyzing Twitter messages.</title>
<date>2010</date>
<booktitle>In ACM Workshop on Soc.Med. Analytics.</booktitle>
<contexts>
<context position="1435" citStr="Culotta, 2010" startWordPosition="204" endWordPosition="205"> et al., 2010; Tumasjan et al., 2010), and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1The DHHS competition relied solely on keyword counting. http://www.nowtrendingchallenge.com/ tion (Asta and Shalizi, 2012). All of these methods rely on a relatively simple NLP approach to analyzing the tweet content, i.e. n-gram models for classifying related or not related to the flu. Yet examinin</context>
</contexts>
<marker>Culotta, 2010</marker>
<rawString>A. Culotta. 2010a. Towards detecting influenza epidemics by analyzing Twitter messages. In ACM Workshop on Soc.Med. Analytics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
</authors>
<title>Detecting influenza epidemics by analyzing Twitter messages. arXiv:1007.4748v1 [cs.IR],</title>
<date>2010</date>
<contexts>
<context position="1435" citStr="Culotta, 2010" startWordPosition="204" endWordPosition="205"> et al., 2010; Tumasjan et al., 2010), and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1The DHHS competition relied solely on keyword counting. http://www.nowtrendingchallenge.com/ tion (Asta and Shalizi, 2012). All of these methods rely on a relatively simple NLP approach to analyzing the tweet content, i.e. n-gram models for classifying related or not related to the flu. Yet examinin</context>
</contexts>
<marker>Culotta, 2010</marker>
<rawString>Aron Culotta. 2010b. Detecting influenza epidemics by analyzing Twitter messages. arXiv:1007.4748v1 [cs.IR], July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona T Diab</author>
<author>Lori Levin</author>
<author>Teruko Mitamura</author>
<author>Owen Rambow</author>
<author>Vinodkumar Prabhakaran</author>
<author>Weiwei Guo</author>
</authors>
<title>Committed belief annotation and tagging.</title>
<date>2009</date>
<booktitle>In ACL Third Linguistic Annotation Workshop.</booktitle>
<marker>Diab, Levin, Mitamura, Rambow, Prabhakaran, Guo, 2009</marker>
<rawString>Mona T. Diab, Lori Levin, Teruko Mitamura, Owen Rambow, Vinodkumar Prabhakaran, and Weiwei Guo. 2009. Committed belief annotation and tagging. In ACL Third Linguistic Annotation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Doan</author>
<author>L Ohno-Machado</author>
<author>N Collier</author>
</authors>
<title>Enhancing Twitter data analysis with simple semantic filtering: Example in tracking influenza-like illnesses. arXiv preprint arXiv:1210.0848.</title>
<date>2012</date>
<contexts>
<context position="17808" citStr="Doan et al. (2012)" startWordPosition="2906" endWordPosition="2909">2009-2010 2011-2012 08/30/09 11/08/09 01/17/10 03/28/10 06/06/10 08/15/10 11/27/11 01/15/12 03/04/12 04/22/12 06/10/12 07/29/12 Date Date Flu Rate CDC Twitter (All Flu) Twitter (Infection Only) Flu Rate the 2011 season, which was much milder and thus harder to detect. Of the Twitter methods, those using our system were dramatically higher, with the Infection curve doing the best by a significant margin. Separating out infection from awareness (A/T) led to significant improvements, while the S/O classifier did not, for unknown reasons. The best result using Twitter reported to date has been by Doan et al. (2012), whose best system had a correlation of 0.9846 during the weeks beginning 8/30/09–05/02/10. Our Infection system had a correlation of 0.9887 during the same period. While Google does better than any of the Twitter systems, we note that Google has access to much more (proprietary) data, and their system is trained to predict CDC trends, whereas our Twitter system is intrinsically trained only on the tweets themselves. Finally, we are also interested in daily trends in addition to weekly, but there is no available evaluation data on this scale. Instead, we computed the stability of each curve, </context>
</contexts>
<marker>Doan, Ohno-Machado, Collier, 2012</marker>
<rawString>S. Doan, L. Ohno-Machado, and N. Collier. 2012. Enhancing Twitter data analysis with simple semantic filtering: Example in tracking influenza-like illnesses. arXiv preprint arXiv:1210.0848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Michael J Paul</author>
<author>Shane Bergsma</author>
<author>Hieu Tran</author>
</authors>
<title>A Twitter geolocation system with applications to public health. Working paper.</title>
<date>2013</date>
<contexts>
<context position="16489" citStr="Dredze et al., 2013" startWordPosition="2671" endWordPosition="2674">elected to maximize F1 on held out 2009 data. 9Since our data set to train related or unrelated focused on tweets that appeared to mention the flu, we first filtered out obvious non-flu tweets by running ATAM and Keywords. 10While the 2009 data is a 10% sample of Twitter, we used a different approach for 2011. To increase the amount of data, we collected Tweets mentioning health keywords and then normalized by the public stream counts. For our analysis, we excluded days that were missing data. Additionally, we used a geolocator based on user provided locations to exclude non-US messages. See (Dredze et al., 2013) for details and code for the geolocator. 792 Figure 1: Left to right: Precision-recall curves for related vs. not related, awareness vs. infection and self vs. others. Figure 2: The Twitter flu rate for two years alongside the ILI rates provided by the CDC. The y-axes are not comparable between the two years due to differences in data collection, but we note that the 2011-12 season was much milder. 100 80 Recall 60 40 20 100 80 Recall 60 40 20 055 60 65 70 75 80 85 Precision 80 F1 = 0.7891 60 40 20 040 50 60 70 80 90 100 Precision F1 = 0.8550 F1 = 0.8499 N-Grams All Features 070 75 80 85 90 9</context>
</contexts>
<marker>Dredze, Paul, Bergsma, Tran, 2013</marker>
<rawString>Mark Dredze, Michael J. Paul, Shane Bergsma, and Hieu Tran. 2013. A Twitter geolocation system with applications to public health. Working paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
</authors>
<title>How social media will change public health.</title>
<date>2012</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1093" citStr="Dredze, 2012" startWordPosition="158" endWordPosition="159">port infection with those that express concerned awareness of the flu. By discriminating these categories, as well as tweets about the authors versus about others, we demonstrate significant improvements on influenza surveillance using Twitter. 1 Introduction Twitter is a fantastic data resource for many tasks: measuring political (O’Connor et al., 2010; Tumasjan et al., 2010), and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sa</context>
</contexts>
<marker>Dredze, 2012</marker>
<rawString>Mark Dredze. 2012. How social media will change public health. IEEE Intelligent Systems, 27(4):81–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sachiko Maskawa Eiji Aramaki</author>
<author>Mizuki Morita</author>
</authors>
<title>Twitter catches the flu: Detecting influenza epidemics using Twitter.</title>
<date>2011</date>
<booktitle>In Empirical Natural Language Processing Conference (EMNLP).</booktitle>
<contexts>
<context position="1484" citStr="Aramaki and Morita, 2011" startWordPosition="210" endWordPosition="213">, and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1The DHHS competition relied solely on keyword counting. http://www.nowtrendingchallenge.com/ tion (Asta and Shalizi, 2012). All of these methods rely on a relatively simple NLP approach to analyzing the tweet content, i.e. n-gram models for classifying related or not related to the flu. Yet examining flu tweets yields a more complex picture: • goi</context>
</contexts>
<marker>Aramaki, Morita, 2011</marker>
<rawString>Sachiko Maskawa Eiji Aramaki and Mizuki Morita. 2011. Twitter catches the flu: Detecting influenza epidemics using Twitter. In Empirical Natural Language Processing Conference (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Empirical Natural Language Processing Conference (EMNLP).</booktitle>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2010. A latent variable model for geographic lexical variation. In Empirical Natural Language Processing Conference (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Epstein</author>
<author>Jon Parker</author>
<author>Derek Cummings</author>
<author>Ross Hammond</author>
</authors>
<title>Coupled contagion dynamics of fear and disease: Mathematical and computational explorations.</title>
<date>2008</date>
<journal>PLoS ONE,</journal>
<volume>3</volume>
<issue>12</issue>
<contexts>
<context position="4311" citStr="Epstein et al., 2008" startWordPosition="655" endWordPosition="658">weets express a concerned awareness as opposed to infection, including fear of getting the flu, an awareness of increased infections, beliefs related to flu infection, and preventative flu measures (e.g. flu shots.) Critically, these people do not seem to have the flu, whereas infection tweets report having the flu. This distinction is similar to modality (Prabhakaran et al., 2012a). Conflating these tweets can hurt surveillance, as around half of our annotated flu messages were awareness. Identifying awareness tweets may be of use in-and-of itself, such as for characterizing fear of illness (Epstein et al., 2008; Epstein, 2009), public perception, and discerning sentiment (e.g. flu is negative, flu shots may be positive.) We focus on surveillance improvements.2 Self vs. Other (S/O) Tweets for both awareness and infection can describe the author (self) or others. It may be that self infection reporting is more informative. We test this hypothesis by classifying tweets as self vs. other. Finding Flu Related Tweets (R/U) We must first identify messages that are flu related. We construct a classifier for flu related vs. unrelated. 3 Features Token sequences (n-grams) are an insufficient feature set, sinc</context>
</contexts>
<marker>Epstein, Parker, Cummings, Hammond, 2008</marker>
<rawString>Joshua Epstein, Jon Parker, Derek Cummings, and Ross Hammond. 2008. Coupled contagion dynamics of fear and disease: Mathematical and computational explorations. PLoS ONE, 3(12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Epstein</author>
</authors>
<title>Modelling to contain pandemics.</title>
<date>2009</date>
<journal>Nature,</journal>
<volume>460</volume>
<issue>7256</issue>
<contexts>
<context position="4327" citStr="Epstein, 2009" startWordPosition="659" endWordPosition="660">ned awareness as opposed to infection, including fear of getting the flu, an awareness of increased infections, beliefs related to flu infection, and preventative flu measures (e.g. flu shots.) Critically, these people do not seem to have the flu, whereas infection tweets report having the flu. This distinction is similar to modality (Prabhakaran et al., 2012a). Conflating these tweets can hurt surveillance, as around half of our annotated flu messages were awareness. Identifying awareness tweets may be of use in-and-of itself, such as for characterizing fear of illness (Epstein et al., 2008; Epstein, 2009), public perception, and discerning sentiment (e.g. flu is negative, flu shots may be positive.) We focus on surveillance improvements.2 Self vs. Other (S/O) Tweets for both awareness and infection can describe the author (self) or others. It may be that self infection reporting is more informative. We test this hypothesis by classifying tweets as self vs. other. Finding Flu Related Tweets (R/U) We must first identify messages that are flu related. We construct a classifier for flu related vs. unrelated. 3 Features Token sequences (n-grams) are an insufficient feature set, since our classes sh</context>
</contexts>
<marker>Epstein, 2009</marker>
<rawString>J.M. Epstein. 2009. Modelling to contain pandemics. Nature, 460(7256):687–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Eysenbach</author>
</authors>
<title>Infodemiology: tracking flurelated searches on the web for syndromic surveillance.</title>
<date>2006</date>
<booktitle>In AMIA Annual Symposium,</booktitle>
<pages>244--248</pages>
<publisher>AMIA.</publisher>
<contexts>
<context position="3004" citStr="Eysenbach, 2006" startWordPosition="456" endWordPosition="457">erned awareness. While infection tweets indicate a rise in infection rate, awareness tweets may not. Automatically making these distinctions may improve influenza surveillance, yet requires more than keywords. We present an approach for differentiating between flu infection and concerned awareness tweets, as well as self vs other, by relying on a deeper analysis of the tweet. We present our features and demonstrate improvements in influenza surveillance. 1.1 Related Work Much of the early work on web-based influenza surveillance relied on query logs and click-through data from search engines (Eysenbach, 2006), most famously Google’s Flu Trends service (Ginsberg et al., 2008; Cook et al., 2011). Other sources of information include articles from the news media and online mailing lists (Brownstein et al., 2010). 2 Capturing Nuanced Trends Previous work has classified messages as being related or not related to influenza, with promising surveillance results, but has ignored nuanced differences between flu tweets. Tweets that are related to 789 Proceedings of NAACL-HLT 2013, pages 789–795, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics flu but do not report an infec</context>
</contexts>
<marker>Eysenbach, 2006</marker>
<rawString>G. Eysenbach. 2006. Infodemiology: tracking flurelated searches on the web for syndromic surveillance. In AMIA Annual Symposium, pages 244–248. AMIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Foster</author>
<author>¨O C¸etinoglu</author>
<author>J Wagner</author>
<author>J Le Roux</author>
<author>S Hogan</author>
<author>J Nivre</author>
<author>D Hogan</author>
<author>J Van Genabith</author>
</authors>
<title>hardtoparse: Pos tagging and parsing the Twitterverse.</title>
<date>2011</date>
<booktitle>In proceedings of the Workshop On Analyzing Microtext (AAAI</booktitle>
<pages>20--25</pages>
<marker>Foster, C¸etinoglu, Wagner, Le Roux, Hogan, Nivre, Hogan, Van Genabith, 2011</marker>
<rawString>J. Foster, ¨O. C¸etinoglu, J. Wagner, J. Le Roux, S. Hogan, J. Nivre, D. Hogan, J. Van Genabith, et al. 2011. # hardtoparse: Pos tagging and parsing the Twitterverse. In proceedings of the Workshop On Analyzing Microtext (AAAI 2011), pages 20–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ginsberg</author>
<author>M H Mohebbi</author>
<author>R S Patel</author>
<author>L Brammer</author>
<author>M S Smolinski</author>
<author>L Brilliant</author>
</authors>
<title>Detecting influenza epidemics using search engine query data.</title>
<date>2008</date>
<journal>Nature,</journal>
<volume>457</volume>
<issue>7232</issue>
<contexts>
<context position="3070" citStr="Ginsberg et al., 2008" startWordPosition="464" endWordPosition="467">fection rate, awareness tweets may not. Automatically making these distinctions may improve influenza surveillance, yet requires more than keywords. We present an approach for differentiating between flu infection and concerned awareness tweets, as well as self vs other, by relying on a deeper analysis of the tweet. We present our features and demonstrate improvements in influenza surveillance. 1.1 Related Work Much of the early work on web-based influenza surveillance relied on query logs and click-through data from search engines (Eysenbach, 2006), most famously Google’s Flu Trends service (Ginsberg et al., 2008; Cook et al., 2011). Other sources of information include articles from the news media and online mailing lists (Brownstein et al., 2010). 2 Capturing Nuanced Trends Previous work has classified messages as being related or not related to influenza, with promising surveillance results, but has ignored nuanced differences between flu tweets. Tweets that are related to 789 Proceedings of NAACL-HLT 2013, pages 789–795, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics flu but do not report an infection can corrupt infection tracking. Concerned Awareness vs. Infec</context>
</contexts>
<marker>Ginsberg, Mohebbi, Patel, Brammer, Smolinski, Brilliant, 2008</marker>
<rawString>J. Ginsberg, M.H. Mohebbi, R.S. Patel, L. Brammer, M.S. Smolinski, and L. Brilliant. 2008. Detecting influenza epidemics using search engine query data. Nature, 457(7232):1012–1014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lamb</author>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>Investigating Twitter as a source for studying behavioral responses to epidemics.</title>
<date>2012</date>
<booktitle>In AAAI Fall Symposium on Information Retrieval and Knowledge Discovery in Biomedical Text.</booktitle>
<contexts>
<context position="11496" citStr="Lamb et al. (2012)" startWordPosition="1880" endWordPosition="1883">0 (O’Connor et al., 2010)4 and 1.8 billion tweets collected from August 2011 to November 2012. To obtain labeled data, we first filtered the data sets for messages containing words related to concern and influenza,5 and used Amazon Mechanical Turk (Callison-Burch and Dredze, 2010) to label tweets as concerned awareness, infection, media and unrelated. We allowed multiple categories per tweet. Annotators also labeled awareness/infection tweets as self, other or both. We included tweets we annotated to measure Turker quality and obtained three annotations per tweet. More details can be found in Lamb et al. (2012). To construct a labeled data set we removed low quality annotators (below 80% accuracy on gold tweets.) This seemed like a difficult task for annotators as a fifth of the data had no annotations after this step. We used the majority label as truth and ties were broken using the remaining low quality annotators. We then hand-corrected all tweets, changing 13.5% of the labels. The resulting data set contained 11,990 tweets (Table 2), 5,990 from 2011-2012 for training and the remaining from 2009-2010 as test.6 4This coincided with the second and larger H1N1 (swine flu) outbreak of 2009; swine fl</context>
</contexts>
<marker>Lamb, Paul, Dredze, 2012</marker>
<rawString>Alex Lamb, Michael J. Paul, and Mark Dredze. 2012. Investigating Twitter as a source for studying behavioral responses to epidemics. In AAAI Fall Symposium on Information Retrieval and Knowledge Discovery in Biomedical Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<contexts>
<context position="10536" citStr="McCallum, 2002" startWordPosition="1731" endWordPosition="1733"> America? as awareness. An equivalent feature is included for phrases ending with “!”. While many of our features can be extracted using a syntactic parser (Foster et al., 2011), tweets are very short, so our simple rules and over-generating features captures the desired effects without parsing. Self Other Total Awareness 23.15% 24.07% 47.22% Infection 37.21% 15.57% 52.78% Total 60.36% 39.64% Table 2: The distribution over labels of the data set. Infection tweets are more likely to be about the author (self) than those expressing awareness. 3.1 Learning We used a log-linear model from Mallet (McCallum, 2002) with L2 regularization. For each task, we first labeled tweets as related/not-related and then classified the related tweets as awareness/infection and self/others. We found this two phase approach worked better than multi-class. 4 Data Collection We used two Twitter data sets: a collection of 2 billion tweets from May 2009 and October 2010 (O’Connor et al., 2010)4 and 1.8 billion tweets collected from August 2011 to November 2012. To obtain labeled data, we first filtered the data sets for messages containing words related to concern and influenza,5 and used Amazon Mechanical Turk (Callison-</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A.K. McCallum. 2002. MALLET: A machine learning for language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From Tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In ICWSM.</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From Tweets to polls: Linking text sentiment to public opinion time series. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>You are what you Tweet: Analyzing Twitter for public health.</title>
<date>2011</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="1551" citStr="Paul and Dredze, 2011" startWordPosition="220" endWordPosition="223">iation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1The DHHS competition relied solely on keyword counting. http://www.nowtrendingchallenge.com/ tion (Asta and Shalizi, 2012). All of these methods rely on a relatively simple NLP approach to analyzing the tweet content, i.e. n-gram models for classifying related or not related to the flu. Yet examining flu tweets yields a more complex picture: • going over to a friends house to check on her son. he has the flu and </context>
<context position="14474" citStr="Paul and Dredze, 2011" startWordPosition="2339" endWordPosition="2342">st government influenza data. We include several baseline methods: Google Flu Trends: Trends from search queries.7 Keywords: Tweets that contained keywords from the DHHS Twitter surveillance competition. ATAM: We obtained 1.6 million tweets that were automatically labeled as influenza/other by ATAM 7http://www.google.org/flutrends/ Data System 2009 2011 Google Flu Trends 0.9929 0.8829 ATAM 0.9698 0.5131 Keywords 0.9771 0.6597 Twitter All Flu 0.9833 0.7247 Infection 0.9897 0.7987 Infection+Self 0.9752 0.6662 Table 4: Correlations against CDC ILI data: Aug 2009- Aug 2010, Dec 2011 to Aug 2012. (Paul and Dredze, 2011). We trained a binary classifier with n-grams and marked tweets as flu infection. We evaluated three trends using our three binary classifiers trained with a reduced feature set close to the n-gram features:8 All Flu: Tweets marked as flu by Keywords or ATAM were then classified as related/unrelated.9 This trend used all flu-related tweets. Infection: Related tweets were classified as either awareness or infection. This used infection tweets. Infection+Self: Infection were then labeled as self or other. This trend used self tweets. All five of these trends were correlated against data from the</context>
</contexts>
<marker>Paul, Dredze, 2011</marker>
<rawString>Michael J. Paul and Mark Dredze. 2011. You are what you Tweet: Analyzing Twitter for public health. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Michael Bloodgood</author>
<author>Mona Diab</author>
<author>Bonnie Dorr</author>
<author>Lori Levin</author>
<author>Christine D Piatko</author>
<author>Owen Rambow</author>
<author>Benjamin Van Durme</author>
</authors>
<date>2012</date>
<marker>Prabhakaran, Bloodgood, Diab, Dorr, Levin, Piatko, Rambow, Van Durme, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen Rambow, and Benjamin Van Durme. 2012a.</rawString>
</citation>
<citation valid="true">
<title>Statistical modality tagging from rule-based annotations and crowdsourcing.</title>
<date>2012</date>
<booktitle>In Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM</booktitle>
<contexts>
<context position="5228" citStr="(2012)" startWordPosition="808" endWordPosition="808">tive. We test this hypothesis by classifying tweets as self vs. other. Finding Flu Related Tweets (R/U) We must first identify messages that are flu related. We construct a classifier for flu related vs. unrelated. 3 Features Token sequences (n-grams) are an insufficient feature set, since our classes share common vocabularies. Consider, • A little worried about the swine flu epidemic! • Robbie might have swine flu. I’m worried. Both tweets mention flu and worried, which distinguish them as flu related but not specifically awareness or infection, nor self or other. Motivated by Bergsma et al. (2012), we complement 3-grams with additional features that capture longer spans of text and generalize using part of speech tags. We begin by processing each tweet using the ARK POS tagger (Gimpel et al., 2011) and find phrase segmentations using punctuation tags.3 Most phrases were two (31.2%) or three (26.6%) tokens long. 2While tweets can both show awareness and report an infection, we formulate a binary task for simplicity since only a small percentage of tweets were so labeled. 3We used whitespace for tokenization, which did about the same as Jerboa (Van Durme, 2012). Class Name Words in Class</context>
<context position="11496" citStr="(2012)" startWordPosition="1883" endWordPosition="1883">et al., 2010)4 and 1.8 billion tweets collected from August 2011 to November 2012. To obtain labeled data, we first filtered the data sets for messages containing words related to concern and influenza,5 and used Amazon Mechanical Turk (Callison-Burch and Dredze, 2010) to label tweets as concerned awareness, infection, media and unrelated. We allowed multiple categories per tweet. Annotators also labeled awareness/infection tweets as self, other or both. We included tweets we annotated to measure Turker quality and obtained three annotations per tweet. More details can be found in Lamb et al. (2012). To construct a labeled data set we removed low quality annotators (below 80% accuracy on gold tweets.) This seemed like a difficult task for annotators as a fifth of the data had no annotations after this step. We used the majority label as truth and ties were broken using the remaining low quality annotators. We then hand-corrected all tweets, changing 13.5% of the labels. The resulting data set contained 11,990 tweets (Table 2), 5,990 from 2011-2012 for training and the remaining from 2009-2010 as test.6 4This coincided with the second and larger H1N1 (swine flu) outbreak of 2009; swine fl</context>
<context position="17808" citStr="(2012)" startWordPosition="2909" endWordPosition="2909">11-2012 08/30/09 11/08/09 01/17/10 03/28/10 06/06/10 08/15/10 11/27/11 01/15/12 03/04/12 04/22/12 06/10/12 07/29/12 Date Date Flu Rate CDC Twitter (All Flu) Twitter (Infection Only) Flu Rate the 2011 season, which was much milder and thus harder to detect. Of the Twitter methods, those using our system were dramatically higher, with the Infection curve doing the best by a significant margin. Separating out infection from awareness (A/T) led to significant improvements, while the S/O classifier did not, for unknown reasons. The best result using Twitter reported to date has been by Doan et al. (2012), whose best system had a correlation of 0.9846 during the weeks beginning 8/30/09–05/02/10. Our Infection system had a correlation of 0.9887 during the same period. While Google does better than any of the Twitter systems, we note that Google has access to much more (proprietary) data, and their system is trained to predict CDC trends, whereas our Twitter system is intrinsically trained only on the tweets themselves. Finally, we are also interested in daily trends in addition to weekly, but there is no available evaluation data on this scale. Instead, we computed the stability of each curve, </context>
</contexts>
<marker>2012</marker>
<rawString>Statistical modality tagging from rule-based annotations and crowdsourcing. In Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Predicting overt display of power in written dialogs.</title>
<date>2012</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="4074" citStr="Prabhakaran et al., 2012" startWordPosition="618" endWordPosition="622">edings of NAACL-HLT 2013, pages 789–795, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics flu but do not report an infection can corrupt infection tracking. Concerned Awareness vs. Infection (A/I) Many flu tweets express a concerned awareness as opposed to infection, including fear of getting the flu, an awareness of increased infections, beliefs related to flu infection, and preventative flu measures (e.g. flu shots.) Critically, these people do not seem to have the flu, whereas infection tweets report having the flu. This distinction is similar to modality (Prabhakaran et al., 2012a). Conflating these tweets can hurt surveillance, as around half of our annotated flu messages were awareness. Identifying awareness tweets may be of use in-and-of itself, such as for characterizing fear of illness (Epstein et al., 2008; Epstein, 2009), public perception, and discerning sentiment (e.g. flu is negative, flu shots may be positive.) We focus on surveillance improvements.2 Self vs. Other (S/O) Tweets for both awareness and infection can describe the author (self) or others. It may be that self infection reporting is more informative. We test this hypothesis by classifying tweets </context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2012b. Predicting overt display of power in written dialogs. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Sadilek</author>
<author>Henry Kautz</author>
<author>Vincent Silenzio</author>
</authors>
<title>Modeling spread of disease from social interactions.</title>
<date>2012</date>
<booktitle>In Sixth AAAI International Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="1633" citStr="Sadilek et al., 2012" startWordPosition="231" endWordPosition="234">milarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1The DHHS competition relied solely on keyword counting. http://www.nowtrendingchallenge.com/ tion (Asta and Shalizi, 2012). All of these methods rely on a relatively simple NLP approach to analyzing the tweet content, i.e. n-gram models for classifying related or not related to the flu. Yet examining flu tweets yields a more complex picture: • going over to a friends house to check on her son. he has the flu and i am worried about him • Starting to get worried about swine flu... Both are relat</context>
</contexts>
<marker>Sadilek, Kautz, Silenzio, 2012</marker>
<rawString>Adam Sadilek, Henry Kautz, and Vincent Silenzio. 2012a. Modeling spread of disease from social interactions. In Sixth AAAI International Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Sadilek</author>
<author>Henry Kautz</author>
<author>Vincent Silenzio</author>
</authors>
<title>Predicting disease transmission from geotagged micro-blog data.</title>
<date>2012</date>
<booktitle>In Twenty-Sixth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1633" citStr="Sadilek et al., 2012" startWordPosition="231" endWordPosition="234">milarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1The DHHS competition relied solely on keyword counting. http://www.nowtrendingchallenge.com/ tion (Asta and Shalizi, 2012). All of these methods rely on a relatively simple NLP approach to analyzing the tweet content, i.e. n-gram models for classifying related or not related to the flu. Yet examining flu tweets yields a more complex picture: • going over to a friends house to check on her son. he has the flu and i am worried about him • Starting to get worried about swine flu... Both are relat</context>
</contexts>
<marker>Sadilek, Kautz, Silenzio, 2012</marker>
<rawString>Adam Sadilek, Henry Kautz, and Vincent Silenzio. 2012b. Predicting disease transmission from geotagged micro-blog data. In Twenty-Sixth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes Twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In WWW,</booktitle>
<location>New York, NY, USA.</location>
<contexts>
<context position="1009" citStr="Sakaki et al., 2010" startWordPosition="145" endWordPosition="148">er, previous work has relied on simple content analysis, which conflates flu tweets that report infection with those that express concerned awareness of the flu. By discriminating these categories, as well as tweets about the authors versus about others, we demonstrate significant improvements on influenza surveillance using Twitter. 1 Introduction Twitter is a fantastic data resource for many tasks: measuring political (O’Connor et al., 2010; Tumasjan et al., 2010), and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagati</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes Twitter users: real-time event detection by social sensors. In WWW, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Signorini</author>
<author>A M Segre</author>
<author>P M Polgreen</author>
</authors>
<title>The use of Twitter to track levels of disease activity and public concern in the US during the influenza a H1N1 pandemic. PLoS</title>
<date>2011</date>
<journal>One,</journal>
<volume>6</volume>
<issue>5</issue>
<contexts>
<context position="1165" citStr="Signorini et al., 2011" startWordPosition="166" endWordPosition="169">the flu. By discriminating these categories, as well as tweets about the authors versus about others, we demonstrate significant improvements on influenza surveillance using Twitter. 1 Introduction Twitter is a fantastic data resource for many tasks: measuring political (O’Connor et al., 2010; Tumasjan et al., 2010), and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1, tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1The DHHS competition relied so</context>
</contexts>
<marker>Signorini, Segre, Polgreen, 2011</marker>
<rawString>A. Signorini, A.M. Segre, and P.M. Polgreen. 2011. The use of Twitter to track levels of disease activity and public concern in the US during the influenza a H1N1 pandemic. PLoS One, 6(5):e19467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tumasjan</author>
<author>T O Sprenger</author>
<author>P G Sandner</author>
<author>I M Welpe</author>
</authors>
<title>Predicting elections with twitter: What 140 characters reveal about political sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the fourth international aaai conference on weblogs and social media,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="859" citStr="Tumasjan et al., 2010" startWordPosition="123" endWordPosition="127">l19,mdredze}@jhu.edu Abstract Twitter has been shown to be a fast and reliable method for disease surveillance of common illnesses like influenza. However, previous work has relied on simple content analysis, which conflates flu tweets that report infection with those that express concerned awareness of the flu. By discriminating these categories, as well as tweets about the authors versus about others, we demonstrate significant improvements on influenza surveillance using Twitter. 1 Introduction Twitter is a fantastic data resource for many tasks: measuring political (O’Connor et al., 2010; Tumasjan et al., 2010), and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji </context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M. Welpe. 2010. Predicting elections with twitter: What 140 characters reveal about political sentiment. In Proceedings of the fourth international aaai conference on weblogs and social media, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Van Durme</author>
</authors>
<title>Jerboa: A toolkit for randomized and streaming algorithms.</title>
<date>2012</date>
<tech>Technical report, Technical Report 7,</tech>
<institution>Human Language Technology Center of Excellence, Johns Hopkins University.</institution>
<marker>Van Durme, 2012</marker>
<rawString>B. Van Durme. 2012. Jerboa: A toolkit for randomized and streaming algorithms. Technical report, Technical Report 7, Human Language Technology Center of Excellence, Johns Hopkins University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>