<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000648">
<title confidence="0.991737">
On Semi-Supervised Learning of Gaussian Mixture Models
for Phonetic Classification∗
</title>
<author confidence="0.998187">
Jui-Ting Huang and Mark Hasegawa-Johnson
</author>
<affiliation confidence="0.867133">
Department of Electrical and Computer Engineering
University of Illinois at Urbana-Champaign
Illinois, IL 61801, USA
</affiliation>
<email confidence="0.998672">
{jhuang29,jhasegaw}@illinois.edu
</email>
<sectionHeader confidence="0.996649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999118576923077">
This paper investigates semi-supervised learn-
ing of Gaussian mixture models using an uni-
fied objective function taking both labeled and
unlabeled data into account. Two methods
are compared in this work – the hybrid dis-
criminative/generative method and the purely
generative method. They differ in the crite-
rion type on labeled data; the hybrid method
uses the class posterior probabilities and the
purely generative method uses the data like-
lihood. We conducted experiments on the
TIMIT database and a standard synthetic data
set from UCI Machine Learning repository.
The results show that the two methods be-
have similarly in various conditions. For both
methods, unlabeled data improve training on
models of higher complexity in which the su-
pervised method performs poorly. In addition,
there is a trend that more unlabeled data re-
sults in more improvement in classification ac-
curacy over the supervised model. We also
provided experimental observations on the rel-
ative weights of labeled and unlabeled parts
of the training objective and suggested a criti-
cal value which could be useful for selecting a
good weighing factor.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98737805">
Speech recognition acoustic models can be trained
using untranscribed speech data (Wessel and Ney,
2005; Lamel et al., 2002; L. Wang and Woodland,
2007). Most such experiments begin by boostraping
∗This research is funded by NSF grants 0534106 and
0703624.
an initial acoustic model using a limited amount of
manually transcribed data (normally in a scale from
30 minutes to several hours), and then the initial
model is used to transcribe a relatively large amount
of untranscribed data. Only the transcriptions with
high confidence measures (Wessel and Ney, 2005;
L. Wang and Woodland, 2007) or high agreement
with closed captions (Lamel et al., 2002) are se-
lected to augment the manually transcribed data, and
new acoustic models are trained on the augmented
data set.
The general procedure described above exactly
lies in the context of semi-supervised learning prob-
lems and can be categorized as a self-training algo-
rithm. Self-training is probably the simplest semi-
supervised learning method, but it is also flexible
to be applied to complex classifiers such as speech
recognition systems. This may be the reason why
little work has been done on exploiting other semi-
supervised learning methods in speech recognition.
Though not incorporated to speech recognizers yet,
there has been some work on semi-supervised learn-
ing of Hidden Markov Models (HMM) for sequen-
tial classification. Inoue and Ueda (2003) treated the
unknown class labels of the unlabeled data as hidden
variables and used the expectation-maximization
(EM) algorithm to optimize the joint likelihood of
labeled and unlabeled data. Recently Ji et al. (2009)
applied a homotopy method to select the optimal
weight to balance between the log likelihood of la-
beled and unlabeled data when training HMMs.
Besides generative training of acoustic models,
discriminative training is another popular paradigm
in the area of speech recognition, but only when
</bodyText>
<page confidence="0.981051">
75
</page>
<note confidence="0.992169">
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 75–83,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999899325">
the transcriptions are available. Wang and Wood-
land (2007) used the self-training method to aug-
ment the training set for discriminative training.
Huang and Hasegawa-Johnson (2008) investigated
another use of discriminative information from la-
beled data by replacing the likelihood of labeled data
with the class posterior probability of labeled data in
the semi-supervised training objective for Gaussian
Mixture Models (GMM), resulting in a hybrid dis-
criminative/generative objective function. Their ex-
perimental results in binary phonetic classification
showed significant improvement in classification ac-
curacy when labeled data are scarce. A similar strat-
egy called ”‘multi-conditional learning”’ was pre-
sented in (Druck et al., 2007) applied to Markov
Random Field models for text classification tasks,
with the difference that the likelihood of labeled data
is also included in the objective. The hybrid dis-
criminative/generative objective function can be in-
terpreted as having an extra regularization term, the
likelihood of unlabeled data, in the discriminative
training criterion for labeled data. However, both
methods in (Huang and Hasegawa-Johnson, 2008)
and (Druck et al., 2007) encountered the same issue
about determining the weights for labeled and un-
labeled part in the objective function and chose to
use a development set to select the optimal weight.
This paper provides an experimental analysis on the
effect of the weight.
With the ultimate goal of applying semi-
supervised learning in speech recognition, this pa-
per investigates the learning capability of algorithms
within Gaussian Mixture Models because GMM is
the basic model inside a HMM, therefore 1) the up-
date equations derived for the parameters of GMM
can be conveniently extended to HMM for speech
recognition. 2) GMM can serve as an initial point
to help us understand more details about the semi-
supervised learning process of spectral features.
This paper makes the following contribution:
</bodyText>
<listItem confidence="0.998875444444444">
• it provides an experimental comparison of hy-
brid and purely generative training objectives.
• it studies the impact of model complexity on
learning capability of algorithms.
• it studies the impact of the amount of unlabeled
data on learning capability of algorithms.
• it analyzes the role of the relative weights of
labeled and unlabeled parts of the training ob-
jective.
</listItem>
<sectionHeader confidence="0.890963" genericHeader="introduction">
2 Algorithm
</sectionHeader>
<bodyText confidence="0.974892846153846">
Suppose a labeled set XL = (x1, ... , xn, ... , xNL)
has NL data points and xn E Rd. YL =
(y1, ... , yn, ... , yNL) are the corresponding class
labels, where yn E {1, 2, ... , Y } and Y is the num-
ber of classes. In addition, we also have an unla-
beled set XU = (x1, ... , xn, ... , xNU) without cor-
responding class labels. Each class is assigned a
Gaussian Mixture model, and all models are trained
given XL and XU. This section first presents the
hybrid discriminative/generative objective function
for training and then the purely generative objective
function. The parameter update equations are also
derived here.
</bodyText>
<subsectionHeader confidence="0.993664">
2.1 Hybrid Objective Function
</subsectionHeader>
<bodyText confidence="0.99967775">
The hybrid discriminative/generative objective func-
tion combines the discriminative criterion for la-
beled data and the generative criterion for unlabeled
data:
</bodyText>
<equation confidence="0.995825">
F (A) = log P (YL|XL; A) + α log P (XU; A), (1)
</equation>
<bodyText confidence="0.997892">
and we chose the parameters so that (1) is maxi-
mized:
</bodyText>
<equation confidence="0.967198666666667">
ˆ
A = arg max F (A) . (2)
λ
</equation>
<bodyText confidence="0.9996618">
The first component considers the log posterior
class probability of the labeled set whereas the sec-
ond component considers the log likelihood of the
unlabeled set weighted by α. In ASR community,
model training based the first component is usually
referred to as Maximum Mutual Information Esti-
mation (MMIE) and the second component Maxi-
mum Likelihood Estimation (MLE), therefore in this
paper we use a brief notation for (1) just for conve-
nience:
</bodyText>
<equation confidence="0.999771">
F (A) = F(DL)
MMI (A) + αF(DU )
ML (A) . (3)
</equation>
<bodyText confidence="0.9998916">
The two components are different in scale. First,
the size of the labeled set is usually smaller than
the size of the unlabeled set in the scenario of semi-
supervised learning, so the sums over the data sets
involve different numbers of terms; Second, the
</bodyText>
<page confidence="0.908498">
76
</page>
<bodyText confidence="0.99717405">
scales of the posterior probability and the likeli-
hood are essentially different, so are their gradients.
While the weight α balances the impacts of two
components on the training process, it may also im-
plicitly normalize the scales of the two components.
In section (3.2) we will discuss and provide a further
experimental analysis.
In this paper, the models to be trained are Gaus-
sian mixture models of continuous spectral feature
vectors for phonetic classes, which can be further
extended to Hidden Markov Models with extra pa-
rameters such as transition probabilities.
The maximization of (1) follows the techniques
in (Povey, 2003), which uses auxiliary functions for
objective maximization; In each iteration, a strong
or weak sense auxiliary function is maximized, such
that if the auxiliary function converges after itera-
tions, the objective function will be at a local maxi-
mum as well.
The objective function (1) can be rewritten as
</bodyText>
<equation confidence="0.8901405">
F (λ) = log P (XL|YL; λ) − log P (XL; λ) (4)
+ α log P (XU; λ),
</equation>
<bodyText confidence="0.9964842">
where the term log P (YL; λ) is removed because it
is independent of acoustic model parameters.
The auxiliary function at the current parameter
λold for (4) is
for the class j and mixture m given as follows:
</bodyText>
<equation confidence="0.9701305">
�
xnum
jm, − xden
jm + αxden
jm(DU) + Djmµjm
snum
jm − sden
jm + αsden
jm(DU)
+Djm σajm + µjm)) − ˆµim,
</equation>
<bodyText confidence="0.647815666666667">
where for clarity the following substitution is used:
γjm = γnum
jm − γden
</bodyText>
<equation confidence="0.9534005">
jm + αγden
jm(DU) + Djm (8)
</equation>
<bodyText confidence="0.984327">
and γjm is the sum of the posterior probabilities of
occupation of mixture component m of class j over
the dataset:
</bodyText>
<equation confidence="0.976747714285714">
γnum
jm (X) = X
xiEX,yi=j
X
γden
jm(X) =
xiEX
</equation>
<bodyText confidence="0.9992561">
and xjm and sjm are respectively the weighted
sum of xi and xai over the whole dataset with the
weight p (m|xi, yi = j) or p (m|xi), depending on
whether the superscript is the numerator or denomi-
nator model. Djm is a constant set to be the greater
of twice the smallest value that guarantees positive
variances or γden
jm (Povey, 2003). The re-estimation
formula for mixture weights is also derived from the
Extended Baum-Welch algorithm:
</bodyText>
<equation confidence="0.986995272727273">
1
ˆµjm = γjm
a1
σjm =
γjm
p (m|xi, yi = j)
(9)
p (m|xi)
G(λ, λ(old)) =Gnum(λ, λ(old)) − Gden(λ, λ(old)) ˆcjm = n ∂F o
+αGden(λ, λ(old); DU) + Gsm(λ, λ(old)), cjm ∂cjm + C P n ∂F o, (10)
(5) m′ cjm′ ∂cjm + C
</equation>
<bodyText confidence="0.9990449375">
where the first three terms are strong-sense auxiliary
functions for the conditional likelihood (referred to
as the numerator(num) model because it appears in
the numerator when computing the class posterior
probability) log P (XL|YL; λ) and the marginal like-
lihoods (referred to as the denominator(den) model
likewise) log P (XL; λ) and α log P (XU; λ) respec-
tively. The last term is a smoothing function that
doesn’t affect the local differential but ensures that
the sum of the first three term is at least a convex
weak-sense auxiliary function for good convergence
in optimization.
Maximization of (5) leads to the update equations
where the derivative was approximated (Merialdo,
1988) in the following form for practical robustness
for small-valued parameters :
</bodyText>
<equation confidence="0.927726833333333">
P . (11)
m′ γden
jm′
Under our hybrid framework, there is an extra term
γden
jm(DU)/ Pm′ γden
</equation>
<bodyText confidence="0.987631333333333">
jm′(DU) that should exist in (11),
but in practice we found that adding this term to the
approximation is not better than the original form.
Therefore, we keep using MMI-only update for mix-
ture weights. The constant C is chosen such that all
parameter derivatives are positive.
</bodyText>
<figure confidence="0.8848375">
∂FMMI
≈
∂cjm
den
γj m
γnum
jm
P −
m′ γnum
jm′
</figure>
<page confidence="0.95383">
77
</page>
<subsectionHeader confidence="0.994645">
2.2 Purely Generative Objective
</subsectionHeader>
<bodyText confidence="0.996127">
In this paper we compare the hybrid objective with
the purely generative one:
</bodyText>
<equation confidence="0.979809">
F (λ) = log P (XL|YL; λ) + α log P (XU; λ),
</equation>
<bodyText confidence="0.9798751875">
(12)
where the two components are total log likelihood of
labeled and unlabeled data respectively. (12) doesn’t
suffer from the problem of combining two heteroge-
neous probabilistic items, and the weight α being
equal to one means that the objective is a joint data
likelihood of labeled and unlabeled set with the as-
sumption that the two sets are independent. How-
ever, DL or DU might just be a sampled set of the
population and might not reflect the true proportion,
so we keep α to allow a flexible combination of two
criteria. On top of that, we need to adjust the relative
weights of the two components in practical experi-
ments.
The parameter update equation is a reduced form
of the equations in Section (2.1):
</bodyText>
<sectionHeader confidence="0.999639" genericHeader="method">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999821538461539">
The purpose of designing the learning algorithms
is for classification/recognition of speech sounds,
so we conducted phonetic classification experiments
using the TIMIT database (Garofolo et al., 1993).
We would like to investigate the relation of learning
capability of semi-supervised algorithms to other
factors and generalize our observations to other data
sets. Therefore, we used another synthetic dataset
Waveform for the evaluation of semi-supervised
learning algorithms for Gaussian Mixture model.
TIMIT: We used the same 48 phone classes and
further grouped into 39 classes according to (Lee
and Hon, 1989) as our final set of phone classes to
model. We extracted 50 speakers out of the NIST
complete test set to form the development set. All
of our experimental analyses were on the develop-
ment set. We used segmental features (Halberstadt,
1998) in the phonetic classification task. For each
phone occurrence, a fixed-length vector was calcu-
lated from the frame-based spectral features (12 PLP
coefficients plus energy) with a 5 ms frame rate and
a 25 ms Hamming window. More specifically, we
divided the frames for each phone into three regions
with 3-4-3 proportion and calculated the PLP av-
erage over each region. Three averages plus the
log duration of that phone gave a 40-dimensional
</bodyText>
<equation confidence="0.534935">
(13 x 3 + 1) measurement vector.
</equation>
<bodyText confidence="0.999897">
Waveform: We used the second versions of
the Waveform dataset available at the UCI reposi-
tory (Asuncion and Newman, 2007). There are three
classes of data. Each token is described by 40 real
attributes, and the class distribution is even.
For waveform, because the class labels are equally
distributed, we simply assigned equal number of
mixtures for each class. For TIMIT, the phone
classes are unevenly distributed, so we assigned
variable number of Gaussian mixtures for each class
by controlling the averaged data counts per mixture.
For all experiments, the initial model is an MLE
model trained with labeled data only.
To construct a mixed labeled/unlabeled data set,
the original training set were randomly divided into
the labeled and unlabeled sets with desired ratio, and
the class labels in the unlabeled set are assumed to be
unknown. To avoid that the classifier performance
may vary with particular portions of data, we ran five
folds for every experiment, each fold corresponding
to different division of training data into labeled and
unlabeled set, and took the averaged performance.
</bodyText>
<subsectionHeader confidence="0.999704">
3.1 Model Complexity
</subsectionHeader>
<bodyText confidence="0.998883866666667">
This section analyzes the learning capability of
semi-supervised learning algorithms for different
model complexities, that is, the number of mix-
tures for Gaussian mixture model. In this experi-
ment, the sizes of labeled and unlabeled set are fixed
(|DL |: |DU |= 1 : 10 and the averaged token
counts per class is around 140 for both data sets),
as we varied the total number of mixtures and eval-
uated the updated model by its classification accu-
racy. For waveform, number of mixtures was set
from 2 to 7; for TIMIT, because the number of mix-
tures per class is determined by the averaged data
counts per mixture c, we set c to 25, 20 and 15 as
the higher c gives less number of mixtures in total.
Figure 3.1 plots the averaged classification accura-
</bodyText>
<equation confidence="0.997">
γnum
jm + αγden
jm(DU)
snum
jm + αsden
jm(DU)
ˆσ2 jm = γnum
jm + αγden
jm(DU)
ˆµjm =
xnum
jm, + αxden
jm(DU)
(13)
− ˆ2
µj (14)
</equation>
<page confidence="0.997175">
78
</page>
<figureCaption confidence="0.991474">
Figure 1: Mean classification accuracies vs. α for different model complexity. The accuracies for the initial MLE
models are indicated in the parentheses. (a) waveform: training with the hybrid objective. (b) waveform: purely
generative objective. (c) TIMIT: training with the hybrid objective. (d) TIMIT: purely generative objective.
</figureCaption>
<equation confidence="0.97807475">
Accuracy ( % )
Accuracy ( % )
Accuracy ( % )
Accuracy ( % )
</equation>
<bodyText confidence="0.97524835">
cies of the updated model versus the value of α with
different model complexities. The ranges of α are
different for waveform and TIMIT because the value
of α for each dataset has different scales.
First of all, the hybrid method and purely gen-
erative method have very similar behaviors in both
waveform and TIMIT; the differences between the
two methods are insignificant regardless of α. The
hybrid method with α = 0 means supervised MMI-
training with labeled data only, and the purely gener-
ative method with α = 0 means extra several rounds
of supervised MLE-training if the convergence cri-
terion is not achieved. With the small amount of la-
beled data, most of hybrid curves start slightly lower
than the purely generative ones at α = 0, but in-
crease to as high as the purely generative ones as α
increases.
For waveform, the accuracies increase with α in-
creases for all cases except for the 2-mixture model.
Table 1 summarizes the numbers from Figure 3.1.
Except for the 2-mixture case, the improvement over
the supervised model (α = 0) is positively corre-
lated to the model complexity, as the largest im-
provements occur at the 5-mixture and 6-mixture
model for the hybrid and purely generative method
respectively. However, the highest complexity does
not necessarily gives the best classification accu-
racy; the 3-mixture model achieves the best accu-
racy among all models after semi-supervised learn-
ing whereas the 2-mixture model is the best model
for supervised learning using labeled data only.
Experiments on TIMIT show a similar behavior1;
as shown in both Figure 3.1 and Table 2, the im-
provement over the supervised model (α = 0) is
also positively correlated to the model complexity,
1Note that our baseline performance (the initial MLE model)
is much worse than benchmark because only 10% of the train-
ing data were used. We justified our baseline model by using
the whole training data and a similar accuracy ( 74%) to other
work (e.g. (Sha and Saul, 2007)) was obtained.
</bodyText>
<page confidence="0.99921">
79
</page>
<tableCaption confidence="0.946051333333333">
Table 1: The accuracies(%) of the initial MLE model, the supervised model (α = 0), the best accuracies with unlabeled
data and the absolute improvements (A) over α = 0 for different model complexities for waveform. The bolded
number is the highest value along the same column.
</tableCaption>
<table confidence="0.997856285714286">
Hybrid Purely generative
#. mix init. acc. α = 0 best acc. A α = 0 best acc. A
2 83.02 81.73 83.74 2.01 82.96 83.14 0.18
3 82.08 81.66 84.69 3.03 82.18 84.58 2.40
4 81.56 80.53 83.93 3.40 81.34 84.13 2.79
5 80.18 80.14 83.82 3.68 80.16 83.84 3.68
6 79.61 79.40 83.19 3.79 79.71 83.31 3.60
</table>
<tableCaption confidence="0.968647333333333">
Table 2: The accuracies(%) of the initial MLE model, the supervised model (α = 0), the best accuracies with unlabeled
data and the absolute improvements (A) over α = 0 for different model complexities for TIMIT. The bolded number
is the highest value along the same column.
</tableCaption>
<table confidence="0.951824">
Hybrid Purely generative
c init. acc. α = 0 best acc. A α = 0 best acc. A
25 55.34 55.47 56.58 1.11 55.32 56.7 1.38
20 55.36 55.67 56.72 1.05 55.2 56.25 1.05
15 54.72 53.71 55.39 1.68 53.7 56.09 2.39
</table>
<bodyText confidence="0.999904571428571">
as the most improvements occur at c = 25 for both
hybrid and purely generative methods. The semi-
supervised model consistently improves over the su-
pervised model. To summarize, unlabeled data im-
prove training on models of higher complexity, and
sometimes it helps achieve the best performance
with a more complex model.
</bodyText>
<subsectionHeader confidence="0.999978">
3.2 Size of Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999876357142857">
In Figure 2, we fixed the size of the labeled set (4%
of the training set) and plotted the averaged classi-
fication accuracies for learning with different sizes
of unlabeled data. First of all, the hybrid method
and purely generative method still behave similarly
in both waveform and TIMIT. For both datasets, the
figures clearly illustrate that more unlabeled data
contributes more improvement over the supervised
model regardless of the value of α. Generally, a data
distribution can be expected more precisely with a
larger sample size from the data pool, therefore we
expect the more unlabeled data the more precise in-
formation about the population, which improves the
learning capability.
</bodyText>
<subsectionHeader confidence="0.999951">
3.3 Discussion of α
</subsectionHeader>
<bodyText confidence="0.999924166666667">
During training, the weighted sum of FMMI and FML
in equation (15) increases with iterations, however
FMMI and FML are not guaranteed to increase indi-
vidually. Figure 3 illustrates how α affects the re-
spective change of the two components for a partic-
ular setting for waveform. When α = 0, the ob-
jective function does not take unlabeled data into
account, so FMMI increases while FML decreases.
FML starts to increase for nonzero α; α = 0.01
corresponds to the case where both objectives in-
creases. As α keeps growing, FMMI starts to de-
crease whereas FML keeps rising. In this partic-
ular example, α = 0.05 is the critical value at
which FMMI changes from increasing to decreas-
ing. According to our observation, the value of α
depends on the dataset and the relative size of la-
beled/unlabeled data. Table 3 shows the critical val-
ues for waveform and TIMIT for different sizes of
labeled data (5,10,15, 20% of the training set) with
a fixed set of unlabeled data (80%.) The numbers are
very different across the datasets, but there is a con-
sistent pattern within the dataset–the critical value
increases as the size of labeled set increases. One
possible explanation is that α contains an normal-
</bodyText>
<page confidence="0.994283">
80
</page>
<figureCaption confidence="0.995326666666667">
Figure 2: Mean classification accuracies vs. α for different amounts of unlabeled data (the percentage in the training
set). The averaged accuracy for the initial MLE model is 81.66% for waveform and 59.41% for TIMIT. (a) waveform:
training with the hybrid objective. (b) waveform: purely generative objective. (c) TIMIT: training with the hybrid
</figureCaption>
<figure confidence="0.743553333333333">
objective. (d) TIMIT: purely generative objective.
Accuracy ( % ) Accuracy ( % )
Accuracy ( %) Accuracy ( %)
</figure>
<bodyText confidence="0.989986">
ization factor with respect to the relative size of la-
beled/unlabeled set. The objective function in (15)
can be rewritten in terms of the normalized objective
with respect to the data size:
</bodyText>
<equation confidence="0.998462">
F (A) = |DL|F(DL) MMI(A)+α|DU|F(DU )
ML (A) . (15)
</equation>
<bodyText confidence="0.999931722222222">
where F(X) means the averaged value over the data
set X. When the labeled set size increases, α may
have to scale up accordingly such that the relative
change of the two averaged component remains in
the same scale.
Although α controls the dominance of the crite-
rion on labeled data or on unlabeled data, the fact
that which dominates the objective or the critical
value does not necessary indicate the best α. How-
ever, we observed that the best α is usually close to
or larger than the critical value, but the exact value
varies with different data. At this point, it might still
be easier to find the best weight using a small de-
velopment set. But this observation also provides a
guide about the reasonable range to search the best
α – searching starting from the critical value and it
should reach the optimal value soon according to the
plots in Figure 3.1.
</bodyText>
<tableCaption confidence="0.975179">
Table 3: The critical values for waveform and TIMIT
for different sizes of labeled data (percentage of training
data) with a fixed set of unlabeled data (80 %.)
</tableCaption>
<table confidence="0.9800884">
Size of labeled data waveform TIMIT
5% 0.09-0.11 0.03-0.04
10% 0.12-0.14 0.07-0.08
15% 0.5-0.6 0.08-0.09
20% 1-1.5 0.11-0.12
</table>
<page confidence="0.991955">
81
</page>
<figureCaption confidence="0.99116">
Figure 3: Accuracy (left), J7MMI (center), and .7ML (right) at different values of alpha.
</figureCaption>
<table confidence="0.445076">
Accuracy (/.) F M MI F—ML / 0 1 1 1
</table>
<sectionHeader confidence="0.737869" genericHeader="method">
3.4 Hybrid Criterion vs. Purely Generative
Criterion
</sectionHeader>
<bodyText confidence="0.999958055555556">
From the previous experiments, we found that the
hybrid criterion and purely generative criterion al-
most match each other in performance and are able
to learn models of the same complexity. This implies
that the criterion on labeled data has less impact on
the overall training direction than unlabeled data. In
Section 3.2, we mentioned that the best α is usually
larger than or close to the critical value around which
the unlabeled data likelihood tends to dominate the
training objective. This again suggests that labeled
data contribute less to the training objective function
compared to unlabeled data, and the criterion on la-
beled data doesn’t matter as much as the criterion on
unlabeled data. It is possible that most of the con-
tributions from labeled data have already been used
for training an initial MLE model, therefore little in-
formation could be extracted in the further training
process.
</bodyText>
<sectionHeader confidence="0.999169" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999160393939394">
Regardless of the dataset and the training objective
type on labeled data, there are some general prop-
erties about the semi-supervised learning algorithms
studied in this work. First, while limited amount of
labeled data can at most train models of lower com-
plexity well, the addition of unlabeled data makes
the updated models of higher complexity much im-
proved and sometimes perform better than less com-
plex models. Second, the amount of unlabeled data
in our semi-supervised framework generally follows
‘the-more-the-better’ principle; there is a trend that
more unlabeled data results in more improvement in
classification accuracy over the supervised model.
We also found that the objective type on labeled
data has little impact on the updated model, in the
sense that hybrid and purely generative objectives
behave similarly in learning capability. The obser-
vation that the best α occurs after the MMI criterion
begins to decrease supports the fact that the criterion
on labeled data contributes less than the criterion on
unlabeled data. This observation is also helpful in
determining the search range for the best α on the
development set by locating the critical value of the
objective as a start point to perform search.
The unified training objective method has a nice
convergence property which self-training methods
can not guarantee. The next step is to extend the
similar framework to speech recognition task where
HMMs are trained and phone boundaries are seg-
mented. It would be interesting to compare it with
self-training methods in different aspects (e.g. per-
formance, reliability, stability and computational ef-
ficiency).
</bodyText>
<page confidence="0.99821">
82
</page>
<sectionHeader confidence="0.993905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897781818182">
A. Asuncion and D.J. Newman. 2007. UCI machine
learning repository.
Gregory Druck, Chris Pal, Andrew McCallum, and Xiao-
jin Zhu. 2007. Semi-supervised classification with hy-
brid generative/discriminative methods. In KDD ’07:
Proceedings of the 13th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 280–289, New York, NY, USA. ACM.
J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus,
D. S. Pallett, and N. L. Dahlgren. 1993. Darpa timit
acoustic phonetic continuous speech corpus.
Andrew K. Halberstadt. 1998. Heterogeneous Acous-
tic Measurements and Multiple Classifiers for Speech
Recognition. Ph.D. thesis, Massachusetts Institute of
Technology.
J.-T. Huang and Mark Hasegawa-Johnson. 2008. Max-
imum mutual information estimation with unlabeled
data for phonetic classification. In Interspeech.
Masashi Inoue and Naonori Ueda. 2003. Exploitation of
unlabeled sequences in hidden markov models. IEEE
Trans. On Pattern Analysis and Machine Intelligence,
25:1570–1581.
Shihao Ji, Layne T. Watson, and Lawrence Carin. 2009.
Semisupervised learning of hidden markov models via
a homotopy method. IEEE Trans. Pattern Anal. Mach.
Intell., 31(2):275–287.
M.J.F. Gales L. Wang and P.C. Woodland. 2007. Un-
supervised training for mandarin broadcast news and
conversation transcription. In Proc. IEEE Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), volume 4, pages 353–356.
Lori Lamel, Jean-Luc Gauvain, and Gilles Adda. 2002.
Lightly supervised and unsupervised acoustic model
training. 16:115–129.
K.-F. Lee and H.-W. Hon. 1989. Speaker-independent
phone recognition using hidden markov models.
IEEE Transactions on Speech and Audio Processing,
37(11):1641–1648.
B. Merialdo. 1988. Phonetic recognition using hid-
den markov models and maximum mutualinformation
training. In Proc. IEEE Conference on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1,
pages 111–114.
Daniel Povey. 2003. Discriminative Training for Large
Vocabulary Speech Recognition. Ph.D. thesis, Cam-
bridge University.
Fei Sha and Lawrence K. Saul. 2007. Large margin hid-
den markov models for automatic speech recognition.
In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 1249–1256. MIT Press, Cambridge, MA.
Frank Wessel and Hermann Ney. 2005. Unsupervised
training of acoustic models for large vocabulary con-
tinuous speech recognition. IEEE Transactions on
Speech and Audio Processing, 13(1):23–31, January.
</reference>
<page confidence="0.99931">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.807179">
<title confidence="0.955005">On Semi-Supervised Learning of Gaussian Mixture Phonetic</title>
<author confidence="0.996224">Jui-Ting Huang</author>
<author confidence="0.996224">Mark</author>
<affiliation confidence="0.9957895">Department of Electrical and Computer University of Illinois at</affiliation>
<address confidence="0.897088">Illinois, IL 61801,</address>
<abstract confidence="0.999793074074074">This paper investigates semi-supervised learning of Gaussian mixture models using an unified objective function taking both labeled and unlabeled data into account. Two methods are compared in this work – the hybrid discriminative/generative method and the purely generative method. They differ in the criterion type on labeled data; the hybrid method uses the class posterior probabilities and the purely generative method uses the data likelihood. We conducted experiments on the TIMIT database and a standard synthetic data set from UCI Machine Learning repository. The results show that the two methods behave similarly in various conditions. For both methods, unlabeled data improve training on models of higher complexity in which the supervised method performs poorly. In addition, there is a trend that more unlabeled data results in more improvement in classification accuracy over the supervised model. We also provided experimental observations on the relative weights of labeled and unlabeled parts of the training objective and suggested a critical value which could be useful for selecting a good weighing factor.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Asuncion</author>
<author>D J Newman</author>
</authors>
<title>UCI machine learning repository.</title>
<date>2007</date>
<contexts>
<context position="13362" citStr="Asuncion and Newman, 2007" startWordPosition="2210" endWordPosition="2213">eatures (Halberstadt, 1998) in the phonetic classification task. For each phone occurrence, a fixed-length vector was calculated from the frame-based spectral features (12 PLP coefficients plus energy) with a 5 ms frame rate and a 25 ms Hamming window. More specifically, we divided the frames for each phone into three regions with 3-4-3 proportion and calculated the PLP average over each region. Three averages plus the log duration of that phone gave a 40-dimensional (13 x 3 + 1) measurement vector. Waveform: We used the second versions of the Waveform dataset available at the UCI repository (Asuncion and Newman, 2007). There are three classes of data. Each token is described by 40 real attributes, and the class distribution is even. For waveform, because the class labels are equally distributed, we simply assigned equal number of mixtures for each class. For TIMIT, the phone classes are unevenly distributed, so we assigned variable number of Gaussian mixtures for each class by controlling the averaged data counts per mixture. For all experiments, the initial model is an MLE model trained with labeled data only. To construct a mixed labeled/unlabeled data set, the original training set were randomly divided</context>
</contexts>
<marker>Asuncion, Newman, 2007</marker>
<rawString>A. Asuncion and D.J. Newman. 2007. UCI machine learning repository.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Chris Pal</author>
<author>Andrew McCallum</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-supervised classification with hybrid generative/discriminative methods.</title>
<date>2007</date>
<booktitle>In KDD ’07: Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>280--289</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4280" citStr="Druck et al., 2007" startWordPosition="639" endWordPosition="642">r discriminative training. Huang and Hasegawa-Johnson (2008) investigated another use of discriminative information from labeled data by replacing the likelihood of labeled data with the class posterior probability of labeled data in the semi-supervised training objective for Gaussian Mixture Models (GMM), resulting in a hybrid discriminative/generative objective function. Their experimental results in binary phonetic classification showed significant improvement in classification accuracy when labeled data are scarce. A similar strategy called ”‘multi-conditional learning”’ was presented in (Druck et al., 2007) applied to Markov Random Field models for text classification tasks, with the difference that the likelihood of labeled data is also included in the objective. The hybrid discriminative/generative objective function can be interpreted as having an extra regularization term, the likelihood of unlabeled data, in the discriminative training criterion for labeled data. However, both methods in (Huang and Hasegawa-Johnson, 2008) and (Druck et al., 2007) encountered the same issue about determining the weights for labeled and unlabeled part in the objective function and chose to use a development s</context>
</contexts>
<marker>Druck, Pal, McCallum, Zhu, 2007</marker>
<rawString>Gregory Druck, Chris Pal, Andrew McCallum, and Xiaojin Zhu. 2007. Semi-supervised classification with hybrid generative/discriminative methods. In KDD ’07: Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 280–289, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Garofolo</author>
<author>L F Lamel</author>
<author>W M Fisher</author>
<author>J G Fiscus</author>
<author>D S Pallett</author>
<author>N L Dahlgren</author>
</authors>
<title>Darpa timit acoustic phonetic continuous speech corpus.</title>
<date>1993</date>
<contexts>
<context position="12107" citStr="Garofolo et al., 1993" startWordPosition="2006" endWordPosition="2009">ption that the two sets are independent. However, DL or DU might just be a sampled set of the population and might not reflect the true proportion, so we keep α to allow a flexible combination of two criteria. On top of that, we need to adjust the relative weights of the two components in practical experiments. The parameter update equation is a reduced form of the equations in Section (2.1): 3 Results and Discussion The purpose of designing the learning algorithms is for classification/recognition of speech sounds, so we conducted phonetic classification experiments using the TIMIT database (Garofolo et al., 1993). We would like to investigate the relation of learning capability of semi-supervised algorithms to other factors and generalize our observations to other data sets. Therefore, we used another synthetic dataset Waveform for the evaluation of semi-supervised learning algorithms for Gaussian Mixture model. TIMIT: We used the same 48 phone classes and further grouped into 39 classes according to (Lee and Hon, 1989) as our final set of phone classes to model. We extracted 50 speakers out of the NIST complete test set to form the development set. All of our experimental analyses were on the develop</context>
</contexts>
<marker>Garofolo, Lamel, Fisher, Fiscus, Pallett, Dahlgren, 1993</marker>
<rawString>J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren. 1993. Darpa timit acoustic phonetic continuous speech corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K Halberstadt</author>
</authors>
<title>Heterogeneous Acoustic Measurements and Multiple Classifiers for Speech Recognition.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="12763" citStr="Halberstadt, 1998" startWordPosition="2112" endWordPosition="2113">ation of learning capability of semi-supervised algorithms to other factors and generalize our observations to other data sets. Therefore, we used another synthetic dataset Waveform for the evaluation of semi-supervised learning algorithms for Gaussian Mixture model. TIMIT: We used the same 48 phone classes and further grouped into 39 classes according to (Lee and Hon, 1989) as our final set of phone classes to model. We extracted 50 speakers out of the NIST complete test set to form the development set. All of our experimental analyses were on the development set. We used segmental features (Halberstadt, 1998) in the phonetic classification task. For each phone occurrence, a fixed-length vector was calculated from the frame-based spectral features (12 PLP coefficients plus energy) with a 5 ms frame rate and a 25 ms Hamming window. More specifically, we divided the frames for each phone into three regions with 3-4-3 proportion and calculated the PLP average over each region. Three averages plus the log duration of that phone gave a 40-dimensional (13 x 3 + 1) measurement vector. Waveform: We used the second versions of the Waveform dataset available at the UCI repository (Asuncion and Newman, 2007).</context>
</contexts>
<marker>Halberstadt, 1998</marker>
<rawString>Andrew K. Halberstadt. 1998. Heterogeneous Acoustic Measurements and Multiple Classifiers for Speech Recognition. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-T Huang</author>
<author>Mark Hasegawa-Johnson</author>
</authors>
<title>Maximum mutual information estimation with unlabeled data for phonetic classification. In Interspeech.</title>
<date>2008</date>
<contexts>
<context position="3721" citStr="Huang and Hasegawa-Johnson (2008)" startWordPosition="561" endWordPosition="564">ect the optimal weight to balance between the log likelihood of labeled and unlabeled data when training HMMs. Besides generative training of acoustic models, discriminative training is another popular paradigm in the area of speech recognition, but only when 75 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 75–83, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics the transcriptions are available. Wang and Woodland (2007) used the self-training method to augment the training set for discriminative training. Huang and Hasegawa-Johnson (2008) investigated another use of discriminative information from labeled data by replacing the likelihood of labeled data with the class posterior probability of labeled data in the semi-supervised training objective for Gaussian Mixture Models (GMM), resulting in a hybrid discriminative/generative objective function. Their experimental results in binary phonetic classification showed significant improvement in classification accuracy when labeled data are scarce. A similar strategy called ”‘multi-conditional learning”’ was presented in (Druck et al., 2007) applied to Markov Random Field models fo</context>
</contexts>
<marker>Huang, Hasegawa-Johnson, 2008</marker>
<rawString>J.-T. Huang and Mark Hasegawa-Johnson. 2008. Maximum mutual information estimation with unlabeled data for phonetic classification. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masashi Inoue</author>
<author>Naonori Ueda</author>
</authors>
<title>Exploitation of unlabeled sequences in hidden markov models.</title>
<date>2003</date>
<journal>IEEE Trans. On Pattern Analysis and Machine Intelligence,</journal>
<pages>25--1570</pages>
<contexts>
<context position="2837" citStr="Inoue and Ueda (2003)" startWordPosition="432" endWordPosition="435">dure described above exactly lies in the context of semi-supervised learning problems and can be categorized as a self-training algorithm. Self-training is probably the simplest semisupervised learning method, but it is also flexible to be applied to complex classifiers such as speech recognition systems. This may be the reason why little work has been done on exploiting other semisupervised learning methods in speech recognition. Though not incorporated to speech recognizers yet, there has been some work on semi-supervised learning of Hidden Markov Models (HMM) for sequential classification. Inoue and Ueda (2003) treated the unknown class labels of the unlabeled data as hidden variables and used the expectation-maximization (EM) algorithm to optimize the joint likelihood of labeled and unlabeled data. Recently Ji et al. (2009) applied a homotopy method to select the optimal weight to balance between the log likelihood of labeled and unlabeled data when training HMMs. Besides generative training of acoustic models, discriminative training is another popular paradigm in the area of speech recognition, but only when 75 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language</context>
</contexts>
<marker>Inoue, Ueda, 2003</marker>
<rawString>Masashi Inoue and Naonori Ueda. 2003. Exploitation of unlabeled sequences in hidden markov models. IEEE Trans. On Pattern Analysis and Machine Intelligence, 25:1570–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shihao Ji</author>
<author>Layne T Watson</author>
<author>Lawrence Carin</author>
</authors>
<title>Semisupervised learning of hidden markov models via a homotopy method.</title>
<date>2009</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="3055" citStr="Ji et al. (2009)" startWordPosition="465" endWordPosition="468">o flexible to be applied to complex classifiers such as speech recognition systems. This may be the reason why little work has been done on exploiting other semisupervised learning methods in speech recognition. Though not incorporated to speech recognizers yet, there has been some work on semi-supervised learning of Hidden Markov Models (HMM) for sequential classification. Inoue and Ueda (2003) treated the unknown class labels of the unlabeled data as hidden variables and used the expectation-maximization (EM) algorithm to optimize the joint likelihood of labeled and unlabeled data. Recently Ji et al. (2009) applied a homotopy method to select the optimal weight to balance between the log likelihood of labeled and unlabeled data when training HMMs. Besides generative training of acoustic models, discriminative training is another popular paradigm in the area of speech recognition, but only when 75 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 75–83, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics the transcriptions are available. Wang and Woodland (2007) used the self-training method to augment the training </context>
</contexts>
<marker>Ji, Watson, Carin, 2009</marker>
<rawString>Shihao Ji, Layne T. Watson, and Lawrence Carin. 2009. Semisupervised learning of hidden markov models via a homotopy method. IEEE Trans. Pattern Anal. Mach. Intell., 31(2):275–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J F Gales L Wang</author>
<author>P C Woodland</author>
</authors>
<title>Unsupervised training for mandarin broadcast news and conversation transcription.</title>
<date>2007</date>
<booktitle>In Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<volume>4</volume>
<pages>353--356</pages>
<contexts>
<context position="1579" citStr="Wang and Woodland, 2007" startWordPosition="233" endWordPosition="236">led data improve training on models of higher complexity in which the supervised method performs poorly. In addition, there is a trend that more unlabeled data results in more improvement in classification accuracy over the supervised model. We also provided experimental observations on the relative weights of labeled and unlabeled parts of the training objective and suggested a critical value which could be useful for selecting a good weighing factor. 1 Introduction Speech recognition acoustic models can be trained using untranscribed speech data (Wessel and Ney, 2005; Lamel et al., 2002; L. Wang and Woodland, 2007). Most such experiments begin by boostraping ∗This research is funded by NSF grants 0534106 and 0703624. an initial acoustic model using a limited amount of manually transcribed data (normally in a scale from 30 minutes to several hours), and then the initial model is used to transcribe a relatively large amount of untranscribed data. Only the transcriptions with high confidence measures (Wessel and Ney, 2005; L. Wang and Woodland, 2007) or high agreement with closed captions (Lamel et al., 2002) are selected to augment the manually transcribed data, and new acoustic models are trained on the </context>
<context position="3600" citStr="Wang and Woodland (2007)" startWordPosition="543" endWordPosition="547">e the joint likelihood of labeled and unlabeled data. Recently Ji et al. (2009) applied a homotopy method to select the optimal weight to balance between the log likelihood of labeled and unlabeled data when training HMMs. Besides generative training of acoustic models, discriminative training is another popular paradigm in the area of speech recognition, but only when 75 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 75–83, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics the transcriptions are available. Wang and Woodland (2007) used the self-training method to augment the training set for discriminative training. Huang and Hasegawa-Johnson (2008) investigated another use of discriminative information from labeled data by replacing the likelihood of labeled data with the class posterior probability of labeled data in the semi-supervised training objective for Gaussian Mixture Models (GMM), resulting in a hybrid discriminative/generative objective function. Their experimental results in binary phonetic classification showed significant improvement in classification accuracy when labeled data are scarce. A similar stra</context>
</contexts>
<marker>Wang, Woodland, 2007</marker>
<rawString>M.J.F. Gales L. Wang and P.C. Woodland. 2007. Unsupervised training for mandarin broadcast news and conversation transcription. In Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 4, pages 353–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Lamel</author>
<author>Jean-Luc Gauvain</author>
<author>Gilles Adda</author>
</authors>
<title>Lightly supervised and unsupervised acoustic model training.</title>
<date>2002</date>
<pages>16--115</pages>
<contexts>
<context position="1550" citStr="Lamel et al., 2002" startWordPosition="228" endWordPosition="231">or both methods, unlabeled data improve training on models of higher complexity in which the supervised method performs poorly. In addition, there is a trend that more unlabeled data results in more improvement in classification accuracy over the supervised model. We also provided experimental observations on the relative weights of labeled and unlabeled parts of the training objective and suggested a critical value which could be useful for selecting a good weighing factor. 1 Introduction Speech recognition acoustic models can be trained using untranscribed speech data (Wessel and Ney, 2005; Lamel et al., 2002; L. Wang and Woodland, 2007). Most such experiments begin by boostraping ∗This research is funded by NSF grants 0534106 and 0703624. an initial acoustic model using a limited amount of manually transcribed data (normally in a scale from 30 minutes to several hours), and then the initial model is used to transcribe a relatively large amount of untranscribed data. Only the transcriptions with high confidence measures (Wessel and Ney, 2005; L. Wang and Woodland, 2007) or high agreement with closed captions (Lamel et al., 2002) are selected to augment the manually transcribed data, and new acoust</context>
</contexts>
<marker>Lamel, Gauvain, Adda, 2002</marker>
<rawString>Lori Lamel, Jean-Luc Gauvain, and Gilles Adda. 2002. Lightly supervised and unsupervised acoustic model training. 16:115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-F Lee</author>
<author>H-W Hon</author>
</authors>
<title>Speaker-independent phone recognition using hidden markov models.</title>
<date>1989</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>37</volume>
<issue>11</issue>
<contexts>
<context position="12522" citStr="Lee and Hon, 1989" startWordPosition="2068" endWordPosition="2071">n The purpose of designing the learning algorithms is for classification/recognition of speech sounds, so we conducted phonetic classification experiments using the TIMIT database (Garofolo et al., 1993). We would like to investigate the relation of learning capability of semi-supervised algorithms to other factors and generalize our observations to other data sets. Therefore, we used another synthetic dataset Waveform for the evaluation of semi-supervised learning algorithms for Gaussian Mixture model. TIMIT: We used the same 48 phone classes and further grouped into 39 classes according to (Lee and Hon, 1989) as our final set of phone classes to model. We extracted 50 speakers out of the NIST complete test set to form the development set. All of our experimental analyses were on the development set. We used segmental features (Halberstadt, 1998) in the phonetic classification task. For each phone occurrence, a fixed-length vector was calculated from the frame-based spectral features (12 PLP coefficients plus energy) with a 5 ms frame rate and a 25 ms Hamming window. More specifically, we divided the frames for each phone into three regions with 3-4-3 proportion and calculated the PLP average over </context>
</contexts>
<marker>Lee, Hon, 1989</marker>
<rawString>K.-F. Lee and H.-W. Hon. 1989. Speaker-independent phone recognition using hidden markov models. IEEE Transactions on Speech and Audio Processing, 37(11):1641–1648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Phonetic recognition using hidden markov models and maximum mutualinformation training.</title>
<date>1988</date>
<booktitle>In Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<volume>1</volume>
<pages>111--114</pages>
<contexts>
<context position="10513" citStr="Merialdo, 1988" startWordPosition="1730" endWordPosition="1731">ditional likelihood (referred to as the numerator(num) model because it appears in the numerator when computing the class posterior probability) log P (XL|YL; λ) and the marginal likelihoods (referred to as the denominator(den) model likewise) log P (XL; λ) and α log P (XU; λ) respectively. The last term is a smoothing function that doesn’t affect the local differential but ensures that the sum of the first three term is at least a convex weak-sense auxiliary function for good convergence in optimization. Maximization of (5) leads to the update equations where the derivative was approximated (Merialdo, 1988) in the following form for practical robustness for small-valued parameters : P . (11) m′ γden jm′ Under our hybrid framework, there is an extra term γden jm(DU)/ Pm′ γden jm′(DU) that should exist in (11), but in practice we found that adding this term to the approximation is not better than the original form. Therefore, we keep using MMI-only update for mixture weights. The constant C is chosen such that all parameter derivatives are positive. ∂FMMI ≈ ∂cjm den γj m γnum jm P − m′ γnum jm′ 77 2.2 Purely Generative Objective In this paper we compare the hybrid objective with the purely generat</context>
</contexts>
<marker>Merialdo, 1988</marker>
<rawString>B. Merialdo. 1988. Phonetic recognition using hidden markov models and maximum mutualinformation training. In Proc. IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 1, pages 111–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Povey</author>
</authors>
<title>Discriminative Training for Large Vocabulary Speech Recognition.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Cambridge University.</institution>
<contexts>
<context position="8228" citStr="Povey, 2003" startWordPosition="1310" endWordPosition="1311">probability and the likelihood are essentially different, so are their gradients. While the weight α balances the impacts of two components on the training process, it may also implicitly normalize the scales of the two components. In section (3.2) we will discuss and provide a further experimental analysis. In this paper, the models to be trained are Gaussian mixture models of continuous spectral feature vectors for phonetic classes, which can be further extended to Hidden Markov Models with extra parameters such as transition probabilities. The maximization of (1) follows the techniques in (Povey, 2003), which uses auxiliary functions for objective maximization; In each iteration, a strong or weak sense auxiliary function is maximized, such that if the auxiliary function converges after iterations, the objective function will be at a local maximum as well. The objective function (1) can be rewritten as F (λ) = log P (XL|YL; λ) − log P (XL; λ) (4) + α log P (XU; λ), where the term log P (YL; λ) is removed because it is independent of acoustic model parameters. The auxiliary function at the current parameter λold for (4) is for the class j and mixture m given as follows: � xnum jm, − xden jm +</context>
<context position="9511" citStr="Povey, 2003" startWordPosition="1557" endWordPosition="1558">)) − ˆµim, where for clarity the following substitution is used: γjm = γnum jm − γden jm + αγden jm(DU) + Djm (8) and γjm is the sum of the posterior probabilities of occupation of mixture component m of class j over the dataset: γnum jm (X) = X xiEX,yi=j X γden jm(X) = xiEX and xjm and sjm are respectively the weighted sum of xi and xai over the whole dataset with the weight p (m|xi, yi = j) or p (m|xi), depending on whether the superscript is the numerator or denominator model. Djm is a constant set to be the greater of twice the smallest value that guarantees positive variances or γden jm (Povey, 2003). The re-estimation formula for mixture weights is also derived from the Extended Baum-Welch algorithm: 1 ˆµjm = γjm a1 σjm = γjm p (m|xi, yi = j) (9) p (m|xi) G(λ, λ(old)) =Gnum(λ, λ(old)) − Gden(λ, λ(old)) ˆcjm = n ∂F o +αGden(λ, λ(old); DU) + Gsm(λ, λ(old)), cjm ∂cjm + C P n ∂F o, (10) (5) m′ cjm′ ∂cjm + C where the first three terms are strong-sense auxiliary functions for the conditional likelihood (referred to as the numerator(num) model because it appears in the numerator when computing the class posterior probability) log P (XL|YL; λ) and the marginal likelihoods (referred to as the de</context>
</contexts>
<marker>Povey, 2003</marker>
<rawString>Daniel Povey. 2003. Discriminative Training for Large Vocabulary Speech Recognition. Ph.D. thesis, Cambridge University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Lawrence K Saul</author>
</authors>
<title>Large margin hidden markov models for automatic speech recognition.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>1249--1256</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="17603" citStr="Sha and Saul, 2007" startWordPosition="2935" endWordPosition="2938"> accuracy among all models after semi-supervised learning whereas the 2-mixture model is the best model for supervised learning using labeled data only. Experiments on TIMIT show a similar behavior1; as shown in both Figure 3.1 and Table 2, the improvement over the supervised model (α = 0) is also positively correlated to the model complexity, 1Note that our baseline performance (the initial MLE model) is much worse than benchmark because only 10% of the training data were used. We justified our baseline model by using the whole training data and a similar accuracy ( 74%) to other work (e.g. (Sha and Saul, 2007)) was obtained. 79 Table 1: The accuracies(%) of the initial MLE model, the supervised model (α = 0), the best accuracies with unlabeled data and the absolute improvements (A) over α = 0 for different model complexities for waveform. The bolded number is the highest value along the same column. Hybrid Purely generative #. mix init. acc. α = 0 best acc. A α = 0 best acc. A 2 83.02 81.73 83.74 2.01 82.96 83.14 0.18 3 82.08 81.66 84.69 3.03 82.18 84.58 2.40 4 81.56 80.53 83.93 3.40 81.34 84.13 2.79 5 80.18 80.14 83.82 3.68 80.16 83.84 3.68 6 79.61 79.40 83.19 3.79 79.71 83.31 3.60 Table 2: The ac</context>
</contexts>
<marker>Sha, Saul, 2007</marker>
<rawString>Fei Sha and Lawrence K. Saul. 2007. Large margin hidden markov models for automatic speech recognition. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 1249–1256. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wessel</author>
<author>Hermann Ney</author>
</authors>
<title>Unsupervised training of acoustic models for large vocabulary continuous speech recognition.</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="1530" citStr="Wessel and Ney, 2005" startWordPosition="224" endWordPosition="227"> various conditions. For both methods, unlabeled data improve training on models of higher complexity in which the supervised method performs poorly. In addition, there is a trend that more unlabeled data results in more improvement in classification accuracy over the supervised model. We also provided experimental observations on the relative weights of labeled and unlabeled parts of the training objective and suggested a critical value which could be useful for selecting a good weighing factor. 1 Introduction Speech recognition acoustic models can be trained using untranscribed speech data (Wessel and Ney, 2005; Lamel et al., 2002; L. Wang and Woodland, 2007). Most such experiments begin by boostraping ∗This research is funded by NSF grants 0534106 and 0703624. an initial acoustic model using a limited amount of manually transcribed data (normally in a scale from 30 minutes to several hours), and then the initial model is used to transcribe a relatively large amount of untranscribed data. Only the transcriptions with high confidence measures (Wessel and Ney, 2005; L. Wang and Woodland, 2007) or high agreement with closed captions (Lamel et al., 2002) are selected to augment the manually transcribed </context>
</contexts>
<marker>Wessel, Ney, 2005</marker>
<rawString>Frank Wessel and Hermann Ney. 2005. Unsupervised training of acoustic models for large vocabulary continuous speech recognition. IEEE Transactions on Speech and Audio Processing, 13(1):23–31, January.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>