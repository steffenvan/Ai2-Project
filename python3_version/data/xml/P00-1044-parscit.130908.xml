<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994988">
Difficulty Indices for the Named Entity Task in Japanese
</title>
<author confidence="0.987395">
Chikashi NOBATA
</author>
<affiliation confidence="0.994166">
Kansai Advanced Research Center, Communications Research Laboratory
</affiliation>
<address confidence="0.858762">
588-2 Iwaoka, Iwaoka-cho, Nishi-ku, Kobe, Hyogo 651-2492, JAPAN
</address>
<email confidence="0.929105">
nova@crl.go. jp
</email>
<author confidence="0.973947">
Satoshi SEKINE
</author>
<affiliation confidence="0.991126">
Computer Science Department, New York University
</affiliation>
<address confidence="0.897089">
715 Broadway, 7th floor, New York, NY 10003, USA
</address>
<email confidence="0.341617">
sekine@c s . nyu . edu
</email>
<author confidence="0.908917">
Jun&apos;ichi TSUJII
</author>
<affiliation confidence="0.996953">
Department of Information Science, University of Tokyo
</affiliation>
<address confidence="0.88505">
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, JAPAN
</address>
<email confidence="0.997473">
tsujii@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.996737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999653333333334">
We propose indices to measure the
difficulty of the named entity (NE)
task by looking at test corpora,
based on expressions inside and out-
side the NEs. These indices are in-
tended to estimate the difficulty of
each task without actually using an
NE system and to be unbiased to-
wards a specific system. The values
of the indices are compared with the
systems&apos; performance in Japanese
documents. We also discuss the dif-
ference between NE classes with the
indices and show useful clues which
will make it easier to recognize NEs.
</bodyText>
<sectionHeader confidence="0.998735" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999533847826087">
The named entity (NE) task is one of the sub-
tasks of information extraction (IE), which
were defined in the 6th Message Understand-
ing Conference (MUC-6)(1995) to improve
modularity and portability of IE systems.
The main objective of the NE task is to iden-
tify some specific noun phrases in the tar-
get documents. The evaluations of the sys-
tems for the NE tasks have been performed
in MUCs, there are 7 entity classes defined
to identify, such as ORGANIZATION, PER-
SON, LOCATION, DATE, TIME, MONEY
and PERCENT. The type of the articles is
restricted for the final goal, information ex-
traction. The target events are executive suc-
cession in MUC-6, and space vehicle or rocket
launching in MUC-7 (1998), while the classes
of NE were the same in both conferences.
IREX (1999) was the workshop for the eval-
uation of information retrieval (IR) and NE
task held in Japan. The documents used in
these tasks are Japanese newspaper articles.
The NE task in IREX is basically based on
that in MUCs, and ARTIFACT class is newly
defined, which contains the name of products,
laws, prizes.
In the IREX NE tasks, we have a dry run
and a formal run evaluation. There are two
tasks in the formal run. One task is called
in this paper a &amp;quot;general&amp;quot; task. In the general
task, the type of documents is not specified,
therefore NE systems need to deal with all the
types of newspaper articles, similar to that in
the dry run. We call the other task a &amp;quot;re-
stricted&amp;quot; task. In the restricted task, articles
are restricted to the ones which describe ar-
rest events by the police.
The MUCs and the IREX workshop have
evaluated the performances of the IE sys-
tems, and the tendency of system perfor-
mances show partially the difficulty of the IE
tasks. However, the evaluation of the systems
do not clearly show in detail why the task is
difficult or what kind of information should
utilized in the IE task. The investigation into
the test corpora are necessary to evaluate the
</bodyText>
<tableCaption confidence="0.994471">
Table 1: Numerical comparison between tasks Table 2: Average F-measures in the IREX NE
</tableCaption>
<note confidence="0.99705375">
Dry run General Restricted
Articles 36 72 20
Words 11173 21321 4892
Characters 20712 39205 8990
</note>
<bodyText confidence="0.9834442">
performance of the systems more precisely.
There is some research that estimates the
difficulty of the IE tasks by looking at the
test corpora. Bagga et. al (1997) created a
semantic network from test corpora in MUCs,
and set the level of the events based on the
created network to analyze the performances
of the IE systems. In the NE tasks, Palmer
et. al (1997) investigated the test corpora
at Multi-lingual Entity task (1996) and esti-
mated the lower bound of the performance of
NE task in six different languages.
In this paper, we propose some indices to
evaluate the difficulty of the named entity
task by looking at a test corpus, based on ex-
pressions inside and outside the entities. The
basic notion to evaluate the difficulty is the
variety of expression. To identify named enti-
ties properly, a certain amount of knowledge
is necessary. We make the hypothesis that the
greater the variety of expression is, the more
effort to create knowledge would be required.
The validity of these indices is evaluated by
the correlation with the NE systems which are
the participants in the IREX workshop.
First, we introduce the NE tasks performed
in the IREX workshop in Section 2, then de-
fine indices to estimate the difficulty of the
NE tasks. Section 3 describes the index based
on the frequency of tokens. Entities, words
and characters are checked as candidates of
tokens. In Section 4, we have another point
of view to describe the method which extracts
useful tokens related with each NE class. A
set of these tokens show the easiness of rec-
ognizing entities in each class, that is, the in-
verted index for the difficulty. In Section 5,
the indices based on expressions around enti-
ties are described and evaluated.
tasks
</bodyText>
<table confidence="0.9988946">
Class Dry run General Restricted
Org. 0.56 0.57 0.55
Person 0.71 0.68 0.69
Loc. 0.66 0.70 0.68
Art. 0.19 0.26 0.58
Date 0.84 0.86 0.89
Time 0.69 0.83 0.90
Money 0.91 0.86 0.91
Percent 1.00 0.86
All 0.66 0.70 0.75
</table>
<sectionHeader confidence="0.957566" genericHeader="method">
2 IREX NE tasks
</sectionHeader>
<bodyText confidence="0.99998916">
In Table 1, the basic statistics in the IREX
NE tasks are shown, such as the number
of articles, words and characters of the test
corpora, and the average F-measureslin the
IREX NE tasks are shown in Table 2. In the
general task and restricted task of the formal
run, we compared the index with the average
F-measures of the 15 NE systems, which are
the participants of the IREX workshop. In
the dry run, we used the F-measure which is
the result of University of Tokyo&apos;s NE system
because we don&apos;t have the results of all the
participants in the dry run.
In Table 2, we can see that entities in
the first four NE classes (ORGANIZATION,
PERSON, LOCATION and ARTIFACT) are
more difficult to be identified than those in
the second four NE classes (DATE, TIME,
MONEY and PERCENT). We therefore dis-
cuss the two groups separately if it is neces-
sary. We call the first four NE classes the
&amp;quot;ENAMEX&amp;quot; group, and the second four NE
classes the &amp;quot;TIMEX-NUMEX&amp;quot; group in the
following part of this paper, based on the de-
notation of the MUCs.
</bodyText>
<footnote confidence="0.998311285714286">
1F-measure is defined and used in MUCs, which is
a measurement combining Recall and Precision. Re-
call is the percentage of the correct answers among the
answers in the key provided by human. Precision is
the percentage of the correct answers among the an-
swers proposed by the system. F-measure is defined
using Recall(R) and Precision(P) as 2PRI(P + R).
</footnote>
<tableCaption confidence="0.8591575">
Table 3: Number of different entities in NE Table 4: Values of Frequency of Entities (FE)
classes
</tableCaption>
<table confidence="0.9993491">
Class Dry run General Restricted
Org. 131 187 48
Person 113 217 71
Loc. 89 191 78
Art. 31 39 9
Date 71 126 49
Time 16 32 15
Money 28 13 7
Percent 6 16 -
All 482(485) 818(821) 277
</table>
<sectionHeader confidence="0.844663" genericHeader="method">
3 Frequency of tokens
</sectionHeader>
<subsectionHeader confidence="0.999342">
3.1 Frequency of entities
</subsectionHeader>
<bodyText confidence="0.999655607142857">
At first, we will define an index that shows
the difficulty of NE tasks using the frequency
of tokens and also the variety of them. The
assumption here is that a variety of expres-
sions make it difficult to define entities of NE
class or create knowledge base for the class.
Table 3 shows the number of different enti-
ties in NE classes for the tasks in the IREX
workshop. The number of PERCENT enti-
ties in the restricted task remains blank, be-
cause the test corpus doesn&apos;t have any PER-
CENT entities. The number of different enti-
ties in &amp;quot;All&amp;quot; is a little fewer than the sum of
the number of different entities in each class.
The reason is that three entities are catego-
rized into more than one class in the dry run
and in the general task of the formal run, re-
spectively. Although these entities also make
the NE task difficult, we don&apos;t use them as an
index because the number of entities is small.
The number of different entities alone is in-
fluenced by the corpus size and not suitable
for the index that should uniformly show the
difficulty for different NE tasks, therefore it
should be normalized. The first index for the
difficulty of NE we introduce is the normal-
ized number of different entities. The index,
Frequency of Entities(FE), is defined in Equa-
</bodyText>
<equation confidence="0.70460325">
tion 1:
DE
FE= (1)
NE
</equation>
<tableCaption confidence="0.377931333333333">
DE denotes the number of different entities in
a class, and NE is the frequency of entities in
a class.
</tableCaption>
<table confidence="0.997621684210526">
Class Dry run General Restricted
Org. 0.61 0.48 0.65
(=131/214) (=187/389) (=48/ 74)
Person 0.67 0.61 0.73
(=113/169) (=217/355) (=71/ 97)
Loc. 0.46 0.46 0.74
(=89/192) (=190/416) (=78/106)
Art. 0.71 0.80 0.69
(=30/ 42) (=39/ 49) (=9/ 13)
Date 0.33 0.18 0.24
(=36/110) (=51/277) (=17/ 72)
Time 0.46 0.27 0.53
(=11/ 24) (=16/ 59) (=10/ 19)
Money 0.09 0.13 0.12
(= 3/ 33) (= 2/ 15) (= 1/ 8)
Percent 0.50 0.29
(= 3/ 6) (= 6/ 21)
All 0.53 0.45 0.60
(=415/790) (=706/1581) (=235/389)
</table>
<bodyText confidence="0.961310769230769">
When FE is calculated, numeral figures
are considered to be the same, and replaced
by the meta character &apos;#&apos;. In the NE
classes, temporal(DATE, TIME) or numeri-
cal(MONEY, PERCENT) ones can be iden-
tified more easily than the others which will
result from the uniformity of numerals. When
each distinct numeral is recognized as the
same character, the number of different en-
tities is reduced in temporal or numerical NE
classes and match our intuition better. The
values of FE for each class in different tasks
are shown in Table 4.
</bodyText>
<subsectionHeader confidence="0.999912">
3.2 Frequency of words / characters
</subsectionHeader>
<bodyText confidence="0.984837133333333">
We can define another indices, Frequency
of Words(FW) and Frequency of Charac-
ter(FC), since entities in the definition of FE
can be replaced with words or characters to
estimate the difficulty of the NE task. The
expectation here is that words or characters
have higher frequency than the entire entities,
therefore the index based on words or charac-
ters will be more robust than entities with
regard to the corpus size.
Word segmentation for calculating FW is
performed using a morphological analyzer JU-
MAN (Matsumoto et al., 1997). When the
boundary of an entity is different from the re-
sult of segmentation, the word on the edge
</bodyText>
<tableCaption confidence="0.937883">
Table 5: Values of Frequency of Charac- Table 6: Correlation coefficients between in-
</tableCaption>
<table confidence="0.979469285714286">
ters(FC) dices and F-measure
Task FE FW FC
Class Dry run General Restricted
Org. 0.29 0.20 0.38
(=258/883) (=365/1792) (=139/365)
Person 0.39 0.26 0.48
(=222/575) (=319/1228) (=148/311)
Loc. 0.30 0.19 0.34
(=186/618) (=284/1491) (=155/462)
Art. 0.53 0.50 0.58
(=131/245) (=175/ 347) (= 34/ 59)
Date 0.16 0.07 0.07
(= 44/282) (= 54/ 737) (= 15/226)
Time 0.18 0.09 0.14
(= 12/ 66) (= 16/ 182) (= 10/ 71)
Money 0.06 0.09 0.12
(= 4 / 72) (= 3/ 34) (= 2/ 16)
Percent 0.38 0.10
(= 5 / 13) (= 7/ 58)
All 0.20 0.12 0.24
(=555/2754) (=717/5869) (=355/1510)
</table>
<bodyText confidence="0.972487444444444">
of the entity is split again. Each distinct nu-
meral is also recognized as the same charac-
ter in FW and FC. The values of FW have
basically the same tendency as those of FC,
therefore we show only the values of FC in Ta-
ble 5. FC shows the difference between classes
more clearly than FE. The values of FC have
especially decreased in the NE classes of the
TIMEX-NUMEX group.
</bodyText>
<subsectionHeader confidence="0.999748">
3.3 Validity of indices
</subsectionHeader>
<bodyText confidence="0.999824833333333">
We estimated the correlation coefficients be-
tween the defined indices and the F-measure
of the NE systems over eight NE classes, as
shown in Table 6. The greater value of the de-
fined three indices indicates that identifying
entities is more difficult, therefore the nega-
tive correlation with F-measures is desirable
for these indices.
We can see that all of the indices have quite
high correlation with F-measure. While FW,
FC have the lower correlation than FE in the
dry run, they have higher correlation in the
two tasks of the formal run. Considering that
F-measure of the dry run is just one system&apos;s
result, the evaluated performance about the
tasks of the formal run is more reliable. Char-
acters are therefore better tokens than enti-
ties and words to estimate the difficulty of
</bodyText>
<table confidence="0.97268575">
Dry run -0.66 -0.63 -0.61
General -0.91 -0.92 -0.97
Restricted -0.80 -0.87 -0.89
Japanese NE tasks.
</table>
<sectionHeader confidence="0.822626" genericHeader="method">
4 Token index
</sectionHeader>
<bodyText confidence="0.995076733333333">
In this section, we focus on finding useful clues
to identify entities. If a class has some specific
tokens, identifying entities in each class will
be easier than in other classes. In the indices
we defined earlier, only the frequency of each
token in one class is considered. In the previ-
ous assumption, if the tokens often appear in
one class, they are regarded as good indica-
tors of the class, and make the identifying the
entities of the class more easily. However, if
such tokens appear broadly in the whole doc-
ument, our assumption is not correct. The
frequency in the whole corpus should be con-
sidered in addition to the frequency in one
class.
</bodyText>
<subsectionHeader confidence="0.938039">
4.1 Character index
</subsectionHeader>
<bodyText confidence="0.989298375">
We define Character Index for each character
(CI,) to indicate how peculiar the character
is to the class. We chose here characters as
tokens for the new index, since they are shown
to be good indices in the previous result. The
definition itself can be applied for the other
type of tokens. CI, for a character c in a class
L is defined in Equation 2:
</bodyText>
<equation confidence="0.711176666666667">
ni,(c) n (c)
ULC- (2)
Nc n (c)
</equation>
<bodyText confidence="0.513474636363636">
Where, nL (c) is the frequency of c in the
class L, n(c) is the frequency of c in the whole
corpus. /q&apos;, is the number of characters in the
class L. The first item in the right side n ge)
denotes the percentage of the appearance of
the character c in the class L, and the second
term nL (C) denotes how the appearance of the
n(c)
character c is different between in the class L
and in the whole corpus. If c is shown mostly
in the class L, ni,(c) is close to n(c).
</bodyText>
<tableCaption confidence="0.977796">
Table 7: Values of Character Index(CI)
</tableCaption>
<table confidence="0.9996334">
Class Dry run General Restricted
Org. 0.34 0.31 0.45
Person 0.51 0.45 0.59
Location 0.38 0.40 0.56
Artifact 0.21 0.15 0.27
Date 0.39 0.48 0.60
Time 0.36 0.40 0.47
Money 0.47 0.51 0.51
Percent 0.33 0.27
All 0.57 0.58 0.71
</table>
<bodyText confidence="0.955968176470589">
The summation of CI, of characters in the
class can be used for another index to show
the difficulty of NE tasks. The index, Charac-
ter Index(CI), reverses the measure to show
the difficulty. The greater the value is, the
easier identifying expressions in the class is.
Therefore positive correlation with F-measure
is desirable.
Table 7 shows the result of CI about every
class. When all the characters in the class L
are shown only in L, CI becomes the maxi-
mum value 1. On the other hand, when the
characters in the class L uniformly appear in
the entire corpus, CI becomes close to the
NL
minimum value , where N is the number
of characters in the corpus.
</bodyText>
<subsectionHeader confidence="0.995707">
4.2 Validity of CI
</subsectionHeader>
<bodyText confidence="0.999746338461539">
The second column of Table 8 shows the cor-
relation coefficient between CI and perfor-
mances of systems. The results show that
CI is not a so suitable index to illustrate the
difficulty of tasks as the indices we defined
earlier. One possible reason is that CI is the
summation of the CI, values of all the charac-
ters in one class. Characters that have lower
value of CI, are not likely to make it easier
to identify entities in the NE class, so these
characters should be removed when CI is cal-
culated. Some threshold is required to select
the characters related with the class. To de-
cide the suitable threshold for CI, we observed
the change of the correlation between CI and
F-measure in the different IREX tasks when
the threshold for Cr, is changed.
Figure 1 shows the relationship between the
correlation coefficient and the threshold for
CI,. The axis for the threshold for CI, has a
logarithmic scale. We can see that the corre-
lation coefficient between the indices and F-
measures once increased and then decreased
in every task when the threshold is gradu-
ally lowered. The best correlation coefficients
with F-measure from the graph are shown
with the thresholds in the column &amp;quot;peak&amp;quot; of
Table 8. These values are comparable with
the results of the previous indices, although
the thresholds are simply obtained by com-
parisons with the performance of NE systems.
Not all NE classes require such thresholds
for CI,. To show the distribution of the
CI more clearly, we investigated the CI, val-
ues for characters in the two groups sepa-
rately, the ENAMEX group and the TIMEX-
NUMEX group. Figure 2 shows that the
value of CI, for each character when charac-
ters are listed in the descending order. We
can see that some characters are strongly re-
lated with NE classes in the TIMEX-NUMEX
group, while such characters do not appear in
the ENAMEX group.
This result implies that a lot of charac-
ters are related with the NE classes in the
ENAMEX group, but the contribution of each
character is small because of the low fre-
quency. On the other hand, a small num-
ber of characters are strongly related with
NE classes in the TIMEX-NUMEX group,
and the combination of these characters and
numerals covers most of the entities in the
classes. Consequently, the threshold for CI,
is necessary to the NE classes in the TIMEX-
NUMEX group, because it is expected that
the number of characters used by the NE sys-
tems are restricted.
Figure 3 shows the relationship between the
correlation coefficient and the threshold for
CI, about the NE classes in the ENAMEX
group, and Figure 4 is about those in the
TIMEX-NUMEX group. We can see that the
two groups have the completely different ten-
dency about the correlation with F-measures,
and that the above observation is true.
</bodyText>
<figure confidence="0.99776725">
Value of Cic
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
</figure>
<tableCaption confidence="0.9534485">
Table 8: Correlation coefficients between C/s
and F-measure
</tableCaption>
<table confidence="0.90672525">
Task CI peak(threshold)
Dry run 0.62 0.86(C/, = 0.005)
General 0.75 0.88(C/, = 0.004)
Restricted 0.49 0.96(C/, = 0.009)
</table>
<figureCaption confidence="0.990027">
Figure 1: Change of correlation between CI
and F-measure
</figureCaption>
<subsectionHeader confidence="0.989185">
4.3 Characters selected by CI
</subsectionHeader>
<bodyText confidence="0.999720380952381">
Table 9 shows the characters which have high
CI, value in the NE classes of the TIMEX-
NUMEX group. The target task is the general
one of the formal run. The character &amp;quot;#&amp;quot;
indicates the entire numerals.
We can see there are a few characters which
have comparatively high CI, value in the NE
classes of the TIMEX-NUMEX group. Espe-
cially in the MONEY and PERCENT class,
only one character contributes most of the CI
value of the class. It is natural that the char-
acter &amp;quot;PI&amp;quot; (the unit of Japanese currency) is
dominant in the MONEY class, and that the
character &amp;quot;%&amp;quot; corresponds to the PERCENT
class. Other characters are also intuitively pe-
culiar to the corresponding class. Though the
frequency of numerals in the corpus is quite
large, the above clues are necessary to connect
numerals into each class, because numerals
can often appear in any classes in the TIMEX-
NUMEX group.
</bodyText>
<sectionHeader confidence="0.332901" genericHeader="method">
5 Indices based on context words
</sectionHeader>
<bodyText confidence="0.3125315">
2 4 6 8 10 12 14 16 18 20
Order of characters
</bodyText>
<figureCaption confidence="0.92155175">
Figure 2: Values of CI, for each character
Figure 3: Correlation coefficients between
C/s and F-measure in the NE classes of the
ENAMEX group
</figureCaption>
<bodyText confidence="0.997460625">
entities is not sufficient to describe the diffi-
culty of NE tasks. Even when the class itself
has various entities, they can be easily iden-
tified if the surrounding words are restricted,
i.e. prefixes and suffixes which are not a part
of entity. Therefore, it will be reasonable to
define the index based on expression around
entities.
</bodyText>
<subsectionHeader confidence="0.989855">
5.1 Context word index
</subsectionHeader>
<bodyText confidence="0.999797571428571">
The index based on words around entities,
Context Word Index (CWI), is defined in
Equation 3, which is similar to that of CI.
The difference is that in is added at the de-
nominator of the second term. The in denotes
the range of words that are regarded as the
context words.
</bodyText>
<figure confidence="0.999455921052631">
Dry run -
_ General
Restricted .
0.9
1
0.1
i
0.8
0.7
0.6
05
0.4
Correlation Coefficient with F-measure
0.3
le 05 0.0001 0.001 0.01
Threshold for Clc
Dry run:ENAMEX -.-
General:ENAMEX
Restricted: ENAMEX
Dry run:TIMEX-NUMEX -.-
General:TIMEX-NUMEX
Restncted:TIMEX-NUMEX
i
Dry run
General *
Restricted
0.8
le-05 0.0001 0.001 0.01 0.1 1
Threshold for Clc in the ENAMEX group
Correlation Coefficient with F-measure
08
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
</figure>
<footnote confidence="0.3980235">
In this section, we consider expressions
around entities. Only the observation in the
</footnote>
<page confidence="0.329239">
CW/z=
</page>
<figure confidence="0.8426285">
w Nbw m • n(w)
znL(w) nL(w)
(3)
-0.6
le 05 0.0001 0.001 0.01 0.1 1
Threshold for Clc in the TIMEX-NUMEX group
</figure>
<figureCaption confidence="0.907401">
Figure 4: Correlation coefficients between
</figureCaption>
<tableCaption confidence="0.8333195">
C/s and F-measure in the NE classes of the
TIMEX-NUMEX group
Table 9: High CI, characters in the TIMEX-
NUMEX group
</tableCaption>
<table confidence="0.999669368421053">
Class Cie nL (c) Character
Date 0.1113 277 4 (numerals)
0.1071 143 H (day)
0.0931 75 )1 (month)
0.0893 98 4e (year)
0.0421 31 Pi (last)
Time 0.1868 34 1 (noon)
0.0586 32 FI# (time)
0.0368 23 tk (after)
0.0352 8 A (night)
0.0330 6 9 (evening)
Money 0.4412 15 P1 (currency unit)
0.0588 2 (currency unit)
0.0091 17 4
Percent 0.1379 8 %
0.0616 5 iii (times)
0.0276 4 * (half)
0.0212 4 III (tenth of)
0.0134 27 4
</table>
<bodyText confidence="0.98875625">
Table 10 shows the values of CW/, when the
range of words is set to 1 word. The highest
value among NE classes for each task is in
bold type.
</bodyText>
<subsectionHeader confidence="0.977042">
5.2 Words selected by CW/
</subsectionHeader>
<bodyText confidence="0.997636875">
Table 11 shows the values of the correlation
coefficient between CW/s and F-measures.
There is not so strong correlation as shown
in the previous indices. We have to say this
index does not provide enough information
to show how the expressions around entities
should be utilized. However, we can see some
context words which seem useful for the NE
</bodyText>
<tableCaption confidence="0.97942">
Table 11: Correlation coefficients between
CW/s and F-measure
</tableCaption>
<table confidence="0.9906068">
CWIfol: Following words
Task m=1 m=2 m=3 m=4
Dry run -0.01 -0.24 -0.07 -0.09
General 0.14 0.29 0.00 0.02
Restricted 0.06 0.46 0.36 0.10
</table>
<tableCaption confidence="0.996381">
Table 12: High CWIpre words of TIME enti-
ties in the general task
</tableCaption>
<table confidence="0.992769125">
CWIprew nL(w) Word
0.1805 35 # II (the #th day)
0.0920 8 IA li (the same day)
0.0086 1 # IP 4 A#IEf
(the #th day of the #th month in the #th year)
0.0067 5 IP (the same)
0.0057 1 PVIP#11 #Ef
(the #th day of the #th month last year)
</table>
<bodyText confidence="0.999908772727273">
task. We show some examples in Table 12,
13, and Table 14. Figures beside each word
in tables means the value of CW/„, and the
frequency around the class (nL(w)).
In Table 10, TIME class has relatively high
CWIpre values in all tasks than any other
class. As shown in Table 12, the reason is
that TIME expressions usually follow DATE
entities. PERSON class has also relatively
high CWIfol values than any other class in
all tasks, and the example words in Table 13
show that common suffixes and titles often
follow person names. These words should
be helpful for identifying the name of people
in texts. ARTIFACT, MONEY and TIME
classes have high CW/ value in the restricted
task of the formal run, and one word con-
tributes most of the part as shown in Table 14.
The reason would be that the articles in the
test corpus are restricted to the arrest events,
and that the usage of words are rather fixed
than other types of articles.
</bodyText>
<sectionHeader confidence="0.995195" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999636">
We defined indices to show the difficulty of the
NE tasks and evaluate the validity by compar-
</bodyText>
<figure confidence="0.986320555555556">
0.67 0.39 0.46 0.20
Restricted
CWIpre: Preceding words
Task m=1 m=2 m=3 m=4
Dry run
General
-0.07 -0.34 -0.53 -0.49
0.66 -0.01 -0.01 -0.04
Correlation Coefficient with F-measure
</figure>
<tableCaption confidence="0.996698">
Table 10: Values of Context Word Index(CWI)
</tableCaption>
<table confidence="0.999542545454545">
Class Dry run General Restricted
Preceding Following Preceding Following Preceding Following
Org. 0.23 0.30 0.16 0.22 0.15 0.20
Person 0.18 0.47 0.17 0.53 0.16 0.58
Loc. 0.22 0.35 0.20 0.21 0.29 0.27
Art. 0.09 0.10 0.05 0.18 0.02 0.56
Date 0.13 0.25 0.15 0.22 0.14 0.33
Time 0.29 0.07 0.29 0.20 0.44 0.40
Money 0.14 0.20 0.25 0.28 0.37 0.45
Percent 0.07 0.04 0.12 0.27
All 0.32 0.41 0.30 0.36 0.34 0.43
</table>
<tableCaption confidence="0.983534">
Table 13: High CWIfol words of PERSON
entities
</tableCaption>
<table confidence="0.539604">
Task CWIfol. nL(w) Word
Restricted 0.0471 30 9.6-&amp;quot;Rit&apos; (suspect)
Restricted 0.0407 33 (
Dry run 0.0406 28 .E (Mr. or Ms.)
General 0.0370 54 A) (Mr. or Ms.)
Restricted 0.0340 13 Ai
Dry run 0.0228 17 Ai
General 0.0214 29 .E
General 0.0170 28 7
General 0.0164 25 *V( (defendant)
</table>
<bodyText confidence="0.999719428571429">
ing them with the performance of the NE sys-
tems which participated to the IREX work-
shop. The correlation coefficients between the
defined indices and the system performances
are quite high, the best result is 0.97. We also
proposed the methods to select useful charac-
ters or words to make it easier to recognize
named entities, and showed examples.
We would like to improve the index based
on the contextual expressions around entities
further, and our ultimate goal is to automat-
ically acquire useful information in order to
recognize the named entities in the given do-
main.
</bodyText>
<sectionHeader confidence="0.99191" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.958364666666667">
We would like to thank our colleagues at Uni-
versity of Tokyo. In particular, Dr. Nigel
Collier gave us help and suggestions.
</bodyText>
<sectionHeader confidence="0.987007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995236777777778">
ARPA. 1996. Message Understanding Evaluation
and Conference.
Amit Bagga and Alan W. Biremann. 1997.
Analyzing the Complexity of a Domain
With Respect To An Information Extraction
Task. In The Tenth International Confer-
ence on Research on Computational Linguis-
tics(ROCLING X), pages 175-184, August.
DARPA. 1995. Proceedings of the Sixth Message
Understanding Conference(MUC-6), Columbia,
MD, USA, November. Morgan Kaufmann.
DARPA. 1998. Proceedings of the Seventh Mes-
sage Understanding Conference(MUC-7), Fair-
fax, VA, USA, May.
IREX Committee. 1999. Proceedings of the IREX
Workshop, KKR Hotel, Tokyo, Japan, Septem-
ber.
Yuji Matsumoto, Sadao Kurohashi, Osamu Ya-
maji, Yuu Taeki, and Makoto Nagao, 1997.
Japanese morphological analyzing System: JU-
MAN. Kyoto University, Nara Institute of Sci-
ence and Technology.
David D. Palmer and David S. Day. 1997. A
Statistical Profile of the Named Entity Task.
In Proceedings of the Fifth Conference on Ap-
plied Natural Language Processing (ANLP&apos;97),
pages 190-193.
</reference>
<tableCaption confidence="0.752752">
Table 14: High CWIfol words in the restricted
</tableCaption>
<figure confidence="0.5168288">
task
Class CWIfol Word
Artifact 0.5640 0.5470 A&amp; (violation)
Time 0.4015 0.3876 := 6 (about, around)
Money 0.4460 0.3750 C n (equal to)
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.374883">
<title confidence="0.996647">Difficulty Indices for the Named Entity Task in Japanese</title>
<author confidence="0.887096">Chikashi NOBATA</author>
<affiliation confidence="0.991756">Kansai Advanced Research Center, Communications Research Laboratory</affiliation>
<address confidence="0.999207">588-2 Iwaoka, Iwaoka-cho, Nishi-ku, Kobe, Hyogo 651-2492, JAPAN</address>
<email confidence="0.660979">nova@crl.go.jp</email>
<author confidence="0.869267">Satoshi SEKINE</author>
<affiliation confidence="0.999997">Computer Science Department, New York University</affiliation>
<address confidence="0.999982">715 Broadway, 7th floor, New York, NY 10003, USA</address>
<email confidence="0.829215">sekine@cs.nyu.edu</email>
<author confidence="0.91746">Jun&apos;ichi TSUJII</author>
<affiliation confidence="0.999943">Department of Information Science, University of Tokyo</affiliation>
<address confidence="0.993649">Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, JAPAN</address>
<email confidence="0.981345">tsujii@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.9995271875">We propose indices to measure the difficulty of the named entity (NE) task by looking at test corpora, based on expressions inside and outside the NEs. These indices are intended to estimate the difficulty of each task without actually using an NE system and to be unbiased towards a specific system. The values of the indices are compared with the systems&apos; performance in Japanese documents. We also discuss the difference between NE classes with the indices and show useful clues which will make it easier to recognize NEs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ARPA</author>
</authors>
<title>Message Understanding Evaluation and Conference.</title>
<date>1996</date>
<marker>ARPA, 1996</marker>
<rawString>ARPA. 1996. Message Understanding Evaluation and Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Alan W Biremann</author>
</authors>
<title>Analyzing the Complexity of a Domain With Respect To An Information Extraction Task.</title>
<date>1997</date>
<booktitle>In The Tenth International Conference on Research on Computational Linguistics(ROCLING X),</booktitle>
<pages>175--184</pages>
<marker>Bagga, Biremann, 1997</marker>
<rawString>Amit Bagga and Alan W. Biremann. 1997. Analyzing the Complexity of a Domain With Respect To An Information Extraction Task. In The Tenth International Conference on Research on Computational Linguistics(ROCLING X), pages 175-184, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference(MUC-6),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Columbia, MD, USA,</location>
<marker>DARPA, 1995</marker>
<rawString>DARPA. 1995. Proceedings of the Sixth Message Understanding Conference(MUC-6), Columbia, MD, USA, November. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference(MUC-7),</booktitle>
<location>Fairfax, VA, USA,</location>
<marker>DARPA, 1998</marker>
<rawString>DARPA. 1998. Proceedings of the Seventh Message Understanding Conference(MUC-7), Fairfax, VA, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IREX Committee</author>
</authors>
<date>1999</date>
<booktitle>Proceedings of the IREX Workshop, KKR Hotel,</booktitle>
<location>Tokyo, Japan,</location>
<marker>Committee, 1999</marker>
<rawString>IREX Committee. 1999. Proceedings of the IREX Workshop, KKR Hotel, Tokyo, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Sadao Kurohashi</author>
<author>Osamu Yamaji</author>
<author>Yuu Taeki</author>
<author>Makoto Nagao</author>
</authors>
<title>Japanese morphological analyzing System:</title>
<date>1997</date>
<institution>JUMAN. Kyoto University, Nara Institute of Science and Technology.</institution>
<contexts>
<context position="9803" citStr="Matsumoto et al., 1997" startWordPosition="1716" endWordPosition="1719">or each class in different tasks are shown in Table 4. 3.2 Frequency of words / characters We can define another indices, Frequency of Words(FW) and Frequency of Character(FC), since entities in the definition of FE can be replaced with words or characters to estimate the difficulty of the NE task. The expectation here is that words or characters have higher frequency than the entire entities, therefore the index based on words or characters will be more robust than entities with regard to the corpus size. Word segmentation for calculating FW is performed using a morphological analyzer JUMAN (Matsumoto et al., 1997). When the boundary of an entity is different from the result of segmentation, the word on the edge Table 5: Values of Frequency of Charac- Table 6: Correlation coefficients between inters(FC) dices and F-measure Task FE FW FC Class Dry run General Restricted Org. 0.29 0.20 0.38 (=258/883) (=365/1792) (=139/365) Person 0.39 0.26 0.48 (=222/575) (=319/1228) (=148/311) Loc. 0.30 0.19 0.34 (=186/618) (=284/1491) (=155/462) Art. 0.53 0.50 0.58 (=131/245) (=175/ 347) (= 34/ 59) Date 0.16 0.07 0.07 (= 44/282) (= 54/ 737) (= 15/226) Time 0.18 0.09 0.14 (= 12/ 66) (= 16/ 182) (= 10/ 71) Money 0.06 0.0</context>
</contexts>
<marker>Matsumoto, Kurohashi, Yamaji, Taeki, Nagao, 1997</marker>
<rawString>Yuji Matsumoto, Sadao Kurohashi, Osamu Yamaji, Yuu Taeki, and Makoto Nagao, 1997. Japanese morphological analyzing System: JUMAN. Kyoto University, Nara Institute of Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>David S Day</author>
</authors>
<title>A Statistical Profile of the Named Entity Task.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP&apos;97),</booktitle>
<pages>190--193</pages>
<marker>Palmer, Day, 1997</marker>
<rawString>David D. Palmer and David S. Day. 1997. A Statistical Profile of the Named Entity Task. In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP&apos;97), pages 190-193.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>