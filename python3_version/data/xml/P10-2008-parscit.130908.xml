<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.037262">
<title confidence="0.994755">
Authorship Attribution Using Probabilistic Context-Free Grammars
</title>
<author confidence="0.990529">
Sindhu Raghavan Adriana Kovashka Raymond Mooney
</author>
<affiliation confidence="0.994954666666667">
Department of Computer Science
The University of Texas at Austin
1 University Station C0500
</affiliation>
<address confidence="0.612656">
Austin, TX 78712-0233, USA
</address>
<email confidence="0.999612">
{sindhu,adriana,mooney}@cs.utexas.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999602">
In this paper, we present a novel approach
for authorship attribution, the task of iden-
tifying the author of a document, using
probabilistic context-free grammars. Our
approach involves building a probabilistic
context-free grammar for each author and
using this grammar as a language model
for classification. We evaluate the perfor-
mance of our method on a wide range of
datasets to demonstrate its efficacy.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961127272727">
Natural language processing allows us to build
language models, and these models can be used
to distinguish between languages. In the con-
text of written text, such as newspaper articles or
short stories, the author’s style could be consid-
ered a distinct “language.” Authorship attribution,
also referred to as authorship identification or pre-
diction, studies strategies for discriminating be-
tween the styles of different authors. These strate-
gies have numerous applications, including set-
tling disputes regarding the authorship of old and
historically important documents (Mosteller and
Wallace, 1984), automatic plagiarism detection,
determination of document authenticity in court
(Juola and Sofko, 2004), cyber crime investiga-
tion (Zheng et al., 2009), and forensics (Luyckx
and Daelemans, 2008).
The general approach to authorship attribution
is to extract a number of style markers from the
text and use these style markers as features to train
a classifier (Burrows, 1987; Binongo and Smith,
1999; Diederich et al., 2000; Holmes and Forsyth,
1995; Joachims, 1998; Mosteller and Wallace,
1984). These style markers could include the
frequencies of certain characters, function words,
phrases or sentences. Peng et al. (2003) build a
character-level n-gram model for each author. Sta-
matatos et al. (1999) and Luyckx and Daelemans
(2008) use a combination of word-level statistics
and part-of-speech counts or n-grams. Baayen et
al. (1996) demonstrate that the use of syntactic
features from parse trees can improve the accu-
racy of authorship attribution. While there have
been several approaches proposed for authorship
attribution, it is not clear if the performance of one
is better than the other. Further, it is difficult to
compare the performance of these algorithms be-
cause they were primarily evaluated on different
datasets. For more information on the current state
of the art for authorship attribution, we refer the
reader to a detailed survey by Stamatatos (2009).
We further investigate the use of syntactic infor-
mation by building complete models of each au-
thor’s syntax to distinguish between authors. Our
approach involves building a probabilistic context-
free grammar (PCFG) for each author and using
this grammar as a language model for classifica-
tion. Experiments on a variety of corpora includ-
ing poetry and newspaper articles on a number of
topics demonstrate that our PCFG approach per-
forms fairly well, but it only outperforms a bi-
gram language model on a couple of datasets (e.g.
poetry). However, combining our approach with
other methods results in an ensemble that performs
the best on most datasets.
</bodyText>
<sectionHeader confidence="0.965172" genericHeader="method">
2 Authorship Attribution using PCFG
</sectionHeader>
<bodyText confidence="0.999260916666667">
We now describe our approach to authorship at-
tribution. Given a training set of documents from
different authors, we build a PCFG for each author
based on the documents they have written. Given
a test document, we parse it using each author’s
grammar and assign it to the author whose PCFG
produced the highest likelihood for the document.
In order to build a PCFG, a standard statistical
parser takes a corpus of parse trees of sentences
as training input. Since we do not have access to
authors’ documents annotated with parse trees,
we use a statistical parser trained on a generic
</bodyText>
<page confidence="0.99128">
38
</page>
<note confidence="0.50631">
Proceedings of the ACL 2010 Conference Short Papers, pages 38–42,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.982658153846154">
corpus like the Wall Street Journal (WSJ) or
Brown corpus from the Penn Treebank (http:
//www.cis.upenn.edu/˜treebank/)
to automatically annotate (i.e. treebank) the
training documents for each author. In our
experiments, we used the Stanford Parser (Klein
and Manning, 2003b; Klein and Manning,
2003a) and the OpenNLP sentence segmenter
(http://opennlp.sourceforge.net/).
Our approach is summarized below:
Input – A training set of documents labeled
with author names and a test set of documents with
unknown authors.
</bodyText>
<listItem confidence="0.961294533333333">
1. Train a statistical parser on a generic corpus
like the WSJ or Brown corpus.
2. Treebank each training document using the
parser trained in Step 1.
3. Train a PCFG GZ for each author AZ using the
treebanked documents for that author.
4. For each test document, compute its likeli-
hood for each grammar GZ by multiplying the
probability of the top PCFG parse for each
sentence.
5. For each test document, find the author AZ
whose grammar GZ results in the highest like-
lihood score.
Output – A label (author name) for each docu-
ment in the test set.
</listItem>
<sectionHeader confidence="0.99574" genericHeader="method">
3 Experimental Comparison
</sectionHeader>
<bodyText confidence="0.9996485">
This section describes experiments evaluating our
approach on several real-world datasets.
</bodyText>
<subsectionHeader confidence="0.998563">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999905386363637">
We collected a variety of documents with known
authors including news articles on a wide range of
topics and literary works like poetry. We down-
loaded all texts from the Internet and manually re-
moved extraneous information as well as titles, au-
thor names, and chapter headings. We collected
several news articles from the New York Times
online journal (http://global.nytimes.
com/) on topics related to business, travel, and
football. We also collected news articles on
cricket from the ESPN cricinfo website (http:
//www.cricinfo.com). In addition, we col-
lected poems from the Project Gutenberg web-
site(http://www.gutenberg.org/wiki/
Main_Page). We attempted to collect sets of
documents on a shared topic written by multiple
authors. This was done to ensure that the datasets
truly tested authorship attribution as opposed to
topic identification. However, since it is very dif-
ficult to find authors that write literary works on
the same topic, the Poetry dataset exhibits higher
topic variability than our news datasets. We had
5 different datasets in total – Football, Business,
Travel, Cricket, and Poetry. The number of au-
thors in our datasets ranged from 3 to 6.
For each dataset, we split the documents into
training and test sets. Previous studies (Stamatatos
et al., 1999) have observed that having unequal
number of words per author in the training set
leads to poor performance for the authors with
fewer words. Therefore, we ensured that, in the
training set, the total number of words per author
was roughly the same. We would like to note that
we could have also selected the training set such
that the total number of sentences per author was
roughly the same. However, since we would like
to compare the performance of the PCFG-based
approach with a bag-of-words baseline, we de-
cided to normalize the training set based on the
number of words, rather than sentences. For test-
ing, we used 15 documents per author for datasets
with news articles and 5 or 10 documents per au-
thor for the Poetry dataset. More details about the
datasets can be found in Table 1.
</bodyText>
<table confidence="0.864542">
Dataset # authors # words/auth # docs/auth # sent/auth
Football 3 14374.67 17.3 786.3
Business 6 11215.5 14.16 543.6
Travel 4 23765.75 28 1086
Cricket 4 23357.25 24.5 1189.5
Poetry 6 7261.83 24.16 329
</table>
<tableCaption confidence="0.912745">
Table 1: Statistics for the training datasets used in
our experiments. The numbers in columns 3, 4 and
</tableCaption>
<sectionHeader confidence="0.82294" genericHeader="method">
5 are averages.
</sectionHeader>
<subsectionHeader confidence="0.999401">
3.2 Methodology
</subsectionHeader>
<bodyText confidence="0.999956">
We evaluated our approach to authorship predic-
tion on the five datasets described above. For news
articles, we used the first 10 sections of the WSJ
corpus, which consists of annotated news articles
on finance, to build the initial statistical parser in
</bodyText>
<page confidence="0.997161">
39
</page>
<bodyText confidence="0.999948911764706">
Step 1. For Poetry, we used 7 sections of the
Brown corpus which consists of annotated docu-
ments from different areas of literature.
In the basic approach, we trained a PCFG model
for each author based solely on the documents
written by that author. However, since the num-
ber of documents per author is relatively low, this
leads to very sparse training data. Therefore, we
also augmented the training data by adding one,
two or three sections of the WSJ or Brown corpus
to each training set, and up-sampling (replicating)
the data from the original author. We refer to this
model as “PCFG-I”, where I stands for interpo-
lation since this effectively exploits linear interpo-
lation with the base corpus to smooth parameters.
Based on our preliminary experiments, we repli-
cated the original data three or four times.
We compared the performance of our approach
to bag-of-words classification and n-gram lan-
guage models. When using bag-of-words, one
generally removes commonly occurring “stop
words.” However, for the task of authorship pre-
diction, we hypothesized that the frequency of
specific stop words could provide useful infor-
mation about the author’s writing style. Prelim-
inary experiments verified that eliminating stop
words degraded performance; therefore, we did
not remove them. We used the Maximum Entropy
(MaxEnt) and Naive Bayes classifiers in the MAL-
LET software package (McCallum, 2002) as ini-
tial baselines. We surmised that a discriminative
classifier like MaxEnt might perform better than
a generative classifier like Naive Bayes. How-
ever, when sufficient training data is not available,
generative models are known to perform better
than discriminative models (Ng and Jordan, 2001).
Hence, we chose to compare our method to both
Naive Bayes and MaxEnt.
We also compared the performance of the
PCFG approach against n-gram language models.
Specifically, we tried unigram, bigram and trigram
models. We used the same background corpus
mixing method used for the PCFG-I model to ef-
fectively smooth the n-gram models. Since a gen-
erative model like Naive Bayes that uses n-gram
frequencies is equivalent to an n-gram language
model, we also used the Naive Bayes classifier in
MALLET to implement the n-gram models. Note
that a Naive-Bayes bag-of-words model is equiva-
lent to a unigram language model.
While the PCFG model captures the author’s
writing style at the syntactic level, it may not accu-
rately capture lexical information. Since both syn-
tactic and lexical information is presumably useful
in capturing the author’s overall writing style, we
also developed an ensemble using a PCFG model,
the bag-of-words MaxEnt classifier, and an n-
gram language model. We linearly combined the
confidence scores assigned by each model to each
author, and used the combined score for the final
classification. We refer to this model as “PCFG-
E”, where E stands for ensemble. We also de-
veloped another ensemble based on MaxEnt and
n-gram language models to demonstrate the con-
tribution of the PCFG model to the overall per-
formance of PCFG-E. For each dataset, we report
accuracy, the fraction of the test documents whose
authors were correctly identified.
</bodyText>
<subsectionHeader confidence="0.870736">
3.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9999153125">
Table 2 shows the accuracy of authorship predic-
tion on different datasets. For the n-gram mod-
els, we only report the results for the bigram
model with smoothing (Bigram-I) as it was the
best performing model for most datasets (except
for Cricket and Poetry). For the Cricket dataset,
the trigram-I model was the best performing n-
gram model with an accuracy of 98.34%. Gener-
ally, a higher order n-gram model (n = 3 or higher)
performs poorly as it requires a fair amount of
smoothing due to the exponential increase in all
possible n-gram combinations. Hence, the supe-
rior performance of the trigram-I model on the
Cricket dataset was a surprising result. For the
Poetry dataset, the unigram-I model performed
best among the smoothed n-gram models at 81.8%
accuracy. This is unsurprising because as men-
tioned above, topic information is strongest in
the Poetry dataset, and it is captured well in the
unigram model. For bag-of-words methods, we
find that the generatively trained Naive Bayes
model (unigram language model) performs bet-
ter than or equal to the discriminatively trained
MaxEnt model on most datasets (except for Busi-
ness). This result is not suprising since our
datasets are limited in size, and generative models
tend to perform better than discriminative meth-
ods when there is very little training data available.
Amongst the different baseline models (MaxEnt,
Naive Bayes, Bigram-I), we find Bigram-I to be
the best performing model (except for Cricket and
Poetry). For both Cricket and Poetry, Naive Bayes
</bodyText>
<page confidence="0.997243">
40
</page>
<table confidence="0.9996385">
Dataset MaxEnt Naive Bayes Bigram-I PCFG PCFG-I PCFG-E MaxEnt+Bigram-I
Football 84.45 86.67 86.67 93.34 80 91.11 86.67
Business 83.34 77.78 90.00 77.78 85.56 91.11 92.22
Travel 83.34 83.34 91.67 81.67 86.67 91.67 90.00
Cricket 91.67 95.00 91.67 86.67 91.67 95.00 93.34
Poetry 56.36 78.18 70.90 78.18 83.63 87.27 76.36
</table>
<tableCaption confidence="0.72746">
Table 2: Accuracy in % for authorship prediction on different datasets. Bigram-I refers to the bigram
language model with smoothing. PCFG-E refers to the ensemble based on MaxEnt, Bigram-I, and
PCFG-I. MaxEnt+Bigram-I refers to the ensemble based on MaxEnt and Bigram-I.
</tableCaption>
<bodyText confidence="0.9998063875">
is the best performing baseline model. While dis-
cussing the performance of the PCFG model and
its variants, we consider the best performing base-
line model.
We observe that the basic PCFG model and the
PCFG-I model do not usually outperform the best
baseline method (except for Football and Poetry,
as discussed below). For Football, the basic PCFG
model outperforms the best baseline, while for
Poetry, the PCFG-I model outperforms the best
baseline. Further, the performance of the basic
PCFG model is inferior to that of PCFG-I for most
datasets, likely due to the insufficient training data
used in the basic model. Ideally one would use
more training documents, but in many domains
it is impossible to obtain a large corpus of doc-
uments written by a single author. For example,
as Luyckx and Daelemans (2008) argue, in foren-
sics one would like to identify the authorship of
documents based on a limited number of docu-
ments written by the author. Hence, we investi-
gated smoothing techniques to improve the perfor-
mance of the basic PCFG model. We found that
the interpolation approach resulted in a substan-
tial improvement in the performance of the PCFG
model for all but the Football dataset (discussed
below). However, for some datasets, even this
improvement was not sufficient to outperform the
best baseline.
The results for PCFG and PCFG-I demon-
strate that syntactic information alone is gener-
ally a bit less accurate than using n-grams. In or-
der to utilize both syntactic and lexical informa-
tion, we developed PCFG-E as described above.
We combined the best n-gram model (Bigram-I)
and PCFG model (PCFG-I) with MaxEnt to build
PCFG-E. For the Travel dataset, we find that the
performance of the PCFG-E model is equal to that
of the best constituent model (Bigram-I). For the
remaining datasets, the performance of PCFG-E
is better than the best constituent model. Further-
more, for the Football, Cricket and Poetry datasets
this improvement is quite substantial. We now
find that the performance of some variant of PCFG
is always better than or equal to that of the best
baseline. While the basic PCFG model outper-
forms the baseline for the Football dataset, PCFG-
E outperforms the best baseline for the Poetry
and Business datasets. For the Cricket and Travel
datasets, the performance of the PCFG-E model
equals that of the best baseline. In order to as-
sess the statistical significance of any performance
difference between the best PCFG model and the
best baseline, we performed the McNemar’s test,
a non-parametric test for binomial variables (Ros-
ner, 2005). We found that the difference in the
performance of the two methods was not statisti-
cally significant at .05 significance level for any of
the datasets, probably due to the small number of
test samples.
The performance of PCFG and PCFG-I is par-
ticularly impressive on the Football and Poetry
datasets. For the Football dataset, the basic PCFG
model is the best performing PCFG model and it
performs much better than other methods. It is sur-
prising that smoothing using PCFG-I actually re-
sults in a drop in performance on this dataset. We
hypothesize that the authors in the Football dataset
may have very different syntactic writing styles
that are effectively captured by the basic PCFG
model. Smoothing the data apparently weakens
this signal, hence causing a drop in performance.
For Poetry, PCFG-I achieves much higher accu-
racy than the baselines. This is impressive given
the much looser syntactic structure of poetry com-
pared to news articles, and it indicates the value of
syntactic information for distinguishing between
literary authors.
Finally, we consider the specific contribution of
the PCFG-I model towards the performance of
</bodyText>
<page confidence="0.998033">
41
</page>
<bodyText confidence="0.9997692">
the PCFG-E ensemble. Based on comparing the
results for PCFG-E and MaxEnt+Bigram-I, we
find that there is a drop in performance for most
datasets when removing PCFG-I from the ensem-
ble. This drop is quite substantial for the Football
and Poetry datasets. This indicates that PCFG-I
is contributing substantially to the performance of
PCFG-E. Thus, it further illustrates the impor-
tance of broader syntactic information for the task
of authorship attribution.
</bodyText>
<sectionHeader confidence="0.998378" genericHeader="discussions">
4 Future Work and Conclusions
</sectionHeader>
<bodyText confidence="0.999945642857143">
In this paper, we have presented our ongoing work
on authorship attribution, describing a novel ap-
proach that uses probabilistic context-free gram-
mars. We have demonstrated that both syntac-
tic and lexical information are useful in effec-
tively capturing authors’ overall writing style. To
this end, we have developed an ensemble ap-
proach that performs better than the baseline mod-
els on several datasets. An interesting extension
of our current approach is to consider discrimina-
tive training of PCFGs for each author. Finally,
we would like to compare the performance of our
method to other state-of-the-art approaches to au-
thorship prediction.
</bodyText>
<sectionHeader confidence="0.998455" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9423745">
Experiments were run on the Mastodon Cluster,
provided by NSF Grant EIA-0303609.
</bodyText>
<sectionHeader confidence="0.998247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996042121621622">
H. Baayen, H. van Halteren, and F. Tweedie. 1996.
Outside the cave of shadows: using syntactic annota-
tion to enhance authorship attribution. Literary and
Linguistic Computing, 11(3):121–132, September.
Binongo and Smith. 1999. A Study of Oscar Wilde’s
Writings. Journal of Applied Statistics, 26:781.
J Burrows. 1987. Word-patterns and Story-shapes:
The Statistical Analysis of Narrative Style.
Joachim Diederich, J¨org Kindermann, Edda Leopold,
and Gerhard Paass. 2000. Authorship Attribu-
tion with Support Vector Machines. Applied Intel-
ligence, 19:2003.
D. I. Holmes and R. S. Forsyth. 1995. The Federal-
ist Revisited: New Directions in Authorship Attri-
bution. Literary and Linguistic Computing, 10:111–
127.
Thorsten Joachims. 1998. Text categorization with
Support Vector Machines: Learning with many rel-
evant features. In Proceedings of the 10th European
Conference on Machine Learning (ECML), pages
137–142, Berlin, Heidelberg. Springer-Verlag.
Patrick Juola and John Sofko. 2004. Proving and
Improving Authorship Attribution Technologies. In
Proceedings of Canadian Symposium for Text Anal-
ysis (CaSTA).
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL), pages 423–430, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Dan Klein and Christopher D. Manning. 2003b. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Infor-
mation Processing Systems 15 (NIPS), pages 3–10.
MIT Press.
Kim Luyckx and Walter Daelemans. 2008. Author-
ship Attribution and Verification with Many Authors
and Limited Data. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING), pages 513–520, August.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case of
the Federalist Papers. Springer-Verlag.
Andrew Y. Ng and Michael I. Jordan. 2001. On Dis-
criminative vs. Generative classifiers: A compari-
son of logistic regression and naive Bayes. In Ad-
vances in Neural Information Processing Systems 14
(NIPS), pages 841–848.
Fuchun Peng, Dale Schuurmans, Viado Keselj, and
Shaojun Wang. 2003. Language Independent
Authorship Attribution using Character Level Lan-
guage Models. In Proceedings of the 10th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL).
Bernard Rosner. 2005. Fundamentals of Biostatistics.
Duxbury Press.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 1999.
Automatic Authorship Attribution. In Proceedings
of the 9th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 158–164, Morristown, NJ, USA. Association
for Computational Linguistics.
E. Stamatatos. 2009. A Survey of Modern Author-
ship Attribution Methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538–556.
Rong Zheng, Yi Qin, Zan Huang, and Hsinchun
Chen. 2009. Authorship Analysis in Cybercrime
Investigation. Lecture Notes in Computer Science,
2665/2009:959.
</reference>
<page confidence="0.999295">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.602036">
<title confidence="0.999894">Authorship Attribution Using Probabilistic Context-Free Grammars</title>
<author confidence="0.9998">Sindhu Raghavan Adriana Kovashka Raymond Mooney</author>
<affiliation confidence="0.874337333333333">Department of Computer Science The University of Texas at Austin 1 University Station C0500</affiliation>
<address confidence="0.957459">Austin, TX 78712-0233, USA</address>
<abstract confidence="0.997122">In this paper, we present a novel approach for authorship attribution, the task of identifying the author of a document, using probabilistic context-free grammars. Our approach involves building a probabilistic context-free grammar for each author and using this grammar as a language model for classification. We evaluate the performance of our method on a wide range of datasets to demonstrate its efficacy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Baayen</author>
<author>H van Halteren</author>
<author>F Tweedie</author>
</authors>
<title>Outside the cave of shadows: using syntactic annotation to enhance authorship attribution.</title>
<date>1996</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>11</volume>
<issue>3</issue>
<marker>Baayen, van Halteren, Tweedie, 1996</marker>
<rawString>H. Baayen, H. van Halteren, and F. Tweedie. 1996. Outside the cave of shadows: using syntactic annotation to enhance authorship attribution. Literary and Linguistic Computing, 11(3):121–132, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binongo</author>
<author>Smith</author>
</authors>
<title>A Study of Oscar Wilde’s Writings.</title>
<date>1999</date>
<journal>Journal of Applied Statistics,</journal>
<pages>26--781</pages>
<contexts>
<context position="1706" citStr="Binongo and Smith, 1999" startWordPosition="243" endWordPosition="246">iminating between the styles of different authors. These strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans (2008) use a combination of word-level statistics and part-of-speech counts or n-grams. Baayen et al. (1996) demonstrate that the use of syntactic features from parse trees can improve the accuracy of authorship attribution. While there have been several approaches pr</context>
</contexts>
<marker>Binongo, Smith, 1999</marker>
<rawString>Binongo and Smith. 1999. A Study of Oscar Wilde’s Writings. Journal of Applied Statistics, 26:781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burrows</author>
</authors>
<title>Word-patterns and Story-shapes: The Statistical Analysis of Narrative Style.</title>
<date>1987</date>
<contexts>
<context position="1681" citStr="Burrows, 1987" startWordPosition="241" endWordPosition="242">egies for discriminating between the styles of different authors. These strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans (2008) use a combination of word-level statistics and part-of-speech counts or n-grams. Baayen et al. (1996) demonstrate that the use of syntactic features from parse trees can improve the accuracy of authorship attribution. While there have b</context>
</contexts>
<marker>Burrows, 1987</marker>
<rawString>J Burrows. 1987. Word-patterns and Story-shapes: The Statistical Analysis of Narrative Style.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Diederich</author>
<author>J¨org Kindermann</author>
<author>Edda Leopold</author>
<author>Gerhard Paass</author>
</authors>
<title>Authorship Attribution with Support Vector Machines. Applied Intelligence,</title>
<date>2000</date>
<pages>19--2003</pages>
<contexts>
<context position="1730" citStr="Diederich et al., 2000" startWordPosition="247" endWordPosition="250">les of different authors. These strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans (2008) use a combination of word-level statistics and part-of-speech counts or n-grams. Baayen et al. (1996) demonstrate that the use of syntactic features from parse trees can improve the accuracy of authorship attribution. While there have been several approaches proposed for authorship at</context>
</contexts>
<marker>Diederich, Kindermann, Leopold, Paass, 2000</marker>
<rawString>Joachim Diederich, J¨org Kindermann, Edda Leopold, and Gerhard Paass. 2000. Authorship Attribution with Support Vector Machines. Applied Intelligence, 19:2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D I Holmes</author>
<author>R S Forsyth</author>
</authors>
<title>The Federalist Revisited: New Directions in Authorship Attribution. Literary and Linguistic Computing,</title>
<date>1995</date>
<pages>10--111</pages>
<contexts>
<context position="1756" citStr="Holmes and Forsyth, 1995" startWordPosition="251" endWordPosition="254">. These strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans (2008) use a combination of word-level statistics and part-of-speech counts or n-grams. Baayen et al. (1996) demonstrate that the use of syntactic features from parse trees can improve the accuracy of authorship attribution. While there have been several approaches proposed for authorship attribution, it is not clear</context>
</contexts>
<marker>Holmes, Forsyth, 1995</marker>
<rawString>D. I. Holmes and R. S. Forsyth. 1995. The Federalist Revisited: New Directions in Authorship Attribution. Literary and Linguistic Computing, 10:111– 127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with Support Vector Machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th European Conference on Machine Learning (ECML),</booktitle>
<pages>137--142</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1772" citStr="Joachims, 1998" startWordPosition="255" endWordPosition="256">merous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans (2008) use a combination of word-level statistics and part-of-speech counts or n-grams. Baayen et al. (1996) demonstrate that the use of syntactic features from parse trees can improve the accuracy of authorship attribution. While there have been several approaches proposed for authorship attribution, it is not clear if the performa</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with Support Vector Machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning (ECML), pages 137–142, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Juola</author>
<author>John Sofko</author>
</authors>
<title>Proving and Improving Authorship Attribution Technologies.</title>
<date>2004</date>
<booktitle>In Proceedings of Canadian Symposium for Text Analysis (CaSTA).</booktitle>
<contexts>
<context position="1410" citStr="Juola and Sofko, 2004" startWordPosition="195" endWordPosition="198"> be used to distinguish between languages. In the context of written text, such as newspaper articles or short stories, the author’s style could be considered a distinct “language.” Authorship attribution, also referred to as authorship identification or prediction, studies strategies for discriminating between the styles of different authors. These strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (199</context>
</contexts>
<marker>Juola, Sofko, 2004</marker>
<rawString>Patrick Juola and John Sofko. 2004. Proving and Improving Authorship Attribution Technologies. In Proceedings of Canadian Symposium for Text Analysis (CaSTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4383" citStr="Klein and Manning, 2003" startWordPosition="667" endWordPosition="670"> parser takes a corpus of parse trees of sentences as training input. Since we do not have access to authors’ documents annotated with parse trees, we use a statistical parser trained on a generic 38 Proceedings of the ACL 2010 Conference Short Papers, pages 38–42, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics corpus like the Wall Street Journal (WSJ) or Brown corpus from the Penn Treebank (http: //www.cis.upenn.edu/˜treebank/) to automatically annotate (i.e. treebank) the training documents for each author. In our experiments, we used the Stanford Parser (Klein and Manning, 2003b; Klein and Manning, 2003a) and the OpenNLP sentence segmenter (http://opennlp.sourceforge.net/). Our approach is summarized below: Input – A training set of documents labeled with author names and a test set of documents with unknown authors. 1. Train a statistical parser on a generic corpus like the WSJ or Brown corpus. 2. Treebank each training document using the parser trained in Step 1. 3. Train a PCFG GZ for each author AZ using the treebanked documents for that author. 4. For each test document, compute its likelihood for each grammar GZ by multiplying the probability of the top PCFG p</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL), pages 423–430, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS),</booktitle>
<pages>3--10</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4383" citStr="Klein and Manning, 2003" startWordPosition="667" endWordPosition="670"> parser takes a corpus of parse trees of sentences as training input. Since we do not have access to authors’ documents annotated with parse trees, we use a statistical parser trained on a generic 38 Proceedings of the ACL 2010 Conference Short Papers, pages 38–42, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics corpus like the Wall Street Journal (WSJ) or Brown corpus from the Penn Treebank (http: //www.cis.upenn.edu/˜treebank/) to automatically annotate (i.e. treebank) the training documents for each author. In our experiments, we used the Stanford Parser (Klein and Manning, 2003b; Klein and Manning, 2003a) and the OpenNLP sentence segmenter (http://opennlp.sourceforge.net/). Our approach is summarized below: Input – A training set of documents labeled with author names and a test set of documents with unknown authors. 1. Train a statistical parser on a generic corpus like the WSJ or Brown corpus. 2. Treebank each training document using the parser trained in Step 1. 3. Train a PCFG GZ for each author AZ using the treebanked documents for that author. 4. For each test document, compute its likelihood for each grammar GZ by multiplying the probability of the top PCFG p</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Advances in Neural Information Processing Systems 15 (NIPS), pages 3–10. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Luyckx</author>
<author>Walter Daelemans</author>
</authors>
<title>Authorship Attribution and Verification with Many Authors and Limited Data.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>513--520</pages>
<contexts>
<context position="1502" citStr="Luyckx and Daelemans, 2008" startWordPosition="209" endWordPosition="212">paper articles or short stories, the author’s style could be considered a distinct “language.” Authorship attribution, also referred to as authorship identification or prediction, studies strategies for discriminating between the styles of different authors. These strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans (2008) use a combination of word-level statistics and part-of-sp</context>
<context position="14086" citStr="Luyckx and Daelemans (2008)" startWordPosition="2250" endWordPosition="2253">G model and the PCFG-I model do not usually outperform the best baseline method (except for Football and Poetry, as discussed below). For Football, the basic PCFG model outperforms the best baseline, while for Poetry, the PCFG-I model outperforms the best baseline. Further, the performance of the basic PCFG model is inferior to that of PCFG-I for most datasets, likely due to the insufficient training data used in the basic model. Ideally one would use more training documents, but in many domains it is impossible to obtain a large corpus of documents written by a single author. For example, as Luyckx and Daelemans (2008) argue, in forensics one would like to identify the authorship of documents based on a limited number of documents written by the author. Hence, we investigated smoothing techniques to improve the performance of the basic PCFG model. We found that the interpolation approach resulted in a substantial improvement in the performance of the PCFG model for all but the Football dataset (discussed below). However, for some datasets, even this improvement was not sufficient to outperform the best baseline. The results for PCFG and PCFG-I demonstrate that syntactic information alone is generally a bit </context>
</contexts>
<marker>Luyckx, Daelemans, 2008</marker>
<rawString>Kim Luyckx and Walter Daelemans. 2008. Authorship Attribution and Verification with Many Authors and Limited Data. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 513–520, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="9371" citStr="McCallum, 2002" startWordPosition="1488" endWordPosition="1489">al data three or four times. We compared the performance of our approach to bag-of-words classification and n-gram language models. When using bag-of-words, one generally removes commonly occurring “stop words.” However, for the task of authorship prediction, we hypothesized that the frequency of specific stop words could provide useful information about the author’s writing style. Preliminary experiments verified that eliminating stop words degraded performance; therefore, we did not remove them. We used the Maximum Entropy (MaxEnt) and Naive Bayes classifiers in the MALLET software package (McCallum, 2002) as initial baselines. We surmised that a discriminative classifier like MaxEnt might perform better than a generative classifier like Naive Bayes. However, when sufficient training data is not available, generative models are known to perform better than discriminative models (Ng and Jordan, 2001). Hence, we chose to compare our method to both Naive Bayes and MaxEnt. We also compared the performance of the PCFG approach against n-gram language models. Specifically, we tried unigram, bigram and trigram models. We used the same background corpus mixing method used for the PCFG-I model to effect</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David L Wallace</author>
</authors>
<title>Applied Bayesian and Classical Inference: The Case of the Federalist Papers.</title>
<date>1984</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1305" citStr="Mosteller and Wallace, 1984" startWordPosition="182" endWordPosition="185">s efficacy. 1 Introduction Natural language processing allows us to build language models, and these models can be used to distinguish between languages. In the context of written text, such as newspaper articles or short stories, the author’s style could be considered a distinct “language.” Authorship attribution, also referred to as authorship identification or prediction, studies strategies for discriminating between the styles of different authors. These strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or s</context>
</contexts>
<marker>Mosteller, Wallace, 1984</marker>
<rawString>Frederick Mosteller and David L. Wallace. 1984. Applied Bayesian and Classical Inference: The Case of the Federalist Papers. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14 (NIPS),</booktitle>
<pages>841--848</pages>
<contexts>
<context position="9670" citStr="Ng and Jordan, 2001" startWordPosition="1531" endWordPosition="1534"> of specific stop words could provide useful information about the author’s writing style. Preliminary experiments verified that eliminating stop words degraded performance; therefore, we did not remove them. We used the Maximum Entropy (MaxEnt) and Naive Bayes classifiers in the MALLET software package (McCallum, 2002) as initial baselines. We surmised that a discriminative classifier like MaxEnt might perform better than a generative classifier like Naive Bayes. However, when sufficient training data is not available, generative models are known to perform better than discriminative models (Ng and Jordan, 2001). Hence, we chose to compare our method to both Naive Bayes and MaxEnt. We also compared the performance of the PCFG approach against n-gram language models. Specifically, we tried unigram, bigram and trigram models. We used the same background corpus mixing method used for the PCFG-I model to effectively smooth the n-gram models. Since a generative model like Naive Bayes that uses n-gram frequencies is equivalent to an n-gram language model, we also used the Naive Bayes classifier in MALLET to implement the n-gram models. Note that a Naive-Bayes bag-of-words model is equivalent to a unigram l</context>
</contexts>
<marker>Ng, Jordan, 2001</marker>
<rawString>Andrew Y. Ng and Michael I. Jordan. 2001. On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes. In Advances in Neural Information Processing Systems 14 (NIPS), pages 841–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Dale Schuurmans</author>
<author>Viado Keselj</author>
<author>Shaojun Wang</author>
</authors>
<title>Language Independent Authorship Attribution using Character Level Language Models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="1933" citStr="Peng et al. (2003)" startWordPosition="276" endWordPosition="279">c plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans (2008) use a combination of word-level statistics and part-of-speech counts or n-grams. Baayen et al. (1996) demonstrate that the use of syntactic features from parse trees can improve the accuracy of authorship attribution. While there have been several approaches proposed for authorship attribution, it is not clear if the performance of one is better than the other. Further, it is difficult to compare the performance of these algorithms because they were primarily evaluated on different d</context>
</contexts>
<marker>Peng, Schuurmans, Keselj, Wang, 2003</marker>
<rawString>Fuchun Peng, Dale Schuurmans, Viado Keselj, and Shaojun Wang. 2003. Language Independent Authorship Attribution using Character Level Language Models. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Rosner</author>
</authors>
<title>Fundamentals of Biostatistics.</title>
<date>2005</date>
<publisher>Duxbury Press.</publisher>
<contexts>
<context position="15843" citStr="Rosner, 2005" startWordPosition="2542" endWordPosition="2544">e substantial. We now find that the performance of some variant of PCFG is always better than or equal to that of the best baseline. While the basic PCFG model outperforms the baseline for the Football dataset, PCFGE outperforms the best baseline for the Poetry and Business datasets. For the Cricket and Travel datasets, the performance of the PCFG-E model equals that of the best baseline. In order to assess the statistical significance of any performance difference between the best PCFG model and the best baseline, we performed the McNemar’s test, a non-parametric test for binomial variables (Rosner, 2005). We found that the difference in the performance of the two methods was not statistically significant at .05 significance level for any of the datasets, probably due to the small number of test samples. The performance of PCFG and PCFG-I is particularly impressive on the Football and Poetry datasets. For the Football dataset, the basic PCFG model is the best performing PCFG model and it performs much better than other methods. It is surprising that smoothing using PCFG-I actually results in a drop in performance on this dataset. We hypothesize that the authors in the Football dataset may have</context>
</contexts>
<marker>Rosner, 2005</marker>
<rawString>Bernard Rosner. 2005. Fundamentals of Biostatistics. Duxbury Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
<author>N Fakotakis</author>
<author>G Kokkinakis</author>
</authors>
<title>Automatic Authorship Attribution.</title>
<date>1999</date>
<booktitle>In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>158--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2012" citStr="Stamatatos et al. (1999)" startWordPosition="288" endWordPosition="292">Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans (2008) use a combination of word-level statistics and part-of-speech counts or n-grams. Baayen et al. (1996) demonstrate that the use of syntactic features from parse trees can improve the accuracy of authorship attribution. While there have been several approaches proposed for authorship attribution, it is not clear if the performance of one is better than the other. Further, it is difficult to compare the performance of these algorithms because they were primarily evaluated on different datasets. For more information on the current state of the art for authorship at</context>
<context position="6585" citStr="Stamatatos et al., 1999" startWordPosition="1021" endWordPosition="1024">d to collect sets of documents on a shared topic written by multiple authors. This was done to ensure that the datasets truly tested authorship attribution as opposed to topic identification. However, since it is very difficult to find authors that write literary works on the same topic, the Poetry dataset exhibits higher topic variability than our news datasets. We had 5 different datasets in total – Football, Business, Travel, Cricket, and Poetry. The number of authors in our datasets ranged from 3 to 6. For each dataset, we split the documents into training and test sets. Previous studies (Stamatatos et al., 1999) have observed that having unequal number of words per author in the training set leads to poor performance for the authors with fewer words. Therefore, we ensured that, in the training set, the total number of words per author was roughly the same. We would like to note that we could have also selected the training set such that the total number of sentences per author was roughly the same. However, since we would like to compare the performance of the PCFG-based approach with a bag-of-words baseline, we decided to normalize the training set based on the number of words, rather than sentences</context>
</contexts>
<marker>Stamatatos, Fakotakis, Kokkinakis, 1999</marker>
<rawString>E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 1999. Automatic Authorship Attribution. In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 158–164, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
</authors>
<title>A Survey of Modern Authorship Attribution Methods.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>3</issue>
<contexts>
<context position="2684" citStr="Stamatatos (2009)" startWordPosition="397" endWordPosition="398">ord-level statistics and part-of-speech counts or n-grams. Baayen et al. (1996) demonstrate that the use of syntactic features from parse trees can improve the accuracy of authorship attribution. While there have been several approaches proposed for authorship attribution, it is not clear if the performance of one is better than the other. Further, it is difficult to compare the performance of these algorithms because they were primarily evaluated on different datasets. For more information on the current state of the art for authorship attribution, we refer the reader to a detailed survey by Stamatatos (2009). We further investigate the use of syntactic information by building complete models of each author’s syntax to distinguish between authors. Our approach involves building a probabilistic contextfree grammar (PCFG) for each author and using this grammar as a language model for classification. Experiments on a variety of corpora including poetry and newspaper articles on a number of topics demonstrate that our PCFG approach performs fairly well, but it only outperforms a bigram language model on a couple of datasets (e.g. poetry). However, combining our approach with other methods results in a</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>E. Stamatatos. 2009. A Survey of Modern Authorship Attribution Methods. Journal of the American Society for Information Science and Technology, 60(3):538–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Zheng</author>
<author>Yi Qin</author>
<author>Zan Huang</author>
<author>Hsinchun Chen</author>
</authors>
<date>2009</date>
<booktitle>Authorship Analysis in Cybercrime Investigation. Lecture Notes in Computer Science,</booktitle>
<pages>2665--2009</pages>
<contexts>
<context position="1458" citStr="Zheng et al., 2009" startWordPosition="203" endWordPosition="206">ontext of written text, such as newspaper articles or short stories, the author’s style could be considered a distinct “language.” Authorship attribution, also referred to as authorship identification or prediction, studies strategies for discriminating between the styles of different authors. These strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans (2008) use a combina</context>
</contexts>
<marker>Zheng, Qin, Huang, Chen, 2009</marker>
<rawString>Rong Zheng, Yi Qin, Zan Huang, and Hsinchun Chen. 2009. Authorship Analysis in Cybercrime Investigation. Lecture Notes in Computer Science, 2665/2009:959.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>