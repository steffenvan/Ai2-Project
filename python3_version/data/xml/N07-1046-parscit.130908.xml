<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.944064">
A Log-linear Block Transliteration Model based on Bi-Stream HMMs
</title>
<author confidence="0.956711">
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vogel
</author>
<affiliation confidence="0.730482">
{bzhao, nbach, ianlane, vogel}@cs.cmu.edu
Language Technologies Institute
School of Computer Science, Carnegie Mellon University
</affiliation>
<sectionHeader confidence="0.953975" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999905611111111">
We propose a novel HMM-based framework to
accurately transliterate unseen named entities.
The framework leverages features in letter-
alignment and letter n-gram pairs learned from
available bilingual dictionaries. Letter-classes,
such as vowels/non-vowels, are integrated to
further improve transliteration accuracy. The
proposed transliteration system is applied to
out-of-vocabulary named-entities in statistical
machine translation (SMT), and a significant
improvement over traditional transliteration ap-
proach is obtained. Furthermore, by incor-
porating an automatic spell-checker based on
statistics collected from web search engines,
transliteration accuracy is further improved.
The proposed system is implemented within
our SMT system and applied to a real transla-
tion scenario from Arabic to English.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960871428572">
Cross-lingual natural language applications, such as in-
formation retrieval, question answering, and machine
translation for web-documents (e.g. Google translation),
are becoming increasingly important. However, current
state-of-the-art statistical machine translation (SMT) sys-
tems cannot yet translate named-entities which are not
seen during training. New named-entities, such as per-
son, organization, and location names are continually
emerging on the World-Wide-Web. To realize effective
cross-lingual natural language applications, handling out-
of-vocabulary named-entities is becoming more crucial.
Named entities (NEs) can be translated via transliter-
ation: mapping symbols from one writing system to an-
other. Letters of the source language are typically trans-
formed into the target language with similar pronunci-
ation. Transliteration between languages which share
similar alphabets and sound systems is usually not dif-
ficult, because the majority of letters remain the same.
However, the task is significantly more difficult when the
language pairs are considerably different, for example,
English-Arabic, English-Chinese, and English-Japanese.
In this paper, we focus on forward transliteration from
Arabic to English.
The work in (Arbabi et al., 1994), to our knowledge, is
the first work on machine transliteration of Arabic names
into English, French, and Spanish. The idea is to vow-
elize Arabic names by adding appropriate vowels and uti-
lizing a phonetic look-up table to provide the spelling in
the target language. Their framework is strictly applica-
ble within standard Arabic morphological rules. Knight
and Graehl (1997) introduced finite state transducers that
implement back-transliteration from Japanese to English,
which was then extended to Arabic-English in (Stalls and
Knight, 1998). Al-Onaizan and Knight (2002) translit-
erated named entities in Arabic text to English by com-
bining phonetic-based and spelling-based models, and re-
ranking candidates with full-name web counts, named en-
tities co-reference, and contextual web counts. Huang
(2005) proposed a specific model for Chinese-English
name transliteration with clusterings of names’ origins,
and appropriate hypotheses are generated given the ori-
gins. All of these approaches, however, are not based
on a SMT-framework. Technologies developed for SMT
are borrowed in Virga and Khudanpur (2003) and Ab-
dulJaleel and Larkey (2003). Standard SMT alignment
models (Brown et al., 1993) are used to align letter-pairs
within named entity pairs for transliteration. Their ap-
proach are generative models for letter-to-letter transla-
tions, and the letter-alignment is augmented with heuris-
tics. Letter-level contextual information is shown to be
very helpful for transliteration. Oh and Choi (2002)
used conversion units for English-Korean Transliteration;
Goto et al. (2003) used conversion units, mapping En-
glish letter-sequence into Japanese Katakana character
string. Li et al. (2004) presented a framework allowing
direct orthographical mapping of transliteration units be-
tween English and Chinese, and an extended model is
presented in Ekbal et al. (2006).
We propose a block-level transliteration framework, as
shown in Figure 1, to model letter-level context infor-
mation for transliteration at two levels. First, we pro-
pose a bi-stream HMM incorporating letter-clusters to
better model the vowel and non-vowel transliterations
with position-information, i.e., initial and final, to im-
prove the letter-level alignment accuracy. Second, based
on the letter-alignment, we propose letter n-gram (letter-
sequence) alignment models (block) to automatically
learn the mappings from source letter n-grams to target
letter n-grams. A few features specific for transliterations
are explored, and a log-linear model is used to combine
</bodyText>
<page confidence="0.975317">
364
</page>
<affiliation confidence="0.262825">
Proceedings of NAACL HLT 2007, pages 364–371,
</affiliation>
<note confidence="0.717372">
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9947885">
Figure 1: Transliteration System Structure. The upper-part is
the two-directional Bi-Stream HMM for letter-alignment; the
lower-part is a log-linear model for combining different feature
functions for block-level transliteration.
</figureCaption>
<bodyText confidence="0.999858416666667">
these features to learn block-level transliteration-pairs
from training data. The proposed transliteration frame-
work obtained significant improvements over a strong
baseline transliteration approach similar to AbdulJaleel
and Larkey (2003) and Virga and Khudanpur (2003).
The remainder of this paper is organized as follows.
In Section 2, we formulate the transliteration as a general
translation problem; in Section 4, we propose a log-linear
alignment model with a local search algorithm to model
the letter n-gram translation pairs; in Section 5, exper-
iments are presented. Conclusions and discussions are
given in Section 6.
</bodyText>
<sectionHeader confidence="0.750904" genericHeader="method">
2 Transliteration as Translation
</sectionHeader>
<bodyText confidence="0.99761884">
Transliteration can be viewed as a special case of transla-
tion. In this approach, source and target NEs are split into
letter sequences, and each sequence is treated as apseudo
sentence. The appealing reason of formulating transliter-
ation in this way is to utilize advanced alignment models,
which share ideas applied also within phrase-based sta-
tistical machine translation (Koehn, 2004).
To apply this approach to transliteration, however,
some unique aspects should be considered. First, letters
should be generated from left to right, without any re-
ordering. Thus, the transliteration models can only exe-
cute forward sequential jumps. Second, for unvowelized
languages such as Arabic, a single Arabic letter typically
maps to less than four English letters. Thus, the fertility
for each letter should be recognized to ensure reasonable
length relevance. Third, the position of the letter within
a NE is important. For example, in Arabic, letters such
as “al” at the beginning of the NE can only be translated
into “the” or “al”. Therefore position information should
be considered within the alignment models.
Incorporating the above considerations, transliteration
can be formulated as a noisy channel model. Let fJ1 =
f1f2...fJ denote the source NE with J letters, eI1 =
e1e2...eI be an English transliteration candidate with I
letters. According to Bayesian decision rule:
</bodyText>
<equation confidence="0.9756975">
eI1= arg max P(eI1|fJ1 )=argmax P(fJ1 |eI1)P(eI1), (1)
{eI1l {eI1l
</equation>
<bodyText confidence="0.999756266666667">
where P(fJ1 |eI1) is the letter translation model and P(eI1)
is the English letter sequence model corresponding to
the monolingual language models in SMT. In this noisy-
channel scheme, P(fJ1 |eI1) is the key component for
transliteration, in which the transliteration between eI1
and fJ1 can be modeled at either letter-to-letter level, or
letter n-gram transliteration level (block-level).
Our transliteration models are illustrated in Figure 1.
We propose a Bi-Stream HMM of P(fJ1 |eI1) to infer
letter-to-letter alignments in two directions: Arabic-to-
English (F-to-E) and English-to-Arabic (E-to-F), shown
in the upper-part in Figure 1; refined alignment is then
obtained. We propose a log-linear model to extract block-
level transliterations with additional informative features,
as illustrated in the lower-part of Figure 1.
</bodyText>
<sectionHeader confidence="0.941434" genericHeader="method">
3 Bi-Stream HMMs for Transliteration
</sectionHeader>
<bodyText confidence="0.999715857142857">
Standard IBM translation models (Brown et al., 1993)
can be used to obtain letter-to-letter translations. How-
ever, these models are not directly suitable, because
letter-alignment within NEs is strictly left-to-right. This
sequential property is well suited to HMMs (Vogel et al.,
1996), in which the jumps from the current aligned posi-
tion can only be forward.
</bodyText>
<subsectionHeader confidence="0.997783">
3.1 Bi-Stream HMMs
</subsectionHeader>
<bodyText confidence="0.999311666666667">
We propose a bi-stream HMM for letter-alignment within
NE pairs. For the source NE fJ1 and a target NE eI1, a bi-
stream HMM is defined as follows:
</bodyText>
<equation confidence="0.999593666666667">
�p(fJ 1 |eI 1)= JJJ
aJ p(fj|eaj)p(cfj|ceaj )p(aj|aj−1), (2)
1 j=1
</equation>
<bodyText confidence="0.999764785714286">
where aj maps fj to the English letter eaj at the position
aj in the English named entity. p(aj|aj−1) is the transi-
tion probability distribution assuming first-order Markov
dependency; p(fj|eaj) is a letter-to-letter translation lex-
icon; cfj is the letter cluster of fj and p(cfj|ceaj ) is a
cluster level translation lexicon. As mentioned in the
above, the vowel/non-vowel linguistic features can be uti-
lized to cluster the letters. The letters from the same clus-
ter tend to share the similar letter transliteration forms.
p(cfj|ceaj ) enables to leverage such letter-correlation in
the transliteration process.
The HMM in Eqn. 2 generates two streams of observa-
tions: the letters together with the letters’ classes follow-
ing the distribution of p(fj|eaj) and p(cfj|ceaj ) at each
</bodyText>
<page confidence="0.999195">
365
</page>
<figureCaption confidence="0.9899675">
Figure 2: Block of letters for transliteration. A block is defined
by the left- and right- boundaries in the NE-pair.
</figureCaption>
<bodyText confidence="0.9384205">
state, respectively. To be in accordance with the mono-
tone nature of the NE’s alignment mentioned before, we
enforce the following constraints in Eqn. 3, so that the
transition can only jump forward or stay at the same state:
</bodyText>
<equation confidence="0.93355">
aj−aj_1&gt;0 dj E 11,J]. (3)
</equation>
<bodyText confidence="0.9999496">
Since the two streams are conditionally independent
given the current state, the extended EM is straight-
forward, with only small modifications of the standard
forward-backward algorithm (Zhao et al., 2005), for pa-
rameter estimation.
</bodyText>
<subsectionHeader confidence="0.998931">
3.2 Designing Letter-Classes
</subsectionHeader>
<bodyText confidence="0.999979875">
Pronunciation is typically highly structured. For in-
stance, in English the pronunciation structure of “cvc”
(consonant-vowel-consonant) is common. By incorpo-
rating letter classes into the proposed two-stream FrMM,
the models’ expressiveness and robustness can be im-
proved. In this work, we focus on transliteration of Ara-
bic NEs into English. We define six non-overlapping
letter classes: vowel, consonant, initial, final, noclass,
and unknown. Initial and final classes represent semantic
markers at the beginning or end of NEs such as “Al” and
“wAl” (in romanization form). Noclass signifies letters
which can be pronounced as both a vowel and a conso-
nant depending on context, for example, the English let-
ter “y”. The unknown class is reserved for punctuations
and letters that we do not have enough linguistic clues for
mapping them to phonemes.
</bodyText>
<sectionHeader confidence="0.984847" genericHeader="method">
4 Transliteration Blocks
</sectionHeader>
<bodyText confidence="0.999943928571429">
To further leverage the information from the letter-
context beyond the letter-classes incorporated in our bi-
stream FrMM in Eqn. 2, we define letter n-grams, which
consist of n consecutive letters, as the basic transliter-
ation unit. A block is defined as a pair of such letter
n-grams which are transliterations of each other. Dur-
ing decoding of unseen NEs, transliteration is performed
block-by-block, rather than letter-by-letter. The goal of
transliteration model is to learn high-quality translitera-
tion blocks from the training data in a unsupervised fash-
ion.
Specifically, a block X can be represented by its left
and right boundaries in the source and target NEs shown
in Figure 2:
</bodyText>
<equation confidence="0.999712">
X = (fj+l j ,ei+k
i ), (4)
</equation>
<bodyText confidence="0.9533675">
where fj+l
j is the source letter-ngram with (l + 1) letters
in source language, and its projection of ei +k iin the En-
glish NE with left boundary at the position of i, and right
boundary at (i + k).
We formulate the block extraction as a local search
problem following the work in Zhao and Waibel (2005):
given a source letter n-gram fj+l
j , search for the pro-
jected boundaries of candidate target letter n-gram ei +k
i
according to a weighted combination of the diverse fea-
tures in a log-linear model detailed in §4.3. The log-linear
model serves as a performance measure to guide the local
search, which, in our setup, is randomized hill-climbing,
to extract bilingual letter n-gram transliteration pairs.
</bodyText>
<subsectionHeader confidence="0.799491">
4.1 Features for Block Transliteration
</subsectionHeader>
<bodyText confidence="0.9998265">
Three features: fertility, distortion, and lexical transla-
tion are investigated for inferring transliteration blocks
from the NE pairs. Each feature corresponds to one as-
pect of the block within the context of a given NE pair.
</bodyText>
<subsectionHeader confidence="0.515307">
4.1.1 Letter n-gram Fertility
</subsectionHeader>
<bodyText confidence="0.999775">
The fertility P(φ|e) of a target letter e specifies the
probability of generating φ source letters for translitera-
tion. The fertilities can be easily read-off from the letter-
alignment, i.e., the output from the Bi-stream FrMM.
Given letter fertility model P(φ|ei), a target letter n-gram
eI1, and a source n-gram fJ1 of length J, we compute a
probability of letter n-gram length relevance: P(J|eI1)
via a dynamic programming.
The probability of generating J letters by the English
letter n-gram eI1 is defined:
</bodyText>
<equation confidence="0.9968">
P(J|eI1) = max
{φ1,J=E;=1 φiI
</equation>
<bodyText confidence="0.779534777777778">
The recursively updated cost φU, i] in dynamic program-
ming is defined as follows:
φU, i] = max I φU, i − 1] + log PNull(0|ei) , (6)
φU − 1,i − 1] + log Pφ(1|ei)
φU − 2,i − 1] + log Pφ(2|ei)
φU − 3, i − 1] + log Pφ(3|ei)
where PNull(0|ei) is the probability of generating a Null
letter from ei; Pφ(k=1|ei) is the letter-fertility model of
generating one source letter from ei; φU, i] is the cost
</bodyText>
<equation confidence="0.996933666666667">
I
P(φi|ei). (5)
i=1
</equation>
<page confidence="0.987329">
366
</page>
<bodyText confidence="0.997397625">
so far for generating j letters from i consecutive English
letters (letter n-gram) ei1 e1, · · · , ei.
After computing the cost of φ[J, I], the probability
P(J|eI1) is computed for generating the length of the
source NE fJ1 from the English NE eI1 shown in Eqn. 5.
With this letter n-gram fertility model, for every block,
we can compute a fertility score to estimate how relevant
the lengths of the transliteration-pairs are.
</bodyText>
<subsectionHeader confidence="0.978892">
4.1.2 Distortion of Centers
</subsectionHeader>
<bodyText confidence="0.9225454">
When aligning blocks of letters within transliteration
pairs, we expect most of them are close to the diagonal
due to the monotone alignment nature. Thus, a simple
position metric is proposed for each block considering
the relative positions within NE-pairs.
The center Ofj+l
j of the source phrase fj+l
j with a
length of (l + 1) is simply a normalized relative position
in the source entity defined as follows:
</bodyText>
<equation confidence="0.9990995">
j&apos; (7)
l + 1.
</equation>
<bodyText confidence="0.993307461538461">
For the center of English letter-phrase ei+k
i , we first
define the expected corresponding relative center for ev-
ery source letter fj0 using the lexicalized position score
as follows:
where P(fj0|ei) is the letter translation lexicon estimated
in IBM Models 1-5. i is the position index, which
is weighted by the letter-level translation probabilities;
the term of Ei+k
i0=i P(fj0|ei0) provides a normalization so
that the expected center is within the range of the target
length. The expected center for ei+k iis simply the aver-
age of the Oei+k i(fj0):
</bodyText>
<equation confidence="0.9810775">
Oei+k
i (fj0) (9)
</equation>
<bodyText confidence="0.930160777777778">
Given the estimated centers of Ofj+l and Oe%+k, we
j
can compute how close they are via the probability of
P(O fjj+l|Oei+ki). In our case, because of the mono-
tone alignment nature of transliteration pairs, a simple
gaussian model is employed to enforce that the point
(Oei+k
i , O fj+l) is not far away from the diagonal.
j
</bodyText>
<subsectionHeader confidence="0.49494">
4.1.3 Letter Lexical Transliteration
</subsectionHeader>
<bodyText confidence="0.996071333333333">
Similar to IBM Model-1 (Brown et al., 1993), we use
a “bag-of-letter” generative model within a block to ap-
proximate the lexical transliteration equivalence:
</bodyText>
<equation confidence="0.982419">
P(fj0|ei0)P(ei0|ei+k
i ), (10)
</equation>
<bodyText confidence="0.992955">
where P(ei0|ei+k
i ) &apos; 1/(k+1) is approximated by a bag-
of-word unigram. Since named entities are usually rela-
tively short, this approximation works reasonably well in
practice.
</bodyText>
<subsectionHeader confidence="0.969328">
4.2 Extended Feature Functions
</subsectionHeader>
<bodyText confidence="0.99991125">
Because of the underlying nature of the noisy-channel
model in our proposed transliteration approach in Section
2, the three base feature functions are extended to cover
the directions both from target-to-source and source-to-
target. Therefore, we have in total six feature functions
for inferring transliteration blocks from a named entity
pair.
Besides the above six feature functions, we also com-
pute the average letter-alignment links per block. We
count the number of letter-alignment links within the
block, and normalize the number by the length of the
source letter-ngram. Note that, we can refine the letter-
alignment by growing the intersections of the two di-
rection letter-alignments from Bi-stream HMM via ad-
ditional aligned letter-pairs seen in the union of the two.
In a way, this approach is similar to those of refining the
word-level alignment for SMT in (Och and Ney, 2003).
This step is shown in the upper-part in Figure 1.
Overall, our proposed feature functions cover rela-
tively different aspects for transliteration blocks: the
block level length relevance probability in Eqn. 5, lexical
translation equivalence, and positions’ distortion from a
gaussian distribution in Eqn. 8, in both directions; and
the average number of letter-alignment links within the
block. Also, these feature functions are positive and
bounded within [0, 1]. Therefore, it is suitable to apply a
log-linear model (in §4.3) to combine the weighted indi-
vidual strengths from the proposed feature functions for
better modeling the quality of the candidate translitera-
tion blocks. This log-linear model will serve as a per-
formance measure in a local-search in §4.4 for inferring
transliteration blocks.
</bodyText>
<subsectionHeader confidence="0.994831">
4.3 Log-Linear Transliteration Model
</subsectionHeader>
<bodyText confidence="0.999957">
We propose a log-linear model to combine the seven fea-
ture functions in §4.1 with proper weights as in Eqn. 11:
</bodyText>
<equation confidence="0.982891">
exp(EM m=1 λmφm(X,e,f))
Pr(X|e,f)=
E,
{X0} exp(EM m=1 λmφm(X&apos;, e, f))
(11)
</equation>
<bodyText confidence="0.999572714285714">
where φm(X, e, f) are the real-valued bounded feature
functions corresponding to the seven models introduced
in §4.1. The log-linear model’s parameters are the
weights {λm} associated with each feature function.
With hand-labeled data, {λm} can be learnt via gen-
eralized iterative scaling algorithm (GIS) (Darroch and
Ratcliff, 1972) or improved iterative scaling (IIS) (Berger
</bodyText>
<equation confidence="0.969994125">
Oei+k
i (fj0) =E(i+k)
k + 1 i0=i P(fj0|ei0)
E(i+k)
1 i0=i i&apos; · P(fj0|ei0)
·
,(8)
Ofj+l = 1 j0=j+l
j �
j0=j
l + 1
1
� j+l
j0=j
l + 1
Oei+k =
i
P(fj+l
j |ei+k
i )=
j0=j iE
i + k
j+l
H
</equation>
<page confidence="0.994321">
367
</page>
<bodyText confidence="0.995385105263158">
et al., 1996). However, as these algorithms are computa-
tionally expensive, we apply an alternative approach us-
ing a simplex down-hill algorithm to optimize the weights
toward better F-measure of block transliterations. Each
feature function corresponds to one dimension in the sim-
plex, and the local optimum only happens at a vertex of
the simplex. Simplex-downhill has several advantages:
it is an efficient approach for optimizing multi-variables
given some performance measure. We compute the F-
measure against a gold-standard block set extracted from
hand-labeled letter-alignment.
To build gold-standard blocks from hand-labeled
letter-alignment, we propose the block transliteration co-
herence in a two-stage fashion. First is the forward pro-
jection: for each candidate source letter-ngram fj+n
j ,
search for its left-most el and right-most er projected
positions in the target NE according to the given letter-
alignment. Second is the backward projection: for the
target letter-gram erl , search for its left-most fl, and right-
most fr, projected positions in the source NE. Now if
l&apos;&gt;j and r&apos;&lt;_j+n, i.e. frl is contained within the source
letter-ngram fj+n
j , then this block X = (fj+n
j , er l ) is de-
fined as coherent for the aligned pairs: (fj+n
j , erl ) . We
accept coherent X as gold-standard blocks. This block
transliteration coherence is generally sound for extracting
the gold-blocks mostly because of the the monotone left-
to-right nature of the letter-alignment for transliteration.
A related coherence assumption can be found in (Fox,
2002), where their assumption on phrase-pairs for sta-
tistical machine translation is shown to be somewhat re-
strictive for SMT. This is mainly because the word align-
ment is often non-monotone, especially for langauge-
pairs from different families such as Arabic-English and
Chinese-English.
</bodyText>
<subsectionHeader confidence="0.999188">
4.4 Aligning Letter-Blocks: a Local Search
</subsectionHeader>
<bodyText confidence="0.977691">
Aligning the blocks within NE pairs can be formulated
as a local search given the heuristic function defined in
Eqn. 11. To be more specific: given a Arabic letter-ngram
fj+l
j , our algorithm searches for the best translation can-
didate ei+k iin the target named entities. In our implemen-
tation, we use stochastic hill-climbing with Eqn. 11 as the
performance measure. Down-hill moves are accepted to
allow one or two left and right null letters to be attached
to ei+k ito expand the table of transliteration-blocks.
To make the local search more effective, we normal-
ize the letter translation lexicon p(f|e) within the parallel
entity pair as in:
</bodyText>
<equation confidence="0.992444">
P�(f |e) = J P(f |e)
�j,=1 P(fj,|e)
</equation>
<bodyText confidence="0.999972083333334">
In this way, the distribution of P�(f|e) is sharper and more
focused in the context of an entity pair.
Overall, given the parallel NE pairs, we can train the
letter level translation models in both directions via the
Bi-stream HMM in Eqn. 2. From the letter-alignment,
we can build the letter translation lexicons and fertility
tables. With these tables, the base feature functions are
then computed for each candidate block, and the features
are combined in the log-linear model in Eqn. 11. Given
a named-entity pair in the training data, we rank all the
transliteration blocks by the scores using the log-linear
model. This step is shown in the lower-part in Figure 1.
</bodyText>
<subsectionHeader confidence="0.996388">
4.5 Decoding Unseen NEs
</subsectionHeader>
<bodyText confidence="0.9999496">
The decoding of NEs is an extension to the noisy-channel
scheme in Eqn. 1. In our configurations for NE translit-
eration, the extracted transliteration blocks are used. Our
letter ngram is a standard letter-ngram model trained us-
ing the SriLM toolkit (Stolcke, 2002). To transliterate the
unseen NEs, the decoder (Hewavitharana et al., 2005) is
configured for monotone decoding. It loads the transliter-
ation blocks and the letter-ngram LM, and it decodes the
unseen Arabic named entities with block-based translit-
eration from left to right.
</bodyText>
<sectionHeader confidence="0.997218" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996262">
5.1 The Data
</subsectionHeader>
<bodyText confidence="0.999929130434783">
We have 74,887 bilingual geographic names from
LDC2005G01-NGA, 11,212 bilingual person names
from LDC2005G021, and about 6,000 bilingual names
extracted from the BAMA2 dictionary. In total, there are
92,099 NE pairs. We split them into three parts: 91,459
pairs as the training dataset, 100 pairs as the development
dataset, and 540 unique NE pairs as the held-out dataset.
An additional test set is collected from the TIDES 2003
Arabic-English machine translation evaluation test set.
The 663 sentences contain 286 unique words, which were
not covered by the available training data. From this set
of untranslated words, we manually labeled the entities of
persons, locations and organizations, giving a total of 97
unique un-translated NEs. The BAMA toolkit was used
to romanize the Arabic words. Some names from this test
set are shown in Figure 1.
These untranslated NEs make up only a very small
fraction of all words in the test set. Therefore, having
correct transliterations would give only small improve-
ments in terms of BLEU (Papineni et al., 2002) and NIST
scores. However, successfully translating these unknown
NEs is very crucial for cross-lingual distillation tasks or
question-answering based on the MT-output.
</bodyText>
<footnote confidence="0.990147">
1The corpus is provided as FOUO (for official use only) in
the DARPA-GALE project
2LDC2004L02: Buckwalter Arabic Morphological Ana-
lyzer version 2.0
</footnote>
<equation confidence="0.262616">
(12)
</equation>
<page confidence="0.995364">
368
</page>
<tableCaption confidence="0.919628">
Table 1: Test Set Examples.
Table 2: Transliteration accuracy for different translitera-
tion models.
</tableCaption>
<table confidence="0.985477">
System Accuracy
Baseline 39.18%
L-Block 41.24%
LCBE 46.39%
</table>
<bodyText confidence="0.999859785714286">
To evaluate the transliteration performance, we use
edit-distance between the hypothesis against a reference
set. This is to count the number of insertions, dele-
tions, and substitutions required to correct the hypoth-
esis to match the given reference. An edit-distance of
zero is a perfect match. However, NEs typically have
more than one correct variant. For example, the Arabic
name “mHmd” (in romanized form) can be transliterated
as Muhammad or Mohammed; both are considered as
correct transliterations. Ideally, we want to have all vari-
ants as reference transliterations. To enable our translit-
eration evaluation to be more informative given only one
reference, edit-distance of one between hypothesis and
reference is considered to be an acceptable match.
</bodyText>
<subsectionHeader confidence="0.999921">
5.2 Comparison of Transliteration Models
</subsectionHeader>
<bodyText confidence="0.999988542372882">
We compare the performance of three systems within our
proposed framework in Figure.1: the baseline Block sys-
tem, a system in which we use a log-linear combination
of alignment features as described in §4.3, we call the the
L-Block system, and finally a system, which also uses
the bi-stream HMM alignment model as described in §3.
This last system will be denoted LCBE system.
The baseline is based on the refined letter-alignment
from the two directions of IBM-Model-4, trained with a
scheme of 15h545 using GIZA++ (Och and Ney, 2004).
The final alignment was obtained by growing the inter-
sections between Arabic-to-English (AE) and English-
to-Arabic (EA) alignments with additional aligned letter-
pairs seen in the union. This is to compensate for the
inherent asymmetry in alignment models. Blocks (letter-
ngram pairs) were collected directly from the refined
letter-alignment, using the same algorithm as described
in §4.3 for extracting gold-standard letter blocks. There is
no length restrictions to the letter-ngram extracted in our
system. All the blocks were then scored using relative
frequencies and lexical scores in both directions, similar
to the scoring of phrase-pairs in SMT (Koehn, 2004).
In the L-Block system additional feature functions as
defined in §4.1 were computed on top of the letter-level
alignment obtained from the baseline system. A log-
linear model combining these features was learned with
the gold-blocks described in §4.3. Transliteration blocks
were extracted using the local-search §4.4. The other
components remained the same as in the baseline system.
The LCBE system is an extension to both the baseline
and the L-Block system. The key difference in LCBE
is that our proposed bi-stream HMM in Eqn. 2 was ap-
plied in both directions with extended letter-classes. The
resulting combined alignment was used together with all
features of the L-Block system to guide the local-search
for extracting the blocks. The same procedure of decod-
ing was then carried out for the unseen NEs using the
extracted blocks.
To build the letter language model for the decoding
process, we first split the English entities into charac-
ters; additional position indicators “ begin” and “ end”
were added to the begin and end position of the named-
entity; “ middle” was added between the first name and
last name. A letter-trigram language model with SRI LM
toolkit (Stolcke, 2002) was then built using the target side
(English) of NE pairs tagged with the above position in-
formation.
Table 2 shows that the baseline system gives an accu-
racy of 39.18%, while the extended systems L-Block and
LCBE give 41.24% and 46.39%, respectively. These re-
sults show that the additional features besides the letter-
alignment are helpful. The L-Block system, which uses
these features, outperforms the baseline system signifi-
cantly by 2.1% absolute in accuracy. The results also
show that the bi-stream HMM alignment, which uses not
only the letters but also the letter-classes, leads to signif-
icant improvement. It outperforms the L-Block system,
which does not leverage the letter-classes and monotone
alignment, by 4.15% absolute.
</bodyText>
<subsectionHeader confidence="0.998653">
5.3 Incorporation of Spell Checking
</subsectionHeader>
<bodyText confidence="0.999987333333333">
Our spelling-checker is based on the suggested word-
forms from web search engines for ambiguous candi-
dates. We collected web statistics frequency for both the
proposed transliteration candidates from our system, and
also the suggested candidates from web-search engines.
All the candidates were re-ranked by their frequencies.
Figure 3 shows the performances on the held-out set,
using system LCBE augmented with a spell-checker
(LCBE+Spell), with varying sizes of N-best hypotheses
lists. The held-out set contains 540 unique named entity
pairs. We show accuracy when exact match is requested
and when an edit distances of one is allowed.
</bodyText>
<page confidence="0.997807">
369
</page>
<tableCaption confidence="0.684249">
Table 3: Transliteration examples between LCBE+Spell
and Google web translation.
</tableCaption>
<figureCaption confidence="0.99674">
Figure 3: Transliteration accuracy of LCBE and LCBE+Spell
models for 540 named entity pairs in the held-out set.
Figure 4: Transliteration accuracy of N-best hypotheses for
LCBE and LCBE+Spell models it the MT-03 test set.
</figureCaption>
<bodyText confidence="0.988959636363636">
Figure 4 shows the performances in the unseen test set
of LCBE and LCBE+Spell, with varying sizes of N-best
hypotheses lists. LCBE+Spell reaches 52% accuracy in
1-best hypothesis. In the 5-best and 10-best cases, the ac-
curacies of LCBE+Spell system archive the highest per-
formances with 66% and 72.16% respectively. The spell-
checker increases the 1-best accuracy by 11.12% and the
10-best accuracy by 7.69%. All these improvements are
statistically significant. These results are also comparable
to other state-of-the-art statistical Arabic name transliter-
ation systems such as (Al-Onaizan and Knight, 2002).
</bodyText>
<subsectionHeader confidence="0.990785">
5.4 Comparison with the Google Web Translation
</subsectionHeader>
<bodyText confidence="0.99980325">
We finally compared our best system with the
state-of-the-art Arabic-English Google Web Translation
(Google). Table 3 shows transliteration examples from
our best system in comparison with Google (as in June
20, 2006)3. The Google system achieved 45.36% accu-
racy for the 1-best hypothesis, which is comparable to
the results when using the LCBE transliteration system,
while LCBE+Spell archived 52%.
</bodyText>
<footnote confidence="0.987937">
3http://www.google.com/translate t
</footnote>
<sectionHeader confidence="0.992227" genericHeader="conclusions">
6 Conclusions and Discussions
</sectionHeader>
<bodyText confidence="0.999494115384615">
In this paper we proposed a novel transliteration model.
Viewing transliteration as a translation task we adopt
alignment and decoding techniques used in a phrase-
based statistical machine translation system to work on
letter sequences instead of word sequences. To improve
the performance we extended the FMM alignment model
into a bi-stream FMM alignment by incorporating letter-
classes into the alignment process. We also showed that a
block-extraction approach, which uses a log-linear com-
bination of multiple alignment features, can give signif-
icant improvements in transliteration accuracy. Finally,
spell-checking based on work occurrence statistics ob-
tained from the web gave an additional boost in translit-
eration accuracy.
The goal for this work is to improve the quality of ma-
chine translation, esp. when used in cross-lingual infor-
mation retrieval and distillation tasks, by incorporating
the proposed framework to handle unknown words. Fig-
ure 5 gives an example of the difference named entity
transliteration can make. Shown are the original SMT
system output, the translation when the proposed translit-
eration models are used to translate the unknown named-
entities, and the reference translation. A comparison of
the two SMT outputs indicates that integrating the pro-
posed transliteration model into our machine translation
system can significantly improve translation utility.
</bodyText>
<sectionHeader confidence="0.917855" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9806955">
This work was partially supported by grants from
DARPA (GALE project) and NFS (Str-Dust project).
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9952058">
Nasreen AbdulJaleel and Leah Larkey. 2003. Statistical
transliteration for English-Arabic cross language informa-
tion retrieval. In Proceedings of the 12th International
Conference on Information and Knowledge Management,
New Orleans, LA, USA, November.
</reference>
<page confidence="0.997849">
370
</page>
<figureCaption confidence="0.842639">
Figure 5: Incorporation of the transliteration model to our
SMT System.
</figureCaption>
<reference confidence="0.999435022727272">
Yaser Al-Onaizan and Kevin Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of ACL
Workshop on Computational Approaches to Semitic Lan-
guages, Philadelphia, PA, USA.
Mansur Arbabi, Scott M. Fischthal, Vincent C. Cheng, and
Elizabeth Bart. 1994. Algorithms for Arabic name translit-
eration. In IBM Journal of Research and Development,
volume 38(2), pages 183–193.
Adam L. Berger, Vincent Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. In Computational Linguistics,
volume 22 of 1, pages 39–71, March.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. In Computa-
tional Linguistics, volume 19(2), pages 263–331.
J.N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. In Annals of Mathematical
Statistics, volume 43, pages 1470–1480.
Asif Ekbal, S. Naskar, and S. Bandyopadhyay. 2006. A modi-
fied joint source channel model for machine transliteration.
In Proceedings of COLING/ACL, pages 191–198, Australia.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 304–311,
Philadelphia, PA, July 6-7.
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa
Ehara. 2003. Transliteration considering context informa-
tion based on the maximum entropy method. In Proceedings
ofMT-Summit IY, New Orleans, Louisiana, USA.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hildebrand,
Matthias Eck, Chiori Hori, Stephan Vogel, and Alex Waibel.
2005. The CMU statistical machine translation system
for IWSLT2005. In The 2005 International Workshop on
Spoken Language Translation.
Fei Huang. 2005. Cluster-specific name transliteration. In
Proceedings of the HLT-EMNLP 2005, Vancouver, BC,
Canada, October.
Kevin Knight and Jonathan Graehl. 1997. Machine transliter-
ation. In Proceedings of the Conference of the Association
for Computational Linguistics (ACL), Madrid, Spain.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based smt. In Proceedings of the Conference of
the Association for Machine Translation in the Americans
(AMTA), Washington DC, USA.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-
channel model for machine transliteration. In Proceedings
of 42nd ACL, pages 159–166, Barcelona, Spain.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 1:29, pages 19–51.
Franz J. Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. In Computa-
tional Linguistics, volume 30, pages 417–449.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-Korean
transliteration model using pronunciation and contextual
rules. In Proceedings of COLING-2002, pages 1–7, Taipei,
Taiwan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proc. of the 40th Annual Conf. of the
Association for Computational Linguistics (ACL 02), pages
311–318, Philadelphia, PA, July.
Bonnie Stalls and Kevin Knight. 1998. Translating names
and technical terms in Arabic text. In Proceedings of the
COLING/ACL Workshop on Computational Approaches to
Semitic Languages, Montreal, Quebec, Canada.
Andreas Stolcke. 2002. SRILM – An extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 901–904, Denver.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration
of proper names in cross-lingual information retrieval. In
Proceedings of the ACL Workshop on Multi-lingual Named
Entity Recognition, Edmonton, Canada.
Stephan. Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM based word alignment in statistical machine
translation. In Proc. The 16th Int. Conf. on Computational
Lingustics, (COLING-1996), pages 836–841, Copenhagen,
Denmark.
Bing Zhao and Alex Waibel. 2005. Learning a log-linear
model with bilingual phrase-pair features for statistical
machine translation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, Jeju Island,
Korean, October.
Bing Zhao, Eric P. Xing, and Alex Waibel. 2005. Bilingual
word spectral clustering for statistical machine translation.
In Proceedings of the ACL Workshop on Building and Using
Parallel Texts, pages 25–32, Ann Arbor, Michigan, June.
</reference>
<page confidence="0.998736">
371
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423160">
<title confidence="0.999063">A Log-linear Block Transliteration Model based on Bi-Stream HMMs</title>
<author confidence="0.99909">Nguyen Bach Zhao</author>
<author confidence="0.99909">Ian Lane</author>
<email confidence="0.951323">nbach,ianlane,</email>
<affiliation confidence="0.7359875">Language Technologies School of Computer Science, Carnegie Mellon University</affiliation>
<abstract confidence="0.995710789473684">We propose a novel HMM-based framework to accurately transliterate unseen named entities. The framework leverages features in letteralignment and letter n-gram pairs learned from available bilingual dictionaries. Letter-classes, such as vowels/non-vowels, are integrated to further improve transliteration accuracy. The proposed transliteration system is applied to out-of-vocabulary named-entities in statistical machine translation (SMT), and a significant improvement over traditional transliteration approach is obtained. Furthermore, by incorporating an automatic spell-checker based on statistics collected from web search engines, transliteration accuracy is further improved. The proposed system is implemented within our SMT system and applied to a real translation scenario from Arabic to English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nasreen AbdulJaleel</author>
<author>Leah Larkey</author>
</authors>
<title>Statistical transliteration for English-Arabic cross language information retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th International Conference on Information and Knowledge Management,</booktitle>
<location>New Orleans, LA, USA,</location>
<contexts>
<context position="3485" citStr="AbdulJaleel and Larkey (2003)" startWordPosition="475" endWordPosition="479">in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthograph</context>
<context position="5474" citStr="AbdulJaleel and Larkey (2003)" startWordPosition="749" endWordPosition="752">g-linear model is used to combine 364 Proceedings of NAACL HLT 2007, pages 364–371, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics Figure 1: Transliteration System Structure. The upper-part is the two-directional Bi-Stream HMM for letter-alignment; the lower-part is a log-linear model for combining different feature functions for block-level transliteration. these features to learn block-level transliteration-pairs from training data. The proposed transliteration framework obtained significant improvements over a strong baseline transliteration approach similar to AbdulJaleel and Larkey (2003) and Virga and Khudanpur (2003). The remainder of this paper is organized as follows. In Section 2, we formulate the transliteration as a general translation problem; in Section 4, we propose a log-linear alignment model with a local search algorithm to model the letter n-gram translation pairs; in Section 5, experiments are presented. Conclusions and discussions are given in Section 6. 2 Transliteration as Translation Transliteration can be viewed as a special case of translation. In this approach, source and target NEs are split into letter sequences, and each sequence is treated as apseudo </context>
</contexts>
<marker>AbdulJaleel, Larkey, 2003</marker>
<rawString>Nasreen AbdulJaleel and Leah Larkey. 2003. Statistical transliteration for English-Arabic cross language information retrieval. In Proceedings of the 12th International Conference on Information and Knowledge Management, New Orleans, LA, USA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Machine transliteration of names in Arabic text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="2914" citStr="Al-Onaizan and Knight (2002)" startWordPosition="392" endWordPosition="395"> Arabic to English. The work in (Arbabi et al., 1994), to our knowledge, is the first work on machine transliteration of Arabic names into English, French, and Spanish. The idea is to vowelize Arabic names by adding appropriate vowels and utilizing a phonetic look-up table to provide the spelling in the target language. Their framework is strictly applicable within standard Arabic morphological rules. Knight and Graehl (1997) introduced finite state transducers that implement back-transliteration from Japanese to English, which was then extended to Arabic-English in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment mode</context>
<context position="29369" citStr="Al-Onaizan and Knight, 2002" startWordPosition="4581" endWordPosition="4584"> the MT-03 test set. Figure 4 shows the performances in the unseen test set of LCBE and LCBE+Spell, with varying sizes of N-best hypotheses lists. LCBE+Spell reaches 52% accuracy in 1-best hypothesis. In the 5-best and 10-best cases, the accuracies of LCBE+Spell system archive the highest performances with 66% and 72.16% respectively. The spellchecker increases the 1-best accuracy by 11.12% and the 10-best accuracy by 7.69%. All these improvements are statistically significant. These results are also comparable to other state-of-the-art statistical Arabic name transliteration systems such as (Al-Onaizan and Knight, 2002). 5.4 Comparison with the Google Web Translation We finally compared our best system with the state-of-the-art Arabic-English Google Web Translation (Google). Table 3 shows transliteration examples from our best system in comparison with Google (as in June 20, 2006)3. The Google system achieved 45.36% accuracy for the 1-best hypothesis, which is comparable to the results when using the LCBE transliteration system, while LCBE+Spell archived 52%. 3http://www.google.com/translate t 6 Conclusions and Discussions In this paper we proposed a novel transliteration model. Viewing transliteration as a </context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Machine transliteration of names in Arabic text. In Proceedings of ACL Workshop on Computational Approaches to Semitic Languages, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mansur Arbabi</author>
<author>Scott M Fischthal</author>
<author>Vincent C Cheng</author>
<author>Elizabeth Bart</author>
</authors>
<title>Algorithms for Arabic name transliteration.</title>
<date>1994</date>
<journal>In IBM Journal of Research and Development,</journal>
<volume>38</volume>
<issue>2</issue>
<pages>183--193</pages>
<contexts>
<context position="2339" citStr="Arbabi et al., 1994" startWordPosition="306" endWordPosition="309">d via transliteration: mapping symbols from one writing system to another. Letters of the source language are typically transformed into the target language with similar pronunciation. Transliteration between languages which share similar alphabets and sound systems is usually not difficult, because the majority of letters remain the same. However, the task is significantly more difficult when the language pairs are considerably different, for example, English-Arabic, English-Chinese, and English-Japanese. In this paper, we focus on forward transliteration from Arabic to English. The work in (Arbabi et al., 1994), to our knowledge, is the first work on machine transliteration of Arabic names into English, French, and Spanish. The idea is to vowelize Arabic names by adding appropriate vowels and utilizing a phonetic look-up table to provide the spelling in the target language. Their framework is strictly applicable within standard Arabic morphological rules. Knight and Graehl (1997) introduced finite state transducers that implement back-transliteration from Japanese to English, which was then extended to Arabic-English in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named ent</context>
</contexts>
<marker>Arbabi, Fischthal, Cheng, Bart, 1994</marker>
<rawString>Mansur Arbabi, Scott M. Fischthal, Vincent C. Cheng, and Elizabeth Bart. 1994. Algorithms for Arabic name transliteration. In IBM Journal of Research and Development, volume 38(2), pages 183–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>22</volume>
<pages>39--71</pages>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. In Computational Linguistics, volume 22 of 1, pages 39–71, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>19</volume>
<issue>2</issue>
<pages>263--331</pages>
<contexts>
<context position="3537" citStr="Brown et al., 1993" startWordPosition="484" endWordPosition="487">nsliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between Englis</context>
<context position="8261" citStr="Brown et al., 1993" startWordPosition="1172" endWordPosition="1175">led at either letter-to-letter level, or letter n-gram transliteration level (block-level). Our transliteration models are illustrated in Figure 1. We propose a Bi-Stream HMM of P(fJ1 |eI1) to infer letter-to-letter alignments in two directions: Arabic-toEnglish (F-to-E) and English-to-Arabic (E-to-F), shown in the upper-part in Figure 1; refined alignment is then obtained. We propose a log-linear model to extract blocklevel transliterations with additional informative features, as illustrated in the lower-part of Figure 1. 3 Bi-Stream HMMs for Transliteration Standard IBM translation models (Brown et al., 1993) can be used to obtain letter-to-letter translations. However, these models are not directly suitable, because letter-alignment within NEs is strictly left-to-right. This sequential property is well suited to HMMs (Vogel et al., 1996), in which the jumps from the current aligned position can only be forward. 3.1 Bi-Stream HMMs We propose a bi-stream HMM for letter-alignment within NE pairs. For the source NE fJ1 and a target NE eI1, a bistream HMM is defined as follows: �p(fJ 1 |eI 1)= JJJ aJ p(fj|eaj)p(cfj|ceaj )p(aj|aj−1), (2) 1 j=1 where aj maps fj to the English letter eaj at the position </context>
<context position="15634" citStr="Brown et al., 1993" startWordPosition="2406" endWordPosition="2409">term of Ei+k i0=i P(fj0|ei0) provides a normalization so that the expected center is within the range of the target length. The expected center for ei+k iis simply the average of the Oei+k i(fj0): Oei+k i (fj0) (9) Given the estimated centers of Ofj+l and Oe%+k, we j can compute how close they are via the probability of P(O fjj+l|Oei+ki). In our case, because of the monotone alignment nature of transliteration pairs, a simple gaussian model is employed to enforce that the point (Oei+k i , O fj+l) is not far away from the diagonal. j 4.1.3 Letter Lexical Transliteration Similar to IBM Model-1 (Brown et al., 1993), we use a “bag-of-letter” generative model within a block to approximate the lexical transliteration equivalence: P(fj0|ei0)P(ei0|ei+k i ), (10) where P(ei0|ei+k i ) &apos; 1/(k+1) is approximated by a bagof-word unigram. Since named entities are usually relatively short, this approximation works reasonably well in practice. 4.2 Extended Feature Functions Because of the underlying nature of the noisy-channel model in our proposed transliteration approach in Section 2, the three base feature functions are extended to cover the directions both from target-to-source and source-totarget. Therefore, we</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. In Computational Linguistics, volume 19(2), pages 263–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<booktitle>In Annals of Mathematical Statistics,</booktitle>
<volume>43</volume>
<pages>1470--1480</pages>
<contexts>
<context position="18241" citStr="Darroch and Ratcliff, 1972" startWordPosition="2808" endWordPosition="2811">asure in a local-search in §4.4 for inferring transliteration blocks. 4.3 Log-Linear Transliteration Model We propose a log-linear model to combine the seven feature functions in §4.1 with proper weights as in Eqn. 11: exp(EM m=1 λmφm(X,e,f)) Pr(X|e,f)= E, {X0} exp(EM m=1 λmφm(X&apos;, e, f)) (11) where φm(X, e, f) are the real-valued bounded feature functions corresponding to the seven models introduced in §4.1. The log-linear model’s parameters are the weights {λm} associated with each feature function. With hand-labeled data, {λm} can be learnt via generalized iterative scaling algorithm (GIS) (Darroch and Ratcliff, 1972) or improved iterative scaling (IIS) (Berger Oei+k i (fj0) =E(i+k) k + 1 i0=i P(fj0|ei0) E(i+k) 1 i0=i i&apos; · P(fj0|ei0) · ,(8) Ofj+l = 1 j0=j+l j � j0=j l + 1 1 � j+l j0=j l + 1 Oei+k = i P(fj+l j |ei+k i )= j0=j iE i + k j+l H 367 et al., 1996). However, as these algorithms are computationally expensive, we apply an alternative approach using a simplex down-hill algorithm to optimize the weights toward better F-measure of block transliterations. Each feature function corresponds to one dimension in the simplex, and the local optimum only happens at a vertex of the simplex. Simplex-downhill has</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J.N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. In Annals of Mathematical Statistics, volume 43, pages 1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asif Ekbal</author>
<author>S Naskar</author>
<author>S Bandyopadhyay</author>
</authors>
<title>A modified joint source channel model for machine transliteration.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>191--198</pages>
<contexts>
<context position="4209" citStr="Ekbal et al. (2006)" startWordPosition="580" endWordPosition="583">airs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between English and Chinese, and an extended model is presented in Ekbal et al. (2006). We propose a block-level transliteration framework, as shown in Figure 1, to model letter-level context information for transliteration at two levels. First, we propose a bi-stream HMM incorporating letter-clusters to better model the vowel and non-vowel transliterations with position-information, i.e., initial and final, to improve the letter-level alignment accuracy. Second, based on the letter-alignment, we propose letter n-gram (lettersequence) alignment models (block) to automatically learn the mappings from source letter n-grams to target letter n-grams. A few features specific for tra</context>
</contexts>
<marker>Ekbal, Naskar, Bandyopadhyay, 2006</marker>
<rawString>Asif Ekbal, S. Naskar, and S. Bandyopadhyay. 2006. A modified joint source channel model for machine transliteration. In Proceedings of COLING/ACL, pages 191–198, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>304--311</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="20030" citStr="Fox, 2002" startWordPosition="3113" endWordPosition="3114"> projection: for the target letter-gram erl , search for its left-most fl, and rightmost fr, projected positions in the source NE. Now if l&apos;&gt;j and r&apos;&lt;_j+n, i.e. frl is contained within the source letter-ngram fj+n j , then this block X = (fj+n j , er l ) is defined as coherent for the aligned pairs: (fj+n j , erl ) . We accept coherent X as gold-standard blocks. This block transliteration coherence is generally sound for extracting the gold-blocks mostly because of the the monotone leftto-right nature of the letter-alignment for transliteration. A related coherence assumption can be found in (Fox, 2002), where their assumption on phrase-pairs for statistical machine translation is shown to be somewhat restrictive for SMT. This is mainly because the word alignment is often non-monotone, especially for langaugepairs from different families such as Arabic-English and Chinese-English. 4.4 Aligning Letter-Blocks: a Local Search Aligning the blocks within NE pairs can be formulated as a local search given the heuristic function defined in Eqn. 11. To be more specific: given a Arabic letter-ngram fj+l j , our algorithm searches for the best translation candidate ei+k iin the target named entities. </context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 304–311, Philadelphia, PA, July 6-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Naoto Kato</author>
<author>Noriyoshi Uratani</author>
<author>Terumasa Ehara</author>
</authors>
<title>Transliteration considering context information based on the maximum entropy method.</title>
<date>2003</date>
<booktitle>In Proceedings ofMT-Summit IY,</booktitle>
<location>New Orleans, Louisiana, USA.</location>
<contexts>
<context position="3923" citStr="Goto et al. (2003)" startWordPosition="538" endWordPosition="541">ins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between English and Chinese, and an extended model is presented in Ekbal et al. (2006). We propose a block-level transliteration framework, as shown in Figure 1, to model letter-level context information for transliteration at two levels. First, we propose a bi-stream HMM incorporating letter-clusters to better model the vowel and non-vowel transliterations with position-information, i.e., initial</context>
</contexts>
<marker>Goto, Kato, Uratani, Ehara, 2003</marker>
<rawString>Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa Ehara. 2003. Transliteration considering context information based on the maximum entropy method. In Proceedings ofMT-Summit IY, New Orleans, Louisiana, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjika Hewavitharana</author>
<author>Bing Zhao</author>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Chiori Hori</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>The CMU statistical machine translation system for IWSLT2005.</title>
<date>2005</date>
<booktitle>In The 2005 International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="22076" citStr="Hewavitharana et al., 2005" startWordPosition="3447" endWordPosition="3450"> candidate block, and the features are combined in the log-linear model in Eqn. 11. Given a named-entity pair in the training data, we rank all the transliteration blocks by the scores using the log-linear model. This step is shown in the lower-part in Figure 1. 4.5 Decoding Unseen NEs The decoding of NEs is an extension to the noisy-channel scheme in Eqn. 1. In our configurations for NE transliteration, the extracted transliteration blocks are used. Our letter ngram is a standard letter-ngram model trained using the SriLM toolkit (Stolcke, 2002). To transliterate the unseen NEs, the decoder (Hewavitharana et al., 2005) is configured for monotone decoding. It loads the transliteration blocks and the letter-ngram LM, and it decodes the unseen Arabic named entities with block-based transliteration from left to right. 5 Experiments 5.1 The Data We have 74,887 bilingual geographic names from LDC2005G01-NGA, 11,212 bilingual person names from LDC2005G021, and about 6,000 bilingual names extracted from the BAMA2 dictionary. In total, there are 92,099 NE pairs. We split them into three parts: 91,459 pairs as the training dataset, 100 pairs as the development dataset, and 540 unique NE pairs as the held-out dataset.</context>
</contexts>
<marker>Hewavitharana, Zhao, Hildebrand, Eck, Hori, Vogel, Waibel, 2005</marker>
<rawString>Sanjika Hewavitharana, Bing Zhao, Almut Silja Hildebrand, Matthias Eck, Chiori Hori, Stephan Vogel, and Alex Waibel. 2005. The CMU statistical machine translation system for IWSLT2005. In The 2005 International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
</authors>
<title>Cluster-specific name transliteration.</title>
<date>2005</date>
<booktitle>In Proceedings of the HLT-EMNLP 2005,</booktitle>
<location>Vancouver, BC, Canada,</location>
<contexts>
<context position="3146" citStr="Huang (2005)" startWordPosition="428" endWordPosition="429"> a phonetic look-up table to provide the spelling in the target language. Their framework is strictly applicable within standard Arabic morphological rules. Knight and Graehl (1997) introduced finite state transducers that implement back-transliteration from Japanese to English, which was then extended to Arabic-English in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Let</context>
</contexts>
<marker>Huang, 2005</marker>
<rawString>Fei Huang. 2005. Cluster-specific name transliteration. In Proceedings of the HLT-EMNLP 2005, Vancouver, BC, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>Machine transliteration.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</booktitle>
<location>Madrid,</location>
<contexts>
<context position="2715" citStr="Knight and Graehl (1997)" startWordPosition="366" endWordPosition="369">ntly more difficult when the language pairs are considerably different, for example, English-Arabic, English-Chinese, and English-Japanese. In this paper, we focus on forward transliteration from Arabic to English. The work in (Arbabi et al., 1994), to our knowledge, is the first work on machine transliteration of Arabic names into English, French, and Spanish. The idea is to vowelize Arabic names by adding appropriate vowels and utilizing a phonetic look-up table to provide the spelling in the target language. Their framework is strictly applicable within standard Arabic morphological rules. Knight and Graehl (1997) introduced finite state transducers that implement back-transliteration from Japanese to English, which was then extended to Arabic-English in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All o</context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1997. Machine transliteration. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based smt.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation in the Americans (AMTA),</booktitle>
<location>Washington DC, USA.</location>
<contexts>
<context position="6285" citStr="Koehn, 2004" startWordPosition="877" endWordPosition="878">inear alignment model with a local search algorithm to model the letter n-gram translation pairs; in Section 5, experiments are presented. Conclusions and discussions are given in Section 6. 2 Transliteration as Translation Transliteration can be viewed as a special case of translation. In this approach, source and target NEs are split into letter sequences, and each sequence is treated as apseudo sentence. The appealing reason of formulating transliteration in this way is to utilize advanced alignment models, which share ideas applied also within phrase-based statistical machine translation (Koehn, 2004). To apply this approach to transliteration, however, some unique aspects should be considered. First, letters should be generated from left to right, without any reordering. Thus, the transliteration models can only execute forward sequential jumps. Second, for unvowelized languages such as Arabic, a single Arabic letter typically maps to less than four English letters. Thus, the fertility for each letter should be recognized to ensure reasonable length relevance. Third, the position of the letter within a NE is important. For example, in Arabic, letters such as “al” at the beginning of the N</context>
<context position="25851" citStr="Koehn, 2004" startWordPosition="4033" endWordPosition="4034">sections between Arabic-to-English (AE) and Englishto-Arabic (EA) alignments with additional aligned letterpairs seen in the union. This is to compensate for the inherent asymmetry in alignment models. Blocks (letterngram pairs) were collected directly from the refined letter-alignment, using the same algorithm as described in §4.3 for extracting gold-standard letter blocks. There is no length restrictions to the letter-ngram extracted in our system. All the blocks were then scored using relative frequencies and lexical scores in both directions, similar to the scoring of phrase-pairs in SMT (Koehn, 2004). In the L-Block system additional feature functions as defined in §4.1 were computed on top of the letter-level alignment obtained from the baseline system. A loglinear model combining these features was learned with the gold-blocks described in §4.3. Transliteration blocks were extracted using the local-search §4.4. The other components remained the same as in the baseline system. The LCBE system is an extension to both the baseline and the L-Block system. The key difference in LCBE is that our proposed bi-stream HMM in Eqn. 2 was applied in both directions with extended letter-classes. The </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based smt. In Proceedings of the Conference of the Association for Machine Translation in the Americans (AMTA), Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint sourcechannel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of 42nd ACL,</booktitle>
<pages>159--166</pages>
<location>Barcelona,</location>
<contexts>
<context position="4036" citStr="Li et al. (2004)" startWordPosition="554" endWordPosition="557">d in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between English and Chinese, and an extended model is presented in Ekbal et al. (2006). We propose a block-level transliteration framework, as shown in Figure 1, to model letter-level context information for transliteration at two levels. First, we propose a bi-stream HMM incorporating letter-clusters to better model the vowel and non-vowel transliterations with position-information, i.e., initial and final, to improve the letter-level alignment accuracy. Second, based on the letter-alignment, we propose let</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint sourcechannel model for machine transliteration. In Proceedings of 42nd ACL, pages 159–166, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<contexts>
<context position="16877" citStr="Och and Ney, 2003" startWordPosition="2600" endWordPosition="2603">e functions for inferring transliteration blocks from a named entity pair. Besides the above six feature functions, we also compute the average letter-alignment links per block. We count the number of letter-alignment links within the block, and normalize the number by the length of the source letter-ngram. Note that, we can refine the letteralignment by growing the intersections of the two direction letter-alignments from Bi-stream HMM via additional aligned letter-pairs seen in the union of the two. In a way, this approach is similar to those of refining the word-level alignment for SMT in (Och and Ney, 2003). This step is shown in the upper-part in Figure 1. Overall, our proposed feature functions cover relatively different aspects for transliteration blocks: the block level length relevance probability in Eqn. 5, lexical translation equivalence, and positions’ distortion from a gaussian distribution in Eqn. 8, in both directions; and the average number of letter-alignment links within the block. Also, these feature functions are positive and bounded within [0, 1]. Therefore, it is suitable to apply a log-linear model (in §4.3) to combine the weighted individual strengths from the proposed featur</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. In Computational Linguistics, volume 1:29, pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>30</volume>
<pages>417--449</pages>
<contexts>
<context position="25184" citStr="Och and Ney, 2004" startWordPosition="3932" endWordPosition="3935">is considered to be an acceptable match. 5.2 Comparison of Transliteration Models We compare the performance of three systems within our proposed framework in Figure.1: the baseline Block system, a system in which we use a log-linear combination of alignment features as described in §4.3, we call the the L-Block system, and finally a system, which also uses the bi-stream HMM alignment model as described in §3. This last system will be denoted LCBE system. The baseline is based on the refined letter-alignment from the two directions of IBM-Model-4, trained with a scheme of 15h545 using GIZA++ (Och and Ney, 2004). The final alignment was obtained by growing the intersections between Arabic-to-English (AE) and Englishto-Arabic (EA) alignments with additional aligned letterpairs seen in the union. This is to compensate for the inherent asymmetry in alignment models. Blocks (letterngram pairs) were collected directly from the refined letter-alignment, using the same algorithm as described in §4.3 for extracting gold-standard letter blocks. There is no length restrictions to the letter-ngram extracted in our system. All the blocks were then scored using relative frequencies and lexical scores in both dire</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. In Computational Linguistics, volume 30, pages 417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
</authors>
<title>An English-Korean transliteration model using pronunciation and contextual rules.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING-2002,</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3846" citStr="Oh and Choi (2002)" startWordPosition="528" endWordPosition="531">gs of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between English and Chinese, and an extended model is presented in Ekbal et al. (2006). We propose a block-level transliteration framework, as shown in Figure 1, to model letter-level context information for transliteration at two levels. First, we propose a bi-stream HMM incorporating letter-clusters to better model the </context>
</contexts>
<marker>Oh, Choi, 2002</marker>
<rawString>Jong-Hoon Oh and Key-Sun Choi. 2002. An English-Korean transliteration model using pronunciation and contextual rules. In Proceedings of COLING-2002, pages 1–7, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="23361" citStr="Papineni et al., 2002" startWordPosition="3653" endWordPosition="3656">-English machine translation evaluation test set. The 663 sentences contain 286 unique words, which were not covered by the available training data. From this set of untranslated words, we manually labeled the entities of persons, locations and organizations, giving a total of 97 unique un-translated NEs. The BAMA toolkit was used to romanize the Arabic words. Some names from this test set are shown in Figure 1. These untranslated NEs make up only a very small fraction of all words in the test set. Therefore, having correct transliterations would give only small improvements in terms of BLEU (Papineni et al., 2002) and NIST scores. However, successfully translating these unknown NEs is very crucial for cross-lingual distillation tasks or question-answering based on the MT-output. 1The corpus is provided as FOUO (for official use only) in the DARPA-GALE project 2LDC2004L02: Buckwalter Arabic Morphological Analyzer version 2.0 (12) 368 Table 1: Test Set Examples. Table 2: Transliteration accuracy for different transliteration models. System Accuracy Baseline 39.18% L-Block 41.24% LCBE 46.39% To evaluate the transliteration performance, we use edit-distance between the hypothesis against a reference set. T</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Stalls</author>
<author>Kevin Knight</author>
</authors>
<title>Translating names and technical terms in Arabic text.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING/ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="2884" citStr="Stalls and Knight, 1998" startWordPosition="388" endWordPosition="391">rward transliteration from Arabic to English. The work in (Arbabi et al., 1994), to our knowledge, is the first work on machine transliteration of Arabic names into English, French, and Spanish. The idea is to vowelize Arabic names by adding appropriate vowels and utilizing a phonetic look-up table to provide the spelling in the target language. Their framework is strictly applicable within standard Arabic morphological rules. Knight and Graehl (1997) introduced finite state transducers that implement back-transliteration from Japanese to English, which was then extended to Arabic-English in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003</context>
</contexts>
<marker>Stalls, Knight, 1998</marker>
<rawString>Bonnie Stalls and Kevin Knight. 1998. Translating names and technical terms in Arabic text. In Proceedings of the COLING/ACL Workshop on Computational Approaches to Semitic Languages, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver.</location>
<contexts>
<context position="22001" citStr="Stolcke, 2002" startWordPosition="3438" endWordPosition="3439"> tables, the base feature functions are then computed for each candidate block, and the features are combined in the log-linear model in Eqn. 11. Given a named-entity pair in the training data, we rank all the transliteration blocks by the scores using the log-linear model. This step is shown in the lower-part in Figure 1. 4.5 Decoding Unseen NEs The decoding of NEs is an extension to the noisy-channel scheme in Eqn. 1. In our configurations for NE transliteration, the extracted transliteration blocks are used. Our letter ngram is a standard letter-ngram model trained using the SriLM toolkit (Stolcke, 2002). To transliterate the unseen NEs, the decoder (Hewavitharana et al., 2005) is configured for monotone decoding. It loads the transliteration blocks and the letter-ngram LM, and it decodes the unseen Arabic named entities with block-based transliteration from left to right. 5 Experiments 5.1 The Data We have 74,887 bilingual geographic names from LDC2005G01-NGA, 11,212 bilingual person names from LDC2005G021, and about 6,000 bilingual names extracted from the BAMA2 dictionary. In total, there are 92,099 NE pairs. We split them into three parts: 91,459 pairs as the training dataset, 100 pairs a</context>
<context position="27041" citStr="Stolcke, 2002" startWordPosition="4228" endWordPosition="4229">ded letter-classes. The resulting combined alignment was used together with all features of the L-Block system to guide the local-search for extracting the blocks. The same procedure of decoding was then carried out for the unseen NEs using the extracted blocks. To build the letter language model for the decoding process, we first split the English entities into characters; additional position indicators “ begin” and “ end” were added to the begin and end position of the namedentity; “ middle” was added between the first name and last name. A letter-trigram language model with SRI LM toolkit (Stolcke, 2002) was then built using the target side (English) of NE pairs tagged with the above position information. Table 2 shows that the baseline system gives an accuracy of 39.18%, while the extended systems L-Block and LCBE give 41.24% and 46.39%, respectively. These results show that the additional features besides the letteralignment are helpful. The L-Block system, which uses these features, outperforms the baseline system significantly by 2.1% absolute in accuracy. The results also show that the bi-stream HMM alignment, which uses not only the letters but also the letter-classes, leads to signific</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Language Processing, volume 2, pages 901–904, Denver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Virga</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Transliteration of proper names in cross-lingual information retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Multi-lingual Named Entity Recognition,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="3451" citStr="Virga and Khudanpur (2003)" startWordPosition="470" endWordPosition="473">hen extended to Arabic-English in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a fr</context>
<context position="5505" citStr="Virga and Khudanpur (2003)" startWordPosition="754" endWordPosition="757">364 Proceedings of NAACL HLT 2007, pages 364–371, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics Figure 1: Transliteration System Structure. The upper-part is the two-directional Bi-Stream HMM for letter-alignment; the lower-part is a log-linear model for combining different feature functions for block-level transliteration. these features to learn block-level transliteration-pairs from training data. The proposed transliteration framework obtained significant improvements over a strong baseline transliteration approach similar to AbdulJaleel and Larkey (2003) and Virga and Khudanpur (2003). The remainder of this paper is organized as follows. In Section 2, we formulate the transliteration as a general translation problem; in Section 4, we propose a log-linear alignment model with a local search algorithm to model the letter n-gram translation pairs; in Section 5, experiments are presented. Conclusions and discussions are given in Section 6. 2 Transliteration as Translation Transliteration can be viewed as a special case of translation. In this approach, source and target NEs are split into letter sequences, and each sequence is treated as apseudo sentence. The appealing reason </context>
</contexts>
<marker>Virga, Khudanpur, 2003</marker>
<rawString>Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of proper names in cross-lingual information retrieval. In Proceedings of the ACL Workshop on Multi-lingual Named Entity Recognition, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney Vogel</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM based word alignment in statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. The 16th Int. Conf. on Computational Lingustics, (COLING-1996),</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<marker>Vogel, Tillmann, 1996</marker>
<rawString>Stephan. Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM based word alignment in statistical machine translation. In Proc. The 16th Int. Conf. on Computational Lingustics, (COLING-1996), pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Alex Waibel</author>
</authors>
<title>Learning a log-linear model with bilingual phrase-pair features for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Jeju Island, Korean,</location>
<contexts>
<context position="12113" citStr="Zhao and Waibel (2005)" startWordPosition="1801" endWordPosition="1804">ock, rather than letter-by-letter. The goal of transliteration model is to learn high-quality transliteration blocks from the training data in a unsupervised fashion. Specifically, a block X can be represented by its left and right boundaries in the source and target NEs shown in Figure 2: X = (fj+l j ,ei+k i ), (4) where fj+l j is the source letter-ngram with (l + 1) letters in source language, and its projection of ei +k iin the English NE with left boundary at the position of i, and right boundary at (i + k). We formulate the block extraction as a local search problem following the work in Zhao and Waibel (2005): given a source letter n-gram fj+l j , search for the projected boundaries of candidate target letter n-gram ei +k i according to a weighted combination of the diverse features in a log-linear model detailed in §4.3. The log-linear model serves as a performance measure to guide the local search, which, in our setup, is randomized hill-climbing, to extract bilingual letter n-gram transliteration pairs. 4.1 Features for Block Transliteration Three features: fertility, distortion, and lexical translation are investigated for inferring transliteration blocks from the NE pairs. Each feature corres</context>
</contexts>
<marker>Zhao, Waibel, 2005</marker>
<rawString>Bing Zhao and Alex Waibel. 2005. Learning a log-linear model with bilingual phrase-pair features for statistical machine translation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, Jeju Island, Korean, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
<author>Alex Waibel</author>
</authors>
<title>Bilingual word spectral clustering for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>25--32</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="10163" citStr="Zhao et al., 2005" startWordPosition="1481" endWordPosition="1484"> of p(fj|eaj) and p(cfj|ceaj ) at each 365 Figure 2: Block of letters for transliteration. A block is defined by the left- and right- boundaries in the NE-pair. state, respectively. To be in accordance with the monotone nature of the NE’s alignment mentioned before, we enforce the following constraints in Eqn. 3, so that the transition can only jump forward or stay at the same state: aj−aj_1&gt;0 dj E 11,J]. (3) Since the two streams are conditionally independent given the current state, the extended EM is straightforward, with only small modifications of the standard forward-backward algorithm (Zhao et al., 2005), for parameter estimation. 3.2 Designing Letter-Classes Pronunciation is typically highly structured. For instance, in English the pronunciation structure of “cvc” (consonant-vowel-consonant) is common. By incorporating letter classes into the proposed two-stream FrMM, the models’ expressiveness and robustness can be improved. In this work, we focus on transliteration of Arabic NEs into English. We define six non-overlapping letter classes: vowel, consonant, initial, final, noclass, and unknown. Initial and final classes represent semantic markers at the beginning or end of NEs such as “Al” a</context>
</contexts>
<marker>Zhao, Xing, Waibel, 2005</marker>
<rawString>Bing Zhao, Eric P. Xing, and Alex Waibel. 2005. Bilingual word spectral clustering for statistical machine translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 25–32, Ann Arbor, Michigan, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>