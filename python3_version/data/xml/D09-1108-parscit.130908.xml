<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9971865">
Fast Translation Rule Matching for Syntax-based Statistical
Machine Translation
</title>
<author confidence="0.996169">
Hui Zhang1, 2 Min Zhang1 Haizhou Li1 Chew Lim Tan2
</author>
<affiliation confidence="0.98735">
1Institute for Infocomm Research 2National University of Singapore
</affiliation>
<email confidence="0.98072">
zhangh1982@gmail.com {mzhang, hli}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993111" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997906294117647">
In a linguistically-motivated syntax-based trans-
lation system, the entire translation process is
normally carried out in two steps, translation
rule matching and target sentence decoding us-
ing the matched rules. Both steps are very time-
consuming due to the tremendous number of
translation rules, the exhaustive search in trans-
lation rule matching and the complex nature of
the translation task itself. In this paper, we pro-
pose a hyper-tree-based fast algorithm for trans-
lation rule matching. Experimental results on
the NIST MT-2003 Chinese-English translation
task show that our algorithm is at least 19 times
faster in rule matching and is able to help to
save 57% of overall translation time over previ-
ous methods when using large fragment transla-
tion rules.
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990838903225807">
Recently linguistically-motivated syntax-based
translation method has achieved great success in
statistical machine translation (SMT) (Galley et al.,
2004; Liu et al., 2006, 2007; Zhang et al., 2007,
2008a; Mi et al., 2008; Mi and Huang 2008;
Zhang et al., 2009). It translates a source sentence
to its target one in two steps by using structured
translation rules. In the first step, which is called
translation rule matching step, all the applicable1
translation rules are extracted from the entire rule
set by matching the source parse tree/forest. The
second step is to decode the source sentence into
its target one using the extracted translation rules.
Both of the two steps are very time-consuming
due to the exponential number of translation rules
and the complex nature of machine translation as
1 Given a source structure (either a parse tree or a parse
forest), a translation rule is applicable if and only if the
left hand side of the translation rule exactly matches a
tree fragment of the given source structure.
an NP-hard search problem (Knight, 1999). In the
SMT research community, the second step has
been well studied and many methods have been
proposed to speed up the decoding process, such
as node-based or span-based beam search with
different pruning strategies (Liu et al., 2006;
Zhang et al., 2008a, 2008b) and cube pruning
(Huang and Chiang, 2007; Mi et al., 2008). How-
ever, the first step attracts less attention. The pre-
vious solution to this problem is to do exhaustive
searching with heuristics on each tree/forest node
or on each source span. This solution becomes
computationally infeasible when it is applied to
packed forests with loose pruning threshold or rule
sets with large tree fragments of large rule height
and width. This not only overloads the translation
process but also compromises the translation per-
formance since as shown in our experiments the
large tree fragment rules are also very useful.
To solve the above issue, in this paper, we pro-
pose a hyper-tree-based fast algorithm for transla-
tion rule matching. Our solution includes two
steps. In the first step, all the translation rules are
re-organized using our proposed hyper-tree struc-
ture, which is a compact representation of the en-
tire translation rule set, in order to make the com-
mon parts of translation rules shared as much as
possible. This enables the common parts of differ-
ent translation rules to be visited only once in rule
matching. Please note that the first step can be
easily done off-line very fast. As a result, it does
not consume real translation time. In the second
step, we design a recursive algorithm to traverse
the hyper-tree structure and the input source forest
in a top-down manner to do the rule matching be-
tween them. As we will show later, the hyper-tree
structure and the recursive algorithm significantly
improve the speed of the rule matching and the
entire translation process compared with previous
methods.
With the proposed algorithm, we are able to
carry out experiments with very loose pruning
</bodyText>
<page confidence="0.963267">
1037
</page>
<note confidence="0.99661">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1037–1045,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9959909375">
thresholds and larger tree fragment rules effi-
ciently. Experimental results on the NIST MT-
2003 Chinese-English translation task shows that
our algorithm is 19 times faster in rule matching
and is able to save 57% of overall translation time
over previous methods when using large fragment
translation rules with height up to 5. It also shows
that the larger rules with height of up to 5 signifi-
cantly outperforms the rules with height of up to 3
by around 1 BLEU score.
The rest of this paper is organized as follows.
Section 2 introduces the syntax-based translation
system that we are working on. Section 3 reviews
the previous work. Section 4 explains our solution
while section 5 reports the experimental results.
Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.993727" genericHeader="method">
2 Syntax-based Translation
</sectionHeader>
<bodyText confidence="0.999528333333333">
This section briefly introduces the forest/tree-
based tree-to-string translation model which
serves as the translation platform in this paper.
</bodyText>
<subsectionHeader confidence="0.884942">
2.1 Tree-to-string model
</subsectionHeader>
<bodyText confidence="0.999902111111111">
ping process, which first breaks the source syntax
tree into many tree fragments and then maps each
tree fragment into its corresponding target transla-
tion using translation rules, finally combines these
target translations into a complete sentence. Fig. 1
illustrates this process. In real translation, the
number of possible tree fragment segmentations
for a given input tree is exponential in the number
of tree nodes.
</bodyText>
<subsectionHeader confidence="0.990065">
2.2 Forest-based translation
</subsectionHeader>
<bodyText confidence="0.987417714285714">
To overcome parse error for SMT, Mi and Huang
(2008) propose forest-based translation by using a
packed forest instead of a single syntax tree as the
translation input. A packed forest (Tomita 1987;
Klein and Manning, 2001; Huang and Chiang,
2005) is a compact representation of many possi-
ble parse trees of a sentence, which can be for-
mally described as a triple , where V is
the set of non-terminal nodes, E is the set of hy-
per-edges and S is a sentence represented as an
ordered word sequence. A hyper-edge in a packed
forest is a group of edges in a tree which connects
a father node to all its children nodes, representing
a CFG-based parse rule. Fig. 2 is a packed forest
incorporating two parse trees T1 and T2 of a sen-
tence as shown in Fig. 3 and Fig. 4. Given a hy-
per-edge e, let h be its father node, then we say
that e is attached to h.
A non-terminal node in a packed forest can be
represented as “label [start, stop]”, where “label”
is its syntax category and “[start, stop]” is the
range of words it covers. For example, the node in
Fig. 5 pointed by the dark arrow is labelled as
“NP[3,4]”, where NP is its label and [3,4] means
that it covers the span from the 3rd word to the 4th
word. In forest-based translation, rule matching is
much more complicated than the tree-based one.
XNA declaration is related to some regulation
</bodyText>
<figureCaption confidence="0.8197875">
Figure 1. A tree-to-string translation process.
The tree-to-string model (Galley et al. 2004; Liu et
al. 2006) views the translation as a structure map-
Figure 2. A packed forest
</figureCaption>
<bodyText confidence="0.997609666666667">
Zhang et al. (2009) reduce the tree sequence
problem into tree problem by introducing virtual
node and related forest conversion algorithms, so
</bodyText>
<page confidence="0.991688">
1038
</page>
<bodyText confidence="0.998942111111111">
the algorithm proposed in this paper is also appli-
cable to the tree sequence-based models.
For example, if we want to extract useful rules
for node NP[3,4] in Fig 5, we have to generate all
the tree fragments rooted at node NP[3,4] as
shown in Fig 6, and then query each fragment in
the rule set. Let be a node in the packed forest,
represents the number of possible tree frag-
ments rooted at node , then we have:
</bodyText>
<figureCaption confidence="0.777711">
Figure 3. Tree 1 (T1) Figure 4. Tree 2 (T2)
</figureCaption>
<sectionHeader confidence="0.991404" genericHeader="method">
3 Matching Methods in Previous Work
</sectionHeader>
<bodyText confidence="0.995912">
In this section, we discuss the two typical rule
matching algorithms used in previous work.
</bodyText>
<subsectionHeader confidence="0.985872">
3.1 Exhaustive search by tree fragments
</subsectionHeader>
<bodyText confidence="0.994168">
This method generates all possible tree fragments
rooted by each node in the source parse tree or
forest, and then matches all the generated tree
fragments against the source parts (left hand side)
of translation rules to extract the useful rules
(Zhang et al., 2008a).
</bodyText>
<figureCaption confidence="0.7031362">
Figure 5. Node NP[3,4] in packed forest
Figure 6. Candidate fragments on NP[3,4]
e is a hyper—edge ci is the
attached to h ith children
node in e
</figureCaption>
<bodyText confidence="0.999986222222222">
The above equation shows that the number of
tree fragments is exponential to the span size, the
height and the number of hyper-edges it covers. In
a real system, one can use heuristics, e.g. the max-
imum number of nodes and the maximum height
of fragment, to limit the number of possible frag-
ments. However, these heuristics are very subjec-
tive and hard to optimize. In addition, they may
filter out some “good” fragments.
</bodyText>
<subsectionHeader confidence="0.996371">
3.2 Exhaustive search by rules
</subsectionHeader>
<bodyText confidence="0.999472777777778">
This method does not generate any source tree
fragments. Instead, it does top-down recursive
matching from each node one-by-one with each
translation rule in the rule set (Mi and Huang
2008).
For example, given a translation rule with its
left hand side as shown in Fig. 7, the rule match-
ing between the given rule and the node IP[1,4] in
Fig. 2 can be done as follows.
</bodyText>
<listItem confidence="0.91936825">
1. Decompose the left hand side of the transla-
tion rule as shown in Fig. 7 into a sequence of hy-
per-edges in top-down, left-to-right order as fol-
lows:
</listItem>
<equation confidence="0.9196685">
IP =&gt; NP VP; NP =&gt; NP NP; NP =&gt; NN;
NN =&gt; 声明
</equation>
<figureCaption confidence="0.99095">
Figure 7. The left hand side of a rule
</figureCaption>
<listItem confidence="0.9968144">
2. Pattern match these hyper-edges(rule) one-
by-one in top-down left-to-right order from node
IP[1,4]. If there is a continuous path in the forest
matching all of these hyper-edges in order, then
we can say that the rule is useful and matchable
</listItem>
<equation confidence="0.601859">
i
</equation>
<page confidence="0.944549">
1039
</page>
<bodyText confidence="0.9996015">
with the tree fragment covered by the continuous
path. The following illustrates the matching steps:
</bodyText>
<listItem confidence="0.98193147368421">
1. Match hyper-edge “IP =&gt; NP VP” with node
IP[1,4]. There are two hyper-edges in the forest
matching it: “IP[1,4] =&gt; NP[1,1] VP[2,4]” and
“IP[1,4] =&gt; NP[1,2] VP [3,4]”, which generates
two candidate paths.
2. Since hyper-edge “NP =&gt; NP NP” fails to
match NP[1,1], the path initiated with “IP[1,4] =&gt;
NP[1,1] VP[2,4]” is pruned out.
3. Since there is a hyper-edge “NP[1,2] =&gt;
NP[1,1] NP[2,2]” matching “NP =&gt; NP NP” on
NP[1,2], then continue for further matching.
4. Since “NP=&gt;NN” on NP[2,2] matches
“NP[2,2] =&gt; NN[2,2]”, then continue for further
matching.
5. “NN=&gt;声明” on NN[2,2] matches “NN[2,2]
=&gt;声明” and it is the last hyper-edge in the input
rules. Finally, there is one continuous path suc-
cessfully matching the left hand side of the input
rule.
</listItem>
<bodyText confidence="0.999983">
This method is able to avoid the exponential
problem of the first method as described in the
previous subsection. However, it has to do one-by-
one pattern matching for each rule on each node.
When the rule set is very large (indeed it is very
large in the forest-based model even with a small
training set), it becomes very slow, and even much
slower than the first method.
</bodyText>
<sectionHeader confidence="0.985589" genericHeader="method">
4 The Proposed Hyper-tree-based Rule
</sectionHeader>
<subsectionHeader confidence="0.581245">
Matching Algorithm
</subsectionHeader>
<bodyText confidence="0.9999005">
In this section, we first explain the motivation why
we re-organize the translation rule sets, and then
elaborate how to re-organize the translation rules
using our proposed hyper-tree structure. Finally
we discuss the top-down rule matching algorithm
between forest and hyper-tree.
</bodyText>
<subsectionHeader confidence="0.97099">
4.1 Motivation
</subsectionHeader>
<figureCaption confidence="0.940104">
Figure 8. Two rules’ left hand side
Figure 9. Common part of the two rules’ left hand
sides in Figure 8
</figureCaption>
<bodyText confidence="0.9666102">
Fig. 9 shows the common part of the left hand
sides of two translation rules as shown in Fig. 8.
In previous rule matching algorithm, the common
parts are matched as many times as they appear in
the rule set, which reduces the rule matching
speed significantly. This motivates us to propose
the hyper-tree structure and the rule matching al-
gorithm to make the common parts shared by mul-
tiple translation rules to be visited only once in the
entire rule matching process.
</bodyText>
<subsectionHeader confidence="0.998239">
4.2 Hyper-node, hyper-path and hyper-tree
</subsectionHeader>
<bodyText confidence="0.9998795">
A hyper-tree is a compact representation of a
group of tree translation rules with common parts
shared. It consists of a set of hyper-nodes with
edges connecting different hyper-nodes into a big
tree. A hyper-tree is constructed from the transla-
tion rule sets in two steps:
</bodyText>
<listItem confidence="0.9641546">
1) Convert each tree translation rule into a hy-
per-path;
2) Construct the hyper-tree by incrementally
adding each individual hyper-path into the
hyper-tree.
</listItem>
<bodyText confidence="0.98139752173913">
A tree rule can be converted into a hyper-path
without losing information. Fig. 10 demonstrates
the conversion process:
1) We first fill the rule tree with virtual nodes
to make all its leaves have the same depth
to the root;
2) We then group all the nodes in the same
tree level to form a single hyper-node,
where we use a comma as a delimiter to
separate the tree nodes with different father
nodes;
3) A hyper-path is a set of hyper-nodes linked
in a top-down manner.
The commas and virtual nodes are introduced
to help to recover the original tree from the hyper-
path. Given a tree node in a hyper-node, if there
are n commas before it, then its father node is the
(n+1)th tree node in the father hyper-node. If we
could find father node for each node in hyper-
nodes, then it is straightforward to recover the
original tree from the hyper-path by just adding
the edges between original father and children
nodes except the virtual node .
</bodyText>
<page confidence="0.952108">
1040
</page>
<bodyText confidence="0.996750833333333">
After converting each tree rule into a hyper-
path, we can organize the entire rule set into a big
hyper-tree as shown in Figure 11. The concept of
hyper-path and hyper-tree could be viewed as an
extension of the &amp;quot;prefix merging&amp;quot; ideas for CFG
rules (Klein and Manning 2001).
</bodyText>
<figureCaption confidence="0.999965">
Figure 10. Convert tree to hyper-path
Figure 11. A hyper-tree example
</figureCaption>
<bodyText confidence="0.999881512820513">
Algorithm 1 shows how to organize the rule set
into a big hyper-tree. The general process is that
for each rule we convert it into a hyper-path and
then add the hyper-path into a hyper-tree incre-
mentally. However, there are many different hy-
per-trees generated given a big rule set. We then
introduce a TOP label as the root node to link all
the individual hyper-trees to a single big hyper-
tree. Algorithm 2 shows the process of adding a
hyper-path into a hyper-tree. Given a hyper-path,
we do a top-down matching between the hyper-
tree and the input hyper-path from root hyper-
node until a leaf hyper-node is reached or there is
no matching hyper-node at some level found.
Then we add the remaining unmatchable part of
the input hyper-path as the descendants of the last
matchable hyper-node.
Please note that in Fig. 10 and Fig. 11, we ig-
nore the target side (right hand side) of translation
rules for easy discussion. Indeed, we can easily
represent all the complete translation rules (not
only left hand side) in Fig. 11 by simply adding
the corresponding rule target sides into each hy-
per-node as done by line 5 of Algorithm 1.
Any hyper-path from the root to any hyper-
node (not necessarily be a leaf of the hyper-tree)
in a hyper-tree can represent a tree fragment. As a
result, the hyper-tree in Fig. 11 can represent up to
6 candidate tree fragments. It is easy to understand
that the maximum number of tree fragments that a
hyper-tree can represent is equal to the number of
hyper-nodes in it except the root. It is worth not-
ing that a hyper-node in a hyper-tree without any
target side rule attached means there is no transla-
tion rule corresponding to the tree fragment repre-
sented by the hyper-path from the root to the cur-
rent hyper-node. The compact representation of
the rule set by hyper-tree enables a fast algorithm
to do translation rule matching.
</bodyText>
<construct confidence="0.4018365">
Algorithm 1. Compile rule set into hyper-tree
Input: rule set
</construct>
<listItem confidence="0.88488825">
Output: hyper-tree
1. Initialize hyper-tree as a TOP node
2. for each rule in rule set do
3. Convert the left hand side tree to a hyper-path p
4. Add hyper-path p into hyper-tree
5. Add rule’s right hand side to the leaf hyper-node of
a hyper-path in the hyper-tree
6. end for
</listItem>
<construct confidence="0.2882945">
Algorithm 2. Add hyper-path into hyper-tree
Input: hyper-path p and hyper-tree t
</construct>
<subsectionHeader confidence="0.321758">
Notation:
</subsectionHeader>
<bodyText confidence="0.933338">
h: the height of hyper-path p
p(i) : the hyper-node of ith level (top-down) of p
TN: the hyper-node in hyper-tree
Output: updated hyper-tree t
</bodyText>
<listItem confidence="0.999501125">
1. Initialize TN as TOP
2. for i := 1 to h do
3. if there is a child c of TN has the same label as p(i)
then
4. TN := c
5. else
6. Add a child c to TN, label c as p(i)
7. TN := c
</listItem>
<subsectionHeader confidence="0.4444715">
4.3 Translation rule matching between forest
and hyper-tree
</subsectionHeader>
<bodyText confidence="0.997598875">
Given the source parse forest and the translation
rules represented in the hyper-tree structure, here
we present a fast matching algorithm to extract so-
called useful translation rules from the entire rule
set in a top-down manner for each node of the for-
est.
As shown in Algorithm 3, the general process
of the matching algorithm is as follows:
</bodyText>
<page confidence="0.943554">
1041
</page>
<table confidence="0.888431">
Algorithm 3. Rule matching on one node
Input: hyper-tree T, forest F, and node n
Notation:
FP: a pair &lt;FNS, TN&gt;, FNS is the frontier nodes of
matched tree fragment,
TN is the hyper-tree node matching it
SFP: the queue of FP
Output: Available rules on node n
</table>
<listItem confidence="0.971636555555556">
1. if there is no child c of TOP having the same label as n
then
2. Return failure.
3. else
4. Initialize FP as &lt;{n},c&gt; and put it into SFP
5. for each FP in SFP do
6. SFP Å PropagateNextLevel(FP.FNS, FP.TN)
7. for each FP in SFP do
8. if the rule set attached to FP.TN is not empty
</listItem>
<figure confidence="0.743521823529412">
then
9. Add FP to result
Algorithm 4. PropagateNextLevel
Input: Frontier node sequence FNS, hyper-tree node TN
Notation:
CT: a child node of TN
the number of node sequence (separated by
comma, see Fig 11) in CT is equal to the number
of node in TN.
CT(i) : the ith node sequence in hyper-node CT
FNS(i): the ith node in FNS
TFNS: the temporary set of frontier node sequence
RFNS: the result set of frontier node sequence
FP: a pair of frontier node sequence
and hyper-tree node
RFP: the result set of FP
Output: RFP
</figure>
<listItem confidence="0.743910714285714">
1. for each child hyper-node CT of TN do
2. for i:= 1 to the number of node sequence in CT do
3. empty TFNS
4. if CT(i) == then
5. Add FNS(i) to TFNS.
6. else
7. for each hyper-edge e attached to FNS(i) do
8. if e.children match CT(i) then
9. Add e.children to TFNS
10. if TFNS is empty then
11. empty RFNS
12. break
13. else if i == 1 then
14. RFNS := TFNS
15. else
16. RFNS := RFNS TFNS
17. for each FNS in RFNS do
18. add &lt;FNS, CT &gt; into RFP
1) For each node n of the source forest if no
child node of TOP in hyper-tree has the same label
with it, it means that no rule matches any tree
fragments rooted at the node n (i.e., no useful
rules to be used for the node n) (line 1-2)
2) Otherwise, we match the sub-forest starting
from the node n against a sub-hyper-tree starting
from the matchable child node of TOP layer by
layer in a top-down manner. There may be many
possible tree fragments rooted at node n and each
</listItem>
<bodyText confidence="0.986575410714286">
of them may have multiple useful translation rules.
In our implementation, we maintain a data struc-
ture of FP = &lt;FNS, TN&gt; to record the currently
matched tree fragment of forest and its corres-
ponding hyper-tree node in the rule set, where
FNS is the frontier node set of the current tree
fragment and TN is the hyper-tree node. The data
structure FP is used to help extract useful transla-
tion rules and is also used for further matching of
larger tree fragments. Finally, all the FPs for the
node n are kept in a queue. During the search, the
queue size is dynamically increased. The matching
algorithm terminates when all the FPs have been
visited (line 5-6 and Algorithm 4).
3) In the final queue, each element (FP) of the
queue contains the frontier node sequence of the
matched tree fragment and its corresponding hy-
per-tree node. If the target side of a rule in the hy-
per-tree node is not empty, we just output the
frontier nodes of the matched tree fragment, its
root node n and all the useful translation rules for
later translation process.
Algorithm 4 describes the detailed process of
how to propagate the matching process down to
the next level. &lt;FNS, TN&gt; is the current level
frontier node sequence and hyper-tree node. Given
a child hyper-node CT of TN (line 1), we try to
find the group of next level frontier node sequence
to match it (line 2-18). As shown in Fig 11, a hy-
per-node consists of a sequence of node sequence
with comma as delimiter. For the ith node se-
quence CT(i) in CT, If CT(i) is , that means
FNS(i) is a leaf/frontier node in the matched tree
fragment and thus no need to propagate to the next
level (line 4-5). Otherwise, we try each hyper-
edge e of FNS(i) to see whether its children match
CT(i), and put the children of the matched hyper-
edge into a temp set TFNS (line 7-9). If the temp
set is empty, that means the current matching fails
and no further expansion needs (line 10-12). Oth-
erwise, we integrate current matched children into
the final group of frontier node sequence (line 13-
16) by Descartes Product ( ). Finally, we con-
struct all the &lt;FNS, TN&gt; pair for next level
matching (line 17-18).
It would be interesting to study the time com-
plexity of our Algorithm 3 and 4. Suppose the
maximum number of children of each hyper-node
in hyper-tree is N (line 1), the maximum number
of node sequence in CT is M (line 2), the maxi-
mum number of hyper-edge in each node in
packed forest is K (line 7), the maximum number
of hyper-edge with same children representation
in each node in packed forest is C (i.e. the maxi-
mum size of TFNS in line 16, and the maximum
complexity of the Descartes Product in line 16
</bodyText>
<page confidence="0.990086">
1042
</page>
<bodyText confidence="0.9999624">
would be CM), then the time complexity upper-
bound of Algorithm 4 is O(NM(K+CM)). For Al-
gorithm 3, its time complexity is O(RNM(K+CM)),
where R is the maximum number of tree fragment
matched in each node.
</bodyText>
<sectionHeader confidence="0.99844" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999319">
5.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999807761904762">
We carry out experiment on Chinese-English
NIST evaluation tasks. We use FBIS corpus
(250K sentence pairs) as training data with the
source side parsed by a modified Charniak parser
(Charniak 2000) which can output a packed forest.
The Charniak Parser is trained on CTB5, tuned on
301-325 portion, with F1 score of 80.85% on 271-
300 portion. We use GIZA++ (Och and Ney, 2003)
to do m-to-n word-alignment and adopt heuristic
“grow-diag-final-and” to do refinement. A 4-gram
language model is trained on Gigaword 3 Xinhua
portion by SRILM toolkit (Stolcke, 2002) with
Kneser-Ney smoothing. We use NIST 2002 as
development set and NIST 2003 as test set. The
feature weights are tuned by the modified Koehn’s
MER (Och, 2003, Koehn, 2007) trainer. We use
case-sensitive BLEU-4 (Papineni et al., 2002) to
measure the quality of translation result. Zhang et
al. 2004’s implementation is used to do significant
test.
Following (Mi and Huang 2008), we use viterbi
algorithm to prune the forest. Instead of using a
static pruning threshold (Mi and Huang 2008), we
set the threshold as the distance of the probabili-
ties of the nth best tree and the 1st best tree. It
means the pruned forest is able to at least keep all
the top n best trees. However, because of the shar-
ing nature of the packed forest, it may still contain
a large number of additional trees. Our statistic
shows that when we set the threshold as the 100th
best tree, the average number of all possible trees
in the forest is 1.2*105 after pruning.
In our experiments, we compare our algorithm
with the two traditional algorithms as discussed in
section 3. For the “Exhaustive search by tree” al-
gorithm, we use a bottom-up dynamic program-
ming algorithm to generate all the candidate tree
fragments rooted at each node. For the “Exhaus-
tive search by rule” algorithm, we group all rules
with the same left hand side in order to remove the
duplicated matching for the same left hand side
rules. All these settings aim for fair comparison.
</bodyText>
<subsectionHeader confidence="0.996426">
5.2 Accuracy, speed vs. rule heights
</subsectionHeader>
<bodyText confidence="0.9993210625">
We first compare the three algorithms’ perfor-
mance by setting the maximum rule height from 1
to 5. We set the forest pruning threshold to the
100th best parse tree.
Table 1 compares the speed of the three algo-
rithms. It clearly shows that the speed of both of
the two traditional algorithms increases dramati-
cally while the speed of our hyper-tree based algo-
rithm is almost linear to the tree height. In the case
of rule height of 5, the hyper-tree algorithm is at
least 19 times (9.329/0.486) faster than the two
traditional algorithms and saves 8.843(9.329 -
0.486) seconds in rule matching for each sentence
on average, which contributes 57% (8.843/(9.329
+ 6.21)) speed improvement to the overall transla-
tion.
</bodyText>
<subsectionHeader confidence="0.672577">
Rule Matching
</subsectionHeader>
<table confidence="0.998159">
H Exhaus- Exhaus- Hyper- D
tive tive tree-
by tree by rule based
1 0.043 0.077 0.083 2.96
2 0.047 0.920 0.173 3.56
3 0.237 9.572 0.358 4.02
4 2.300 48.90 0.450 5.27
5 9.329 90.80 0.486 6.21
</table>
<tableCaption confidence="0.885897333333333">
Table 1. Speed in seconds per sentence vs. rule
height; “H” is rule height, “D” represents the de-
coding time after rule matching
</tableCaption>
<table confidence="0.991240142857143">
Height BLEU
1 0.1646
2 0.2498
3 0.2824
4 0.2874
5 0.2925
Moses 0.2625
</table>
<tableCaption confidence="0.999619">
Table 2. BLEU vs. rule height
</tableCaption>
<bodyText confidence="0.994009083333333">
Table 2 reports the BLEU score with different
rule heights, where Moses, a state-of-the-art
phrase-based SMT system, serves as the baseline
system. It shows the BLEU score consistently
improves as the rule height increases. In addition,
one can see that the rules with maximum height of
5 are able to outperform the rules with maximum
height of 3 by 1 BLEU score (p&lt;0.05) and signifi-
cantly outperforms Moses by 3 BLEU score
(p&lt;0.01). To our knowledge, this is the first time
to report the performance of rules up to height of 5
for forest-based translation model.
</bodyText>
<page confidence="0.973969">
1043
</page>
<bodyText confidence="0.999847636363636">
We also study the distribution of the rules used
in the 1-best translation output. The results are
shown in Table 3; we could see something inter-
esting that is as the rule height increases, the total
number of rules with that height decreases, while
the percentage of partial-lexicalized increases
dramatically. And one thing needs to note is the
percentage of partial-lexicalized rules with height
of 1 is 0, since there is no partial-lexicalized rule
with height of 1 in the rule set (the father node of
a word is a pos tag node).
</bodyText>
<table confidence="0.999766142857143">
H Rule Type Percentage (%)
Total F P U
1 9814 76.58 0 23.42
2 5289 44.99 46.40 8.60
3 3925 18.39 77.25 4.35
4 1810 7.90 87.68 4.41
5 511 6.46 90.50 3.04
</table>
<tableCaption confidence="0.864193">
Table 3. statistics of rules used in the 1-best trans-
lation output, “F” means full-lexicalized, “P”
means partial-lexicalized, “U” means unlexiclaizd.
</tableCaption>
<subsectionHeader confidence="0.966979">
5.3 Speed vs. forest pruning threshold
</subsectionHeader>
<bodyText confidence="0.999915">
This section studies the impact of the forest prun-
ing threshold on the rule matching speed when
setting the maximum rule height to 5.
</bodyText>
<subsectionHeader confidence="0.557066">
Rule Matching
</subsectionHeader>
<table confidence="0.999042888888889">
Threshold Exhaus- Exhaus- Hyper-
tive tive tree-
by tree by rule based
1 1.2 23.66 0.171
10 3.1 36.42 0.234
50 5.7 66.20 0.405
100 9.3 90.80 0.486
200 27.3 104.86 0.598
500 133.6 148.54 0.873
</table>
<tableCaption confidence="0.999766">
Table 4. Speed in seconds per sentence vs. for-
</tableCaption>
<bodyText confidence="0.973188733333333">
est pruning threshold
In Table 4, we can see that our hyper-tree based
algorithm is the fastest among the three algorithms
in all pruning threshold settings and even 150
times faster than both of the two traditional algo-
rithms with threshold of 500th best. Table 5 shows
the average number of parse trees embedded in a
packed forest with different pruning thresholds per
sentence. We can see that the number of trees in-
creases exponentially when the pruning threshold
increases linearly. When the threshold is 500th best,
the average number of trees per sentence is
1.49*109. However, even in this extreme case, the
hyper-tree based algorithm is still capable of com-
pleting rule matching within 1 second.
</bodyText>
<table confidence="0.985709">
Threshold Number of Trees
1 1
10 32
50 5922
100 128860
200 2.75*106
500 1.49*109
</table>
<tableCaption confidence="0.99735">
Table 5. Average number of trees in packed
</tableCaption>
<bodyText confidence="0.774164">
forest with different pruning threshold.
</bodyText>
<subsectionHeader confidence="0.971341">
5.4 Hyper-tree compression rate
</subsectionHeader>
<bodyText confidence="0.999950411764706">
As we describe in section 4.2, theoretically the
number of tree fragments that a hyper-tree can
represent is equal to the number of hyper-nodes in
it. However, in real rule set, there is no guarantee
that each tree fragment in the hyper-tree has cor-
responding translation rules. To gain insights into
how effective the compact representation of the
hyper-tree and how many hyper-nodes without
translation rules, we define the compression rate
as follows.
Table 6 reports the different statistics on the
rule sets with different maximum rule heights
ranging from 1 to 5. The reported statistics are the
number of rules, the number of unique left hand
side (since there may be more than one rules hav-
ing the same left hand side), the number of hyper-
nodes and the compression rate.
</bodyText>
<table confidence="0.9505535">
H n_rules n_LHS n_nodes c_rate
1 21588 10779 10779 100%
2 141632 51807 51903 99.8%
3 1.73*106 491268 494919 99.2%
4 8.65*106 2052731 2083296 98.5%
5 1.89*107 3966742 4043824 98.1%
</table>
<bodyText confidence="0.9435548">
Table 6. Statistics of rule set and hyper-tree. “H”
is rule height, “n_rules” is the number of rules,
“n_LHS” is the number of unique left hand side,
“n_nodes” is the number of hyper-nodes in hyper-
tree and “c_rate” is the compression rate.
</bodyText>
<tableCaption confidence="0.620887">
Table 6 shows that in all the five cases, the
compression rates of hyper-tree are all more than
</tableCaption>
<page confidence="0.990485">
1044
</page>
<bodyText confidence="0.99906">
98%. It means that almost all the tree fragments
embedded in the hyper-tree have corresponding
translation rules. As a result, we are able to use
almost only one hyper-edge (i.e. only the frontier
nodes of a tree fragment without any internal
nodes) to represent all the rules with the same left
hand side. This suggests that our hyper-tree is par-
ticularly effective in representing the tree transla-
tion rules compactly. It also shows that there are a
lot of common parts among different translation
rules.
All the experiments reported in this section
convincingly demonstrate the effectiveness of our
proposed hyper-tree representation of translation
rules and the hyper-tree-based rule matching algo-
rithm.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99996680952381">
In this paper, we propose the concept of hyper-
tree for compact rule representation and a hyper-
tree-based fast algorithm for translation rule
matching in a forest-based translation system. We
compare our algorithm with two previous widely-
used rule matching algorithms. Experimental re-
sults on the NIST Chinese-English MT 2003 eval-
uation data set show the rules with maximum rule
height of 5 outperform those with height 3 by 1.0
BLEU and outperform MOSES by 3.0 BLEU. In
the same test cases, our algorithm is at least 19
times faster than the two traditional algorithms,
and contributes 57% speed improvement to the
overall translation. We also show that in a more
challenging setting (forest containing 1.49*109
trees on average) our algorithm is 150 times faster
than the two traditional algorithms. Finally, we
show that the hyper-tree structure has more than
98% compression rate. It means the compact re-
presentation by the hyper-tree is very effective for
translation rules.
</bodyText>
<sectionHeader confidence="0.999308" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984918304347826">
Eugene Charniak. 2000. A maximum-entropy inspired
parser. NAACL-00.
Michel Galley, Mark Hopkins, Kevin Knight and Da-
niel Marcu. 2004. What’s in a translation rule?
HLT-NAACL-04.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. IWPT-05.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. ACL-07. 144–151
The corresponding authors of this paper are Hui Zhang
(zhangh1982@gmail.com) and Min Zhang
(mzhang@i2r.a-star.edu.sg)
Dan Klein and Christopher D. Manning. 2001. Parsing
and Hypergraphs. IWPT-2001.
Dan Klein and Christopher D. Manning. 2001. Parsing
with Treebank Grammars: Empirical Bounds, Theo-
retical Models, and the Structure of the Penn Tree-
bank. ACL - 2001. 338-345.
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. CL: J99-4005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
ACL-07. 177-180. (poster)
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. COLING-ACL-06. 609-616.
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.
2007. Forest-to-String Statistical Translation Rules.
ACL-07. 704-711.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. ACL-HLT-08. 192-199.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. EMNLP-08. 206-214
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. ACL-03. 160-167.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics. 29(1) 19-51
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. ACL-02.311-318.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904.
Masaru Tomita. 1987. An Efficient Augmented-
Context-Free Parsing Algorithm. Computational
Linguistics 13(1-2): 31-46.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew
Lim Tan. 2009. Forest-based Tree Sequence to
String Translation Model. ACL-IJCNLP-09.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew
Lim Tan, Sheng Li. 2008a. A Tree Sequence Align-
ment-based Tree-to-Tree Translation Model. ACL-
HLT-08. 559-567.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, Sheng
Li. 2008b. Grammar Comparison Study for Transla-
tional Equivalence Modeling and Statistical Ma-
chine Translation. COLING-08. 1097-1104.
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement
do we need to have a better system? LREC-04
</reference>
<page confidence="0.988066">
1045
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.465183">
<title confidence="0.9977085">Fast Translation Rule Matching for Syntax-based Statistical Machine Translation</title>
<author confidence="0.731051">Min Haizhou Chew Lim</author>
<affiliation confidence="0.663041">for Infocomm Research University of</affiliation>
<email confidence="0.618596">zhangh1982@gmail.com{mzhang,hli}@i2r.a-star.edu.sgtancl@comp.nus.edu.sg</email>
<abstract confidence="0.998865166666667">In a linguistically-motivated syntax-based translation system, the entire translation process is normally carried out in two steps, translation rule matching and target sentence decoding using the matched rules. Both steps are very timeconsuming due to the tremendous number of translation rules, the exhaustive search in translation rule matching and the complex nature of the translation task itself. In this paper, we propose a hyper-tree-based fast algorithm for translation rule matching. Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<date>2000</date>
<note>A maximum-entropy inspired parser. NAACL-00.</note>
<contexts>
<context position="21689" citStr="Charniak 2000" startWordPosition="3806" endWordPosition="3807">ame children representation in each node in packed forest is C (i.e. the maximum size of TFNS in line 16, and the maximum complexity of the Descartes Product in line 16 1042 would be CM), then the time complexity upperbound of Algorithm 4 is O(NM(K+CM)). For Algorithm 3, its time complexity is O(RNM(K+CM)), where R is the maximum number of tree fragment matched in each node. 5 Experiment 5.1 Experimental settings We carry out experiment on Chinese-English NIST evaluation tasks. We use FBIS corpus (250K sentence pairs) as training data with the source side parsed by a modified Charniak parser (Charniak 2000) which can output a packed forest. The Charniak Parser is trained on CTB5, tuned on 301-325 portion, with F1 score of 80.85% on 271- 300 portion. We use GIZA++ (Och and Ney, 2003) to do m-to-n word-alignment and adopt heuristic “grow-diag-final-and” to do refinement. A 4-gram language model is trained on Gigaword 3 Xinhua portion by SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We use NIST 2002 as development set and NIST 2003 as test set. The feature weights are tuned by the modified Koehn’s MER (Och, 2003, Koehn, 2007) trainer. We use case-sensitive BLEU-4 (Papineni et al., 2002) </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy inspired parser. NAACL-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<pages>04</pages>
<contexts>
<context position="1215" citStr="Galley et al., 2004" startWordPosition="169" endWordPosition="172">arch in translation rule matching and the complex nature of the translation task itself. In this paper, we propose a hyper-tree-based fast algorithm for translation rule matching. Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules. 1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation (SMT) (Galley et al., 2004; Liu et al., 2006, 2007; Zhang et al., 2007, 2008a; Mi et al., 2008; Mi and Huang 2008; Zhang et al., 2009). It translates a source sentence to its target one in two steps by using structured translation rules. In the first step, which is called translation rule matching step, all the applicable1 translation rules are extracted from the entire rule set by matching the source parse tree/forest. The second step is to decode the source sentence into its target one using the extracted translation rules. Both of the two steps are very time-consuming due to the exponential number of translation rul</context>
<context position="7092" citStr="Galley et al. 2004" startWordPosition="1160" endWordPosition="1163">we say that e is attached to h. A non-terminal node in a packed forest can be represented as “label [start, stop]”, where “label” is its syntax category and “[start, stop]” is the range of words it covers. For example, the node in Fig. 5 pointed by the dark arrow is labelled as “NP[3,4]”, where NP is its label and [3,4] means that it covers the span from the 3rd word to the 4th word. In forest-based translation, rule matching is much more complicated than the tree-based one. XNA declaration is related to some regulation Figure 1. A tree-to-string translation process. The tree-to-string model (Galley et al. 2004; Liu et al. 2006) views the translation as a structure mapFigure 2. A packed forest Zhang et al. (2009) reduce the tree sequence problem into tree problem by introducing virtual node and related forest conversion algorithms, so 1038 the algorithm proposed in this paper is also applicable to the tree sequence-based models. For example, if we want to extract useful rules for node NP[3,4] in Fig 5, we have to generate all the tree fragments rooted at node NP[3,4] as shown in Fig 6, and then query each fragment in the rule set. Let be a node in the packed forest, represents the number of possible</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What’s in a translation rule? HLT-NAACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<date>2005</date>
<note>Better k-best Parsing. IWPT-05.</note>
<contexts>
<context position="5905" citStr="Huang and Chiang, 2005" startWordPosition="937" endWordPosition="940">ments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be formally described as a triple , where V is the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule. Fig. 2 is a packed forest incorporating two parse trees T1 and T2 of a sentence as shown in Fig. 3 and Fig. 4. Given a hyper-edge e, let h be its father node, then we say that e is attached to h.</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best Parsing. IWPT-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<pages>07--144</pages>
<contexts>
<context position="2436" citStr="Huang and Chiang, 2007" startWordPosition="376" endWordPosition="379">les and the complex nature of machine translation as 1 Given a source structure (either a parse tree or a parse forest), a translation rule is applicable if and only if the left hand side of the translation rule exactly matches a tree fragment of the given source structure. an NP-hard search problem (Knight, 1999). In the SMT research community, the second step has been well studied and many methods have been proposed to speed up the decoding process, such as node-based or span-based beam search with different pruning strategies (Liu et al., 2006; Zhang et al., 2008a, 2008b) and cube pruning (Huang and Chiang, 2007; Mi et al., 2008). However, the first step attracts less attention. The previous solution to this problem is to do exhaustive searching with heuristics on each tree/forest node or on each source span. This solution becomes computationally infeasible when it is applied to packed forests with loose pruning threshold or rule sets with large tree fragments of large rule height and width. This not only overloads the translation process but also compromises the translation performance since as shown in our experiments the large tree fragment rules are also very useful. To solve the above issue, in </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. ACL-07. 144–151</rawString>
</citation>
<citation valid="true">
<title>The corresponding authors of this paper are Hui Zhang (zhangh1982@gmail.com) and Min Zhang</title>
<date></date>
<marker></marker>
<rawString>The corresponding authors of this paper are Hui Zhang (zhangh1982@gmail.com) and Min Zhang (mzhang@i2r.a-star.edu.sg)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and Hypergraphs.</title>
<date>2001</date>
<contexts>
<context position="5880" citStr="Klein and Manning, 2001" startWordPosition="933" endWordPosition="936"> tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be formally described as a triple , where V is the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule. Fig. 2 is a packed forest incorporating two parse trees T1 and T2 of a sentence as shown in Fig. 3 and Fig. 4. Given a hyper-edge e, let h be its father node, then we say</context>
<context position="13521" citStr="Klein and Manning 2001" startWordPosition="2288" endWordPosition="2291"> hyper-node, if there are n commas before it, then its father node is the (n+1)th tree node in the father hyper-node. If we could find father node for each node in hypernodes, then it is straightforward to recover the original tree from the hyper-path by just adding the edges between original father and children nodes except the virtual node . 1040 After converting each tree rule into a hyperpath, we can organize the entire rule set into a big hyper-tree as shown in Figure 11. The concept of hyper-path and hyper-tree could be viewed as an extension of the &amp;quot;prefix merging&amp;quot; ideas for CFG rules (Klein and Manning 2001). Figure 10. Convert tree to hyper-path Figure 11. A hyper-tree example Algorithm 1 shows how to organize the rule set into a big hyper-tree. The general process is that for each rule we convert it into a hyper-path and then add the hyper-path into a hyper-tree incrementally. However, there are many different hyper-trees generated given a big rule set. We then introduce a TOP label as the root node to link all the individual hyper-trees to a single big hypertree. Algorithm 2 shows the process of adding a hyper-path into a hyper-tree. Given a hyper-path, we do a top-down matching between the hy</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and Hypergraphs. IWPT-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the Penn Treebank. ACL -</title>
<date>2001</date>
<pages>338--345</pages>
<contexts>
<context position="5880" citStr="Klein and Manning, 2001" startWordPosition="933" endWordPosition="936"> tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be formally described as a triple , where V is the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule. Fig. 2 is a packed forest incorporating two parse trees T1 and T2 of a sentence as shown in Fig. 3 and Fig. 4. Given a hyper-edge e, let h be its father node, then we say</context>
<context position="13521" citStr="Klein and Manning 2001" startWordPosition="2288" endWordPosition="2291"> hyper-node, if there are n commas before it, then its father node is the (n+1)th tree node in the father hyper-node. If we could find father node for each node in hypernodes, then it is straightforward to recover the original tree from the hyper-path by just adding the edges between original father and children nodes except the virtual node . 1040 After converting each tree rule into a hyperpath, we can organize the entire rule set into a big hyper-tree as shown in Figure 11. The concept of hyper-path and hyper-tree could be viewed as an extension of the &amp;quot;prefix merging&amp;quot; ideas for CFG rules (Klein and Manning 2001). Figure 10. Convert tree to hyper-path Figure 11. A hyper-tree example Algorithm 1 shows how to organize the rule set into a big hyper-tree. The general process is that for each rule we convert it into a hyper-path and then add the hyper-path into a hyper-tree incrementally. However, there are many different hyper-trees generated given a big rule set. We then introduce a TOP label as the root node to link all the individual hyper-trees to a single big hypertree. Algorithm 2 shows the process of adding a hyper-path into a hyper-tree. Given a hyper-path, we do a top-down matching between the hy</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the Penn Treebank. ACL - 2001. 338-345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding Complexity in WordReplacement Translation Models.</title>
<date>1999</date>
<pages>99--4005</pages>
<location>CL:</location>
<contexts>
<context position="2129" citStr="Knight, 1999" startWordPosition="327" endWordPosition="328">anslation rules are extracted from the entire rule set by matching the source parse tree/forest. The second step is to decode the source sentence into its target one using the extracted translation rules. Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as 1 Given a source structure (either a parse tree or a parse forest), a translation rule is applicable if and only if the left hand side of the translation rule exactly matches a tree fragment of the given source structure. an NP-hard search problem (Knight, 1999). In the SMT research community, the second step has been well studied and many methods have been proposed to speed up the decoding process, such as node-based or span-based beam search with different pruning strategies (Liu et al., 2006; Zhang et al., 2008a, 2008b) and cube pruning (Huang and Chiang, 2007; Mi et al., 2008). However, the first step attracts less attention. The previous solution to this problem is to do exhaustive searching with heuristics on each tree/forest node or on each source span. This solution becomes computationally infeasible when it is applied to packed forests with </context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding Complexity in WordReplacement Translation Models. CL: J99-4005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<volume>07</volume>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL-07. 177-180. (poster)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-toString Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<volume>06</volume>
<pages>609--616</pages>
<contexts>
<context position="1233" citStr="Liu et al., 2006" startWordPosition="173" endWordPosition="176">ule matching and the complex nature of the translation task itself. In this paper, we propose a hyper-tree-based fast algorithm for translation rule matching. Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules. 1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation (SMT) (Galley et al., 2004; Liu et al., 2006, 2007; Zhang et al., 2007, 2008a; Mi et al., 2008; Mi and Huang 2008; Zhang et al., 2009). It translates a source sentence to its target one in two steps by using structured translation rules. In the first step, which is called translation rule matching step, all the applicable1 translation rules are extracted from the entire rule set by matching the source parse tree/forest. The second step is to decode the source sentence into its target one using the extracted translation rules. Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex</context>
<context position="7110" citStr="Liu et al. 2006" startWordPosition="1164" endWordPosition="1167">ached to h. A non-terminal node in a packed forest can be represented as “label [start, stop]”, where “label” is its syntax category and “[start, stop]” is the range of words it covers. For example, the node in Fig. 5 pointed by the dark arrow is labelled as “NP[3,4]”, where NP is its label and [3,4] means that it covers the span from the 3rd word to the 4th word. In forest-based translation, rule matching is much more complicated than the tree-based one. XNA declaration is related to some regulation Figure 1. A tree-to-string translation process. The tree-to-string model (Galley et al. 2004; Liu et al. 2006) views the translation as a structure mapFigure 2. A packed forest Zhang et al. (2009) reduce the tree sequence problem into tree problem by introducing virtual node and related forest conversion algorithms, so 1038 the algorithm proposed in this paper is also applicable to the tree sequence-based models. For example, if we want to extract useful rules for node NP[3,4] in Fig 5, we have to generate all the tree fragments rooted at node NP[3,4] as shown in Fig 6, and then query each fragment in the rule set. Let be a node in the packed forest, represents the number of possible tree fragments ro</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-toString Alignment Template for Statistical Machine Translation. COLING-ACL-06. 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-String Statistical Translation Rules.</title>
<date>2007</date>
<volume>07</volume>
<pages>704--711</pages>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. Forest-to-String Statistical Translation Rules. ACL-07. 704-711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<pages>08--192</pages>
<contexts>
<context position="1283" citStr="Mi et al., 2008" startWordPosition="183" endWordPosition="186">ion task itself. In this paper, we propose a hyper-tree-based fast algorithm for translation rule matching. Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules. 1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation (SMT) (Galley et al., 2004; Liu et al., 2006, 2007; Zhang et al., 2007, 2008a; Mi et al., 2008; Mi and Huang 2008; Zhang et al., 2009). It translates a source sentence to its target one in two steps by using structured translation rules. In the first step, which is called translation rule matching step, all the applicable1 translation rules are extracted from the entire rule set by matching the source parse tree/forest. The second step is to decode the source sentence into its target one using the extracted translation rules. Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as 1 Given a source</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. ACL-HLT-08. 192-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based Translation Rule Extraction.</title>
<date>2008</date>
<pages>206--214</pages>
<contexts>
<context position="1302" citStr="Mi and Huang 2008" startWordPosition="187" endWordPosition="190">In this paper, we propose a hyper-tree-based fast algorithm for translation rule matching. Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules. 1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation (SMT) (Galley et al., 2004; Liu et al., 2006, 2007; Zhang et al., 2007, 2008a; Mi et al., 2008; Mi and Huang 2008; Zhang et al., 2009). It translates a source sentence to its target one in two steps by using structured translation rules. In the first step, which is called translation rule matching step, all the applicable1 translation rules are extracted from the entire rule set by matching the source parse tree/forest. The second step is to decode the source sentence into its target one using the extracted translation rules. Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as 1 Given a source structure (either </context>
<context position="5710" citStr="Mi and Huang (2008)" startWordPosition="906" endWordPosition="909">d tree-to-string translation model which serves as the translation platform in this paper. 2.1 Tree-to-string model ping process, which first breaks the source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be formally described as a triple , where V is the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule. </context>
<context position="9010" citStr="Mi and Huang 2008" startWordPosition="1498" endWordPosition="1501">that the number of tree fragments is exponential to the span size, the height and the number of hyper-edges it covers. In a real system, one can use heuristics, e.g. the maximum number of nodes and the maximum height of fragment, to limit the number of possible fragments. However, these heuristics are very subjective and hard to optimize. In addition, they may filter out some “good” fragments. 3.2 Exhaustive search by rules This method does not generate any source tree fragments. Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). For example, given a translation rule with its left hand side as shown in Fig. 7, the rule matching between the given rule and the node IP[1,4] in Fig. 2 can be done as follows. 1. Decompose the left hand side of the translation rule as shown in Fig. 7 into a sequence of hyper-edges in top-down, left-to-right order as follows: IP =&gt; NP VP; NP =&gt; NP NP; NP =&gt; NN; NN =&gt; 声明 Figure 7. The left hand side of a rule 2. Pattern match these hyper-edges(rule) oneby-one in top-down left-to-right order from node IP[1,4]. If there is a continuous path in the forest matching all of these hyper-edges in or</context>
<context position="22431" citStr="Mi and Huang 2008" startWordPosition="3926" endWordPosition="3929"> on 271- 300 portion. We use GIZA++ (Och and Ney, 2003) to do m-to-n word-alignment and adopt heuristic “grow-diag-final-and” to do refinement. A 4-gram language model is trained on Gigaword 3 Xinhua portion by SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We use NIST 2002 as development set and NIST 2003 as test set. The feature weights are tuned by the modified Koehn’s MER (Och, 2003, Koehn, 2007) trainer. We use case-sensitive BLEU-4 (Papineni et al., 2002) to measure the quality of translation result. Zhang et al. 2004’s implementation is used to do significant test. Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. It means the pruned forest is able to at least keep all the top n best trees. However, because of the sharing nature of the packed forest, it may still contain a large number of additional trees. Our statistic shows that when we set the threshold as the 100th best tree, the average number of all possible trees in the forest is 1.2*105 after pruning. In our experiments, we compare ou</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. EMNLP-08. 206-214</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<pages>03--160</pages>
<contexts>
<context position="22212" citStr="Och, 2003" startWordPosition="3894" endWordPosition="3895">raining data with the source side parsed by a modified Charniak parser (Charniak 2000) which can output a packed forest. The Charniak Parser is trained on CTB5, tuned on 301-325 portion, with F1 score of 80.85% on 271- 300 portion. We use GIZA++ (Och and Ney, 2003) to do m-to-n word-alignment and adopt heuristic “grow-diag-final-and” to do refinement. A 4-gram language model is trained on Gigaword 3 Xinhua portion by SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We use NIST 2002 as development set and NIST 2003 as test set. The feature weights are tuned by the modified Koehn’s MER (Och, 2003, Koehn, 2007) trainer. We use case-sensitive BLEU-4 (Papineni et al., 2002) to measure the quality of translation result. Zhang et al. 2004’s implementation is used to do significant test. Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. It means the pruned forest is able to at least keep all the top n best trees. However, because of the sharing nature of the packed forest, it may still contain a larg</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. ACL-03. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics.</title>
<date>2003</date>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="21868" citStr="Och and Ney, 2003" startWordPosition="3837" endWordPosition="3840">ould be CM), then the time complexity upperbound of Algorithm 4 is O(NM(K+CM)). For Algorithm 3, its time complexity is O(RNM(K+CM)), where R is the maximum number of tree fragment matched in each node. 5 Experiment 5.1 Experimental settings We carry out experiment on Chinese-English NIST evaluation tasks. We use FBIS corpus (250K sentence pairs) as training data with the source side parsed by a modified Charniak parser (Charniak 2000) which can output a packed forest. The Charniak Parser is trained on CTB5, tuned on 301-325 portion, with F1 score of 80.85% on 271- 300 portion. We use GIZA++ (Och and Ney, 2003) to do m-to-n word-alignment and adopt heuristic “grow-diag-final-and” to do refinement. A 4-gram language model is trained on Gigaword 3 Xinhua portion by SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We use NIST 2002 as development set and NIST 2003 as test set. The feature weights are tuned by the modified Koehn’s MER (Och, 2003, Koehn, 2007) trainer. We use case-sensitive BLEU-4 (Papineni et al., 2002) to measure the quality of translation result. Zhang et al. 2004’s implementation is used to do significant test. Following (Mi and Huang 2008), we use viterbi algorithm to prune t</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics. 29(1) 19-51</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>ToddWard</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<pages>02--311</pages>
<contexts>
<context position="22288" citStr="Papineni et al., 2002" startWordPosition="3903" endWordPosition="3906"> parser (Charniak 2000) which can output a packed forest. The Charniak Parser is trained on CTB5, tuned on 301-325 portion, with F1 score of 80.85% on 271- 300 portion. We use GIZA++ (Och and Ney, 2003) to do m-to-n word-alignment and adopt heuristic “grow-diag-final-and” to do refinement. A 4-gram language model is trained on Gigaword 3 Xinhua portion by SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We use NIST 2002 as development set and NIST 2003 as test set. The feature weights are tuned by the modified Koehn’s MER (Och, 2003, Koehn, 2007) trainer. We use case-sensitive BLEU-4 (Papineni et al., 2002) to measure the quality of translation result. Zhang et al. 2004’s implementation is used to do significant test. Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. It means the pruned forest is able to at least keep all the top n best trees. However, because of the sharing nature of the packed forest, it may still contain a large number of additional trees. Our statistic shows that when we set the thres</context>
</contexts>
<marker>Papineni, Roukos, ToddWard, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, ToddWard and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. ACL-02.311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<pages>02--901</pages>
<contexts>
<context position="22053" citStr="Stolcke, 2002" startWordPosition="3866" endWordPosition="3867">in each node. 5 Experiment 5.1 Experimental settings We carry out experiment on Chinese-English NIST evaluation tasks. We use FBIS corpus (250K sentence pairs) as training data with the source side parsed by a modified Charniak parser (Charniak 2000) which can output a packed forest. The Charniak Parser is trained on CTB5, tuned on 301-325 portion, with F1 score of 80.85% on 271- 300 portion. We use GIZA++ (Och and Ney, 2003) to do m-to-n word-alignment and adopt heuristic “grow-diag-final-and” to do refinement. A 4-gram language model is trained on Gigaword 3 Xinhua portion by SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We use NIST 2002 as development set and NIST 2003 as test set. The feature weights are tuned by the modified Koehn’s MER (Och, 2003, Koehn, 2007) trainer. We use case-sensitive BLEU-4 (Papineni et al., 2002) to measure the quality of translation result. Zhang et al. 2004’s implementation is used to do significant test. Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. It mean</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. ICSLP-02. 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An Efficient AugmentedContext-Free Parsing Algorithm.</title>
<date>1987</date>
<journal>Computational Linguistics</journal>
<volume>13</volume>
<issue>1</issue>
<pages>31--46</pages>
<contexts>
<context position="5855" citStr="Tomita 1987" startWordPosition="931" endWordPosition="932">source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence. Fig. 1 illustrates this process. In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes. 2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be formally described as a triple , where V is the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence. A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule. Fig. 2 is a packed forest incorporating two parse trees T1 and T2 of a sentence as shown in Fig. 3 and Fig. 4. Given a hyper-edge e, let h be its</context>
</contexts>
<marker>Tomita, 1987</marker>
<rawString>Masaru Tomita. 1987. An Efficient AugmentedContext-Free Parsing Algorithm. Computational Linguistics 13(1-2): 31-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Aiti Aw and Chew Lim Tan.</title>
<date>2009</date>
<pages>09</pages>
<contexts>
<context position="1323" citStr="Zhang et al., 2009" startWordPosition="191" endWordPosition="194">ropose a hyper-tree-based fast algorithm for translation rule matching. Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules. 1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation (SMT) (Galley et al., 2004; Liu et al., 2006, 2007; Zhang et al., 2007, 2008a; Mi et al., 2008; Mi and Huang 2008; Zhang et al., 2009). It translates a source sentence to its target one in two steps by using structured translation rules. In the first step, which is called translation rule matching step, all the applicable1 translation rules are extracted from the entire rule set by matching the source parse tree/forest. The second step is to decode the source sentence into its target one using the extracted translation rules. Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as 1 Given a source structure (either a parse tree or a par</context>
<context position="7196" citStr="Zhang et al. (2009)" startWordPosition="1180" endWordPosition="1183">art, stop]”, where “label” is its syntax category and “[start, stop]” is the range of words it covers. For example, the node in Fig. 5 pointed by the dark arrow is labelled as “NP[3,4]”, where NP is its label and [3,4] means that it covers the span from the 3rd word to the 4th word. In forest-based translation, rule matching is much more complicated than the tree-based one. XNA declaration is related to some regulation Figure 1. A tree-to-string translation process. The tree-to-string model (Galley et al. 2004; Liu et al. 2006) views the translation as a structure mapFigure 2. A packed forest Zhang et al. (2009) reduce the tree sequence problem into tree problem by introducing virtual node and related forest conversion algorithms, so 1038 the algorithm proposed in this paper is also applicable to the tree sequence-based models. For example, if we want to extract useful rules for node NP[3,4] in Fig 5, we have to generate all the tree fragments rooted at node NP[3,4] as shown in Fig 6, and then query each fragment in the rule set. Let be a node in the packed forest, represents the number of possible tree fragments rooted at node , then we have: Figure 3. Tree 1 (T1) Figure 4. Tree 2 (T2) 3 Matching Me</context>
</contexts>
<marker>Zhang, Zhang, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew Lim Tan. 2009. Forest-based Tree Sequence to String Translation Model. ACL-IJCNLP-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
</authors>
<title>Ai Ti Aw, Jun Sun, Sheng Li and Chew Lim Tan.</title>
<date>2007</date>
<volume>07</volume>
<pages>535--542</pages>
<marker>Zhang, Jiang, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li and Chew Lim Tan. 2007. A Tree-to-Tree Alignment-based Model for Statistical Machine Translation. MT-Summit-07. 535-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
</authors>
<title>Hongfei Jiang, Aiti Aw,</title>
<date>2008</date>
<volume>08</volume>
<pages>559--567</pages>
<location>Haizhou Li, Chew Lim Tan, Sheng Li.</location>
<marker>Zhang, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, Sheng Li. 2008a. A Tree Sequence Alignment-based Tree-to-Tree Translation Model. ACLHLT-08. 559-567.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Haizhou Li</author>
</authors>
<title>Aiti Aw, Sheng Li.</title>
<booktitle>2008b. Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation. COLING-08.</booktitle>
<pages>1097--1104</pages>
<marker>Zhang, Jiang, Li, </marker>
<rawString>Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, Sheng Li. 2008b. Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation. COLING-08. 1097-1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC-04</title>
<date>2004</date>
<contexts>
<context position="22352" citStr="Zhang et al. 2004" startWordPosition="3914" endWordPosition="3917">k Parser is trained on CTB5, tuned on 301-325 portion, with F1 score of 80.85% on 271- 300 portion. We use GIZA++ (Och and Ney, 2003) to do m-to-n word-alignment and adopt heuristic “grow-diag-final-and” to do refinement. A 4-gram language model is trained on Gigaword 3 Xinhua portion by SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We use NIST 2002 as development set and NIST 2003 as test set. The feature weights are tuned by the modified Koehn’s MER (Och, 2003, Koehn, 2007) trainer. We use case-sensitive BLEU-4 (Papineni et al., 2002) to measure the quality of translation result. Zhang et al. 2004’s implementation is used to do significant test. Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. It means the pruned forest is able to at least keep all the top n best trees. However, because of the sharing nature of the packed forest, it may still contain a large number of additional trees. Our statistic shows that when we set the threshold as the 100th best tree, the average number of all possible </context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC-04</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>