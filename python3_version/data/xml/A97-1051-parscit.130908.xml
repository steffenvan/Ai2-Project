<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000150">
<title confidence="0.990535">
Mixed-Initiative Development of Language Processing Systems
</title>
<author confidence="0.984929">
David Day, John Aberdeen, Lynette Hirschman,
Robyn Kozierok, Patricia Robinson and Marc Vilain
</author>
<affiliation confidence="0.9278185">
Advanced Information Systems Center
The MITRE Corporation
</affiliation>
<address confidence="0.952665">
202 Burlington Road
Bedford, Massachusetts 01730 U.S.A.
</address>
<email confidence="0.9206285">
fday,aberdeen,lynette @mitre.org
{robyn,parann,mbv } @tnitre.org
</email>
<sectionHeader confidence="0.993598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999827913043478">
Historically, tailoring language processing systems to
specific domains and languages for which they were not
originally built has required a great deal of effort.
Recent advances in corpus-based manual and automatic
training methods have shown promise in reducing the
time and cost of this porting process. These
developments have focused even greater attention on the
bottleneck of acquiring reliable, manually tagged
training data. This paper describes a new set of
integrated tools, collectively called the Alembic
Workbench, that uses a mixed-initiative approach to
&amp;quot;bootstrapping&amp;quot; the manual tagging process, with the
goal of &apos;educing the overhead associated with corpus
development. Initial empirical studies using the
Alembic Workbench to annotate &amp;quot;named entities&amp;quot;
demonstrates that this approach can approximately
double the production rate. As an added benefit, the
combined efforts of machine and user produce domain-
specific annotation rules that can be used to annotate
similar texts automatically through the Alembic NLP
system. The ultimate goal of this project is to enable
end users to generate a practical domain-specific
information extraction system within a single session.
</bodyText>
<sectionHeader confidence="0.997788" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999977232142857">
In the absence of complete and deep text understanding,
implementing information extraction systems remains a
delicate balance between general theories of language
processing and domain-specific heuristics. Recent
developments in the area of corpus-based language
processing systems indicate that the successful
application of any system to a new task depends to a
very large extent on the careful and frequent evaluation
of the evolving system against training and test corpora.
This has focused increased attention on the importance
of obtaining reliable training corpora. Unfortunately,
acquiring such data has usually been a labor-intensive
and time-consuming exercise.
The goal of the Alembic Workbench is to dramatically
accelerate the process by which language processing
systems are tailored to perform new tasks. The
philosophy motivating our work is to maximally reuse
and re-apply every kernel of knowledge available at each
step of the tailoring process. In particular, our approach
applies a bootstrapping procedure to the development of
the training corpus itself. By re-investing the
knowledge available in the earliest training data to pre-
tag subsequent un-tagged data, the Alembic Workbench
can transform the process of manual tagging to one
dominated by manual review. In the limit, if the pre-
tagging process performs well enough, it becomes the
domain-specific automatic tagging procedure itself, =I
can be applied to those new documents from which
information is to be extracted.
As we and others in the information extraction arena
have noticed, the quality of text processing heuristics is
influenced critically not only by the power of one&apos;s
linguistic theory, but also by the ability to evaluate
those theories quickly and reliably. Therefore, building
new information extraction systems requires an
integrated environment that supports: (1) the
development of a domain-specific annotated corpus; (2)
the multi-faceted analysis of that corpus; (3) the ability
to quickly generate hypotheses as to how to extract or
tag information in that corpus; and (4) the ability to
quickly evaluate and analyze the performance of those
hypotheses. The Alembic Workbench is our attempt to
build such an environment.
As the Message Understanding Conferences move into
their tenth year, we have seen a growing recognition of
the value of balanced evaluations against a common test
corpus. What is unique in our approach is to integrate
system development with the corpus annotation process
itself. The early indications are that at the very least
this integration can significantly increase the
productivity of the corpus annotator. We believe that
the benefits will flow in the other direction as well, and
that a concomitant increase in system performance will
follow as one applies the same mixed-initiative
development environment to the problem of domain-
specific tailoring of the language processing system.
</bodyText>
<page confidence="0.992255">
348
</page>
<figure confidence="0.841792695652174">
Alembic Workbench
/11FS/alfsysleinsiatibidataidemo -1111 MwS-11-1-5 key.sgml
&amp;quot;rgiq Perso
Aleinbic Workbftich
Options I tRilitie, j Help
Libiguao: Ei-N115h T c
WiShinitl&amp;quot; VA Exchange Ally, Semis
To Be Strong Concfidate to Head $E4
a VJetfrch Erlrimpi and Owild Wesset,.
Staff Reporters 0 The Wall Street Journal
01/19/93
WALL ST IEET JOURNAL (i), PAGE A?
ConsueliWilibiiigtrin, a longtime %JAR staffer and
an expert in sectuities laws is a liadini uldiclale to be
chairwoinan of the ii
administralicn
Ms. Washing-tea, 44 years al&amp; 5.11.1 be the twat woman and
Itot block to head the five --niciat)er commission that nveiseed FliA ai c r:e1,il tOctotlen?.—
ecurities markets.
1
I pret es5 Text-,,
Mm Ws shingkon&apos;s CAlidiciazy is being championed by StVefai
&apos; Learn Mantic Fbrezt I itidift4 Rules—
</figure>
<figureCaption confidence="0.277568166666667">
owerful lawn-sakors includln her boss, Chairman 4-4itiOhigel
or the tlii , . eat 41.0iiiiO4, siie cud, Perform firer Analysia o! Rules...
a COUti4C1 to the committee. Ms. Witeltnigtiin mid Mr. Dingell fil ,„trao. phrase
considered allies of the securities exchaanes. while banks and:
1 Compare Alternate Annotatiori—
(It&gt; gDOC) &lt;MSGSEcti
</figureCaption>
<bodyText confidence="0.329468">
/. igure 1. Screen dump of a typical Alembic Workbench session.
</bodyText>
<sectionHeader confidence="0.936344" genericHeader="introduction">
2. Alembic Workbench: A brief description
</sectionHeader>
<bodyText confidence="0.999804684210526">
The Alembic Workbench provides a graphical user
interface by which texts can be annotated using the
mouse and user-defined key bindings. The Workbench
mouse interface is engineered specifically to minimize
hand motion. This allows text markup to proceed very
quickly. Once a text has been marked up, the user&apos;s
annotations are highlighted in colors specified by the
user. A &amp;quot;mouse line&amp;quot; at the bottom of the text window
provides further visual feedback indicating all of the
annotations associated with the location under the
mouse cursor, including document structure markup, if
available. An example screen image from a typical
session with the Workbench is shown above.
Our focus in building the Alembic Workbench is to
provide a natural but powerful environment for
annotating texts in the service of developing natural
language processing systems. To this end we have
incorporated a growing number of analysis and reporting
features. The current set of utilities includes:
</bodyText>
<listItem confidence="0.998621608695652">
• A string-matching mechanism that can
automatically replicate new markup to identical
instances elsewhere in the document.
• A rule language for constructing task-specific
phrase tagging and/or pre-tagging rule sets.
• A tool that generates phrase-based KWIC (&amp;quot;key-
word in context&amp;quot;) reports to help the user identify
common patterns in the markup.
• A procedure that generates word lists based on their
frequency. This tool also measures the degree to
which a word occurs in different markup contexts.
• A visualization component for viewing inter-
annotator (or key/answer) agreement.
• A scorer that allows arbitrary SGML markup to be
selected for scoring.
• A full-featured interface to the multi-stage
architecture of the Alembic text processing system.
• An interface to Alembic&apos;s phrase-rule learning
system for generating new application-specific rule
sets.
• The Alembic Workbench also provides specialized
interfaces for supporting more complex, linked
markup such as that needed for coreference. Another
</listItem>
<page confidence="0.998771">
349
</page>
<bodyText confidence="0.99888603030303">
interface is geared towards capturing arbitrary n-ary
relations between tagged elements in a text (these
have been called &amp;quot;Scenario Templates&amp;quot; in MUC).
More details about the implementation of the
Workbench are provided in Section 7.
The development of the Alembic Workbench environ-
ment came about as a result of MITRE&apos; s efforts at
refining and modifying our natural language processing
system, Alembic [1,7], to new tasks: the Message
Understanding Conferences (MUC5 and MUC6), and the
TIPSTER Multi-lingual Entity Task (MET1). (See [6]
for an overview and history of MUC6 and the `Named
Entity Task&amp;quot;.) The Alembic text processing system
applies Eric Brill&apos;s notion of rule sequences [2,3] at
almost every one of its processing stages, from part-of-
speech tagging to phrase tagging, and even to some
portions of semantic interpretation and inference.
While its name indicates its lineage, we do not view the
Alembic Workbench as wedded to the Alembic text
processing system alone. We intend to provide a well-
documented API in the near future for external utilities
to be incorporated smoothly into the corpus/system
development environment. We envision two classes of
external utilities: tagging utilities and analysis utilities.
By integrating other tagging modules (including
complete NLP systems), we hope those systems can be
more efficiently customized when the cycle of analysis,
hypothesis generation and testing is tightened into a
well-integrated loop. The current version of the tool
supports viewing, annotating and analyzing documents
in 7-bit, 8-bit and 2-byte character sets. Current
support includes the Latin-1 languages, Japanese (JS),
Chinese (GB1232), Russian, Greek and Thai.
</bodyText>
<sectionHeader confidence="0.723877" genericHeader="method">
3. Increasing manual annotation productivity
through pre-tagging
</sectionHeader>
<bodyText confidence="0.999989093023256">
A motivating idea in the design of the Alembic
Workbench is to apply any available information as
early and as often as possible to reduce the burden of
manual tagging. In addition to careful interface design
and support for user-customization, a core mechanism
for enhancing this process is through pre-tagging.
The generation of reliably tagged text corpora requires
that a human annotator read and certify all of the
annotations applied to a document. This is especially
true if the annotations are to be used for subsequent
manual or automatic training procedures. However,
much of the drudgery of this process can be removed if
the most obvious and/or oft-repeated expressions can be
tagged prior to the annotator&apos;s efforts. One way of
doing this is to apply tags to any and all strings in a
document that match a given string. This is the nature
of the &amp;quot;auto-tagging&apos; facility built-in to the Workbench
interface. For example, in annotating journalistic
document collections with &amp;quot;Named Entity&amp;quot; tags, one
might want to simply pre-tag every occurrence of
&amp;quot;President Clinton&amp;quot; with Person..&apos; Of course, these
actions should be taken with some care, since mis-
tagging entities throughout a document might actually
lead to an increase in effort required to accurately fix or
remove tags in the document.
A more powerful approach is to allow patterns, or rules,
to form the basis for this pre-tagging. The Alembic
phrase-rule interpreter provides the basis for developing
rule-based pre-tagging heuristics in the Workbench. In
the current version of the Workbench, the user is free to
compose these &amp;quot;phrase?&apos; rules and group them into
specialized rule sets. Figure 2 shows an example
sequence of rules that could be composed for pre-tagging
a corpus with Person tags. The Brill control regime
interprets these rules strictly sequentially: rule n is
applied wherever in the text it can be; it is then
discarded and rule n+1 is consulted. There is no
unconstrained forward chaining using a &amp;quot;soup&amp;quot; of rules
as in a standard production (or rule-based) system. The
Alembic &amp;quot;phrase?&apos; rule interpreter has been applied to
tagging named entities, sentence chunks, simple entity
relations (&amp;quot;template element&amp;quot; in the parlance of MUC6),
and other varieties of phrases.
</bodyText>
<figure confidence="0.871177875">
(def-phraser-rule
:anchor :lexeme
:conditions (:left-1 :lex (&amp;quot;Mr.&amp;quot; &amp;quot;Ms.&amp;quot; &amp;quot;Dr.&amp;quot; ...))
:actions (:create-phrase :person))
(def-phraser-rule
:conditions (:phrase :phrase-label :person)
(:right-1 :pos :NNP)
:actions (:expand :right-1))
</figure>
<figureCaption confidence="0.8890945">
Figure 2. An example Alembic rule sequence that (1)
produces Person phrases around any word immediately to
</figureCaption>
<bodyText confidence="0.833086">
the right of a title and/or honorific, and then (2) grows the
extent of the phrase to the right one lexeme, if that word is
a proper noun.
</bodyText>
<sectionHeader confidence="0.825543" genericHeader="method">
4. Mixed-initiative text annotation
</sectionHeader>
<bodyText confidence="0.999688375">
In addition to allowing users to define pre-tagging rules,
we have developed a learning procedure that can be used
to induce these rules from small training corpora.
Operationally, an annotator starts by generating a small
initial corpus and then invokes the learner to derive a set
of pre-tagging rules. These rules can then be applied to
new, unseen texts to pre-tag them. Figure 3 illustrates
this bootstrapping cycle.
</bodyText>
<footnote confidence="0.833055">
1 The Named Entity task from MUC6 consists of adding
tags to indicate expressions of type Person, Location,
Organization, Date, Time and Money, see [6].
</footnote>
<page confidence="0.995024">
350
</page>
<bodyText confidence="0.9996957">
The earlier we can extract heuristic rules on the basis of
manually tagged data, the earlier the user can be relieved
from some portion of the chore of physically marking
up the text—the user will need to edit and/or add only a
fraction of the total phrases in a given document. In
our experience of applying the Alembic phrase rule
learner to named-entity and similar problems, our error-
reduction learning method requires only modest amounts
of training data. (We present performance details in
Section 6.)
</bodyText>
<subsectionHeader confidence="0.911686666666667">
Domain-specific
Tagging Rules
Training/Testing corpora
</subsectionHeader>
<bodyText confidence="0.966073342857143">
Figure 3. The Alembic Workbench seeks to involve
the user in a corpus development cycle, making use of
pre-tagging facilities, analysis facilities, and the
automatic generation of pre-tagging rule sets through
machine learning.
As the human annotator continues generating reliable
training data, she may, at convenient intervals, re-
invoke the learning process. As the amount of training
data increases, the performance of the learned rules tends
to increase, and so the amount of labor saved in pre-
tagging subsequent training data is further increased.
The bootstrapping effect tends to increase over time.
For the &amp;quot;named entity&amp;quot; task in MUC6 approximately
25,000 words were provided as annotated training data
by the conference organizers (&amp;quot;formal training&amp;quot; and
&amp;quot;dryrun&amp;quot; data sets). Prior to developing the Alembic
Workbench, we were able to use this amount of data in
Alembic to generate a system performing at 85.2 P&amp;R
on unseen test data.2 Based on the tagging rates we
have measured thus far using the Workbench, it would
take somewhere between 1.5 to 2.5 hours to tag these
25,000 words of data.
There is a limit on how much one can reduce the time-
requirements for generating reliable training data—this
is the rate required by a human domain expert to
carefully read and edit a perfectly pre-annotated training
corpus. Training data cannot be generated without this
2 P&amp;R (or F-measure) is a weighted combination of recall
and precision.
human investment.3 Indeed, in situations where the
quality of the data is particularly important (as it is in,
say a multi-system evaluation such as MUC), it is
typical that multiple reviews of the same corpus is
performed by various annotators, especially given the
known ambiguity of any annotation task definition.
</bodyText>
<sectionHeader confidence="0.698007" genericHeader="method">
5. Manual refinement of automatically
derived pre-tagging heuristics
</sectionHeader>
<bodyText confidence="0.975731136363637">
In the previous section we presented our approach to
mixed-initiative corpus development and tagging
heuristics without assuming any sophistication on the
part of the human user beyond a clear understanding of
the information extraction task being addressed.
Usually, however, even a lay end-user is likely to have
a number of intuitions about how the un-annotated data
could be pre-tagged to reduce the burden of manual
tagging. Hand-coded rules can be applied in concert
with the machine-derived rules mentioned earlier. One
way this can be done is by invoking the rule learning
subsequent to the application of the hand-coded pre-
tagging rules. On the other hand, if the user notices a
consistent mistake being made by the machine-learned
rules early in the bootstrapping process, the user can
augment the machine-derived rule sequence with
manually composed rules. In fact, every rule composed
by the learning procedure is completely inspectable by
the user, and so some users may want to modify
individual machine-derived rules, perhaps to expand their
generality beyond the particular data available in the
emerging corpus.
This is another way, then, that the Alembic Workbench
environment enables and encourages the mixed, or
cooperative, application of human and machine skills to
the combined task of developing a domain-specific
corpus and set of extraction heuristics.
Of course, composing rules is somewhat akin to
programming, and not all users will be inclined, or
well-equipped, to become involved in this process. One
impediment to end-users composing their own rules is
the particular syntax of Alembic&apos;s phraser rules, so we
anticipate exploring other, simpler rule languages that
will encourage end-user participation. Another approach
that we are interested in exploring involves supporting
more indirect feedback or directives from the user that
are rooted more closely to examples in the data.
3 This is not to say that high-quality machine-tagged data
cannot be generated faster than this, and that these data
may indeed be helpful in the learning procedure of some
other systems. But all such data will remain suspect as far
as being considered part of an annotated training corpus
until inspected by a human, given the vagaries of genre and
style that can easily foil the most sophisticated systems.
</bodyText>
<figure confidence="0.992695">
Unprocessed material
11,
Alembic
***bench
</figure>
<page confidence="0.98336">
351
</page>
<bodyText confidence="0.996357594594595">
Similarities and differences between manual
and automatic rule formation
The automatic rule-learning procedure uses a generate-
and-test approach to learn a sequence of rules. A set of
rule schemata, defining a set of possible rule instances
determines the rule space that the learning procedure
explores. The learner uses indexing based on the actual
data present in the corpus to help it explore the rule
space efficiently. The learning process is initiated by
deriving and applying an initial labeling function based
on the differences between an un-annotated version and a
correctly annotated version of the corpus. Then, during
each learning cycle, the learner tries out applicable rule
instances and selects the rule that most improves the
score when applied to the corpus. The score is
determined by evaluating the corpus as currently
annotated against the correctly annotated version, using
some evaluation function (generally precision, recall or
F-measure). The corpus annotation is updated by
applying the chosen rule, and the learning cycle repeats.
This cycle is continued until a stopping criterion is
reached, which is usually defined as the point where
performance improvement falls below a threshold, or
ceases. Other alternatives include setting a strict limit
on the number of rules, and testing the performance
improvement of a rule on a corpus distinct from the
training set.
Of course, there are two important advantages that a
human expert might have over the machine algorithm:
linguistic intuition and world knowledge. Rules that
include references to a single lexeme can be expanded to
more general applicability by the human expert who is
able to predict alternatives that lie outside the current
corpus available to the machine. By supporting
multiple ways in which rules can be hypothesized,
refined and tested, the strengths of both sources of
knowledge can be brought to bear.
</bodyText>
<sectionHeader confidence="0.988373" genericHeader="method">
6. Experimental Results
</sectionHeader>
<bodyText confidence="0.999925085714286">
We are still in the early stages of evaluating the
performance of the Alembic Workbench along a number
of different dimensions. However, the results from early
experiments are encouraging. Figure 4 compares the
productivity rates using different corpus development
utilities. These are indicated by the four categories on
the X-axis: (1) using SGML-mode in emacs (by an
expert user); (2) using the Workbench interface and
&amp;quot;auto-tag&amp;quot; string-matching utility only; (3) using the
Workbench following the application of learned tagging
rules derived from 5 short documents—approximately
1,500 words, and (4) using the Workbench following
the application of learned tagging rules again, but this
time with the learned rules having trained on 100
documents (approximately 48,000 words), instead of
only five documents.
As can be seen in these experiments, there is a clear
increase in the productivity as a function of both the
user interface (second column) and the application of
pre-tagging rules (third and fourth columns). The large
step in performance between columns three and four
indicate that repeated invocation of the learning process
during the intermediate stages of the corpus
development cycle will likely result in acceleration of
the annotation rate. (As it happens, these results are
probably underestimating the pre-tagging productivity.
The reason for this is that the version of the Workbench
used was not yet able to incorporate date and time
annotations generated by a separate pre-processing step;
this date and time tagger performs at an extremely high
level of precision for this genre—in the high nineties
P&amp;R.) These initial experiments involved a single
expert annotator on a single tagging task (MUC6 named
entity). The annotator was very familiar with the
tagging task.
</bodyText>
<figureCaption confidence="0.99950525">
Figure 4. Two measures of corpus annotation
productivity using the Alembic Workbench. The X-axis
indicates what kind of corpus-development utilities were
used: (1) SGML-mode of emacs text editor; (2) Workbench
(AWB) manual interface only, (3) AWB rule-learning
bootstrap method with 5-document training set; (4) AWB
rule-learning bootstrap method with 100-document
training set. See discussion in text.
</figureCaption>
<bodyText confidence="0.999764">
To place this in the perspective of the human annotator,
after only about 15 minutes of named entity tagging,
having annotated some 1,500 words of text with
approximately 150 phrases, the phrase rule learner can
derive heuristic rules that produce a pre-tagging
performance rate (P&amp;R) of between 50 and 60 percent.
Of course, this performance is far short of what is
needed for a practical extraction system, but it already
constitutes a major source for labor savings, since
50 to 60 percent of the annotations that need to be
moused (or clicked) in are already there. Since the
precision at this early stage is only around 60 percent,
there will be extra phrases that need (1) to be removed,
(2) their assigned category changed (from, say,
</bodyText>
<page confidence="0.996627">
352
</page>
<bodyText confidence="0.999909283018868">
organization to person), or (3) their boundaries adjusted.
It turns out that for the first two of these kinds of
precision errors, the manual corrections are extremely
quick to perform. (Boundaries are not really difficult to
modify, but the time required is approximately the same
as inserting a tag from scratch.) In addition, making
these corrections removes both a precision and a recall
error at the same time. Therefore, it turns out that even
at this very early stage, the modest pre-tagging
performance gained from applying the learning
procedure provides measurable performance
improvement.
In order to obtain more detailed results on the effect of
pre-tagging corpora, we conducted another experiment in
which we made direct use of the iterative automatic
generation of rules from a growing manually-tagged
corpus. Using the same skilled annotator, we
introduced a completely new corpus for which named-
entity tagging happened to be needed within our
company. We randomly divided approximately 50
documents of varying sizes into five groups. The word
counts for these five groups were: Groupl: 19,300;
Group2: 13,800; Group3: 6,300; Group4: 15,800;
Group5: 8,000; for a total of 63,000 words. After
manually tagging the first group, we invoked the rule
learning procedure. Applying the learning procedure on
each training set required two to three hours of elapsed
time on a Sun Sparc Ultra. The new tagging rules were
then applied to the next ten documents prior to being
manually tagged/edited. This enlarged corpus was then
used to derive a new rule set to be applied to the next
group of documents, and so on. A summarization of
the results are presented in Figure 5.
Clearly, more experiments are called for—we plan to
conduct these across different annotators, task types, and
languages, to better evaluate productivity, quality and
other aspects of the annotation process.
It is extremely difficult to control many of the features
that influence the annotation process, such as the
intrinsic complexity of the topic in a particular
document, the variation in tag-density (tags per word)
that may occur, the user&apos;s own training effect as the
structure and content of documents become more
familiar, office distractions, etc. In order to gain a
better understanding of the underlying tagging
performance of the rule learner, and so separate out
some of these human factors issues, we ran an
automated experiment in which different random subsets
of sentences were used to train rule sets, which were
then evaluated on a static test corpus. The results
shown in Figure 6 give some indication of the ability
of the rule-sequence learning procedure to glean useful
generalizations from meager amounts of training data.
</bodyText>
<figure confidence="0.976499">
Performance of Learned Rule Set as a
Function of Training Set Size
z 80 -1118111111re
i! 70 Precision
60
0 50
I3200
10
a 0
Training Set Size (Named Entities)
</figure>
<figureCaption confidence="0.876509166666667">
Figure 5. Tagging productivity gains with the
incremental application of automatically acquired rule sets.
Figure 6. Performance of learned rules on independent
test set of 662 sentences.
Average Performance of Learned Rules as
a Function of Training Set Size
</figureCaption>
<table confidence="0.991584846153846">
80
g -Series ii
o
et 50 -
Et 74 4300
I I- 20 -
a 10 -
0
ot to ro C&apos;t r-
Lti ,,i COto
,- to ol :7; r-
- .- ot a
Training Entities
</table>
<figure confidence="0.950944363636364">
Productivity by Group
30
25 -
15 -
.C71
41 1 0 -
I-
s —
Tag/Minute I
1 2 3 4 5
Group
</figure>
<bodyText confidence="0.999472428571429">
The first observation we make is that there is a clear and
obvious direction of improvement—by the time 30
documents have been tagged, the annotation rate on
Group 4 has increased considerably. It is important to
note, however, that there is still noise in the curve. In
addition, the granularity is perhaps still too coarse to
measure the incremental influences of pre-tagging rules.
</bodyText>
<figureCaption confidence="0.998755">
Figure 7. Averaged F-measure performance figures.
</figureCaption>
<bodyText confidence="0.966233">
One clear effect of increasing training set size is a
reduction in the sensitivity of the learning procedure to
particular training sets. We hypothesize that this effect
is partly indicative of the generalization behavior on
which the learning procedure is based, which amplifies
</bodyText>
<page confidence="0.998258">
353
</page>
<bodyText confidence="0.9999835">
the effects of choosing more or less representative
training sentences by chance. Since the learning
process is not merely memorizing phrases, but
generating contextual rules to try to predict phrase types
and extents, the rules are very sensitive to extremely
small selections of training sentences. Figure 7 shows
the F-measure performance smoothed by averaging
neighboring data points, to get a clearer picture of the
general tendency.
We should note that the Alembic Workbench, having
been developed only recently in our laboratory, was not
available to us in the course of our effort to apply the
Alembic system to the MUC6 and MET tasks.
Therefore we have not been able to measure its
influence in preparing for a particular new text
processing task. We intend to use the system to prepare
for future evaluations (including MUC7 and MET2) and
to carefully evaluate the Alembic Workbench as an
environment for the mixed-initiative development of
information extraction systems in multiple languages.
</bodyText>
<sectionHeader confidence="0.984356" genericHeader="method">
7. Implementation
</sectionHeader>
<bodyText confidence="0.999905607142857">
The Alembic Workbench interface has been written in
Tc1/11. Some of the analysis and reporting utilities
(available from within the interface as well as Unix
command-line utilities) are written in Pert, C or Lisp.
The separate Alembic NLP system consists of C pre-
processing taggers (for dates, word and sentence
tokenization and part-of-speech assignments) and a Lisp
image that incorporates the rest of Alembic: the phrase-
rule interpreter, the phrase rule learner, and a number of
discourse-level inference mechanisms described in [8].
This code currently runs on Sun workstations running
Sun OS 4.1.3 and Solaris 2.4 (Sun OS 5.4) and greater;
we have begun porting the system to Windows
NT/Windows 95. We anticipate providing an API for
integrating other NLP systems in the near future.
The Workbench reads and saves its work in the form of
SGML-encoded files, though the original document need
not contain any SGML mark-up at all. These files are
parsed with the help of an SGML normalizer.4 During
the course of the annotation process the Workbench
uses a &amp;quot;Parallel Tag File&amp;quot; (PTF) format, which
separates out the embedded annotations from the source
text, and organizes user-defined sets of annotations
within distinct &amp;quot;tag files.&amp;quot; While these files are
generally hidden from the user, they provide a basis for
the combination and separation of document annotations
(&amp;quot;tagsets&amp;quot;) without needing to modify or otherwise
disturb the base document. This allows the user to view
</bodyText>
<footnote confidence="0.9791076">
4 In cases where documents use some of the more complex
aspects of SGML, the user supplies a Document Type
Description (DTD) file for use in normalization. For simple
SGML documents, or documents with no original SGML
markup at all, no DTD needs to be specified.
</footnote>
<bodyText confidence="0.9998364">
only Named Entity tags, or only tokenization tags, or
any desired subset of tagsets. Thus, the Workbench is
written to be TIPSTER-compliant, though it is not
itself a document manager as envisioned by that
architecture (see [5]). We anticipate integrating the
Workbench with other TIPSTER compliant modules
and document managers via the exchange of SGML-
formatted documents. The Parallel Tag File (PTF)
format used by the Workbench provides another means
by which a translator could be written.
</bodyText>
<sectionHeader confidence="0.986786" genericHeader="discussions">
8. Future Work
</sectionHeader>
<bodyText confidence="0.999815627906977">
Broadly defined, there are two distinct types of users
who we imagine will find the Workbench useful: NLP
researchers and information extraction system end-users.
While our dominant focus so far has been on supporting
the language research community, it is important to
remember that new domains for language processing
generally, and information extraction in particular, will
have their own domain experts, and we want the text
annotation aspects of the tool to be quite usable by a
wide population. In this vein we would like to enable
virtually any user to be able to compose new patterns
(rules) for performing pre-tagging on the data. While
the current rule language has a simple syntax, as well as
an extremely simple control regimen, we do not
imagine all users will want to engage directly in an
exploration for pre-tagging rules. A goal for our future
research is to explore new methods for incorporating
end-user feedback to the learning procedure. This
feedback might include modifying a very simplified
form of a single rule for greater generality by
integrating thesauri to construct word-list suggestions.
We also would like to give users immediate feedback as
to how a single rule applies (correctly and incorrectly)
to many different phrases in the corpus.
In this paper we have concentrated on the named entity
task as a generic case of corpus annotation. Of course,
there are many different ways in which corpora are being
annotated for many different tasks. Some of the specific
extensions to the user interface that we have already
begun building include part-of-speech tagging (and
&amp;quot;dense&amp;quot; markup more generally), and full parse syntactic
tagging (where we believe reliable training data can be
obtained much more quickly than heretofore). In these
and other instances the tagging process can be
accelerated by applying partial knowledge early on,
transforming the task once again into that of editing and
correcting. Most of these tagging tasks would be
improved by making use of methods that preferentially
select ambiguous data for manual annotation—for
example, as described in [4].
There are a number of psychological and human factors
issues that arise when one considers how the pre-
annotated data in a mixed-initiative system may affect
</bodyText>
<page confidence="0.996639">
354
</page>
<bodyText confidence="0.99998">
the human editing or post-processing. If the pre-
tagging process has a relatively high recall, then we
hypothesize that the human will tend increasingly to
trust the pre-annotations, and thereby forget to read the
texts carefully to discover any phrases that escaped
being annotated. A similar effect seems possible for
relatively high precision systems, though proper
interface design (to highlight the type assigned to a
particular phrase) should be able to mitigate these
tendencies. A more subtle interaction is &amp;quot;theory creep,&amp;quot;
where the heuristics induced by the machine learning
component begin to be adopted by the human annotator,
due, in many cases, to the intrinsic ambiguity of
defining annotation tasks in the first place. In all of
these cases the most reliable method for detecting these
human/machine interactions is probably to use some
representative sub-population of the corpus documents
to measure and analyze the inter-annotator agreement
between human annotators who have and who have not
been exposed to the machine derived heuristics for
assigning annotations.
</bodyText>
<sectionHeader confidence="0.99882" genericHeader="acknowledgments">
9. Conclusions
</sectionHeader>
<bodyText confidence="0.999971935483871">
On the basis of observing our own and others&apos;
experiences in building and porting natural language
systems for new domains, we have come to appreciate
the pivotal role played in continuous evaluation
throughout the system development cycle. But
evaluation rests on an oracle, and for text processing,
that oracle is the training and test corpora for a
particular task. This has led us to develop a tailoring
environment which focuses all of the available
knowledge on accelerating the corpus development
process. The very same learning procedure that is used
to bootstrap the manual tagging process leads
eventually to the derivation of tagging heuristics that
can be applied in the operational setting to unseen
documents. Rules derived manually, automatically, and
through a combination of efforts have been applied
successfully in a variety of languages, including
English, Spanish, Portuguese, Japanese and Chinese.
The tailoring environment, known as the Alembic
Workbench, has been built and used within our
organization, and we are making it available to other
organizations involved in the development of language
processing systems and/or annotated corpora. Initial
experiments indicate an significant improvement in the
rate at which annotated corpora can be generated using
the Alembic Workbench methodology. Earlier work
has shown that with the training data obtained in the
course of only a couple of hours of text annotation, an
information extraction system can be induced purely
automatically that achieves a very competitive level of
performance.
</bodyText>
<sectionHeader confidence="0.996482" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99986584375">
[1] John Aberdeen, John Burger, David Day, Lynette
Hirschman, David Palmer, Patricia Robinson, and Marc
Vilain. 1996. The Alembic system as used in MET. In
Proceedings of the TIPSTER 24 Month Workshop,
May.
[2] Eric Brill. 1992. A simple rule-based part of speech
tagger. In Proceedings of the Third Conference on
Applied Natural Language Processing, Trento.
[3] Eric Brill. 1993. A Corpus-Based Approach to
Language Learning. Ph.D. thesis, University of
Pennsylvania, Philadelphia, Penn.
[4] Sean P. Engelson and Ido Dagan. 1996. Minimizing
manual annotation cost in supervised training from
corpora. Computation and Linguistic E-Print Service
(cmp-lg/9606030), June.
[5] Ralph Grishman. 1995. &apos;TIPSTER phase 11
architecture design. World Wide Web document.
URL=http://cs.nyu.edu/cs/faculty/grishtnan/tipster.html
[6] Ralph Grishman and Beth Sundheim. 1996.
Message Understanding Conference-6: A Brief
History. In International Conference on Computational
Linguistics, Copenhagen, Denmark, August. The
International Committee on Computational Linguistics.
[7] Marc Vilain and David Day. 1996. Finite-state
parsing by rule sequences. In International Conference
on Computational Linguistics, Copenhagen, Denmark,
August. The International Committee on
Computational Linguistics.
[8] Marc Vilain. 1993. Validation of terminological
inference in an information extraction task. In
Proceedings of the ARPA Workshop on Human
Language Technology, Plainsboro, New Jersey.
</reference>
<page confidence="0.999007">
355
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.516068">
<title confidence="0.999997">Mixed-Initiative Development of Language Processing Systems</title>
<author confidence="0.999232">David Day</author>
<author confidence="0.999232">John Aberdeen</author>
<author confidence="0.999232">Lynette Hirschman</author>
<author confidence="0.999232">Robyn Kozierok</author>
<author confidence="0.999232">Patricia Robinson</author>
<author confidence="0.999232">Marc Vilain</author>
<affiliation confidence="0.998046">Advanced Information Systems Center The MITRE Corporation</affiliation>
<address confidence="0.997405">202 Burlington Road Bedford, Massachusetts 01730 U.S.A.</address>
<email confidence="0.68623">fday,aberdeen,lynette@mitre.org{robyn,parann,mbv}@tnitre.org</email>
<abstract confidence="0.999597166666667">Historically, tailoring language processing systems to specific domains and languages for which they were not originally built has required a great deal of effort. Recent advances in corpus-based manual and automatic training methods have shown promise in reducing the time and cost of this porting process. These developments have focused even greater attention on the bottleneck of acquiring reliable, manually tagged training data. This paper describes a new set of integrated tools, collectively called the Alembic that uses a to &amp;quot;bootstrapping&amp;quot; the manual tagging process, with the goal of &apos;educing the overhead associated with corpus development. Initial empirical studies using the Alembic Workbench to annotate &amp;quot;named entities&amp;quot; demonstrates that this approach can approximately double the production rate. As an added benefit, the combined efforts of machine and user produce domainspecific annotation rules that can be used to annotate similar texts automatically through the Alembic NLP system. The ultimate goal of this project is to enable end users to generate a practical domain-specific information extraction system within a single session.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Aberdeen</author>
<author>John Burger</author>
<author>David Day</author>
<author>Lynette Hirschman</author>
<author>David Palmer</author>
<author>Patricia Robinson</author>
<author>Marc Vilain</author>
</authors>
<title>The Alembic system as used in MET.</title>
<date>1996</date>
<booktitle>In Proceedings of the TIPSTER 24 Month Workshop,</booktitle>
<contexts>
<context position="8121" citStr="[1,7]" startWordPosition="1211" endWordPosition="1211">rating new application-specific rule sets. • The Alembic Workbench also provides specialized interfaces for supporting more complex, linked markup such as that needed for coreference. Another 349 interface is geared towards capturing arbitrary n-ary relations between tagged elements in a text (these have been called &amp;quot;Scenario Templates&amp;quot; in MUC). More details about the implementation of the Workbench are provided in Section 7. The development of the Alembic Workbench environment came about as a result of MITRE&apos; s efforts at refining and modifying our natural language processing system, Alembic [1,7], to new tasks: the Message Understanding Conferences (MUC5 and MUC6), and the TIPSTER Multi-lingual Entity Task (MET1). (See [6] for an overview and history of MUC6 and the `Named Entity Task&amp;quot;.) The Alembic text processing system applies Eric Brill&apos;s notion of rule sequences [2,3] at almost every one of its processing stages, from part-ofspeech tagging to phrase tagging, and even to some portions of semantic interpretation and inference. While its name indicates its lineage, we do not view the Alembic Workbench as wedded to the Alembic text processing system alone. We intend to provide a well</context>
</contexts>
<marker>[1]</marker>
<rawString>John Aberdeen, John Burger, David Day, Lynette Hirschman, David Palmer, Patricia Robinson, and Marc Vilain. 1996. The Alembic system as used in MET. In Proceedings of the TIPSTER 24 Month Workshop, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<location>Trento.</location>
<contexts>
<context position="8403" citStr="[2,3]" startWordPosition="1255" endWordPosition="1255">s in a text (these have been called &amp;quot;Scenario Templates&amp;quot; in MUC). More details about the implementation of the Workbench are provided in Section 7. The development of the Alembic Workbench environment came about as a result of MITRE&apos; s efforts at refining and modifying our natural language processing system, Alembic [1,7], to new tasks: the Message Understanding Conferences (MUC5 and MUC6), and the TIPSTER Multi-lingual Entity Task (MET1). (See [6] for an overview and history of MUC6 and the `Named Entity Task&amp;quot;.) The Alembic text processing system applies Eric Brill&apos;s notion of rule sequences [2,3] at almost every one of its processing stages, from part-ofspeech tagging to phrase tagging, and even to some portions of semantic interpretation and inference. While its name indicates its lineage, we do not view the Alembic Workbench as wedded to the Alembic text processing system alone. We intend to provide a welldocumented API in the near future for external utilities to be incorporated smoothly into the corpus/system development environment. We envision two classes of external utilities: tagging utilities and analysis utilities. By integrating other tagging modules (including complete NLP</context>
</contexts>
<marker>[2]</marker>
<rawString>Eric Brill. 1992. A simple rule-based part of speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Corpus-Based Approach to Language Learning.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, Penn.</location>
<contexts>
<context position="8403" citStr="[2,3]" startWordPosition="1255" endWordPosition="1255">s in a text (these have been called &amp;quot;Scenario Templates&amp;quot; in MUC). More details about the implementation of the Workbench are provided in Section 7. The development of the Alembic Workbench environment came about as a result of MITRE&apos; s efforts at refining and modifying our natural language processing system, Alembic [1,7], to new tasks: the Message Understanding Conferences (MUC5 and MUC6), and the TIPSTER Multi-lingual Entity Task (MET1). (See [6] for an overview and history of MUC6 and the `Named Entity Task&amp;quot;.) The Alembic text processing system applies Eric Brill&apos;s notion of rule sequences [2,3] at almost every one of its processing stages, from part-ofspeech tagging to phrase tagging, and even to some portions of semantic interpretation and inference. While its name indicates its lineage, we do not view the Alembic Workbench as wedded to the Alembic text processing system alone. We intend to provide a welldocumented API in the near future for external utilities to be incorporated smoothly into the corpus/system development environment. We envision two classes of external utilities: tagging utilities and analysis utilities. By integrating other tagging modules (including complete NLP</context>
</contexts>
<marker>[3]</marker>
<rawString>Eric Brill. 1993. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania, Philadelphia, Penn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean P Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>Computation and Linguistic E-Print Service (cmp-lg/9606030),</booktitle>
<contexts>
<context position="31919" citStr="[4]" startWordPosition="4984" endWordPosition="4984">sions to the user interface that we have already begun building include part-of-speech tagging (and &amp;quot;dense&amp;quot; markup more generally), and full parse syntactic tagging (where we believe reliable training data can be obtained much more quickly than heretofore). In these and other instances the tagging process can be accelerated by applying partial knowledge early on, transforming the task once again into that of editing and correcting. Most of these tagging tasks would be improved by making use of methods that preferentially select ambiguous data for manual annotation—for example, as described in [4]. There are a number of psychological and human factors issues that arise when one considers how the preannotated data in a mixed-initiative system may affect 354 the human editing or post-processing. If the pretagging process has a relatively high recall, then we hypothesize that the human will tend increasingly to trust the pre-annotations, and thereby forget to read the texts carefully to discover any phrases that escaped being annotated. A similar effect seems possible for relatively high precision systems, though proper interface design (to highlight the type assigned to a particular phra</context>
</contexts>
<marker>[4]</marker>
<rawString>Sean P. Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. Computation and Linguistic E-Print Service (cmp-lg/9606030), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>TIPSTER phase 11 architecture design. World Wide Web document. URL=http://cs.nyu.edu/cs/faculty/grishtnan/tipster.html</title>
<date>1995</date>
<contexts>
<context position="29530" citStr="[5]" startWordPosition="4604" endWordPosition="4604">tions (&amp;quot;tagsets&amp;quot;) without needing to modify or otherwise disturb the base document. This allows the user to view 4 In cases where documents use some of the more complex aspects of SGML, the user supplies a Document Type Description (DTD) file for use in normalization. For simple SGML documents, or documents with no original SGML markup at all, no DTD needs to be specified. only Named Entity tags, or only tokenization tags, or any desired subset of tagsets. Thus, the Workbench is written to be TIPSTER-compliant, though it is not itself a document manager as envisioned by that architecture (see [5]). We anticipate integrating the Workbench with other TIPSTER compliant modules and document managers via the exchange of SGMLformatted documents. The Parallel Tag File (PTF) format used by the Workbench provides another means by which a translator could be written. 8. Future Work Broadly defined, there are two distinct types of users who we imagine will find the Workbench useful: NLP researchers and information extraction system end-users. While our dominant focus so far has been on supporting the language research community, it is important to remember that new domains for language processin</context>
</contexts>
<marker>[5]</marker>
<rawString>Ralph Grishman. 1995. &apos;TIPSTER phase 11 architecture design. World Wide Web document. URL=http://cs.nyu.edu/cs/faculty/grishtnan/tipster.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message Understanding Conference-6: A Brief History.</title>
<date>1996</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="8250" citStr="[6]" startWordPosition="1230" endWordPosition="1230">, linked markup such as that needed for coreference. Another 349 interface is geared towards capturing arbitrary n-ary relations between tagged elements in a text (these have been called &amp;quot;Scenario Templates&amp;quot; in MUC). More details about the implementation of the Workbench are provided in Section 7. The development of the Alembic Workbench environment came about as a result of MITRE&apos; s efforts at refining and modifying our natural language processing system, Alembic [1,7], to new tasks: the Message Understanding Conferences (MUC5 and MUC6), and the TIPSTER Multi-lingual Entity Task (MET1). (See [6] for an overview and history of MUC6 and the `Named Entity Task&amp;quot;.) The Alembic text processing system applies Eric Brill&apos;s notion of rule sequences [2,3] at almost every one of its processing stages, from part-ofspeech tagging to phrase tagging, and even to some portions of semantic interpretation and inference. While its name indicates its lineage, we do not view the Alembic Workbench as wedded to the Alembic text processing system alone. We intend to provide a welldocumented API in the near future for external utilities to be incorporated smoothly into the corpus/system development environme</context>
<context position="12818" citStr="[6]" startWordPosition="1939" endWordPosition="1939">ed-initiative text annotation In addition to allowing users to define pre-tagging rules, we have developed a learning procedure that can be used to induce these rules from small training corpora. Operationally, an annotator starts by generating a small initial corpus and then invokes the learner to derive a set of pre-tagging rules. These rules can then be applied to new, unseen texts to pre-tag them. Figure 3 illustrates this bootstrapping cycle. 1 The Named Entity task from MUC6 consists of adding tags to indicate expressions of type Person, Location, Organization, Date, Time and Money, see [6]. 350 The earlier we can extract heuristic rules on the basis of manually tagged data, the earlier the user can be relieved from some portion of the chore of physically marking up the text—the user will need to edit and/or add only a fraction of the total phrases in a given document. In our experience of applying the Alembic phrase rule learner to named-entity and similar problems, our errorreduction learning method requires only modest amounts of training data. (We present performance details in Section 6.) Domain-specific Tagging Rules Training/Testing corpora Figure 3. The Alembic Workbench</context>
</contexts>
<marker>[6]</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: A Brief History. In International Conference on Computational Linguistics, Copenhagen, Denmark, August. The International Committee on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>David Day</author>
</authors>
<title>Finite-state parsing by rule sequences.</title>
<date>1996</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="8121" citStr="[1,7]" startWordPosition="1211" endWordPosition="1211">rating new application-specific rule sets. • The Alembic Workbench also provides specialized interfaces for supporting more complex, linked markup such as that needed for coreference. Another 349 interface is geared towards capturing arbitrary n-ary relations between tagged elements in a text (these have been called &amp;quot;Scenario Templates&amp;quot; in MUC). More details about the implementation of the Workbench are provided in Section 7. The development of the Alembic Workbench environment came about as a result of MITRE&apos; s efforts at refining and modifying our natural language processing system, Alembic [1,7], to new tasks: the Message Understanding Conferences (MUC5 and MUC6), and the TIPSTER Multi-lingual Entity Task (MET1). (See [6] for an overview and history of MUC6 and the `Named Entity Task&amp;quot;.) The Alembic text processing system applies Eric Brill&apos;s notion of rule sequences [2,3] at almost every one of its processing stages, from part-ofspeech tagging to phrase tagging, and even to some portions of semantic interpretation and inference. While its name indicates its lineage, we do not view the Alembic Workbench as wedded to the Alembic text processing system alone. We intend to provide a well</context>
</contexts>
<marker>[7]</marker>
<rawString>Marc Vilain and David Day. 1996. Finite-state parsing by rule sequences. In International Conference on Computational Linguistics, Copenhagen, Denmark, August. The International Committee on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
</authors>
<title>Validation of terminological inference in an information extraction task.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<location>Plainsboro, New Jersey.</location>
<contexts>
<context position="28097" citStr="[8]" startWordPosition="4368" endWordPosition="4368">rmation extraction systems in multiple languages. 7. Implementation The Alembic Workbench interface has been written in Tc1/11. Some of the analysis and reporting utilities (available from within the interface as well as Unix command-line utilities) are written in Pert, C or Lisp. The separate Alembic NLP system consists of C preprocessing taggers (for dates, word and sentence tokenization and part-of-speech assignments) and a Lisp image that incorporates the rest of Alembic: the phraserule interpreter, the phrase rule learner, and a number of discourse-level inference mechanisms described in [8]. This code currently runs on Sun workstations running Sun OS 4.1.3 and Solaris 2.4 (Sun OS 5.4) and greater; we have begun porting the system to Windows NT/Windows 95. We anticipate providing an API for integrating other NLP systems in the near future. The Workbench reads and saves its work in the form of SGML-encoded files, though the original document need not contain any SGML mark-up at all. These files are parsed with the help of an SGML normalizer.4 During the course of the annotation process the Workbench uses a &amp;quot;Parallel Tag File&amp;quot; (PTF) format, which separates out the embedded annotati</context>
</contexts>
<marker>[8]</marker>
<rawString>Marc Vilain. 1993. Validation of terminological inference in an information extraction task. In Proceedings of the ARPA Workshop on Human Language Technology, Plainsboro, New Jersey.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>