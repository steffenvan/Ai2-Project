<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.998573">
Cross-Language Text Classification
using Structural Correspondence Learning
</title>
<author confidence="0.71809">
Peter Prettenhofer and Benno Stein
</author>
<affiliation confidence="0.433319">
Bauhaus-Universit¨at Weimar
</affiliation>
<address confidence="0.598076">
D-99421 Weimar, Germany
</address>
<email confidence="0.777436">
{peter.prettenhofer,benno.stein}@uni-weimar.de
</email>
<sectionHeader confidence="0.993843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990684210526">
We present a new approach to cross-
language text classification that builds on
structural correspondence learning, a re-
cently proposed theory for domain adap-
tation. The approach uses unlabeled doc-
uments, along with a simple word trans-
lation oracle, in order to induce task-
specific, cross-lingual word correspon-
dences. We report on analyses that reveal
quantitative insights about the use of un-
labeled data and the complexity of inter-
language correspondence modeling.
We conduct experiments in the field
of cross-language sentiment classification,
employing English as source language,
and German, French, and Japanese as tar-
get languages. The results are convincing;
they demonstrate both the robustness and
the competitiveness of the presented ideas.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99844312962963">
This paper deals with cross-language text classifi-
cation problems. The solution of such problems
requires the transfer of classification knowledge
between two languages. Stated precisely: We are
given a text classification task -y in a target lan-
guage T for which no labeled documents are avail-
able. -y may be a spam filtering task, a topic cate-
gorization task, or a sentiment classification task.
In addition, we are given labeled documents for
the identical task in a different source language S.
Such type of cross-language text classification
problems are addressed by constructing a clas-
sifier fS with training documents written in S
and by applying fS to unlabeled documents writ-
ten in T . For the application of fS under lan-
guage T different approaches are current practice:
machine translation of unlabeled documents from
T to S, dictionary-based translation of unlabeled
documents from T to S, or language-independent
concept modeling by means of comparable cor-
pora. The mentioned approaches have their pros
and cons, some of which are discussed below.
Here we propose a different approach to cross-
language text classification which adopts ideas
from the field of multi-task learning (Ando and
Zhang, 2005a). Our approach builds upon struc-
tural correspondence learning, SCL, a recently
proposed theory for domain adaptation in the
field of natural language processing (Blitzer et al.,
2006).
Similar to SCL, our approach induces corre-
spondences among the words from both languages
by means of a small number of so-called pivots. In
our context a pivot is a pair of words, {wS, wT },
from the source language S and the target lan-
guage T, which possess a similar semantics. Test-
ing the occurrence of wS or wT in a set of unla-
beled documents from S and T yields two equiv-
alence classes across these languages: one class
contains the documents where either wS or wT oc-
cur, the other class contains the documents where
neither wS nor wT occur. Ideally, a pivot splits
the set of unlabeled documents with respect to the
semantics that is associated with {wS, wT }. The
correlation between wS or wT and other words w,
w V {wS, wT } is modeled by a linear classifier,
which then is used as a language-independent pre-
dictor for the two equivalence classes. As we will
see, a small number of pivots can capture a suffi-
ciently large part of the correspondences between
S and T in order to (1) construct a cross-lingual
representation and (2) learn a classifier fST for the
task -y that operates on this representation. Several
advantages follow from our approach:
</bodyText>
<listItem confidence="0.99456175">
• Task specificity. The approach exploits the
words’ pragmatics since it considers—during
the pivot selection step—task-specific char-
acteristics of language use.
</listItem>
<page confidence="0.957879">
1118
</page>
<note confidence="0.9372615">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1118–1127,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.785344">
• Efficiency in terms of linguistic resources.
</listItem>
<bodyText confidence="0.964069">
The approach uses unlabeled documents
from both languages along with a small num-
ber (100 - 500) of translated words, instead
of employing a parallel corpus or an exten-
sive bilingual dictionary.
</bodyText>
<listItem confidence="0.613262">
• Efficiency in terms of computing resources.
</listItem>
<bodyText confidence="0.9997725">
The approach solves the classification prob-
lem directly, instead of resorting to a more
general and potentially much harder problem
such as machine translation. Note that the use
of such technology is prohibited in certain sit-
uations (market competitors) or restricted by
environmental constraints (offline situations,
high latency, bandwidth capacity).
Contributions Our contributions to the outlined
field are threefold: First, the identification and uti-
lization of the theory of SCL to cross-language
text classification, which has, to the best of our
knowledge, not been investigated before. Sec-
ond, the further development and adaptation of
SCL towards a technology that is competitive with
the state-of-the-art in cross-language text classifi-
cation. Third, an in-depth analysis with respect
to important hyperparameters such as the ratio
of labeled and unlabeled documents, the number
of pivots, and the optimum dimensionality of the
cross-lingual representation. In this connection we
compile extensive corpora in the languages En-
glish, German, French, and Japanese, and for dif-
ferent sentiment classification tasks.
The paper is organized as follows: Section 2
surveys related work. Section 3 states the termi-
nology for cross-language text classification. Sec-
tion 4 describes our main contribution, a new ap-
proach to cross-language text classification based
on structural correspondence learning. Section 5
presents experimental results in the context of
cross-language sentiment classification.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996442741379311">
Cross-Language Text Classification Bel et al.
(2003) belong to the first who explicitly consid-
ered the problem of cross-language text classi-
fication. Their research, however, is predated
by work in cross-language information retrieval,
CLIR, where similar problems are addressed
(Oard, 1998). Traditional approaches to cross-
language text classification and CLIR use linguis-
tic resources such as bilingual dictionaries or par-
allel corpora to induce correspondences between
two languages (Lavrenko et al., 2002; Olsson et
al., 2005). Dumais et al. (1997) is considered as
seminal work in CLIR: they propose a method
which induces semantic correspondences between
two languages by performing latent semantic anal-
ysis, LSA, on a parallel corpus. Li and Taylor
(2007) improve upon this method by employing
kernel canonical correlation analysis, CCA, in-
stead of LSA. The major limitation of these ap-
proaches is their computational complexity and,
in particular, the dependence on a parallel cor-
pus, which is hard to obtain—especially for less
resource-rich languages. Gliozzo and Strappar-
ava (2005) circumvent the dependence on a par-
allel corpus by using so-called multilingual do-
main models, which can be acquired from com-
parable corpora in an unsupervised manner. In
(Gliozzo and Strapparava, 2006) they show for
particular tasks that their approach can achieve a
performance close to that of monolingual text clas-
sification.
Recent work in cross-language text classifica-
tion focuses on the use of automatic machine
translation technology. Most of these methods in-
volve two steps: (1) translation of the documents
into the source or the target language, and (2) di-
mensionality reduction or semi-supervised learn-
ing to reduce the noise introduced by the ma-
chine translation. Methods which follow this two-
step approach include the EM-based approach by
Rigutini et al. (2005), the CCA approach by For-
tuna and Shawe-Taylor (2005), the information
bottleneck approach by Ling et al. (2008), and the
co-training approach by Wan (2009).
Domain Adaptation Domain adaptation refers
to the problem of adapting a statistical classifier
trained on data from one (or more) source domains
(e.g., newswire texts) to a different target domain
(e.g., legal texts). In the basic domain adaptation
setting we are given labeled data from the source
domain and unlabeled data from the target domain,
and the goal is to train a classifier for the target
domain. Beyond this setting one can further dis-
tinguish whether a small amount of labeled data
from the target domain is available (Daume, 2007;
Finkel and Manning, 2009) or not (Blitzer et al.,
2006; Jiang and Zhai, 2007). The latter setting is
referred to as unsupervised domain adaptation.
</bodyText>
<page confidence="0.994314">
1119
</page>
<bodyText confidence="0.99997412">
Note that, cross-language text classification
can be cast as an unsupervised domain adapta-
tion problem by considering each language as a
separate domain. Blitzer et al. (2006) propose
an effective algorithm for unsupervised domain
adaptation, called structural correspondence learn-
ing. First, SCL identifies features that general-
ize across domains, which the authors call pivots.
SCL then models the correlation between the piv-
ots and all other features by training linear clas-
sifiers on the unlabeled data from both domains.
This information is used to induce correspon-
dences among features from the different domains
and to learn a shared representation that is mean-
ingful across both domains. SCL is related to the
structural learning paradigm introduced by Ando
and Zhang (2005a). The basic idea of structural
learning is to constrain the hypothesis space of a
learning task by considering multiple different but
related tasks on the same input space. Ando and
Zhang (2005b) present a semi-supervised learning
method based on this paradigm, which generates
related tasks from unlabeled data. Quattoni et al.
(2007) apply structural learning to image classifi-
cation in settings where little labeled data is given.
</bodyText>
<sectionHeader confidence="0.944031" genericHeader="method">
3 Cross-Language Text Classification
</sectionHeader>
<bodyText confidence="0.999440590909091">
This section introduces basic models and termi-
nology.
In standard text classification, a document d
is represented under the bag-of-words model as
|V |-dimensional feature vector x E X, where V ,
the vocabulary, denotes an ordered set of words,
xi E x denotes the normalized frequency of word
i in d, and X is an inner product space. DS
denotes the training set and comprises tuples of
the form (x, y), which associate a feature vector
x E X with a class label y E Y . The goal is to
find a classifier f : X —* Y that predicts the la-
bels of new, previously unseen documents. With-
out loss of generality we restrict ourselves to bi-
nary classification problems and linear classifiers,
i.e., Y = {+1, -1} and f(x) = sign(wTx). w is a
weight vector that parameterizes the classifier, [·]T
denotes the matrix transpose. The computation of
w from DS is referred to as model estimation or
training. A common choice for w is given by a
vector w* that minimizes the regularized training
error:
</bodyText>
<equation confidence="0.952348">
1:
w* = argmin L(y, wTx) + A IIwI12 (1)
wER|V  |(x,y)EDS 2
</equation>
<bodyText confidence="0.999589744680851">
L is a loss function that measures the quality
of the classifier, A is a non-negative regulariza-
tion parameter that penalizes model complexity,
and �w�2 = wTw. Different choices for L entail
different classifier types; e.g., when choosing the
hinge loss function for L one obtains the popular
Support Vector Machine classifier (Zhang, 2004).
Standard text classification distinguishes be-
tween labeled (training) documents and unlabeled
(test) documents. Cross-language text classifica-
tion poses an extra constraint in that training doc-
uments and test documents are written in different
languages. Here, the language of the training doc-
uments is referred to as source language S, and
the language of the test documents is referred to as
target language T . The vocabulary V divides into
VS and VT , called vocabulary of the source lan-
guage and vocabulary of the target language, with
VS n VT = 0. I.e., documents from the training
set and the test set map on two non-overlapping
regions of the feature space. Thus, a linear classi-
fier fS trained on DS associates non-zero weights
only with words from VS, which in turn means that
fS cannot be used to classify documents written
in T .
One way to overcome this “feature barrier” is
to find a cross-lingual representation for docu-
ments written in S and T , which enables the trans-
fer of classification knowledge between the two
languages. Intuitively, one can understand such
a cross-lingual representation as a concept space
that underlies both languages. In the following,
we will use 0 to denote a map that associates the
original |V |-dimensional representation of a doc-
ument d written in S or T with its cross-lingual
representation. Once such a mapping is found the
cross-language text classification problem reduces
to a standard classification problem in the cross-
lingual space. Note that the existing methods for
cross-language text classification can be character-
ized by the way 0 is constructed. For instance,
cross-language latent semantic indexing (Dumais
et al., 1997) and cross-language explicit semantic
analysis (Potthast et al., 2008) estimate 0 using a
parallel corpus. Other methods use linguistic re-
sources such as a bilingual dictionary to obtain 0
(Bel et al., 2003; Olsson et al., 2005).
</bodyText>
<page confidence="0.981638">
1120
</page>
<sectionHeader confidence="0.975375" genericHeader="method">
4 Cross-Language
</sectionHeader>
<subsectionHeader confidence="0.972541">
Structural Correspondence Learning
</subsectionHeader>
<bodyText confidence="0.999888166666667">
We now present a novel method for learning a
map B by exploiting relations from unlabeled doc-
uments written in S and T . The proposed method,
which we call cross-language structural corre-
spondence learning, CL-SCL, addresses the fol-
lowing learning setup (see also Figure 1):
</bodyText>
<listItem confidence="0.835499904761905">
• Given a set of labeled training documents DS
written in language S, the goal is to create a
text classifier for documents written in a dif-
ferent language T . We refer to this classifi-
cation task as the target task. An example for
the target task is the determination of senti-
ment polarity, either positive or negative, of
book reviews written in German (T) given a
set of training reviews written in English (S).
• In addition to the labeled training docu-
ments DS we have access to unlabeled doc-
uments DS,u and DT ,u from both languages
S and T . Let Du denote DS,u U DT ,u.
• Finally, we are given a budget of calls to a
word translation oracle (e.g., a domain ex-
pert) to map words in the source vocabu-
lary VS to their corresponding translations in
the target vocabulary VT . For simplicity and
without loss of applicability we assume here
that the word translation oracle maps each
word in VS to exactly one word in VT .
</listItem>
<bodyText confidence="0.999165411764706">
CL-SCL comprises three steps: In the first step,
CL-SCL selects word pairs {wS, wT }, called piv-
ots, where wS E VS and wT E VT . Pivots have to
satisfy the following conditions:
Confidence Both words, wS and wT , are predic-
tive for the target task.
Support Both words, wS and wT , occur fre-
quently in DS,u and DT ,u respectively.
The confidence condition ensures that, in the
second step of CL-SCL, only those correlations
are modeled that are useful for discriminative
learning. The support condition, on the other
hand, ensures that these correlations can be es-
timated accurately. Considering our sentiment
classification example, the word pair {excellentS,
exzellentT } satisfies both conditions: (1) the
words are strong indicators of positive sentiment,
</bodyText>
<figureCaption confidence="0.892758">
Figure 1: The document sets underlying CL-SCL.
</figureCaption>
<bodyText confidence="0.959593444444445">
The subscripts S, T , and u designate “source lan-
guage”, “target language”, and “unlabeled”.
and (2) the words occur frequently in book reviews
from both languages. Note that the support of wS
and wT can be determined from the unlabeled data
Du. The confidence, however, can only be deter-
mined for wS since the setting gives us access to
labeled data from S only.
We use the following heuristic to form an or-
dered set P of pivots: First, we choose a subset
VP from the source vocabulary VS, |VP  |« |VS|,
which contains those words with the highest mu-
tual information with respect to the class label of
the target task in DS. Second, for each word
wS E VP we find its translation in the target vo-
cabulary VT by querying the translation oracle; we
refer to the resulting set of word pairs as the can-
didate pivots, P&apos; :
</bodyText>
<equation confidence="0.754043">
P&apos; = {{wS, TRANSLATE(wS)}  |wS E VP}
</equation>
<bodyText confidence="0.998108">
We then enforce the support condition by elim-
inating in P&apos; all candidate pivots {wS, wT } where
the document frequency of wS in DS,u or of wT
in DT ,u is smaller than some threshold 0:
</bodyText>
<equation confidence="0.986236">
P = CANDIDATEELIMINATION(P&apos;, cp)
</equation>
<bodyText confidence="0.999928875">
Let m denote |P|, the number of pivots.
In the second step, CL-SCL models the corre-
lations between each pivot {wS, wT } E P and all
other words w E V \ {wS, wT }. This is done by
training linear classifiers that predict whether or
not wS or wT occur in a document, based on the
other words. For this purpose a training set Dl is
created for each pivot pl E P :
</bodyText>
<equation confidence="0.631957">
Dl = {(MASK(x, pl), IN(x, pl))  |x E Du}
X = (x1 , ...
</equation>
<figure confidence="0.77073725">
... , x|V|)
y
term frequencies
Positive class label
Words in VS Words in VT Class
label
DS
Du
DS,u
DT,u
No value
Negative class label
</figure>
<page confidence="0.907889">
1121
</page>
<equation confidence="0.724142">
MASK(x, pl) is a function that returns a copy of
</equation>
<bodyText confidence="0.995374411764706">
x where the components associated with the two
words in pl are set to zero—which is equivalent
to removing these words from the feature space.
IN(x, pl) returns +1 if one of the components of x
associated with the words in pl is non-zero and -1
otherwise. For each Dl a linear classifier, charac-
terized by the parameter vector wl, is trained by
minimizing Equation (1) on Dl. Note that each
training set Dl contains documents from both lan-
guages. Thus, for a pivot pl = {wS, wT } the vec-
tor wl captures both the correlation between wS
and VS \ {wS} and the correlation between wT
and VT \ {wT }.
In the third step, CL-SCL identifies correlations
across pivots by computing the singular value de-
composition of the |V |×m-dimensional parameter
matrix W, W = [w1 ... w��:
</bodyText>
<equation confidence="0.923696">
UΣVT = SVD(W)
</equation>
<bodyText confidence="0.998446444444444">
Recall that W encodes the correlation structure
between pivot and non-pivot words in the form
of multiple linear classifiers. Thus, the columns
of U identify common substructures among these
classifiers. Choosing the columns of U associated
with the largest singular values yields those sub-
structures that capture most of the correlation in
W. We define B as those columns of U that are
associated with the k largest singular values:
</bodyText>
<equation confidence="0.995687">
B = UT
[1:k, 1:|V |]
</equation>
<bodyText confidence="0.9998204">
Algorithm 1 summarizes the three steps of CL-
SCL. At training and test time, we apply the pro-
jection B to each input instance x. The vector v*
that minimizes the regularized training error for
DS in the projected space is defined as follows:
</bodyText>
<equation confidence="0.913896333333333">
�
v* = argmin L(y, vT Bx) + A k v k2 (2)
vERk (x,y)EDS 2
</equation>
<bodyText confidence="0.996662">
The resulting classifier fST , which will operate
in the cross-lingual setting, is defined as follows:
</bodyText>
<equation confidence="0.648684">
fST (x) = sign(v*T Bx)
</equation>
<subsectionHeader confidence="0.990161">
4.1 An Alternative View of CL-SCL
</subsectionHeader>
<bodyText confidence="0.99982025">
An alternative view of cross-language structural
correspondence learning is provided by the frame-
work of structural learning (Ando and Zhang,
2005a). The basic idea of structural learning is
</bodyText>
<equation confidence="0.939907142857143">
Algorithm 1 CL-SCL
Input: Labeled source data DS
Unlabeled data Du = DS,u ∪ DT,u
Parameters: m, k, A, and 0
Output: k × |V |-dimensional matrix B
1. SELECTPIVOTS(DS, m)
VP = MUTUALINFORMATION(DS)
PI = {{wS, TRANSLATE(wS)}  |wS ∈ VP}
P = CANDIDATEELIMINATION(PI, 0)
2. TRAINPIVOTPREDICTORS(Du, P)
for l = 1 to m do
Dl = {(MASK(x, pl), IN(x, pl))  |x ∈ Du}
wl = argmin E L(y,wTx))+ �2kwk2
wERIVI (x,y)EDl
end for
W = [w1 ... w.. �
3. COMPUTESVD(W, k)
UΣVT = SVD(W)
T
B _ — U[1:k, 1:JVJ]
output {B}
</equation>
<bodyText confidence="0.999987833333333">
to constrain the hypothesis space, i.e., the space of
possible weight vectors, of the target task by con-
sidering multiple different but related prediction
tasks. In our context these auxiliary tasks are rep-
resented by the pivot predictors, i.e., the columns
of W. Each column vector wl can be considered
as a linear classifier which performs well in both
languages. I.e., we regard the column space of W
as an approximation to the subspace of bilingual
classifiers. By computing SVD(W) one obtains
a compact representation of this column space in
the form of an orthonormal basis BT.
The subspace is used to constrain the learning of
the target task by restricting the weight vector w to
lie in the subspace defined by BT. Following Ando
and Zhang (2005a) and Quattoni et al. (2007) we
choose w for the target task to be w* = BT v*,
where v* is defined as follows:
</bodyText>
<equation confidence="0.969693666666667">
�
v� = argmin L(y, (BTv)Tx) + A kvk2 (3)
vERk (x,y)EDS 2
</equation>
<bodyText confidence="0.995616">
Since (BT v)T = vT B it follows that this view
of CL-SCL corresponds to the induction of a new
feature space given by Equation 2.
</bodyText>
<page confidence="0.989352">
1122
</page>
<figure confidence="0.361346">
5 Experiments MeCab is used for Japanese word segmentation.2
</figure>
<bodyText confidence="0.999757888888889">
We evaluate CL-SCL for the task of cross-
language sentiment classification using English
as source language and German, French, and
Japanese as target languages. Special emphasis is
put on corpus construction, determination of upper
bounds and baselines, and a sensitivity analysis of
important hyperparameters. All data described in
the following is publicly available from our project
website.1
</bodyText>
<subsectionHeader confidence="0.993356">
5.1 Dataset and Preprocessing
</subsectionHeader>
<bodyText confidence="0.98049272972973">
We compiled a new dataset for cross-language
sentiment classification by crawling product re-
views from Amazon.{de  |fr  |co.jp}. The crawled
part of the corpus contains more than 4 million
reviews in the three languages German, French,
and Japanese. The corpus is extended with En-
glish product reviews provided by Blitzer et al.
(2007). Each review contains a category label,
a title, the review text, and a rating of 1-5 stars.
Following Blitzer et al. (2007) a review with &gt;3
(&lt;3) stars is labeled as positive (negative); other
reviews are discarded. For each language the la-
beled reviews are grouped according to their cate-
gory label, whereas we restrict our experiments to
three categories: books, dvds, and music.
Since most of the crawled reviews are posi-
tive (80%), we decide to balance the number of
positive and negative reviews. In this study, we
are interested in whether the cross-lingual repre-
sentation induced by CL-SCL captures the differ-
ence between positive and negative reviews; by
balancing the reviews we ensure that the imbal-
ance does not affect the learned model. Balancing
is achieved by deleting reviews from the major-
ity class uniformly at random for each language-
specific category. The resulting sets are split into
three disjoint, balanced sets, containing training
documents, test documents, and unlabeled docu-
ments; the respective set sizes are 2,000, 2,000,
and 9,000-50,000. See Table 1 for details.
For each of the nine target-language-category-
combinations a text classification task is created
by taking the training set of the product category in
S and the test set of the same product category in
T . A document d is described as normalized fea-
ture vector x under a unigram bag-of-words docu-
ment representation. The morphological analyzer
</bodyText>
<footnote confidence="0.997464">
1http://www.webis.de/research/corpora/
webis-cls-10/
</footnote>
<subsectionHeader confidence="0.993077">
5.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999983441860465">
Throughout the experiments linear classifiers are
employed; they are trained by minimizing Equa-
tion (1), using a stochastic gradient descent (SGD)
algorithm. In particular, the learning rate schedule
from PEGASOS is adopted (Shalev-Shwartz et al.,
2007), and the modified Huber loss, introduced by
Zhang (2004), is chosen as loss function L.3
SGD receives two hyperparameters as input: the
number of iterations T, and the regularization pa-
rameter A. In our experiments T is always set to
106, which is about the number of iterations re-
quired for SGD to converge. For the target task,
A is determined by 3-fold cross-validation, testing
for A all values 10−�, i E [0; 6]. For the pivot pre-
diction task, A is set to the small value of 10−5, in
order to favor model accuracy over generalizabil-
ity.
The computational bottleneck of CL-SCL is the
SVD of the dense parameter matrix W. Here we
follow Blitzer et al. (2006) and set the negative
values in W to zero, which yields a sparse repre-
sentation. For the SVD computation the Lanczos
algorithm provided by SVDLIBC is employed.4
We investigated an alternative approach to obtain
a sparse W by directly enforcing sparse pivot pre-
dictors wl through L1-regularization (Tsuruoka et
al., 2009), but didn’t pursue this strategy due to
unstable results. Since SGD is sensitive to fea-
ture scaling the projection Ox is post-processed as
follows: (1) Each feature of the cross-lingual rep-
resentation is standardized to zero mean and unit
variance, where mean and variance are estimated
on DS U D, (2) The cross-lingual document rep-
resentations are scaled by a constant α such that
|DS|−&apos; ExEDg I1αex1I = 1.
We use Google Translate as word translation or-
acle, which returns a single translation for each
query word.5 Though such a context free transla-
tion is suboptimum we do not sanitize the returned
words to demonstrate the robustness of CL-SCL
with respect to translation noise. To ensure the re-
producibility of our results we cache all queries to
the translation oracle.
</bodyText>
<footnote confidence="0.9989424">
2http://mecab.sourceforge.net
3Our implementation is available at http://github.
com/pprett/bolt
4http://tedlab.mit.edu/˜dr/SVDLIBC/
5http://translate.google.com
</footnote>
<page confidence="0.807115">
1123
</page>
<table confidence="0.999177181818182">
T Category Unlabeled data Upper Bound CL-MT CL-SCL
|DS,v. ||DT ,v. |µ v µ v A µ v A
books 50,000 50,000 83.79 (±0.20) 79.68 (±0.13) 4.11 79.50 (±0.33) 4.29
German dvd 30,000 50,000 81.78 (±0.27) 77.92 (±0.25) 3.86 76.92 (±0.07) 4.86
music 25,000 50,000 82.80 (±0.13) 77.22 (±0.23) 5.58 77.79 (±0.02) 5.00
books 50,000 32,000 83.92 (±0.14) 80.76 (±0.34) 3.16 78.49 (±0.03) 5.43
French dvd 30,000 9,000 83.40 (±0.28) 78.83 (±0.19) 4.57 78.80 (±0.01) 4.60
music 25,000 16,000 86.09 (±0.13) 75.78 (±0.65) 10.31 77.92 (±0.03) 8.17
books 50,000 50,000 79.39 (±0.27) 70.22 (±0.27) 9.17 73.09 (±0.07) 6.30
Japanese dvd 30,000 50,000 81.56 (±0.28) 71.30 (±0.28) 10.26 71.07 (±0.02) 10.49
music 25,000 50,000 82.33 (±0.13) 72.02 (±0.29) 10.31 75.11 (±0.06) 7.22
</table>
<tableCaption confidence="0.899492">
Table 1: Cross-language sentiment classification results. For each task, the number of unlabeled docu-
ments from S and T is given. Accuracy scores (mean p and standard deviation Q of 10 repetitions of
SGD) on the test set of the target language T are reported. A gives the difference in accuracy to the
upper bound. CL-SCL uses m = 450, k = 100, and 0 = 30.
</tableCaption>
<subsectionHeader confidence="0.998413">
5.3 Upper Bound and Baseline
</subsectionHeader>
<bodyText confidence="0.99999255">
To get an upper bound on the performance of
a cross-language method we first consider the
monolingual setting. For each target-language-
category-combination a linear classifier is learned
on the training set and tested on the test set. The
resulting accuracy scores are referred to as upper
bound; it informs us about the expected perfor-
mance on the target task if training data in the tar-
get language is available.
We chose a machine translation baseline
to compare CL-SCL to another cross-language
method. Statistical machine translation technol-
ogy offers a straightforward solution to the prob-
lem of cross-language text classification and has
been used in a number of cross-language senti-
ment classification studies (Hiroshi et al., 2004;
Bautin et al., 2008; Wan, 2009). Our baseline
CL-MT works as follows: (1) learn a linear clas-
sifier on the training data, and (2) translate the test
documents into the source language,6 (3) predict
</bodyText>
<footnote confidence="0.400102">
6Again we use Google Translate.
</footnote>
<bodyText confidence="0.998149666666667">
the sentiment polarity of the translated test doc-
uments. Note that the baseline CL-MT does not
make use of unlabeled documents.
</bodyText>
<subsectionHeader confidence="0.99967">
5.4 Performance Results and Sensitivity
</subsectionHeader>
<bodyText confidence="0.999794111111111">
Table 1 contrasts the classification performance of
CL-SCL with the upper bound and with the base-
line. Observe that the upper bound does not ex-
hibit a great variability across the three languages.
The average accuracy is about 82%, which is con-
sistent with prior work on monolingual sentiment
analysis (Pang et al., 2002; Blitzer et al., 2007).
The performance of CL-MT, however, differs con-
siderably between the two European languages
and Japanese: for Japanese, the average difference
between the upper bound and CL-MT (9.9%) is
about twice as much as for German and French
(5.3%). This difference can be explained by the
fact that machine translation works better for Eu-
ropean than for Asian languages such as Japanese.
Recall that CL-SCL receives three hyperparam-
eters as input: the number of pivots m, the di-
mensionality of the cross-lingual representation k,
</bodyText>
<subsectionHeader confidence="0.828459">
English German
Pivot Semantics Pragmatics Semantics Pragmatics
</subsectionHeader>
<bodyText confidence="0.6545496">
{beautifulS, sch¨onT } amazing, beauty, picture, pattern, poetry, sch¨oner (more beautiful), bilder (pictures),
lovely photographs, paintings traurig (sad) illustriert (illustrated)
{boringS, langweiligT } plain, asleep, characters, pages, langatmig (lengthy), charaktere (characters),
dry, long story einfach (plain), handlung (plot),
entt¨auscht (disappointed) seiten (pages)
</bodyText>
<tableCaption confidence="0.9764395">
Table 2: Semantic and pragmatic correlations identified for the two pivots {beautifulS, sch¨onT } and
{boringS, langweiligT } in English and German book reviews.
</tableCaption>
<page confidence="0.994641">
1124
</page>
<figureCaption confidence="0.789794">
Figure 2: Influence of unlabeled data and hyperparameters on the performance of CL-SCL. The rows
show the performance of CL-SCL as a function of (1) the ratio between labeled and unlabeled documents,
(2) the number of pivots m, and (3) the dimensionality of the cross-lingual representation k.
</figureCaption>
<bodyText confidence="0.999407952380952">
and the minimum support 0 of a pivot in DS,u
and DT ,u. For comparison purposes we use fixed
values of m = 450, k = 100, and 0 = 30.
The results show the competitiveness of CL-SCL
compared to CL-MT. Although CL-MT outper-
forms CL-SCL on most tasks for German and
French, the difference in accuracy can be consid-
ered as small (&lt;1%); merely for French book and
music reviews the difference is about 2%. For
Japanese, however, CL-SCL outperforms CL-MT
on most tasks with a difference in accuracy of
about 3%. The results indicate that if the dif-
ference between the upper bound and CL-MT is
large, CL-SCL can circumvent the loss in accu-
racy. Experiments with language-specific settings
revealed that for Japanese a smaller number of piv-
ots (150&lt;m&lt;250) performs significantly better.
Thus, the reported results for Japanese can be con-
sidered as pessimistic.
Primarily responsible for the effectiveness of
CL-SCL is its task specificity, i.e., the ways in
which context contributes to meaning (pragmat-
ics). Due to the use of task-specific, unlabeled
data, relevant characteristics are captured by the
pivot classifiers. Table 2 exemplifies this with two
pivots for German book reviews. The rows of the
table show those words which have the highest
correlation with the pivots {beautifulS, sch¨onT }
and {boringS, langweiligT }. We can distinguish
between (1) correlations that reflect similar mean-
ing, such as “amazing”, “lovely”, or “plain”, and
(2) correlations that reflect the pivot pragmatics
with respect to the task, such as “picture”, “po-
etry”, or “pages”. Note in this connection that au-
thors of book reviews tend to use the word “beau-
tiful” to refer to illustrations or poetry. While the
first type of word correlations can be obtained by
methods that operate on parallel corpora, the sec-
ond type of correlation requires an understanding
of the task-specific language use.
In the following we discuss the sensitivity of
each hyperparameter in isolation while keeping
</bodyText>
<page confidence="0.973216">
1125
</page>
<bodyText confidence="0.998694333333333">
the others fixed at m = 450, k = 100, and 0 = 30.
The experiments are illustrated in Figure 2.
Unlabeled Data The first row of Figure 2 shows
the performance of CL-SCL as a function of the
ratio of labeled and unlabeled documents. A ratio
of 1 means that |DS,. |= |DT ,. |= 2,000, while
a ratio of 25 corresponds to the setting of Table 1.
As expected, an increase in unlabeled documents
results in an improved performance, however, we
observe a saturation at a ratio of 10 across all nine
tasks.
Number of Pivots The second row shows the in-
fluence of the number of pivots m on the perfor-
mance of CL-SCL. Compared to the size of the
vocabularies VS and VT , which is in 105 order
of magnitude, the number of pivots is very small.
The plots show that even a small number of piv-
ots captures a significant amount of the correspon-
dence between S and T .
Dimensionality of the Cross-Lingual Represen-
tation The third row shows the influence of the
dimensionality of the cross-lingual representation
k on the performance of CL-SCL. Obviously the
SVD is crucial to the success of CL-SCL if m
is sufficiently large. Observe that the value of k
is task-insensitive: a value of 75&lt;k&lt;150 works
equally well across all tasks.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999996578947369">
The paper introduces a novel approach to cross-
language text classification, called cross-language
structural correspondence learning. The approach
uses unlabeled documents along with a word
translation oracle to automatically induce task-
specific, cross-lingual correspondences. Our con-
tributions include the adaptation of SCL for the
problem of cross-language text classification and
a well-founded empirical analysis. The analy-
sis covers performance and robustness issues in
the context of cross-language sentiment classifica-
tion with English as source language and German,
French, and Japanese as target languages. The re-
sults show that CL-SCL is competitive with state-
of-the-art machine translation technology while
requiring fewer resources.
Future work includes the extension of CL-SCL
towards a general approach for cross-lingual adap-
tation of natural language processing technology.
</bodyText>
<sectionHeader confidence="0.994653" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996935905660378">
Rie-K. Ando and Tong Zhang. 2005a. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res., 6:1817–
1853.
Rie-K. Ando and Tong Zhang. 2005b. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of ACL-05, pages 1–
9, Ann Arbor.
Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena.
2008. International sentiment analysis for news and
blogs. In Proceedings of ICWSM-08, pages 19–26,
Seattle.
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization. In Proceed-
ings of ECDL-03, pages 126–139, Trondheim.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural corre-
spondence learning. In Proceedings of EMNLP-06,
pages 120–128, Sydney.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL-07, pages 440–447,
Prague.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL-07, pages 256–263,
Prague.
Susan T. Dumais, Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic cross-language retrieval using latent semantic
indexing. In AAAI Symposium on CrossLanguage
Text and Speech Retrieval.
Jenny-R. Finkel and Christopher-D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of HLT/NAACL-09, pages 602–610, Boul-
der.
Blaˇz Fortuna and John Shawe-Taylor. 2005. The use
of machine translation tools for cross-lingual text
mining. In Proceedings of the ICML Workshop on
Learning with Multiple Views.
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage text categorization by acquiring multilingual
domain models from comparable corpora. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts.
Alfio Gliozzo and Carlo Strapparava. 2006. Exploit-
ing comparable corpora and bilingual dictionaries
for cross-language text categorization. In Proceed-
ings of ACL-06, pages 553–560, Sydney.
Kanayama Hiroshi, Nasukawa Tetsuya, and Watanabe
Hideo. 2004. Deeper sentiment analysis using
machine translation technology. In Proceedings of
COLING-04, pages 494–500, Geneva.
</reference>
<page confidence="0.867035">
1126
</page>
<reference confidence="0.999666148148148">
Jing Jiang and Chengxiang Zhai. 2007. A two-stage
approach to domain adaptation for statistical classi-
fiers. In Proceedings of CIKM-07, pages 401–410,
Lisbon.
Victor Lavrenko, Martin Choquette, and W. Bruce
Croft. 2002. Cross-lingual relevance models. In
Proceedings of SIGIR-02, pages 175–182, Tampere.
Yaoyong Li and John S. Taylor. 2007. Advanced
learning algorithms for cross-language patent re-
trieval and classification. Inf. Process. Manage.,
43(5):1183–1199.
Xiao Ling, Gui-R. Xue, Wenyuan Dai, Yun Jiang,
Qiang Yang, and Yong Yu. 2008. Can chinese web
pages be classified with english data source? In Pro-
ceedings of WWW-08, pages 969–978, Beijing.
Douglas W. Oard. 1998. A comparative study of query
and document translation for cross-language infor-
mation retrieval. In Proceedings ofAMTA-98, pages
472–483, Langhorne.
J. Scott Olsson, Douglas W. Oard, and Jan Hajiˇc. 2005.
Cross-language text classification. In Proceedings
of SIGIR-05, pages 645–646, Salvador.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP-02, pages 79–86, Philadelphia.
Martin Potthast, Benno Stein, and Maik Anderka.
2008. A wikipedia-based multilingual retrieval
model. In Proceedings of ECIR-08, pages 522–530,
Glasgow.
Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2007. Learning visual representations using images
with captions. In Proceedings of CVPR-07, pages
1–8, Minneapolis.
Leonardo Rigutini, Marco Maggini, and Bing Liu.
2005. An em based training algorithm for cross-
language text categorization. In Proceedings of WI-
05, pages 529–535, Compi`egne.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal estimated sub-gradient
solver for svm. In Proceedings of ICML-07, pages
807–814, Corvalis.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proceedings of ACL/AFNLP-09, pages
477–485, Singapore.
Xiaojun Wan. 2009. Co-training for cross-
lingual sentiment classification. In Proceedings of
ACL/AFNLP-09, pages 235–243, Singapore.
Tong Zhang. 2004. Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. In Proceedings of ICML-04, pages 116–
124, Banff.
</reference>
<page confidence="0.995203">
1127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.742916">
<title confidence="0.9986775">Cross-Language Text Classification using Structural Correspondence Learning</title>
<author confidence="0.905437">Prettenhofer Stein</author>
<affiliation confidence="0.874931">Bauhaus-Universit¨at Weimar</affiliation>
<address confidence="0.999229">D-99421 Weimar, Germany</address>
<abstract confidence="0.99417785">We present a new approach to crosslanguage text classification that builds on structural correspondence learning, a recently proposed theory for domain adaptation. The approach uses unlabeled documents, along with a simple word translation oracle, in order to induce taskspecific, cross-lingual word correspondences. We report on analyses that reveal quantitative insights about the use of unlabeled data and the complexity of interlanguage correspondence modeling. We conduct experiments in the field of cross-language sentiment classification, employing English as source language, and German, French, and Japanese as target languages. The results are convincing; they demonstrate both the robustness and the competitiveness of the presented ideas.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie-K Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>6</volume>
<pages>1853</pages>
<contexts>
<context position="2199" citStr="Ando and Zhang, 2005" startWordPosition="323" endWordPosition="326">fier fS with training documents written in S and by applying fS to unlabeled documents written in T . For the application of fS under language T different approaches are current practice: machine translation of unlabeled documents from T to S, dictionary-based translation of unlabeled documents from T to S, or language-independent concept modeling by means of comparable corpora. The mentioned approaches have their pros and cons, some of which are discussed below. Here we propose a different approach to crosslanguage text classification which adopts ideas from the field of multi-task learning (Ando and Zhang, 2005a). Our approach builds upon structural correspondence learning, SCL, a recently proposed theory for domain adaptation in the field of natural language processing (Blitzer et al., 2006). Similar to SCL, our approach induces correspondences among the words from both languages by means of a small number of so-called pivots. In our context a pivot is a pair of words, {wS, wT }, from the source language S and the target language T, which possess a similar semantics. Testing the occurrence of wS or wT in a set of unlabeled documents from S and T yields two equivalence classes across these languages</context>
<context position="9216" citStr="Ando and Zhang (2005" startWordPosition="1417" endWordPosition="1420"> et al. (2006) propose an effective algorithm for unsupervised domain adaptation, called structural correspondence learning. First, SCL identifies features that generalize across domains, which the authors call pivots. SCL then models the correlation between the pivots and all other features by training linear classifiers on the unlabeled data from both domains. This information is used to induce correspondences among features from the different domains and to learn a shared representation that is meaningful across both domains. SCL is related to the structural learning paradigm introduced by Ando and Zhang (2005a). The basic idea of structural learning is to constrain the hypothesis space of a learning task by considering multiple different but related tasks on the same input space. Ando and Zhang (2005b) present a semi-supervised learning method based on this paradigm, which generates related tasks from unlabeled data. Quattoni et al. (2007) apply structural learning to image classification in settings where little labeled data is given. 3 Cross-Language Text Classification This section introduces basic models and terminology. In standard text classification, a document d is represented under the ba</context>
<context position="18569" citStr="Ando and Zhang, 2005" startWordPosition="3065" endWordPosition="3068">T [1:k, 1:|V |] Algorithm 1 summarizes the three steps of CLSCL. At training and test time, we apply the projection B to each input instance x. The vector v* that minimizes the regularized training error for DS in the projected space is defined as follows: � v* = argmin L(y, vT Bx) + A k v k2 (2) vERk (x,y)EDS 2 The resulting classifier fST , which will operate in the cross-lingual setting, is defined as follows: fST (x) = sign(v*T Bx) 4.1 An Alternative View of CL-SCL An alternative view of cross-language structural correspondence learning is provided by the framework of structural learning (Ando and Zhang, 2005a). The basic idea of structural learning is Algorithm 1 CL-SCL Input: Labeled source data DS Unlabeled data Du = DS,u ∪ DT,u Parameters: m, k, A, and 0 Output: k × |V |-dimensional matrix B 1. SELECTPIVOTS(DS, m) VP = MUTUALINFORMATION(DS) PI = {{wS, TRANSLATE(wS)} |wS ∈ VP} P = CANDIDATEELIMINATION(PI, 0) 2. TRAINPIVOTPREDICTORS(Du, P) for l = 1 to m do Dl = {(MASK(x, pl), IN(x, pl)) |x ∈ Du} wl = argmin E L(y,wTx))+ �2kwk2 wERIVI (x,y)EDl end for W = [w1 ... w.. � 3. COMPUTESVD(W, k) UΣVT = SVD(W) T B _ — U[1:k, 1:JVJ] output {B} to constrain the hypothesis space, i.e., the space of possibl</context>
<context position="19861" citStr="Ando and Zhang (2005" startWordPosition="3294" endWordPosition="3297">ut related prediction tasks. In our context these auxiliary tasks are represented by the pivot predictors, i.e., the columns of W. Each column vector wl can be considered as a linear classifier which performs well in both languages. I.e., we regard the column space of W as an approximation to the subspace of bilingual classifiers. By computing SVD(W) one obtains a compact representation of this column space in the form of an orthonormal basis BT. The subspace is used to constrain the learning of the target task by restricting the weight vector w to lie in the subspace defined by BT. Following Ando and Zhang (2005a) and Quattoni et al. (2007) we choose w for the target task to be w* = BT v*, where v* is defined as follows: � v� = argmin L(y, (BTv)Tx) + A kvk2 (3) vERk (x,y)EDS 2 Since (BT v)T = vT B it follows that this view of CL-SCL corresponds to the induction of a new feature space given by Equation 2. 1122 5 Experiments MeCab is used for Japanese word segmentation.2 We evaluate CL-SCL for the task of crosslanguage sentiment classification using English as source language and German, French, and Japanese as target languages. Special emphasis is put on corpus construction, determination of upper bou</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie-K. Ando and Tong Zhang. 2005a. A framework for learning predictive structures from multiple tasks and unlabeled data. J. Mach. Learn. Res., 6:1817– 1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie-K Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A highperformance semi-supervised learning method for text chunking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<location>Ann Arbor.</location>
<contexts>
<context position="2199" citStr="Ando and Zhang, 2005" startWordPosition="323" endWordPosition="326">fier fS with training documents written in S and by applying fS to unlabeled documents written in T . For the application of fS under language T different approaches are current practice: machine translation of unlabeled documents from T to S, dictionary-based translation of unlabeled documents from T to S, or language-independent concept modeling by means of comparable corpora. The mentioned approaches have their pros and cons, some of which are discussed below. Here we propose a different approach to crosslanguage text classification which adopts ideas from the field of multi-task learning (Ando and Zhang, 2005a). Our approach builds upon structural correspondence learning, SCL, a recently proposed theory for domain adaptation in the field of natural language processing (Blitzer et al., 2006). Similar to SCL, our approach induces correspondences among the words from both languages by means of a small number of so-called pivots. In our context a pivot is a pair of words, {wS, wT }, from the source language S and the target language T, which possess a similar semantics. Testing the occurrence of wS or wT in a set of unlabeled documents from S and T yields two equivalence classes across these languages</context>
<context position="9216" citStr="Ando and Zhang (2005" startWordPosition="1417" endWordPosition="1420"> et al. (2006) propose an effective algorithm for unsupervised domain adaptation, called structural correspondence learning. First, SCL identifies features that generalize across domains, which the authors call pivots. SCL then models the correlation between the pivots and all other features by training linear classifiers on the unlabeled data from both domains. This information is used to induce correspondences among features from the different domains and to learn a shared representation that is meaningful across both domains. SCL is related to the structural learning paradigm introduced by Ando and Zhang (2005a). The basic idea of structural learning is to constrain the hypothesis space of a learning task by considering multiple different but related tasks on the same input space. Ando and Zhang (2005b) present a semi-supervised learning method based on this paradigm, which generates related tasks from unlabeled data. Quattoni et al. (2007) apply structural learning to image classification in settings where little labeled data is given. 3 Cross-Language Text Classification This section introduces basic models and terminology. In standard text classification, a document d is represented under the ba</context>
<context position="18569" citStr="Ando and Zhang, 2005" startWordPosition="3065" endWordPosition="3068">T [1:k, 1:|V |] Algorithm 1 summarizes the three steps of CLSCL. At training and test time, we apply the projection B to each input instance x. The vector v* that minimizes the regularized training error for DS in the projected space is defined as follows: � v* = argmin L(y, vT Bx) + A k v k2 (2) vERk (x,y)EDS 2 The resulting classifier fST , which will operate in the cross-lingual setting, is defined as follows: fST (x) = sign(v*T Bx) 4.1 An Alternative View of CL-SCL An alternative view of cross-language structural correspondence learning is provided by the framework of structural learning (Ando and Zhang, 2005a). The basic idea of structural learning is Algorithm 1 CL-SCL Input: Labeled source data DS Unlabeled data Du = DS,u ∪ DT,u Parameters: m, k, A, and 0 Output: k × |V |-dimensional matrix B 1. SELECTPIVOTS(DS, m) VP = MUTUALINFORMATION(DS) PI = {{wS, TRANSLATE(wS)} |wS ∈ VP} P = CANDIDATEELIMINATION(PI, 0) 2. TRAINPIVOTPREDICTORS(Du, P) for l = 1 to m do Dl = {(MASK(x, pl), IN(x, pl)) |x ∈ Du} wl = argmin E L(y,wTx))+ �2kwk2 wERIVI (x,y)EDl end for W = [w1 ... w.. � 3. COMPUTESVD(W, k) UΣVT = SVD(W) T B _ — U[1:k, 1:JVJ] output {B} to constrain the hypothesis space, i.e., the space of possibl</context>
<context position="19861" citStr="Ando and Zhang (2005" startWordPosition="3294" endWordPosition="3297">ut related prediction tasks. In our context these auxiliary tasks are represented by the pivot predictors, i.e., the columns of W. Each column vector wl can be considered as a linear classifier which performs well in both languages. I.e., we regard the column space of W as an approximation to the subspace of bilingual classifiers. By computing SVD(W) one obtains a compact representation of this column space in the form of an orthonormal basis BT. The subspace is used to constrain the learning of the target task by restricting the weight vector w to lie in the subspace defined by BT. Following Ando and Zhang (2005a) and Quattoni et al. (2007) we choose w for the target task to be w* = BT v*, where v* is defined as follows: � v� = argmin L(y, (BTv)Tx) + A kvk2 (3) vERk (x,y)EDS 2 Since (BT v)T = vT B it follows that this view of CL-SCL corresponds to the induction of a new feature space given by Equation 2. 1122 5 Experiments MeCab is used for Japanese word segmentation.2 We evaluate CL-SCL for the task of crosslanguage sentiment classification using English as source language and German, French, and Japanese as target languages. Special emphasis is put on corpus construction, determination of upper bou</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie-K. Ando and Tong Zhang. 2005b. A highperformance semi-supervised learning method for text chunking. In Proceedings of ACL-05, pages 1– 9, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bautin</author>
<author>Lohit Vijayarenu</author>
<author>Steven Skiena</author>
</authors>
<title>International sentiment analysis for news and blogs.</title>
<date>2008</date>
<booktitle>In Proceedings of ICWSM-08,</booktitle>
<pages>19--26</pages>
<location>Seattle.</location>
<contexts>
<context position="26572" citStr="Bautin et al., 2008" startWordPosition="4379" endWordPosition="4382">ry-combination a linear classifier is learned on the training set and tested on the test set. The resulting accuracy scores are referred to as upper bound; it informs us about the expected performance on the target task if training data in the target language is available. We chose a machine translation baseline to compare CL-SCL to another cross-language method. Statistical machine translation technology offers a straightforward solution to the problem of cross-language text classification and has been used in a number of cross-language sentiment classification studies (Hiroshi et al., 2004; Bautin et al., 2008; Wan, 2009). Our baseline CL-MT works as follows: (1) learn a linear classifier on the training data, and (2) translate the test documents into the source language,6 (3) predict 6Again we use Google Translate. the sentiment polarity of the translated test documents. Note that the baseline CL-MT does not make use of unlabeled documents. 5.4 Performance Results and Sensitivity Table 1 contrasts the classification performance of CL-SCL with the upper bound and with the baseline. Observe that the upper bound does not exhibit a great variability across the three languages. The average accuracy is </context>
</contexts>
<marker>Bautin, Vijayarenu, Skiena, 2008</marker>
<rawString>Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena. 2008. International sentiment analysis for news and blogs. In Proceedings of ICWSM-08, pages 19–26, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuria Bel</author>
<author>Cornelis H A Koster</author>
<author>Marta Villegas</author>
</authors>
<title>Cross-lingual text categorization.</title>
<date>2003</date>
<booktitle>In Proceedings of ECDL-03,</booktitle>
<pages>126--139</pages>
<location>Trondheim.</location>
<contexts>
<context position="5761" citStr="Bel et al. (2003)" startWordPosition="878" endWordPosition="881">representation. In this connection we compile extensive corpora in the languages English, German, French, and Japanese, and for different sentiment classification tasks. The paper is organized as follows: Section 2 surveys related work. Section 3 states the terminology for cross-language text classification. Section 4 describes our main contribution, a new approach to cross-language text classification based on structural correspondence learning. Section 5 presents experimental results in the context of cross-language sentiment classification. 2 Related Work Cross-Language Text Classification Bel et al. (2003) belong to the first who explicitly considered the problem of cross-language text classification. Their research, however, is predated by work in cross-language information retrieval, CLIR, where similar problems are addressed (Oard, 1998). Traditional approaches to crosslanguage text classification and CLIR use linguistic resources such as bilingual dictionaries or parallel corpora to induce correspondences between two languages (Lavrenko et al., 2002; Olsson et al., 2005). Dumais et al. (1997) is considered as seminal work in CLIR: they propose a method which induces semantic correspondences</context>
<context position="12966" citStr="Bel et al., 2003" startWordPosition="2045" endWordPosition="2048">ocument d written in S or T with its cross-lingual representation. Once such a mapping is found the cross-language text classification problem reduces to a standard classification problem in the crosslingual space. Note that the existing methods for cross-language text classification can be characterized by the way 0 is constructed. For instance, cross-language latent semantic indexing (Dumais et al., 1997) and cross-language explicit semantic analysis (Potthast et al., 2008) estimate 0 using a parallel corpus. Other methods use linguistic resources such as a bilingual dictionary to obtain 0 (Bel et al., 2003; Olsson et al., 2005). 1120 4 Cross-Language Structural Correspondence Learning We now present a novel method for learning a map B by exploiting relations from unlabeled documents written in S and T . The proposed method, which we call cross-language structural correspondence learning, CL-SCL, addresses the following learning setup (see also Figure 1): • Given a set of labeled training documents DS written in language S, the goal is to create a text classifier for documents written in a different language T . We refer to this classification task as the target task. An example for the target t</context>
</contexts>
<marker>Bel, Koster, Villegas, 2003</marker>
<rawString>Nuria Bel, Cornelis H. A. Koster, and Marta Villegas. 2003. Cross-lingual text categorization. In Proceedings of ECDL-03, pages 126–139, Trondheim.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP-06,</booktitle>
<pages>120--128</pages>
<location>Sydney.</location>
<contexts>
<context position="2384" citStr="Blitzer et al., 2006" startWordPosition="350" endWordPosition="353">ce: machine translation of unlabeled documents from T to S, dictionary-based translation of unlabeled documents from T to S, or language-independent concept modeling by means of comparable corpora. The mentioned approaches have their pros and cons, some of which are discussed below. Here we propose a different approach to crosslanguage text classification which adopts ideas from the field of multi-task learning (Ando and Zhang, 2005a). Our approach builds upon structural correspondence learning, SCL, a recently proposed theory for domain adaptation in the field of natural language processing (Blitzer et al., 2006). Similar to SCL, our approach induces correspondences among the words from both languages by means of a small number of so-called pivots. In our context a pivot is a pair of words, {wS, wT }, from the source language S and the target language T, which possess a similar semantics. Testing the occurrence of wS or wT in a set of unlabeled documents from S and T yields two equivalence classes across these languages: one class contains the documents where either wS or wT occur, the other class contains the documents where neither wS nor wT occur. Ideally, a pivot splits the set of unlabeled docume</context>
<context position="8336" citStr="Blitzer et al., 2006" startWordPosition="1282" endWordPosition="1285">ach by Wan (2009). Domain Adaptation Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one (or more) source domains (e.g., newswire texts) to a different target domain (e.g., legal texts). In the basic domain adaptation setting we are given labeled data from the source domain and unlabeled data from the target domain, and the goal is to train a classifier for the target domain. Beyond this setting one can further distinguish whether a small amount of labeled data from the target domain is available (Daume, 2007; Finkel and Manning, 2009) or not (Blitzer et al., 2006; Jiang and Zhai, 2007). The latter setting is referred to as unsupervised domain adaptation. 1119 Note that, cross-language text classification can be cast as an unsupervised domain adaptation problem by considering each language as a separate domain. Blitzer et al. (2006) propose an effective algorithm for unsupervised domain adaptation, called structural correspondence learning. First, SCL identifies features that generalize across domains, which the authors call pivots. SCL then models the correlation between the pivots and all other features by training linear classifiers on the unlabeled</context>
<context position="23410" citStr="Blitzer et al. (2006)" startWordPosition="3874" endWordPosition="3877">d by Zhang (2004), is chosen as loss function L.3 SGD receives two hyperparameters as input: the number of iterations T, and the regularization parameter A. In our experiments T is always set to 106, which is about the number of iterations required for SGD to converge. For the target task, A is determined by 3-fold cross-validation, testing for A all values 10−�, i E [0; 6]. For the pivot prediction task, A is set to the small value of 10−5, in order to favor model accuracy over generalizability. The computational bottleneck of CL-SCL is the SVD of the dense parameter matrix W. Here we follow Blitzer et al. (2006) and set the negative values in W to zero, which yields a sparse representation. For the SVD computation the Lanczos algorithm provided by SVDLIBC is employed.4 We investigated an alternative approach to obtain a sparse W by directly enforcing sparse pivot predictors wl through L1-regularization (Tsuruoka et al., 2009), but didn’t pursue this strategy due to unstable results. Since SGD is sensitive to feature scaling the projection Ox is post-processed as follows: (1) Each feature of the cross-lingual representation is standardized to zero mean and unit variance, where mean and variance are es</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of EMNLP-06, pages 120–128, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07,</booktitle>
<pages>440--447</pages>
<location>Prague.</location>
<contexts>
<context position="20985" citStr="Blitzer et al. (2007)" startWordPosition="3481" endWordPosition="3484">e as target languages. Special emphasis is put on corpus construction, determination of upper bounds and baselines, and a sensitivity analysis of important hyperparameters. All data described in the following is publicly available from our project website.1 5.1 Dataset and Preprocessing We compiled a new dataset for cross-language sentiment classification by crawling product reviews from Amazon.{de |fr |co.jp}. The crawled part of the corpus contains more than 4 million reviews in the three languages German, French, and Japanese. The corpus is extended with English product reviews provided by Blitzer et al. (2007). Each review contains a category label, a title, the review text, and a rating of 1-5 stars. Following Blitzer et al. (2007) a review with &gt;3 (&lt;3) stars is labeled as positive (negative); other reviews are discarded. For each language the labeled reviews are grouped according to their category label, whereas we restrict our experiments to three categories: books, dvds, and music. Since most of the crawled reviews are positive (80%), we decide to balance the number of positive and negative reviews. In this study, we are interested in whether the cross-lingual representation induced by CL-SCL c</context>
<context position="27294" citStr="Blitzer et al., 2007" startWordPosition="4498" endWordPosition="4501">, and (2) translate the test documents into the source language,6 (3) predict 6Again we use Google Translate. the sentiment polarity of the translated test documents. Note that the baseline CL-MT does not make use of unlabeled documents. 5.4 Performance Results and Sensitivity Table 1 contrasts the classification performance of CL-SCL with the upper bound and with the baseline. Observe that the upper bound does not exhibit a great variability across the three languages. The average accuracy is about 82%, which is consistent with prior work on monolingual sentiment analysis (Pang et al., 2002; Blitzer et al., 2007). The performance of CL-MT, however, differs considerably between the two European languages and Japanese: for Japanese, the average difference between the upper bound and CL-MT (9.9%) is about twice as much as for German and French (5.3%). This difference can be explained by the fact that machine translation works better for European than for Asian languages such as Japanese. Recall that CL-SCL receives three hyperparameters as input: the number of pivots m, the dimensionality of the cross-lingual representation k, English German Pivot Semantics Pragmatics Semantics Pragmatics {beautifulS, sc</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of ACL-07, pages 440–447, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07,</booktitle>
<pages>256--263</pages>
<location>Prague.</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of ACL-07, pages 256–263, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>Todd A Letsche</author>
<author>Michael L Littman</author>
<author>Thomas K Landauer</author>
</authors>
<title>Automatic cross-language retrieval using latent semantic indexing.</title>
<date>1997</date>
<booktitle>In AAAI Symposium on CrossLanguage Text and Speech Retrieval.</booktitle>
<contexts>
<context position="6261" citStr="Dumais et al. (1997)" startWordPosition="951" endWordPosition="954"> context of cross-language sentiment classification. 2 Related Work Cross-Language Text Classification Bel et al. (2003) belong to the first who explicitly considered the problem of cross-language text classification. Their research, however, is predated by work in cross-language information retrieval, CLIR, where similar problems are addressed (Oard, 1998). Traditional approaches to crosslanguage text classification and CLIR use linguistic resources such as bilingual dictionaries or parallel corpora to induce correspondences between two languages (Lavrenko et al., 2002; Olsson et al., 2005). Dumais et al. (1997) is considered as seminal work in CLIR: they propose a method which induces semantic correspondences between two languages by performing latent semantic analysis, LSA, on a parallel corpus. Li and Taylor (2007) improve upon this method by employing kernel canonical correlation analysis, CCA, instead of LSA. The major limitation of these approaches is their computational complexity and, in particular, the dependence on a parallel corpus, which is hard to obtain—especially for less resource-rich languages. Gliozzo and Strapparava (2005) circumvent the dependence on a parallel corpus by using so-</context>
<context position="12760" citStr="Dumais et al., 1997" startWordPosition="2012" endWordPosition="2015">derstand such a cross-lingual representation as a concept space that underlies both languages. In the following, we will use 0 to denote a map that associates the original |V |-dimensional representation of a document d written in S or T with its cross-lingual representation. Once such a mapping is found the cross-language text classification problem reduces to a standard classification problem in the crosslingual space. Note that the existing methods for cross-language text classification can be characterized by the way 0 is constructed. For instance, cross-language latent semantic indexing (Dumais et al., 1997) and cross-language explicit semantic analysis (Potthast et al., 2008) estimate 0 using a parallel corpus. Other methods use linguistic resources such as a bilingual dictionary to obtain 0 (Bel et al., 2003; Olsson et al., 2005). 1120 4 Cross-Language Structural Correspondence Learning We now present a novel method for learning a map B by exploiting relations from unlabeled documents written in S and T . The proposed method, which we call cross-language structural correspondence learning, CL-SCL, addresses the following learning setup (see also Figure 1): • Given a set of labeled training docu</context>
</contexts>
<marker>Dumais, Letsche, Littman, Landauer, 1997</marker>
<rawString>Susan T. Dumais, Todd A. Letsche, Michael L. Littman, and Thomas K. Landauer. 1997. Automatic cross-language retrieval using latent semantic indexing. In AAAI Symposium on CrossLanguage Text and Speech Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny-R Finkel</author>
<author>Christopher-D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT/NAACL-09,</booktitle>
<pages>602--610</pages>
<location>Boulder.</location>
<contexts>
<context position="8307" citStr="Finkel and Manning, 2009" startWordPosition="1276" endWordPosition="1279"> (2008), and the co-training approach by Wan (2009). Domain Adaptation Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one (or more) source domains (e.g., newswire texts) to a different target domain (e.g., legal texts). In the basic domain adaptation setting we are given labeled data from the source domain and unlabeled data from the target domain, and the goal is to train a classifier for the target domain. Beyond this setting one can further distinguish whether a small amount of labeled data from the target domain is available (Daume, 2007; Finkel and Manning, 2009) or not (Blitzer et al., 2006; Jiang and Zhai, 2007). The latter setting is referred to as unsupervised domain adaptation. 1119 Note that, cross-language text classification can be cast as an unsupervised domain adaptation problem by considering each language as a separate domain. Blitzer et al. (2006) propose an effective algorithm for unsupervised domain adaptation, called structural correspondence learning. First, SCL identifies features that generalize across domains, which the authors call pivots. SCL then models the correlation between the pivots and all other features by training linear</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny-R. Finkel and Christopher-D. Manning. 2009. Hierarchical bayesian domain adaptation. In Proceedings of HLT/NAACL-09, pages 602–610, Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaˇz Fortuna</author>
<author>John Shawe-Taylor</author>
</authors>
<title>The use of machine translation tools for cross-lingual text mining.</title>
<date>2005</date>
<booktitle>In Proceedings of the ICML Workshop on Learning with Multiple Views.</booktitle>
<contexts>
<context position="7630" citStr="Fortuna and Shawe-Taylor (2005)" startWordPosition="1165" endWordPosition="1169">006) they show for particular tasks that their approach can achieve a performance close to that of monolingual text classification. Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) translation of the documents into the source or the target language, and (2) dimensionality reduction or semi-supervised learning to reduce the noise introduced by the machine translation. Methods which follow this twostep approach include the EM-based approach by Rigutini et al. (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al. (2008), and the co-training approach by Wan (2009). Domain Adaptation Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one (or more) source domains (e.g., newswire texts) to a different target domain (e.g., legal texts). In the basic domain adaptation setting we are given labeled data from the source domain and unlabeled data from the target domain, and the goal is to train a classifier for the target domain. Beyond this setting one can further distinguish whether a small amount of labeled dat</context>
</contexts>
<marker>Fortuna, Shawe-Taylor, 2005</marker>
<rawString>Blaˇz Fortuna and John Shawe-Taylor. 2005. The use of machine translation tools for cross-lingual text mining. In Proceedings of the ICML Workshop on Learning with Multiple Views.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Cross language text categorization by acquiring multilingual domain models from comparable corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts.</booktitle>
<contexts>
<context position="6801" citStr="Gliozzo and Strapparava (2005)" startWordPosition="1033" endWordPosition="1037">s between two languages (Lavrenko et al., 2002; Olsson et al., 2005). Dumais et al. (1997) is considered as seminal work in CLIR: they propose a method which induces semantic correspondences between two languages by performing latent semantic analysis, LSA, on a parallel corpus. Li and Taylor (2007) improve upon this method by employing kernel canonical correlation analysis, CCA, instead of LSA. The major limitation of these approaches is their computational complexity and, in particular, the dependence on a parallel corpus, which is hard to obtain—especially for less resource-rich languages. Gliozzo and Strapparava (2005) circumvent the dependence on a parallel corpus by using so-called multilingual domain models, which can be acquired from comparable corpora in an unsupervised manner. In (Gliozzo and Strapparava, 2006) they show for particular tasks that their approach can achieve a performance close to that of monolingual text classification. Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) translation of the documents into the source or the target language, and (2) dimensionality reduction or semi-s</context>
</contexts>
<marker>Gliozzo, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo and Carlo Strapparava. 2005. Cross language text categorization by acquiring multilingual domain models from comparable corpora. In Proceedings of the ACL Workshop on Building and Using Parallel Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-06,</booktitle>
<pages>553--560</pages>
<location>Sydney.</location>
<contexts>
<context position="7003" citStr="Gliozzo and Strapparava, 2006" startWordPosition="1066" endWordPosition="1069"> languages by performing latent semantic analysis, LSA, on a parallel corpus. Li and Taylor (2007) improve upon this method by employing kernel canonical correlation analysis, CCA, instead of LSA. The major limitation of these approaches is their computational complexity and, in particular, the dependence on a parallel corpus, which is hard to obtain—especially for less resource-rich languages. Gliozzo and Strapparava (2005) circumvent the dependence on a parallel corpus by using so-called multilingual domain models, which can be acquired from comparable corpora in an unsupervised manner. In (Gliozzo and Strapparava, 2006) they show for particular tasks that their approach can achieve a performance close to that of monolingual text classification. Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) translation of the documents into the source or the target language, and (2) dimensionality reduction or semi-supervised learning to reduce the noise introduced by the machine translation. Methods which follow this twostep approach include the EM-based approach by Rigutini et al. (2005), the CCA approach by Fort</context>
</contexts>
<marker>Gliozzo, Strapparava, 2006</marker>
<rawString>Alfio Gliozzo and Carlo Strapparava. 2006. Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization. In Proceedings of ACL-06, pages 553–560, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kanayama Hiroshi</author>
<author>Nasukawa Tetsuya</author>
<author>Watanabe Hideo</author>
</authors>
<title>Deeper sentiment analysis using machine translation technology.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>494--500</pages>
<location>Geneva.</location>
<contexts>
<context position="26551" citStr="Hiroshi et al., 2004" startWordPosition="4375" endWordPosition="4378"> target-languagecategory-combination a linear classifier is learned on the training set and tested on the test set. The resulting accuracy scores are referred to as upper bound; it informs us about the expected performance on the target task if training data in the target language is available. We chose a machine translation baseline to compare CL-SCL to another cross-language method. Statistical machine translation technology offers a straightforward solution to the problem of cross-language text classification and has been used in a number of cross-language sentiment classification studies (Hiroshi et al., 2004; Bautin et al., 2008; Wan, 2009). Our baseline CL-MT works as follows: (1) learn a linear classifier on the training data, and (2) translate the test documents into the source language,6 (3) predict 6Again we use Google Translate. the sentiment polarity of the translated test documents. Note that the baseline CL-MT does not make use of unlabeled documents. 5.4 Performance Results and Sensitivity Table 1 contrasts the classification performance of CL-SCL with the upper bound and with the baseline. Observe that the upper bound does not exhibit a great variability across the three languages. The</context>
</contexts>
<marker>Hiroshi, Tetsuya, Hideo, 2004</marker>
<rawString>Kanayama Hiroshi, Nasukawa Tetsuya, and Watanabe Hideo. 2004. Deeper sentiment analysis using machine translation technology. In Proceedings of COLING-04, pages 494–500, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>Chengxiang Zhai</author>
</authors>
<title>A two-stage approach to domain adaptation for statistical classifiers.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM-07,</booktitle>
<pages>401--410</pages>
<location>Lisbon.</location>
<contexts>
<context position="8359" citStr="Jiang and Zhai, 2007" startWordPosition="1286" endWordPosition="1289">ain Adaptation Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one (or more) source domains (e.g., newswire texts) to a different target domain (e.g., legal texts). In the basic domain adaptation setting we are given labeled data from the source domain and unlabeled data from the target domain, and the goal is to train a classifier for the target domain. Beyond this setting one can further distinguish whether a small amount of labeled data from the target domain is available (Daume, 2007; Finkel and Manning, 2009) or not (Blitzer et al., 2006; Jiang and Zhai, 2007). The latter setting is referred to as unsupervised domain adaptation. 1119 Note that, cross-language text classification can be cast as an unsupervised domain adaptation problem by considering each language as a separate domain. Blitzer et al. (2006) propose an effective algorithm for unsupervised domain adaptation, called structural correspondence learning. First, SCL identifies features that generalize across domains, which the authors call pivots. SCL then models the correlation between the pivots and all other features by training linear classifiers on the unlabeled data from both domains</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and Chengxiang Zhai. 2007. A two-stage approach to domain adaptation for statistical classifiers. In Proceedings of CIKM-07, pages 401–410, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Martin Choquette</author>
<author>W Bruce Croft</author>
</authors>
<title>Cross-lingual relevance models.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR-02,</booktitle>
<pages>175--182</pages>
<location>Tampere.</location>
<contexts>
<context position="6217" citStr="Lavrenko et al., 2002" startWordPosition="943" endWordPosition="946">ection 5 presents experimental results in the context of cross-language sentiment classification. 2 Related Work Cross-Language Text Classification Bel et al. (2003) belong to the first who explicitly considered the problem of cross-language text classification. Their research, however, is predated by work in cross-language information retrieval, CLIR, where similar problems are addressed (Oard, 1998). Traditional approaches to crosslanguage text classification and CLIR use linguistic resources such as bilingual dictionaries or parallel corpora to induce correspondences between two languages (Lavrenko et al., 2002; Olsson et al., 2005). Dumais et al. (1997) is considered as seminal work in CLIR: they propose a method which induces semantic correspondences between two languages by performing latent semantic analysis, LSA, on a parallel corpus. Li and Taylor (2007) improve upon this method by employing kernel canonical correlation analysis, CCA, instead of LSA. The major limitation of these approaches is their computational complexity and, in particular, the dependence on a parallel corpus, which is hard to obtain—especially for less resource-rich languages. Gliozzo and Strapparava (2005) circumvent the </context>
</contexts>
<marker>Lavrenko, Choquette, Croft, 2002</marker>
<rawString>Victor Lavrenko, Martin Choquette, and W. Bruce Croft. 2002. Cross-lingual relevance models. In Proceedings of SIGIR-02, pages 175–182, Tampere.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaoyong Li</author>
<author>John S Taylor</author>
</authors>
<title>Advanced learning algorithms for cross-language patent retrieval and classification.</title>
<date>2007</date>
<journal>Inf. Process. Manage.,</journal>
<volume>43</volume>
<issue>5</issue>
<contexts>
<context position="6471" citStr="Li and Taylor (2007)" startWordPosition="984" endWordPosition="987">cation. Their research, however, is predated by work in cross-language information retrieval, CLIR, where similar problems are addressed (Oard, 1998). Traditional approaches to crosslanguage text classification and CLIR use linguistic resources such as bilingual dictionaries or parallel corpora to induce correspondences between two languages (Lavrenko et al., 2002; Olsson et al., 2005). Dumais et al. (1997) is considered as seminal work in CLIR: they propose a method which induces semantic correspondences between two languages by performing latent semantic analysis, LSA, on a parallel corpus. Li and Taylor (2007) improve upon this method by employing kernel canonical correlation analysis, CCA, instead of LSA. The major limitation of these approaches is their computational complexity and, in particular, the dependence on a parallel corpus, which is hard to obtain—especially for less resource-rich languages. Gliozzo and Strapparava (2005) circumvent the dependence on a parallel corpus by using so-called multilingual domain models, which can be acquired from comparable corpora in an unsupervised manner. In (Gliozzo and Strapparava, 2006) they show for particular tasks that their approach can achieve a pe</context>
</contexts>
<marker>Li, Taylor, 2007</marker>
<rawString>Yaoyong Li and John S. Taylor. 2007. Advanced learning algorithms for cross-language patent retrieval and classification. Inf. Process. Manage., 43(5):1183–1199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Ling</author>
<author>Gui-R Xue</author>
<author>Wenyuan Dai</author>
<author>Yun Jiang</author>
<author>Qiang Yang</author>
<author>Yong Yu</author>
</authors>
<title>Can chinese web pages be classified with english data source?</title>
<date>2008</date>
<booktitle>In Proceedings of WWW-08,</booktitle>
<pages>969--978</pages>
<location>Beijing.</location>
<contexts>
<context position="7689" citStr="Ling et al. (2008)" startWordPosition="1175" endWordPosition="1178">rformance close to that of monolingual text classification. Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) translation of the documents into the source or the target language, and (2) dimensionality reduction or semi-supervised learning to reduce the noise introduced by the machine translation. Methods which follow this twostep approach include the EM-based approach by Rigutini et al. (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al. (2008), and the co-training approach by Wan (2009). Domain Adaptation Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one (or more) source domains (e.g., newswire texts) to a different target domain (e.g., legal texts). In the basic domain adaptation setting we are given labeled data from the source domain and unlabeled data from the target domain, and the goal is to train a classifier for the target domain. Beyond this setting one can further distinguish whether a small amount of labeled data from the target domain is available (Daume, 2007; Finkel </context>
</contexts>
<marker>Ling, Xue, Dai, Jiang, Yang, Yu, 2008</marker>
<rawString>Xiao Ling, Gui-R. Xue, Wenyuan Dai, Yun Jiang, Qiang Yang, and Yong Yu. 2008. Can chinese web pages be classified with english data source? In Proceedings of WWW-08, pages 969–978, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas W Oard</author>
</authors>
<title>A comparative study of query and document translation for cross-language information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings ofAMTA-98,</booktitle>
<pages>472--483</pages>
<location>Langhorne.</location>
<contexts>
<context position="6000" citStr="Oard, 1998" startWordPosition="914" endWordPosition="915">3 states the terminology for cross-language text classification. Section 4 describes our main contribution, a new approach to cross-language text classification based on structural correspondence learning. Section 5 presents experimental results in the context of cross-language sentiment classification. 2 Related Work Cross-Language Text Classification Bel et al. (2003) belong to the first who explicitly considered the problem of cross-language text classification. Their research, however, is predated by work in cross-language information retrieval, CLIR, where similar problems are addressed (Oard, 1998). Traditional approaches to crosslanguage text classification and CLIR use linguistic resources such as bilingual dictionaries or parallel corpora to induce correspondences between two languages (Lavrenko et al., 2002; Olsson et al., 2005). Dumais et al. (1997) is considered as seminal work in CLIR: they propose a method which induces semantic correspondences between two languages by performing latent semantic analysis, LSA, on a parallel corpus. Li and Taylor (2007) improve upon this method by employing kernel canonical correlation analysis, CCA, instead of LSA. The major limitation of these </context>
</contexts>
<marker>Oard, 1998</marker>
<rawString>Douglas W. Oard. 1998. A comparative study of query and document translation for cross-language information retrieval. In Proceedings ofAMTA-98, pages 472–483, Langhorne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Scott Olsson</author>
<author>Douglas W Oard</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Cross-language text classification.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR-05,</booktitle>
<pages>645--646</pages>
<location>Salvador.</location>
<marker>Olsson, Oard, Hajiˇc, 2005</marker>
<rawString>J. Scott Olsson, Douglas W. Oard, and Jan Hajiˇc. 2005. Cross-language text classification. In Proceedings of SIGIR-05, pages 645–646, Salvador.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-02,</booktitle>
<pages>79--86</pages>
<location>Philadelphia.</location>
<contexts>
<context position="27271" citStr="Pang et al., 2002" startWordPosition="4494" endWordPosition="4497">n the training data, and (2) translate the test documents into the source language,6 (3) predict 6Again we use Google Translate. the sentiment polarity of the translated test documents. Note that the baseline CL-MT does not make use of unlabeled documents. 5.4 Performance Results and Sensitivity Table 1 contrasts the classification performance of CL-SCL with the upper bound and with the baseline. Observe that the upper bound does not exhibit a great variability across the three languages. The average accuracy is about 82%, which is consistent with prior work on monolingual sentiment analysis (Pang et al., 2002; Blitzer et al., 2007). The performance of CL-MT, however, differs considerably between the two European languages and Japanese: for Japanese, the average difference between the upper bound and CL-MT (9.9%) is about twice as much as for German and French (5.3%). This difference can be explained by the fact that machine translation works better for European than for Asian languages such as Japanese. Recall that CL-SCL receives three hyperparameters as input: the number of pivots m, the dimensionality of the cross-lingual representation k, English German Pivot Semantics Pragmatics Semantics Pra</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of EMNLP-02, pages 79–86, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Maik Anderka</author>
</authors>
<title>A wikipedia-based multilingual retrieval model.</title>
<date>2008</date>
<booktitle>In Proceedings of ECIR-08,</booktitle>
<pages>522--530</pages>
<location>Glasgow.</location>
<contexts>
<context position="12830" citStr="Potthast et al., 2008" startWordPosition="2021" endWordPosition="2024"> underlies both languages. In the following, we will use 0 to denote a map that associates the original |V |-dimensional representation of a document d written in S or T with its cross-lingual representation. Once such a mapping is found the cross-language text classification problem reduces to a standard classification problem in the crosslingual space. Note that the existing methods for cross-language text classification can be characterized by the way 0 is constructed. For instance, cross-language latent semantic indexing (Dumais et al., 1997) and cross-language explicit semantic analysis (Potthast et al., 2008) estimate 0 using a parallel corpus. Other methods use linguistic resources such as a bilingual dictionary to obtain 0 (Bel et al., 2003; Olsson et al., 2005). 1120 4 Cross-Language Structural Correspondence Learning We now present a novel method for learning a map B by exploiting relations from unlabeled documents written in S and T . The proposed method, which we call cross-language structural correspondence learning, CL-SCL, addresses the following learning setup (see also Figure 1): • Given a set of labeled training documents DS written in language S, the goal is to create a text classifie</context>
</contexts>
<marker>Potthast, Stein, Anderka, 2008</marker>
<rawString>Martin Potthast, Benno Stein, and Maik Anderka. 2008. A wikipedia-based multilingual retrieval model. In Proceedings of ECIR-08, pages 522–530, Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Michael Collins</author>
<author>Trevor Darrell</author>
</authors>
<title>Learning visual representations using images with captions.</title>
<date>2007</date>
<booktitle>In Proceedings of CVPR-07,</booktitle>
<pages>1--8</pages>
<location>Minneapolis.</location>
<contexts>
<context position="9553" citStr="Quattoni et al. (2007)" startWordPosition="1469" endWordPosition="1472">beled data from both domains. This information is used to induce correspondences among features from the different domains and to learn a shared representation that is meaningful across both domains. SCL is related to the structural learning paradigm introduced by Ando and Zhang (2005a). The basic idea of structural learning is to constrain the hypothesis space of a learning task by considering multiple different but related tasks on the same input space. Ando and Zhang (2005b) present a semi-supervised learning method based on this paradigm, which generates related tasks from unlabeled data. Quattoni et al. (2007) apply structural learning to image classification in settings where little labeled data is given. 3 Cross-Language Text Classification This section introduces basic models and terminology. In standard text classification, a document d is represented under the bag-of-words model as |V |-dimensional feature vector x E X, where V , the vocabulary, denotes an ordered set of words, xi E x denotes the normalized frequency of word i in d, and X is an inner product space. DS denotes the training set and comprises tuples of the form (x, y), which associate a feature vector x E X with a class label y E</context>
<context position="19890" citStr="Quattoni et al. (2007)" startWordPosition="3299" endWordPosition="3302">. In our context these auxiliary tasks are represented by the pivot predictors, i.e., the columns of W. Each column vector wl can be considered as a linear classifier which performs well in both languages. I.e., we regard the column space of W as an approximation to the subspace of bilingual classifiers. By computing SVD(W) one obtains a compact representation of this column space in the form of an orthonormal basis BT. The subspace is used to constrain the learning of the target task by restricting the weight vector w to lie in the subspace defined by BT. Following Ando and Zhang (2005a) and Quattoni et al. (2007) we choose w for the target task to be w* = BT v*, where v* is defined as follows: � v� = argmin L(y, (BTv)Tx) + A kvk2 (3) vERk (x,y)EDS 2 Since (BT v)T = vT B it follows that this view of CL-SCL corresponds to the induction of a new feature space given by Equation 2. 1122 5 Experiments MeCab is used for Japanese word segmentation.2 We evaluate CL-SCL for the task of crosslanguage sentiment classification using English as source language and German, French, and Japanese as target languages. Special emphasis is put on corpus construction, determination of upper bounds and baselines, and a sens</context>
</contexts>
<marker>Quattoni, Collins, Darrell, 2007</marker>
<rawString>Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2007. Learning visual representations using images with captions. In Proceedings of CVPR-07, pages 1–8, Minneapolis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Rigutini</author>
<author>Marco Maggini</author>
<author>Bing Liu</author>
</authors>
<title>An em based training algorithm for crosslanguage text categorization.</title>
<date>2005</date>
<booktitle>In Proceedings of WI05,</booktitle>
<pages>529--535</pages>
<contexts>
<context position="7577" citStr="Rigutini et al. (2005)" startWordPosition="1157" endWordPosition="1160">vised manner. In (Gliozzo and Strapparava, 2006) they show for particular tasks that their approach can achieve a performance close to that of monolingual text classification. Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) translation of the documents into the source or the target language, and (2) dimensionality reduction or semi-supervised learning to reduce the noise introduced by the machine translation. Methods which follow this twostep approach include the EM-based approach by Rigutini et al. (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al. (2008), and the co-training approach by Wan (2009). Domain Adaptation Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one (or more) source domains (e.g., newswire texts) to a different target domain (e.g., legal texts). In the basic domain adaptation setting we are given labeled data from the source domain and unlabeled data from the target domain, and the goal is to train a classifier for the target domain. Beyond this setting one can furt</context>
</contexts>
<marker>Rigutini, Maggini, Liu, 2005</marker>
<rawString>Leonardo Rigutini, Marco Maggini, and Bing Liu. 2005. An em based training algorithm for crosslanguage text categorization. In Proceedings of WI05, pages 529–535, Compi`egne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for svm.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML-07,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="22749" citStr="Shalev-Shwartz et al., 2007" startWordPosition="3755" endWordPosition="3758">egorycombinations a text classification task is created by taking the training set of the product category in S and the test set of the same product category in T . A document d is described as normalized feature vector x under a unigram bag-of-words document representation. The morphological analyzer 1http://www.webis.de/research/corpora/ webis-cls-10/ 5.2 Implementation Throughout the experiments linear classifiers are employed; they are trained by minimizing Equation (1), using a stochastic gradient descent (SGD) algorithm. In particular, the learning rate schedule from PEGASOS is adopted (Shalev-Shwartz et al., 2007), and the modified Huber loss, introduced by Zhang (2004), is chosen as loss function L.3 SGD receives two hyperparameters as input: the number of iterations T, and the regularization parameter A. In our experiments T is always set to 106, which is about the number of iterations required for SGD to converge. For the target task, A is determined by 3-fold cross-validation, testing for A all values 10−�, i E [0; 6]. For the pivot prediction task, A is set to the small value of 10−5, in order to favor model accuracy over generalizability. The computational bottleneck of CL-SCL is the SVD of the d</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for svm. In Proceedings of ICML-07, pages 807–814, Corvalis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/AFNLP-09,</booktitle>
<pages>477--485</pages>
<contexts>
<context position="23730" citStr="Tsuruoka et al., 2009" startWordPosition="3925" endWordPosition="3928">oss-validation, testing for A all values 10−�, i E [0; 6]. For the pivot prediction task, A is set to the small value of 10−5, in order to favor model accuracy over generalizability. The computational bottleneck of CL-SCL is the SVD of the dense parameter matrix W. Here we follow Blitzer et al. (2006) and set the negative values in W to zero, which yields a sparse representation. For the SVD computation the Lanczos algorithm provided by SVDLIBC is employed.4 We investigated an alternative approach to obtain a sparse W by directly enforcing sparse pivot predictors wl through L1-regularization (Tsuruoka et al., 2009), but didn’t pursue this strategy due to unstable results. Since SGD is sensitive to feature scaling the projection Ox is post-processed as follows: (1) Each feature of the cross-lingual representation is standardized to zero mean and unit variance, where mean and variance are estimated on DS U D, (2) The cross-lingual document representations are scaled by a constant α such that |DS|−&apos; ExEDg I1αex1I = 1. We use Google Translate as word translation oracle, which returns a single translation for each query word.5 Though such a context free translation is suboptimum we do not sanitize the return</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proceedings of ACL/AFNLP-09, pages 477–485, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for crosslingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/AFNLP-09,</booktitle>
<pages>235--243</pages>
<contexts>
<context position="7733" citStr="Wan (2009)" startWordPosition="1184" endWordPosition="1185">cation. Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) translation of the documents into the source or the target language, and (2) dimensionality reduction or semi-supervised learning to reduce the noise introduced by the machine translation. Methods which follow this twostep approach include the EM-based approach by Rigutini et al. (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al. (2008), and the co-training approach by Wan (2009). Domain Adaptation Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one (or more) source domains (e.g., newswire texts) to a different target domain (e.g., legal texts). In the basic domain adaptation setting we are given labeled data from the source domain and unlabeled data from the target domain, and the goal is to train a classifier for the target domain. Beyond this setting one can further distinguish whether a small amount of labeled data from the target domain is available (Daume, 2007; Finkel and Manning, 2009) or not (Blitzer et al., 2</context>
<context position="26584" citStr="Wan, 2009" startWordPosition="4383" endWordPosition="4384">ar classifier is learned on the training set and tested on the test set. The resulting accuracy scores are referred to as upper bound; it informs us about the expected performance on the target task if training data in the target language is available. We chose a machine translation baseline to compare CL-SCL to another cross-language method. Statistical machine translation technology offers a straightforward solution to the problem of cross-language text classification and has been used in a number of cross-language sentiment classification studies (Hiroshi et al., 2004; Bautin et al., 2008; Wan, 2009). Our baseline CL-MT works as follows: (1) learn a linear classifier on the training data, and (2) translate the test documents into the source language,6 (3) predict 6Again we use Google Translate. the sentiment polarity of the translated test documents. Note that the baseline CL-MT does not make use of unlabeled documents. 5.4 Performance Results and Sensitivity Table 1 contrasts the classification performance of CL-SCL with the upper bound and with the baseline. Observe that the upper bound does not exhibit a great variability across the three languages. The average accuracy is about 82%, w</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for crosslingual sentiment classification. In Proceedings of ACL/AFNLP-09, pages 235–243, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
</authors>
<title>Solving large scale linear prediction problems using stochastic gradient descent algorithms.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML-04,</booktitle>
<pages>116--124</pages>
<location>Banff.</location>
<contexts>
<context position="11071" citStr="Zhang, 2004" startWordPosition="1737" endWordPosition="1738">ssifier, [·]T denotes the matrix transpose. The computation of w from DS is referred to as model estimation or training. A common choice for w is given by a vector w* that minimizes the regularized training error: 1: w* = argmin L(y, wTx) + A IIwI12 (1) wER|V |(x,y)EDS 2 L is a loss function that measures the quality of the classifier, A is a non-negative regularization parameter that penalizes model complexity, and �w�2 = wTw. Different choices for L entail different classifier types; e.g., when choosing the hinge loss function for L one obtains the popular Support Vector Machine classifier (Zhang, 2004). Standard text classification distinguishes between labeled (training) documents and unlabeled (test) documents. Cross-language text classification poses an extra constraint in that training documents and test documents are written in different languages. Here, the language of the training documents is referred to as source language S, and the language of the test documents is referred to as target language T . The vocabulary V divides into VS and VT , called vocabulary of the source language and vocabulary of the target language, with VS n VT = 0. I.e., documents from the training set and th</context>
<context position="22806" citStr="Zhang (2004)" startWordPosition="3766" endWordPosition="3767">ining set of the product category in S and the test set of the same product category in T . A document d is described as normalized feature vector x under a unigram bag-of-words document representation. The morphological analyzer 1http://www.webis.de/research/corpora/ webis-cls-10/ 5.2 Implementation Throughout the experiments linear classifiers are employed; they are trained by minimizing Equation (1), using a stochastic gradient descent (SGD) algorithm. In particular, the learning rate schedule from PEGASOS is adopted (Shalev-Shwartz et al., 2007), and the modified Huber loss, introduced by Zhang (2004), is chosen as loss function L.3 SGD receives two hyperparameters as input: the number of iterations T, and the regularization parameter A. In our experiments T is always set to 106, which is about the number of iterations required for SGD to converge. For the target task, A is determined by 3-fold cross-validation, testing for A all values 10−�, i E [0; 6]. For the pivot prediction task, A is set to the small value of 10−5, in order to favor model accuracy over generalizability. The computational bottleneck of CL-SCL is the SVD of the dense parameter matrix W. Here we follow Blitzer et al. (2</context>
</contexts>
<marker>Zhang, 2004</marker>
<rawString>Tong Zhang. 2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of ICML-04, pages 116– 124, Banff.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>