<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008342">
<title confidence="0.998207">
Transonics: A Practical Speech-to-Speech Translator for English-Farsi
Medical Dialogues
</title>
<author confidence="0.840861666666667">
Emil Ettelaie, Sudeep Gandhe, Panayiotis Georgiou, Robert Belvin
Kevin Knight, Daniel Marcu, Shrikanth Narayanan , HRL Laboratories, LLC
David Traum 3011 Malibu Canyon Rd.
</author>
<affiliation confidence="0.993627">
University of Southern California Malibu, CA 90265
</affiliation>
<address confidence="0.950022">
Los Angeles, CA 90089 rsbelvin@hrl.com
</address>
<email confidence="0.952526">
ettelaie@isi.edu, gandhe@ict.usc.edu,
georgiou@sipi.usc.edu, knight@isi.edu,
marcu@isi.edu, shri@sipi.usc.edu,
traum@ict.use.edu
</email>
<sectionHeader confidence="0.996565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99984975">
We briefly describe a two-way speech-to-
speech English-Farsi translation system
prototype developed for use in doctor-
patient interactions. The overarching
philosophy of the developers has been to
create a system that enables effective
communication, rather than focusing on
maximizing component-level perform-
ance. The discussion focuses on the gen-
eral approach and evaluation of the
system by an independent government
evaluation team.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99962675">
In this paper we give a brief description of a
two-way speech-to-speech translation system,
which was created under a collaborative effort
between three organizations within USC (the
Speech Analysis and Interpretation Lab of the
Electrical Engineering department, the Information
Sciences Institute, and the Institute for Creative
Technologies) and the Information Sciences Lab of
HRL Laboratories. The system is intended to pro-
vide a means of enabling communication between
monolingual English speakers and monolingual
Farsi (Persian) speakers. The system is targeted at
a domain which may be roughly characterized as
&amp;quot;urgent care&amp;quot; medical interactions, where the Eng-
lish speaker is a medical professional and the Farsi
speaker is the patient. In addition to providing a
brief description of the system (and pointers to pa-
pers which contain more detailed information), we
give an overview of the major system evaluation
activities.
</bodyText>
<sectionHeader confidence="0.936298" genericHeader="method">
2 General Design of the system
</sectionHeader>
<bodyText confidence="0.997070285714286">
Our system is comprised of seven speech and
language processing components, as shown in Fig.
1. Modules communicate using a centralized mes-
sage-passing system. The individual subsystems
are the Automatic Speech Recognition (ASR) sub-
system, which uses n-gram Language Models
(LM) and produces n-best lists/lattices along with
the decoding confidence scores. The output of the
ASR is sent to the Dialog Manager (DM), which
displays the n-best and passes one hypothesis on to
the translation modules, according to a user-
configurable state. The DM sends translation re-
quests to the Machine Translation (MT) unit. The
MT unit works in two modes: Classifier based MT
and a fully Stochastic MT. Depending on the dia-
logue manager mode, translations can be sent to
the unit selection based Text-To-Speech synthe-
sizer (TTS), to provide the spoken output. The
same basic pipeline works in both directions: Eng-
lish ASR, English-Persian MT, Persian TTS, or
Persian ASR, Persian-English MT, English TTS.
There is, however, an asymmetry in the dia-
logue management and control, given the desire for
the English-speaking doctor to be in control of the
device and the primary &amp;quot;director&amp;quot; of the dialog.
The English ASR used the University of Colo-
rado Sonic recognizer, augmented primarily with
LM data collected from multiple sources, including
</bodyText>
<page confidence="0.997558">
89
</page>
<note confidence="0.393465">
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 89–92, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<figure confidence="0.9716523">
GUI:
prompts,
confirmations,
ASR switch
Dialog
Manager
Prompts or TTS
Farsi
ASR
Farsi
ASR
English
Prompts or TTS
English
MT
English to Farsi
Farsi to English
SMT
English to Farsi
Farsi to English
</figure>
<figureCaption confidence="0.952192">
Figure 1: Architecture of the Transonics system. The Dialogue Manager acts as the hub through which the
individual components interact.
</figureCaption>
<bodyText confidence="0.999857074074074">
our own large-scale simulated doctor-patient dia-
logue corpus based on recordings of medical stu-
dents examining standardized patients (details in
Belvin et al. 2004).1 The Farsi acoustic models r e-
quired an eclectic approach due to the lack of ex-
isting labeled speech corpora. The approach
included borrowing acoustic data from English by
means of developing a sub-phonetic mapping be-
tween the two languages, as detailed in (Srini-
vasamurthy &amp; Narayanan 2003), as well as use of
a small existing Farsi speech corpus (FARSDAT),
and our own team-internally generated acoustic
data. Language modeling data was also obtained
from multiple sources. The Defense Language
Institute translated approximately 600,000 words
of English medical dialogue data (including our
standardized patient data mentioned above), and in
addition, we were able to obtain usable Farsi text
from mining the web for electronic news sources.
Other smaller amounts of training data were ob
tained from various sources, as detailed in (Nara-
yanan et al. 2003, 2004). Additional detail on de-
velopment methods for all of these components,
system integration and evaluation can also be
found in the papers just cited.
The MT components, as noted, consist of both a
Classifier and a stochastic translation engine, both
</bodyText>
<footnote confidence="0.878338">
1 Standardized Patients are typically actors who have been
trained by doctors or nurses to portray symptoms of particular
illnesses or injuries. They are used extensively in medical
education so that doctors in training don&apos;t have to &amp;quot;practice&amp;quot;
on real patients.
</footnote>
<bodyText confidence="0.957586142857143">
developed by USC-ISI team members. The Eng-
lish Classifier uses approximately 1400 classes
consisting mostly of standard questions used by
medical care providers in medical interviews.
Each class has a large number of paraphrases asso-
ciated with it, such that if the care provider speaks
one of those phrases, the system will identify it
with the class and translate it to Farsi via table-
lookup. If the Classifier cannot succeed in finding
a match exceeding a confidence threshold, the sto-
chastic MT engine will be employed. The sto-
chastic MT engine relies on n-gram
correspondences between the source and target
languages. As with ASR, the performance of the
component is highly dependent on very large
amounts of training data. Again, there were multi-
ple sources of training data used, the most signifi-
cant being the data generated by our own team&apos;s
English collection effort, supported by translation
into Farsi by DLI. Further details of the MT com-
ponents can be found in Narayanan et al., op.cit.
</bodyText>
<sectionHeader confidence="0.990458" genericHeader="method">
3 Enabling Effective Communication
</sectionHeader>
<bodyText confidence="0.9999615">
The approach taken in the development of Tran-
sonics was what can be referred to as the total
communication pathway. We are not so concerned
with trying to maximize the performance of a
given component of the system, but rather with the
effectiveness of the system as a whole in facilitat-
ing actual communication. To this end, our design
and development included the following:
</bodyText>
<page confidence="0.990477">
90
</page>
<bodyText confidence="0.999348409090909">
i. an &amp;quot;educated guess&amp;quot; capability (system
guessing at the meaning of an utterance) from the
Classifier translation mechanism—this proved very
useful for noisy ASR output, especially for the re-
stricted domain of medical interviews.
ii. a flexible and robust SMT good for filling in
where the more accurate Classifier misses.
iii. exploitation of a partial n-best list as part of
the GUI used by the doctor/medic for the English
ASR component and the Farsi-to-English transla-
tion component.
iv. a dialog manager which in essence occa-
sionally makes &amp;quot;suggestions&amp;quot; (for next questions
for the doctor to ask) based on query sets which are
topically related to the query the system believes it
recognized the doctor to have spoken.
Overall, the system achieves a respectable level of
performance in terms of allowing users to follow a
conversational thread in a fairly coherent way, de-
spite the presence of frequent ungrammatical or
awkward translations (i.e. despite what we might
call non-catastrophic errors).
</bodyText>
<sectionHeader confidence="0.951075" genericHeader="method">
4 Testing and Evaluation
</sectionHeader>
<bodyText confidence="0.99935671830986">
In addition to our own laboratory tests, the sys-
tem was evaluated by MITRE as part of the
DARPA program. There were two parts to the
MITRE evaluations, a &amp;quot;live&amp;quot; part, designed pri-
marily to evaluate the overall task-oriented effec-
tiveness of the systems, and a &amp;quot;canned&amp;quot; part,
designed primarily to evaluate individual compo-
nents of the systems.
The live evaluation consisted of six medical
professionals (doctors, corpsmen and physician’s
assistants from the Naval Medical Center at Quan-
tico, and a nurse from a civilian institution) con-
ducting unrehearsed &amp;quot;focused history and physical
exam&amp;quot; style interactions with Farsi speakers play-
ing the role of patients, where the English-speaking
doctor and the Farsi-speaking patient communi-
cated by means of the Transonics system. Since
the cases were common enough to be within the
realm of general internal medicine, there was no
attempt to align ailments with medical specializa-
tions among the medical professionals.
MITRE endeavored to find primarily monolin-
gual Farsi speakers to play the role of patient, so as
to provide a true test of the system to enable com-
munication between people who would otherwise
have no way to communicate. This goal was only
partially realized, since one of the two Farsi patient
role-players was partially competent in English.2
The Farsi-speaking role-players were trained by a
medical education specialist in how to simulate
symptoms of someone with particular injuries or
illnesses. Each Farsi-speaking patient role-player
received approximately 30 minutes of training for
any given illness or injury. The approach was
similar to that used in training standardized pa-
tients, mentioned above (footnote 1) in connection
with generation of the dialogue corpus.
MITRE established a number of their own met-
rics for measuring the success of the systems, as
well as using previously established metrics. A
full discussion of these metrics and the results ob-
tained for the Transonics system is beyond the
scope of this paper, though we will note that one of
the most important of these was task-completion.
There were 5 significant facts (5 distinct facts for
each of 12 different scenarios) that the medical
professional should have discovered in the process
of interviewing/examining each Farsi patient. The
USC/HRL system averaged 3 out of the 5 facts,
which was a slightly above-average score among
the 4 systems evaluated. A &amp;quot;significant fact&amp;quot; con-
sisted of determining a fact which was critical for
diagnosis, such as the fact that the patient had been
injured in a fall down a stairway, the fact that the
patient was experiencing blurred vision, and so on.
Significant facts did not include items such as a
patient&apos;s age or marital status.3 We report on this
measure in that it is perhaps the single most im-
portant component in the assessment, in our opin-
ion, in that it is an indication of many aspects of
the system, including both directions of the trans-
lation system. That is, the doctor will very likely
conclude correct findings only if his/her question is
translated correctly to the patient, and also if the
patient&apos;s answer is translated correctly for the doc-
tor. In a true medical exam, the doctor may have
2 There were additional difficulties encountered as well, hav-
ing to do with one of the role-players not adequately grasping
the goal of role-playing. This experience highlighted the
many challenges inherent in simulating domain-specific
spontaneous dialogue.
</bodyText>
<footnote confidence="0.536183666666667">
3 Unfortunately, there was no baseline evaluation this could be
compared to, such as assessing whether any of the critical
facts could be determined without the use of the system at all.
</footnote>
<page confidence="0.997226">
91
</page>
<bodyText confidence="0.998886944444444">
other means of determining some critical facts
even in the absence of verbal communication, but
in the role-playing scenario described, this is very
unlikely. Although this measure is admittedly
coarse-grained, it simultaneously shows, in a crude
sense, that the USC/HRL system compared fa-
vorably against the other 3 systems in the evalua-
tion, and also that there is still significant room for
improvement in the state of the art.
As noted, MITRE devised a component evalua-
tion process also consisting of running 5 scripted
dialogs through the systems and then measuring
ASR and MT performance. The two primary
component measures were a version of BLEU for
the MT component (modified slightly to handle the
much shorter sentences typical of this kind of dia-
log) and a standard Word-Error Rate for the ASR
output. These scores are shown below.
</bodyText>
<tableCaption confidence="0.998447">
Table 1: Farsi BLEU Scores
</tableCaption>
<table confidence="0.98574825">
IBM BLEU IBM BLEU
ASR TEXT
English to Farsi 0.2664 0.3059
Farsi to English 0.2402 0.2935
</table>
<bodyText confidence="0.9975518">
The reason for the two different BLEU scores is
that one was calculated based on the ASR compo-
nent output being translated to the other language,
while the other was calculated from human tran-
scribed text being translated to the other language.
</bodyText>
<tableCaption confidence="0.988654">
Table 2: HRL/USC WER for Farsi and English
</tableCaption>
<table confidence="0.9402165">
English Farsi
WER 11.5% 13.4%
</table>
<sectionHeader confidence="0.998175" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9985533">
In this paper we have given an overview of the
design, implementation and evaluation of the Tran-
sonics speech-to-speech translation system for nar-
row domain two-way translation. Although there
are still many significant hurdles to be overcome
before this kind of technology can be called truly
robust, with appropriate training and two coopera-
tive interlocutors, we can now see some degree of
genuine communication being enabled. And this is
very encouraging indeed.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.987676666666667">
This work was supported primarily by the DARPA
CAST/Babylon program, contract N66001-02-C-
6023.
</bodyText>
<sectionHeader confidence="0.997139" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999932962962963">
R. Belvin, W. May, S. Narayanan, P. Georgiou, S. Gan-
javi. 2004. Creation of a Doctor-Patient Dialogue
Corpus Using Standardized Patients. In Proceedings of
the Language Resources and Evaluation Conference
(LREC), Lisbon, Portugal.
S. Ganjavi, P. G. Georgiou, and S. Narayanan. 2003.
Ascii based transcription schemes for languages with
the Arabic script: The case of Persian. In Proc. IEEE
ASRU, St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum and D. Wang. 2003. Transonics: A speech
to speech system for English-Persian Interactions,
Proc. IEEE ASRU, St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M.
Hein, S. Kadambe, K. Knight, D. Marcu, H. E.
Neely, N. Srinivasamurthy, D. Traum, and D. Wang.
2004. The Transonics Spoken Dialogue Translator:
An aid for English-Persian Doctor-Patient interviews,
in Working Notes of the AAAI Fall symposium on
Dialogue Systems for Health Communication, pp 97-
-103.
N. Srinivasamurthy, and S. Narayanan. 2003. Language
adaptive Persian speech recognition. In proceedings
of Eurospeech 2003.
</reference>
<page confidence="0.996024">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.211219">
<title confidence="0.9990415">Transonics: A Practical Speech-to-Speech Translator for English-Farsi Medical Dialogues</title>
<author confidence="0.911342">Emil Ettelaie</author>
<author confidence="0.911342">Sudeep Gandhe</author>
<author confidence="0.911342">Panayiotis Georgiou</author>
<author confidence="0.911342">Robert Belvin</author>
<affiliation confidence="0.481326">Kevin Knight, Daniel Marcu, Shrikanth Narayanan , HRL Laboratories, LLC</affiliation>
<address confidence="0.654552666666667">David Traum 3011 Malibu Canyon Rd. University of Southern California Malibu, CA 90265 Los Angeles, CA 90089 rsbelvin@hrl.com</address>
<email confidence="0.9588105">ettelaie@isi.edu,gandhe@ict.usc.edu,georgiou@sipi.usc.edu,knight@isi.edu,marcu@isi.edu,shri@sipi.usc.edu,traum@ict.use.edu</email>
<abstract confidence="0.997673461538461">We briefly describe a two-way speech-tospeech English-Farsi translation system prototype developed for use in doctorpatient interactions. The overarching philosophy of the developers has been to create a system that enables effective communication, rather than focusing on maximizing component-level performance. The discussion focuses on the general approach and evaluation of the system by an independent government evaluation team.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Belvin</author>
<author>W May</author>
<author>S Narayanan</author>
<author>P Georgiou</author>
<author>S Ganjavi</author>
</authors>
<title>Creation of a Doctor-Patient Dialogue Corpus Using Standardized Patients.</title>
<date>2004</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="3881" citStr="Belvin et al. 2004" startWordPosition="565" endWordPosition="568">ive Poster and Demonstration Sessions, pages 89–92, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics GUI: prompts, confirmations, ASR switch Dialog Manager Prompts or TTS Farsi ASR Farsi ASR English Prompts or TTS English MT English to Farsi Farsi to English SMT English to Farsi Farsi to English Figure 1: Architecture of the Transonics system. The Dialogue Manager acts as the hub through which the individual components interact. our own large-scale simulated doctor-patient dialogue corpus based on recordings of medical students examining standardized patients (details in Belvin et al. 2004).1 The Farsi acoustic models r equired an eclectic approach due to the lack of existing labeled speech corpora. The approach included borrowing acoustic data from English by means of developing a sub-phonetic mapping between the two languages, as detailed in (Srinivasamurthy &amp; Narayanan 2003), as well as use of a small existing Farsi speech corpus (FARSDAT), and our own team-internally generated acoustic data. Language modeling data was also obtained from multiple sources. The Defense Language Institute translated approximately 600,000 words of English medical dialogue data (including our stan</context>
</contexts>
<marker>Belvin, May, Narayanan, Georgiou, Ganjavi, 2004</marker>
<rawString>R. Belvin, W. May, S. Narayanan, P. Georgiou, S. Ganjavi. 2004. Creation of a Doctor-Patient Dialogue Corpus Using Standardized Patients. In Proceedings of the Language Resources and Evaluation Conference (LREC), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ganjavi</author>
<author>P G Georgiou</author>
<author>S Narayanan</author>
</authors>
<title>Ascii based transcription schemes for languages with the Arabic script: The case of Persian.</title>
<date>2003</date>
<booktitle>In Proc. IEEE ASRU,</booktitle>
<publisher>St. Thomas, U.S. Virgin Islands.</publisher>
<marker>Ganjavi, Georgiou, Narayanan, 2003</marker>
<rawString>S. Ganjavi, P. G. Georgiou, and S. Narayanan. 2003. Ascii based transcription schemes for languages with the Arabic script: The case of Persian. In Proc. IEEE ASRU, St. Thomas, U.S. Virgin Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>S Ananthakrishnan</author>
<author>R Belvin</author>
<author>E Ettelaie</author>
<author>S Ganjavi</author>
<author>P Georgiou</author>
<author>C Hein</author>
<author>S Kadambe</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>H Neely</author>
<author>N Srinivasamurthy</author>
<author>D Traum</author>
<author>D Wang</author>
</authors>
<title>Transonics: A speech to speech system for English-Persian Interactions,</title>
<date>2003</date>
<booktitle>Proc. IEEE ASRU,</booktitle>
<publisher>St. Thomas, U.S. Virgin Islands.</publisher>
<contexts>
<context position="4741" citStr="Narayanan et al. 2003" startWordPosition="699" endWordPosition="703">ges, as detailed in (Srinivasamurthy &amp; Narayanan 2003), as well as use of a small existing Farsi speech corpus (FARSDAT), and our own team-internally generated acoustic data. Language modeling data was also obtained from multiple sources. The Defense Language Institute translated approximately 600,000 words of English medical dialogue data (including our standardized patient data mentioned above), and in addition, we were able to obtain usable Farsi text from mining the web for electronic news sources. Other smaller amounts of training data were ob tained from various sources, as detailed in (Narayanan et al. 2003, 2004). Additional detail on development methods for all of these components, system integration and evaluation can also be found in the papers just cited. The MT components, as noted, consist of both a Classifier and a stochastic translation engine, both 1 Standardized Patients are typically actors who have been trained by doctors or nurses to portray symptoms of particular illnesses or injuries. They are used extensively in medical education so that doctors in training don&apos;t have to &amp;quot;practice&amp;quot; on real patients. developed by USC-ISI team members. The English Classifier uses approximately 140</context>
</contexts>
<marker>Narayanan, Ananthakrishnan, Belvin, Ettelaie, Ganjavi, Georgiou, Hein, Kadambe, Knight, Marcu, Neely, Srinivasamurthy, Traum, Wang, 2003</marker>
<rawString>S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ettelaie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe, K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy, D. Traum and D. Wang. 2003. Transonics: A speech to speech system for English-Persian Interactions, Proc. IEEE ASRU, St. Thomas, U.S. Virgin Islands.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Narayanan</author>
<author>S Ananthakrishnan</author>
<author>R Belvin</author>
<author>E Ettelaie</author>
<author>S Gandhe</author>
<author>S Ganjavi</author>
<author>P G Georgiou</author>
<author>C M Hein</author>
<author>S Kadambe</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>H E Neely</author>
<author>N Srinivasamurthy</author>
<author>D Traum</author>
<author>D Wang</author>
</authors>
<title>The Transonics Spoken Dialogue Translator: An aid for English-Persian Doctor-Patient interviews,</title>
<date>2004</date>
<booktitle>in Working Notes of the AAAI Fall symposium on Dialogue Systems for Health Communication,</booktitle>
<pages>97--103</pages>
<marker>Narayanan, Ananthakrishnan, Belvin, Ettelaie, Gandhe, Ganjavi, Georgiou, Hein, Kadambe, Knight, Marcu, Neely, Srinivasamurthy, Traum, Wang, 2004</marker>
<rawString>S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ettelaie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M. Hein, S. Kadambe, K. Knight, D. Marcu, H. E. Neely, N. Srinivasamurthy, D. Traum, and D. Wang. 2004. The Transonics Spoken Dialogue Translator: An aid for English-Persian Doctor-Patient interviews, in Working Notes of the AAAI Fall symposium on Dialogue Systems for Health Communication, pp 97--103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Srinivasamurthy</author>
<author>S Narayanan</author>
</authors>
<title>Language adaptive Persian speech recognition.</title>
<date>2003</date>
<booktitle>In proceedings of Eurospeech</booktitle>
<contexts>
<context position="4174" citStr="Srinivasamurthy &amp; Narayanan 2003" startWordPosition="612" endWordPosition="616">h SMT English to Farsi Farsi to English Figure 1: Architecture of the Transonics system. The Dialogue Manager acts as the hub through which the individual components interact. our own large-scale simulated doctor-patient dialogue corpus based on recordings of medical students examining standardized patients (details in Belvin et al. 2004).1 The Farsi acoustic models r equired an eclectic approach due to the lack of existing labeled speech corpora. The approach included borrowing acoustic data from English by means of developing a sub-phonetic mapping between the two languages, as detailed in (Srinivasamurthy &amp; Narayanan 2003), as well as use of a small existing Farsi speech corpus (FARSDAT), and our own team-internally generated acoustic data. Language modeling data was also obtained from multiple sources. The Defense Language Institute translated approximately 600,000 words of English medical dialogue data (including our standardized patient data mentioned above), and in addition, we were able to obtain usable Farsi text from mining the web for electronic news sources. Other smaller amounts of training data were ob tained from various sources, as detailed in (Narayanan et al. 2003, 2004). Additional detail on dev</context>
</contexts>
<marker>Srinivasamurthy, Narayanan, 2003</marker>
<rawString>N. Srinivasamurthy, and S. Narayanan. 2003. Language adaptive Persian speech recognition. In proceedings of Eurospeech 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>