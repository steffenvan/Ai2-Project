<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9992755">
A generative model for unsupervised discovery of relations and argument
classes from clinical texts
</title>
<author confidence="0.994128">
Bryan Rink and Sanda Harabagiu
</author>
<affiliation confidence="0.9950605">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.934532">
Richardson, TX, USA
</address>
<email confidence="0.999826">
{bryan,sanda}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.996676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999820714285714">
This paper presents a generative model for
the automatic discovery of relations between
entities in electronic medical records. The
model discovers relation instances and their
types by determining which context tokens ex-
press the relation. Additionally, the valid se-
mantic classes for each type of relation are de-
termined. We show that the model produces
clusters of relation trigger words which bet-
ter correspond with manually annotated re-
lations than several existing clustering tech-
niques. The discovered relations reveal some
of the implicit semantic structure present in
patient records.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990668627451">
Semantic relations in electronic medical records
(EMRs) capture important meaning about the as-
sociations between medical concepts. Knowledge
about how concepts such as medical problems, treat-
ments, and tests are related can be used to improve
medical care by speeding up the retrieval of relevant
patient information or alerting doctors to critical in-
formation that may have been overlooked. When
doctors write progress notes and discharge sum-
maries they include information about how treat-
ments (e.g., aspirin, stent) were administered for
problems (e.g. pain, lesion) along with the out-
come, such as an improvement or deterioration. Ad-
ditionally, a doctor will describe the tests (e.g., x-
ray, blood sugar level) performed on a patient and
whether the tests were conducted to investigate a
known problem or revealed a new one. These textual
descriptions written in a patient’s record encode im-
portant information about the relationships between
the problems a patients has, the treatments taken for
the problems, and the tests which reveal and investi-
gate the problems.
The ability to accurately detect semantic rela-
tions in EMRs, such as Treatment-Administered-for-
Problem, can aid in querying medical records. Af-
ter a preprocessing phase in which the relations are
detected in all records they can be indexed and re-
trieved later as needed. A doctor could search for
all the times that a certain treatment has been used
on a particular problem, or determine all the treat-
ments used for a specific problem. An additional
application is the use of the relational information
to flag situations that merit further review. If a pa-
tient’s medical record indicates a test that was found
to reveal a critical problem but no subsequent treat-
ment was performed for the problem, the patient’s
record could be flagged for review. Similarly, if
a Treatment-Worsens-Problem relation is detected
previously in a patient’s record, that information can
be brought to the attention of a doctor who advises
such a treatment in the future. By considering all
of the relations present in a corpus, better medical
ontologies could be built automatically or existing
ones can be improved by adding additional connec-
tions between concepts that have a relation in text.
Given the large size of EMR repositories, we ar-
gue that it is quite important to have the ability to
perform relation discovery between medical con-
cepts. Relations between medical concepts benefit
translational medicine whenever possible relations
are known. Uzuner et al. (2011) show that super-
</bodyText>
<page confidence="0.975222">
519
</page>
<note confidence="0.9579155">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 519–528,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99981647826087">
vised methods recognize such relations with high ac-
curacy. However, large sets of annotated relations
need to be provided for this purpose. To address
both the problem of discovering unknown relations
between medical concepts and the related problem
of generating examples for known relations, we have
developed an unsupervised method. This approach
has the advantages of not requiring an expensive an-
notation effort to provide training data for seman-
tic relations, which is particularly difficult for medi-
cal records, characterized by many privacy concerns.
Our analysis shows a high level of overlap between
the manually annotated relations and those that were
discovered automatically. Our experimental results
show that this approach improves upon simpler clus-
tering techniques.
The remainder of this paper is organized as fol-
lows. Section 2 discusses the related work. Section
3 reports our novel generative model for discovering
relations in EMRs, Section 4 details the inference
and parameter estimation of our method. Section
5 details our experiments, Section 6 discusses our
findings. Section 7 summarizes the conclusions.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999014">
Previous methods for unsupervised relation dis-
covery have also relied on clustering techniques.
One technique uses the context of entity arguments
to cluster, while another is to perform a post-
processing step to cluster relations found using an
existing relation extraction system. The approaches
most similar to ours have taken features from the
context of pairs of entities and used those features to
form a clustering space. In Hasegawa et al. (2004),
those features are tokens found within a context win-
dow of the entity pair. Distance between entity pairs
is then computed using cosine similarity. In another
approach, Rosenfeld and Feldman (2007) use hierar-
chical agglomerative clustering along with features
based on token patterns seen in the context, again
compared by cosine similarity.
Other approaches to unsupervised relation dis-
covery have relied on a two-step process where a
number of relations are extracted, usually from a
predicate-argument structure. Then similar relations
are clustered together since synonymous predicates
should be considered the same relation (e.g. “ac-
quire” and “purchase”). Yates (2009) considers the
output from an open information extraction system
(Yates et al., 2007) and clusters predicates and argu-
ments using string similarity and a combination of
constraints. Syed and Viegas (2010) also perform a
clustering on the output of an existing relation ex-
traction system by considering the number of times
two relations share the same exact arguments. Sim-
ilar relations are expected to have the same pairs
of arguments (e.g. “Ford produces cars” and “Ford
manufactures cars”). These approaches and others
(Agichtein and Gravano, 2000; Pantel and Pennac-
chiotti, 2006) rely on an assumption that relations
are context-independent, such as when a person is
born, or the capital of a nation. Our method will
discover relations that can depend on the context as
well. For instance, “penicillin” may be causally re-
lated to “allergic reaction” in one patient’s medical
record but not in another. The relation between the
two entities is not globally constant and should be
considered only within the scope of one patient’s
records.
Additionally, these two-step approaches tend
to rely on predicate-argument structures such as
subject-verb-object triples to detect arbitrary rela-
tions (Syed and Viegas, 2010; Yates et al., 2007).
Such approaches can take advantage of the large
body of research that has been done on extracting
syntactic parse structure and semantic role infor-
mation from text. However, these approaches can
overlook relations in text which do not map easily
onto those structures. Unlike these approaches, our
model can detect relations that are not expressed as
a verb, such as “[cough] + [green sputum]” to ex-
press a conjunction or “[Cl] 119 mEq / L [High]” to
express that a test reading is indicating a problem.
The 2010 i2b2/VA Challenge (Uzuner et al.,
2011) developed a set of annotations for medical
concepts and relations on medical progress notes
and discharge summaries. One task at the challenge
involved developing systems for the extraction of
eight types of relations between concepts. We use
this data set to compare our unsupervised method
with others.
The advantage of our work over existing unsu-
pervised approaches is the simultaneous clustering
of both argument words and relation trigger words.
These broad clusters handle: (i) synonyms, (ii) argu-
</bodyText>
<page confidence="0.990306">
520
</page>
<bodyText confidence="0.996652">
ment semantic classes, and (iii) words belonging to
the same relation.
</bodyText>
<sectionHeader confidence="0.9976965" genericHeader="method">
3 A Generative Model for Discovering
Relations
</sectionHeader>
<subsectionHeader confidence="0.999609">
3.1 Unsupervised Relation Discovery
</subsectionHeader>
<bodyText confidence="0.998042423076923">
A simple approach to discovering relations between
medical entities in clinical texts uses a clustering ap-
proach, e.g. Latent Dirichlet Allocation (LDA) (Blei
et al., 2003). We start with an assumption that rela-
tions exist between two entities, which we call argu-
ments, and may be triggered by certain words be-
tween those entities which we call trigger words.
For example, given the text “[x-ray] revealed [lung
cancer]”, the first argument is x-ray, the second ar-
gument is lung cancer, and the trigger word is re-
vealed. We further assume that the arguments must
belong to a small set of semantic classes specific to
the relation. For instance, x-ray belongs to a class
of medical tests, whereas lung cancer belongs to a
class of medical problems. While relations may ex-
ist between distant entities in text, we focus on those
pairs of entities in text which have no other entities
between them. This increases the likelihood of a re-
lation existing between the entities and minimizes
the number of context words (words between the en-
tities) that are not relevant to the relation.
With these assumptions we build a baseline rela-
tion discovery using LDA. LDA is used as a baseline
because of its similarities with our own generative
model presented in the next section. Each consec-
utive pair of entities in text is extracted, along with
the tokens found between them. Each of the entities
in a pair is split into tokens which are taken along
with the context tokens to form a single pseudo-
document. When the LDA is processed on all such
pseudo-documents, clusters containing words which
co-occur are formed. Our assumption that relation
arguments come from a small set of semantic classes
should lead to clusters which align with relations
since the two arguments of a relation will co-occur
in the pseudo-documents. Furthermore, those argu-
ment tokens should co-occur with relation trigger
words as well.
This LDA-based approach was examined on elec-
tronic medical records from the 2010 i2b2/VA Chal-
lenge data set (Uzuner et al., 2011). The data set
Cluster 1
Words: secondary, due, likely, patient, disease,
liver, abdominal, cancer, pulmonary, respiratory,
elevated, volume, chronic, edema, related
“Correct” instances: [Metastatic colon cancer]
with [abdominal carcinomatosis]; [symptoms]
were due to [trauma]
“Incorrect” instances: [mildly improving symp-
toms] , plan will be to continue with [his cur-
rent medicines]; [prophylaxis] against [peptic ul-
cer disease]
</bodyText>
<equation confidence="0.449317">
Cluster 2:
</equation>
<bodyText confidence="0.998270666666667">
Words: examination, no, positive, culture, exam,
blood, patient, revealed, cultures, physical, out,
urine, notable, showed, cells
“Correct” instances: [a blood culture] grew out
[Staphylococcusaureus]; [tamponade] by [exam-
ination]
“Incorrect” instances: [the intact drain] drain-
ing [bilious material]; [a Pseudomonas cellulitis]
and [subsequent sepsis]
</bodyText>
<figureCaption confidence="0.793554">
Figure 1: Two clusters found by examining the most
likely words under two LDA topics. The instances are
pseudo-documents whose probability of being assigned
to that cluster was over 70%
</figureCaption>
<bodyText confidence="0.999949428571429">
contains manually annotated medical entities which
were used to form the pairs of entities needed. For
example, Figure 1 illustrates examples of two clus-
ters out of 15 discovered automatically using LDA
on the corpus. The first cluster appears to contain
words which indicate a relation whose two argu-
ments are both medical problems (e.g. “disease”,
“cancer”, “edema”). The trigger words seem to in-
dicate a possible causal relation (e.g., “due”, “re-
lated”, “secondary”). The second cluster contains
words relevant to medical tests (e.g. “examination”,
“culture”) and their findings (“revealed”, “showed”,
“positive”). As illustrated in Figure 1, some of the
context words are not necessarily related to the re-
lation. The word “patient” for instance is present
in both clusters but is not a trigger word because
it is likely to be seen in the context of any rela-
tion in medical text. The LDA-based model treats
all words equally and cannot identify which words
are likely trigger words and which ones are general
words, which merely occur frequently in the context
</bodyText>
<page confidence="0.986575">
521
</page>
<bodyText confidence="0.999613684210527">
of a relation.
In addition, while the LDA approach can de-
tect argument words which co-occur with trigger
words (e.g., “examination” and “showed”), the clus-
ters produced with LDA do not differentiate between
contextual words and words which belong to the ar-
guments of the relation. An approach which mod-
els arguments separately from context words could
learn the semantic classes of those arguments and
thus better model relations. Considering the exam-
ples from Figure 1, a model which could cluster
“examination”, “exam”, “cultures”, and “culture”
into one medical test cluster and “disease”, “cancer”
and “edema” into a medical problem cluster separate
from the relation trigger words and general words
should model relations more accurately by better re-
flecting the implicit structure of the text. Because of
these limitations many relations discovered in this
way are not accurate, as can be seen in Figure 1.
</bodyText>
<subsectionHeader confidence="0.998826">
3.2 Relation Discovery Model (RDM)
</subsectionHeader>
<bodyText confidence="0.984249444444444">
The limitations identified in the LDA-based ap-
proach are solved by a novel relation discovery
model (RDM) which jointly models relation argu-
ment semantic classes and considers them separately
from the context words. Relations triggered by pairs
of medical entities enable us to consider three ob-
servable features: (A1) the first argument; (A2)
the second argument; and (CW) the context words
found between A1 and A2.
For instance, in sentence S1 the arguments are
A1=“some air hunger” and A2=“his tidal volume”
while the context words are “last”, “night”, “when”,
“I”, and “dropped”.
S1: He developed [some air hunger]PROB last night
when I dropped [his tidal volume]TREAT from 450
to 350.
In the RDM, the contextual words are assumed to
come from a mixture model with 2 mixture compo-
nents: a relation trigger word (x = 0), or a general
word (x = 1), where x is a variable representing
which mixture component a word belongs to. In
sentence S1 for example, the word “dropped” can
be seen as a trigger word for a Treatment-Causes-
Problem relation. The remaining words are not trig-
ger words and hence are seen as general words.
Under the RDM’s mixture model, the probability
of a context word is:
</bodyText>
<equation confidence="0.994505333333333">
P(wC|tr, z) =
P(wC|tr, x = 0) x P(x = 0|tr) +
P(wC|z,x = 1) x P(x = 1|tr)
</equation>
<bodyText confidence="0.99943348">
Where wC is a context word, the variable tr is
the relation type, and z is the general word class.
The variable x chooses whether a context word
comes from a relation-specific distribution of trig-
ger words, or from a general word class. In the
RDM, the two argument classes are modeled jointly
as P(c1, c2|tr), where c1 and c2 are two semantic
classes associated with a relation of type tr. How-
ever the assignment of classes to arguments depends
on a directionality variable d. If d = 0, then the first
argument is assigned semantic class c1 and the sec-
ond is assigned class c2. When d = 1 however, the
class assignments are swapped. This models the fact
that a relation’s arguments do not come in a fixed
order, “[MRI] revealed [tumor]” is the same type of
relation as “[tumor] was revealed by [x-ray]”. Fig-
ure 2 shows the graphical model for the RDM. Each
candidate relation is modeled independently, with a
total of I relation candidates. Variable w1 is a word
observed from the first argument, and w2 is a word
observed from the second argument. The model
takes parameters for the number of relations types
(R), the number of argument semantic classes (A),
and the number of general word classes (K). The
generative process for the RDM is:
</bodyText>
<listItem confidence="0.9991692">
1. For relation type r = 1..R:
(a) Draw a binomial distribution ur from
Beta(αx) representing the mixture distri-
bution for relation r
(b) Draw a joint semantic class distribution
</listItem>
<equation confidence="0.569199">
ψ1,2
r E RC×C from Dirichlet(α1,2).
</equation>
<listItem confidence="0.9860802">
2. Draw a categorical word distribution φzz′ from
Dirichlet(6z) for each general word class
z′ = 1..K
3. Draw a categorical word distribution φrr′ from
Dirichlet(6r) for each r′ = 1..R
4. for semantic class a′ = 1..A:
(a) Draw categorical word distributions
ω1a′ and ω2a′ from Dirichlet(61) and
Dirichlet(62) for the first and second
arguments, respectively.
</listItem>
<page confidence="0.955609">
522
</page>
<figure confidence="0.999626833333333">
α1,2
αz
αx
αr
αd
ψ1,2
δ R
σ
ρ
R
R
tr
θ
c1,2
d
z
x
we
w1
w2
We
W1
W2
I
ω1
ω2
φz
φr
K
R
A
A
β1
β2
βz
βr
</figure>
<figureCaption confidence="0.994253">
Figure 2: Graphical model for the RDM. c1,2 represents the joint generation of c1 and c2
</figureCaption>
<equation confidence="0.987836111111111">
P(tr, d|tr −i, d−i, c1,2
−i , x−i, z−i, wC −i, w1 −i, w2 −ii α,β) a u1 X u2 X u3
__ f (tr)+αr f (tr ,d)+αd f (tr c1 c2)+α1,2
u1 I+Rαr X f(tr)+αd+αd X f(tr)+C×Cα1,2 qq qq
_ 7�7WC fi (zj)+αz f(tr,xi)+αx f(tr,w� )+Nr f(zj,wj )+Nz
u2 — 117 WC+KαzgqX f(tr)+2αx X (1x=
q0 f(tr)+Wβr + 1x=1 f(zj)+Wβz )
_ 7�7W1 f(a1,wj)+N1 W2 f(a2,w2j)+N2
u3 — 117 f(a1)+Wβ1 X 11j f(a2)+Wβ2
</equation>
<figureCaption confidence="0.97666575">
Figure 3: Gibbs sampling update equation for variables tr and d for the ith relation candidate. The variables a1 = c1
and a2 = c2 if d = 0, or a1 = c2 and a2 = c1 if d = 1. W is the size of the vocabulary. f(•) is the count of
the number of times that event occurred, excluding assignments for the relation instance being sampled. For instance,
f(tr, d) = EIk#i I[trk = tri ∧ dk = di]
</figureCaption>
<listItem confidence="0.998920833333333">
5. Draw a categorical relation type distribution ρ
from Dirichlet(αr)
6. For each pair of consecutive entities in the cor-
pus, i = 1..I:
(a) Sample a relation type tr from ρ
(b) Jointly sample semantic classes c1 and c2
</listItem>
<bodyText confidence="0.704112666666667">
for the first and second arguments from
ψ1,2
tr
</bodyText>
<listItem confidence="0.962127571428571">
(c) Draw a general word class categorical dis-
tribution θ from Dirichlet(αz)
(d) For each token j = 1..W1 in the first ar-
gument: Sample a word w1j from ω1c1 if
d = 0 or ω2c2 if d = 1
(e) For each token j = 1..W2 in the second
argument: Sample a word w2 jfrom ω2c2 if
d = 0 or ω1c1 ifd= 1
(f) For each token j = 1..WC in the context
of the entities:
i. Sample a general word class z from θ
ii. Sample a mixture component x from
σtr
iii. Sample a word from φrtr if x = 0 or
</listItem>
<equation confidence="0.676262">
φzz if x = 1.
</equation>
<bodyText confidence="0.9994509375">
In the RDM, words from the arguments are in-
formed by the relation through an argument seman-
tic class which is sampled from P(c1, c2|tr) =ψ1,2
tr .
Furthermore, words from the context are informed
by the relation type. These dependencies enable
more coherent relation clusters to form during pa-
rameter estimation because argument classes and re-
lation trigger words are co-clustered.
We chose to model two distinct sets of entity
words (ω1 and ω2) depending on whether the entity
occurred in the first argument or the second argu-
ment of the relation. The intuition for using disjoint
sets of entities is based on the observation that an
entity may be expressed differently if it comes first
or second in the text.
</bodyText>
<sectionHeader confidence="0.996808" genericHeader="method">
4 Inference and Parameter Estimation
</sectionHeader>
<bodyText confidence="0.9999705">
Assignments to the hidden variables in RDM can
be made by performing collapsed Gibbs sampling
(Griffiths and Steyvers, 2004). The joint probability
of the data is:
</bodyText>
<page confidence="0.957067">
523
</page>
<equation confidence="0.9958825">
P(wC, w1, w2; α,β) a
P(σ|αx)P(ρ|αr)P(δ|αd)P(ψ1,2|α1,2)
XP(φz|βz)P(φr|βr)P(ω1|β1)P(ω2|β2)
X HIi [P(θi|αz)P(tri |ρ)P(di|tr, δtr)P(c1i , c2i |tr, ψ1,2)
X 7H WC,i P(zi,j |θi)P(xi,j  |tri , σtri )P(wCi,j  |xi,j,tri,zi,j )
X 7HW1,i P(w1j  |di, cz,2, ω1)
X �W2,i P(w2j |di, c1,2
i ,ω2)]
</equation>
<bodyText confidence="0.999817285714286">
We need to sample variables tr, d, c1,2, x, and
z. We sample tr and d jointly while each of the
other variables is sampled individually. After
integrating out the multinomial distributions, we
can sample tr and d from the equation in Figure 3
The update equations for the remaining variables
can be derived from the same equation by dropping
terms which are constant across changes in that vari-
able.
In our experiments the hyperparameters were set
to αx = 1.0, αz = 1.0, α1,2 = 1.0, αd 0 = 2, αd1 =
1, βr = 0.01,βz = 0.01, β1 = 1.0, β2 = 1.0.
Changing the hyperparameters did not significantly
affect the results.
</bodyText>
<sectionHeader confidence="0.993138" genericHeader="method">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.898264">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99947295">
We evaluated the RDM using a corpus of electronic
medical records provided by the 2010 i2b2/VA
Challenge (Uzuner et al., 2011). We used the
training set, which consists of 349 medical records
from 4 hospitals, annotated with medical concepts
(specifically problems, treatments, and tests),
along with any relations present between those
concepts. We used these manually annotated
relations to evaluate how well the RDM performs
at relation discovery. The corpus is annotated
with a set of eight relations: Treatment-Addresses-
Problem, Treatment-Causes-Problem, Treatment-
Improves-Problem, Treatment-Worsens-Problem,
Treatment-Not-Administered-due-to-Problem, Test-
Reveals-Problem, Test-Conducted-for-Problem, and
Problem-Indicates-Problem. The data contains
13,460 pairs of consecutive concepts, of which
3,613 (26.8%) have a relation belonging to the list
above. We assess the model using two versions of
this data set consisting of: those pairs of consecutive
</bodyText>
<table confidence="0.9984776875">
Relation 1 Relation 2 Relation 3 Relation 4
mg ( due showed
p.r.n. ) consistent no
p.o. Working not revealed
hours ICD9 likely evidence
prn Problem secondary done
q Diagnosis patient 2007
needed 30 ( performed
day cont started demonstrated
q. ): most without
4 closed s/p normal
2 SNMCT seen shows
every **ID-NUM related found
one PRN requiring showing
two mL including negative
8 ML felt well
</table>
<figureCaption confidence="0.998185">
Figure 4: Relation trigger words found by the RDM
</figureCaption>
<bodyText confidence="0.999973636363636">
entities which have a manually annotated relation
(DS1), and secondly, all consecutive pairs of entities
(DS2). DS1 allows us to assess the RDM’s cluster-
ing without the noise introduced from those pairs
lacking a true relation. Evaluations on DS2 will
indicate the level of degradation caused by large
numbers of entity pairs that have no true relation.
We also use a separate test set to assess how well
the model generalizes to new data. The test set
contains 477 documents comprising 9,069 manually
annotated relations.
</bodyText>
<subsectionHeader confidence="0.997876">
5.2 Analysis
</subsectionHeader>
<bodyText confidence="0.999255555555556">
Figure 4 illustrates four of the fifteen trigger word
clusters (most likely words according to φr) learned
from dataset DS1 using the best set of parameters
according to normalized mutual information (NMI)
as described in section 5.3. These parameters were:
R = 9 relations, K = 15 general word classes, and
A = 15 argument classes. Examination of the most
likely words reveals a variety of trigger words, be-
yond obvious explicit ones. Example sentences for
the relation types from Figure 4 are presented in Fig-
ure 5 and discussed below.
Relation Type 1
Instances of this discovered relation are often found
embedded in long lists of drugs prescribed to the
patient. Tokens such as “p.o.” and “p.r.n.”, mean-
ing respectively “by mouth” and “when necessary”,
are indicative of a prescription relation. The learned
relation specifically considers arguments of a drug
</bodyText>
<page confidence="0.994024">
524
</page>
<subsectionHeader confidence="0.625336">
Instances of Relation Type 1
</subsectionHeader>
<listItem confidence="0.990854">
1. Haldol 0.5-1 milligrams p.o. q.6-8h. p.r.n. agitation
2. plavix every day to prevent failure of these stents
3. KBL mouthwash, 15 ccp .o. q.d. prn mouth discomfort
4. Miconazole nitrate powder tid prn for groin rash
5. AmBisome 300 mg IV q.d. for treatment of her hepatic candidiasis
</listItem>
<subsectionHeader confidence="0.472537">
Instances of Relation Type 2
</subsectionHeader>
<listItem confidence="0.9983324">
1. MAGNESIUM HYDROXIDE SUSP 30 ML ) , 30 mL , Susp , By Mouth, At Bedtime, PRN, For Constipation
2. Depression, major ( ICD9 296.00 , Working, Problem) cont NOS home meds
3. Diabetes mellitus type II ( ICD9 250.00 , Working, Problem ) cont home meds
4. ASCITES ( ICD9 789.5 , Working, Diagnosis) on spironalactone
5. *Dilutional hyponatremia ( SNMCT **ID-NUM , Working, Diagnosis) improved with fluid restriction
</listItem>
<subsectionHeader confidence="0.705898">
Instances of Relation Type 3
</subsectionHeader>
<listItem confidence="0.9905718">
1. ESRD secondary to her DM
2. slightly lightheaded and with increased HR
3. a 40% RCA, which was hazy
4. echogenic kidneys consistent with renal parenchymal disease
5. *Librium for alcohol withdrawal
</listItem>
<subsectionHeader confidence="0.520809">
Instances of Relation Type 4
</subsectionHeader>
<listItem confidence="0.996922333333333">
1. V-P lung scan was performed on May 24 2007 , showed low probability of PE
2. a bedside transthoracic echocardiogram done in the Cardiac Catheterization laboratory without evidence of
an effusion
3. exploration of the abdomen revealed significant nodularity of the liver
4. Echocardiogram showed moderate dilated left atrium
5. An MRI of the right leg was done which was equivocal for osteomyelitis
</listItem>
<figureCaption confidence="0.981887">
Figure 5: Examples for four of the discovered relations. Those marked with an asterisk have a different manually
chosen relation than the others
</figureCaption>
<bodyText confidence="0.983272666666667">
and a symptom treated by that drug. The closest
manually chosen relation is Treatment-Addresses-
Problem which included drugs as treatments.
</bodyText>
<sectionHeader confidence="0.421391" genericHeader="method">
Relation Type 2
</sectionHeader>
<bodyText confidence="0.961604236842105">
Relation 2 captures a similar kind of relation to Re-
lation 1. All five examples for Relation 1 in Fig-
ure 5 came from a different set of hospitals than the
examples for Relation 2. This indicates the model
is detecting stylistic differences in addition to se-
mantic differences. This is one of shortcomings of
simple generative models. Because they cannot re-
flect the true underlying distribution of the data they
will model the observations in ways that are irrel-
evant to the task at hand. Relation 2 also contains
certain punctuation, such as parentheses which the
examples show are used to delineate a treatment
code. Instances of Relation 2 were often marked
as Treatment-Addresses-Problem relations by anno-
tators.
Relation Type 3
The third relation captures problems which are re-
lated to each other. The manual annotations contain
a very similar relation called Problem-Indicates-
Problem. This relation is also similar to Cluster 1
from Section 3.1, however under the RDM the words
are much more specific to the relation. This relation
is difficult to discover accurately because of the in-
frequent use of strong trigger words to indicate the
relation. Instead, the model must rely more on the
semantic classes of the arguments, which in this case
will both be types of medical problems.
Relation Type 4
The fourth relation is detecting instances where a
medical test has revealed some problem. This cor-
responds to the Test-Reveals-Problem relation from
the data. Many good trigger words for that relation
have high probability under Relation 4. A compar-
ison of the RDM’s Relation 4 with LDA’s cluster 2
from Figure 1 shows that many words not relevant
to the relation itself are now absent.
Argument classes
Figure 6 shows the 3 most frequent semantic classes
</bodyText>
<page confidence="0.991336">
525
</page>
<table confidence="0.994021545454545">
Concept 1 Concept 2 Concept 3
CT pain Percocet
scan disease Hgb
chest right Hct
x-ray left Anion
examination renal Vicodin
Chest patient RDW
EKG artery Bili
MRI - RBC
culture symptoms Ca
head mild Gap
</table>
<figureCaption confidence="0.984428">
Figure 6: Concept words found by the RDM
</figureCaption>
<bodyText confidence="0.999980882352941">
for the first argument of a relation (ω1). Most of the
other classes were assigned rarely, accounting for
only 19% of the instances collectively. Human an-
notators of the data set chose three argument classes:
Problems, Treatments, and Tests. Concept 1 aligns
closely with a test semantic class. Concept 2 seems
to be capturing medical problems and their descrip-
tions. Finally, Concept 3 appears to be a combina-
tion of treatments (drugs) and tests. Tokens such as
“Hgb”, “Hct”, “Anion”, and “RDW” occur almost
exclusively in entities marked as tests by annotators.
It is not clear why this cluster contains both types
of words, but many of the high ranking words be-
yond the top ten do correspond to treatments, such as
“Morphine”, “Albumin”, “Ativan”, and “Tylenol”.
Thus the discovered argument classes show some
similarity to the ones chosen by annotators.
</bodyText>
<subsectionHeader confidence="0.941368">
5.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.978406266666667">
For a more objective analysis of the relations de-
tected, we evaluated the discovered relation types
by comparing them with the manually annotated
ones from the data using normalized mutual infor-
mation (NMI) (Manning et al., 2008). NMI is an
information-theoretic measure of the quality of a
clustering which indicates how much information
about the gold classes is obtained by knowing the
clustering. It is normalized to have a range from 0.0
to 1.0. It is defined as:
[H(Ω) + H(C)]/2
where Ω is the system-produced clustering, C is the
gold clustering, I is the mutual information, and H
is the entropy. The mutual information of two clus-
terings can be defined as:
</bodyText>
<equation confidence="0.985469">
|ωk ∩ cj |N|ωk ∩ cj|
N log2 |ωk||cj|
</equation>
<bodyText confidence="0.996934">
where N is the number of items in the clustering.
The entropy is defined as
</bodyText>
<equation confidence="0.983076">
|ωk ||ωk|
N log2 N
</equation>
<bodyText confidence="0.999746428571428">
The reference clusters consist of all relations an-
notated with the same relation type. The predicted
clusters consist of all relations which were assigned
the same relation type.
In addition to NMI, we also compute the F mea-
sure (Amig´o et al., 2009). The F measure is com-
puted as:
</bodyText>
<sectionHeader confidence="0.369292" genericHeader="method">
maxj{F (Li, Cj)}
</sectionHeader>
<bodyText confidence="0.975744">
where
</bodyText>
<equation confidence="0.995211666666667">
2 × Recall(Li, Cj) × Precision(Li, Cj)
F (Li, Cj) =
Recall(Li, Cj) + Precision(Li, Cj)
</equation>
<bodyText confidence="0.691345">
and Precision is defined as:
</bodyText>
<equation confidence="0.8109742">
Precision(Ci, Lj) = |Ci ∩ Lj|
|Ci|
while Recall is simply precision with the arguments
swapped:
Recall(L, C) = Precision(C, L)
</equation>
<bodyText confidence="0.999895083333333">
Table 1 shows the NMI and F measure scores for
several baselines along with the RDM. Evaluation
was performed on both DS1 (concept pairs having
a manually annotated relation) and DS2 (all con-
secutive concept pairs). For DS2 we learned the
models using all of the data, and evaluated on those
entity pairs which had a manual relation annotated.
The LDA-based model from Section 3.1 is used as
one baseline. Two other baselines are K-means and
Complete-Link hierarchical agglomerative cluster-
ing using TF-IDF vectors of the context and argu-
ment words (similar to Hasegawa et al. (2004)).
</bodyText>
<figure confidence="0.809598181818182">
I(Ω; C)
NMI(Ω; C) =
I(Ω, C) = � �
k j
�
H(Ω) = −
k
F =�
i
|Li|
n
</figure>
<page confidence="0.992339">
526
</page>
<table confidence="0.9995545">
Method DS1 DS2
NMI F NMI F
Train set
Complete-link 4.2 37.8 N/A N/A
K-means 8.25 38.0 5.4 38.1
LDA baseline 12.8 23.0 15.6 26.2
RDM 18.2 39.1 18.1 37.4
Test set
LDA baseline 10.0 26.1 11.5 26.3
RDM 11.8 37.7 14.0 36.4
</table>
<tableCaption confidence="0.999274">
Table 1: NMI and F measure scores for the RDM and
</tableCaption>
<bodyText confidence="0.9721756">
baselines. The first two columns of numbers show the
scores when evaluation is restricted to only those pairs
of concepts which had a relation identified by annotators.
The last two columns are the NMI and F measure scores
when each method clusters all consecutive entity pairs,
but is only evaluated on those with a relation identified
by annotators.
Complete-link clustering did not finish on DS2
because of the large size of the data set. This high-
lights another advantage of the RDM. Hierarchical
agglomerative clustering is quadratic in the size of
the number of instances to be clustered, while the
RDM’s time and memory requirements both grow
linearly in the number of entity pairs. The scores
shown in Table 1 use the best parameterization of
each model as measured by NMI. For DS1 the
best LDA-based model used 15 clusters. K-means
achieved the best result with 40 clusters, while the
best Complete-Link clustering was obtained by us-
ing 40 clusters. The best RDM model used parame-
ters R = 9 relation, K = 15 general word classes,
and A = 15 argument classes. For DS2 the best
number of clusters for LDA was 10, while K-means
performed best with 58 clusters. The best RDM
model used R = 100 relations, K = 50 general
word classes, and A = 15 argument classes. The
LDA-based approach saw an improvement when us-
ing the larger data set, however the RDM still per-
formed the best.
To assess how well the RDM performs on unseen
data we also evaluated the relations extracted by the
model on the test set. Only the RDM and LDA mod-
els were evaluated as clusters produced by K-means
and hierarchical clustering are valid only for the data
used to generate the clusters. Generative models on
the other hand can provide an estimate of the proba-
bility for each relation type on unseen text. For each
model we generate 10 samples after a burn in pe-
riod of 30 iterations and form clusters by assigning
each pair of concepts to the relation assigned most
often in the samples. The results of this evaluation
are presented in Table 1. While these cluster scores
are lower than those on the data used to train the
models, they still show the RDM outperforming the
LDA baseline model.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999992828571429">
The relation and argument clusters determined by
the RDM provide a better unsupervised relation dis-
covery method than the baselines. The RDM does
this using no knowledge about syntax or semantics
outside of that used to determine concepts. The
analysis shows that words highly indicative of rela-
tions are detected and clustered automatically, with-
out the need for prior annotation of relations or even
the choice of a predetermined set of relation types.
The discovered relations can be interpreted by a hu-
man or labeled automatically using a technique such
as the one presented in Pantel and Ravichandran
(2004). The fact that the discovered relations and ar-
gument classes align well with those chosen by an-
notators on the same data justify our assumptions
about relations being present and discoverable by
the way they are expressed in text. Table 1 shows
that the model does not perform as well when many
of the pairs of entities do not have a relation, but it
still performs better than the baselines.
While the RDM relies in large part on trigger
words for making clustering decisions it is also ca-
pable of including examples which do not contain
any contextual words between the arguments. In ad-
dition to modeling trigger words, a joint distribution
on argument semantic classes is also incorporated.
This allows the model to determine a relation type
even in the absence of triggers. For example, con-
sider the entity pair “[lung cancer] [XRT]”, where
XRT stands for external radiation therapy. By deter-
mining the semantic classes for the arguments (lung
cancer is a Problem, and XRT is a test), the set of
possible relations between the arguments can be nar-
rowed down. For instance, XRT is unlikely to be
in a causal relationship with a problem, or to make
</bodyText>
<page confidence="0.8758">
527
</page>
<bodyText confidence="0.99176675">
a problem worse. A further aid is the fact that the bayesian nonparametric inference of topic hierarchies.
learned relationships may be specialized. For in- J. ACM, 57(2):1–30.
stance, there may be a learned relation type such T. L Griffiths and M. Steyvers. 2004. Finding scien-
as “Cancer treatment addresses cancer problem”. In tific topics. Proceedings of the National Academy of
this case, seeing a type of cancer (lung cancer) and a Sciences of the United States of America, 101(Suppl
type of cancer treatment (XRT) would be strong ev- 1):5228.
idence for that type of relation, even without trigger Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
words. 2004. Discovering relations among named entities
7 Conclusions from large corpora. In Proceedings of the 42nd An-
We presented a novel unsupervised approach to dis- nual Meeting on Association for Computational Lin-
covering relations in the narrative of electronic med- guistics, ACL ’04, Stroudsburg, PA, USA. Association
ical records. We developed a generative model for Computational Linguistics. ACM ID: 1219008.
which can simultaneously cluster relation trigger C. D Manning, P. Raghavan, and H. Sch¨utze. 2008. In-
words as well as relation arguments. The model troduction to information retrieval, volume 1. Cam-
makes use of only the tokens found in the con- bridge University Press.
text of pairs of entities. Unlike many previous ap- P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
proaches, we assign relations to entities at the lo- aging generic patterns for automatically harvesting se-
cation those entities appear in text, allowing us to mantic relations. In Annual Meeting Association for
discover context-sensitive relations. The RDM out- Computational Linguistics, volume 44, page 113.
performs baselines built using Latent Dirichlet Allo- P. Pantel and D. Ravichandran. 2004. Automati-
cation and traditional clustering methods. The dis- cally labeling semantic classes. In Proceedings of
covered relations can be used for a number of ap- HLT/NAACL, volume 4, page 321–328.
plications such as detecting when certain treatments Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
were administered or determining if a necessary test tering for unsupervised relation identification. In Pro-
has been performed. Future work will include trans- ceedings of the sixteenth ACM conference on Con-
forming the RDM into a non-parametric model by ference on information and knowledge management,
using the Chinese Restaurant Process (CRP) (Blei et CIKM ’07, page 411–418, New York, NY, USA.
al., 2010). The CRP can be used to determine the ACM. ACM ID: 1321499.
number of relations, argument classes, and general Z. Syed and E. Viegas. 2010. A hybrid approach to
word classes automatically. unsupervised relation discovery based on linguistic
References analysis and semantic typing. In Proceedings of the
Eugene Agichtein and Luis Gravano. 2000. Snowball: NAACL HLT 2010 First International Workshop on
extracting relations from large plain-text collections. Formalisms and Methodology for Learning by Read-
In Proceedings of the Fifth ACM Conference on Digi- ing, page 105–113.
tal libraries, pages 85–94, San Antonio, Texas, United Ozlem Uzuner, Brett South, Shuying Shen, and Scott Du-
States. ACM. Vall. 2011. 2010 i2b2/VA challenge on concepts, as-
</bodyText>
<reference confidence="0.715741363636364">
E. Amig´o, J. Gonzalo, J. Artiles, and F. Verdejo. 2009. A sertions, and relations in clinical text. Accepted for
comparison of extrinsic clustering evaluation metrics publication.
based on formal constraints. Information Retrieval, A. Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broad-
12(4):461–486. head, and S. Soderland. 2007. TextRunner: open in-
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. formation extraction on the web. In Proceedings of
2003. Latent dirichlet allocation. Journal ofMachine Human Language Technologies: The Annual Confer-
Learning Research, 3:993–1022. ence of the North American Chapter of the Association
David M. Blei, Thomas L. Griffiths, and Michael I. Jor- for Computational Linguistics, page 25–26.
dan. 2010. The nested chinese restaurant process and Alexander Yates. 2009. Unsupervised resolution of ob-
528 jects and relations on the web. Journal of Artificial
Intelligence Research, 34(1).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.420077">
<title confidence="0.9890735">A generative model for unsupervised discovery of relations and classes from clinical texts</title>
<author confidence="0.994483">Bryan Rink</author>
<author confidence="0.994483">Sanda</author>
<affiliation confidence="0.805951333333333">Human Language Technology Research University of Texas at Richardson, TX,</affiliation>
<abstract confidence="0.9986156">This paper presents a generative model for the automatic discovery of relations between entities in electronic medical records. The model discovers relation instances and their types by determining which context tokens express the relation. Additionally, the valid semantic classes for each type of relation are determined. We show that the model produces clusters of relation trigger words which better correspond with manually annotated relations than several existing clustering techniques. The discovered relations reveal some of the implicit semantic structure present in patient records.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>