<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9993365">
A Discriminative Syntactic Model for Source Permutation
via Tree Transduction
</title>
<author confidence="0.998488">
Maxim Khalilov and Khalil Sima’an
</author>
<affiliation confidence="0.997894">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<email confidence="0.996207">
{m.khalilov,k.simaan}@uva.nl
</email>
<sectionHeader confidence="0.997354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999904809523809">
A major challenge in statistical machine
translation is mitigating the word or-
der differences between source and tar-
get strings. While reordering and lexical
translation choices are often conducted in
tandem, source string permutation prior
to translation is attractive for studying re-
ordering using hierarchical and syntactic
structure. This work contributes an ap-
proach for learning source string permu-
tation via transfer of the source syntax
tree. We present a novel discriminative,
probabilistic tree transduction model, and
contribute a set of empirical upperbounds
on translation performance for English-
to-Dutch source string permutation under
sequence and parse tree constraints. Fi-
nally, the translation performance of our
learning model is shown to outperform the
state-of-the-art phrase-based system sig-
nificantly.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985806125">
From its beginnings, statistical machine transla-
tion (SMT) has faced a word reordering challenge
that has a major impact on translation quality.
While standard mechanisms embedded in phrase-
based SMT systems, e.g. (Och and Ney, 2004),
deal efficiently with word reordering within a lim-
ited window of words, they are still not expected
to handle all possible reorderings that involve
words beyond this relatively narrow window, e.g.,
(Tillmann and Ney, 2003; Zens and Ney, 2003;
Tillman, 2004). More recent work handles word
order differences between source and target lan-
guages using hierarchical methods that draw on
Inversion Transduction Grammar (ITG), e.g., (Wu
and Wong, 1998; Chiang, 2005). In principle,
the latter approach explores reordering defined by
the choice of swapping the order of sibling sub-
trees under each node in a binary parse-tree of the
source/target sentence.
An alternative approach aims at minimizing the
need for reordering during translation by permut-
ing the source sentence as a pre-translation step,
e.g., (Collins et al., 2005; Xia and McCord, 2004;
Wang et al., 2007; Khalilov, 2009). In effect,
the translation process works with a model for
source permutation (s -+ s0) followed by trans-
lation model (s -+ t), where s and t are source
and target strings and s0 is the target-like permuted
source string. In how far can source permutation
reduce the need for reordering in conjunction with
translation is an empirical question.
In this paper we define source permutation as
the problem of learning how to transfer a given
source parse-tree into a parse-tree that minimizes
the divergence from target word-order. We model
the tree transfer τs -+ τs0 as a sequence of local,
independent transduction operations, each trans-
forming the current intermediate tree τs0 into the
i
next intermediate tree τs0
</bodyText>
<equation confidence="0.687947">
i+1
. A transduction operation merely per-
n
</equation>
<bodyText confidence="0.999766857142857">
mutes the sequence of n &gt; 1 children of a single
node in an intermediate tree, i.e., unlike previous
work, we do not binarize the trees. The number
of permutations is factorial in n, and learning a
sequence of transductions for explaining a source
permutation can be computationally rather chal-
lenging (see (Tromble and Eisner, 2009)). Yet,
</bodyText>
<equation confidence="0.993413">
, with τs0 = τs and
τs0 = τs
</equation>
<page confidence="0.943473">
92
</page>
<note confidence="0.979879">
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 92–100,
COLING 2010, Beijing, August 2010.
</note>
<bodyText confidence="0.999656882352941">
from the limited perspective of source string per-
mutation (s -+ s0), another challenge is to inte-
grate a figure of merit that measures in how far s
resembles a plausible target word-order.
We contribute solutions to these challenging
problems. Firstly, we learn the transduction
operations using a discriminative estimate of
P(7r(αx)  |Nx, αx, contextx), where Nx is the la-
bel of node (address) x, Nx -+ αx is the context-
free production under x, 7r(αx) is a permutation of
αx and contextx represents a surrounding syntac-
tic context. As a result, this constrains {7r(αx)}
only to those found in the training data, and it
conditions the transduction application probabil-
ity on its specific contexts. Secondly, in every se-
transductions, we prefer those local transductions
on 30 that lead to source string permutation sz
</bodyText>
<equation confidence="0.9589355">
a−1
0
</equation>
<bodyText confidence="0.999225433333333">
that are closer to target word order than sz�1; we
employ s0 language model probability ratios as a
measure of word order improvement.
In how far does the assumption of source per-
mutation provide any window for improvement
over a phrase-based translation system? We con-
duct experiments on translating from English into
Dutch, two languages which are characterized
by a number of systematic divergences between
them. Initially, we conduct oracle experiments
with varying constraints on source permutation
to set upperbounds on performance relative to
a state-of-the-art system. Translating the oracle
source string permutation (obtained by untangling
the crossing alignments) offers a large margin of
improvement, whereas the oracle parse tree per-
mutation provides a far smaller improvement. A
minor change to the latter to also permute con-
stituents that include words aligned with NULL,
offers further improvement, yet lags bahind bare
string permutation. Subsequently, we present
translation results using our learning approach,
and exhibit a significant improvement in BLEU
score over the state-of-the-art baseline system.
Our analysis shows that syntactic structure can
provide important clues for reordering in trans-
lation, especially for dealing with long distance
cases found in, e.g., English and Dutch. Yet, tree
transduction by merely permuting the order of sis-
ter subtrees might turn out insufficient.
</bodyText>
<sectionHeader confidence="0.799633" genericHeader="introduction">
2 Baseline: Phrase-based SMT
</sectionHeader>
<bodyText confidence="0.999586333333333">
Given a word-aligned parallel corpus, phrase-
based systems (Och and Ney, 2002; Koehn et al.,
2003) work with (in principle) arbitrarily large
phrase pairs (also called blocks) acquired from
word-aligned parallel data under a simple defi-
nition of translational equivalence (Zens et al.,
2002). The conditional probabilities of one phrase
given its counterpart are interpolated log-linearly
together with a set of other model estimates:
</bodyText>
<equation confidence="0.98489125">
� M
ei = arg max1:Amhm(ei, fi ) (1)
e�1
m=1
</equation>
<bodyText confidence="0.99414904">
where a feature function hm refer to a system
model, and the corresponding Am refers to the rel-
ative weight given to this model. A phrase-based
system employs feature functions for a phrase pair
translation model, a language model, a reordering
model, and a model to score translation hypothesis
according to length. The weights Am are usually
optimized for system performance (Och, 2003) as
measured by BLEU (Papineni et al., 2002). Two
reordering methods are widely used in phrase-
based systems.
Distance-based A simple distance-based re-
ordering model default for Moses system is the
first reordering technique under consideration.
This model provides the decoder with a cost lin-
ear to the distance between words that should be
reordered.
MSD A lexicalized block-oriented data-driven
reordering model (Tillman, 2004) considers three
different orientations: monotone (M), swap (S),
and discontinuous (D). The reordering probabili-
ties are conditioned on the lexical context of each
phrase pair, and decoding works with a block se-
quence generation process with the possibility of
swapping a pair of blocks.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="related work">
3 Related Work on Source Permutation
</sectionHeader>
<bodyText confidence="0.99949175">
The integration of linguistic syntax into SMT
systems offers a potential solution to reordering
problem. For example, syntax is successfully
integrated into hierarchical SMT (Zollmann and
</bodyText>
<figure confidence="0.5711595">
quence s
0 0 = s, � � � , s
n = s resulting from a tree
0 0
</figure>
<page confidence="0.989614">
93
</page>
<bodyText confidence="0.999887791666667">
Venugopal, 2006). Similarly, the tree-to-string
syntax-based transduction approach offers a com-
plete translation framework (Galley et al., 2006).
The idea of augmenting SMT by a reordering
step prior to translation has often been shown to
improve translation quality. Clause restructuring
performed with hand-crafted reordering rules for
German-to-English and Chinese-to-English tasks
are presented in (Collins et al., 2005) and (Wang
et al., 2007), respectively. In (Xia and McCord,
2004; Khalilov, 2009) word reordering is ad-
dressed by exploiting syntactic representations of
source and target texts.
Other reordering models operate provide the
decoder with multiple word orders. For ex-
ample, the MaxEnt reordering model described
in (Xiong et al., 2006) provides a hierarchi-
cal phrasal reordering system integrated within
a CKY-style decoder. In (Galley and Manning,
2008) the authors present an extension of the fa-
mous MSD model (Tillman, 2004) able to handle
long-distance word-block permutations. Coming
up-to-date, in (PVS, 2010) an effective application
of data mining techniques to syntax-driven source
reordering for MT is presented.
Recently, Tromble and Eisner (2009) define
source permutation as learning source permuta-
tions; the model works with a preference matrix
for word pairs, expressing preference for their two
alternative orders, and a corresponding weight
matrix that is fit to the parallel data. The huge
space of permutations is then structured using a
binary synchronous context-free grammar (Binary
ITG) with O(n3) parsing complexity, and the per-
mutation score is calculated recursively over the
tree at every node as the accumulation of the
relative differences between the word-pair scores
taken from the preference matrix. Application to
German-to-English translation exhibits some per-
formance improvement.
Our work is in the general learning direction
taken in (Tromble and Eisner, 2009) but differs
both in defining the space of permutations, using
local probabilistic tree transductions, as well as in
the learning objective aiming at scoring permuta-
tions based on a log-linear interpolation of a lo-
cal syntax-based model with a global string-based
(language) model.
</bodyText>
<sectionHeader confidence="0.848354" genericHeader="method">
4 Pre-Translation Source Permutation
</sectionHeader>
<bodyText confidence="0.961618523809524">
Given a word-aligned parallel corpus, we define
the source string permutation as the task of learn-
ing to unfold the crossing alignments between
sentence pairs in the parallel corpus. Let be given
a source-target sentence pair s -+ t with word
alignment set a between their words. Unfold-
ing the crossing instances in a should lead to as
,
monotone an alignment aas possible between a
,
permutation sof s and the target string t. Con-
ducting such a “monotonization” on the parallel
corpus gives two parallel corpora: (1) a source-
to-permutation parallel corpus (s -+ s,) and
(2) a source permutation-to-target parallel corpus
,
(s� t). The latter corpus is word-aligned au-
tomatically again and used for training a phrase-
based translation system, while the former corpus
is used for training our model for pre-translation
source permutation via parse tree transductions.
</bodyText>
<figureCaption confidence="0.6830225">
Figure 1: Example of crossing alignments and
long-distance reordering using a source parse tree.
</figureCaption>
<bodyText confidence="0.999953266666667">
In itself, the problem of permuting the source
string to unfold the crossing alignments is compu-
tationally intractable (see (Tromble and Eisner,
2009)). However, different kinds of constraints
can be made on unfolding the crossing alignments
in a. A common approach in hierarchical SMT is
to assume that the source string has a binary parse
tree, and the set of eligible permutations is defined
by binary ITG transductions on this tree. This de-
fines permutations that can be obtained only by
at most inverting pairs of children under nodes of
the source tree. Figure 1 exhibits a long distance
reordering of the verb in English-to-Dutch transla-
tion: inverting the order of the children under the
VP node would unfold the crossing alignment.
</bodyText>
<page confidence="0.996143">
94
</page>
<subsectionHeader confidence="0.994106">
4.1 Oracle Performance
</subsectionHeader>
<bodyText confidence="0.999768076923077">
As has been shown in the literature (Costa-juss`a
and Fonollosa, 2006; Khalilov and Sima’an, 2010;
Wang et al., 2007), source and target texts mono-
tonization leads to a significant improvement in
terms of translation quality. However it is not
known how many alignment crossings can be un-
folded under different parse tree conditions. In or-
der to gauge the impact of corpus monotonization
on translation system performance, we trained a
set of oracle translation systems, which create
target sentences that follow the source language
word order using the word alignment links and
various constraints.
</bodyText>
<figureCaption confidence="0.623605">
(b) Parse tree and corre-
sponding alignment.
Figure 2: Reordering example.
</figureCaption>
<bodyText confidence="0.998903695652174">
The set-up of our experiments and corpus char-
acteristics are detailed in Section 5. Table 1 re-
ports translation scores of the oracle systems. No-
tice that all the numbers are calculated on the re-
aligned corpora. Baseline results are provided for
informative purposes.
String permutation The first oracle system
under consideration is created by traversing
the string from left to right and unfolding all
crossing alignment links (we call this system
oracle-string). For example in Figure 2(a),
the oracle-string system generates a string “do
so gladly” swapping the words “do” and
“gladly” without considering the parse tree.
The first line of the table shows the performance
of the oracle-string system with monotone source
and target portions of the corpus.
Oracle under tree constraint We use a syntac-
tic parser for parsing the English source sentences
that provide n-ary constituency parses. Now we
constrain unfolding crossing alignments only to
those alignment links which agree with the struc-
ture of the source-side parse tree and consider the
constituents which include aligned tokens only.
Unfolding a crossing alignment is modeled as per-
muting the children of a node in the parse tree. We
refer to this oracle system as oracle-tree. For ex-
ample provided in Figure 2(b), there is no way to
construct a monotonized version of the sentence
since the word “so” is aligned to NULL and im-
pedes swapping the order of VB and ADJP under
the VP.
Oracle under relaxed tree constraint The
oracle-tree system does not permute the words
which are both (1) not found in the alignment and
(2) are spanned by the sub-trees sibling to the re-
ordering constituents. Now we introduce a re-
laxed version of the parse tree constraint: the or-
der of the children of a node is permuted when
the node covers the reordering constituents and
also when the frontier contains leaf nodes aligned
with NULL (oracle-span). For example, in Fig-
ure 2(c) the English word “so” is not aligned, but
according to the relaxed version, must move to-
gether with the word “gladly” since they share
a parent node (ADJP).
</bodyText>
<table confidence="0.996879">
Source BLEU NIST
baseline dist 24.04 6.29
baseline MSD 24.04 6.28
oracle − string 27.02 6.51
oracle − tree 24.09 6.30
oracle − span 24.95 6.37
</table>
<tableCaption confidence="0.999932">
Table 1: Translation scores of oracle systems.
</tableCaption>
<bodyText confidence="0.999658666666667">
The main conclusion which can be drawn from
the oracle results is that there is a possibility for
relatively big (�3 BLEU points) improvement
with complete unfolding of crossing alignments
and very limited (�0.05 BLEU points) with the
same done under the parse tree constraint. A tree-
based system that allows for permuting unaligned
words that are covered by a dominating parent
node shows more improvement in terms of BLEU
and NIST scores (�0.9 BLEU points).
The gap between oracle-string and oracle-tree
performance is due to alignment crossings which
</bodyText>
<figure confidence="0.660028333333333">
(a) Word alignment.
(c) Word alignment
and ADJP span.
</figure>
<page confidence="0.958858">
95
</page>
<bodyText confidence="0.855764666666667">
cannot be unfolded under trees (illustrated in Fig-
ure 3), but possibly also due to parse and align-
ment errors.
</bodyText>
<figureCaption confidence="0.9572165">
Figure 3: Example of alignment crossing that does
not agree with the parse tree.
</figureCaption>
<equation confidence="0.972383">
local transduction:
P(7- 07-s0_) Pz P(7r(αx)  |Nx -+ αx, Cx) (2)
i i 1
</equation>
<bodyText confidence="0.979383071428571">
where 7r(αx) is a permutation of αx (the ordered
sequence of node labels under x) and Cx is a local
tree context of node x in tree 7-0 . One wrin-
si−1
kle in this definition is that the number of possi-
ble permutations of αx is factorial in the length
of αx. Fortunately, the source permuted training
data exhibits only a fraction of possible permuta-
tions even for longer αx sequences. Furthermore,
by conditioning the probability on local context,
the general applicability of the permutation is re-
strained.
Given this definition, we define the probabil-
ity of the sequence of local tree transductions
</bodyText>
<figure confidence="0.957394857142857">
4.2 Source Permutation via Syntactic
Transfer
0 -+ ... -+ 7-s
7-s
0
0 as
n
</figure>
<bodyText confidence="0.9642108125">
Given a parallel corpus with string pairs s -+ t
with word alignment a, we create a source per-
muted parallel corpus s -+ s0 by unfolding the
crossing alignments in a: this is done by scanning
the string s from left to right and moving words in-
volved in crossing alignments to positions where
the crossing alignments are unfolded). The source
strings s are parsed, leading to a single parse tree
7-s per source string.
Our model aims at learning from the source
0
permuted parallel corpus s -+ sa probabilistic
optimization arg maxπ(s) P(7r(s)  |s, 7-s). We as-
sume that the set of permutations {7r(s)} is de-
fined through a finite set of local transductions
over the tree 7-s. Hence, we view the permutations
</bodyText>
<equation confidence="0.720905625">
0
leading from s to sas a sequence of local tree
transductions s0-+ ... -+ 7-
s
0
0 0
and sn = s, and each transduction 7-0-+ 7-
si−1 s0i
</equation>
<bodyText confidence="0.907878333333333">
is defined using a tree transduction operation that
at most permutes the children of a single node in
7-0 as defined next.
</bodyText>
<equation confidence="0.583103">
si−1
A local transduction 7- 0-+ 7-
si−1 s
by an operation that applies to a single node with
address x in s0
</equation>
<bodyText confidence="0.9999156">
i−1, labeled Nx, and may permute
the ordered sequence of children αx dominated by
node x. This constitutes a direct generalization of
the ITG binary inversion transduction operation.
We assign a conditional probability to each such
</bodyText>
<equation confidence="0.9952635">
P(7-s0 -+ ... -+ 7-s0) =
0 n
</equation>
<bodyText confidence="0.9788930625">
The problem of calculating the most likely per-
mutation under this transduction model is made
difficult by the fact that different transduction se-
quences may lead to the same permutation, which
demands summing over these sequences. Fur-
thermore, because every local transduction condi-
tions on local context of an intermediate tree, this
quickly risks becoming intractable (even when we
use packed forests). In practice we take a prag-
matic approach and greedily select at every inter-
mediate point 7-s0-+ 7-s the single most likely
local transduction that can be conducted on any
node of the current intermediate tree 7- 0 using
si−1
an interpolation of the term in Equation 2 with
string probability ratios as follows:
</bodyText>
<equation confidence="0.998908454545455">
P(s0 i−1)
P(7r(αx)  |Nx -+ αx, Cx) x
n
i=1 P(7-s0 i  |7-s0i−1) (3)
0
, where s� = s
0
n
0 is modelled
i
P(s0i)
</equation>
<bodyText confidence="0.928873555555556">
The rationale behind this log-linear interpolation
is that our source permutation approach aims at
0
finding the optimal permutation sof s that can
serve as input for a subsequent translation model.
Hence, we aim at tree transductions that are syn-
tactically motivated that also lead to improved
string permutation. In this sense, the tree trans-
duction definitions can be seen as an efficient and
</bodyText>
<page confidence="0.980653">
96
</page>
<bodyText confidence="0.9985574">
syntactically informed way to define the space of
possible permutations.
We estimate the string probabilities P(s�i) us-
ing 5-gram language models trained on the s
side of the source permuted parallel corpus s -+
</bodyText>
<equation confidence="0.995652333333333">
0
s
P(7r(αx)  |Nx -+ αx, Cx) using a Maximum-
</equation>
<bodyText confidence="0.9995815">
Entropy framework, where feature functions are
defined to capture the permutation as a class, the
node label Nx and its head POS tag, the child
sequence αx together with the corresponding se-
quence of head POS tags and other features corre-
sponding to different contextual information.
We were particularly interested in those linguis-
tic features that motivate reordering phenomena
from the syntactic and linguistic perspective. The
features that were used for training the permuta-
tion system are extracted for every internal node
of the source tree that has more than one child:
</bodyText>
<listItem confidence="0.9995935">
• Local tree topology. Sub-tree instances that
include parent node and the ordered se-
quence of child node labels.
• Dependency features. Features that deter-
mine the POS tag of the head word of the cur-
rent node, together with the sequence of POS
tags of the head words of its child nodes.
• Syntactic features. Three binary features
</listItem>
<bodyText confidence="0.848385857142857">
from this class describe: (1) whether the par-
ent node is a child of the node annotated with
the same syntactic category, (2) whether the
parent node is a descendant of the node an-
notated with the same syntactic category, and
(3) if the current subtree is embedded into a
“SENT-SBAR” sub-tree. The latter feature in-
tends to model the divergence in word order
in relative clauses between Dutch and En-
glish which is illustrated in Figure 1.
In initial experiments we piled up all feature func-
tions into a single model. Preliminary results
showed that the system performance increases if
the set of patterns is split into partial classes con-
ditioned on the current node label. Hence, we
trained four separate MaxEnt models for the cate-
gories with potentially high number of crossing
alignments, namely VP, NP, SENT, and SBAR.
For combinatory models we use the following no-
tations: M4 = EiE[ NP, VP, SENT, SBAR] Mi and M2 =
�iE[VP, SENT] Mi.
</bodyText>
<sectionHeader confidence="0.977033" genericHeader="evaluation">
5 Experiments and results
</sectionHeader>
<bodyText confidence="0.99914788">
The SMT system used in the experiments was
implemented within the open-source MOSES
toolkit (Koehn et al., 2007). Standard train-
ing and weight tuning procedures which were
used to build our system are explained in details
on the MOSES web page1. The MSD model
was used together with a distance-based reorder-
ing model. Word alignment was estimated with
GIZA++ tool2 (Och, 2003), coupled with mk-
cls3 (Och, 1999), which allows for statistical word
clustering for better generalization. An 5-gram
target language model was estimated using the
SRI LM toolkit (Stolcke, 2002) and smoothed
with modified Kneser-Ney discounting. We use
the Stanford parser4 (Klein and Manning, 2003)
as a source-side parsing engine. The parser was
trained on the English treebank set provided with
14 syntactic categories and 48 POS tags. The
evaluation conditions were case-sensitive and in-
cluded punctuation marks. For Maximum En-
tropy modeling we used the maxent toolkit5.
Data The experiment results were obtained us-
ing the English-Dutch corpus of the European Par-
liament Plenary Session transcription (EuroParl).
Training corpus statistics can be found in Table 2.
</bodyText>
<table confidence="0.9996058">
Dutch English
Sentences 1.2 M 1.2 M
Words 32.9 M 33.0 M
Average sentence length 27.20 27.28
Vocabulary 228 K 104 K
</table>
<tableCaption confidence="0.9295335">
Table 2: Basic statistics of the English-Dutch Eu-
roParl training corpus.
</tableCaption>
<bodyText confidence="0.8081675">
The development and test datasets were ran-
domly chosen from the corpus and consisted of
</bodyText>
<footnote confidence="0.928007625">
1http://www.statmt.org/moses/
2code.google.com/p/giza-pp/
3http://www.fjoch.com/mkcls.html
4http://nlp.stanford.edu/software/
lex-parser.shtml
5http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html
. We estimate the conditional probability
</footnote>
<page confidence="0.999324">
97
</page>
<bodyText confidence="0.999693961538462">
500 and 1,000 sentences, respectively. Both were
provided with one reference translation.
Results Evaluation of the system performance
is twofold. In the first step, we analyze the qual-
ity of reordering method itself. In the next step
we look at the automatic translation scores and
evaluate the impact which the choice of reorder-
ing strategy has on the translation quality. In
both stages of evaluation, the results are con-
trasted with the performance shown by the stan-
dard phrase-based SMT system (baseline) and
with oracle results.
Source reordering analysis Table 3 shows the
parameters of the reordered system allowing to as-
sess the effectiveness of reordering permutations,
namely: (1) a total number of crossings found in
the word alignment (#C), (2) the size of the re-
sulting phrase table (PT), (3) BLEU, NIST, and
WER scores obtained using monotonized parallel
corpus (oracle) as a reference.
All the numbers are calculated on the re-aligned
corpora. Calculations are done on the basis of the
100,000 line extraction from the corpus6 and cor-
responding alignment matrix. The baseline rows
show the number of alignment crossings found in
the original (unmonotonized) corpus.
</bodyText>
<table confidence="0.9969274375">
System #C PT Scores
BLEU NIST WER
Oracle
string 54.6K 48.4M - - -
tree 187.3K 30.3M 71.73 17.01 16.77
span 146.9K 33.0M 73.41 17.11 15.73
Baselines
baselines 187.0K 29.8M 71.70 17.07 16.55
Category models
MNP 188.9K 29.7M 71.63 17.07 16.52
MVP 168.1K 29.8M 73.17 17.16 15.99
MSENT 171.0K 29.8M 73.08 17.08 16.10
MSBAR 188.6K 29.8M 72.89 16.90 16.41
Combinatory models
M4 193.2K 29.1M 70.98 16.85 16.78
M2 165.4K 29.9M 73.07 16.92 15.88
</table>
<tableCaption confidence="0.9929865">
Table 3: Main parameters of the tree-based re-
ordering system.
</tableCaption>
<footnote confidence="0.6197065">
6A smaller portion of the corpus is used for analysis in
order to reduce evaluation time.
</footnote>
<bodyText confidence="0.95271525">
Translation scores The evaluation results for
the development and test corpora are reported in
Table 4. They include two baseline configurations
(dist and MSD), oracle results and contrasts them
with the performance shown by different combi-
nations of single-category tree-based reordering
models. Best scores within each experimental sec-
tion are placed in cells filled with grey.
</bodyText>
<table confidence="0.998843923076923">
System Dev Test
BLEU BLEU NIST
baseline dist 23.88 24.04 6.29
baseline MSD 24.07 24.04 6.28
oracle-string 26.28 27.02 6.50
oracle-tree 23.84 24.09 6.30
oracle-span 24.79 24.95 6.35
MNP 23.79 23.81 6.27
MVP 24.16 24.55 6.29
MSENT 24.27 24.56 6.32
MSBAR 23.99 24.12 6.27
M4 23.50 23.86 6.29
M2 24.28 24.64 6.33
</table>
<tableCaption confidence="0.999581">
Table 4: Experimental results.
</tableCaption>
<subsectionHeader confidence="0.854836">
Analysis The number of crossings
</subsectionHeader>
<bodyText confidence="0.999961238095238">
found in word alignment intersection and
BLEU/NIST/WER scores estimated on reordered
data vs. monotonized data report the reordering
algorithm effectiveness. A big gap between num-
ber of crossings and total number of reorderings
per corpus found in oracle-string system7 and
baseline systems demonstrates the possible reduc-
tion of system’s non-monotonicity. The difference
in number of crossings and BLEU/NIST/WER
scores between the oracle-span and the best
performing MaxEnt models (namely, M2) shows
the level of performance of the prediction module.
A number of distinct phrase translation pairs in
the translation table implicitly reveals the general-
ization capabilities of the translation system since
it simplifies the translation task. From the other
hand, increased number of shorter phrases can add
noise in the reordered data and makes decoding
more complex. Hence, the size of phrase table it-
self can not be considered as a robust indicator of
its translation potential.
</bodyText>
<footnote confidence="0.886398666666667">
7The number of crossings for oracle configuration is not
zero since this parameter is calculated on the re-aligned cor-
pus.
</footnote>
<page confidence="0.978621">
98
</page>
<bodyText confidence="0.998787227272727">
Table 4 shows that three of six MaxEnt re-
ordering systems outperform baseline systems by
about 0.5-0.6 BLEU points, that is statistically
significant8. The combination of NP, NP, SENT,
and SBAR models do not show good performance
possibly due to increased sparseness of reorder-
ing patterns. However, the system that consider
only the Mu P and MSENT models achieves 0.62
BLEU score gain over the baseline configurations.
The main conclusion which can be drawn from
analysis of Tables 3 and 4 is that there is an
evident correlation between characteristics of re-
ordering system and performance demonstrated
by the translation system trained on the corpus
with reordered source part.
Example Figure 4 exemplifies the sentences
that presumably benefits from the monotonization
of the source part of the parallel corpus. The ex-
ample demonstrates a pervading syntactic distinc-
tion between English and Dutch: the reordering of
verb-phrase subconstituents VP NP PP within the
relative clause into PP NP VP.
</bodyText>
<sectionHeader confidence="0.997241" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.981799529411764">
We introduced a tree-based reordering model that
aims at monotonizing the word order of source
8All statistical significance calculations are done for a
95% confidence interval and 1000 resamples, following the
guidelines from (Koehn, 2004).
and target languages as a pre-translation step. Our
model avoids complete generalization of reorder-
ing instances by using tree contexts and limit-
ing the permutations to data instances. From a
learning perspective, our work shows that navigat-
ing a large space of intermediate tree transforma-
tions can be conducted effectively using both the
source-side syntactic tree and a language model
of the idealized (target-like) source-permuted lan-
guage.
We have shown the potential for translation
quality improvement when target sentences are
created following the source language word or-
der (,:3 BLEU points over the standard phrase-
based SMT) and under parse tree constraint (P 0.9
BLEU points). As can be seen from these re-
sults, our model exhibits competitive translation
performance scores compared with the standard
distance-based and lexical reordering.
The gap between the oracle and our system’s
results leaves room for improvement. We intend
to study extensions of the current tree transfer
model to narrow this performance gap. As a first
step we are combining isolated models for con-
crete syntactic categories and aggregating more
features into the MaxEnt model. Algorithmic im-
provements, such as beam-search and chart pars-
ing, could allow us to apply our method to full
parse-forests as opposed to a single parse tree.
</bodyText>
<figure confidence="0.955289857142857">
(a) Original parse tree. (b) Reordered parse tree.
Src: that ... to lead the Commission during the next five-year term
Ref.: dat ... om de komende vijfjaar de Commissie te leiden
Baseline MSD: dat ... om het voortouw te nemen in de Commissie tijdens de komende vijfjaar
Rrd src: that ... during the next five-year term the Commission to lead
M2 : dat ... om de Commissie tijdens de komende vijfjaar te leiden
(c) Translations.
</figure>
<figureCaption confidence="0.999846">
Figure 4: Example of tree-based monotonization.
</figureCaption>
<page confidence="0.997025">
99
</page>
<sectionHeader confidence="0.998313" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999851505263158">
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of ACL’05, pages 263–270.
M. Collins, P. Koehn, and I. Kuˇcerov´a. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of ACL’05, pages 531–540.
M. R. Costa-juss`a and J. A. R. Fonollosa. 2006.
Statistical machine reordering. In Proceedings of
HLT/EMNLP’06, pages 70–76.
M. Galley and Ch. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
Proceedings of EMNLP’08, pages 848–856.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang, and I. Thaye. 2006. Scalable in-
ference and training of context-rich syntactic trans-
lation models. In Proc. of COLING/ACL’06, pages
961–968.
M. Khalilov and K. Sima’an. 2010. Source reordering
using maxent classifiers and supertags. In Proc. of
EAMT’10, pages 292–299.
M. Khalilov. 2009. New statistical and syntactic mod-
els for machine translation. Ph.D. thesis, Universi-
tat Polit`ecnica de Catalunya, October.
D. Klein and C. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the 41st Annual
Meeting of the ACL’03, pages 423–430.
Ph. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based machine translation. In Proceedings
of the HLT-NAACL 2003, pages 48–54.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: open-source
toolkit for statistical machine translation. In Pro-
ceedings of ACL 2007, pages 177–180.
Ph. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP’04, pages 388–395.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL’02, pages
295–302.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417–449.
F. Och. 1999. An efficient method for determin-
ing bilingual word classes. In Proceedings of ACL
1999, pages 71–76.
F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL’03,
pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL’02, pages 311–
318.
A. PVS. 2010. A data mining approach to
learn reorder rules for SMT. In Proceedings of
NAACL/HLT’10, pages 52–57.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proceedings of SLP’02, pages
901–904.
C. Tillman. 2004. A unigram orientation model for
statistical machine translation. In Proceedings of
HLT-NAACL’04, pages 101–104.
C. Tillmann and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 1(29):93–133.
R. Tromble and J. Eisner. 2009. Learning linear order-
ing problems for better translation. In Proceedings
of EMNLP’09, pages 1007–1016.
C. Wang, M. Collins, and Ph. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proceedings of EMNLP-CoNLL’07, pages
737–745.
D. Wu and H. Wong. 1998. Machine translation wih a
stochastic grammatical channel. In Proceedings of
ACL-COLING’98, pages 1408–1415.
F. Xia and M. McCord. 2004. Improving a statistical
MT system with automatically learned rewrite pat-
terns. In Proceedings of COLING’04, pages 508–
514.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum en-
tropy based phrase reordering model for statistical
machine translation. In Proceedings of ACL’06,
pages 521–528.
R. Zens and H. Ney. 2003. A comparative study on
reordering constraints in statistical machine transla-
tion. In Proceedings of ACL’03, pages 144–151.
R. Zens, F. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In Proceedings of KI:
Advances in Arti�cial Intelligence, pages 18–32.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proceedings of NAACL’06, pages 138–141.
</reference>
<page confidence="0.990736">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573586">
<title confidence="0.9927295">A Discriminative Syntactic Model for Source via Tree Transduction</title>
<author confidence="0.828787">Khalilov</author>
<affiliation confidence="0.9912465">Institute for Logic, Language and University of</affiliation>
<abstract confidence="0.986515545454545">A major challenge in statistical machine translation is mitigating the word order differences between source and target strings. While reordering and lexical translation choices are often conducted in string permutation to translation is attractive for studying reordering using hierarchical and syntactic structure. This work contributes an approach for learning source string permutation via transfer of the source syntax tree. We present a novel discriminative, probabilistic tree transduction model, and contribute a set of empirical upperbounds on translation performance for Englishto-Dutch source string permutation under sequence and parse tree constraints. Finally, the translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1754" citStr="Chiang, 2005" startWordPosition="250" endWordPosition="251">ering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word order differences between source and target languages using hierarchical methods that draw on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005). In principle, the latter approach explores reordering defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. An alternative approach aims at minimizing the need for reordering during translation by permuting the source sentence as a pre-translation step, e.g., (Collins et al., 2005; Xia and McCord, 2004; Wang et al., 2007; Khalilov, 2009). In effect, the translation process works with a model for source permutation (s -+ s0) followed by translation model (s -+ t), where s and t are source and target strings and s0</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL’05, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL’05, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-juss`a</author>
<author>J A R Fonollosa</author>
</authors>
<title>Statistical machine reordering.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/EMNLP’06,</booktitle>
<pages>70--76</pages>
<marker>Costa-juss`a, Fonollosa, 2006</marker>
<rawString>M. R. Costa-juss`a and J. A. R. Fonollosa. 2006. Statistical machine reordering. In Proceedings of HLT/EMNLP’06, pages 70–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP’08,</booktitle>
<pages>848--856</pages>
<contexts>
<context position="8459" citStr="Manning, 2008" startWordPosition="1303" endWordPosition="1304">ality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Recently, Tromble and Eisner (2009) define source permutation as learning source permutations; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and a corresponding weight matrix that is fit to the parallel data. The huge space of permutations is then structured using</context>
</contexts>
<marker>Manning, 2008</marker>
<rawString>M. Galley and Ch. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of EMNLP’08, pages 848–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thaye</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL’06,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="7728" citStr="Galley et al., 2006" startWordPosition="1194" endWordPosition="1197">ng probabilities are conditioned on the lexical context of each phrase pair, and decoding works with a block sequence generation process with the possibility of swapping a pair of blocks. 3 Related Work on Source Permutation The integration of linguistic syntax into SMT systems offers a potential solution to reordering problem. For example, syntax is successfully integrated into hierarchical SMT (Zollmann and quence s 0 0 = s, � � � , s n = s resulting from a tree 0 0 93 Venugopal, 2006). Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong e</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thaye, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thaye. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of COLING/ACL’06, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Khalilov</author>
<author>K Sima’an</author>
</authors>
<title>Source reordering using maxent classifiers and supertags.</title>
<date>2010</date>
<booktitle>In Proc. of EAMT’10,</booktitle>
<pages>292--299</pages>
<marker>Khalilov, Sima’an, 2010</marker>
<rawString>M. Khalilov and K. Sima’an. 2010. Source reordering using maxent classifiers and supertags. In Proc. of EAMT’10, pages 292–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Khalilov</author>
</authors>
<title>New statistical and syntactic models for machine translation.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Polit`ecnica de Catalunya,</institution>
<contexts>
<context position="2175" citStr="Khalilov, 2009" startWordPosition="317" endWordPosition="318">cent work handles word order differences between source and target languages using hierarchical methods that draw on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005). In principle, the latter approach explores reordering defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. An alternative approach aims at minimizing the need for reordering during translation by permuting the source sentence as a pre-translation step, e.g., (Collins et al., 2005; Xia and McCord, 2004; Wang et al., 2007; Khalilov, 2009). In effect, the translation process works with a model for source permutation (s -+ s0) followed by translation model (s -+ t), where s and t are source and target strings and s0 is the target-like permuted source string. In how far can source permutation reduce the need for reordering in conjunction with translation is an empirical question. In this paper we define source permutation as the problem of learning how to transfer a given source parse-tree into a parse-tree that minimizes the divergence from target word-order. We model the tree transfer τs -+ τs0 as a sequence of local, independe</context>
<context position="8089" citStr="Khalilov, 2009" startWordPosition="1248" endWordPosition="1249">integrated into hierarchical SMT (Zollmann and quence s 0 0 = s, � � � , s n = s resulting from a tree 0 0 93 Venugopal, 2006). Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques to syntax-driven </context>
</contexts>
<marker>Khalilov, 2009</marker>
<rawString>M. Khalilov. 2009. New statistical and syntactic models for machine translation. Ph.D. thesis, Universitat Polit`ecnica de Catalunya, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL’03,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="21409" citStr="Klein and Manning, 2003" startWordPosition="3461" endWordPosition="3464">nted within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page1. The MSD model was used together with a distance-based reordering model. Word alignment was estimated with GIZA++ tool2 (Och, 2003), coupled with mkcls3 (Och, 1999), which allows for statistical word clustering for better generalization. An 5-gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and smoothed with modified Kneser-Ney discounting. We use the Stanford parser4 (Klein and Manning, 2003) as a source-side parsing engine. The parser was trained on the English treebank set provided with 14 syntactic categories and 48 POS tags. The evaluation conditions were case-sensitive and included punctuation marks. For Maximum Entropy modeling we used the maxent toolkit5. Data The experiment results were obtained using the English-Dutch corpus of the European Parliament Plenary Session transcription (EuroParl). Training corpus statistics can be found in Table 2. Dutch English Sentences 1.2 M 1.2 M Words 32.9 M 33.0 M Average sentence length 27.20 27.28 Vocabulary 228 K 104 K Table 2: Basic </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the ACL’03, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och Koehn</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<pages>48--54</pages>
<marker>Koehn, Marcu, 2003</marker>
<rawString>Ph. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-based machine translation. In Proceedings of the HLT-NAACL 2003, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoang Koehn</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open-source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>177--180</pages>
<contexts>
<context position="20847" citStr="Koehn et al., 2007" startWordPosition="3373" endWordPosition="3376">ents we piled up all feature functions into a single model. Preliminary results showed that the system performance increases if the set of patterns is split into partial classes conditioned on the current node label. Hence, we trained four separate MaxEnt models for the categories with potentially high number of crossing alignments, namely VP, NP, SENT, and SBAR. For combinatory models we use the following notations: M4 = EiE[ NP, VP, SENT, SBAR] Mi and M2 = �iE[VP, SENT] Mi. 5 Experiments and results The SMT system used in the experiments was implemented within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page1. The MSD model was used together with a distance-based reordering model. Word alignment was estimated with GIZA++ tool2 (Och, 2003), coupled with mkcls3 (Och, 1999), which allows for statistical word clustering for better generalization. An 5-gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and smoothed with modified Kneser-Ney discounting. We use the Stanford parser4 (Klein and Manning, 2003) as a source-side parsing engine. The </context>
</contexts>
<marker>Koehn, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open-source toolkit for statistical machine translation. In Proceedings of ACL 2007, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="27303" citStr="Koehn, 2004" startWordPosition="4367" endWordPosition="4368"> reordered source part. Example Figure 4 exemplifies the sentences that presumably benefits from the monotonization of the source part of the parallel corpus. The example demonstrates a pervading syntactic distinction between English and Dutch: the reordering of verb-phrase subconstituents VP NP PP within the relative clause into PP NP VP. 6 Conclusions and future work We introduced a tree-based reordering model that aims at monotonizing the word order of source 8All statistical significance calculations are done for a 95% confidence interval and 1000 resamples, following the guidelines from (Koehn, 2004). and target languages as a pre-translation step. Our model avoids complete generalization of reordering instances by using tree contexts and limiting the permutations to data instances. From a learning perspective, our work shows that navigating a large space of intermediate tree transformations can be conducted effectively using both the source-side syntactic tree and a language model of the idealized (target-like) source-permuted language. We have shown the potential for translation quality improvement when target sentences are created following the source language word order (,:3 BLEU poin</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Ph. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP’04, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="5789" citStr="Och and Ney, 2002" startWordPosition="889" endWordPosition="892">r improvement, yet lags bahind bare string permutation. Subsequently, we present translation results using our learning approach, and exhibit a significant improvement in BLEU score over the state-of-the-art baseline system. Our analysis shows that syntactic structure can provide important clues for reordering in translation, especially for dealing with long distance cases found in, e.g., English and Dutch. Yet, tree transduction by merely permuting the order of sister subtrees might turn out insufficient. 2 Baseline: Phrase-based SMT Given a word-aligned parallel corpus, phrasebased systems (Och and Ney, 2002; Koehn et al., 2003) work with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a simple definition of translational equivalence (Zens et al., 2002). The conditional probabilities of one phrase given its counterpart are interpolated log-linearly together with a set of other model estimates: � M ei = arg max1:Amhm(ei, fi ) (1) e�1 m=1 where a feature function hm refer to a system model, and the corresponding Am refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair tra</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL’02, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1292" citStr="Och and Ney, 2004" startWordPosition="177" endWordPosition="180">present a novel discriminative, probabilistic tree transduction model, and contribute a set of empirical upperbounds on translation performance for Englishto-Dutch source string permutation under sequence and parse tree constraints. Finally, the translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly. 1 Introduction From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word order differences between source and target languages using hierarchical methods that draw on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005). In principle, the latter approach explores reordering defined by the choice of swapping the order of sibling subtrees under each node in</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>71--76</pages>
<contexts>
<context position="21144" citStr="Och, 1999" startWordPosition="3425" endWordPosition="3426">mber of crossing alignments, namely VP, NP, SENT, and SBAR. For combinatory models we use the following notations: M4 = EiE[ NP, VP, SENT, SBAR] Mi and M2 = �iE[VP, SENT] Mi. 5 Experiments and results The SMT system used in the experiments was implemented within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page1. The MSD model was used together with a distance-based reordering model. Word alignment was estimated with GIZA++ tool2 (Och, 2003), coupled with mkcls3 (Och, 1999), which allows for statistical word clustering for better generalization. An 5-gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and smoothed with modified Kneser-Ney discounting. We use the Stanford parser4 (Klein and Manning, 2003) as a source-side parsing engine. The parser was trained on the English treebank set provided with 14 syntactic categories and 48 POS tags. The evaluation conditions were case-sensitive and included punctuation marks. For Maximum Entropy modeling we used the maxent toolkit5. Data The experiment results were obtained using the English</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>F. Och. 1999. An efficient method for determining bilingual word classes. In Proceedings of ACL 1999, pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="6579" citStr="Och, 2003" startWordPosition="1017" endWordPosition="1018">al equivalence (Zens et al., 2002). The conditional probabilities of one phrase given its counterpart are interpolated log-linearly together with a set of other model estimates: � M ei = arg max1:Amhm(ei, fi ) (1) e�1 m=1 where a feature function hm refer to a system model, and the corresponding Am refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights Am are usually optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002). Two reordering methods are widely used in phrasebased systems. Distance-based A simple distance-based reordering model default for Moses system is the first reordering technique under consideration. This model provides the decoder with a cost linear to the distance between words that should be reordered. MSD A lexicalized block-oriented data-driven reordering model (Tillman, 2004) considers three different orientations: monotone (M), swap (S), and discontinuous (D). The reordering probabilities are conditioned on the lexical context of each phrase </context>
<context position="21111" citStr="Och, 2003" startWordPosition="3419" endWordPosition="3420">tegories with potentially high number of crossing alignments, namely VP, NP, SENT, and SBAR. For combinatory models we use the following notations: M4 = EiE[ NP, VP, SENT, SBAR] Mi and M2 = �iE[VP, SENT] Mi. 5 Experiments and results The SMT system used in the experiments was implemented within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page1. The MSD model was used together with a distance-based reordering model. Word alignment was estimated with GIZA++ tool2 (Och, 2003), coupled with mkcls3 (Och, 1999), which allows for statistical word clustering for better generalization. An 5-gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and smoothed with modified Kneser-Ney discounting. We use the Stanford parser4 (Klein and Manning, 2003) as a source-side parsing engine. The parser was trained on the English treebank set provided with 14 syntactic categories and 48 POS tags. The evaluation conditions were case-sensitive and included punctuation marks. For Maximum Entropy modeling we used the maxent toolkit5. Data The experiment result</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL’03, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="6623" citStr="Papineni et al., 2002" startWordPosition="1023" endWordPosition="1026">2). The conditional probabilities of one phrase given its counterpart are interpolated log-linearly together with a set of other model estimates: � M ei = arg max1:Amhm(ei, fi ) (1) e�1 m=1 where a feature function hm refer to a system model, and the corresponding Am refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights Am are usually optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002). Two reordering methods are widely used in phrasebased systems. Distance-based A simple distance-based reordering model default for Moses system is the first reordering technique under consideration. This model provides the decoder with a cost linear to the distance between words that should be reordered. MSD A lexicalized block-oriented data-driven reordering model (Tillman, 2004) considers three different orientations: monotone (M), swap (S), and discontinuous (D). The reordering probabilities are conditioned on the lexical context of each phrase pair, and decoding works with a block sequen</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL’02, pages 311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A PVS</author>
</authors>
<title>A data mining approach to learn reorder rules for SMT.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL/HLT’10,</booktitle>
<pages>52--57</pages>
<contexts>
<context position="8620" citStr="PVS, 2010" startWordPosition="1327" endWordPosition="1328">nd (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Recently, Tromble and Eisner (2009) define source permutation as learning source permutations; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and a corresponding weight matrix that is fit to the parallel data. The huge space of permutations is then structured using a binary synchronous context-free grammar (Binary ITG) with O(n3) parsing complexity, and the permutation score is calculated recursively over the tree at every</context>
</contexts>
<marker>PVS, 2010</marker>
<rawString>A. PVS. 2010. A data mining approach to learn reorder rules for SMT. In Proceedings of NAACL/HLT’10, pages 52–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM: an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of SLP’02,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="21304" citStr="Stolcke, 2002" startWordPosition="3448" endWordPosition="3449"> �iE[VP, SENT] Mi. 5 Experiments and results The SMT system used in the experiments was implemented within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page1. The MSD model was used together with a distance-based reordering model. Word alignment was estimated with GIZA++ tool2 (Och, 2003), coupled with mkcls3 (Och, 1999), which allows for statistical word clustering for better generalization. An 5-gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and smoothed with modified Kneser-Ney discounting. We use the Stanford parser4 (Klein and Manning, 2003) as a source-side parsing engine. The parser was trained on the English treebank set provided with 14 syntactic categories and 48 POS tags. The evaluation conditions were case-sensitive and included punctuation marks. For Maximum Entropy modeling we used the maxent toolkit5. Data The experiment results were obtained using the English-Dutch corpus of the European Parliament Plenary Session transcription (EuroParl). Training corpus statistics can be found in Table 2. Dutch English Sentences 1</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM: an extensible language modeling toolkit. In Proceedings of SLP’02, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL’04,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="1551" citStr="Tillman, 2004" startWordPosition="220" endWordPosition="221">rmance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly. 1 Introduction From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word order differences between source and target languages using hierarchical methods that draw on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005). In principle, the latter approach explores reordering defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. An alternative approach aims at minimizing the need for reordering during translation by permuting the source sentence as a pre-translation step, e.g., (Collins et al., 2005; Xia and McCord, 2004; Wang et al</context>
<context position="7008" citStr="Tillman, 2004" startWordPosition="1081" endWordPosition="1082">ion model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights Am are usually optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002). Two reordering methods are widely used in phrasebased systems. Distance-based A simple distance-based reordering model default for Moses system is the first reordering technique under consideration. This model provides the decoder with a cost linear to the distance between words that should be reordered. MSD A lexicalized block-oriented data-driven reordering model (Tillman, 2004) considers three different orientations: monotone (M), swap (S), and discontinuous (D). The reordering probabilities are conditioned on the lexical context of each phrase pair, and decoding works with a block sequence generation process with the possibility of swapping a pair of blocks. 3 Related Work on Source Permutation The integration of linguistic syntax into SMT systems offers a potential solution to reordering problem. For example, syntax is successfully integrated into hierarchical SMT (Zollmann and quence s 0 0 = s, � � � , s n = s resulting from a tree 0 0 93 Venugopal, 2006). Simila</context>
<context position="8532" citStr="Tillman, 2004" startWordPosition="1316" endWordPosition="1317">for German-to-English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Recently, Tromble and Eisner (2009) define source permutation as learning source permutations; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and a corresponding weight matrix that is fit to the parallel data. The huge space of permutations is then structured using a binary synchronous context-free grammar (Binary ITG) with O(n3) parsin</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>C. Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL’04, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Word reordering and a dynamic programming beam search algorithm for statistical machine translation.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>1</volume>
<issue>29</issue>
<contexts>
<context position="1515" citStr="Tillmann and Ney, 2003" startWordPosition="212" endWordPosition="215"> constraints. Finally, the translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly. 1 Introduction From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word order differences between source and target languages using hierarchical methods that draw on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005). In principle, the latter approach explores reordering defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. An alternative approach aims at minimizing the need for reordering during translation by permuting the source sentence as a pre-translation step, e.g., (Collins et al., 20</context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>C. Tillmann and H. Ney. 2003. Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational Linguistics, 1(29):93–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tromble</author>
<author>J Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP’09,</booktitle>
<pages>1007--1016</pages>
<contexts>
<context position="3269" citStr="Tromble and Eisner, 2009" startWordPosition="498" endWordPosition="501">e-tree that minimizes the divergence from target word-order. We model the tree transfer τs -+ τs0 as a sequence of local, independent transduction operations, each transforming the current intermediate tree τs0 into the i next intermediate tree τs0 i+1 . A transduction operation merely pern mutes the sequence of n &gt; 1 children of a single node in an intermediate tree, i.e., unlike previous work, we do not binarize the trees. The number of permutations is factorial in n, and learning a sequence of transductions for explaining a source permutation can be computationally rather challenging (see (Tromble and Eisner, 2009)). Yet, , with τs0 = τs and τs0 = τs 92 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 92–100, COLING 2010, Beijing, August 2010. from the limited perspective of source string permutation (s -+ s0), another challenge is to integrate a figure of merit that measures in how far s resembles a plausible target word-order. We contribute solutions to these challenging problems. Firstly, we learn the transduction operations using a discriminative estimate of P(7r(αx) |Nx, αx, contextx), where Nx is the label of node (address) x, Nx -+ αx is the context</context>
<context position="8763" citStr="Tromble and Eisner (2009)" startWordPosition="1345" endWordPosition="1348">ctic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Recently, Tromble and Eisner (2009) define source permutation as learning source permutations; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and a corresponding weight matrix that is fit to the parallel data. The huge space of permutations is then structured using a binary synchronous context-free grammar (Binary ITG) with O(n3) parsing complexity, and the permutation score is calculated recursively over the tree at every node as the accumulation of the relative differences between the word-pair scores taken from the preference matrix. Application to German-to-E</context>
<context position="10934" citStr="Tromble and Eisner, 2009" startWordPosition="1681" endWordPosition="1684">ves two parallel corpora: (1) a sourceto-permutation parallel corpus (s -+ s,) and (2) a source permutation-to-target parallel corpus , (s� t). The latter corpus is word-aligned automatically again and used for training a phrasebased translation system, while the former corpus is used for training our model for pre-translation source permutation via parse tree transductions. Figure 1: Example of crossing alignments and long-distance reordering using a source parse tree. In itself, the problem of permuting the source string to unfold the crossing alignments is computationally intractable (see (Tromble and Eisner, 2009)). However, different kinds of constraints can be made on unfolding the crossing alignments in a. A common approach in hierarchical SMT is to assume that the source string has a binary parse tree, and the set of eligible permutations is defined by binary ITG transductions on this tree. This defines permutations that can be obtained only by at most inverting pairs of children under nodes of the source tree. Figure 1 exhibits a long distance reordering of the verb in English-to-Dutch translation: inverting the order of the children under the VP node would unfold the crossing alignment. 94 4.1 Or</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>R. Tromble and J. Eisner. 2009. Learning linear ordering problems for better translation. In Proceedings of EMNLP’09, pages 1007–1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL’07,</booktitle>
<pages>737--745</pages>
<marker>Koehn, 2007</marker>
<rawString>C. Wang, M. Collins, and Ph. Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLP-CoNLL’07, pages 737–745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>H Wong</author>
</authors>
<title>Machine translation wih a stochastic grammatical channel.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL-COLING’98,</booktitle>
<pages>1408--1415</pages>
<contexts>
<context position="1739" citStr="Wu and Wong, 1998" startWordPosition="246" endWordPosition="249"> faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word order differences between source and target languages using hierarchical methods that draw on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005). In principle, the latter approach explores reordering defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. An alternative approach aims at minimizing the need for reordering during translation by permuting the source sentence as a pre-translation step, e.g., (Collins et al., 2005; Xia and McCord, 2004; Wang et al., 2007; Khalilov, 2009). In effect, the translation process works with a model for source permutation (s -+ s0) followed by translation model (s -+ t), where s and t are source and target</context>
</contexts>
<marker>Wu, Wong, 1998</marker>
<rawString>D. Wu and H. Wong. 1998. Machine translation wih a stochastic grammatical channel. In Proceedings of ACL-COLING’98, pages 1408–1415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING’04,</booktitle>
<pages>508--514</pages>
<contexts>
<context position="2139" citStr="Xia and McCord, 2004" startWordPosition="309" endWordPosition="312">ns and Ney, 2003; Tillman, 2004). More recent work handles word order differences between source and target languages using hierarchical methods that draw on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005). In principle, the latter approach explores reordering defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. An alternative approach aims at minimizing the need for reordering during translation by permuting the source sentence as a pre-translation step, e.g., (Collins et al., 2005; Xia and McCord, 2004; Wang et al., 2007; Khalilov, 2009). In effect, the translation process works with a model for source permutation (s -+ s0) followed by translation model (s -+ t), where s and t are source and target strings and s0 is the target-like permuted source string. In how far can source permutation reduce the need for reordering in conjunction with translation is an empirical question. In this paper we define source permutation as the problem of learning how to transfer a given source parse-tree into a parse-tree that minimizes the divergence from target word-order. We model the tree transfer τs -+ τ</context>
<context position="8072" citStr="Xia and McCord, 2004" startWordPosition="1244" endWordPosition="1247">yntax is successfully integrated into hierarchical SMT (Zollmann and quence s 0 0 = s, � � � , s n = s resulting from a tree 0 0 93 Venugopal, 2006). Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques </context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>F. Xia and M. McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proceedings of COLING’04, pages 508– 514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Xiong</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL’06,</booktitle>
<pages>521--528</pages>
<contexts>
<context position="8340" citStr="Xiong et al., 2006" startWordPosition="1284" endWordPosition="1287">, 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Recently, Tromble and Eisner (2009) define source permutation as learning source permutations; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and </context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of ACL’06, pages 521–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="1535" citStr="Zens and Ney, 2003" startWordPosition="216" endWordPosition="219">he translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly. 1 Introduction From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word order differences between source and target languages using hierarchical methods that draw on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005). In principle, the latter approach explores reordering defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. An alternative approach aims at minimizing the need for reordering during translation by permuting the source sentence as a pre-translation step, e.g., (Collins et al., 2005; Xia and McCord, </context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>R. Zens and H. Ney. 2003. A comparative study on reordering constraints in statistical machine translation. In Proceedings of ACL’03, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of KI: Advances in Arti�cial Intelligence,</booktitle>
<pages>18--32</pages>
<contexts>
<context position="6003" citStr="Zens et al., 2002" startWordPosition="921" endWordPosition="924">ine system. Our analysis shows that syntactic structure can provide important clues for reordering in translation, especially for dealing with long distance cases found in, e.g., English and Dutch. Yet, tree transduction by merely permuting the order of sister subtrees might turn out insufficient. 2 Baseline: Phrase-based SMT Given a word-aligned parallel corpus, phrasebased systems (Och and Ney, 2002; Koehn et al., 2003) work with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a simple definition of translational equivalence (Zens et al., 2002). The conditional probabilities of one phrase given its counterpart are interpolated log-linearly together with a set of other model estimates: � M ei = arg max1:Amhm(ei, fi ) (1) e�1 m=1 where a feature function hm refer to a system model, and the corresponding Am refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights Am are usually optimized for system performance (Och, 2003) as measured by BLEU (Pa</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>R. Zens, F. Och, and H. Ney. 2002. Phrase-based statistical machine translation. In Proceedings of KI: Advances in Arti�cial Intelligence, pages 18–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL’06,</booktitle>
<pages>138--141</pages>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>A. Zollmann and A. Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of NAACL’06, pages 138–141.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>