<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000612">
<title confidence="0.9968945">
Corpus-Based Induction of Syntactic Structure:
Models of Dependency and Constituency
</title>
<author confidence="0.999259">
Dan Klein
</author>
<affiliation confidence="0.988686">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.903837">
Stanford, CA 94305-9040
</address>
<email confidence="0.998797">
klein@cs.stanford.edu
</email>
<author confidence="0.993373">
Christopher D. Manning
</author>
<affiliation confidence="0.9880785">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.903773">
Stanford, CA 94305-9040
</address>
<email confidence="0.998855">
manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995784090909091">
We present a generative model for the unsupervised
learning of dependency structures. We also describe
the multiplicative combination of this dependency model
with a model of linear constituency. The product model
outperforms both components on their respective evalu-
ation metrics, giving the best published figures for un-
supervised dependency parsing and unsupervised con-
stituency parsing. We also demonstrate that the com-
bined model works and is robust cross-linguistically, be-
ing able to exploit either attachment or distributional reg-
ularities that are salient in the data.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998736875">
The task of statistically inducing hierarchical syn-
tactic structure over unannotated sentences of nat-
ural language has received a great deal of atten-
tion (Carroll and Charniak, 1992; Pereira and Sch-
abes, 1992; Brill, 1993; Stolcke and Omohundro,
1994). Researchers have explored this problem for
a variety of reasons: to argue empirically against
the poverty of the stimulus (Clark, 2001), to use
induction systems as a first stage in constructing
large treebanks (van Zaanen, 2000), to build better
language models (Baker, 1979; Chen, 1995), and
to examine cognitive issues in language learning
(Solan et al., 2003). An important distinction should
be drawn between work primarily interested in the
weak generative capacity of models, where model-
ing hierarchical structure is only useful insofar as it
leads to improved models over observed structures
(Baker, 1979; Chen, 1995), and work interested in
the strong generative capacity of models, where the
unobserved structure itself is evaluated (van Zaa-
nen, 2000; Clark, 2001; Klein and Manning, 2002).
This paper falls into the latter category; we will be
inducing models of linguistic constituency and de-
pendency with the goal of recovering linguistically
plausible structures. We make no claims as to the
cognitive plausibility of the induction mechanisms
we present here; however, the ability of these sys-
tems to recover substantial linguistic patterns from
surface yields alone does speak to the strength of
support for these patterns in the data, and hence un-
dermines arguments based on “the poverty of the
stimulus” (Chomsky, 1965).
</bodyText>
<sectionHeader confidence="0.988637" genericHeader="method">
2 Unsupervised Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999925166666667">
Most recent progress in unsupervised parsing has
come from tree or phrase-structure grammar based
models (Clark, 2001; Klein and Manning, 2002),
but there are compelling reasons to reconsider un-
supervised dependency parsing. First, most state-of-
the-art supervised parsers make use of specific lexi-
cal information in addition to word-class level infor-
mation – perhaps lexical information could be a use-
ful source of information for unsupervised methods.
Second, a central motivation for using tree struc-
tures in computational linguistics is to enable the
extraction of dependencies – function-argument and
modification structures – and it might be more ad-
vantageous to induce such structures directly. Third,
as we show below, for languages such as Chinese,
which have few function words, and for which the
definition of lexical categories is much less clear,
dependency structures may be easier to detect.
</bodyText>
<subsectionHeader confidence="0.970212">
2.1 Representation and Evaluation
</subsectionHeader>
<bodyText confidence="0.999937642857143">
An example dependency representation of a short
sentence is shown in figure 1(a), where, follow-
ing the traditional dependency grammar notation,
the regent or head of a dependency is marked with
the tail of the dependency arrow, and the dependent
is marked with the arrowhead (Mel&apos;ˇcuk, 1988). It
will be important in what follows to see that such
a representation is isomorphic (in terms of strong
generative capacity) to a restricted form of phrase
structure grammar, where the set of terminals and
nonterminals is identical, and every rule is of the
form X → X Y or X → Y X (Miller, 1999), giving
the isomorphic representation of figure 1(a) shown
in figure 1(b).1 Depending on the model, part-of-
</bodyText>
<footnote confidence="0.5893565">
1Strictly, such phrase structure trees are isomorphic not to
flat dependency structures, but to specific derivations of those
</footnote>
<figure confidence="0.994766333333333">
VBD
fell
in
fell
IN
NN
in
September
Factory
payrolls
NNS
VBD
ROOT
NN
NNS
VBD
IN
NN
VBD
NNS
NN
IN
Factory
payrolls
September
NN
NP
NNS
S
VBD
PP
VP
Factory
payrolls
fell
NN
IN
in September
(a) Classical Dependency Structure (b) Dependency Structure as CF Tree (c) CFG Structure
</figure>
<figureCaption confidence="0.999981">
Figure 1: Three kinds of parse structures.
</figureCaption>
<bodyText confidence="0.999511931818182">
speech categories may be included in the depen-
dency representation, as shown here, or dependen-
cies may be directly between words. Below, we will
assume an additonal reserved nonterminal ROOT,
whose sole dependent is the head of the sentence.
This simplifies the notation, math, and the evalua-
tion metric.
A dependency analysis will always consist of ex-
actly as many dependencies as there are words in the
sentence. For example, in the dependency structure
of figure 1(b), the dependencies are {(ROOT, fell),
(fell, payrolls), (fell, in), (in, September), (payrolls,
Factory)}. The quality of a hypothesized depen-
dency structure can hence be evaluated by accuracy
as compared to a gold-standard dependency struc-
ture, by reporting the percentage of dependencies
shared between the two analyses.
In the next section, we discuss several models of
dependency structure, and throughout this paper we
report the accuracy of various methods at recover-
ing gold-standard dependency parses from various
corpora, detailed here. WSJ is the entire Penn En-
glish Treebank WSJ portion. WSJ10 is the subset
of sentences which contained 10 words or less after
the removal of punctuation. CTB10 is the sentences
of the same length from the Penn Chinese treebank
(v3). NEGRA10 is the same, for the German NE-
GRA corpus, based on the supplied conversion of
the NEGRA corpus into Penn treebank format. In
most of the present experiments, the provided parts-
of-speech were used as the input alphabet, though
we also present limited experimentation with syn-
thetic parts-of-speech.
It is important to note that the Penn treebanks do
not include dependency annotations; however, the
automatic dependency rules from (Collins, 1999)
are sufficiently accurate to be a good benchmark
for unsupervised systems for the time being (though
see below for specific issues). Similar head-finding
rules were used for Chinese experiments. The NE-
GRA corpus, however, does supply hand-annotated
dependency structures.
structures which specify orders of attachment among multiple
dependents which share a common head.
</bodyText>
<listItem confidence="0.707811">
• • • • •ROOT
</listItem>
<figureCaption confidence="0.995653">
Figure 2: Dependency graph with skeleton chosen, but
words not populated.
</figureCaption>
<bodyText confidence="0.999991333333333">
Where possible, we report an accuracy figure for
both directed and undirected dependencies. Report-
ing undirected numbers has two advantages: first, it
facilitates comparison with earlier work, and, more
importantly, it allows one to partially obscure the
effects of alternate analyses, such as the system-
atic choice between a modal and a main verb for
the head of a sentence (in either case, the two verbs
would be linked, but the direction would vary).
</bodyText>
<subsectionHeader confidence="0.99365">
2.2 Dependency Models
</subsectionHeader>
<bodyText confidence="0.999980296296296">
The dependency induction task has received rela-
tively little attention; the best known work is Car-
roll and Charniak (1992), Yuret (1998), and Paskin
(2002). All systems that we are aware of operate un-
der the assumption that the probability of a depen-
dency structure is the product of the scores of the
dependencies (attachments) in that structure. De-
pendencies are seen as ordered (head, dependent)
pairs of words, but the score of a dependency can
optionally condition on other characteristics of the
structure, most often the direction of the depen-
dency (whether the arrow points left or right).
Some notation before we present specific mod-
els: a dependency d is a pair (h, a) of a head and
argument, which are words in a sentence s, in a cor-
pus S. For uniformity of notation with section 4,
words in s are specified as size-one spans of s: for
example the first word would be 0s1. A dependency
structure D over a sentence is a set of dependencies
(arcs) which form a planar, acyclic graph rooted at
the special symbol ROOT, and in which each word
in s appears as an argument exactly once. For a de-
pendency structure D, there is an associated graph
G which represents the number of words and arrows
between them, without specifying the words them-
selves (see figure 2). A graph G and sentence s to-
gether thus determine a dependency structure. The
</bodyText>
<table confidence="0.998445473684211">
Model Dir. Undir.
English (WSJ)
Paskin 01 39.7
RANDOM 41.7
Charniak and Carroll 92-inspired 44.7
ADJACENT 53.2
DMV 54.4
English (WSJ10)
RANDOM 30.1 45.6
ADJACENT 33.6 56.7
DMV 43.2 63.7
German (NEGRA10)
RANDOM 21.8 41.5
ADJACENT 32.6 51.2
DMV 36.3 55.8
Chinese (CTB10)
RANDOM 35.9 47.3
ADJACENT 30.2 47.3
DMV 42.5 54.2
</table>
<figureCaption confidence="0.997069">
Figure 3: Parsing performance (directed and undirected
dependency accuracy) of various dependency models on
various treebanks, along with baselines.
</figureCaption>
<bodyText confidence="0.9992216">
dependency structure is the object generated by all
of the models that follow; the steps in the deriva-
tions vary from model to model.
Existing generative dependency models intended
for unsupervised learning have chosen to first gen-
erate a word-free graph G, then populate the sen-
tence s conditioned on G. For instance, the model of
Paskin (2002), which is broadly similar to the semi-
probabilistic model in Yuret (1998), first chooses a
graph G uniformly at random (such as figure 2),
then fills in the words, starting with a fixed root
symbol (assumed to be at the rightmost end), and
working down G until an entire dependency struc-
ture D is filled in (figure 1a). The corresponding
probabilistic model is
</bodyText>
<equation confidence="0.9961352">
P(D) = P(s, G)
= P(G)P(s|G)
�
= P(G)
(i, j,dir)EG
</equation>
<bodyText confidence="0.999964619047619">
In Paskin (2002), the distribution P(G) is fixed to be
uniform, so the only model parameters are the con-
ditional multinomial distributions P(a|h, dir) that
encode which head words take which other words
as arguments. The parameters for left and right ar-
guments of a single head are completely indepen-
dent, while the parameters for first and subsequent
arguments in the same direction are identified.
In those experiments, the model above was
trained on over 30M words of raw newswire, using
EM in an entirely unsupervised fashion, and at great
computational cost. However, as shown in figure 3,
the resulting parser predicted dependencies at be-
low chance level (measured by choosing a random
dependency structure). This below-random perfor-
mance seems to be because the model links word
pairs which have high mutual information (such
as occurrences of congress and bill) regardless of
whether they are plausibly syntactically related. In
practice, high mutual information between words is
often stronger between two topically similar nouns
than between, say, a preposition and its object.
One might hope that the problem with this model
is that the actual lexical items are too semanti-
cally charged to represent workable units of syn-
tactic structure. If one were to apply the Paskin
(2002) model to dependency structures parameter-
ized simply on the word-classes, the result would
be isomorphic to the “dependency PCFG” models
described in Carroll and Charniak (1992). In these
models, Carroll and Charniak considered PCFGs
with precisely the productions (discussed above)
that make them isomorphic to dependency gram-
mars, with the terminal alphabet being simply parts-
of-speech. Here, the rule probabilities are equiva-
lent to P(Y|X, right) and P(Y|X, left) respectively.2
The actual experiments in Carroll and Charniak
(1992) do not report accuracies that we can compare
to, but they suggest that the learned grammars were
of extremely poor quality. With hindsight, however,
the main issue in their experiments appears to be not
their model, but that they randomly initialized the
production (attachment) probabilities. As a result,
their learned grammars were of very poor quality
and had high variance. However, one nice property
of their structural constraint, which all dependency
models share, is that the symbols in the grammar are
not symmetric. Even with a grammar in which the
productions are initially uniform, a symbol X can
only possibly have non-zero posterior likelihood
over spans which contain a matching terminal X.
Therefore, one can start with uniform rewrites and
let the interaction between the data and the model
structure break the initial symmetry. If one recasts
their experiments in this way, they achieve an accu-
racy of 44.7% on the Penn treebank, which is higher
than choosing a random dependency structure, but
lower than simply linking all adjacent words into a
left-headed (and right-branching) structure (53.2%).
A huge limitation of both of the above models is
that they are incapable of encoding even first-order
valence facts. For example, the latter model learns
that nouns to the left of the verb (usually subjects)
</bodyText>
<footnote confidence="0.911452">
2There is another, subtle distinction: in the Paskin work,
a canonical ordering of multiple attachments was fixed, while
in the Carroll and Charniak work all attachment orders are con-
sidered, giving a numerical bias towards structures where heads
take more than one argument.
</footnote>
<equation confidence="0.458042">
P(i−1si|j−1sj, dir) .
h hl hl Fhl
h Fal Fal hl h STOP STOP hl
i j k i j k i j i j
(a) (b) (c) (d)
</equation>
<figureCaption confidence="0.9979015">
Figure 4: Dependency configurations in a lexicalized tree: (a) right attachment, (b) left attachment, (c) right stop, (d)
left stop. h and a are head and argument words, respectively, while i, j, and k are positions between words.
</figureCaption>
<bodyText confidence="0.998655">
attach to the verb. But then, given a NOUN NOUN
VERB sequence, both nouns will attach to the verb
– there is no way that the model can learn that verbs
have exactly one subject. We now turn to an im-
proved dependency model that addresses this prob-
lem.
</bodyText>
<sectionHeader confidence="0.997125" genericHeader="method">
3 An Improved Dependency Model
</sectionHeader>
<bodyText confidence="0.999965261538462">
The dependency models discussed above are dis-
tinct from dependency models used inside high-
performance supervised probabilistic parsers in sev-
eral ways. First, in supervised models, a head out-
ward process is modeled (Eisner, 1996; Collins,
1999). In such processes, heads generate a sequence
of arguments outward to the left or right, condition-
ing on not only the identity of the head and direc-
tion of the attachment, but also on some notion of
distance or valence. Moreover, in a head-outward
model, it is natural to model stop steps, where the
final argument on each side of a head is always the
special symbol STOP. Models like Paskin (2002)
avoid modeling STOP by generating the graph skele-
ton G first, uniformly at random, then populating
the words of s conditioned on G. Previous work
(Collins, 1999) has stressed the importance of in-
cluding termination probabilities, which allows the
graph structure to be generated jointly with the ter-
minal words, precisely because it does allow the
modeling of required dependents.
We propose a simple head-outward dependency
model over word classes which includes a model
of valence, which we call DMV (for dependency
model with valence). We begin at the ROOT. In the
standard way, each head generates a series of non-
STOP arguments to one side, then a STOP argument
to that side, then non-STOP arguments to the other
side, then a second STOP.
For example, in the dependency structure in fig-
ure 1, we first generate a single child of ROOT, here
fell. Then we recurse to the subtree under fell. This
subtree begins with generating the right argument
in. We then recurse to the subtree under in (gener-
ating September to the right, a right STOP, and a left
STOP). Since there are no more right arguments af-
ter in, its right STOP is generated, and the process
moves on to the left arguments offell.
In this process, there are two kinds of deriva-
tion events, whose local probability factors consti-
tute the model’s parameters. First, there is the de-
cision at any point whether to terminate (generate
STOP) or not: PSTOP(STOP1h, dir, adj). This is a bi-
nary decision conditioned on three things: the head
h, the direction (generating to the left or right of
the head), and the adjacency (whether or not an ar-
gument has been generated yet in the current di-
rection, a binary variable). The stopping decision
is estimated directly, with no smoothing. If a stop
is generated, no more arguments are generated for
the current head to the current side. If the current
head’s argument generation does not stop, another
argument is chosen using: PCHOOSE(aIh, dir). Here,
the argument is picked conditionally on the iden-
tity of the head (which, recall, is a word class) and
the direction. This term, also, is not smoothed in
any way. Adjacency has no effect on the identity
of the argument, only on the likelihood of termina-
tion. After an argument is generated, its subtree in
the dependency structure is recursively generated.
Formally, for a dependency structure D, let
each word h have left dependents depsD(h,l)
and right dependents depsD(h, r). The follow-
ing recursion defines the probability of the frag-
ment D(h) of the dependency tree rooted at h:
</bodyText>
<equation confidence="0.98749025">
P(D(h)) = � H PSTOP(—STOPIh, dir, adj)
direll,r} aedepsD(h,dir)
PCHOOSE(alh,dir)P(D(a))
PSTOP(STOPlh, dir, adj)
</equation>
<bodyText confidence="0.961236771428571">
One can view a structure generated by this deriva-
tional process as a “lexicalized” tree composed of
the local binary and unary context-free configura-
tions shown in figure 4.3 Each configuration equiv-
alently represents either a head-outward derivation
step or a context-free rewrite rule. There are four
such configurations. Figure 4(a) shows a head h
3It is lexicalized in the sense that the labels in the tree are
derived from terminal symbols, but in our experiments the ter-
minals were word classes, not individual lexical items.
taking a right argument a. The tree headed by h
contains h itself, possibly some right arguments of
h, but no left arguments of h (they attach after all
the right arguments). The tree headed by a contains
a itself, along with all of its left and right children.
Figure 4(b) shows a head h taking a left argument a
– the tree headed by h must have already generated
its right stop to do so. Figure 4(c) and figure 4(d)
show the sealing operations, where STOP derivation
steps are generated. The left and right marks on
node labels represent left and right STOPs that have
been generated.4
The basic inside-outside algorithm (Baker, 1979)
can be used for re-estimation. For each sentence
s E S, it gives us cs(x : i, j), the expected frac-
tion of parses of s with a node labeled x extend-
ing from position i to position j. The model can
be re-estimated from these counts. For example, to
re-estimate an entry of PSTOP(STOP|h, left, non-adj)
according to a current model O, we calculate two
quantities.5 The first is the (expected) number of
trees headed by h] whose rightmost edge i is strictly
left of h. The second is the number of trees headed
by Fh] with rightmost edge i strictly left of h. The
ratio is the MLE of that local probability factor:
</bodyText>
<equation confidence="0.97069">
PSTOP(STOP|h, left, non-adj) =
PsES K&lt;loc(h) Pk c(h] : i, k)
PsES K&lt;loc(h) Pk c(Fh] : i, k)
</equation>
<bodyText confidence="0.999683571428571">
This can be intuitively thought of as the relative
number of times a tree headed by h had already
taken at least one argument to the left, had an op-
portunity to take another, but didn’t.6
Initialization is important to the success of any
local search procedure. We chose to initialize EM
not with an initial model, but with an initial guess
at posterior distributions over dependency structures
(completions). For the first-round, we constructed
a somewhat ad-hoc “harmonic” completion where
all non-ROOT words took the same number of ar-
guments, and each took other words as arguments
in inverse proportion to (a constant plus) the dis-
tance between them. The ROOT always had a single
</bodyText>
<footnote confidence="0.969827363636364">
4Note that the asymmetry of the attachment rules enforces
the right-before-left attachment convention. This is harmless
and arbitrary as far as dependency evaluations go, but imposes
an x-bar-like structure on the constituency assertions made by
this model. This bias/constraint is dealt with in section 5.
5To simplify notation, we assume each word h occurs at
most one time in a given sentence, between indexes loc(h) and
loc(h) + 1).
6As a final note, in addition to enforcing the right-argument-
first convention, we constrained ROOT to have at most a single
dependent, by a similar device.
</footnote>
<bodyText confidence="0.999976043478261">
argument and took each word with equal probabil-
ity. This structure had two advantages: first, when
testing multiple models, it is easier to start them all
off in a common way by beginning with an M-step,
and, second, it allowed us to point the model in the
vague general direction of what linguistic depen-
dency structures should look like.
On the WSJ10 corpus, the DMV model recov-
ers a substantial fraction of the broad dependency
trends: 43.2% of guessed directed dependencies
were correct (63.7% ignoring direction). To our
knowledge, this is the first published result to break
the adjacent-word heuristic (at 33.6% for this cor-
pus). Verbs are the sentence heads, prepositions
take following noun phrases as arguments, adverbs
attach to verbs, and so on. The most common source
of discrepancy between the test dependencies and
the model’s guesses is a result of the model system-
atically choosing determiners as the heads of noun
phrases, while the test trees have the rightmost noun
as the head. The model’s choice is supported by
a good deal of linguistic research (Abney, 1987),
and is sufficiently systematic that we also report the
scores where the NP headship rule is changed to per-
colate determiners when present. On this adjusted
metric, the score jumps hugely to 55.7% directed
(and 67.9% undirected).
This model also works on German and Chinese at
above-baseline levels (55.8% and 54.2% undirected,
respectively), with no modifications whatsoever. In
German, the largest source of errors is also the
systematic postulation of determiner-headed noun-
phrases. In Chinese, the primary mismatch is that
subjects are considered to be the heads of sentences
rather than verbs.
This dependency induction model is reasonably
successful. However, our intuition is still that the
model can be improved by paying more attention
to syntactic constituency. To this end, after briefly
recapping the model of Klein and Manning (2002),
we present a combined model that exploits depen-
dencies and constituencies. As we will see, this
combined model finds correct dependencies more
successfully than the model above, and finds con-
stituents more successfully than the model of Klein
and Manning (2002).
</bodyText>
<sectionHeader confidence="0.994224" genericHeader="method">
4 Distributional Constituency Induction
</sectionHeader>
<bodyText confidence="0.998682">
In linear distributional clustering, items (e.g., words
or word sequences) are represented by characteristic
distributions over their linear contexts (e.g., multi-
nomial models over the preceding and following
words, see figure 5). These context distributions
are then clustered in some way, often using standard
</bodyText>
<figure confidence="0.987777272727273">
Span Label Constituent Context
(0,5) S NN NNS VBD IN NN O – O
(0,2) NP NNNNS O – VBD
(2,5) VP VBD IN NN NNS – O
(3,5) PP IN NN VBD – O
(0,1) NN NN O – NNS
(1,2) NNS NNS NN – VBD
(2,3) VBD VBD NNS – IN
(3,4) IN IN VBD – NN
(4,5) NN NNS IN – O
(a) (b)
</figure>
<figureCaption confidence="0.9874714">
Figure 5: The CCM model’s generative process for the
sentence in figure 1. (a) A binary tree-equivalent brack-
eting is chosen at random. (b) Each span generates its
yield and context (empty spans not shown here). Deriva-
tions which are not coherent are given mass zero.
</figureCaption>
<bodyText confidence="0.998362222222222">
i−1si — jsj+1 (see figure 5). The model generates
all constituent-context pairs, span by span.
The first stage is to choose a bracketing B for
the sentence, which is a maximal non-crossing sub-
set of the spans (equivalent to a binary tree). In
the basic model, P(B) is uniform over binary trees.
Then, for each (i, j), the subspan and context pair
(isj, i−1si — jsj+1) is generated via a class-
conditional independence model:
</bodyText>
<equation confidence="0.9944175">
P(s, B) = P(B) F1 P(isj|bij)P(i−1si — jsj+1|bij)
(i,j)
</equation>
<bodyText confidence="0.999696666666667">
data clustering methods. In the most common case,
the items are words, and one uses distributions over
adjacent words to induce word classes. Previous
work has shown that even this quite simple repre-
sentation allows the induction of quite high quality
word classes, largely corresponding to traditional
parts of speech (Finch, 1993; Sch¨utze, 1995; Clark,
2000). A typical pattern would be that stocks and
treasuries both frequently occur before the words
fell and rose, and might therefore be put into the
same class.
Clark (2001) and Klein and Manning (2002)
show that this approach can be successfully used
for discovering syntactic constituents as well. How-
ever, as one might expect, it is easier to cluster
word sequences (or word class sequences) than to
tell how to put them together into trees. In par-
ticular, if one is given all contiguous subsequences
(subspans) from a corpus of sentences, most natu-
ral clusters will not represent valid constituents (to
the extent that constituency of a non-situated se-
quence is even a well-formed notion). For exam-
ple, it is easy enough to discover that DET N and
DET ADJ N are similar and that V PREP DET and
V PREP DET ADJ are similar, but it is much less
clear how to discover that the former pair are gen-
erally constituents while the latter pair are generally
not. In Klein and Manning (2002), we proposed a
constituent-context model (CCM) which solves this
problem by building constituency decisions directly
into the distributional model, by earmarking a sin-
gle cluster d for non-constituents. During the cal-
culation of cluster assignments, only a non-crossing
subset of the observed word sequences can be as-
signed to other, constituent clusters. This integrated
approach is empirically successful.
The CCM works as follows. Sentences are given
as sequences s of word classes (parts-of-speech or
otherwise). One imagines each sentence as a list
of the O(n2) index pairs (i, j), each followed by
the corresponding subspan isj and linear context
That is, all spans guess their sequences and contexts
given only a constituency decision b.7
This is a model P(s, B) over hidden bracketings
and observed sentences, and it is estimated via EM
to maximize the sentence likelihoods P(s) over the
training corpus. Figure 6 shows the accuracy of the
CCM model not only on English but for the Chinese
and German corpora discussed above.8 Results are
reported at convergence; for the English case, F1
is monotonic during training, while for the others,
there is an earlier peak.
Also shown is an upper bound (the target trees are
not all binary and so any all-binary system will over-
propose constituents). Klein and Manning (2002)
gives comparative numbers showing that the basic
CCM outperforms other recent systems on the ATIS
corpus (which many other constituency induction
systems have reported on). While absolute numbers
are hard to compare across corpora, all the systems
compared to in Klein and Manning (2002) parsed
below a right-branching baseline, while the CCM is
substantially above it.
</bodyText>
<sectionHeader confidence="0.994138" genericHeader="method">
5 A Combined Model
</sectionHeader>
<bodyText confidence="0.9997746">
The two models described above have some com-
mon ground. Both can be seen as models over lexi-
calized trees composed of the configurations in fig-
ure 4. For the DMV, it is already a model over these
structures. At the “attachment” rewrite for the CCM
</bodyText>
<footnote confidence="0.874991166666667">
7As is typical of distributional clustering, positions in the
corpus can get generated multiple times. Since derivations
need not be consistent, the entire model is mass deficient when
viewed as a model over sentences.
8In Klein and Manning (2002), we reported results using
unlabeled bracketing statistics which gave no credit for brack-
ets which spanned the entire sentence (raising the scores) but
macro-averaged over sentences (lowering the scores). The
numbers here hew more closely to the standard methods used
for evaluating supervised parsers, by being micro-averaged and
including full-span brackets. However, the scores are, overall,
approximately the same.
</footnote>
<bodyText confidence="0.644462">
in (a/b), we assign the quantity:
</bodyText>
<equation confidence="0.9996395">
P(isk|true)P(i−1si ∼ ksk+1|true)
P(isk|false)P(i−1si ∼ ksk+1|false)
</equation>
<bodyText confidence="0.99987075">
which is the odds ratio of generating the subse-
quence and context for span hi, ki as a constituent
as opposed to a non-constituent. If we multiply all
trees’ attachment scores by
</bodyText>
<equation confidence="0.488085">
Hhi,ji P(isj|false)P(i−1si ∼ jsj+1|false)
</equation>
<bodyText confidence="0.999650081081081">
the denominators of the odds ratios cancel, and we
are left with each tree being assigned the probability
it would have received under the CCM.9
In this way, both models can be seen as generat-
ing either constituency or dependency structures. Of
course, the CCM will generate fairly random depen-
dency structures (constrained only by bracketings).
Getting constituency structures from the DMV is
also problematic, because the choice of which side
to first attach arguments on has ramifications on
constituency – it forces x-bar-like structures – even
though it is an arbitrary convention as far as depen-
dency evaluations are concerned. For example, if
we attach right arguments first, then a verb with a
left subject and a right object will attach the ob-
ject first, giving traditional VPs, while the other at-
tachment order gives subject-verb groups. To avoid
this bias, we alter the DMV in the following ways.
When using the dependency model alone, we allow
each word to have even probability for either gener-
ation order (but in each actual head derivation, only
one order occurs). When using the models together,
better performance was obtained by releasing the
one-side-attaching-first requirement entirely.
In figure 6, we give the behavior of the CCM con-
stituency model and the DMV dependency model
on both constituency and dependency induction.
Unsurprisingly, their strengths are complementary.
The CCM is better at recovering constituency, and
the dependency model is better at recovering depen-
dency structures. It is reasonable to hope that a com-
bination model might exhibit the best of both. In the
supervised parsing domain, for example, scoring a
lexicalized tree with the product of a simple lexical
dependency model and a PCFG model can outper-
form each factor on its respective metric (Klein and
Manning, 2003).
</bodyText>
<footnote confidence="0.669771285714286">
9This scoring function as described is not a generative
model over lexicalized trees, because it has no generation step
at which nodes’ lexical heads are chosen. This can be corrected
by multiplying in a “head choice” factor of 1/(k − j) at each fi-
nal “sealing” configuration (d). In practice, this correction fac-
tor was harmful for the model combination, since it duplicated
a strength of the dependency model, badly.
</footnote>
<table confidence="0.999816307692308">
Model UP UR UF1 Dir Undir
English (WSJ10 – 7422 Sentences)
LBRANCH/RHEAD 25.6 32.6 28.7 33.6 56.7
RANDOM 31.0 39.4 34.7 30.1 45.6
RBRANCH/LHEAD 55.1 70.0 61.7 24.0 55.9
DMV 46.6 59.2 52.1 43.2 62.7
CCM 64.2 81.6 71.9 23.8 43.3
DMV+CCM (POS) 69.3 88.0 77.6 47.5 64.5
DMV+CCM (DISTR.) 65.2 82.8 72.9 42.3 60.4
UBOUND 78.8 100.0 88.1 100.0 100.0
German (NEGRA10 – 2175 Sentences)
LBRANCH/RHEAD 27.4 48.8 35.1 32.6 51.2
RANDOM 27.9 49.6 35.7 21.8 41.5
RBRANCH/LHEAD 33.8 60.1 43.3 21.0 49.9
DMV 38.4 69.5 49.5 40.0 57.8
CCM 48.1 85.5 61.6 25.5 44.9
DMV+CCM 49.6 89.7 63.9 50.6 64.7
UBOUND 56.3 100.0 72.1 100.0 100.0
Chinese (CTB10 – 2437 Sentences)
LBRANCH/RHEAD 26.3 48.8 34.2 30.2 43.9
RANDOM 27.3 50.7 35.5 35.9 47.3
RBRANCH/LHEAD 29.0 53.9 37.8 14.2 41.5
DMV 35.9 66.7 46.7 42.5 54.2
CCM 34.6 64.3 45.0 23.8 40.5
DMV+CCM 33.3 62.0 43.3 55.2 60.3
UBOUND 53.9 100.0 70.1 100.0 100.0
</table>
<figureCaption confidence="0.970761">
Figure 6: Parsing performance of the combined model
on various treebanks, along with baselines.
</figureCaption>
<bodyText confidence="0.99092855319149">
In the combined model, we score each tree with
the product of the probabilities from the individ-
ual models above. We use the inside-outside algo-
rithm to sum over all lexicalized trees, similar to the
situation in section 3. The tree configurations are
shown in figure 4. For each configuration, the rele-
vant scores from each model are multiplied together.
For example, consider figure 4(a). From the CCM
we must generate isk as a constituent and its cor-
responding context. From the dependency model,
we pay the cost of h taking a as a right argument
(PCHOOSE), as well as the cost of choosing not to
stop (PSTOP). We then running the inside-outside al-
gorithm over this product model. For the results,
we can extract the sufficient statistics needed to re-
estimate both individual models.10
The models in combination were intitialized in
the same way as when they were run individually.
Sufficient statistics were separately taken off these
individual completions. From then on, the resulting
models were used together during re-estimation.
Figure 6 summarizes the results. The combined
model beats the CCM on English F1: 77.6 vs. 71.9.
The figure also shows the combination model’s
score when using word classes which were induced
entirely automatically, using the simplest distribu-
tional clustering method of Sch¨utze (1995). These
classes show some degradation, e.g. 72.9 F1, but it
10The product, like the CCM itself, is mass-deficient.
is worth noting that these totally unsupervised num-
bers are better than the performance of the CCM
model of Klein and Manning (2002) running off
of Penn treebank word classes. Again, if we mod-
ify the gold standard so as to make determiners the
head of NPs, then this model with distributional tags
scores 50.6% on directed and 64.8% on undirected
dependency accuracy.
On the German data, the combination again out-
performs each factor alone, though while the com-
bination was most helpful at boosting constituency
quality for English, for German it provided a larger
boost to the dependency structures. Finally, on
the Chinese data, the combination did substantially
boost dependency accuracy over either single factor,
but actually suffered a small drop in constituency.11
Overall, the combination is able to combine the in-
dividual factors in an effective way.
</bodyText>
<sectionHeader confidence="0.999817" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991461538462">
We have presented a successful new dependency-
based model for the unsupervised induction of syn-
tactic structure, which picks up the key ideas that
have made dependency models successful in super-
vised statistical parsing work. We proceeded to
show that it works cross-linguistically. We then
demonstrated how this model could be combined
with the previous best constituent-induction model
to produce a combination which, in general, sub-
stantially outperforms either individual model, on
either metric. A key reason that these models are ca-
pable of recovering structure more accurately than
previous work is that they minimize the amount of
hidden structure that must be induced. In particu-
lar, neither model attempts to learn intermediate, re-
cursive categories with no direct connection to sur-
face statistics. Our results here are just on the un-
grounded induction of syntactic structure. Nonethe-
less, we see the investigation of what patterns can
be recovered from corpora as important, both from a
computational perspective and from a philosophical
one. It demonstrates that the broad constituent and
dependency structure of a language can be recov-
ered quite successfully (individually or, more effec-
tively, jointly) from a very modest amount of train-
ing data.
</bodyText>
<sectionHeader confidence="0.994198" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<footnote confidence="0.732641333333333">
This work was supported by a Microsoft Gradu-
ate Research Fellowship to the first author and by
11This seems to be partially due to the large number of un-
analyzed fragments in the Chinese gold standard, which leave
a very large fraction of the posited bracketings completely un-
judged.
</footnote>
<bodyText confidence="0.9674772">
the Advanced Research and Development Activity
(ARDA)’s Advanced Question Answering for Intel-
ligence (AQUAINT) Program. This work also ben-
efited from an enormous amount of useful feedback,
from many audiences and individuals.
</bodyText>
<sectionHeader confidence="0.995788" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999956360655738">
Stephen P. Abney. 1987. The English Noun Phrase in its Sentential
Aspect. Ph.D. thesis, MIT.
James K. Baker. 1979. Trainable grammars for speech recognition. In
D. H. Klatt and J. J. Wolf, editors, Speech Communication Papers
for the 97th Meeting of the Acoustical Society of America, pages
547–550.
Eric Brill. 1993. Automatic grammar induction and parsing free text:
A transformation-based approach. In ACL 31, pages 259–265.
Glenn Carroll and Eugene Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from corpora. In Carl
Weir, Stephen Abney, Ralph Grishman, and Ralph Weischedel, edi-
tors, Working Notes of the Workshop Statistically-Based NLP Tech-
niques, pages 1–13. AAAI Press, Menlo Park, CA.
Stanley F. Chen. 1995. Bayesian grammar induction for language
modeling. In ACL 33, pages 228–235.
Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press,
Cambridge, MA.
Alexander Clark. 2000. Inducing syntactic categories by context distri-
bution clustering. In The Fourth Conference on Natural Language
Learning.
Alexander Clark. 2001. Unsupervised induction of stochastic context-
free grammars using distributional clustering. In The Fifth Confer-
ence on Natural Language Learning.
Michael Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
Jason Eisner. 1996. Three new probabilistic models for dependency
parsing: An exploration. In COLING 16, pages 340–345.
Steven Paul Finch. 1993. Finding Structure in Language. Ph.D. thesis,
University of Edinburgh.
Dan Klein and Christopher D. Manning. 2002. A generative
constituent-context model for improved grammar induction. In ACL
40, pages 128–135.
Dan Klein and Christopher D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In Suzanna
Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances
in Neural Information Processing Systems 15, Cambridge, MA.
MIT Press.
Igor Aleksandrovich Melcuk. 1988. Dependency Syntax: theory and
practice. State University of New York Press, Albany, NY.
Philip H. Miller. 1999. Strong Generative Capacity. CSLI Publications,
Stanford, CA.
Mark A. Paskin. 2002. Grammatical bigrams. In T. G. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in Neural Infor-
mation Processing Systems 14, Cambridge, MA. MIT Press.
Fernando Pereira and Yves Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL 30, pages 128–135.
Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging. In EACL
7, pages 141–148.
Zach Solan, Eytan Ruppin, David Horn, and Shimon Edelman. 2003.
Automatic acquisition and efficient representation of syntactic
structures. In Suzanna Becker, Sebastian Thrun, and Klaus Ober-
mayer, editors, Advances in Neural Information Processing Systems
15, Cambridge, MA. MIT Press.
Andreas Stolcke and Stephen M. Omohundro. 1994. Inducing proba-
bilistic grammars by Bayesian model merging. In Grammatical In-
ference and Applications: Proceedings of the Second International
Colloquium on Grammatical Inference. Springer Verlag.
Menno van Zaanen. 2000. ABL: Alignment-based learning. In COL-
ING 18, pages 961–967.
Deniz Yuret. 1998. Discovery of Linguistic Relations Using Lexical
Attraction. Ph.D. thesis, MIT.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976313">
<title confidence="0.998643">Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency</title>
<author confidence="0.999996">Dan Klein</author>
<affiliation confidence="0.999924">Computer Science Department Stanford University</affiliation>
<address confidence="0.999718">Stanford, CA 94305-9040</address>
<email confidence="0.999638">klein@cs.stanford.edu</email>
<author confidence="0.999845">Christopher D Manning</author>
<affiliation confidence="0.999867">Computer Science Department Stanford University</affiliation>
<address confidence="0.999176">Stanford, CA 94305-9040</address>
<email confidence="0.999501">manning@cs.stanford.edu</email>
<abstract confidence="0.99839475">present a generative model for the learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for undependency parsing constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen P Abney</author>
</authors>
<title>The English Noun Phrase in its Sentential Aspect.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="21387" citStr="Abney, 1987" startWordPosition="3506" endWordPosition="3507">dencies were correct (63.7% ignoring direction). To our knowledge, this is the first published result to break the adjacent-word heuristic (at 33.6% for this corpus). Verbs are the sentence heads, prepositions take following noun phrases as arguments, adverbs attach to verbs, and so on. The most common source of discrepancy between the test dependencies and the model’s guesses is a result of the model systematically choosing determiners as the heads of noun phrases, while the test trees have the rightmost noun as the head. The model’s choice is supported by a good deal of linguistic research (Abney, 1987), and is sufficiently systematic that we also report the scores where the NP headship rule is changed to percolate determiners when present. On this adjusted metric, the score jumps hugely to 55.7% directed (and 67.9% undirected). This model also works on German and Chinese at above-baseline levels (55.8% and 54.2% undirected, respectively), with no modifications whatsoever. In German, the largest source of errors is also the systematic postulation of determiner-headed nounphrases. In Chinese, the primary mismatch is that subjects are considered to be the heads of sentences rather than verbs. </context>
</contexts>
<marker>Abney, 1987</marker>
<rawString>Stephen P. Abney. 1987. The English Noun Phrase in its Sentential Aspect. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition. In</title>
<date>1979</date>
<booktitle>Speech Communication Papers for the 97th Meeting of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<editor>D. H. Klatt and J. J. Wolf, editors,</editor>
<contexts>
<context position="1438" citStr="Baker, 1979" startWordPosition="201" endWordPosition="202">chment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of l</context>
<context position="18325" citStr="Baker, 1979" startWordPosition="2983" endWordPosition="2984">a. The tree headed by h contains h itself, possibly some right arguments of h, but no left arguments of h (they attach after all the right arguments). The tree headed by a contains a itself, along with all of its left and right children. Figure 4(b) shows a head h taking a left argument a – the tree headed by h must have already generated its right stop to do so. Figure 4(c) and figure 4(d) show the sealing operations, where STOP derivation steps are generated. The left and right marks on node labels represent left and right STOPs that have been generated.4 The basic inside-outside algorithm (Baker, 1979) can be used for re-estimation. For each sentence s E S, it gives us cs(x : i, j), the expected fraction of parses of s with a node labeled x extending from position i to position j. The model can be re-estimated from these counts. For example, to re-estimate an entry of PSTOP(STOP|h, left, non-adj) according to a current model O, we calculate two quantities.5 The first is the (expected) number of trees headed by h] whose rightmost edge i is strictly left of h. The second is the number of trees headed by Fh] with rightmost edge i strictly left of h. The ratio is the MLE of that local probabili</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In D. H. Klatt and J. J. Wolf, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, pages 547–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: A transformation-based approach.</title>
<date>1993</date>
<booktitle>In ACL 31,</booktitle>
<pages>259--265</pages>
<contexts>
<context position="1131" citStr="Brill, 1993" startWordPosition="154" endWordPosition="155">duct model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved </context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993. Automatic grammar induction and parsing free text: A transformation-based approach. In ACL 31, pages 259–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Eugene Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<booktitle>Working Notes of the Workshop Statistically-Based NLP Techniques,</booktitle>
<pages>1--13</pages>
<editor>In Carl Weir, Stephen Abney, Ralph Grishman, and Ralph Weischedel, editors,</editor>
<publisher>AAAI Press,</publisher>
<location>Menlo Park, CA.</location>
<contexts>
<context position="1091" citStr="Carroll and Charniak, 1992" startWordPosition="145" endWordPosition="148">ency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only</context>
<context position="7377" citStr="Carroll and Charniak (1992)" startWordPosition="1139" endWordPosition="1143">en, but words not populated. Where possible, we report an accuracy figure for both directed and undirected dependencies. Reporting undirected numbers has two advantages: first, it facilitates comparison with earlier work, and, more importantly, it allows one to partially obscure the effects of alternate analyses, such as the systematic choice between a modal and a main verb for the head of a sentence (in either case, the two verbs would be linked, but the direction would vary). 2.2 Dependency Models The dependency induction task has received relatively little attention; the best known work is Carroll and Charniak (1992), Yuret (1998), and Paskin (2002). All systems that we are aware of operate under the assumption that the probability of a dependency structure is the product of the scores of the dependencies (attachments) in that structure. Dependencies are seen as ordered (head, dependent) pairs of words, but the score of a dependency can optionally condition on other characteristics of the structure, most often the direction of the dependency (whether the arrow points left or right). Some notation before we present specific models: a dependency d is a pair (h, a) of a head and argument, which are words in </context>
<context position="11292" citStr="Carroll and Charniak (1992)" startWordPosition="1796" endWordPosition="1799">currences of congress and bill) regardless of whether they are plausibly syntactically related. In practice, high mutual information between words is often stronger between two topically similar nouns than between, say, a preposition and its object. One might hope that the problem with this model is that the actual lexical items are too semantically charged to represent workable units of syntactic structure. If one were to apply the Paskin (2002) model to dependency structures parameterized simply on the word-classes, the result would be isomorphic to the “dependency PCFG” models described in Carroll and Charniak (1992). In these models, Carroll and Charniak considered PCFGs with precisely the productions (discussed above) that make them isomorphic to dependency grammars, with the terminal alphabet being simply partsof-speech. Here, the rule probabilities are equivalent to P(Y|X, right) and P(Y|X, left) respectively.2 The actual experiments in Carroll and Charniak (1992) do not report accuracies that we can compare to, but they suggest that the learned grammars were of extremely poor quality. With hindsight, however, the main issue in their experiments appears to be not their model, but that they randomly in</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Carl Weir, Stephen Abney, Ralph Grishman, and Ralph Weischedel, editors, Working Notes of the Workshop Statistically-Based NLP Techniques, pages 1–13. AAAI Press, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Bayesian grammar induction for language modeling.</title>
<date>1995</date>
<booktitle>In ACL 33,</booktitle>
<pages>228--235</pages>
<contexts>
<context position="1451" citStr="Chen, 1995" startWordPosition="203" endWordPosition="204">tributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic con</context>
</contexts>
<marker>Chen, 1995</marker>
<rawString>Stanley F. Chen. 1995. Bayesian grammar induction for language modeling. In ACL 33, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2499" citStr="Chomsky, 1965" startWordPosition="366" endWordPosition="367"> itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic constituency and dependency with the goal of recovering linguistically plausible structures. We make no claims as to the cognitive plausibility of the induction mechanisms we present here; however, the ability of these systems to recover substantial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data, and hence undermines arguments based on “the poverty of the stimulus” (Chomsky, 1965). 2 Unsupervised Dependency Parsing Most recent progress in unsupervised parsing has come from tree or phrase-structure grammar based models (Clark, 2001; Klein and Manning, 2002), but there are compelling reasons to reconsider unsupervised dependency parsing. First, most state-ofthe-art supervised parsers make use of specific lexical information in addition to word-class level information – perhaps lexical information could be a useful source of information for unsupervised methods. Second, a central motivation for using tree structures in computational linguistics is to enable the extraction</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<date>2000</date>
<booktitle>In The Fourth Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="24209" citStr="Clark, 2000" startWordPosition="3977" endWordPosition="3978">nary tree). In the basic model, P(B) is uniform over binary trees. Then, for each (i, j), the subspan and context pair (isj, i−1si — jsj+1) is generated via a classconditional independence model: P(s, B) = P(B) F1 P(isj|bij)P(i−1si — jsj+1|bij) (i,j) data clustering methods. In the most common case, the items are words, and one uses distributions over adjacent words to induce word classes. Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes, largely corresponding to traditional parts of speech (Finch, 1993; Sch¨utze, 1995; Clark, 2000). A typical pattern would be that stocks and treasuries both frequently occur before the words fell and rose, and might therefore be put into the same class. Clark (2001) and Klein and Manning (2002) show that this approach can be successfully used for discovering syntactic constituents as well. However, as one might expect, it is easier to cluster word sequences (or word class sequences) than to tell how to put them together into trees. In particular, if one is given all contiguous subsequences (subspans) from a corpus of sentences, most natural clusters will not represent valid constituents </context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>Alexander Clark. 2000. Inducing syntactic categories by context distribution clustering. In The Fourth Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Unsupervised induction of stochastic contextfree grammars using distributional clustering.</title>
<date>2001</date>
<booktitle>In The Fifth Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="1298" citStr="Clark, 2001" startWordPosition="179" endWordPosition="180">constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is ev</context>
<context position="2652" citStr="Clark, 2001" startWordPosition="387" endWordPosition="388">uistic constituency and dependency with the goal of recovering linguistically plausible structures. We make no claims as to the cognitive plausibility of the induction mechanisms we present here; however, the ability of these systems to recover substantial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data, and hence undermines arguments based on “the poverty of the stimulus” (Chomsky, 1965). 2 Unsupervised Dependency Parsing Most recent progress in unsupervised parsing has come from tree or phrase-structure grammar based models (Clark, 2001; Klein and Manning, 2002), but there are compelling reasons to reconsider unsupervised dependency parsing. First, most state-ofthe-art supervised parsers make use of specific lexical information in addition to word-class level information – perhaps lexical information could be a useful source of information for unsupervised methods. Second, a central motivation for using tree structures in computational linguistics is to enable the extraction of dependencies – function-argument and modification structures – and it might be more advantageous to induce such structures directly. Third, as we sho</context>
<context position="24379" citStr="Clark (2001)" startWordPosition="4006" endWordPosition="4007">ional independence model: P(s, B) = P(B) F1 P(isj|bij)P(i−1si — jsj+1|bij) (i,j) data clustering methods. In the most common case, the items are words, and one uses distributions over adjacent words to induce word classes. Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes, largely corresponding to traditional parts of speech (Finch, 1993; Sch¨utze, 1995; Clark, 2000). A typical pattern would be that stocks and treasuries both frequently occur before the words fell and rose, and might therefore be put into the same class. Clark (2001) and Klein and Manning (2002) show that this approach can be successfully used for discovering syntactic constituents as well. However, as one might expect, it is easier to cluster word sequences (or word class sequences) than to tell how to put them together into trees. In particular, if one is given all contiguous subsequences (subspans) from a corpus of sentences, most natural clusters will not represent valid constituents (to the extent that constituency of a non-situated sequence is even a well-formed notion). For example, it is easy enough to discover that DET N and DET ADJ N are similar</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001. Unsupervised induction of stochastic contextfree grammars using distributional clustering. In The Fifth Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6319" citStr="Collins, 1999" startWordPosition="976" endWordPosition="977">t of sentences which contained 10 words or less after the removal of punctuation. CTB10 is the sentences of the same length from the Penn Chinese treebank (v3). NEGRA10 is the same, for the German NEGRA corpus, based on the supplied conversion of the NEGRA corpus into Penn treebank format. In most of the present experiments, the provided partsof-speech were used as the input alphabet, though we also present limited experimentation with synthetic parts-of-speech. It is important to note that the Penn treebanks do not include dependency annotations; however, the automatic dependency rules from (Collins, 1999) are sufficiently accurate to be a good benchmark for unsupervised systems for the time being (though see below for specific issues). Similar head-finding rules were used for Chinese experiments. The NEGRA corpus, however, does supply hand-annotated dependency structures. structures which specify orders of attachment among multiple dependents which share a common head. • • • • •ROOT Figure 2: Dependency graph with skeleton chosen, but words not populated. Where possible, we report an accuracy figure for both directed and undirected dependencies. Reporting undirected numbers has two advantages:</context>
<context position="14109" citStr="Collins, 1999" startWordPosition="2264" endWordPosition="2265">e head and argument words, respectively, while i, j, and k are positions between words. attach to the verb. But then, given a NOUN NOUN VERB sequence, both nouns will attach to the verb – there is no way that the model can learn that verbs have exactly one subject. We now turn to an improved dependency model that addresses this problem. 3 An Improved Dependency Model The dependency models discussed above are distinct from dependency models used inside highperformance supervised probabilistic parsers in several ways. First, in supervised models, a head outward process is modeled (Eisner, 1996; Collins, 1999). In such processes, heads generate a sequence of arguments outward to the left or right, conditioning on not only the identity of the head and direction of the attachment, but also on some notion of distance or valence. Moreover, in a head-outward model, it is natural to model stop steps, where the final argument on each side of a head is always the special symbol STOP. Models like Paskin (2002) avoid modeling STOP by generating the graph skeleton G first, uniformly at random, then populating the words of s conditioned on G. Previous work (Collins, 1999) has stressed the importance of includi</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In COLING 16,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="14093" citStr="Eisner, 1996" startWordPosition="2262" endWordPosition="2263">op. h and a are head and argument words, respectively, while i, j, and k are positions between words. attach to the verb. But then, given a NOUN NOUN VERB sequence, both nouns will attach to the verb – there is no way that the model can learn that verbs have exactly one subject. We now turn to an improved dependency model that addresses this problem. 3 An Improved Dependency Model The dependency models discussed above are distinct from dependency models used inside highperformance supervised probabilistic parsers in several ways. First, in supervised models, a head outward process is modeled (Eisner, 1996; Collins, 1999). In such processes, heads generate a sequence of arguments outward to the left or right, conditioning on not only the identity of the head and direction of the attachment, but also on some notion of distance or valence. Moreover, in a head-outward model, it is natural to model stop steps, where the final argument on each side of a head is always the special symbol STOP. Models like Paskin (2002) avoid modeling STOP by generating the graph skeleton G first, uniformly at random, then populating the words of s conditioned on G. Previous work (Collins, 1999) has stressed the impor</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In COLING 16, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Paul Finch</author>
</authors>
<title>Finding Structure in Language.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="24179" citStr="Finch, 1993" startWordPosition="3973" endWordPosition="3974">the spans (equivalent to a binary tree). In the basic model, P(B) is uniform over binary trees. Then, for each (i, j), the subspan and context pair (isj, i−1si — jsj+1) is generated via a classconditional independence model: P(s, B) = P(B) F1 P(isj|bij)P(i−1si — jsj+1|bij) (i,j) data clustering methods. In the most common case, the items are words, and one uses distributions over adjacent words to induce word classes. Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes, largely corresponding to traditional parts of speech (Finch, 1993; Sch¨utze, 1995; Clark, 2000). A typical pattern would be that stocks and treasuries both frequently occur before the words fell and rose, and might therefore be put into the same class. Clark (2001) and Klein and Manning (2002) show that this approach can be successfully used for discovering syntactic constituents as well. However, as one might expect, it is easier to cluster word sequences (or word class sequences) than to tell how to put them together into trees. In particular, if one is given all contiguous subsequences (subspans) from a corpus of sentences, most natural clusters will not</context>
</contexts>
<marker>Finch, 1993</marker>
<rawString>Steven Paul Finch. 1993. Finding Structure in Language. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In ACL 40,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="1962" citStr="Klein and Manning, 2002" startWordPosition="279" endWordPosition="282">age in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic constituency and dependency with the goal of recovering linguistically plausible structures. We make no claims as to the cognitive plausibility of the induction mechanisms we present here; however, the ability of these systems to recover substantial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data, and hence undermines arguments based on “the poverty of the stimulus” (Chomsky, 1965). 2 Unsupervised Dependency Parsing Most recent progress in uns</context>
<context position="22234" citStr="Klein and Manning (2002)" startWordPosition="3634" endWordPosition="3637">ected). This model also works on German and Chinese at above-baseline levels (55.8% and 54.2% undirected, respectively), with no modifications whatsoever. In German, the largest source of errors is also the systematic postulation of determiner-headed nounphrases. In Chinese, the primary mismatch is that subjects are considered to be the heads of sentences rather than verbs. This dependency induction model is reasonably successful. However, our intuition is still that the model can be improved by paying more attention to syntactic constituency. To this end, after briefly recapping the model of Klein and Manning (2002), we present a combined model that exploits dependencies and constituencies. As we will see, this combined model finds correct dependencies more successfully than the model above, and finds constituents more successfully than the model of Klein and Manning (2002). 4 Distributional Constituency Induction In linear distributional clustering, items (e.g., words or word sequences) are represented by characteristic distributions over their linear contexts (e.g., multinomial models over the preceding and following words, see figure 5). These context distributions are then clustered in some way, ofte</context>
<context position="24408" citStr="Klein and Manning (2002)" startWordPosition="4009" endWordPosition="4012">e model: P(s, B) = P(B) F1 P(isj|bij)P(i−1si — jsj+1|bij) (i,j) data clustering methods. In the most common case, the items are words, and one uses distributions over adjacent words to induce word classes. Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes, largely corresponding to traditional parts of speech (Finch, 1993; Sch¨utze, 1995; Clark, 2000). A typical pattern would be that stocks and treasuries both frequently occur before the words fell and rose, and might therefore be put into the same class. Clark (2001) and Klein and Manning (2002) show that this approach can be successfully used for discovering syntactic constituents as well. However, as one might expect, it is easier to cluster word sequences (or word class sequences) than to tell how to put them together into trees. In particular, if one is given all contiguous subsequences (subspans) from a corpus of sentences, most natural clusters will not represent valid constituents (to the extent that constituency of a non-situated sequence is even a well-formed notion). For example, it is easy enough to discover that DET N and DET ADJ N are similar and that V PREP DET and V PR</context>
<context position="26517" citStr="Klein and Manning (2002)" startWordPosition="4362" endWordPosition="4365">exts given only a constituency decision b.7 This is a model P(s, B) over hidden bracketings and observed sentences, and it is estimated via EM to maximize the sentence likelihoods P(s) over the training corpus. Figure 6 shows the accuracy of the CCM model not only on English but for the Chinese and German corpora discussed above.8 Results are reported at convergence; for the English case, F1 is monotonic during training, while for the others, there is an earlier peak. Also shown is an upper bound (the target trees are not all binary and so any all-binary system will overpropose constituents). Klein and Manning (2002) gives comparative numbers showing that the basic CCM outperforms other recent systems on the ATIS corpus (which many other constituency induction systems have reported on). While absolute numbers are hard to compare across corpora, all the systems compared to in Klein and Manning (2002) parsed below a right-branching baseline, while the CCM is substantially above it. 5 A Combined Model The two models described above have some common ground. Both can be seen as models over lexicalized trees composed of the configurations in figure 4. For the DMV, it is already a model over these structures. At</context>
<context position="32936" citStr="Klein and Manning (2002)" startWordPosition="5403" endWordPosition="5406">al completions. From then on, the resulting models were used together during re-estimation. Figure 6 summarizes the results. The combined model beats the CCM on English F1: 77.6 vs. 71.9. The figure also shows the combination model’s score when using word classes which were induced entirely automatically, using the simplest distributional clustering method of Sch¨utze (1995). These classes show some degradation, e.g. 72.9 F1, but it 10The product, like the CCM itself, is mass-deficient. is worth noting that these totally unsupervised numbers are better than the performance of the CCM model of Klein and Manning (2002) running off of Penn treebank word classes. Again, if we modify the gold standard so as to make determiners the head of NPs, then this model with distributional tags scores 50.6% on directed and 64.8% on undirected dependency accuracy. On the German data, the combination again outperforms each factor alone, though while the combination was most helpful at boosting constituency quality for English, for German it provided a larger boost to the dependency structures. Finally, on the Chinese data, the combination did substantially boost dependency accuracy over either single factor, but actually s</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In ACL 40, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems 15,</booktitle>
<editor>In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="29966" citStr="Klein and Manning, 2003" startWordPosition="4908" endWordPosition="4911"> entirely. In figure 6, we give the behavior of the CCM constituency model and the DMV dependency model on both constituency and dependency induction. Unsurprisingly, their strengths are complementary. The CCM is better at recovering constituency, and the dependency model is better at recovering dependency structures. It is reasonable to hope that a combination model might exhibit the best of both. In the supervised parsing domain, for example, scoring a lexicalized tree with the product of a simple lexical dependency model and a PCFG model can outperform each factor on its respective metric (Klein and Manning, 2003). 9This scoring function as described is not a generative model over lexicalized trees, because it has no generation step at which nodes’ lexical heads are chosen. This can be corrected by multiplying in a “head choice” factor of 1/(k − j) at each final “sealing” configuration (d). In practice, this correction factor was harmful for the model combination, since it duplicated a strength of the dependency model, badly. Model UP UR UF1 Dir Undir English (WSJ10 – 7422 Sentences) LBRANCH/RHEAD 25.6 32.6 28.7 33.6 56.7 RANDOM 31.0 39.4 34.7 30.1 45.6 RBRANCH/LHEAD 55.1 70.0 61.7 24.0 55.9 DMV 46.6 5</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances in Neural Information Processing Systems 15, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Aleksandrovich Melcuk</author>
</authors>
<title>Dependency Syntax: theory and practice.</title>
<date>1988</date>
<publisher>Press,</publisher>
<institution>State University of New York</institution>
<location>Albany, NY.</location>
<marker>Melcuk, 1988</marker>
<rawString>Igor Aleksandrovich Melcuk. 1988. Dependency Syntax: theory and practice. State University of New York Press, Albany, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip H Miller</author>
</authors>
<title>Strong Generative Capacity.</title>
<date>1999</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="4065" citStr="Miller, 1999" startWordPosition="615" endWordPosition="616">ntation and Evaluation An example dependency representation of a short sentence is shown in figure 1(a), where, following the traditional dependency grammar notation, the regent or head of a dependency is marked with the tail of the dependency arrow, and the dependent is marked with the arrowhead (Mel&apos;ˇcuk, 1988). It will be important in what follows to see that such a representation is isomorphic (in terms of strong generative capacity) to a restricted form of phrase structure grammar, where the set of terminals and nonterminals is identical, and every rule is of the form X → X Y or X → Y X (Miller, 1999), giving the isomorphic representation of figure 1(a) shown in figure 1(b).1 Depending on the model, part-of1Strictly, such phrase structure trees are isomorphic not to flat dependency structures, but to specific derivations of those VBD fell in fell IN NN in September Factory payrolls NNS VBD ROOT NN NNS VBD IN NN VBD NNS NN IN Factory payrolls September NN NP NNS S VBD PP VP Factory payrolls fell NN IN in September (a) Classical Dependency Structure (b) Dependency Structure as CF Tree (c) CFG Structure Figure 1: Three kinds of parse structures. speech categories may be included in the depend</context>
</contexts>
<marker>Miller, 1999</marker>
<rawString>Philip H. Miller. 1999. Strong Generative Capacity. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Paskin</author>
</authors>
<title>Grammatical bigrams. In</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7410" citStr="Paskin (2002)" startWordPosition="1147" endWordPosition="1148"> report an accuracy figure for both directed and undirected dependencies. Reporting undirected numbers has two advantages: first, it facilitates comparison with earlier work, and, more importantly, it allows one to partially obscure the effects of alternate analyses, such as the systematic choice between a modal and a main verb for the head of a sentence (in either case, the two verbs would be linked, but the direction would vary). 2.2 Dependency Models The dependency induction task has received relatively little attention; the best known work is Carroll and Charniak (1992), Yuret (1998), and Paskin (2002). All systems that we are aware of operate under the assumption that the probability of a dependency structure is the product of the scores of the dependencies (attachments) in that structure. Dependencies are seen as ordered (head, dependent) pairs of words, but the score of a dependency can optionally condition on other characteristics of the structure, most often the direction of the dependency (whether the arrow points left or right). Some notation before we present specific models: a dependency d is a pair (h, a) of a head and argument, which are words in a sentence s, in a corpus S. For </context>
<context position="9416" citStr="Paskin (2002)" startWordPosition="1491" endWordPosition="1492">0) RANDOM 21.8 41.5 ADJACENT 32.6 51.2 DMV 36.3 55.8 Chinese (CTB10) RANDOM 35.9 47.3 ADJACENT 30.2 47.3 DMV 42.5 54.2 Figure 3: Parsing performance (directed and undirected dependency accuracy) of various dependency models on various treebanks, along with baselines. dependency structure is the object generated by all of the models that follow; the steps in the derivations vary from model to model. Existing generative dependency models intended for unsupervised learning have chosen to first generate a word-free graph G, then populate the sentence s conditioned on G. For instance, the model of Paskin (2002), which is broadly similar to the semiprobabilistic model in Yuret (1998), first chooses a graph G uniformly at random (such as figure 2), then fills in the words, starting with a fixed root symbol (assumed to be at the rightmost end), and working down G until an entire dependency structure D is filled in (figure 1a). The corresponding probabilistic model is P(D) = P(s, G) = P(G)P(s|G) � = P(G) (i, j,dir)EG In Paskin (2002), the distribution P(G) is fixed to be uniform, so the only model parameters are the conditional multinomial distributions P(a|h, dir) that encode which head words take whic</context>
<context position="11115" citStr="Paskin (2002)" startWordPosition="1772" endWordPosition="1773">hoosing a random dependency structure). This below-random performance seems to be because the model links word pairs which have high mutual information (such as occurrences of congress and bill) regardless of whether they are plausibly syntactically related. In practice, high mutual information between words is often stronger between two topically similar nouns than between, say, a preposition and its object. One might hope that the problem with this model is that the actual lexical items are too semantically charged to represent workable units of syntactic structure. If one were to apply the Paskin (2002) model to dependency structures parameterized simply on the word-classes, the result would be isomorphic to the “dependency PCFG” models described in Carroll and Charniak (1992). In these models, Carroll and Charniak considered PCFGs with precisely the productions (discussed above) that make them isomorphic to dependency grammars, with the terminal alphabet being simply partsof-speech. Here, the rule probabilities are equivalent to P(Y|X, right) and P(Y|X, left) respectively.2 The actual experiments in Carroll and Charniak (1992) do not report accuracies that we can compare to, but they sugges</context>
<context position="14508" citStr="Paskin (2002)" startWordPosition="2336" endWordPosition="2337">d above are distinct from dependency models used inside highperformance supervised probabilistic parsers in several ways. First, in supervised models, a head outward process is modeled (Eisner, 1996; Collins, 1999). In such processes, heads generate a sequence of arguments outward to the left or right, conditioning on not only the identity of the head and direction of the attachment, but also on some notion of distance or valence. Moreover, in a head-outward model, it is natural to model stop steps, where the final argument on each side of a head is always the special symbol STOP. Models like Paskin (2002) avoid modeling STOP by generating the graph skeleton G first, uniformly at random, then populating the words of s conditioned on G. Previous work (Collins, 1999) has stressed the importance of including termination probabilities, which allows the graph structure to be generated jointly with the terminal words, precisely because it does allow the modeling of required dependents. We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence). We begin at the ROOT. In the standard way, each head generat</context>
</contexts>
<marker>Paskin, 2002</marker>
<rawString>Mark A. Paskin. 2002. Grammatical bigrams. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In ACL 30,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="1118" citStr="Pereira and Schabes, 1992" startWordPosition="149" endWordPosition="153">inear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In ACL 30, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In EACL 7,</booktitle>
<pages>141--148</pages>
<marker>Sch¨utze, 1995</marker>
<rawString>Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging. In EACL 7, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zach Solan</author>
<author>Eytan Ruppin</author>
<author>David Horn</author>
<author>Shimon Edelman</author>
</authors>
<title>Automatic acquisition and efficient representation of syntactic structures.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems 15,</booktitle>
<editor>In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1526" citStr="Solan et al., 2003" startWordPosition="213" endWordPosition="216">on The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic constituency and dependency with the goal of recovering linguistically plausib</context>
</contexts>
<marker>Solan, Ruppin, Horn, Edelman, 2003</marker>
<rawString>Zach Solan, Eytan Ruppin, David Horn, and Shimon Edelman. 2003. Automatic acquisition and efficient representation of syntactic structures. In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances in Neural Information Processing Systems 15, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Stephen M Omohundro</author>
</authors>
<title>Inducing probabilistic grammars by Bayesian model merging.</title>
<date>1994</date>
<booktitle>In Grammatical Inference and Applications: Proceedings of the Second International Colloquium on Grammatical Inference.</booktitle>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="1161" citStr="Stolcke and Omohundro, 1994" startWordPosition="156" endWordPosition="159">tperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structure</context>
</contexts>
<marker>Stolcke, Omohundro, 1994</marker>
<rawString>Andreas Stolcke and Stephen M. Omohundro. 1994. Inducing probabilistic grammars by Bayesian model merging. In Grammatical Inference and Applications: Proceedings of the Second International Colloquium on Grammatical Inference. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
</authors>
<title>ABL: Alignment-based learning.</title>
<date>2000</date>
<booktitle>In COLING 18,</booktitle>
<pages>961--967</pages>
<marker>van Zaanen, 2000</marker>
<rawString>Menno van Zaanen. 2000. ABL: Alignment-based learning. In COLING 18, pages 961–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Discovery of Linguistic Relations Using Lexical Attraction.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="7391" citStr="Yuret (1998)" startWordPosition="1144" endWordPosition="1145">Where possible, we report an accuracy figure for both directed and undirected dependencies. Reporting undirected numbers has two advantages: first, it facilitates comparison with earlier work, and, more importantly, it allows one to partially obscure the effects of alternate analyses, such as the systematic choice between a modal and a main verb for the head of a sentence (in either case, the two verbs would be linked, but the direction would vary). 2.2 Dependency Models The dependency induction task has received relatively little attention; the best known work is Carroll and Charniak (1992), Yuret (1998), and Paskin (2002). All systems that we are aware of operate under the assumption that the probability of a dependency structure is the product of the scores of the dependencies (attachments) in that structure. Dependencies are seen as ordered (head, dependent) pairs of words, but the score of a dependency can optionally condition on other characteristics of the structure, most often the direction of the dependency (whether the arrow points left or right). Some notation before we present specific models: a dependency d is a pair (h, a) of a head and argument, which are words in a sentence s, </context>
<context position="9489" citStr="Yuret (1998)" startWordPosition="1503" endWordPosition="1504">M 35.9 47.3 ADJACENT 30.2 47.3 DMV 42.5 54.2 Figure 3: Parsing performance (directed and undirected dependency accuracy) of various dependency models on various treebanks, along with baselines. dependency structure is the object generated by all of the models that follow; the steps in the derivations vary from model to model. Existing generative dependency models intended for unsupervised learning have chosen to first generate a word-free graph G, then populate the sentence s conditioned on G. For instance, the model of Paskin (2002), which is broadly similar to the semiprobabilistic model in Yuret (1998), first chooses a graph G uniformly at random (such as figure 2), then fills in the words, starting with a fixed root symbol (assumed to be at the rightmost end), and working down G until an entire dependency structure D is filled in (figure 1a). The corresponding probabilistic model is P(D) = P(s, G) = P(G)P(s|G) � = P(G) (i, j,dir)EG In Paskin (2002), the distribution P(G) is fixed to be uniform, so the only model parameters are the conditional multinomial distributions P(a|h, dir) that encode which head words take which other words as arguments. The parameters for left and right arguments o</context>
</contexts>
<marker>Yuret, 1998</marker>
<rawString>Deniz Yuret. 1998. Discovery of Linguistic Relations Using Lexical Attraction. Ph.D. thesis, MIT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>