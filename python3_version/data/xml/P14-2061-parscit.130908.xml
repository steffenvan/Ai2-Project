<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008368">
<title confidence="0.912878">
Unsupervised Feature Learning for Visual Sign Language Identification
Binyam Gebrekidan Gebre1, Onno Crasborn2, Peter Wittenburg1,
</title>
<author confidence="0.977914">
Sebastian Drude1, Tom Heskes2
</author>
<affiliation confidence="0.966533">
1Max Planck Institute for Psycholinguistics, 2Radboud University Nijmegen
</affiliation>
<email confidence="0.968515">
bingeb@mpi.nl,o.crasborn@let.ru.nl,peter.wittenburg@mpi.nl,
sebastian.drude@mpi.nl,t.heskes@science.ru.nl
</email>
<sectionHeader confidence="0.993747" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999298277777778">
Prior research on language identification fo-
cused primarily on text and speech. In this
paper, we focus on the visual modality and
present a method for identifying sign lan-
guages solely from short video samples. The
method is trained on unlabelled video data (un-
supervised feature learning) and using these
features, it is trained to discriminate between
six sign languages (supervised learning). We
ran experiments on short video samples in-
volving 30 signers (about 6 hours in total). Us-
ing leave-one-signer-out cross-validation, our
evaluation shows an average best accuracy of
84%. Given that sign languages are under-
resourced, unsupervised feature learning tech-
niques are the right tools and our results indi-
cate that this is realistic for sign language iden-
tification.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991715741935484">
The task of automatic language identification is
to quickly identify the identity of the language
given utterances. Performing this task is key in
applications involving multiple languages such as
machine translation and information retrieval (e.g.
metadata creation for large audiovisual archives).
Prior research on language identification is
heavily biased towards written and spoken lan-
guages (Dunning, 1994; Zissman, 1996; Li et al.,
2007; Singer et al., 2012). While language iden-
tification in signed languages is yet to be studied,
significant progress has been recorded for written
and spoken languages.
Written languages can be identified to about
99% accuracy using Markov models (Dunning,
1994). This accuracy is so high that current
research has shifted to related more challeng-
ing problems: language variety identification
(Zampieri and Gebre, 2012), native language iden-
tification (Tetreault et al., 2013) and identification
at the extremes of scales; many more languages,
smaller training data, shorter document lengths
(Baldwin and Lui, 2010).
Spoken languages can be identified to accura-
cies that range from 79-98% using different mod-
els (Zissman, 1996; Singer et al., 2003). The
methods used in spoken language identification
have also been extended to a related class of prob-
lems: native accent identification (Chen et al.,
2001; Choueiter et al., 2008; Wu et al., 2010) and
foreign accent identification (Teixeira et al., 1996).
While some work exists on sign language
recognition1 (Starner and Pentland, 1997; Starner
et al., 1998; Gavrila, 1999; Cooper et al., 2012),
very little research exists on sign language iden-
tification except for the work by (Gebre et al.,
2013), where it is shown that sign language identi-
fication can be done using linguistically motivated
features. Accuracies of 78% and 95% are reported
on signer independent and signer dependent iden-
tification of two sign languages.
This paper has two goals. First, to present a
method to identify sign languages using features
learned by unsupervised techniques (Hinton and
Salakhutdinov, 2006; Coates et al., 2011). Sec-
ond, to evaluate the method on six sign languages
under different conditions.
Our contributions: a) show that unsupervised
feature learning techniques, currently popular in
many pattern recognition problems, also work for
visual sign languages. More specifically, we show
how K-means and sparse autoencoder can be used
to learn features for sign language identification.
b) demonstrate the impact on performance of vary-
ing the number of features (aka, feature maps or
filter sizes), the patch dimensions (from 2D to 3D)
and the number of frames (video length).
1There is a difference between sign language recognition
and identification. Sign language recognition is the recogni-
tion of the meaning of the signs in a given known sign lan-
guage, whereas sign language identification is the recognition
of the sign language itself from given signs.
</bodyText>
<page confidence="0.959091">
370
</page>
<bodyText confidence="0.989670666666667">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 370–376,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
2 The challenges in sign language the differences in the phonetic realization of words
identification (their articulation) may be much larger.
The challenges in sign language identification
arise from three sources as described below.
</bodyText>
<subsectionHeader confidence="0.986434">
2.1 Iconicity in sign languages
</subsectionHeader>
<bodyText confidence="0.99995">
The relationship between forms and meanings are
not totally arbitrary (Perniss et al., 2010). Both
signed and spoken languages manifest iconicity,
that is forms of words or signs are somehow mo-
tivated by the meaning of the word or sign. While
sign languages show a lot of iconicity in the lex-
icon (Taub, 2001), this has not led to a universal
sign language. The same concept can be iconi-
cally realised by the manual articulators in a way
that conforms to the phonological regularities of
the languages, but still lead to different sign forms.
Iconicity is also used in the morphosyntax and
discourse structure of all sign languages, however,
and there we see many similarities between sign
languages. Both real-world and imaginary objects
and locations are visualised in the space in front
of the signer, and can have an impact on the artic-
ulation of signs in various ways. Also, the use of
constructed action appears to be used in many sign
languages in similar ways. The same holds for the
rich use of non-manual articulators in sentences
and the limited role of facial expressions in the
lexicon: these too make sign languages across the
world very similar in appearance, even though the
meaning of specific articulations may differ (Cras-
born, 2006).
</bodyText>
<subsectionHeader confidence="0.998922">
2.2 Differences between signers
</subsectionHeader>
<bodyText confidence="0.999991176470588">
Just as speakers have different voices unique to
each individual, signers have also different sign-
ing styles that are likely unique to each individual.
Signers’ uniqueness results from how they articu-
late the shapes and movements that are specified
by the linguistic structure of the language. The
variability between signers either in terms of phys-
ical properties (hand sizes, colors, etc) or in terms
of articulation (movements) is such that it does not
affect the understanding of the sign language by
humans, but that it may be difficult for machines
to generalize over multiple individuals. At present
we do not know whether the differences between
signers using the same language are of a similar or
different nature than the differences between dif-
ferent languages. At the level of phonology, there
are few differences between sign languages, but
</bodyText>
<subsectionHeader confidence="0.998916">
2.3 Diverse environments
</subsectionHeader>
<bodyText confidence="0.999983666666667">
The visual ’activity’ of signing comes in a context
of a specific environment. This environment can
include the visual background and camera noises.
The background objects of the video may also in-
clude dynamic objects – increasing the ambiguity
of signing activity. The properties and configu-
rations of the camera induce variations of scale,
translation, rotation, view, occlusion, etc. These
variations coupled with lighting conditions may
introduce noise. These challenges are by no means
specific to sign interaction, and are found in many
other computer vision tasks.
</bodyText>
<sectionHeader confidence="0.990449" genericHeader="introduction">
3 Method
</sectionHeader>
<bodyText confidence="0.999989571428571">
Our method performs two important tasks. First,
it learns a feature representation from patches of
unlabelled raw video data (Hinton and Salakhut-
dinov, 2006; Coates et al., 2011). Second, it looks
for activations of the learned representation (by
convolution) and uses these activations to learn a
classifier to discriminate between sign languages.
</bodyText>
<subsectionHeader confidence="0.999124">
3.1 Unsupervised feature learning
</subsectionHeader>
<bodyText confidence="0.999900666666667">
Given samples of sign language videos (unknown
sign language with one signer per video), our sys-
tem performs the following steps to learn a feature
representation (note that these video samples are
separate from the video samples that are later used
for classifier learning or testing):
</bodyText>
<listItem confidence="0.839405">
1. Extract patches. Extract small videos (here-
</listItem>
<bodyText confidence="0.797102222222222">
after called patches) randomly from any-
where in the video samples. We fix the
size of the patches such that they all have r
rows, c columns and f frames and we ex-
tract patches m times. This gives us X =
{x(1), x(1), ... , x(m)}, where x(z) E RN and
N = r*c*f (the size of a patch). For our ex-
periments, we extract 100,000 patches of size
15 * 15 * 1 (2D) and 15 * 15 * 2 (3D).
</bodyText>
<listItem confidence="0.987555666666667">
2. Normalize the patches. There is evidence
that normalization and whitening (Hyv¨arinen
and Oja, 2000) improve performance in un-
supervised feature learning (Coates et al.,
2011). We therefore normalize every patch
x(z) by subtracting the mean and dividing by
</listItem>
<page confidence="0.996502">
371
</page>
<figureCaption confidence="0.999952">
Figure 1: Illustration of feature extraction: convolution and pooling.
</figureCaption>
<bodyText confidence="0.907223428571429">
the standard deviation of its elements. For vi-
sual data, normalization corresponds to local
brightness and contrast normalization.
3. Learn a feature-mapping. Our unsuper-
vised algorithm takes in the normalized and
whitened dataset X = {x(1), x(1), ... , x(m)}
and maps each input vector x(i) to a new fea-
ture vector of K features (f : RN , RK).
We use two unsupervised learning algorithms
a) K-means b) sparse autoencoders.
(a) K-means clustering: we train K-means
to learns K c(k) centroids that mini-
mize the distance between data points
and their nearest centroids (Coates and
Ng, 2012). Given the learned centroids
c(k), we measure the distance of each
data point (patch) to the centroids. Natu-
rally, the data points are at different dis-
tances to each centroid, we keep the dis-
tances that are below the average of the
distances and we set the other to zero:
</bodyText>
<equation confidence="0.997808">
fk(x) = max{0, µ(z) − zk1 (1)
</equation>
<bodyText confidence="0.885498666666667">
where zk = IIx − c(k)112 and µ(z) is the
mean of the elements of z.
(b) Sparse autoencoder: we train a sin-
gle layer autoencoder with K hid-
den nodes using backpropagation to
minimize squared reconstruction error.
At the hidden layer, the features are
mapped using a rectified linear (ReL)
function (Maas et al., 2013) as follows:
</bodyText>
<equation confidence="0.995956">
f(x) = g(Wx + b) (2)
</equation>
<bodyText confidence="0.999923222222222">
where g(z) = max(z, 0). Note that ReL
nodes have advantages over sigmoid or
tanh functions; they create sparse repre-
sentations and are suitable for naturally
sparse data (Glorot et al., 2011).
From K-means, we get K RN centroids and from
the sparse autoencoder, we get W E RKxN and
b E RK filters. We call both the centroids and
filters as the learned features.
</bodyText>
<subsectionHeader confidence="0.996676">
3.2 Classifier learning
</subsectionHeader>
<bodyText confidence="0.999683333333333">
Given the learned features, the feature mapping
functions and a set of labeled training videos, we
extract features as follows:
</bodyText>
<listItem confidence="0.999904727272727">
1. Convolutional extraction: Extract features
from equally spaced sub-patches covering the
video sample.
2. Pooling: Pool features together over four
non-overlapping regions of the input video to
reduce the number of features. We perform
max pooling for K-means and mean pooling
for the sparse autoencoder over 2D regions
(per frame) and over 3D regions (per all se-
quence of frames).
3. Learning: Learn a linear classifier to predict
</listItem>
<bodyText confidence="0.807332">
the labels given the feature vectors. We use
logistic regression classifier and support vec-
tor machines (Pedregosa et al., 2011).
The extraction of classifier features through
convolution and pooling is illustrated in figure 1.
</bodyText>
<page confidence="0.995783">
372
</page>
<sectionHeader confidence="0.999005" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.885744">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999977666666667">
Our experimental data consist of videos of 30
signers equally divided between six sign lan-
guages: British sign language (BSL), Danish
(DSL), French Belgian (FBSL), Flemish (FSL),
Greek (GSL), and Dutch (NGT). The data for the
unsupervised feature learning comes from half of
the BSL and GSL videos in the Dicta-Sign cor-
pus2. Part of the other half, involving 5 signers, is
used along with the other sign language videos for
learning and testing classifiers.
For the unsupervised feature learning, two types
of patches are created: 2D dimensions (15 * 15)
and 3D (15 * 15 * 2). Each type consists of ran-
domly selected 100,000 patches and involves 16
different signers. For the supervised learning, 200
videos (consisting of 1 through 4 frames taken at a
step of 2) are randomly sampled per sign language
per signer (for a total of 6,000 samples).
</bodyText>
<subsectionHeader confidence="0.997123">
4.2 Data preprocessing
</subsectionHeader>
<bodyText confidence="0.999963352941177">
The data preprocessing stage has two goals.
First, to remove any non-signing signals that re-
main constant within videos of a single sign lan-
guage but that are different across sign languages.
For example, if the background of the videos is
different across sign languages, then classifying
the sign languages could be done with perfection
by using signals from the background. To avoid
this problem, we removed the background by us-
ing background subtraction techniques and manu-
ally selected thresholds.
The second reason for data preprocessing is to
make the input size smaller and uniform. The
videos are colored and their resolutions vary from
320 * 180 to 720 * 576. We converted the videos
to grayscale and resized their heights to 144 and
cropped out the central 144 * 144 patches.
</bodyText>
<subsectionHeader confidence="0.988392">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999821142857143">
We evaluate our system in terms of average accu-
racies. We train and test our system in leave-one-
signer-out cross-validation, where videos from
four signers are used for training and videos of the
remaining signer are used for testing. Classifica-
tion algorithms are used with their default settings
and the classification strategy is one-vs.-rest.
</bodyText>
<footnote confidence="0.904321">
2http://www.dictasign.eu/
</footnote>
<sectionHeader confidence="0.99544" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999898">
Our best average accuracy (84.03%) is obtained
using 500 K-means features which are extracted
over four frames (taken at a step of 2). This ac-
curacy obtained for six languages is much higher
than the 78% accuracy obtained for two sign lan-
guages (Gebre et al., 2013). The latter uses lin-
guistically motivated features that are extracted
over video lengths of at least 10 seconds. Our sys-
tem uses learned features that are extracted over
much smaller video lengths (about half a second).
All classification accuracies are presented in ta-
ble 5 for 2D and table 5 for 3D. Classification con-
fusions are shown in table 5. Figure 2 shows fea-
tures learned by K-means and sparse autoencoder.
</bodyText>
<figure confidence="0.988174">
(a) K-means features (b) SAE features
</figure>
<figureCaption confidence="0.993187333333333">
Figure 2: All 100 features learned from 100,000
patches of size 15*15. K-means learned relatively
more curving edges than the sparse auto encoder.
</figureCaption>
<table confidence="0.99610025">
K-means Sparse Autoencoder
K LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM
# of frames = 1
100 69.23 70.60 67.42 73.85 74.53 71.8
300 76.08 77.37 74.80 72.27 70.67 68.90
500 83.03 79.88 77.92 67.50 69.38 66.20
# of frames = 2
100 71.15 72.07 67.42 72.78 74.62 72.08
300 77.33 78.27 76.60 71.85 71.07 68.27
500 83.58 79.50 79.90 67.73 70.15 66.45
# of frames = 3
100 71.42 73.10 67.82 65.70 67.52 63.68
300 78.40 78.57 76.50 72.53 71.68 68.18
500 83.48 80.05 80.57 67.85 70.85 66.77
# of frames = 4
100 71.88 73.05 68.70 64.93 67.48 63.80
300 79.32 78.65 76.42 72.27 72.18 68.35
500 84.03 80.38 80.50 68.25 71.57 67.27
K = # of features, SVM = SVM with linear kernel
LR-L? = Logistic Regression with L1 and L2 penalty
</table>
<tableCaption confidence="0.98813">
Table 1: 2D filters (15 * 15): Leave-one-signer-out
cross-validation average accuracies.
</tableCaption>
<page confidence="0.994239">
373
</page>
<figure confidence="0.99872232051282">
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10
1
2
3
4
BSL 5
6
7
8
9
10
1
2
3
4
DSL 5
6
7
8
9
10
1
2
3
4
FBSL 5
6
7
8
9
10
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10
FSL
GSL
NGT
</figure>
<figureCaption confidence="0.993166333333333">
Figure 3: Visualization of coefficients of Lasso (logistic regression with L1 penalty) for each sign lan-
guage with respect to each of the 100 filters of the sparse autoencoder. The 100 filters are shown in figure
2(b). Each grid cell represents a frame and each filter is activated in 4 non-overlapping pooling regions.
</figureCaption>
<table confidence="0.999824785714286">
K-means Sparse Autoencoder
K LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM
# of frames = 2
100 70.63 69.62 68.87 67.40 66.53 65.73
300 73.73 74.05 73.03 72.83 73.48 70.52
500 75.30 76.53 75.40 72.28 74.65 68.72
# of frames = 3
100 72.48 73.30 70.33 68.68 67.40 68.33
300 74.78 74.95 74.77 74.20 74.72 70.85
500 77.27 77.50 76.17 72.40 75.45 69.42
# of frames = 4
100 74.85 73.97 69.23 68.68 67.80 68.80
300 76.23 76.58 74.08 74.43 75.20 70.65
500 79.08 78.63 76.63 73.50 76.23 70.53
</table>
<tableCaption confidence="0.9686175">
Table 2: 3D filters (15 * 15 * 2): Leave-one-signer-
out cross-validation average accuracies.
</tableCaption>
<table confidence="0.999936714285714">
BSL DSL FBSL FSL GSL NGT
BSL 56.11 2.98 1.79 3.38 24.11 11.63
DSL 2.87 92.37 0.95 0.46 3.16 0.18
FBSL 1.48 1.96 79.04 4.69 6.62 6.21
FSL 6.96 2.96 2.06 60.81 18.15 9.07
GSL 5.50 2.55 1.67 2.57 86.05 1.65
NGT 9.08 1.33 3.98 18.76 4.41 62.44
</table>
<tableCaption confidence="0.998725">
Table 3: Confusion matrix – confusions averaged
</tableCaption>
<bodyText confidence="0.986104285714286">
over all settings for K-means and sparse autoen-
coder with 2D and 3D filters (i.e. for all # of
frames, all filter sizes and all classifiers).
Tables 5 and 5 indicate that K-means performs
better with 2D filters and that sparse autoencoder
performs better with 3D filters. Note that features
from 2D filters are pooled over each frame and
concatenated whereas, features from 3D filters are
pooled over all frames.
Which filters are active for which language?
Figure 3 shows visualization of the strength of fil-
ter activation for each sign language. The figure
shows what Lasso looks for when it identifies any
of the six sign languages.
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999847761904762">
Given that sign languages are under-resourced,
unsupervised feature learning techniques are the
right tools and our results show that this is realis-
tic for sign language identification.
Future work can extend this work in two direc-
tions: 1) by increasing the number of sign lan-
guages and signers to check the stability of the
learned feature activations and to relate these to
iconicity and signer differences 2) by comparing
our method with deep learning techniques. In our
experiments, we used a single hidden layer of fea-
tures, but it is worth researching into deeper layers
to improve performance and gain more insight into
the hierarchical composition of features.
Other questions for future work. How good are
human beings at identifying sign languages? Can
a machine be used to evaluate the quality of sign
language interpreters by comparing them to a na-
tive language model? The latter question is partic-
ularly important given what happened at the Nel-
son Mandela’s memorial service3.
</bodyText>
<footnote confidence="0.947512">
3http://www.youtube.com/watch?v=X-DxGoIVUWo
</footnote>
<page confidence="0.997314">
374
</page>
<sectionHeader confidence="0.988944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999560869158878">
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229–237. Association for Computational Lin-
guistics.
Tao Chen, Chao Huang, E. Chang, and Jingchun Wang.
2001. Automatic accent identification using gaus-
sian mixture models. In Automatic Speech Recog-
nition and Understanding, 2001. ASRU ’01. IEEE
Workshop on, pages 343–346.
Ghinwa Choueiter, Geoffrey Zweig, and Patrick
Nguyen. 2008. An empirical study of automatic ac-
cent classification. In Acoustics, Speech and Signal
Processing, 2008. ICASSP 2008. IEEE International
Conference on, pages 4265–4268. IEEE.
Adam Coates and Andrew Y Ng. 2012. Learn-
ing feature representations with k-means. In Neu-
ral Networks: Tricks of the Trade, pages 561–580.
Springer.
Adam Coates, Andrew Y Ng, and Honglak Lee. 2011.
An analysis of single-layer networks in unsuper-
vised feature learning. In International Conference
on Artificial Intelligence and Statistics, pages 215–
223.
H. Cooper, E.J. Ong, N. Pugeault, and R. Bowden.
2012. Sign language recognition using sub-units.
Journal of Machine Learning Research, 13:2205–
2231.
Onno Crasborn, 2006. Nonmanual structures in sign
languages, volume 8, pages 668–672. Elsevier, Ox-
ford.
T. Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Dariu M Gavrila. 1999. The visual analysis of human
movement: A survey. Computer vision and image
understanding, 73(1):82–98.
Binyam Gebrekidan Gebre, Peter Wittenburg, and Tom
Heskes. 2013. Automatic sign language identifica-
tion. In Proceedings of ICIP 2013.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&amp;CP Vol-
ume, volume 15, pages 315–323.
Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504–507.
Aapo Hyv¨arinen and Erkki Oja. 2000. Independent
component analysis: algorithms and applications.
Neural networks, 13(4):411–430.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007. A
vector space modeling approach to spoken language
identification. Audio, Speech, and Language Pro-
cessing, IEEE Transactions on, 15(1):271–284.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectifier nonlinearities improve neural net-
work acoustic models. In Proceedings of the ICML.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. The Journal of Ma-
chine Learning Research, 12:2825–2830.
Pamela Perniss, Robin L Thompson, and Gabriella
Vigliocco. 2010. Iconicity as a general property
of language: evidence from spoken and signed lan-
guages. Frontiers in psychology, 1.
E. Singer, PA Torres-Carrasquillo, TP Gleason,
WM Campbell, and D.A. Reynolds. 2003. Acous-
tic, phonetic, and discriminative approaches to auto-
matic language identification. In Proc. Eurospeech,
volume 9.
E. Singer, P. Torres-Carrasquillo, D. Reynolds, A. Mc-
Cree, F. Richardson, N. Dehak, and D. Sturim.
2012. The mitll nist lre 2011 language recogni-
tion system. In Odyssey 2012-The Speaker and Lan-
guage Recognition Workshop.
Thad Starner and Alex Pentland. 1997. Real-time
american sign language recognition from video us-
ing hidden markov models. In Motion-Based Recog-
nition, pages 227–243. Springer.
Thad Starner, Joshua Weaver, and Alex Pentland.
1998. Real-time american sign language recogni-
tion using desk and wearable computer based video.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1371–1375.
Sarah Taub. 2001. Language from the body: iconicity
and metaphor in American Sign Language. Cam-
bridge University Press, Cambridge.
C. Teixeira, I. Trancoso, and A. Serralheiro. 1996. Ac-
cent identification. In Spoken Language, 1996. IC-
SLP 96. Proceedings., Fourth International Confer-
ence on, volume 3, pages 1784–1787 vol.3.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. NAACL/HLT 2013, page 48.
Tingyao Wu, Jacques Duchateau, Jean-Pierre Martens,
and Dirk Van Compernolle. 2010. Feature subset
selection for improved native accent identification.
Speech Communication, 52(2):83–98.
Marcos Zampieri and Binyam Gebrekidan Gebre.
2012. Automatic identification of language vari-
eties: The case of portuguese. In Proceedings of
KONVENS, pages 233–237.
</reference>
<page confidence="0.987897">
375
</page>
<reference confidence="0.93885525">
M.A. Zissman. 1996. Comparison of four approaches
to automatic language identification of telephone
speech. IEEE Transactions on Speech and Audio
Processing, 4(1):31–44.
</reference>
<page confidence="0.99905">
376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.754684">
<title confidence="0.999641">Unsupervised Feature Learning for Visual Sign Language Identification</title>
<author confidence="0.940978">Gebrekidan Onno Peter</author>
<affiliation confidence="0.998174">Planck Institute for Psycholinguistics, University</affiliation>
<email confidence="0.978919">sebastian.drude@mpi.nl,t.heskes@science.ru.nl</email>
<abstract confidence="0.990466631578947">Prior research on language identification focused primarily on text and speech. In this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples. The method is trained on unlabelled video data (unsupervised feature learning) and using these features, it is trained to discriminate between six sign languages (supervised learning). We ran experiments on short video samples involving 30 signers (about 6 hours in total). Using leave-one-signer-out cross-validation, our evaluation shows an average best accuracy of Given that sign languages are underresourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language identification: The long and the short of the matter.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>229--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2197" citStr="Baldwin and Lui, 2010" startWordPosition="303" endWordPosition="306">., 2007; Singer et al., 2012). While language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages. Written languages can be identified to about 99% accuracy using Markov models (Dunning, 1994). This accuracy is so high that current research has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except f</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 229–237. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Chen</author>
<author>Chao Huang</author>
<author>E Chang</author>
<author>Jingchun Wang</author>
</authors>
<title>Automatic accent identification using gaussian mixture models.</title>
<date>2001</date>
<booktitle>In Automatic Speech Recognition and Understanding,</booktitle>
<pages>343--346</pages>
<contexts>
<context position="2486" citStr="Chen et al., 2001" startWordPosition="350" endWordPosition="353">h that current research has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. F</context>
</contexts>
<marker>Chen, Huang, Chang, Wang, 2001</marker>
<rawString>Tao Chen, Chao Huang, E. Chang, and Jingchun Wang. 2001. Automatic accent identification using gaussian mixture models. In Automatic Speech Recognition and Understanding, 2001. ASRU ’01. IEEE Workshop on, pages 343–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ghinwa Choueiter</author>
<author>Geoffrey Zweig</author>
<author>Patrick Nguyen</author>
</authors>
<title>An empirical study of automatic accent classification.</title>
<date>2008</date>
<booktitle>In Acoustics, Speech and Signal Processing,</booktitle>
<pages>4265--4268</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="2510" citStr="Choueiter et al., 2008" startWordPosition="354" endWordPosition="357">arch has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. First, to present a metho</context>
</contexts>
<marker>Choueiter, Zweig, Nguyen, 2008</marker>
<rawString>Ghinwa Choueiter, Geoffrey Zweig, and Patrick Nguyen. 2008. An empirical study of automatic accent classification. In Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on, pages 4265–4268. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Coates</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning feature representations with k-means.</title>
<date>2012</date>
<booktitle>In Neural Networks: Tricks of the Trade,</booktitle>
<pages>561--580</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9336" citStr="Coates and Ng, 2012" startWordPosition="1452" endWordPosition="1455">ction: convolution and pooling. the standard deviation of its elements. For visual data, normalization corresponds to local brightness and contrast normalization. 3. Learn a feature-mapping. Our unsupervised algorithm takes in the normalized and whitened dataset X = {x(1), x(1), ... , x(m)} and maps each input vector x(i) to a new feature vector of K features (f : RN , RK). We use two unsupervised learning algorithms a) K-means b) sparse autoencoders. (a) K-means clustering: we train K-means to learns K c(k) centroids that minimize the distance between data points and their nearest centroids (Coates and Ng, 2012). Given the learned centroids c(k), we measure the distance of each data point (patch) to the centroids. Naturally, the data points are at different distances to each centroid, we keep the distances that are below the average of the distances and we set the other to zero: fk(x) = max{0, µ(z) − zk1 (1) where zk = IIx − c(k)112 and µ(z) is the mean of the elements of z. (b) Sparse autoencoder: we train a single layer autoencoder with K hidden nodes using backpropagation to minimize squared reconstruction error. At the hidden layer, the features are mapped using a rectified linear (ReL) function </context>
</contexts>
<marker>Coates, Ng, 2012</marker>
<rawString>Adam Coates and Andrew Y Ng. 2012. Learning feature representations with k-means. In Neural Networks: Tricks of the Trade, pages 561–580. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Coates</author>
<author>Andrew Y Ng</author>
<author>Honglak Lee</author>
</authors>
<title>An analysis of single-layer networks in unsupervised feature learning.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>215--223</pages>
<contexts>
<context position="3242" citStr="Coates et al., 2011" startWordPosition="469" endWordPosition="472">guage recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. First, to present a method to identify sign languages using features learned by unsupervised techniques (Hinton and Salakhutdinov, 2006; Coates et al., 2011). Second, to evaluate the method on six sign languages under different conditions. Our contributions: a) show that unsupervised feature learning techniques, currently popular in many pattern recognition problems, also work for visual sign languages. More specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification. b) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length). 1There is a difference between sig</context>
<context position="7498" citStr="Coates et al., 2011" startWordPosition="1141" endWordPosition="1144">a noises. The background objects of the video may also include dynamic objects – increasing the ambiguity of signing activity. The properties and configurations of the camera induce variations of scale, translation, rotation, view, occlusion, etc. These variations coupled with lighting conditions may introduce noise. These challenges are by no means specific to sign interaction, and are found in many other computer vision tasks. 3 Method Our method performs two important tasks. First, it learns a feature representation from patches of unlabelled raw video data (Hinton and Salakhutdinov, 2006; Coates et al., 2011). Second, it looks for activations of the learned representation (by convolution) and uses these activations to learn a classifier to discriminate between sign languages. 3.1 Unsupervised feature learning Given samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing): 1. Extract patches. Extract small videos (hereafter called patches) randomly from anywhere in the video samp</context>
</contexts>
<marker>Coates, Ng, Lee, 2011</marker>
<rawString>Adam Coates, Andrew Y Ng, and Honglak Lee. 2011. An analysis of single-layer networks in unsupervised feature learning. In International Conference on Artificial Intelligence and Statistics, pages 215– 223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cooper</author>
<author>E J Ong</author>
<author>N Pugeault</author>
<author>R Bowden</author>
</authors>
<title>Sign language recognition using sub-units.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>13</volume>
<pages>2231</pages>
<contexts>
<context position="2727" citStr="Cooper et al., 2012" startWordPosition="388" endWordPosition="391">ny more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. First, to present a method to identify sign languages using features learned by unsupervised techniques (Hinton and Salakhutdinov, 2006; Coates et al., 2011). Second, to evaluate the method on six sign languages under different conditions. Ou</context>
</contexts>
<marker>Cooper, Ong, Pugeault, Bowden, 2012</marker>
<rawString>H. Cooper, E.J. Ong, N. Pugeault, and R. Bowden. 2012. Sign language recognition using sub-units. Journal of Machine Learning Research, 13:2205– 2231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Onno Crasborn</author>
</authors>
<title>Nonmanual structures in sign languages,</title>
<date>2006</date>
<volume>8</volume>
<pages>668--672</pages>
<publisher>Elsevier,</publisher>
<location>Oxford.</location>
<contexts>
<context position="5826" citStr="Crasborn, 2006" startWordPosition="881" endWordPosition="883">ver, and there we see many similarities between sign languages. Both real-world and imaginary objects and locations are visualised in the space in front of the signer, and can have an impact on the articulation of signs in various ways. Also, the use of constructed action appears to be used in many sign languages in similar ways. The same holds for the rich use of non-manual articulators in sentences and the limited role of facial expressions in the lexicon: these too make sign languages across the world very similar in appearance, even though the meaning of specific articulations may differ (Crasborn, 2006). 2.2 Differences between signers Just as speakers have different voices unique to each individual, signers have also different signing styles that are likely unique to each individual. Signers’ uniqueness results from how they articulate the shapes and movements that are specified by the linguistic structure of the language. The variability between signers either in terms of physical properties (hand sizes, colors, etc) or in terms of articulation (movements) is such that it does not affect the understanding of the sign language by humans, but that it may be difficult for machines to generali</context>
</contexts>
<marker>Crasborn, 2006</marker>
<rawString>Onno Crasborn, 2006. Nonmanual structures in sign languages, volume 8, pages 668–672. Elsevier, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Statistical identification of language.</title>
<date>1994</date>
<institution>Computing Research Laboratory, New Mexico State University.</institution>
<contexts>
<context position="1550" citStr="Dunning, 1994" startWordPosition="208" endWordPosition="209">ven that sign languages are underresourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification. 1 Introduction The task of automatic language identification is to quickly identify the identity of the language given utterances. Performing this task is key in applications involving multiple languages such as machine translation and information retrieval (e.g. metadata creation for large audiovisual archives). Prior research on language identification is heavily biased towards written and spoken languages (Dunning, 1994; Zissman, 1996; Li et al., 2007; Singer et al., 2012). While language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages. Written languages can be identified to about 99% accuracy using Markov models (Dunning, 1994). This accuracy is so high that current research has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, s</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>T. Dunning. 1994. Statistical identification of language. Computing Research Laboratory, New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dariu M Gavrila</author>
</authors>
<title>The visual analysis of human movement: A survey. Computer vision and image understanding,</title>
<date>1999</date>
<pages>73--1</pages>
<contexts>
<context position="2705" citStr="Gavrila, 1999" startWordPosition="386" endWordPosition="387">s of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. First, to present a method to identify sign languages using features learned by unsupervised techniques (Hinton and Salakhutdinov, 2006; Coates et al., 2011). Second, to evaluate the method on six sign languages under di</context>
</contexts>
<marker>Gavrila, 1999</marker>
<rawString>Dariu M Gavrila. 1999. The visual analysis of human movement: A survey. Computer vision and image understanding, 73(1):82–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binyam Gebrekidan Gebre</author>
<author>Peter Wittenburg</author>
<author>Tom Heskes</author>
</authors>
<title>Automatic sign language identification.</title>
<date>2013</date>
<booktitle>In Proceedings of ICIP</booktitle>
<contexts>
<context position="2832" citStr="Gebre et al., 2013" startWordPosition="406" endWordPosition="409">es can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. First, to present a method to identify sign languages using features learned by unsupervised techniques (Hinton and Salakhutdinov, 2006; Coates et al., 2011). Second, to evaluate the method on six sign languages under different conditions. Our contributions: a) show that unsupervised feature learning techniques, currently popular in many pattern</context>
<context position="13530" citStr="Gebre et al., 2013" startWordPosition="2155" endWordPosition="2158">ccuracies. We train and test our system in leave-onesigner-out cross-validation, where videos from four signers are used for training and videos of the remaining signer are used for testing. Classification algorithms are used with their default settings and the classification strategy is one-vs.-rest. 2http://www.dictasign.eu/ 5 Results and Discussion Our best average accuracy (84.03%) is obtained using 500 K-means features which are extracted over four frames (taken at a step of 2). This accuracy obtained for six languages is much higher than the 78% accuracy obtained for two sign languages (Gebre et al., 2013). The latter uses linguistically motivated features that are extracted over video lengths of at least 10 seconds. Our system uses learned features that are extracted over much smaller video lengths (about half a second). All classification accuracies are presented in table 5 for 2D and table 5 for 3D. Classification confusions are shown in table 5. Figure 2 shows features learned by K-means and sparse autoencoder. (a) K-means features (b) SAE features Figure 2: All 100 features learned from 100,000 patches of size 15*15. K-means learned relatively more curving edges than the sparse auto encode</context>
</contexts>
<marker>Gebre, Wittenburg, Heskes, 2013</marker>
<rawString>Binyam Gebrekidan Gebre, Peter Wittenburg, and Tom Heskes. 2013. Automatic sign language identification. In Proceedings of ICIP 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Deep sparse rectifier networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP Volume,</booktitle>
<volume>15</volume>
<pages>315--323</pages>
<contexts>
<context position="10180" citStr="Glorot et al., 2011" startWordPosition="1605" endWordPosition="1608">e of the distances and we set the other to zero: fk(x) = max{0, µ(z) − zk1 (1) where zk = IIx − c(k)112 and µ(z) is the mean of the elements of z. (b) Sparse autoencoder: we train a single layer autoencoder with K hidden nodes using backpropagation to minimize squared reconstruction error. At the hidden layer, the features are mapped using a rectified linear (ReL) function (Maas et al., 2013) as follows: f(x) = g(Wx + b) (2) where g(z) = max(z, 0). Note that ReL nodes have advantages over sigmoid or tanh functions; they create sparse representations and are suitable for naturally sparse data (Glorot et al., 2011). From K-means, we get K RN centroids and from the sparse autoencoder, we get W E RKxN and b E RK filters. We call both the centroids and filters as the learned features. 3.2 Classifier learning Given the learned features, the feature mapping functions and a set of labeled training videos, we extract features as follows: 1. Convolutional extraction: Extract features from equally spaced sub-patches covering the video sample. 2. Pooling: Pool features together over four non-overlapping regions of the input video to reduce the number of features. We perform max pooling for K-means and mean poolin</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectifier networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP Volume, volume 15, pages 315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Reducing the dimensionality of data with neural networks.</title>
<date>2006</date>
<journal>Science,</journal>
<volume>313</volume>
<issue>5786</issue>
<contexts>
<context position="3220" citStr="Hinton and Salakhutdinov, 2006" startWordPosition="465" endWordPosition="468">ile some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. First, to present a method to identify sign languages using features learned by unsupervised techniques (Hinton and Salakhutdinov, 2006; Coates et al., 2011). Second, to evaluate the method on six sign languages under different conditions. Our contributions: a) show that unsupervised feature learning techniques, currently popular in many pattern recognition problems, also work for visual sign languages. More specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification. b) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length). 1There is a </context>
<context position="7476" citStr="Hinton and Salakhutdinov, 2006" startWordPosition="1136" endWordPosition="1140"> the visual background and camera noises. The background objects of the video may also include dynamic objects – increasing the ambiguity of signing activity. The properties and configurations of the camera induce variations of scale, translation, rotation, view, occlusion, etc. These variations coupled with lighting conditions may introduce noise. These challenges are by no means specific to sign interaction, and are found in many other computer vision tasks. 3 Method Our method performs two important tasks. First, it learns a feature representation from patches of unlabelled raw video data (Hinton and Salakhutdinov, 2006; Coates et al., 2011). Second, it looks for activations of the learned representation (by convolution) and uses these activations to learn a classifier to discriminate between sign languages. 3.1 Unsupervised feature learning Given samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing): 1. Extract patches. Extract small videos (hereafter called patches) randomly from anyw</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aapo Hyv¨arinen</author>
<author>Erkki Oja</author>
</authors>
<title>Independent component analysis: algorithms and applications. Neural networks,</title>
<date>2000</date>
<pages>13--4</pages>
<marker>Hyv¨arinen, Oja, 2000</marker>
<rawString>Aapo Hyv¨arinen and Erkki Oja. 2000. Independent component analysis: algorithms and applications. Neural networks, 13(4):411–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Bin Ma</author>
<author>Chin-Hui Lee</author>
</authors>
<title>A vector space modeling approach to spoken language identification. Audio, Speech, and Language Processing,</title>
<date>2007</date>
<journal>IEEE Transactions on,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="1582" citStr="Li et al., 2007" startWordPosition="212" endWordPosition="215">derresourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification. 1 Introduction The task of automatic language identification is to quickly identify the identity of the language given utterances. Performing this task is key in applications involving multiple languages such as machine translation and information retrieval (e.g. metadata creation for large audiovisual archives). Prior research on language identification is heavily biased towards written and spoken languages (Dunning, 1994; Zissman, 1996; Li et al., 2007; Singer et al., 2012). While language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages. Written languages can be identified to about 99% accuracy using Markov models (Dunning, 1994). This accuracy is so high that current research has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin</context>
</contexts>
<marker>Li, Ma, Lee, 2007</marker>
<rawString>Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007. A vector space modeling approach to spoken language identification. Audio, Speech, and Language Processing, IEEE Transactions on, 15(1):271–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Awni Y Hannun</author>
<author>Andrew Y Ng</author>
</authors>
<title>Rectifier nonlinearities improve neural network acoustic models.</title>
<date>2013</date>
<booktitle>In Proceedings of the ICML.</booktitle>
<contexts>
<context position="9955" citStr="Maas et al., 2013" startWordPosition="1565" endWordPosition="1568"> Given the learned centroids c(k), we measure the distance of each data point (patch) to the centroids. Naturally, the data points are at different distances to each centroid, we keep the distances that are below the average of the distances and we set the other to zero: fk(x) = max{0, µ(z) − zk1 (1) where zk = IIx − c(k)112 and µ(z) is the mean of the elements of z. (b) Sparse autoencoder: we train a single layer autoencoder with K hidden nodes using backpropagation to minimize squared reconstruction error. At the hidden layer, the features are mapped using a rectified linear (ReL) function (Maas et al., 2013) as follows: f(x) = g(Wx + b) (2) where g(z) = max(z, 0). Note that ReL nodes have advantages over sigmoid or tanh functions; they create sparse representations and are suitable for naturally sparse data (Glorot et al., 2011). From K-means, we get K RN centroids and from the sparse autoencoder, we get W E RKxN and b E RK filters. We call both the centroids and filters as the learned features. 3.2 Classifier learning Given the learned features, the feature mapping functions and a set of labeled training videos, we extract features as follows: 1. Convolutional extraction: Extract features from e</context>
</contexts>
<marker>Maas, Hannun, Ng, 2013</marker>
<rawString>Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In Proceedings of the ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand</author>
</authors>
<title>Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, et</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Bertrand, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python. The Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Perniss</author>
<author>Robin L Thompson</author>
<author>Gabriella Vigliocco</author>
</authors>
<title>Iconicity as a general property of language: evidence from spoken and signed languages. Frontiers</title>
<date>2010</date>
<note>in psychology, 1.</note>
<contexts>
<context position="4666" citStr="Perniss et al., 2010" startWordPosition="682" endWordPosition="685"> of the sign language itself from given signs. 370 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 370–376, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2 The challenges in sign language the differences in the phonetic realization of words identification (their articulation) may be much larger. The challenges in sign language identification arise from three sources as described below. 2.1 Iconicity in sign languages The relationship between forms and meanings are not totally arbitrary (Perniss et al., 2010). Both signed and spoken languages manifest iconicity, that is forms of words or signs are somehow motivated by the meaning of the word or sign. While sign languages show a lot of iconicity in the lexicon (Taub, 2001), this has not led to a universal sign language. The same concept can be iconically realised by the manual articulators in a way that conforms to the phonological regularities of the languages, but still lead to different sign forms. Iconicity is also used in the morphosyntax and discourse structure of all sign languages, however, and there we see many similarities between sign la</context>
</contexts>
<marker>Perniss, Thompson, Vigliocco, 2010</marker>
<rawString>Pamela Perniss, Robin L Thompson, and Gabriella Vigliocco. 2010. Iconicity as a general property of language: evidence from spoken and signed languages. Frontiers in psychology, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Singer</author>
<author>PA Torres-Carrasquillo</author>
<author>TP Gleason</author>
<author>WM Campbell</author>
<author>D A Reynolds</author>
</authors>
<title>Acoustic, phonetic, and discriminative approaches to automatic language identification.</title>
<date>2003</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<volume>9</volume>
<contexts>
<context position="2330" citStr="Singer et al., 2003" startWordPosition="325" endWordPosition="328">corded for written and spoken languages. Written languages can be identified to about 99% accuracy using Markov models (Dunning, 1994). This accuracy is so high that current research has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated f</context>
</contexts>
<marker>Singer, Torres-Carrasquillo, Gleason, Campbell, Reynolds, 2003</marker>
<rawString>E. Singer, PA Torres-Carrasquillo, TP Gleason, WM Campbell, and D.A. Reynolds. 2003. Acoustic, phonetic, and discriminative approaches to automatic language identification. In Proc. Eurospeech, volume 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Singer</author>
<author>P Torres-Carrasquillo</author>
<author>D Reynolds</author>
<author>A McCree</author>
<author>F Richardson</author>
<author>N Dehak</author>
<author>D Sturim</author>
</authors>
<title>The mitll nist lre 2011 language recognition system.</title>
<date>2012</date>
<booktitle>In Odyssey 2012-The Speaker and Language Recognition Workshop.</booktitle>
<contexts>
<context position="1604" citStr="Singer et al., 2012" startWordPosition="216" endWordPosition="219">upervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification. 1 Introduction The task of automatic language identification is to quickly identify the identity of the language given utterances. Performing this task is key in applications involving multiple languages such as machine translation and information retrieval (e.g. metadata creation for large audiovisual archives). Prior research on language identification is heavily biased towards written and spoken languages (Dunning, 1994; Zissman, 1996; Li et al., 2007; Singer et al., 2012). While language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages. Written languages can be identified to about 99% accuracy using Markov models (Dunning, 1994). This accuracy is so high that current research has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoke</context>
</contexts>
<marker>Singer, Torres-Carrasquillo, Reynolds, McCree, Richardson, Dehak, Sturim, 2012</marker>
<rawString>E. Singer, P. Torres-Carrasquillo, D. Reynolds, A. McCree, F. Richardson, N. Dehak, and D. Sturim. 2012. The mitll nist lre 2011 language recognition system. In Odyssey 2012-The Speaker and Language Recognition Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Starner</author>
<author>Alex Pentland</author>
</authors>
<title>Real-time american sign language recognition from video using hidden markov models. In Motion-Based Recognition,</title>
<date>1997</date>
<pages>227--243</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2668" citStr="Starner and Pentland, 1997" startWordPosition="378" endWordPosition="381">lt et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. First, to present a method to identify sign languages using features learned by unsupervised techniques (Hinton and Salakhutdinov, 2006; Coates et al., 2011). Second, to evaluate the </context>
</contexts>
<marker>Starner, Pentland, 1997</marker>
<rawString>Thad Starner and Alex Pentland. 1997. Real-time american sign language recognition from video using hidden markov models. In Motion-Based Recognition, pages 227–243. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Starner</author>
<author>Joshua Weaver</author>
<author>Alex Pentland</author>
</authors>
<title>Real-time american sign language recognition using desk and wearable computer based video.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>12</issue>
<contexts>
<context position="2690" citStr="Starner et al., 1998" startWordPosition="382" endWordPosition="385">ication at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. First, to present a method to identify sign languages using features learned by unsupervised techniques (Hinton and Salakhutdinov, 2006; Coates et al., 2011). Second, to evaluate the method on six sign lan</context>
</contexts>
<marker>Starner, Weaver, Pentland, 1998</marker>
<rawString>Thad Starner, Joshua Weaver, and Alex Pentland. 1998. Real-time american sign language recognition using desk and wearable computer based video. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1371–1375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Taub</author>
</authors>
<title>Language from the body: iconicity and metaphor in American Sign Language.</title>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4883" citStr="Taub, 2001" startWordPosition="724" endWordPosition="725">tion for Computational Linguistics 2 The challenges in sign language the differences in the phonetic realization of words identification (their articulation) may be much larger. The challenges in sign language identification arise from three sources as described below. 2.1 Iconicity in sign languages The relationship between forms and meanings are not totally arbitrary (Perniss et al., 2010). Both signed and spoken languages manifest iconicity, that is forms of words or signs are somehow motivated by the meaning of the word or sign. While sign languages show a lot of iconicity in the lexicon (Taub, 2001), this has not led to a universal sign language. The same concept can be iconically realised by the manual articulators in a way that conforms to the phonological regularities of the languages, but still lead to different sign forms. Iconicity is also used in the morphosyntax and discourse structure of all sign languages, however, and there we see many similarities between sign languages. Both real-world and imaginary objects and locations are visualised in the space in front of the signer, and can have an impact on the articulation of signs in various ways. Also, the use of constructed action</context>
</contexts>
<marker>Taub, 2001</marker>
<rawString>Sarah Taub. 2001. Language from the body: iconicity and metaphor in American Sign Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Teixeira</author>
<author>I Trancoso</author>
<author>A Serralheiro</author>
</authors>
<title>Accent identification.</title>
<date>1996</date>
<booktitle>In Spoken Language,</booktitle>
<volume>3</volume>
<pages>1784--1787</pages>
<contexts>
<context position="2586" citStr="Teixeira et al., 1996" startWordPosition="366" endWordPosition="369">tification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pentland, 1997; Starner et al., 1998; Gavrila, 1999; Cooper et al., 2012), very little research exists on sign language identification except for the work by (Gebre et al., 2013), where it is shown that sign language identification can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages. This paper has two goals. First, to present a method to identify sign languages using features learned by unsupervised techniqu</context>
</contexts>
<marker>Teixeira, Trancoso, Serralheiro, 1996</marker>
<rawString>C. Teixeira, I. Trancoso, and A. Serralheiro. 1996. Accent identification. In Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, volume 3, pages 1784–1787 vol.3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
</authors>
<title>A report on the first native language identification shared task.</title>
<date>2013</date>
<booktitle>NAACL/HLT 2013,</booktitle>
<pages>48</pages>
<contexts>
<context position="2058" citStr="Tetreault et al., 2013" startWordPosition="283" endWordPosition="286">s). Prior research on language identification is heavily biased towards written and spoken languages (Dunning, 1994; Zissman, 1996; Li et al., 2007; Singer et al., 2012). While language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages. Written languages can be identified to about 99% accuracy using Markov models (Dunning, 1994). This accuracy is so high that current research has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some work exists on sign language recognition1 (Starner and Pent</context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, 2013</marker>
<rawString>Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared task. NAACL/HLT 2013, page 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tingyao Wu</author>
<author>Jacques Duchateau</author>
<author>Jean-Pierre Martens</author>
<author>Dirk Van Compernolle</author>
</authors>
<title>Feature subset selection for improved native accent identification.</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<volume>52</volume>
<issue>2</issue>
<marker>Wu, Duchateau, Martens, Van Compernolle, 2010</marker>
<rawString>Tingyao Wu, Jacques Duchateau, Jean-Pierre Martens, and Dirk Van Compernolle. 2010. Feature subset selection for improved native accent identification. Speech Communication, 52(2):83–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
</authors>
<title>Automatic identification of language varieties: The case of portuguese.</title>
<date>2012</date>
<booktitle>In Proceedings of KONVENS,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="2001" citStr="Zampieri and Gebre, 2012" startWordPosition="275" endWordPosition="278">ieval (e.g. metadata creation for large audiovisual archives). Prior research on language identification is heavily biased towards written and spoken languages (Dunning, 1994; Zissman, 1996; Li et al., 2007; Singer et al., 2012). While language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages. Written languages can be identified to about 99% accuracy using Markov models (Dunning, 1994). This accuracy is so high that current research has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (Baldwin and Lui, 2010). Spoken languages can be identified to accuracies that range from 79-98% using different models (Zissman, 1996; Singer et al., 2003). The methods used in spoken language identification have also been extended to a related class of problems: native accent identification (Chen et al., 2001; Choueiter et al., 2008; Wu et al., 2010) and foreign accent identification (Teixeira et al., 1996). While some wo</context>
</contexts>
<marker>Zampieri, Gebre, 2012</marker>
<rawString>Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case of portuguese. In Proceedings of KONVENS, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Zissman</author>
</authors>
<title>Comparison of four approaches to automatic language identification of telephone speech.</title>
<date>1996</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1565" citStr="Zissman, 1996" startWordPosition="210" endWordPosition="211">anguages are underresourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification. 1 Introduction The task of automatic language identification is to quickly identify the identity of the language given utterances. Performing this task is key in applications involving multiple languages such as machine translation and information retrieval (e.g. metadata creation for large audiovisual archives). Prior research on language identification is heavily biased towards written and spoken languages (Dunning, 1994; Zissman, 1996; Li et al., 2007; Singer et al., 2012). While language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages. Written languages can be identified to about 99% accuracy using Markov models (Dunning, 1994). This accuracy is so high that current research has shifted to related more challenging problems: language variety identification (Zampieri and Gebre, 2012), native language identification (Tetreault et al., 2013) and identification at the extremes of scales; many more languages, smaller training data, shorter document</context>
</contexts>
<marker>Zissman, 1996</marker>
<rawString>M.A. Zissman. 1996. Comparison of four approaches to automatic language identification of telephone speech. IEEE Transactions on Speech and Audio Processing, 4(1):31–44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>