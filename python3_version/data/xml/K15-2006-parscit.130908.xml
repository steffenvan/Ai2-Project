<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000657">
<title confidence="0.99822">
A Minimalist Approach to Shallow Discourse Parsing and Implicit
Relation Recognition
</title>
<author confidence="0.824057">
Christian Chiarcos and Niko Schenk
</author>
<affiliation confidence="0.847406">
Applied Computational Linguistics Lab
Goethe University Frankfurt am Main
</affiliation>
<email confidence="0.997681">
{chiarcos,n.schenk}@em.uni-frankfurt.de
</email>
<sectionHeader confidence="0.997371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982">
We describe a minimalist approach to
shallow discourse parsing in the con-
text of the CoNLL 2015 Shared Task.1
Our parser integrates a rule-based compo-
nent for argument identification and data-
driven models for the classification of ex-
plicit and implicit relations. We place spe-
cial emphasis on the evaluation of implicit
sense labeling, we present different feature
sets and show that (i) word embeddings
are competitive with traditional word-level
features, and (ii) that they can be used to
considerably reduce the total number of
features. Despite its simplicity, our parser
is competitive with other systems in terms
of sense recognition and thus provides a
solid ground for further refinement.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933">
Comprehending sentences and other textual units
requires capabilities beyond capturing the lexical
semantics of their components. Contextual in-
formation is needed, i.e., a semantically coher-
ent representation of the logical structure of a
text—be it written or spoken discourse, unidirec-
tional or bidirectional communication, etc. Dif-
ferent formalisms have been proposed to model
these assumptions in frameworks of coherence re-
lations and discourse structure (Mann and Thomp-
son, 1988; Lascarides and Asher, 1993; Webber,
2004). In a more applied NLP context, the goal
of shallow discourse parsing (SDP) is to automat-
ically detect relevant discourse units and to label
the relations that hold between them. Unlike deep
discourse parsing, a stringent logical formaliza-
tion or the establishment of a global data structure,
say, a tree, is not required.
</bodyText>
<footnote confidence="0.9860655">
1http://www.cs.brandeis.edu/˜clp/
conll15st/index.html
</footnote>
<bodyText confidence="0.909142526315789">
With the release of the Penn Discourse Tree-
bank (Prasad et al., 2008, PDTB), annotated train-
ing data for SDP has become available and, as a
consequence, the field has considerably attracted
researchers from the NLP and IR community. In-
formally, the PDTB annotation scheme describes
a discourse unit as a syntactically motivated char-
acter span in the text, and augments with rela-
tions pointing from argument 2 (Arg2, prototypi-
cally, a discourse unit associated with an explicit
discourse marker) to its antecedent, i.e., the dis-
course unit Arg1. Relations are labeled with a re-
lation type (its sense) and the associated discourse
marker (either as found in the text or as inferred
by the annotator). PDTB distinguishes explicit
and implicit relations depending on whether such a
connector or cue phrase (e.g., because) is present,
or not.2 As an illustration, consider the following
example from the PDTB:
Arg1: Solo woodwind players have to be creative
if they want to work a lot
Connector: because
Arg2: their repertoire and audience appeal are
limited
In this explicit relation, Arg1 and Arg2 are directly
connected by the cue word; the relation type is
Contingency.Cause.Reason—one out of roughly
20 three-level senses marking the relation sense
between any given argument pair in the PDTB.
We participate in the CoNLL 2015 Shared Task
(Xue et al., 2015) with a minimalist end-to-end
shallow discourse parser developed from scratch.
It was, however, originally not specifically devel-
oped for this purpose, but created in preparation
of more elaborate experiments on implicit inter-
sentential relations in discourse, an aspect not ex-
plicitly addressed by the evaluation of the Shared
Task.
</bodyText>
<footnote confidence="0.9947975">
2The set of relation types is completed by alternative lex-
icalization (AltLex, discourse marker rephrased), entity rela-
tion (EntRel, i.e., anaphoric coherence), resp. the absence of
any relation (NoRel).
</footnote>
<page confidence="0.98045">
42
</page>
<note confidence="0.9458995">
Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 42–49,
Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999431875">
The remainder of the paper describes the ar-
chitecture and functionality of our system: A
rule-based component identifies explicit and im-
plicit argument-pairs and two statistical, data-
driven models classify senses. Our system suf-
fers from the surface-based definition of argument
spans and their evaluation as string ranges, but
with respect to sense disambiguation (in particu-
lar, in terms of precision), it is competitive with
other systems in the task. Inspired by the diver-
sity of different approaches to handle the more
challenging—and more interesting—non-explicit
relations, our description focuses on inferring im-
plicit senses and benefits from abstracting from
traditional surface-based features in favor of dis-
tributional representations of the argument spans.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99989434375">
At the moment, few full-fledged end-to-end dis-
course parsers exist, but they use different theo-
ries of discourse, e.g., PDTB (Lin et al., 2010),
or RST (duVerle and Prendinger, 2009; Feng and
Hirst, 2012). Most of the literature on automated
discourse analysis has focused on specialized sub-
tasks:
Argument identification is approached by, e.g.,
Ghosh et al. (2012) on the word and inter-
sentential level, using a CRF-based approach in-
cluding local and global features. Kong et al.
(2014) tackle argument span detection on the
constituent-level with features for subtrees and
special constraints.
Explicit relation classification Classifying the
senses of explicit relations is rather straightfor-
ward, given the cue phrase. Pitler and Nenkova
(2009) introduce a refinement using syntactic fea-
tures to disambiguate explicit connectives which
increases performance close to a human baseline.
Implicit relation classification In the early at-
tempt by Marcu and Echihabi (2002), implicit
relation classification was grounded on synthetic
training data (relation patterns with explicit cue
phrases removed) and a Naive Bayes model
trained on word-pair features. Aggregation over
such word-pairs was described by Biran and McK-
eown (2013), while Park and Cardie (2012) opti-
mized feature sets through feature selection, pre-
processing and special binning techniques.
Out of these, implicit relation classification re-
mains the most problematic subtask, and attracted
</bodyText>
<figureCaption confidence="0.997305">
Figure 1: Our three-component SDP pipeline.
</figureCaption>
<bodyText confidence="0.9988033125">
considerable interest: Pitler et al. (2009) present
an extensive evaluation of mostly linguistically
motivated features for implicit sense labeling in
a 4-way classification experiment. Useful indica-
tors, among others, are verb information, polarity
labels and the first and last three words of an ar-
gument. Lin et al. (2009) refine their work by in-
troducing contextual and dependency information
from the argument pairs and show that syntactic
phrase-structure features help in level-2 relation
type classifications. Moreover, Zhou et al. (2010)
use a language model to “predict” explicit connec-
tives from implicit relations. Our approach is most
similar to the one by Rutherford and Xue (2014),
who successfully integrate distributional represen-
tations to substitute word-pair features.
</bodyText>
<sectionHeader confidence="0.996361" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.9993802">
Our SDP system participates in the closed track
of the Shared Task.3 Its components are illus-
trated in Figure 1. Input is tokenized text in the
provided JSON format including meta information
about parts-of-speech and sentence boundaries.
</bodyText>
<subsectionHeader confidence="0.999357">
3.1 Argument Identification
</subsectionHeader>
<bodyText confidence="0.9996134">
The SDP pipeline processes the documents sen-
tence by sentence. Due to the strict time con-
straints of the Shared Task, we have set up a rule-
based detector for both Arg1 and Arg2 spans as
follows:
</bodyText>
<listItem confidence="0.99839825">
• Extract an explicit Arg1–Arg2 pair, where
Arg2 is a complete sentence starting with an
explicit connective.4 The previous sentence
serves as Arg1.
</listItem>
<footnote confidence="0.998565">
3http://www.cs.brandeis.edu/˜clp/
conll15st/dataset.html
4An exhaustive list of explicit cue words was obtained
from the training section of the PDTB, ranging from uni-
grams to 7-grams.
</footnote>
<page confidence="0.999597">
43
</page>
<listItem confidence="0.978438888888889">
• Refining step 1, we extract sentence-
internal explicit Arg1–Arg2 pairs by
applying the pattern BOS-Arg1-cue
word-punctuation-Arg2-EOS.5 Note
that we require a punctuation symbol be-
tween both arguments to prevent the template
from extracting, e.g., coordinated NPs such
as chairman and chief executive.
• We take special care of explicit tem-
poral Arg1–Arg2 relations and ex-
tract patterns of the form BOS-cue
word-Arg2-comma-Arg1-EOS. Cue
words are, e.g., while, although, unless.
• More complicated explicit patterns split the
second argument into two parts by the cue
word as with however in: Argument identi-
fication is tough. Writing patterns, however,
is easy.
• Finally, we extract all relations between adja-
cent, complete sentences as Arg1 and Arg2
spans as implicit, iff Arg1–Arg2 is not al-
ready an explicit relation and Arg1–Arg2
does not cross a paragraph boundary.
• EntRel and AltLex relations are beyond the
scope of our current parser as both taken to-
gether make up only 14.3% of all relations in
the training section of the PDTB.
</listItem>
<bodyText confidence="0.983442923076923">
Post processing A rule-based post-processor is
applied on top of the previous component. Its pur-
pose is to fix token lists for argument spans ac-
cording to the guidelines of the Shared Task as no
partial credit is given for non-exact matches. For
example, a leading or trailing punctuation, quote
or attribution spans must not be part of any of the
arguments.
This rule-based model had specifically to be de-
veloped for the Shared Task; it replaced a more
elaborate argument identifier based on structured
representations rather than character spans to rep-
resent the arguments of discourse relations.
</bodyText>
<subsectionHeader confidence="0.999958">
3.2 Labeling Explicit Senses
</subsectionHeader>
<bodyText confidence="0.940944642857143">
Given two argument spans and an explicit connec-
tive, we aim to predict the correct relation type
5BOS and EOS mark the beginning and the end of sen-
tence, respectively.
(sense). To this end, we trained a simple statis-
tical model6 in a supervised setting on all explicit
relations whose only feature is the cue word itself.
An exhaustive list of cue words (features) was ob-
tained from the training section of the PDTB data.
Moreover, we restricted the set of labels to those
eight senses that appear only frequently enough,
i.e. we excluded those whose proportion is less
than 5% of all explicit senses in the training sec-
tion.
</bodyText>
<subsectionHeader confidence="0.999899">
3.3 Labeling Implicit Senses
</subsectionHeader>
<bodyText confidence="0.9999558">
A third component handles the classification of
implicit senses for any implicit Arg1–Arg2 pair.
Similar to the previous subtask, we restrict the
label set (here to six senses). We trained vari-
ous models only on implicit relations. Inspired
by the previous literature on implicit sense clas-
sification, we experimented with different surface-
based word-pair feature sets for Arg1 and Arg2, as
well as more abstract representations for the word
forms, such as embeddings and word vectors:7
</bodyText>
<listItem confidence="0.9586065">
1. Word-pair (WP) token features of Arg1 and
Arg2: (i) normal-case (N) as encountered in
the text and (ii) after lower-case normaliza-
tion (l), both with frequency thresholds.
2. Similar to (1.) but using word stems (Porter,
1980) instead.
3. Similar to (1.) but using a Brown cluster 3200
representation (Turian et al., 2010) for each
word form if it exists. Otherwise, we use the
word form as feature.
</listItem>
<bodyText confidence="0.99994275">
A subsequent experiment is concerned with find-
ing a more compact representation of both Arg1
and Arg2 spans: For each argument pair, we com-
puted two real-valued vectors (600 features in to-
tal), in which each argument is represented by
a 300-dimensional feature vector. These were
obtained by summing over all skip-gram neural
word embeddings (Mikolov et al., 2013) present
in each argument weighted by the respective num-
ber of elements (embeddings) found in each argu-
ment. The normalization is necessary to handle
sentences of different lengths.
</bodyText>
<footnote confidence="0.947020285714286">
6In all our experiments, we made use of the JAVA imple-
mentation of libsvm (Chang and Lin, 2011) with linear kernel
and default parameters.
7A word-pair is defined as the cross product of any combi-
nation of words in both Arg1 and Arg2. Punctuation symbols
were removed before processing. All features are treated as
boolean if present (true) or absent (false).
</footnote>
<page confidence="0.998798">
44
</page>
<bodyText confidence="0.999755">
Testing the effect of both Brown clusters and
neural word embeddings, a final experiment com-
bines them into one feature set for each implicit
argument pair.
</bodyText>
<sectionHeader confidence="0.999767" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998931">
4.1 Argument Identification
</subsectionHeader>
<bodyText confidence="0.999975333333333">
In the overall task (based on the blind test set),
our system is ranked at position 13 – rather poorly
compared to 17 submitted systems in total (includ-
ing a baseline). This is due to the imperfect argu-
ment identification, and in particular due to the er-
roneous recognition of explicit cue phrases. The
system suffers from low overall recall of the iden-
tified explicit argument spans, including the con-
nective.8 A simple error analysis reveals that pat-
terns in which cue phrases do not directly start
the second argument are hard to identify by our
rule-based system. Moreover, punctuation sym-
bols pose problems to the system as well (cf.
our discussion in Section 4.3). A separate eval-
uation shows that post-processing argument pairs
improves F-score by 2%.
Despite these obvious drawbacks, we would
like to draw special attention to our statistical com-
ponents for sense classification: for the argument
pairs which were correctly recognized, our system
is ranked at position 4 for sense precision, even
outperforming the best three systems. We will
elaborate more on these models in the next sub-
section.
</bodyText>
<subsectionHeader confidence="0.962827">
4.2 Explicit and Implicit Senses
</subsectionHeader>
<bodyText confidence="0.945923037037037">
The classification of explicit senses with only
the connector word as single feature reaches an
accuracy of 80.48% using the PDTB training-
development split. This is still below state-of-the
art (94% in Pitler and Nenkova, 2009)9—yet sat-
isfying for our lightweight system with its original
emphasis on implicit relations.
Table 1 shows the results for implicit sense clas-
sification (472 instances in total) using different
feature sets.10 First, models trained on any of the
feature sets significantly outperform the majority
8Ranks for expl. Arg1-Arg2 prec., recall, F1: 12, 10, 11.
Ranks for expl. connective prec., recall, F1: 15, 16, 15.
9Note, however, that this is 4-way sense classification.
10We also tested a broad band-width of sentiment and
phrase-structure features, but with the resulting accuracies
not outperforming the current experiments, these are omitted
for reasons of brevity.
class baseline (25.4%, Expansion.Conjunction).11
Applying lower-case normalization to the input
tends to improve classifier performance, but using
a frequency threshold on the minimum number of
occurrences of a feature does not: This is an inter-
esting observation and not in line with the previ-
ous literature on implicit sense classification; Lin
et al. (2009), for example, use a frequency cutoff
of 5 for feature selection. Also, stemming as an-
other type of normalization seems not to be useful
either and yields slightly lower accuracies.
Noticeably, substituting surface-level word-pair
features by the Brown Cluster 3200 embeddings
yields a better performance. The difference is,
however, not statistically significant.12 More im-
portant, however, may be the positive side effect
of a smaller feature space (≈1.4 million) which is
reduced by 23%.
We expect the skip-gram neural word embed-
dings (word vectors) to perform even better than
Brown clusters: They are comparable in their con-
textual features but preserve the topology of the
original feature space. Indeed, these are competi-
tive with the low-frequency word-pair features and
even significantly better than the configurations 13,
14, 15. Their greatest benefit can be seen in the
overall number of real-valued features per instance
(which is only 600 in our setting). Finally, a com-
bination of Brown clusters and skip-gram embed-
dings yields the best results for the classification
of implicit senses. This gain over using the em-
beddings alone may possibly be attributed to non-
linearities in the feature space which may be par-
tially captured in the Brown clusters, but not with
embeddings in a SVM.13 We report detailed scores
for this best-performing classifier in Table 2.
</bodyText>
<subsectionHeader confidence="0.9827155">
4.3 Discussion &amp; Open Issues
4.3.1 Argument Span Identification
</subsectionHeader>
<bodyText confidence="0.757126066666666">
Exact argument identification is a crucial prepro-
cessing step for any SDP pipeline. Our shallow
11In all experiments, we applied the χ2 test statistic to as-
sess significance.
12We have tested the other Brown cluster representations
provided, as well, but 100, 320 and 1000 cluster sets yielded
lower accuracies.
13All results reported above were obtained with linear ker-
nels. These experiments have also been conducted with RBF
and polynomial kernels, whose performance was not reported
here, as it did not yield an improvement. However, truly non-
linear models would be possible with multi-layered neural
networks. While this may yield better results for word em-
beddings as features, such an experiment is left for future re-
search.
</bodyText>
<page confidence="0.998591">
45
</page>
<table confidence="0.999432666666667">
N0/l0 N1/l1 N2/l2 N3/l3 N4/l4 N5/l5
WP / Tokens 36.65/38.14 36.23/34.53 33.68/32.84 32.84/33.05 31.57/32.63 30.08/32.63
WP / Stems – /36.23 – /33.89 – /32.84 – /31.99 – /33.05 – /30.72
WP / Brown Cluster 3200 36.86/38.77 35.38/35.17 33.90/36.07 35.38/34.11 34.96/33.47 32.63/33.89
Word Vectors 36.23/37.28
WP / Brown Cluster + Word Vectors 37.28/39.41
</table>
<tableCaption confidence="0.984552666666667">
Table 1: Accuracies for 6-way implicit sense labeling and different feature sets when tokens are treated
in normal-case (N) or after lower-case preprocessing (l). Subscripts indicate frequency thresholds for
feature selection (0 means no threshold applied). Majority class baseline: 25.4%.
</tableCaption>
<table confidence="0.999948">
Prec Rec F1
Expansion.Conjunction 43.09 67.50 52.59
Expansion.Restatement 32.68 49.50 39.37
Comparison.Contrast 42.85 18.29 25.64
Contingency.Cause.Reason 41.26 35.61 38.23
Contingency.Cause.Result 40.00 16.32 23.18
Expansion.Instantiation 46.15 12.76 20.00
</table>
<tableCaption confidence="0.944542333333333">
Table 2: Detailed classification scores for the best-
performing classifier combining Brown Cluster
3200 and skip-gram embeddings.
</tableCaption>
<bodyText confidence="0.999029666666667">
discourse parser suffers from low overall recall
of the correctly recognized (explicit) spans, which
we see as the main source of poor performance in
the task evaluation.
Even though a system description may not be
the right place for a general discussion about the
appropriate representation of how arguments of
discourse relations are to be defined and repre-
sented, we would like to point out that we see a po-
tential issue in the rather strict evaluation of exact
matches within the Shared Task (which does not
allow for partial matches). Likewise problematic
is an arguable definition of gold spans for Arg1
and Arg2 in the provided training data. As an il-
lustration consider the following example:14
</bodyText>
<figure confidence="0.789542">
Gold:
Arg1: At any rate India needs the sugar
Arg2: it will be in sooner or later to buy it
Our System Output:
Arg1: At any rate, she added, “India needs the
sugar
Arg2: it will be in sooner or later to buy it.
</figure>
<bodyText confidence="0.964728428571429">
At least on a general basis, both argument spans
are correctly identified by our system. The only
14Document ID: wsj 2265, Relation ID: 36896.
difference is that punctuation symbols and attribu-
tion spans (she added) are not present in the gold
data. Note, however, that a rule-based removal of
such patterns is far from trivial, as syntactic pat-
terns are complex and the PDTB gold data reveals
many inconsistencies, especially regarding lead-
ing and trailing punctuation symbols. In this par-
ticular example, our system is capable of
(i) identifying the correct explicit connective
(so), and
(ii) classifying its correct sense
(Contingency.Cause.Result).
Nevertheless, it is not given any credit, as the sys-
tem’s token lists do not match the gold data. Very
much related to the span identification problem
sketched above is the detection of discontinuous
argument spans and cases in which our system
adds a subordinate clause to the argument, which
is not present in the gold data. We believe that—in
line with the annotation guidelines of the PDTB—
these are relevant factors to consider when imple-
menting a SDP, but that it should not affect the
overall evaluation in such a strict and rigid manner.
We would therefore encourage future evaluations
to
</bodyText>
<listItem confidence="0.9968916">
• either employ additional metrics permitting
partial matches, e.g., using sliding-window
metrics such as Pevzner and Hearst (2002),
• or to ground argument definitions in psy-
cholinguistically more plausible models of
</listItem>
<bodyText confidence="0.9700732">
propositions, cf. Lascarides and Asher
(1993) or Kintsch (1998), resp.—their more
operationalizable approximation in terms of,
say, frame semantics as previously annotated
for the PDTB data in the context of PropBank
</bodyText>
<page confidence="0.998618">
46
</page>
<bodyText confidence="0.893645666666667">
and NomBank (Palmer et al., 2005; Meyers et
al., 2004).
The latter idea may be challenging, as it involves
efficient handling of multi-layer annotations for
different major annotation projects, yet, experi-
ments in this direction have successfully been con-
ducted (Pustejovsky et al., 2005). This integrative
direction of research has been the original focus of
our system.
</bodyText>
<subsectionHeader confidence="0.985231">
4.3.2 Frequency Cutoffs for Word-Pair
Feature Selection
</subsectionHeader>
<bodyText confidence="0.9999358">
Our experiments indicate that frequency cutoffs
to select word-pair features for implicit relation
recognition do not seem to improve classifier per-
formance. While some previous approaches (most
notably Lin et al., 2009) incorporate cutoffs in
their experiments, others do not. But if a fre-
quency filter is applied, the specific value for the
threshold is usually not motivated.
We see a possible explanation for the negative
impact of cutoffs in the extremely sparse feature
space: Many word-pair features which are present
in the training section of the PDTB are not found
in the development set and vice versa, and with
frequency cutoffs applied, sparsity even grows fur-
ther. Closely related to our observation are ear-
lier findings that using even a small stop word list
has adverse effects on performance, which seems
implausible at first sight (Blair-Goldensohn et al.,
2007).
Biran and McKeown (2013) address this is-
sue in closer detail by replacing the sparse lexi-
cal word-pair features by more dense, aggregated
score features. Based on their experiments, the
authors argue that the most powerful features are
mainly function words. Yet, their lack of seman-
tic content whatsoever still calls for an explanation
why they are useful in distinguishing the different
types of implicit relations—except through over-
fitting the data.
As a side experiment, we performed 10-fold
cross validation on the PDTB, and again trained
implicit relations by varying the cutoff. The re-
sults are in line with our experiments reported in
Table 1 showing the same trend, which reinforces
the aforementioned sparsity issue.
Overall, we believe that more aggregated types
of features have advantages over sparse features
and that they are better in representing the underly-
ing semantic relationship between argument pairs.
We elaborate on this in our final subsection.
</bodyText>
<subsectionHeader confidence="0.989562">
4.3.3 Abstracting from Surface-Level
Features
</subsectionHeader>
<bodyText confidence="0.996150736842105">
Our experiments for implicit relation classification
have shown that is is beneficial to abstract from
surface-level (token) features for two reasons:
(i) word embeddings seem to express a more
general, semantic representation of the un-
derlying relationship between two arguments
in the discourse and
(ii) the number of features involved in a classifi-
cation can be significantly reduced which has
a positive effect on the computational side.
Future research should be concerned with a closer
inspection of how combinations of word embed-
dings can be used to increase classification results,
especially when no explicit connectives are avail-
able. Instead of vector addition, as applied in our
setting, we think that traditional vector-based sim-
ilarity measures comparing both arguments spans
seem to be highly promising in approaching their
underlying semantic relationship.
</bodyText>
<sectionHeader confidence="0.995738" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999970833333333">
In the context of the CoNLL 2015 Shared Task, we
have described a minimalist approach to shallow
discourse parsing with an emphasis on implicit re-
lation recognition.
Our system combines task-specific adaptations,
i.e., rule-based discourse unit identification via
templates, with data-driven models to infer senses
of (esp. implicit) discourse relations.
We described the system architecture and exper-
iments conducted on implicit sense labeling. In
this context, we motivated the need to model the
relationship between arguments in a more abstract
way using distributional representations instead of
surface-based features. Our experiments are in
line with previous work (most notably by Ruther-
ford and Xue, 2014), while having shown that
more abstract representations are at least equally
powerful in predicting the correct senses and, also,
that sparsity issues can be overcome. A slight im-
provement in performance has yielded a combina-
tion of distributional profiles for argument spans
(Brown clusters and skip-gram neural word em-
beddings) which is promising and should be ad-
dressed in closer detail in future work.
</bodyText>
<page confidence="0.999148">
47
</page>
<sectionHeader confidence="0.995188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999188594594595">
Or Biran and Kathleen McKeown. 2013. Aggregated
Word Pair Features for Implicit Discourse Relation
Disambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2013, 4-9 August 2013, Sofia, Bul-
garia, Volume 2: Short Papers, pages 69–73.
Sasha Blair-Goldensohn, Kathleen McKeown, and
Owen Rambow. 2007. Building and Refining
Rhetorical-Semantic Relation Models. In Can-
dace L. Sidner, Tanja Schultz, Matthew Stone, and
ChengXiang Zhai, editors, HLT-NAACL, pages 428–
435. The Association for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.
David A. duVerle and Helmut Prendinger. 2009. A
Novel Discourse Parser Based on Support Vector
Machine Classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Volume 2
- Volume 2, ACL ’09, pages 665–673, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistic Features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ’12, pages 60–68, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Sucheta Ghosh, Giuseppe Riccardi, and Richard Jo-
hansson. 2012. Global Features for Shallow Dis-
course Parsing. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL), pages 150–159.
Walter Kintsch. 1998. Comprehension: A Paradigm
for Cognition. Cambridge University Press, Cam-
bridge.
Fang Kong, Tou Hwee Ng, and Guodong Zhou. 2014.
A Constituent-Based Approach to Argument La-
beling with Joint Inference in Discourse Parsing.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 68–77. Association for Computa-
tional Linguistics.
Alex Lascarides and Nicholas Asher. 1993. Tem-
poral Interpretation, Discourse Relations and Com-
monsense entailment. Linguistics and Philosophy,
16(5):437–493.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing Implicit Discourse Relations in the
Penn Discourse Treebank. In Proceedings of the
2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 1 - Volume
1, EMNLP ’09, pages 343–351, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
PDTB-Styled End-to-End Discourse Parser. CoRR,
abs/1011.0835.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
Unsupervised Approach to Recognizing Discourse
Relations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
ACL ’02, pages 368–375, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. Annotating Noun
Argument Structure for NomBank. In Proceed-
ings of the Fourth International Conference on Lan-
guage Resources and Evaluation (LREC’04). Euro-
pean Language Resources Association (ELRA).
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. CoRR, abs/1301.3781.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Comput. Linguist., 31(1):71–
106, March.
Joonsuk Park and Claire Cardie. 2012. Improving
Implicit Discourse Relation Recognition Through
Feature Set Optimization. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, page 108–112, Seoul,
South Korea, July. Association for Computational
Linguistics, Association for Computational Linguis-
tics.
Lev Pevzner and Marti A Hearst. 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics, 28(1):19–
36.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In ACL 2009, Proceedings of the 47th Annual
Meeting of the Association for Computational Lin-
guistics and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, 2-7
August 2009, Singapore, Short Papers, pages 13–16.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 2 - Vol-
ume 2, ACL ’09, pages 683–691, Stroudsburg, PA,
USA. Association for Computational Linguistics.
</reference>
<page confidence="0.983483">
48
</page>
<reference confidence="0.999669325581395">
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In In Proceedings of LREC.
James Pustejovsky, Adam Meyers, Martha Palmer, and
Massimo Poesio, 2005. Proceedings of the Work-
shop on Frontiers in Corpus Annotations II: Pie
in the Sky, chapter Merging PropBank, NomBank,
TimeBank, Penn Discourse Treebank and Corefer-
ence, pages 5–12. Association for Computational
Linguistics.
Attapol Rutherford and Nianwen Xue. 2014. Discov-
ering Implicit Discourse Relations Through Brown
Cluster Pair Representation and Coreference Pat-
terns. In Proceedings of the 14th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 645–654. Association for
Computational Linguistics.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384–394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Bonnie L. Webber. 2004. D-LTAG: extending lex-
icalized TAG to discourse. Cognitive Science,
28(5):751–779.
Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi
Prasad, Christopher Bryant, and Attapol Rutherford.
2015. The CoNLL-2015 Shared Task on Shallow
Discourse Parsing. In Proceedings of the Nine-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, Beijing, China.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting Discourse
Connectives for Implicit Discourse Relation Recog-
nition. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ’10, pages 1507–1514, Stroudsburg, PA,
USA. Association for Computational Linguistics.
</reference>
<page confidence="0.999546">
49
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.178773">
<title confidence="0.9983435">A Minimalist Approach to Shallow Discourse Parsing and Relation Recognition</title>
<note confidence="0.317686666666667">Chiarcos Applied Computational Linguistics Goethe University Frankfurt am</note>
<abstract confidence="0.999041111111111">We describe a minimalist approach to shallow discourse parsing in the conof the CoNLL 2015 Shared Our parser integrates a rule-based component for argument identification and datadriven models for the classification of explicit and implicit relations. We place special emphasis on the evaluation of implicit sense labeling, we present different feature sets and show that (i) word embeddings are competitive with traditional word-level features, and (ii) that they can be used to considerably reduce the total number of features. Despite its simplicity, our parser is competitive with other systems in terms of sense recognition and thus provides a solid ground for further refinement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Kathleen McKeown</author>
</authors>
<title>Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013,</booktitle>
<pages>4--9</pages>
<location>Sofia, Bulgaria, Volume</location>
<contexts>
<context position="5967" citStr="Biran and McKeown (2013)" startWordPosition="886" endWordPosition="890">relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disambiguate explicit connectives which increases performance close to a human baseline. Implicit relation classification In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phrases removed) and a Naive Bayes model trained on word-pair features. Aggregation over such word-pairs was described by Biran and McKeown (2013), while Park and Cardie (2012) optimized feature sets through feature selection, preprocessing and special binning techniques. Out of these, implicit relation classification remains the most problematic subtask, and attracted Figure 1: Our three-component SDP pipeline. considerable interest: Pitler et al. (2009) present an extensive evaluation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment. Useful indicators, among others, are verb information, polarity labels and the first and last three words of an argument. Lin et al. (2009) refi</context>
<context position="21698" citStr="Biran and McKeown (2013)" startWordPosition="3368" endWordPosition="3371">if a frequency filter is applied, the specific value for the threshold is usually not motivated. We see a possible explanation for the negative impact of cutoffs in the extremely sparse feature space: Many word-pair features which are present in the training section of the PDTB are not found in the development set and vice versa, and with frequency cutoffs applied, sparsity even grows further. Closely related to our observation are earlier findings that using even a small stop word list has adverse effects on performance, which seems implausible at first sight (Blair-Goldensohn et al., 2007). Biran and McKeown (2013) address this issue in closer detail by replacing the sparse lexical word-pair features by more dense, aggregated score features. Based on their experiments, the authors argue that the most powerful features are mainly function words. Yet, their lack of semantic content whatsoever still calls for an explanation why they are useful in distinguishing the different types of implicit relations—except through overfitting the data. As a side experiment, we performed 10-fold cross validation on the PDTB, and again trained implicit relations by varying the cutoff. The results are in line with our expe</context>
</contexts>
<marker>Biran, McKeown, 2013</marker>
<rawString>Or Biran and Kathleen McKeown. 2013. Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 2: Short Papers, pages 69–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Kathleen McKeown</author>
<author>Owen Rambow</author>
</authors>
<title>Building and Refining Rhetorical-Semantic Relation Models.</title>
<date>2007</date>
<pages>428--435</pages>
<editor>In Candace L. Sidner, Tanja Schultz, Matthew Stone, and ChengXiang Zhai, editors, HLT-NAACL,</editor>
<publisher>The Association for Computational Linguistics.</publisher>
<contexts>
<context position="21672" citStr="Blair-Goldensohn et al., 2007" startWordPosition="3364" endWordPosition="3367">experiments, others do not. But if a frequency filter is applied, the specific value for the threshold is usually not motivated. We see a possible explanation for the negative impact of cutoffs in the extremely sparse feature space: Many word-pair features which are present in the training section of the PDTB are not found in the development set and vice versa, and with frequency cutoffs applied, sparsity even grows further. Closely related to our observation are earlier findings that using even a small stop word list has adverse effects on performance, which seems implausible at first sight (Blair-Goldensohn et al., 2007). Biran and McKeown (2013) address this issue in closer detail by replacing the sparse lexical word-pair features by more dense, aggregated score features. Based on their experiments, the authors argue that the most powerful features are mainly function words. Yet, their lack of semantic content whatsoever still calls for an explanation why they are useful in distinguishing the different types of implicit relations—except through overfitting the data. As a side experiment, we performed 10-fold cross validation on the PDTB, and again trained implicit relations by varying the cutoff. The results</context>
</contexts>
<marker>Blair-Goldensohn, McKeown, Rambow, 2007</marker>
<rawString>Sasha Blair-Goldensohn, Kathleen McKeown, and Owen Rambow. 2007. Building and Refining Rhetorical-Semantic Relation Models. In Candace L. Sidner, Tanja Schultz, Matthew Stone, and ChengXiang Zhai, editors, HLT-NAACL, pages 428– 435. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="11693" citStr="Chang and Lin, 2011" startWordPosition="1800" endWordPosition="1803">nt is concerned with finding a more compact representation of both Arg1 and Arg2 spans: For each argument pair, we computed two real-valued vectors (600 features in total), in which each argument is represented by a 300-dimensional feature vector. These were obtained by summing over all skip-gram neural word embeddings (Mikolov et al., 2013) present in each argument weighted by the respective number of elements (embeddings) found in each argument. The normalization is necessary to handle sentences of different lengths. 6In all our experiments, we made use of the JAVA implementation of libsvm (Chang and Lin, 2011) with linear kernel and default parameters. 7A word-pair is defined as the cross product of any combination of words in both Arg1 and Arg2. Punctuation symbols were removed before processing. All features are treated as boolean if present (true) or absent (false). 44 Testing the effect of both Brown clusters and neural word embeddings, a final experiment combines them into one feature set for each implicit argument pair. 4 Evaluation 4.1 Argument Identification In the overall task (based on the blind test set), our system is ranked at position 13 – rather poorly compared to 17 submitted system</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A duVerle</author>
<author>Helmut Prendinger</author>
</authors>
<title>A Novel Discourse Parser Based on Support Vector Machine Classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09,</booktitle>
<pages>665--673</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4919" citStr="duVerle and Prendinger, 2009" startWordPosition="734" endWordPosition="737">ct to sense disambiguation (in particular, in terms of precision), it is competitive with other systems in the task. Inspired by the diversity of different approaches to handle the more challenging—and more interesting—non-explicit relations, our description focuses on inferring implicit senses and benefits from abstracting from traditional surface-based features in favor of distributional representations of the argument spans. 2 Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syn</context>
</contexts>
<marker>duVerle, Prendinger, 2009</marker>
<rawString>David A. duVerle and Helmut Prendinger. 2009. A Novel Discourse Parser Based on Support Vector Machine Classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages 665–673, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Text-level Discourse Parsing with Rich Linguistic Features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4942" citStr="Feng and Hirst, 2012" startWordPosition="738" endWordPosition="741"> particular, in terms of precision), it is competitive with other systems in the task. Inspired by the diversity of different approaches to handle the more challenging—and more interesting—non-explicit relations, our description focuses on inferring implicit senses and benefits from abstracting from traditional surface-based features in favor of distributional representations of the argument spans. 2 Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disa</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Text-level Discourse Parsing with Rich Linguistic Features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 60–68, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Giuseppe Riccardi</author>
<author>Richard Johansson</author>
</authors>
<title>Global Features for Shallow Discourse Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>150--159</pages>
<contexts>
<context position="5103" citStr="Ghosh et al. (2012)" startWordPosition="762" endWordPosition="765">ging—and more interesting—non-explicit relations, our description focuses on inferring implicit senses and benefits from abstracting from traditional surface-based features in favor of distributional representations of the argument spans. 2 Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disambiguate explicit connectives which increases performance close to a human baseline. Implicit relation classification In the early attempt by Marcu and Echihabi </context>
</contexts>
<marker>Ghosh, Riccardi, Johansson, 2012</marker>
<rawString>Sucheta Ghosh, Giuseppe Riccardi, and Richard Johansson. 2012. Global Features for Shallow Discourse Parsing. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 150–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<title>Comprehension: A Paradigm for Cognition.</title>
<date>1998</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="20207" citStr="Kintsch (1998)" startWordPosition="3139" endWordPosition="3140">lause to the argument, which is not present in the gold data. We believe that—in line with the annotation guidelines of the PDTB— these are relevant factors to consider when implementing a SDP, but that it should not affect the overall evaluation in such a strict and rigid manner. We would therefore encourage future evaluations to • either employ additional metrics permitting partial matches, e.g., using sliding-window metrics such as Pevzner and Hearst (2002), • or to ground argument definitions in psycholinguistically more plausible models of propositions, cf. Lascarides and Asher (1993) or Kintsch (1998), resp.—their more operationalizable approximation in terms of, say, frame semantics as previously annotated for the PDTB data in the context of PropBank 46 and NomBank (Palmer et al., 2005; Meyers et al., 2004). The latter idea may be challenging, as it involves efficient handling of multi-layer annotations for different major annotation projects, yet, experiments in this direction have successfully been conducted (Pustejovsky et al., 2005). This integrative direction of research has been the original focus of our system. 4.3.2 Frequency Cutoffs for Word-Pair Feature Selection Our experiments</context>
</contexts>
<marker>Kintsch, 1998</marker>
<rawString>Walter Kintsch. 1998. Comprehension: A Paradigm for Cognition. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Tou Hwee Ng</author>
<author>Guodong Zhou</author>
</authors>
<title>A Constituent-Based Approach to Argument Labeling with Joint Inference in Discourse Parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>68--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5225" citStr="Kong et al. (2014)" startWordPosition="783" endWordPosition="786">stracting from traditional surface-based features in favor of distributional representations of the argument spans. 2 Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disambiguate explicit connectives which increases performance close to a human baseline. Implicit relation classification In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phra</context>
</contexts>
<marker>Kong, Ng, Zhou, 2014</marker>
<rawString>Fang Kong, Tou Hwee Ng, and Guodong Zhou. 2014. A Constituent-Based Approach to Argument Labeling with Joint Inference in Discourse Parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68–77. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<date>1993</date>
<booktitle>Temporal Interpretation, Discourse Relations and Commonsense entailment. Linguistics and Philosophy,</booktitle>
<pages>16--5</pages>
<contexts>
<context position="1467" citStr="Lascarides and Asher, 1993" startWordPosition="208" endWordPosition="211">ther systems in terms of sense recognition and thus provides a solid ground for further refinement. 1 Introduction Comprehending sentences and other textual units requires capabilities beyond capturing the lexical semantics of their components. Contextual information is needed, i.e., a semantically coherent representation of the logical structure of a text—be it written or spoken discourse, unidirectional or bidirectional communication, etc. Different formalisms have been proposed to model these assumptions in frameworks of coherence relations and discourse structure (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004). In a more applied NLP context, the goal of shallow discourse parsing (SDP) is to automatically detect relevant discourse units and to label the relations that hold between them. Unlike deep discourse parsing, a stringent logical formalization or the establishment of a global data structure, say, a tree, is not required. 1http://www.cs.brandeis.edu/˜clp/ conll15st/index.html With the release of the Penn Discourse Treebank (Prasad et al., 2008, PDTB), annotated training data for SDP has become available and, as a consequence, the field has considerably attracted researchers from</context>
<context position="20189" citStr="Lascarides and Asher (1993)" startWordPosition="3134" endWordPosition="3137">our system adds a subordinate clause to the argument, which is not present in the gold data. We believe that—in line with the annotation guidelines of the PDTB— these are relevant factors to consider when implementing a SDP, but that it should not affect the overall evaluation in such a strict and rigid manner. We would therefore encourage future evaluations to • either employ additional metrics permitting partial matches, e.g., using sliding-window metrics such as Pevzner and Hearst (2002), • or to ground argument definitions in psycholinguistically more plausible models of propositions, cf. Lascarides and Asher (1993) or Kintsch (1998), resp.—their more operationalizable approximation in terms of, say, frame semantics as previously annotated for the PDTB data in the context of PropBank 46 and NomBank (Palmer et al., 2005; Meyers et al., 2004). The latter idea may be challenging, as it involves efficient handling of multi-layer annotations for different major annotation projects, yet, experiments in this direction have successfully been conducted (Pustejovsky et al., 2005). This integrative direction of research has been the original focus of our system. 4.3.2 Frequency Cutoffs for Word-Pair Feature Selecti</context>
</contexts>
<marker>Lascarides, Asher, 1993</marker>
<rawString>Alex Lascarides and Nicholas Asher. 1993. Temporal Interpretation, Discourse Relations and Commonsense entailment. Linguistics and Philosophy, 16(5):437–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing Implicit Discourse Relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09,</booktitle>
<pages>343--351</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6562" citStr="Lin et al. (2009)" startWordPosition="974" endWordPosition="977">ran and McKeown (2013), while Park and Cardie (2012) optimized feature sets through feature selection, preprocessing and special binning techniques. Out of these, implicit relation classification remains the most problematic subtask, and attracted Figure 1: Our three-component SDP pipeline. considerable interest: Pitler et al. (2009) present an extensive evaluation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment. Useful indicators, among others, are verb information, polarity labels and the first and last three words of an argument. Lin et al. (2009) refine their work by introducing contextual and dependency information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications. Moreover, Zhou et al. (2010) use a language model to “predict” explicit connectives from implicit relations. Our approach is most similar to the one by Rutherford and Xue (2014), who successfully integrate distributional representations to substitute word-pair features. 3 Approach Our SDP system participates in the closed track of the Shared Task.3 Its components are illustrated in Figure 1. Input is tok</context>
<context position="14555" citStr="Lin et al. (2009)" startWordPosition="2253" endWordPosition="2256"> 9Note, however, that this is 4-way sense classification. 10We also tested a broad band-width of sentiment and phrase-structure features, but with the resulting accuracies not outperforming the current experiments, these are omitted for reasons of brevity. class baseline (25.4%, Expansion.Conjunction).11 Applying lower-case normalization to the input tends to improve classifier performance, but using a frequency threshold on the minimum number of occurrences of a feature does not: This is an interesting observation and not in line with the previous literature on implicit sense classification; Lin et al. (2009), for example, use a frequency cutoff of 5 for feature selection. Also, stemming as another type of normalization seems not to be useful either and yields slightly lower accuracies. Noticeably, substituting surface-level word-pair features by the Brown Cluster 3200 embeddings yields a better performance. The difference is, however, not statistically significant.12 More important, however, may be the positive side effect of a smaller feature space (≈1.4 million) which is reduced by 23%. We expect the skip-gram neural word embeddings (word vectors) to perform even better than Brown clusters: The</context>
<context position="21012" citStr="Lin et al., 2009" startWordPosition="3256" endWordPosition="3259">5; Meyers et al., 2004). The latter idea may be challenging, as it involves efficient handling of multi-layer annotations for different major annotation projects, yet, experiments in this direction have successfully been conducted (Pustejovsky et al., 2005). This integrative direction of research has been the original focus of our system. 4.3.2 Frequency Cutoffs for Word-Pair Feature Selection Our experiments indicate that frequency cutoffs to select word-pair features for implicit relation recognition do not seem to improve classifier performance. While some previous approaches (most notably Lin et al., 2009) incorporate cutoffs in their experiments, others do not. But if a frequency filter is applied, the specific value for the threshold is usually not motivated. We see a possible explanation for the negative impact of cutoffs in the extremely sparse feature space: Many word-pair features which are present in the training section of the PDTB are not found in the development set and vice versa, and with frequency cutoffs applied, sparsity even grows further. Closely related to our observation are earlier findings that using even a small stop word list has adverse effects on performance, which seem</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing Implicit Discourse Relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 343–351, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-Styled End-to-End Discourse Parser.</title>
<date>2010</date>
<location>CoRR, abs/1011.0835.</location>
<contexts>
<context position="4881" citStr="Lin et al., 2010" startWordPosition="728" endWordPosition="731">ring ranges, but with respect to sense disambiguation (in particular, in terms of precision), it is competitive with other systems in the task. Inspired by the diversity of different approaches to handle the more challenging—and more interesting—non-explicit relations, our description focuses on inferring implicit senses and benefits from abstracting from traditional surface-based features in favor of distributional representations of the argument spans. 2 Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (</context>
</contexts>
<marker>Lin, Ng, Kan, 2010</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A PDTB-Styled End-to-End Discourse Parser. CoRR, abs/1011.0835.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1439" citStr="Mann and Thompson, 1988" startWordPosition="203" endWordPosition="207">ser is competitive with other systems in terms of sense recognition and thus provides a solid ground for further refinement. 1 Introduction Comprehending sentences and other textual units requires capabilities beyond capturing the lexical semantics of their components. Contextual information is needed, i.e., a semantically coherent representation of the logical structure of a text—be it written or spoken discourse, unidirectional or bidirectional communication, etc. Different formalisms have been proposed to model these assumptions in frameworks of coherence relations and discourse structure (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004). In a more applied NLP context, the goal of shallow discourse parsing (SDP) is to automatically detect relevant discourse units and to label the relations that hold between them. Unlike deep discourse parsing, a stringent logical formalization or the establishment of a global data structure, say, a tree, is not required. 1http://www.cs.brandeis.edu/˜clp/ conll15st/index.html With the release of the Penn Discourse Treebank (Prasad et al., 2008, PDTB), annotated training data for SDP has become available and, as a consequence, the field has considerabl</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An Unsupervised Approach to Recognizing Discourse Relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>368--375</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5709" citStr="Marcu and Echihabi (2002)" startWordPosition="850" endWordPosition="853">Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disambiguate explicit connectives which increases performance close to a human baseline. Implicit relation classification In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phrases removed) and a Naive Bayes model trained on word-pair features. Aggregation over such word-pairs was described by Biran and McKeown (2013), while Park and Cardie (2012) optimized feature sets through feature selection, preprocessing and special binning techniques. Out of these, implicit relation classification remains the most problematic subtask, and attracted Figure 1: Our three-component SDP pipeline. considerable interest: Pitler et al. (2009) present an extensive evaluat</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An Unsupervised Approach to Recognizing Discourse Relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 368–375, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>Annotating Noun Argument Structure for NomBank.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04). European Language Resources Association (ELRA).</booktitle>
<contexts>
<context position="20418" citStr="Meyers et al., 2004" startWordPosition="3170" endWordPosition="3173"> should not affect the overall evaluation in such a strict and rigid manner. We would therefore encourage future evaluations to • either employ additional metrics permitting partial matches, e.g., using sliding-window metrics such as Pevzner and Hearst (2002), • or to ground argument definitions in psycholinguistically more plausible models of propositions, cf. Lascarides and Asher (1993) or Kintsch (1998), resp.—their more operationalizable approximation in terms of, say, frame semantics as previously annotated for the PDTB data in the context of PropBank 46 and NomBank (Palmer et al., 2005; Meyers et al., 2004). The latter idea may be challenging, as it involves efficient handling of multi-layer annotations for different major annotation projects, yet, experiments in this direction have successfully been conducted (Pustejovsky et al., 2005). This integrative direction of research has been the original focus of our system. 4.3.2 Frequency Cutoffs for Word-Pair Feature Selection Our experiments indicate that frequency cutoffs to select word-pair features for implicit relation recognition do not seem to improve classifier performance. While some previous approaches (most notably Lin et al., 2009) incor</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. Annotating Noun Argument Structure for NomBank. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04). European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="11416" citStr="Mikolov et al., 2013" startWordPosition="1754" endWordPosition="1757">h with frequency thresholds. 2. Similar to (1.) but using word stems (Porter, 1980) instead. 3. Similar to (1.) but using a Brown cluster 3200 representation (Turian et al., 2010) for each word form if it exists. Otherwise, we use the word form as feature. A subsequent experiment is concerned with finding a more compact representation of both Arg1 and Arg2 spans: For each argument pair, we computed two real-valued vectors (600 features in total), in which each argument is represented by a 300-dimensional feature vector. These were obtained by summing over all skip-gram neural word embeddings (Mikolov et al., 2013) present in each argument weighted by the respective number of elements (embeddings) found in each argument. The normalization is necessary to handle sentences of different lengths. 6In all our experiments, we made use of the JAVA implementation of libsvm (Chang and Lin, 2011) with linear kernel and default parameters. 7A word-pair is defined as the cross product of any combination of words in both Arg1 and Arg2. Punctuation symbols were removed before processing. All features are treated as boolean if present (true) or absent (false). 44 Testing the effect of both Brown clusters and neural wo</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="20396" citStr="Palmer et al., 2005" startWordPosition="3166" endWordPosition="3169">ng a SDP, but that it should not affect the overall evaluation in such a strict and rigid manner. We would therefore encourage future evaluations to • either employ additional metrics permitting partial matches, e.g., using sliding-window metrics such as Pevzner and Hearst (2002), • or to ground argument definitions in psycholinguistically more plausible models of propositions, cf. Lascarides and Asher (1993) or Kintsch (1998), resp.—their more operationalizable approximation in terms of, say, frame semantics as previously annotated for the PDTB data in the context of PropBank 46 and NomBank (Palmer et al., 2005; Meyers et al., 2004). The latter idea may be challenging, as it involves efficient handling of multi-layer annotations for different major annotation projects, yet, experiments in this direction have successfully been conducted (Pustejovsky et al., 2005). This integrative direction of research has been the original focus of our system. 4.3.2 Frequency Cutoffs for Word-Pair Feature Selection Our experiments indicate that frequency cutoffs to select word-pair features for implicit relation recognition do not seem to improve classifier performance. While some previous approaches (most notably L</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Comput. Linguist., 31(1):71– 106, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Claire Cardie</author>
</authors>
<title>Improving Implicit Discourse Relation Recognition Through Feature Set Optimization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>108--112</pages>
<location>Seoul, South</location>
<contexts>
<context position="5997" citStr="Park and Cardie (2012)" startWordPosition="892" endWordPosition="895">ing the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disambiguate explicit connectives which increases performance close to a human baseline. Implicit relation classification In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phrases removed) and a Naive Bayes model trained on word-pair features. Aggregation over such word-pairs was described by Biran and McKeown (2013), while Park and Cardie (2012) optimized feature sets through feature selection, preprocessing and special binning techniques. Out of these, implicit relation classification remains the most problematic subtask, and attracted Figure 1: Our three-component SDP pipeline. considerable interest: Pitler et al. (2009) present an extensive evaluation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment. Useful indicators, among others, are verb information, polarity labels and the first and last three words of an argument. Lin et al. (2009) refine their work by introducing c</context>
</contexts>
<marker>Park, Cardie, 2012</marker>
<rawString>Joonsuk Park and Claire Cardie. 2012. Improving Implicit Discourse Relation Recognition Through Feature Set Optimization. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 108–112, Seoul, South Korea, July. Association for Computational Linguistics, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>36</pages>
<contexts>
<context position="20057" citStr="Pevzner and Hearst (2002)" startWordPosition="3115" endWordPosition="3118">uch related to the span identification problem sketched above is the detection of discontinuous argument spans and cases in which our system adds a subordinate clause to the argument, which is not present in the gold data. We believe that—in line with the annotation guidelines of the PDTB— these are relevant factors to consider when implementing a SDP, but that it should not affect the overall evaluation in such a strict and rigid manner. We would therefore encourage future evaluations to • either employ additional metrics permitting partial matches, e.g., using sliding-window metrics such as Pevzner and Hearst (2002), • or to ground argument definitions in psycholinguistically more plausible models of propositions, cf. Lascarides and Asher (1993) or Kintsch (1998), resp.—their more operationalizable approximation in terms of, say, frame semantics as previously annotated for the PDTB data in the context of PropBank 46 and NomBank (Palmer et al., 2005; Meyers et al., 2004). The latter idea may be challenging, as it involves efficient handling of multi-layer annotations for different major annotation projects, yet, experiments in this direction have successfully been conducted (Pustejovsky et al., 2005). Thi</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19– 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using Syntax to Disambiguate Explicit Discourse Connectives in Text.</title>
<date>2009</date>
<booktitle>In ACL 2009, Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>2--7</pages>
<contexts>
<context position="5486" citStr="Pitler and Nenkova (2009)" startWordPosition="818" endWordPosition="821">B (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disambiguate explicit connectives which increases performance close to a human baseline. Implicit relation classification In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phrases removed) and a Naive Bayes model trained on word-pair features. Aggregation over such word-pairs was described by Biran and McKeown (2013), while Park and Cardie (2012) optimized feature sets through feature selection, preprocessing and special binning tech</context>
<context position="13525" citStr="Pitler and Nenkova, 2009" startWordPosition="2099" endWordPosition="2102">es F-score by 2%. Despite these obvious drawbacks, we would like to draw special attention to our statistical components for sense classification: for the argument pairs which were correctly recognized, our system is ranked at position 4 for sense precision, even outperforming the best three systems. We will elaborate more on these models in the next subsection. 4.2 Explicit and Implicit Senses The classification of explicit senses with only the connector word as single feature reaches an accuracy of 80.48% using the PDTB trainingdevelopment split. This is still below state-of-the art (94% in Pitler and Nenkova, 2009)9—yet satisfying for our lightweight system with its original emphasis on implicit relations. Table 1 shows the results for implicit sense classification (472 instances in total) using different feature sets.10 First, models trained on any of the feature sets significantly outperform the majority 8Ranks for expl. Arg1-Arg2 prec., recall, F1: 12, 10, 11. Ranks for expl. connective prec., recall, F1: 15, 16, 15. 9Note, however, that this is 4-way sense classification. 10We also tested a broad band-width of sentiment and phrase-structure features, but with the resulting accuracies not outperformi</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. In ACL 2009, Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP, 2-7 August 2009, Singapore, Short Papers, pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic Sense Prediction for Implicit Discourse Relations in Text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09,</booktitle>
<pages>683--691</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6280" citStr="Pitler et al. (2009)" startWordPosition="931" endWordPosition="934">In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phrases removed) and a Naive Bayes model trained on word-pair features. Aggregation over such word-pairs was described by Biran and McKeown (2013), while Park and Cardie (2012) optimized feature sets through feature selection, preprocessing and special binning techniques. Out of these, implicit relation classification remains the most problematic subtask, and attracted Figure 1: Our three-component SDP pipeline. considerable interest: Pitler et al. (2009) present an extensive evaluation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment. Useful indicators, among others, are verb information, polarity labels and the first and last three words of an argument. Lin et al. (2009) refine their work by introducing contextual and dependency information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications. Moreover, Zhou et al. (2010) use a language model to “predict” explicit connectives from implicit relations. Our approach is </context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic Sense Prediction for Implicit Discourse Relations in Text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages 683–691, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="10878" citStr="Porter, 1980" startWordPosition="1666" endWordPosition="1667">r to the previous subtask, we restrict the label set (here to six senses). We trained various models only on implicit relations. Inspired by the previous literature on implicit sense classification, we experimented with different surfacebased word-pair feature sets for Arg1 and Arg2, as well as more abstract representations for the word forms, such as embeddings and word vectors:7 1. Word-pair (WP) token features of Arg1 and Arg2: (i) normal-case (N) as encountered in the text and (ii) after lower-case normalization (l), both with frequency thresholds. 2. Similar to (1.) but using word stems (Porter, 1980) instead. 3. Similar to (1.) but using a Brown cluster 3200 representation (Turian et al., 2010) for each word form if it exists. Otherwise, we use the word form as feature. A subsequent experiment is concerned with finding a more compact representation of both Arg1 and Arg2 spans: For each argument pair, we computed two real-valued vectors (600 features in total), in which each argument is represented by a 300-dimensional feature vector. These were obtained by summing over all skip-gram neural word embeddings (Mikolov et al., 2013) present in each argument weighted by the respective number of</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M.F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0. In</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="1929" citStr="Prasad et al., 2008" startWordPosition="278" endWordPosition="281">ve been proposed to model these assumptions in frameworks of coherence relations and discourse structure (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004). In a more applied NLP context, the goal of shallow discourse parsing (SDP) is to automatically detect relevant discourse units and to label the relations that hold between them. Unlike deep discourse parsing, a stringent logical formalization or the establishment of a global data structure, say, a tree, is not required. 1http://www.cs.brandeis.edu/˜clp/ conll15st/index.html With the release of the Penn Discourse Treebank (Prasad et al., 2008, PDTB), annotated training data for SDP has become available and, as a consequence, the field has considerably attracted researchers from the NLP and IR community. Informally, the PDTB annotation scheme describes a discourse unit as a syntactically motivated character span in the text, and augments with relations pointing from argument 2 (Arg2, prototypically, a discourse unit associated with an explicit discourse marker) to its antecedent, i.e., the discourse unit Arg1. Relations are labeled with a relation type (its sense) and the associated discourse marker (either as found in the text or </context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Adam Meyers</author>
<author>Martha Palmer</author>
<author>Massimo Poesio</author>
</authors>
<date>2005</date>
<booktitle>Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, chapter Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and Coreference,</booktitle>
<pages>5--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20652" citStr="Pustejovsky et al., 2005" startWordPosition="3204" endWordPosition="3207">h as Pevzner and Hearst (2002), • or to ground argument definitions in psycholinguistically more plausible models of propositions, cf. Lascarides and Asher (1993) or Kintsch (1998), resp.—their more operationalizable approximation in terms of, say, frame semantics as previously annotated for the PDTB data in the context of PropBank 46 and NomBank (Palmer et al., 2005; Meyers et al., 2004). The latter idea may be challenging, as it involves efficient handling of multi-layer annotations for different major annotation projects, yet, experiments in this direction have successfully been conducted (Pustejovsky et al., 2005). This integrative direction of research has been the original focus of our system. 4.3.2 Frequency Cutoffs for Word-Pair Feature Selection Our experiments indicate that frequency cutoffs to select word-pair features for implicit relation recognition do not seem to improve classifier performance. While some previous approaches (most notably Lin et al., 2009) incorporate cutoffs in their experiments, others do not. But if a frequency filter is applied, the specific value for the threshold is usually not motivated. We see a possible explanation for the negative impact of cutoffs in the extremely</context>
</contexts>
<marker>Pustejovsky, Meyers, Palmer, Poesio, 2005</marker>
<rawString>James Pustejovsky, Adam Meyers, Martha Palmer, and Massimo Poesio, 2005. Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, chapter Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and Coreference, pages 5–12. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Attapol Rutherford</author>
<author>Nianwen Xue</author>
</authors>
<title>Discovering Implicit Discourse Relations Through Brown Cluster Pair Representation and Coreference Patterns.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>645--654</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6932" citStr="Rutherford and Xue (2014)" startWordPosition="1030" endWordPosition="1033">ation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment. Useful indicators, among others, are verb information, polarity labels and the first and last three words of an argument. Lin et al. (2009) refine their work by introducing contextual and dependency information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications. Moreover, Zhou et al. (2010) use a language model to “predict” explicit connectives from implicit relations. Our approach is most similar to the one by Rutherford and Xue (2014), who successfully integrate distributional representations to substitute word-pair features. 3 Approach Our SDP system participates in the closed track of the Shared Task.3 Its components are illustrated in Figure 1. Input is tokenized text in the provided JSON format including meta information about parts-of-speech and sentence boundaries. 3.1 Argument Identification The SDP pipeline processes the documents sentence by sentence. Due to the strict time constraints of the Shared Task, we have set up a rulebased detector for both Arg1 and Arg2 spans as follows: • Extract an explicit Arg1–Arg2 p</context>
</contexts>
<marker>Rutherford, Xue, 2014</marker>
<rawString>Attapol Rutherford and Nianwen Xue. 2014. Discovering Implicit Discourse Relations Through Brown Cluster Pair Representation and Coreference Patterns. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 645–654. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-Supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="10974" citStr="Turian et al., 2010" startWordPosition="1680" endWordPosition="1683">ious models only on implicit relations. Inspired by the previous literature on implicit sense classification, we experimented with different surfacebased word-pair feature sets for Arg1 and Arg2, as well as more abstract representations for the word forms, such as embeddings and word vectors:7 1. Word-pair (WP) token features of Arg1 and Arg2: (i) normal-case (N) as encountered in the text and (ii) after lower-case normalization (l), both with frequency thresholds. 2. Similar to (1.) but using word stems (Porter, 1980) instead. 3. Similar to (1.) but using a Brown cluster 3200 representation (Turian et al., 2010) for each word form if it exists. Otherwise, we use the word form as feature. A subsequent experiment is concerned with finding a more compact representation of both Arg1 and Arg2 spans: For each argument pair, we computed two real-valued vectors (600 features in total), in which each argument is represented by a 300-dimensional feature vector. These were obtained by summing over all skip-gram neural word embeddings (Mikolov et al., 2013) present in each argument weighted by the respective number of elements (embeddings) found in each argument. The normalization is necessary to handle sentence</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-Supervised Learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie L Webber</author>
</authors>
<title>D-LTAG: extending lexicalized TAG to discourse.</title>
<date>2004</date>
<journal>Cognitive Science,</journal>
<volume>28</volume>
<issue>5</issue>
<contexts>
<context position="1482" citStr="Webber, 2004" startWordPosition="212" endWordPosition="213">se recognition and thus provides a solid ground for further refinement. 1 Introduction Comprehending sentences and other textual units requires capabilities beyond capturing the lexical semantics of their components. Contextual information is needed, i.e., a semantically coherent representation of the logical structure of a text—be it written or spoken discourse, unidirectional or bidirectional communication, etc. Different formalisms have been proposed to model these assumptions in frameworks of coherence relations and discourse structure (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004). In a more applied NLP context, the goal of shallow discourse parsing (SDP) is to automatically detect relevant discourse units and to label the relations that hold between them. Unlike deep discourse parsing, a stringent logical formalization or the establishment of a global data structure, say, a tree, is not required. 1http://www.cs.brandeis.edu/˜clp/ conll15st/index.html With the release of the Penn Discourse Treebank (Prasad et al., 2008, PDTB), annotated training data for SDP has become available and, as a consequence, the field has considerably attracted researchers from the NLP and IR</context>
</contexts>
<marker>Webber, 2004</marker>
<rawString>Bonnie L. Webber. 2004. D-LTAG: extending lexicalized TAG to discourse. Cognitive Science, 28(5):751–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Hwee Tou Ng</author>
<author>Sameer Pradhan</author>
<author>Rashmi Prasad</author>
<author>Christopher Bryant</author>
<author>Attapol Rutherford</author>
</authors>
<date>2015</date>
<booktitle>The CoNLL-2015 Shared Task on Shallow Discourse Parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="3218" citStr="Xue et al., 2015" startWordPosition="486" endWordPosition="489">relations depending on whether such a connector or cue phrase (e.g., because) is present, or not.2 As an illustration, consider the following example from the PDTB: Arg1: Solo woodwind players have to be creative if they want to work a lot Connector: because Arg2: their repertoire and audience appeal are limited In this explicit relation, Arg1 and Arg2 are directly connected by the cue word; the relation type is Contingency.Cause.Reason—one out of roughly 20 three-level senses marking the relation sense between any given argument pair in the PDTB. We participate in the CoNLL 2015 Shared Task (Xue et al., 2015) with a minimalist end-to-end shallow discourse parser developed from scratch. It was, however, originally not specifically developed for this purpose, but created in preparation of more elaborate experiments on implicit intersentential relations in discourse, an aspect not explicitly addressed by the evaluation of the Shared Task. 2The set of relation types is completed by alternative lexicalization (AltLex, discourse marker rephrased), entity relation (EntRel, i.e., anaphoric coherence), resp. the absence of any relation (NoRel). 42 Proceedings of the Nineteenth Conference on Computational N</context>
</contexts>
<marker>Xue, Ng, Pradhan, Prasad, Bryant, Rutherford, 2015</marker>
<rawString>Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi Prasad, Christopher Bryant, and Attapol Rutherford. 2015. The CoNLL-2015 Shared Task on Shallow Discourse Parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Min Zhou</author>
<author>Yu Xu</author>
<author>Zheng-Yu Niu</author>
<author>Man Lan</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Predicting Discourse Connectives for Implicit Discourse Relation Recognition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>1507--1514</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6783" citStr="Zhou et al. (2010)" startWordPosition="1005" endWordPosition="1008">atic subtask, and attracted Figure 1: Our three-component SDP pipeline. considerable interest: Pitler et al. (2009) present an extensive evaluation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment. Useful indicators, among others, are verb information, polarity labels and the first and last three words of an argument. Lin et al. (2009) refine their work by introducing contextual and dependency information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications. Moreover, Zhou et al. (2010) use a language model to “predict” explicit connectives from implicit relations. Our approach is most similar to the one by Rutherford and Xue (2014), who successfully integrate distributional representations to substitute word-pair features. 3 Approach Our SDP system participates in the closed track of the Shared Task.3 Its components are illustrated in Figure 1. Input is tokenized text in the provided JSON format including meta information about parts-of-speech and sentence boundaries. 3.1 Argument Identification The SDP pipeline processes the documents sentence by sentence. Due to the stric</context>
</contexts>
<marker>Zhou, Xu, Niu, Lan, Su, Tan, 2010</marker>
<rawString>Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan. 2010. Predicting Discourse Connectives for Implicit Discourse Relation Recognition. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 1507–1514, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>