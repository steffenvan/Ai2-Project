<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000172">
<note confidence="0.661706">
SPONSORS:
</note>
<table confidence="0.255067357142857">
Advanced Research and Development Activity (ARDA)
Advanced Question Answering for Intelligence (AQUAINT) program
Defense Advanced Research Projects Agency (DARPA)
Translingual Information Detection, Extraction and Summarization (TIDES) program
INVITED SPEAKERS:
James Cowie, Director, Computing Research Laboratory, New Mexico State University
Randall Flynn, Geographer, National Imagery and Mapping Agency, and
Executive Secretary for Foreign Names, US Board on Geographic Names
PROGRAM COMMITTEE:
Andr´as Kornai, Metacarta, Co-chair
Beth Sundheim, SPAWAR Systems Center (US Dept. of Navy), Co-chair
Doug Appelt, SRI International
Merrick Lex Berman, Harvard Yenching Institute
Sean Boisen, BBN Technologies
</table>
<author confidence="0.751511">
Quintin Congdon, US Army National Ground Intelligence Center
James Cowie, New Mexico State University Computing Research Laboratory
</author>
<affiliation confidence="0.568008333333333">
Linda Hill, University of California, Santa Barbara
Douglas Jones, MIT Lincoln Laboratory
George Wilson, MITRE
</affiliation>
<sectionHeader confidence="0.829199" genericHeader="abstract">
CONFERENCE WEBSITE:
</sectionHeader>
<equation confidence="0.3089555">
http://www.kornai.com/NAACL
PREFACE
</equation>
<bodyText confidence="0.999387014492754">
The analysis of geographic references in natural language text involves, at least conceptually, four
distinct stages. Of course, implementations may vary greatly in how these stages are interleaved. The first
conceptual stage is geographic entity reference detection: strings such as New York, the Amazon delta,
LaGuardia, the San Diego-Tijuana border, [the] Brooklyn Bridge, a mile from downtown Manhattan,
etc. are identified in the text (Rauch et al). Second, contextual information gathering may help identify
the type and approximate location of geographic entities: LaGuardia Airport vs. LaGuardia Community
College, the town of Manhattan (population 44,831), etc. (Manov et al, Bilhaut et al). Third is the actual
disambiguation of the entity with respect to both type (New York City vs. New York State) and location
(Orange County, California vs. Orange County, Florida) (Leidner et al, Waldinger et al, Li et al).
Up to this point we can proceed, at least in principle, entirely on the basis of linguistic reasoning about
the document at hand, but the fourth stage, grounding, which is the assignment of geographic coordinates
to the identified entities, requires background knowledge in some form or another. The single most
important knowledge resource is a gazetteer containing at least longitude and latitude data associated
with each placename, and possibly supplementary information such as elevation, population, province
(or state), type of location, variant spellings, etc. There are large electronic gazetteers in existence that
are publicly available – one of the workshop goals was to discuss how they can be tailored and exploited to
meet the needs of NLP (Kwok and Deng, Axelrod) and conversely, how NLP techniques can be used for
database population, the addition of (partial) information about hitherto missing entries to the gazetteer
(Uryupina). Other background information, such as a collection of grounded documents, may also prove
useful (Smith and Mann, Li et al) both for training and evaluation purposes.
In organizing the workshop into two broad sessions we did not follow the above stages rigidly,
especially as these stages are most relevant for analyzing run-time behavior, while most papers deempha-
sized run-time processing detail (testing) and concentrated on creating and maintaining the background
knowledge they require (training). Our first session gathered those papers that focused on the semantics
of geographic references (feature types, ontologies, disambiguation), and the second session included
those that emphasized systems and gazetteer development. An invited talk kicked off each of the two
sessions, and a discussion period ended each one.
Effective analysis of textual references to places is a critical core technology for a wide variety of
NLP applications. As the 12 papers selected by the Program Committee for presentation in this volume
(from a total of 19 submissions) demonstrate, this is a very active research area, and one that feels the
very same tensions as the rest of NLP. There seems to be kind of Boyle-Mariotte Law (volume times
pressure is constant) in operation: at one end of the scale, researchers apply a lot of “pressure” (deep
analysis) to a few dozen to few thousand items; at the other end, they apply analytic techniques that are
a great deal shallower, but the “volume” of items is considerably larger, often in the millions to hundreds
of millions of items.
To a large extent, the two ends of the scale correspond to the well-articulated rationalist and empiricist
approaches to NLP, but here this seems to be dictated more by the nature of the data at hand than by
overriding philosophical considerations. In this collection, the high pressure/low volume extreme is
exemplified by papers such as those of Bilhaut et al, Waldinger et al, Smith and Mann, Manov et al, and
Southall. The low pressure/high volume extreme is seen in Li et al and Rauch et al, with papers like those
of Leidner et al and Kwok and Deng falling somewhere in the middle.
The workshop brought together researchers from the NLP and Digital Library communities; missing
were researchers from the GIS community. Map-based visualization of the results of the grounding state
of analysis, including the capability for bi-directional (both text-based and map-based) querying, while
obviously of prime concern to the designers of several systems described here, is discussed in detail
only in one of the twelve papers (Leidner et al). Perhaps the call for papers was too restrictive in this
direction, and there is an opportunity for a more user-centric follow-on workshop. Yet as organizers
we feel that advances in core technology that can bring to bear both lexical and spatial background
knowledge are more critical than advances in visualization, if we are to respond to the challenges of
providing accurate analyses of geographic references in broad domains and across languages, and to
provide useful information on subjects for which there is sparse training data.
To support analysis in multilingual and cross-lingual settings, such advances must of necessity begin
with the most mundane aspects of the problem: when to bracket a text string (tagging guidelines), how
to deal with foreign names in native script or in transliteration (text normalization), etc. Recognition
of the various ways that a given place may be referenced in one language is a challenging problem in
and of itself, and issues of name translation and transliteration and special character sets multiply that
problem. While our workshop revealed significant progress (Southall in identifying major relation types
for tagging, Axelrod from the database infrastructure standpoint, and Kwok and Deng concerning the
important special case that is Chinese), clearly much more remains to be done.
Work on applications such as question-answering, multidocument summarization and information
extraction, or first-story detection in streams of broadcast news, requires solid semantic foundations, and
again the workshop has some progress to report (Manov et al on ontology development, Waldinger et al
on axiomatic deduction, Smith and Mann on type classification), but here, perhaps, even more remains
to be done.
We would like to thank the authors, the members of the Program Committee, and the invited speak-
ers for their contributions to the planning and execution of the workshop, the workshop sponsors for
their financial support, and the HLT/NAACL conference organizers, especially Ed Hovy, James Allen,
Steven Abney, and Dragomir Radev, for their significant contributions to the overall management of the
workshop and their direction in preparing the publication of the proceedings.
</bodyText>
<reference confidence="0.320289">
Andr´as Kornai and Beth Sundheim
May 2003
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.007229">
<title confidence="0.768052166666667">SPONSORS: Advanced Research and Development Activity (ARDA) Advanced Question Answering for Intelligence (AQUAINT) program Defense Advanced Research Projects Agency (DARPA) Translingual Information Detection, Extraction and Summarization (TIDES) program INVITED SPEAKERS:</title>
<author confidence="0.626424">James Cowie</author>
<author confidence="0.626424">Computing Research Laboratory Director</author>
<author confidence="0.626424">New Mexico State University Randall Flynn</author>
<author confidence="0.626424">National Imagery Geographer</author>
<author confidence="0.626424">Mapping Agency</author>
<title confidence="0.9302955">Executive Secretary for Foreign Names, US Board on Geographic Names PROGRAM COMMITTEE:</title>
<author confidence="0.791086">Co-chair</author>
<affiliation confidence="0.663931888888889">Doug Appelt, SRI International Merrick Lex Berman, Harvard Yenching Institute Sean Boisen, BBN Technologies Quintin Congdon, US Army National Ground Intelligence Center James Cowie, New Mexico State University Computing Research Laboratory Linda Hill, University of California, Santa Barbara Douglas Jones, MIT Lincoln Laboratory George Wilson, MITRE CONFERENCE WEBSITE:</affiliation>
<web confidence="0.716815">http://www.kornai.com/NAACL PREFACE</web>
<abstract confidence="0.997163739130435">The analysis of geographic references in natural language text involves, at least conceptually, four distinct stages. Of course, implementations may vary greatly in how these stages are interleaved. The first stage is entity reference strings such as York, the Amazon delta, the San Diego-Tijuana border, [the] Brooklyn Bridge, a mile from downtown are identified in the text (Rauch et al). Second, information gathering help identify type and approximate location of geographic entities: LaGuardia LaGuardia the of etc. (Manov et al, Bilhaut et al). Third is the actual the entity with respect to both type York City York and location County, California County, (Leidner et al, Waldinger et al, Li et al). Up to this point we can proceed, at least in principle, entirely on the basis of linguistic reasoning about document at hand, but the fourth stage, which is the assignment of geographic coordinates to the identified entities, requires background knowledge in some form or another. The single most knowledge resource is a at least longitude and latitude data associated with each placename, and possibly supplementary information such as elevation, population, province (or state), type of location, variant spellings, etc. There are large electronic gazetteers in existence that are publicly available – one of the workshop goals was to discuss how they can be tailored and exploited to meet the needs of NLP (Kwok and Deng, Axelrod) and conversely, how NLP techniques can be used for population, addition of (partial) information about hitherto missing entries to the gazetteer (Uryupina). Other background information, such as a collection of grounded documents, may also prove useful (Smith and Mann, Li et al) both for training and evaluation purposes. In organizing the workshop into two broad sessions we did not follow the above stages rigidly, especially as these stages are most relevant for analyzing run-time behavior, while most papers deemphasized run-time processing detail (testing) and concentrated on creating and maintaining the background knowledge they require (training). Our first session gathered those papers that focused on the semantics of geographic references (feature types, ontologies, disambiguation), and the second session included those that emphasized systems and gazetteer development. An invited talk kicked off each of the two sessions, and a discussion period ended each one. Effective analysis of textual references to places is a critical core technology for a wide variety of NLP applications. As the 12 papers selected by the Program Committee for presentation in this volume (from a total of 19 submissions) demonstrate, this is a very active research area, and one that feels the very same tensions as the rest of NLP. There seems to be kind of Boyle-Mariotte Law (volume times pressure is constant) in operation: at one end of the scale, researchers apply a lot of “pressure” (deep analysis) to a few dozen to few thousand items; at the other end, they apply analytic techniques that are a great deal shallower, but the “volume” of items is considerably larger, often in the millions to hundreds of millions of items. a large extent, the two ends of the scale correspond to the well-articulated approaches to NLP, but here this seems to be dictated more by the nature of the data at hand than by overriding philosophical considerations. In this collection, the high pressure/low volume extreme is exemplified by papers such as those of Bilhaut et al, Waldinger et al, Smith and Mann, Manov et al, and Southall. The low pressure/high volume extreme is seen in Li et al and Rauch et al, with papers like those of Leidner et al and Kwok and Deng falling somewhere in the middle. The workshop brought together researchers from the NLP and Digital Library communities; missing were researchers from the GIS community. Map-based visualization of the results of the grounding state of analysis, including the capability for bi-directional (both text-based and map-based) querying, while obviously of prime concern to the designers of several systems described here, is discussed in detail only in one of the twelve papers (Leidner et al). Perhaps the call for papers was too restrictive in this direction, and there is an opportunity for a more user-centric follow-on workshop. Yet as organizers we feel that advances in core technology that can bring to bear both lexical and spatial background knowledge are more critical than advances in visualization, if we are to respond to the challenges of providing accurate analyses of geographic references in broad domains and across languages, and to provide useful information on subjects for which there is sparse training data. To support analysis in multilingual and cross-lingual settings, such advances must of necessity begin with the most mundane aspects of the problem: when to bracket a text string (tagging guidelines), how to deal with foreign names in native script or in transliteration (text normalization), etc. Recognition of the various ways that a given place may be referenced in one language is a challenging problem in and of itself, and issues of name translation and transliteration and special character sets multiply that problem. While our workshop revealed significant progress (Southall in identifying major relation types for tagging, Axelrod from the database infrastructure standpoint, and Kwok and Deng concerning the important special case that is Chinese), clearly much more remains to be done. Work on applications such as question-answering, multidocument summarization and information extraction, or first-story detection in streams of broadcast news, requires solid semantic foundations, and again the workshop has some progress to report (Manov et al on ontology development, Waldinger et al on axiomatic deduction, Smith and Mann on type classification), but here, perhaps, even more remains to be done. We would like to thank the authors, the members of the Program Committee, and the invited speakers for their contributions to the planning and execution of the workshop, the workshop sponsors for their financial support, and the HLT/NAACL conference organizers, especially Ed Hovy, James Allen, Steven Abney, and Dragomir Radev, for their significant contributions to the overall management of the workshop and their direction in preparing the publication of the proceedings.</abstract>
<author confidence="0.979228">Andr´as Kornai</author>
<author confidence="0.979228">Beth Sundheim</author>
<date confidence="0.980472">May 2003</date>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<date>2003</date>
<institution>Andr´as Kornai and Beth Sundheim</institution>
<marker>2003</marker>
<rawString>Andr´as Kornai and Beth Sundheim May 2003</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>