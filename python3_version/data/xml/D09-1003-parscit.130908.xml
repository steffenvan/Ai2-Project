<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001458">
<title confidence="0.9984385">
Semi-supervised Semantic Role Labeling
Using the Latent Words Language Model
</title>
<author confidence="0.995989">
Koen Deschacht Marie-Francine Moens
</author>
<affiliation confidence="0.877943">
Department of computer science Department of computer science
K.U.Leuven, Belgium K.U.Leuven, Belgium
</affiliation>
<email confidence="0.983854">
koen.deschacht@cs.kuleuven.be sien.moens@cs.kuleuven.be
</email>
<sectionHeader confidence="0.993486" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999760791666667">
Semantic Role Labeling (SRL) has proved
to be a valuable tool for performing auto-
matic analysis of natural language texts.
Currently however, most systems rely on
a large training set, which is manually an-
notated, an effort that needs to be repeated
whenever different languages or a differ-
ent set of semantic roles is used in a cer-
tain application. A possible solution for
this problem is semi-supervised learning,
where a small set of training examples
is automatically expanded using unlabeled
texts. We present the Latent Words Lan-
guage Model, which is a language model
that learns word similarities from unla-
beled texts. We use these similarities for
different semi-supervised SRL methods as
additional features or to automatically ex-
pand a small training set. We evaluate the
methods on the PropBank dataset and find
that for small training sizes our best per-
forming system achieves an error reduc-
tion of 33.27% F1-measure compared to
a state-of-the-art supervised baseline.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969130434783">
Automatic analysis of natural language is still a
very hard task to perform for a computer. Al-
though some successful applications have been de-
veloped (see for instance (Chinchor, 1998)), im-
plementing an automatic text analysis system is
still a labour and time intensive task. Many ap-
plications would benefit from an intermediate rep-
resentation of texts, where an automatic analysis
is already performed which is sufficiently general
to be useful in a wide range of applications.
Syntactic analysis of texts (such as Part-Of-
Speech tagging and syntactic parsing) is an ex-
ample of such a generic analysis, and has proved
useful in applications ranging from machine trans-
lation (Marcu et al., 2006) to text mining in the
bio-medical domain (Cohen and Hersh, 2005). A
syntactic parse is however a representation that is
very closely tied with the surface-form of natural
language, in contrast to Semantic Role Labeling
(SRL) which adds a layer of predicate-argument
information that generalizes across different syn-
tactic alternations (Palmer et al., 2005). SRL has
received a lot of attention in the research commu-
nity, and many systems have been developed (see
section 2). Most of these systems rely on a large
dataset for training that is manually annotated. In
this paper we investigate whether we can develop a
system that achieves state-of-the-art semantic role
labeling without relying on a large number of la-
beled examples. We aim to do so by employing the
Latent Words Language Model that learns latent
words from a large unlabeled corpus. Latent words
are words that (unlike observed words) did not oc-
cur at a particular position in a text, but given se-
mantic and syntactic constraints from the context
could have occurred at that particular position.
In section 2 we revise existing work on SRL and
on semi-supervised learning. Section 3 outlines
our supervised classifier for SRL and section 4 dis-
cusses the Latent Words Language Model. In sec-
tion 5 we will combine the two models for semi-
supervised role labeling. We will test the model
on the standard PropBank dataset and compare it
with state-of-the-art semi-supervised SRL systems
in section 6 and finally in section 7 we draw con-
clusions and outline future work.
</bodyText>
<sectionHeader confidence="0.999717" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.996832333333333">
Gildea and Jurafsky (2002) were the first to de-
scribe a statistical system trained on the data from
the FrameNet project to automatically assign se-
mantic roles. This approach was soon followed
by other researchers (Surdeanu et al., 2003; Prad-
han et al., 2004; Xue and Palmer, 2004), focus-
</bodyText>
<page confidence="0.992497">
21
</page>
<note confidence="0.9966315">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21–29,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999169375">
ing on improved sets of features, improved ma-
chine learning methods or both, and SRL became
a shared task at the CoNLL 2004, 2005 and 2008
conferences1. The best system (Johansson and
Nugues, 2008) in CoNLL 2008 achieved an F1-
measure of 81.65% on the workshop’s evaluation
corpus.
Semi-supervised learning has been suggested
by many researchers as a solution to the annota-
tion bottleneck (see (Chapelle et al., 2006; Zhu,
2005) for an overview), and has been applied suc-
cessfully on a number of natural language pro-
cessing tasks. Mann and McCallum (2007) ap-
ply Expectation Regularization to Named Entity
Recognition and Part-Of-Speech tagging, achiev-
ing improved performance when compared to su-
pervised methods, especially on small numbers of
training data. Koo et al. (2008) present an algo-
rithm for dependency parsing that uses clusters of
semantically related words, which were learned
in an unsupervised manner. There has been lit-
tle research on semi-supervised learning for SRL.
We refer to He and Gildea (2006) who tested ac-
tive learning and co-training methods, but found
little or no gain from semi-supervised learning,
and to Swier and Stevenson (2004), who achieved
good results using semi-supervised methods, but
tested their methods on a small number of Verb-
Net roles, which have not been used by other SRL
systems. To the best of our knowledge no sys-
tem was able to reproduce the successful results
of (Swier and Stevenson, 2004) on the PropBank
roleset. Our approach most closely resembles the
work of Fürstenau and Lapata (2009) who auto-
matically expand a small training set using an au-
tomatic dependency alignment of unlabeled sen-
tences. This method was tested on the FrameNet
corpus and improved results when compared to a
fully-supervised classifier. We will discuss their
method in detail in section 5.
</bodyText>
<sectionHeader confidence="0.96418" genericHeader="method">
3 Semantic role labeling
</sectionHeader>
<bodyText confidence="0.9985635">
Fillmore (1968) introduced semantic structures
called semantic frames, describing abstract ac-
tions or common situations (frames) with common
roles and themes (semantic roles). Inspired by this
idea different resources were constructed, includ-
ing FrameNet (Baker et al., 1998) and PropBank
(Palmer et al., 2005). An alternative approach to
semantic role labeling is the framework developed
</bodyText>
<footnote confidence="0.997927">
1See http://www.cnts.ua.ac.be/conll/ for an overview.
</footnote>
<bodyText confidence="0.99952675">
by Halliday (1994) and implemented by Mehay
et al. (2005). PropBank has thus far received the
most attention of the research community, and is
used in our work.
</bodyText>
<subsectionHeader confidence="0.997403">
3.1 PropBank
</subsectionHeader>
<bodyText confidence="0.999903666666667">
The goal of the PropBank project is to add seman-
tic information to the syntactic nodes in the En-
glish Penn Treebank. The main motivation for this
annotation is the preservation of semantic roles
across different syntactic realizations. Take for in-
stance the sentences
</bodyText>
<listItem confidence="0.9931905">
1. The window broke.
2. John broke the window.
</listItem>
<bodyText confidence="0.997687466666667">
In both sentences the constituent “the window” is
broken, although it occurs at different syntactic
positions. The PropBank project defines for a
large collection of verbs (excluding auxiliary
verbs such as “will”, “can”, ...) a set of senses,
that reflect the different meanings and syntactic
alternations of this verb. Every sense has a
number of expected roles, numbered from Arg0
to Arg5. A small number of arguments are shared
among all senses of all verbs, such as temporals
(Arg-TMP), locatives (Arg-LOC) and directionals
(Arg-DIR). Additional to the frame definitions,
PropBank has annotated a large training corpus
containing approximately 113.000 annotated
verbs. An example of an annotated sentence is
[John Arg0][broke BREAK.01] [the window Arg1].
Here BREAK.01 is the first sense of the “break”
verb. Note that (1) although roles are defined for
every frame separately, in reality roles with iden-
tical names are identical or very similar for all
frames, a fact that is exploited to train accurate role
classifiers and (2) semantic role labeling systems
typically assume that a frame is fully expressed in
a single sentence and thus do not try to instanti-
ate roles across sentence boundaries. Although the
original PropBank corpus assigned semantic roles
to syntactic phrases (such as noun phrases), we use
the CoNLL dataset, where the PropBank corpus
was converted to a dependency representation, as-
signing semantic roles to single (head) words.
</bodyText>
<subsectionHeader confidence="0.962735">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.955996">
In this section we discuss the features used in the
semantic role labeling system. All features but the
</bodyText>
<page confidence="0.993928">
22
</page>
<bodyText confidence="0.999891425">
Split path feature are taken from existing seman-
tic role labeling systems, see for example (Gildea
and Jurafsky, 2002; Lim et al., 2004; Thompson
et al., 2006). The number in brackets denotes the
number of unique features for that type.
Word We split every sentence in (unigram) word
tokens, including punctuation. (37079)
Stem We reduce the word tokens to their stem,
e.g. “walks” -&gt; “walk”. (28690)
POS The part-of-speech tag for every word, e.g.
“NNP” (for a singular proper noun). (77)
Neighbor POS’s The concatenated part-of-
speech tags of the word before and the word
just after the current word, e.g. “RBS_JJR”.
(1787)
Path This important feature describes the path
through the dependency tree from the current
word to the position of the predicate, e.g.
“coordTobjTadvTrootJdepJnmodJpmod”,
where ‘T’ indicates going up a constituent
and ‘J’ going down one constituent.
(829642)
Split Path Because of the nature of the path fea-
ture, an explosion of unique features is found
in a given data set. We reduce this by split-
ting the path in different parts and using every
part as a distinct feature. We split, for exam-
ple, the previous path in 6 different features:
“coord”, “Tobj”, “Tadv”, “Troot”, “Jdep”,
“Jnmod”, “Jpmod”. Note that the split path
feature includes the POS feature, since the
first component of the path is the POS tag for
the current word. This feature has not been
used previously for semantic role detection.
(155)
For every word wi in the training and test set we
construct the feature vector f(wi), where at every
position in this vector 1 indicates the presence for
the corresponding feature and 0 the absence of that
feature.
</bodyText>
<subsectionHeader confidence="0.982394">
3.3 Discriminative model
</subsectionHeader>
<bodyText confidence="0.967434">
Discriminative models have been found to outper-
form generative models for many different tasks
including SRL (Lim et al., 2004). For this reason
we also employ discriminative models here. The
structure of the model was inspired by a similar
Figure 1: Discriminative model for SRL. Grey
circles represent observed variables, white circles
hidden variables and arrows directed dependen-
cies. s ranges over all sentences in the corpus and
j over the n words in the sentence.
(although generative) model in (Thompson et al.,
2006) where it was used for semantic frame clas-
sification. The model (fig. 1) assumes that the role
label rij for the word wi is conditioned on the fea-
tures fi and on the role label ri_1j of the previous
word and that the predicate label pj for word wj is
conditioned on the role labels Rj and on the fea-
tures fj. This model can be seen as an extension
of the standard Maximum Entropy Markov Model
(MEMM, see (Ratnaparkhi, 1996)) with an extra
dependency on the predicate label, we will hence-
forth refer to this model as MEMM+pred.
To estimate the parameters of the MEMM+pred
model we turn to the successful Maximum En-
tropy (Berger et al., 1996) parameter estimation
method. The Maximum Entropy principle states
that the best model given the training data is the
model such that the conditional distribution de-
fined by the model has maximum entropy subject
to the constraints represented by the training ex-
amples. There is no closed form solution to find
this maximum and we thus turn to an iterative
method. In this work we use Generalized Itera-
tive Scaling2, but other methods such as (quasi-)
Newton optimization could also have been used.
</bodyText>
<sectionHeader confidence="0.974353" genericHeader="method">
4 Latent Words Language Model
</sectionHeader>
<subsectionHeader confidence="0.958313">
4.1 Rationale
</subsectionHeader>
<bodyText confidence="0.999969166666667">
As discussed in sections 1 and 3 most SRL sys-
tems are trained today on a large set of manually
annotated examples. PropBank for example con-
tains approximately 50000 sentences. This man-
ual annotation is both time and labour-intensive,
and needs to be repeated for new languages or
</bodyText>
<footnote confidence="0.8855115">
2We use the maxent package available on
http://maxent.sourceforge.net/
</footnote>
<page confidence="0.997395">
23
</page>
<bodyText confidence="0.999991295454546">
for new domains requiring a different set of roles.
One approach that can help to solve this problem
is semi-supervised learning, where a small set of
annotated examples is used together with a large
set of unlabeled examples when training a SRL
model.
Manual inspection of the results of the super-
vised model discussed in the previous section
showed that the main source of errors was in-
correct labeling of a word because the word to-
ken did not occur, or occurred only a small num-
ber of times in the training set. We hypothesize
that knowledge of semantic similar words could
overcome this problem by associating words that
occurred infrequently in the training set to sim-
ilar words that occurred more frequently. Fur-
thermore, we would like to learn these similar-
ities automatically, to be independent of knowl-
edge sources that might not be available for all
languages or domains.
The Distributional Hypothesis, supported by
theoretical linguists such as Harris (1954), states
that words that occur in the same contexts tend
to have similar meanings. This suggests that one
can learn the similarity between two words auto-
matically by comparing their relative contexts in
a large unlabeled corpus, which was confirmed by
different researchers (e.g. (Lin, 1998; McDonald
and Ramscar, 2001; Grefenstette, 1994)). Differ-
ent methods for computing word similarities have
been proposed, differing between methods to rep-
resent the context (using dependency relationship
or a window of words) and between methods that,
given a set of contexts, compute the similarity be-
tween different words (ranging from cosine simi-
larity to more complex metrics such as the Jaccard
index). We refer to (Lin, 1998) for a comparison
of the different similarity metrics.
In the next section we propose a novel method
to learn word similarities, the Latent Words Lan-
guage Model (LWLM) (Deschacht and Moens,
2009). This model learns similar words and learns
the a distribution over the contexts in which cer-
tain types of words occur typically.
</bodyText>
<subsectionHeader confidence="0.963335">
4.2 Definition
</subsectionHeader>
<bodyText confidence="0.9931745">
The LWLM introduces for a text T = w1...wN of
length N for every observed word wi at position i
a hidden variable hi. The model is a generative
model for natural language, in which the latent
variable hi is generated by its context C(hi) and the
observed word wi is generated by the latent vari-
able hi. In the current model we assume that the
context is C(hi) = hi−1
</bodyText>
<equation confidence="0.9475958">
i−2hi+2
i+1 where hi−1
i−2 = hi−2hi−1
is the two previous words and hi+2
i+1 = hi+1hi+2 is
</equation>
<bodyText confidence="0.999867161290322">
the two next words. The observed wi has a value
from the vocabulary V, while the hidden variable
hi is unknown, and is modeled as a probability
distribution over all words of V. We will see in
the next section how this distribution is estimated
from a large unlabeled training corpus. The aim
of this model is to estimate, at every position i,
a distribution for hi, assigning high probabilities
to words that are similar to wi, given the context
of this word C(hi), and low probabilities to words
that are not similar to wi in this context.
A possible interpretation of this model states
that every hidden variable hi models the “mean-
ing” for a particular word in a particular context.
In this probabilistic model, when generating a sen-
tence, we generate the meaning of a word (which
is an unobserved representation) with a certain
probability, and then we generate a certain obser-
vation by writing down one of the possible words
that express this meaning.
Creating a representation that models the mean-
ing of a word is an interesting (and controversial)
topic in its own right, but in this work we make
the assumption that the meaning of a particular
word can be modeled using other words. Model-
ing the meaning of a word with other words is not
an unreasonable one, since it is already employed
in practice by humans (e.g. by using dictionar-
ies and thesauri) and machines (e.g. relying on a
lexical resource such as WordNet) in word sense
disambiguation tasks.
</bodyText>
<subsectionHeader confidence="0.995462">
4.3 Parameter estimation
</subsectionHeader>
<bodyText confidence="0.9997263">
As we will further see the LWLM model has three
probability distributions: P(wi|hi), the probability
of the observed word wj given the latent variable
hj, P(hi|hi−1 i−2), the probability of the hidden word
hj given the previous variables hj−2 and hj−1, and
P(hi|hi+2
i+1), the probability of the hidden word hj
given the next variables hj+1 and hj+2. These dis-
tributions need to be learned from a training text
Ttrain =&lt; w0...wz &gt; of length Z.
</bodyText>
<subsubsectionHeader confidence="0.505471">
4.3.1 The Baum-Welch algorithm
</subsubsectionHeader>
<bodyText confidence="0.999991">
The attentive reader will have noticed the sim-
ilarity between the proposed model and a stan-
dard second-order Hidden Markov Model (HMM)
where the hidden state is dependent on the two
</bodyText>
<page confidence="0.994026">
24
</page>
<bodyText confidence="0.999912153846154">
previous states. However, we are not able to use
the standard Baum-Welch (or forward-backward)
algorithm, because the hidden variable hi is mod-
eled as a probability distribution over all words
in the vocabulary V. The Baum-Welch algorithm
would result in an execution time of O(|V|3NG)
where |V |is the size of the vocabulary, N is the
length of the training text and G is the number of
iterations needed to converge. Since in our dataset
the vocabulary size is more than 30K words (see
section 3.2), using this algorithm is not possible.
Instead we use techniques of approximate infer-
ence, i.e. Gibbs sampling.
</bodyText>
<subsectionHeader confidence="0.541117">
4.3.2 Initialization
</subsectionHeader>
<bodyText confidence="0.999899555555556">
Gibbs sampling starts from a random initializa-
tion for the hidden variables and then improves
the estimates in subsequent iterations. In prelimi-
nary experiments it was found that a pure random
initialization results in a very long burn-in-period
and a poor performance of the final model. For
this reason we initially set the distributions for the
hidden words equal to the distribution of words as
given by a standard language model3.
</bodyText>
<subsectionHeader confidence="0.634002">
4.3.3 Gibbs sampling
</subsectionHeader>
<bodyText confidence="0.99248215">
We store the initial estimate of the hidden vari-
ables in M0train =&lt; h0...hZ &gt;, where hi generates
wi at every position i. Gibbs sampling is a Markov
Chain Monte Carlo method that updates the esti-
mates of the hidden variables in a number of it-
erations. Mτtrain denotes the estimate of the hid-
den variables in iteration τ. In every iteration a
new estimate Mτ+1
train is generated from the previ-
ous estimate Mτtrain by selecting a random posi-
tion j and updating the value of the hidden vari-
able at that position. The probability distributions
Pτ(wj|hj), Pτ(hj|hj−1
j−2) and Pτ(hj|hj+2
j+1) are con-
structed by collecting the counts from all positions
i =6 j. The hidden variable hj is dependent on hj−2,
hj−1, hj+1, hj+2 and wj and we can compute the
distribution of possible values for the variable hj
as
</bodyText>
<equation confidence="0.972298">
Pτ(hj|wj,hj−1
0 ,hZ j+1) =
Pτ(wj|hj)Pτ(hj|hj−1
j−2hj+2 j+1)
∑ hi Pτ(wi|hi)Pτ(hj|hj−1
j−2hj+2
j+1)
We set P(hj|hj−1
j−2hj+2
j+1) = P(hj|hj−1
j−2) · P(hj|hj+2
j+1)
which can be easily computed given the above dis-
</equation>
<footnote confidence="0.97052">
3We used the interpolated Kneser-Ney model as described
in (Goodman, 2001).
</footnote>
<bodyText confidence="0.9652491875">
tributions. We select a new value for the hidden
variable according to Pτ(hj|wj,hj−1
0 ,hZ j+1) and
place it at position j in Mτ+1
train. The current esti-
mate for all other unobserved words remains the
same. After performing this iteration a large num-
ber of times (|V  |∗ 10 in this experiment), the dis-
tribution approaches the true maximum likelihood
distribution. Gibbs sampling however samples this
distribution, and thus will never reach it exactly. A
number of iterations (|V  |∗ 100) is then performed
in which Gibbs sampling oscillates around the cor-
rect distribution. We collect independent samples
of this distribution every |V  |∗ 10 iterations, which
are then used to construct the final model.
</bodyText>
<subsectionHeader confidence="0.992795">
4.4 Evaluation of the Language Model
</subsectionHeader>
<bodyText confidence="0.999960947368421">
A first evaluation of the quality of the automat-
ically learned latent words is by translation of
this model into a sequential language model and
by measuring its perplexity on previously unseen
texts. In (Deschacht and Moens, 2009) we per-
form a number of experiments, comparing differ-
ent corpora (news texts from Reuters and from
Associated Press, and articles from Wikipedia)
and n-gram sizes (3-gram and 4-gram). We also
compared the proposed model with two state-of-
the-art language models, Interpolated Kneser-Ney
smoothing and fullibmpredict (Goodman, 2001),
and found that LWLM outperformed both models
on all corpora, with a perplexity reduction ranging
between 12.40% and 5.87%. These results show
that the estimated distributions over latent words
are of a high quality and lead us to believe they
could be used to improve automatic text analysis,
like SRL.
</bodyText>
<sectionHeader confidence="0.930346" genericHeader="method">
5 Role labeling using latent words
</sectionHeader>
<bodyText confidence="0.984504307692308">
The previous section discussed how the LWLM
learns similar words and how these similarities im-
proved the perplexity on an unseen text of the lan-
guage model derived from this model. In this sec-
tion we will see how we integrate the latent words
model in two novel semi-supervised SRL models
and compare these with two state-of-the-art semi-
supervised models for SRL and dependency pars-
ing.
Latent words as additional features
In a first approach we estimate the distribution of
latent words for every word for both the training
and test set. We then use the latent words at every
</bodyText>
<page confidence="0.995587">
25
</page>
<bodyText confidence="0.999599333333334">
position as additional probabilistic features for the
discriminative model. More specifically, we ap-
pend |V |extra values to the feature vector f(wj),
containing the probability distribution over the |V |
possible words for the hidden variable hi4. We call
this the LWFeatures method.
This method has the advantage that it is simple
to implement and that many existing SRL systems
can be easily extended by adding additional fea-
tures. We also expect that this method can be em-
ployed almost effortless in other information ex-
traction tasks, such as Named Entity Recognition
or Part-Of-Speech labeling.
We compare this approach to the semi-
supervised method in Koo et al. (2008) who em-
ploy clusters of related words constructed by the
Brown clustering algorithm (Brown et al., 1992)
for syntactic processing of texts. Interestingly,
this clustering algorithm has a similar objective as
LWLM since it tries to optimize a class-based lan-
guage model in terms of perplexity on an unseen
test text. We employ a slightly different clustering
method here, the fullibmpredict method discussed
in (Goodman, 2001). This method was shown
to outperform the class based model proposed in
(Brown et al., 1992) and can thus be expected to
discover better clusters of words. We append the
feature vector f(wj) with c extra values (where c is
the number of clusters), respectively set to 1 if the
word wi belongs to the corresponding cluster or to
0 otherwise. We call this method the ClusterFea-
tures method.
Automatic expansion of the training set using
predicate argument alignment
We compare our approach with a method proposed
by Fürstenau and Lapata (2009). This approach is
more tailored to the specific case of SRL and is
summarized here.
Given a set of labeled seed verbs with annotated
semantic roles, for every annotated verb a number
of occurrences of this verb is found in unlabeled
texts where the context is similar to the context of
the annotated example. The context is defined here
as all words in the sentence that are direct depen-
dents of this verb, given the syntactic dependency
tree. The similarity between two occurrences of a
particular verb is measured by finding all different
alignments σ : Mσ → {1...n} (Mσ ⊂ {1,...,m})
</bodyText>
<footnote confidence="0.644764">
4Probabilities smaller than 1e10−4 were set to 0 for effi-
ciency reasons.
</footnote>
<bodyText confidence="0.996260333333333">
between the m dependents of the first occurrence
and the n dependents of the second occurrence.
Every alignment σ is assigned a score given by
</bodyText>
<equation confidence="0.8013645">
∑ (A · syn(gi,gσ(i)) + sem(wi,wσ(i)) − B)
i∈Mσ
</equation>
<bodyText confidence="0.999989416666667">
where syn(gi,gσ(i)) denotes the syntactic simi-
larity between grammatical role5 gi of word wi
and grammatical role gσ(i) of word wσ(i), and
sem(wi,wσ(i)) measures the semantic similarity
between words wi and wσ(i). A is a constant
weighting the importance of the syntactic simi-
larity compared to semantic similarity, and B can
be interpreted as the lowest similarity value for
which an alignment between two arguments is
possible. The syntactic similarity syn(gi,gσ(i)) is
defined as 1 if the dependency relations are iden-
tical, 0 &lt; a &lt; 1 if the relations are of the same
type but of a different subtype6 and 0 otherwise.
The semantic similarity sem(wi,wσ(i)) is automat-
ically estimated as the cosine similarity between
the contexts of wi and wσ(i) in a large text cor-
pus. For details we refer to (Fürstenau and Lapata,
2009).
For every verb in the annotated training set we
find the k occurrences of that verb in the unlabeled
texts where the contexts are most similar given the
best alignment. We then expand the training set
with these examples, automatically generating an
annotation using the discovered alignments. The
variable k controls the trade-off between anno-
tation confidence and expansion size. The final
model is then learned by running the supervised
training method on the expanded training set. We
call this method AutomaticExpansionCOS7. The
values for k, a, A and B are optimized automati-
cally in every experiment on a held-out set (dis-
joint from both training and test set).
We adapt this approach by employing a different
method for measuring semantic similarity. Given
two words wi and wσ(i) we estimate the distri-
bution of latent words, respectively L(hi) and
</bodyText>
<footnote confidence="0.997313833333333">
5Note that this is a syntactic role, not a semantic role as
the ones discussed in this article.
6Subtypes are fine-grained distinctions made by the parser
such as the underlying grammatical roles in passive construc-
tions.
7The only major differences with (Fürstenau and Lap-
ata, 2009) are the dependency parser which was used (the
MALT parser (Nivre et al., 2006) instead of the RASP parser
(Briscoe et al., 2006)) and the corpus employed to learn se-
mantic similarities (the Reuters corpus instead of the British
National Corpus). We expect that these differences will only
influence the results minimally.
</footnote>
<page confidence="0.990624">
26
</page>
<table confidence="0.999814166666667">
5% 20% 50% 100%
Supervised 40.49% 67.23% 74.93% 78.65%
LWFeatures 60.29% 72.88% 76.42% 80.98%
CZusterFeatures 59.51% 66.70% 70.15% 72.62%
AutomaticExpansionCOS 47.05% 53.72% 64.51% 70.52%
AutomaticExpansionLW 45.40% 53.82% 65.39% 72.66%
</table>
<tableCaption confidence="0.67416575">
Table 1: Results (in F1-measure) on the CoNLL 2008 test set for the different methods, comparing
the supervised method (Supervised) with the semi-supervised methods LWFeatures, CZusterFeatures,
AutomaticExpansionCOS and AutomaticExpansionLW. See section 5 for details on the different methods.
Best results are in bold.
</tableCaption>
<bodyText confidence="0.994780666666667">
L(h6(i)). We then compute the semantic similarity
measure as the Jensen-Shannon (Lin, 1997) diver-
gence
</bodyText>
<equation confidence="0.972871333333333">
JS(L(hi)||L(h6(i))) =
1 [D(L(hi)||avg)+D(L(h6(i))||avg)�
2
</equation>
<bodyText confidence="0.99931425">
where avg = (L(hi) + L(h6(i)))/2 is the average
between the two distributions and D(L(hi)||avg)
is the Kullback–Leiber divergence (Cover and
Thomas, 2006).
Although this change might appear only a slight
deviation from the original model discussed in
(Fürstenau and Lapata, 2009) it is potentially an
important one, since an accurate semantic similar-
ity measure will greatly influence the accuracy of
the alignments, and thus of the accuracy of the au-
tomatic expansion. We call this method Automat-
icExpansionLW.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999924823529412">
We perform a number of experiments where we
compare the fully supervised model with the semi-
supervised models proposed in the previous sec-
tion. We first train the LWLM model on an unla-
beled 5 million word Reuters corpus8.
We perform different experiments for the super-
vised and the four different semi-supervised meth-
ods (see previous section). Table 1 shows the re-
sults of the different methods on the test set of the
CoNLL 2008 shared task. We experimented with
different sizes for the training set, ranging from
5% to 100%. When using a subset of the full train-
ing set, we run 10 different experiments with ran-
dom subsets and average the results.
We see that the LWFeatures method performs
better than the other methods across all train-
ing sizes. Furthermore, these improvements are
</bodyText>
<footnote confidence="0.974786">
8See http://www.daviddlewis.com/resources
</footnote>
<bodyText confidence="0.999813222222222">
larger for smaller training sets, showing that the
approach can be applied successfully in a setting
where only a small number of training examples
is available.
When comparing the LWFeatures method with
the CZusterFeatures method we see that, although
the CZusterFeatures method has a similar perfor-
mance for small training sizes, this performance
drops for larger training sizes. A possible expla-
nation for this result is the use of the clusters em-
ployed in the CZusterFeatures method. By defini-
tion the clusters merge many words into one clus-
ter, which might lead to good generalization (more
important for small training sizes) but can poten-
tially hurt precision (more important for larger
training sizes).
A third observation that can be made from table
1 is that, although both automatic expansion meth-
ods (AutomaticExpansionCOS and AutomaticEx-
pansionCOS) outperform the supervised method
for the smallest training size, for other sizes of the
training set they perform relatively poorly. An in-
formal inspection showed that for some examples
in the training set, little or no correct similar occur-
rences were found in the unlabeled text. The algo-
rithm described in section 5 adds the most similar
k occurrences to the training set for every anno-
tated example, also for these examples where lit-
tle or no similar occurrences were found. Often
the automatic alignment fails to generate correct
labels for these occurrences and introduces errors
in the training set. In the future we would like to
perform experiments that determine dynamically
(for instance based on the similarity measure be-
tween occurrences) for every annotated example
how many training examples to add.
</bodyText>
<page confidence="0.997369">
27
</page>
<sectionHeader confidence="0.986715" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999996477272727">
We have presented the Latent Words Language
Model and showed how it learns, from unla-
beled texts, latent words that capture the mean-
ing of a certain word, depending on the con-
text. We then experimented with different meth-
ods to incorporate the latent words for Semantic
Role Labeling, and tested different methods on the
PropBank dataset. Our best performing method
showed a significant improvement over the su-
pervised model and over methods previously pro-
posed in the literature. On the full training set
the best method performed 2.33% better than the
fully supervised model, which is a 10.91% error
reduction. Using only 5% of the training data the
best semi-supervised model still achieved 60.29%,
compared to 40.49% by the supervised model,
which is an error reduction of 33.27%. These re-
sults demonstrate that the latent words learned by
the LWLM help for this complex information ex-
traction task. Furthermore we have shown that the
latent words are simple to incorporate in an ex-
isting classifier by adding additional features. We
would like to perform experiments on employing
this model in other information extraction tasks,
such as Word Sense Disambiguation or Named
Entity Recognition. The current model uses the
context in a very straightforward way, i.e. the two
words left and right of the current word, but in
the future we would like to explore more advanced
methods to improve the similarity estimates. Lin
(1998) for example discusses a method where a
syntactic parse of the text is performed and the
context of a word is modeled using dependency
triples.
The other semi-supervised methods proposed
here were less successful, although all improved
on the supervised model for small training sizes.
In the future we would like to improve the de-
scribed automatic expansion methods, since we
feel that their full potential has not yet been
reached. More specifically we plan to experiment
with more advanced methods to decide whether
some automatically generated examples should be
added to the training set.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99996025">
The work reported in this paper was supported
by the EU-IST project CLASS (Cognitive-Level
Annotation using Latent Statistical Structure, IST-
027978) and the IWT-SBO project AMASS++
(IWT-SBO-060051). We thank the anonymous re-
viewers for their helpful comments and Dennis N.
Mehay for his help on clarifying the linguistic mo-
tivation of our models.
</bodyText>
<sectionHeader confidence="0.990339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999756152173913">
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
Berkeley FrameNet project. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics, volume 98.
Montreal, Canada.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational linguistics,
22(1):39–71.
T. Briscoe, J. Carroll, and R. Watson. 2006. The sec-
ond release of the RASP system. In Proceedings of
the Interactive Demo Session of COLING/ACL, vol-
ume 6.
P.F. Brown, R.L. Mercer, V.J. Della Pietra, and J.C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467–479.
O. Chapelle, B. Schölkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
N.A. Chinchor. 1998. Overview of MUC-7/MET-2. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7), volume 1.
A.M. Cohen and W.R. Hersh. 2005. A survey of cur-
rent work in biomedical text mining. Briefings in
Bioinformatics, 6(1):57–71.
T.M. Cover and J.A. Thomas. 2006. Elements of In-
formation Theory. Wiley-Interscience.
Koen Deschacht and Marie-Francine Moens. 2009.
The Latent Words Language Model. In Proceed-
ings of the 18th Annual Belgian-Dutch Conference
on Machine Learning.
C. J. Fillmore. 1968. The case for case. In E. Bach and
R. Harms, editors, Universals in Linguistic Theory.
Rinehart &amp; Winston.
Hagen Fürstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 220–228, Athens, Greece.
Association for Computational Linguistics.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245–288.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling, extended version. Technical re-
port, Microsoft Research.
</reference>
<page confidence="0.9766">
28
</page>
<reference confidence="0.99990464893617">
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Springer.
M.A.K. Halliday. 1994. An Introduction to Functional
Grammar (second edition). Edward Arnold, Lon-
don.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146–162.
S. He and D. Gildea. 2006. Self-training and Co-
training for Semantic Role Labeling: Primary Re-
port. Technical report. TR 891.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis with
propbank and nombank. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 183–187,
Manchester, England, August. Coling 2008 Orga-
nizing Committee.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 595–603.
J.-H. Lim, Y.-S. Hwang, S.-Y. Park, and H.-C. Rim.
2004. Semantic role labeling using maximum en-
tropy model. In Proceedings of the Eighth Confer-
ence on Computational Natural Language Learning,
pages 122–125, Boston, Massachusetts, USA. ACL.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Pro-
ceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics, volume 35,
pages 64–71. ACL.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational Linguistics,
pages 768–774. Association for Computational Lin-
guistics Morristown, NJ, USA.
G.S. Mann and A. McCallum. 2007. Simple, ro-
bust, scalable semi-supervised learning via expecta-
tion regularization. In Proceedings of the 24th In-
ternational Conference on Machine Learning, pages
593–600. ACM Press New York, USA.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntact-
ified target language phrases. In Proceedings of the
Conference on Empirical Methods for Natural Lan-
guage Processing, pages 44–52.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings
of the 23rd Annual Conference of the Cognitive Sci-
ence Society, pages 611–616.
Dennis Mehay, Rik De Busser, and Marie-Francine
Moens. 2005. Labeling generic semantic roles. In
Proceedings of the Sixth International Workshop on
Computational Semantics.
J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A
datadriven parser-generator for dependency parsing.
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation, pages
2216–2219.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proceedings of the Hu-
man Language Technology Conference/North Amer-
ican chapter of the Association of Computational
Linguistics, Boston, MA.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 133–142. Association for Com-
putational Linguistics.
M. Surdeanu, S. Harabagiu, J. Williams, and
P. Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 8–15.
R.S. Swier and S. Stevenson. 2004. Unsupervised se-
mantic role labelling. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 95–102.
C. Thompson, R. Levy, and C. Manning. 2006. A gen-
erative model for FrameNet semantic role labeling .
In Proceedings of the 14th European Conference on
Machine Learning, Cavtat-Dubrovnik, Croatia.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, volume 4.
X. Zhu. 2005. Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences,
University of Wisconsin-Madison.
</reference>
<page confidence="0.999116">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.397846">
<title confidence="0.9995915">Semi-supervised Semantic Role Labeling Using the Latent Words Language Model</title>
<author confidence="0.920935">Koen Deschacht Marie-Francine Moens</author>
<affiliation confidence="0.961086">Department of computer science Department of computer science</affiliation>
<address confidence="0.615737">K.U.Leuven, Belgium K.U.Leuven, Belgium</address>
<email confidence="0.540533">koen.deschacht@cs.kuleuven.besien.moens@cs.kuleuven.be</email>
<abstract confidence="0.99830888">Semantic Role Labeling (SRL) has proved to be a valuable tool for performing automatic analysis of natural language texts. Currently however, most systems rely on a large training set, which is manually annotated, an effort that needs to be repeated whenever different languages or a different set of semantic roles is used in a certain application. A possible solution for this problem is semi-supervised learning, where a small set of training examples is automatically expanded using unlabeled texts. We present the Latent Words Language Model, which is a language model that learns word similarities from unlabeled texts. We use these similarities for different semi-supervised SRL methods as additional features or to automatically expand a small training set. We evaluate the methods on the PropBank dataset and find that for small training sizes our best performing system achieves an error reduction of 33.27% F1-measure compared to a state-of-the-art supervised baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>98</volume>
<location>Montreal, Canada.</location>
<contexts>
<context position="6083" citStr="Baker et al., 1998" startWordPosition="966" endWordPosition="969"> the work of Fürstenau and Lapata (2009) who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences. This method was tested on the FrameNet corpus and improved results when compared to a fully-supervised classifier. We will discuss their method in detail in section 5. 3 Semantic role labeling Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles). Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). An alternative approach to semantic role labeling is the framework developed 1See http://www.cnts.ua.ac.be/conll/ for an overview. by Halliday (1994) and implemented by Mehay et al. (2005). PropBank has thus far received the most attention of the research community, and is used in our work. 3.1 PropBank The goal of the PropBank project is to add semantic information to the syntactic nodes in the English Penn Treebank. The main motivation for this annotation is the preservation of semantic roles across different syntactic realizations. Take for instance the </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, volume 98. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="11162" citStr="Berger et al., 1996" startWordPosition="1802" endWordPosition="1805">or semantic frame classification. The model (fig. 1) assumes that the role label rij for the word wi is conditioned on the features fi and on the role label ri_1j of the previous word and that the predicate label pj for word wj is conditioned on the role labels Rj and on the features fj. This model can be seen as an extension of the standard Maximum Entropy Markov Model (MEMM, see (Ratnaparkhi, 1996)) with an extra dependency on the predicate label, we will henceforth refer to this model as MEMM+pred. To estimate the parameters of the MEMM+pred model we turn to the successful Maximum Entropy (Berger et al., 1996) parameter estimation method. The Maximum Entropy principle states that the best model given the training data is the model such that the conditional distribution defined by the model has maximum entropy subject to the constraints represented by the training examples. There is no closed form solution to find this maximum and we thus turn to an iterative method. In this work we use Generalized Iterative Scaling2, but other methods such as (quasi-) Newton optimization could also have been used. 4 Latent Words Language Model 4.1 Rationale As discussed in sections 1 and 3 most SRL systems are trai</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Interactive Demo Session of COLING/ACL,</booktitle>
<volume>6</volume>
<contexts>
<context position="25681" citStr="Briscoe et al., 2006" startWordPosition="4230" endWordPosition="4233">ining and test set). We adapt this approach by employing a different method for measuring semantic similarity. Given two words wi and wσ(i) we estimate the distribution of latent words, respectively L(hi) and 5Note that this is a syntactic role, not a semantic role as the ones discussed in this article. 6Subtypes are fine-grained distinctions made by the parser such as the underlying grammatical roles in passive constructions. 7The only major differences with (Fürstenau and Lapata, 2009) are the dependency parser which was used (the MALT parser (Nivre et al., 2006) instead of the RASP parser (Briscoe et al., 2006)) and the corpus employed to learn semantic similarities (the Reuters corpus instead of the British National Corpus). We expect that these differences will only influence the results minimally. 26 5% 20% 50% 100% Supervised 40.49% 67.23% 74.93% 78.65% LWFeatures 60.29% 72.88% 76.42% 80.98% CZusterFeatures 59.51% 66.70% 70.15% 72.62% AutomaticExpansionCOS 47.05% 53.72% 64.51% 70.52% AutomaticExpansionLW 45.40% 53.82% 65.39% 72.66% Table 1: Results (in F1-measure) on the CoNLL 2008 test set for the different methods, comparing the supervised method (Supervised) with the semi-supervised methods L</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>T. Briscoe, J. Carroll, and R. Watson. 2006. The second release of the RASP system. In Proceedings of the Interactive Demo Session of COLING/ACL, volume 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>R L Mercer</author>
<author>V J Della Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="21874" citStr="Brown et al., 1992" startWordPosition="3596" endWordPosition="3599">g the probability distribution over the |V | possible words for the hidden variable hi4. We call this the LWFeatures method. This method has the advantage that it is simple to implement and that many existing SRL systems can be easily extended by adding additional features. We also expect that this method can be employed almost effortless in other information extraction tasks, such as Named Entity Recognition or Part-Of-Speech labeling. We compare this approach to the semisupervised method in Koo et al. (2008) who employ clusters of related words constructed by the Brown clustering algorithm (Brown et al., 1992) for syntactic processing of texts. Interestingly, this clustering algorithm has a similar objective as LWLM since it tries to optimize a class-based language model in terms of perplexity on an unseen test text. We employ a slightly different clustering method here, the fullibmpredict method discussed in (Goodman, 2001). This method was shown to outperform the class based model proposed in (Brown et al., 1992) and can thus be expected to discover better clusters of words. We append the feature vector f(wj) with c extra values (where c is the number of clusters), respectively set to 1 if the wo</context>
</contexts>
<marker>Brown, Mercer, Pietra, Lai, 1992</marker>
<rawString>P.F. Brown, R.L. Mercer, V.J. Della Pietra, and J.C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>B Schölkopf</author>
<author>A Zien</author>
<author>editors</author>
</authors>
<title>Semi-Supervised Learning.</title>
<date>2006</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4376" citStr="Chapelle et al., 2006" startWordPosition="696" endWordPosition="699"> Pradhan et al., 2004; Xue and Palmer, 2004), focus21 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21–29, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 conferences1. The best system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1- measure of 81.65% on the workshop’s evaluation corpus. Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006; Zhu, 2005) for an overview), and has been applied successfully on a number of natural language processing tasks. Mann and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data. Koo et al. (2008) present an algorithm for dependency parsing that uses clusters of semantically related words, which were learned in an unsupervised manner. There has been little research on semi-supervised learning for SRL. We refer to He and Gildea (2006</context>
</contexts>
<marker>Chapelle, Schölkopf, Zien, editors, 2006</marker>
<rawString>O. Chapelle, B. Schölkopf, and A. Zien, editors. 2006. Semi-Supervised Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Chinchor</author>
</authors>
<title>Overview of MUC-7/MET-2.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7),</booktitle>
<volume>1</volume>
<contexts>
<context position="1460" citStr="Chinchor, 1998" startWordPosition="217" endWordPosition="218">ge model that learns word similarities from unlabeled texts. We use these similarities for different semi-supervised SRL methods as additional features or to automatically expand a small training set. We evaluate the methods on the PropBank dataset and find that for small training sizes our best performing system achieves an error reduction of 33.27% F1-measure compared to a state-of-the-art supervised baseline. 1 Introduction Automatic analysis of natural language is still a very hard task to perform for a computer. Although some successful applications have been developed (see for instance (Chinchor, 1998)), implementing an automatic text analysis system is still a labour and time intensive task. Many applications would benefit from an intermediate representation of texts, where an automatic analysis is already performed which is sufficiently general to be useful in a wide range of applications. Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an example of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al., 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). A syntactic parse is h</context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>N.A. Chinchor. 1998. Overview of MUC-7/MET-2. In Proceedings of the Seventh Message Understanding Conference (MUC-7), volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Cohen</author>
<author>W R Hersh</author>
</authors>
<title>A survey of current work in biomedical text mining.</title>
<date>2005</date>
<journal>Briefings in Bioinformatics,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="2036" citStr="Cohen and Hersh, 2005" startWordPosition="309" endWordPosition="312">n developed (see for instance (Chinchor, 1998)), implementing an automatic text analysis system is still a labour and time intensive task. Many applications would benefit from an intermediate representation of texts, where an automatic analysis is already performed which is sufficiently general to be useful in a wide range of applications. Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an example of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al., 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). A syntactic parse is however a representation that is very closely tied with the surface-form of natural language, in contrast to Semantic Role Labeling (SRL) which adds a layer of predicate-argument information that generalizes across different syntactic alternations (Palmer et al., 2005). SRL has received a lot of attention in the research community, and many systems have been developed (see section 2). Most of these systems rely on a large dataset for training that is manually annotated. In this paper we investigate whether we can develop a system that achieves state-of-the-art semantic r</context>
</contexts>
<marker>Cohen, Hersh, 2005</marker>
<rawString>A.M. Cohen and W.R. Hersh. 2005. A survey of current work in biomedical text mining. Briefings in Bioinformatics, 6(1):57–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>2006</date>
<publisher>Wiley-Interscience.</publisher>
<contexts>
<context position="26751" citStr="Cover and Thomas, 2006" startWordPosition="4375" endWordPosition="4378">lts (in F1-measure) on the CoNLL 2008 test set for the different methods, comparing the supervised method (Supervised) with the semi-supervised methods LWFeatures, CZusterFeatures, AutomaticExpansionCOS and AutomaticExpansionLW. See section 5 for details on the different methods. Best results are in bold. L(h6(i)). We then compute the semantic similarity measure as the Jensen-Shannon (Lin, 1997) divergence JS(L(hi)||L(h6(i))) = 1 [D(L(hi)||avg)+D(L(h6(i))||avg)� 2 where avg = (L(hi) + L(h6(i)))/2 is the average between the two distributions and D(L(hi)||avg) is the Kullback–Leiber divergence (Cover and Thomas, 2006). Although this change might appear only a slight deviation from the original model discussed in (Fürstenau and Lapata, 2009) it is potentially an important one, since an accurate semantic similarity measure will greatly influence the accuracy of the alignments, and thus of the accuracy of the automatic expansion. We call this method AutomaticExpansionLW. 6 Experiments We perform a number of experiments where we compare the fully supervised model with the semisupervised models proposed in the previous section. We first train the LWLM model on an unlabeled 5 million word Reuters corpus8. We per</context>
</contexts>
<marker>Cover, Thomas, 2006</marker>
<rawString>T.M. Cover and J.A. Thomas. 2006. Elements of Information Theory. Wiley-Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>The Latent Words Language Model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th Annual Belgian-Dutch Conference on Machine Learning.</booktitle>
<contexts>
<context position="13943" citStr="Deschacht and Moens, 2009" startWordPosition="2255" endWordPosition="2258">; McDonald and Ramscar, 2001; Grefenstette, 1994)). Different methods for computing word similarities have been proposed, differing between methods to represent the context (using dependency relationship or a window of words) and between methods that, given a set of contexts, compute the similarity between different words (ranging from cosine similarity to more complex metrics such as the Jaccard index). We refer to (Lin, 1998) for a comparison of the different similarity metrics. In the next section we propose a novel method to learn word similarities, the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). This model learns similar words and learns the a distribution over the contexts in which certain types of words occur typically. 4.2 Definition The LWLM introduces for a text T = w1...wN of length N for every observed word wi at position i a hidden variable hi. The model is a generative model for natural language, in which the latent variable hi is generated by its context C(hi) and the observed word wi is generated by the latent variable hi. In the current model we assume that the context is C(hi) = hi−1 i−2hi+2 i+1 where hi−1 i−2 = hi−2hi−1 is the two previous words and hi+2 i+1 = hi+1hi+2</context>
<context position="19845" citStr="Deschacht and Moens, 2009" startWordPosition="3265" endWordPosition="3268">ihood distribution. Gibbs sampling however samples this distribution, and thus will never reach it exactly. A number of iterations (|V |∗ 100) is then performed in which Gibbs sampling oscillates around the correct distribution. We collect independent samples of this distribution every |V |∗ 10 iterations, which are then used to construct the final model. 4.4 Evaluation of the Language Model A first evaluation of the quality of the automatically learned latent words is by translation of this model into a sequential language model and by measuring its perplexity on previously unseen texts. In (Deschacht and Moens, 2009) we perform a number of experiments, comparing different corpora (news texts from Reuters and from Associated Press, and articles from Wikipedia) and n-gram sizes (3-gram and 4-gram). We also compared the proposed model with two state-ofthe-art language models, Interpolated Kneser-Ney smoothing and fullibmpredict (Goodman, 2001), and found that LWLM outperformed both models on all corpora, with a perplexity reduction ranging between 12.40% and 5.87%. These results show that the estimated distributions over latent words are of a high quality and lead us to believe they could be used to improve </context>
</contexts>
<marker>Deschacht, Moens, 2009</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2009. The Latent Words Language Model. In Proceedings of the 18th Annual Belgian-Dutch Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>The case for case.</title>
<date>1968</date>
<booktitle>Universals in Linguistic Theory.</booktitle>
<editor>In E. Bach and R. Harms, editors,</editor>
<publisher>Rinehart &amp; Winston.</publisher>
<contexts>
<context position="5823" citStr="Fillmore (1968)" startWordPosition="932" endWordPosition="933"> a small number of VerbNet roles, which have not been used by other SRL systems. To the best of our knowledge no system was able to reproduce the successful results of (Swier and Stevenson, 2004) on the PropBank roleset. Our approach most closely resembles the work of Fürstenau and Lapata (2009) who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences. This method was tested on the FrameNet corpus and improved results when compared to a fully-supervised classifier. We will discuss their method in detail in section 5. 3 Semantic role labeling Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles). Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). An alternative approach to semantic role labeling is the framework developed 1See http://www.cnts.ua.ac.be/conll/ for an overview. by Halliday (1994) and implemented by Mehay et al. (2005). PropBank has thus far received the most attention of the research community, and is used in our work. 3.1 PropBan</context>
</contexts>
<marker>Fillmore, 1968</marker>
<rawString>C. J. Fillmore. 1968. The case for case. In E. Bach and R. Harms, editors, Universals in Linguistic Theory. Rinehart &amp; Winston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen Fürstenau</author>
<author>Mirella Lapata</author>
</authors>
<title>Semisupervised semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>220--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<contexts>
<context position="5504" citStr="Fürstenau and Lapata (2009)" startWordPosition="879" endWordPosition="882">There has been little research on semi-supervised learning for SRL. We refer to He and Gildea (2006) who tested active learning and co-training methods, but found little or no gain from semi-supervised learning, and to Swier and Stevenson (2004), who achieved good results using semi-supervised methods, but tested their methods on a small number of VerbNet roles, which have not been used by other SRL systems. To the best of our knowledge no system was able to reproduce the successful results of (Swier and Stevenson, 2004) on the PropBank roleset. Our approach most closely resembles the work of Fürstenau and Lapata (2009) who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences. This method was tested on the FrameNet corpus and improved results when compared to a fully-supervised classifier. We will discuss their method in detail in section 5. 3 Semantic role labeling Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles). Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank (Palmer</context>
<context position="22736" citStr="Fürstenau and Lapata (2009)" startWordPosition="3737" endWordPosition="3740">fferent clustering method here, the fullibmpredict method discussed in (Goodman, 2001). This method was shown to outperform the class based model proposed in (Brown et al., 1992) and can thus be expected to discover better clusters of words. We append the feature vector f(wj) with c extra values (where c is the number of clusters), respectively set to 1 if the word wi belongs to the corresponding cluster or to 0 otherwise. We call this method the ClusterFeatures method. Automatic expansion of the training set using predicate argument alignment We compare our approach with a method proposed by Fürstenau and Lapata (2009). This approach is more tailored to the specific case of SRL and is summarized here. Given a set of labeled seed verbs with annotated semantic roles, for every annotated verb a number of occurrences of this verb is found in unlabeled texts where the context is similar to the context of the annotated example. The context is defined here as all words in the sentence that are direct dependents of this verb, given the syntactic dependency tree. The similarity between two occurrences of a particular verb is measured by finding all different alignments σ : Mσ → {1...n} (Mσ ⊂ {1,...,m}) 4Probabilitie</context>
<context position="24410" citStr="Fürstenau and Lapata, 2009" startWordPosition="4022" endWordPosition="4025">ds wi and wσ(i). A is a constant weighting the importance of the syntactic similarity compared to semantic similarity, and B can be interpreted as the lowest similarity value for which an alignment between two arguments is possible. The syntactic similarity syn(gi,gσ(i)) is defined as 1 if the dependency relations are identical, 0 &lt; a &lt; 1 if the relations are of the same type but of a different subtype6 and 0 otherwise. The semantic similarity sem(wi,wσ(i)) is automatically estimated as the cosine similarity between the contexts of wi and wσ(i) in a large text corpus. For details we refer to (Fürstenau and Lapata, 2009). For every verb in the annotated training set we find the k occurrences of that verb in the unlabeled texts where the contexts are most similar given the best alignment. We then expand the training set with these examples, automatically generating an annotation using the discovered alignments. The variable k controls the trade-off between annotation confidence and expansion size. The final model is then learned by running the supervised training method on the expanded training set. We call this method AutomaticExpansionCOS7. The values for k, a, A and B are optimized automatically in every ex</context>
<context position="26876" citStr="Fürstenau and Lapata, 2009" startWordPosition="4394" endWordPosition="4397">th the semi-supervised methods LWFeatures, CZusterFeatures, AutomaticExpansionCOS and AutomaticExpansionLW. See section 5 for details on the different methods. Best results are in bold. L(h6(i)). We then compute the semantic similarity measure as the Jensen-Shannon (Lin, 1997) divergence JS(L(hi)||L(h6(i))) = 1 [D(L(hi)||avg)+D(L(h6(i))||avg)� 2 where avg = (L(hi) + L(h6(i)))/2 is the average between the two distributions and D(L(hi)||avg) is the Kullback–Leiber divergence (Cover and Thomas, 2006). Although this change might appear only a slight deviation from the original model discussed in (Fürstenau and Lapata, 2009) it is potentially an important one, since an accurate semantic similarity measure will greatly influence the accuracy of the alignments, and thus of the accuracy of the automatic expansion. We call this method AutomaticExpansionLW. 6 Experiments We perform a number of experiments where we compare the fully supervised model with the semisupervised models proposed in the previous section. We first train the LWLM model on an unlabeled 5 million word Reuters corpus8. We perform different experiments for the supervised and the four different semi-supervised methods (see previous section). Table 1 </context>
</contexts>
<marker>Fürstenau, Lapata, 2009</marker>
<rawString>Hagen Fürstenau and Mirella Lapata. 2009. Semisupervised semantic role labeling. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 220–228, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3544" citStr="Gildea and Jurafsky (2002)" startWordPosition="559" endWordPosition="562">ext, but given semantic and syntactic constraints from the context could have occurred at that particular position. In section 2 we revise existing work on SRL and on semi-supervised learning. Section 3 outlines our supervised classifier for SRL and section 4 discusses the Latent Words Language Model. In section 5 we will combine the two models for semisupervised role labeling. We will test the model on the standard PropBank dataset and compare it with state-of-the-art semi-supervised SRL systems in section 6 and finally in section 7 we draw conclusions and outline future work. 2 Related work Gildea and Jurafsky (2002) were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. This approach was soon followed by other researchers (Surdeanu et al., 2003; Pradhan et al., 2004; Xue and Palmer, 2004), focus21 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21–29, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 conferences1. The best system (Johansson and N</context>
<context position="8436" citStr="Gildea and Jurafsky, 2002" startWordPosition="1339" endWordPosition="1342">ally assume that a frame is fully expressed in a single sentence and thus do not try to instantiate roles across sentence boundaries. Although the original PropBank corpus assigned semantic roles to syntactic phrases (such as noun phrases), we use the CoNLL dataset, where the PropBank corpus was converted to a dependency representation, assigning semantic roles to single (head) words. 3.2 Features In this section we discuss the features used in the semantic role labeling system. All features but the 22 Split path feature are taken from existing semantic role labeling systems, see for example (Gildea and Jurafsky, 2002; Lim et al., 2004; Thompson et al., 2006). The number in brackets denotes the number of unique features for that type. Word We split every sentence in (unigram) word tokens, including punctuation. (37079) Stem We reduce the word tokens to their stem, e.g. “walks” -&gt; “walk”. (28690) POS The part-of-speech tag for every word, e.g. “NNP” (for a singular proper noun). (77) Neighbor POS’s The concatenated part-ofspeech tags of the word before and the word just after the current word, e.g. “RBS_JJR”. (1787) Path This important feature describes the path through the dependency tree from the current </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling, extended version.</title>
<date>2001</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="18872" citStr="Goodman, 2001" startWordPosition="3107" endWordPosition="3108">the hidden variable at that position. The probability distributions Pτ(wj|hj), Pτ(hj|hj−1 j−2) and Pτ(hj|hj+2 j+1) are constructed by collecting the counts from all positions i =6 j. The hidden variable hj is dependent on hj−2, hj−1, hj+1, hj+2 and wj and we can compute the distribution of possible values for the variable hj as Pτ(hj|wj,hj−1 0 ,hZ j+1) = Pτ(wj|hj)Pτ(hj|hj−1 j−2hj+2 j+1) ∑ hi Pτ(wi|hi)Pτ(hj|hj−1 j−2hj+2 j+1) We set P(hj|hj−1 j−2hj+2 j+1) = P(hj|hj−1 j−2) · P(hj|hj+2 j+1) which can be easily computed given the above dis3We used the interpolated Kneser-Ney model as described in (Goodman, 2001). tributions. We select a new value for the hidden variable according to Pτ(hj|wj,hj−1 0 ,hZ j+1) and place it at position j in Mτ+1 train. The current estimate for all other unobserved words remains the same. After performing this iteration a large number of times (|V |∗ 10 in this experiment), the distribution approaches the true maximum likelihood distribution. Gibbs sampling however samples this distribution, and thus will never reach it exactly. A number of iterations (|V |∗ 100) is then performed in which Gibbs sampling oscillates around the correct distribution. We collect independent s</context>
<context position="20175" citStr="Goodman, 2001" startWordPosition="3315" endWordPosition="3316">he final model. 4.4 Evaluation of the Language Model A first evaluation of the quality of the automatically learned latent words is by translation of this model into a sequential language model and by measuring its perplexity on previously unseen texts. In (Deschacht and Moens, 2009) we perform a number of experiments, comparing different corpora (news texts from Reuters and from Associated Press, and articles from Wikipedia) and n-gram sizes (3-gram and 4-gram). We also compared the proposed model with two state-ofthe-art language models, Interpolated Kneser-Ney smoothing and fullibmpredict (Goodman, 2001), and found that LWLM outperformed both models on all corpora, with a perplexity reduction ranging between 12.40% and 5.87%. These results show that the estimated distributions over latent words are of a high quality and lead us to believe they could be used to improve automatic text analysis, like SRL. 5 Role labeling using latent words The previous section discussed how the LWLM learns similar words and how these similarities improved the perplexity on an unseen text of the language model derived from this model. In this section we will see how we integrate the latent words model in two nove</context>
<context position="22195" citStr="Goodman, 2001" startWordPosition="3647" endWordPosition="3648">ost effortless in other information extraction tasks, such as Named Entity Recognition or Part-Of-Speech labeling. We compare this approach to the semisupervised method in Koo et al. (2008) who employ clusters of related words constructed by the Brown clustering algorithm (Brown et al., 1992) for syntactic processing of texts. Interestingly, this clustering algorithm has a similar objective as LWLM since it tries to optimize a class-based language model in terms of perplexity on an unseen test text. We employ a slightly different clustering method here, the fullibmpredict method discussed in (Goodman, 2001). This method was shown to outperform the class based model proposed in (Brown et al., 1992) and can thus be expected to discover better clusters of words. We append the feature vector f(wj) with c extra values (where c is the number of clusters), respectively set to 1 if the word wi belongs to the corresponding cluster or to 0 otherwise. We call this method the ClusterFeatures method. Automatic expansion of the training set using predicate argument alignment We compare our approach with a method proposed by Fürstenau and Lapata (2009). This approach is more tailored to the specific case of SR</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling, extended version. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in automatic thesaurus discovery.</title>
<date>1994</date>
<publisher>Springer.</publisher>
<contexts>
<context position="13366" citStr="Grefenstette, 1994" startWordPosition="2164" endWordPosition="2165">occurred more frequently. Furthermore, we would like to learn these similarities automatically, to be independent of knowledge sources that might not be available for all languages or domains. The Distributional Hypothesis, supported by theoretical linguists such as Harris (1954), states that words that occur in the same contexts tend to have similar meanings. This suggests that one can learn the similarity between two words automatically by comparing their relative contexts in a large unlabeled corpus, which was confirmed by different researchers (e.g. (Lin, 1998; McDonald and Ramscar, 2001; Grefenstette, 1994)). Different methods for computing word similarities have been proposed, differing between methods to represent the context (using dependency relationship or a window of words) and between methods that, given a set of contexts, compute the similarity between different words (ranging from cosine similarity to more complex metrics such as the Jaccard index). We refer to (Lin, 1998) for a comparison of the different similarity metrics. In the next section we propose a novel method to learn word similarities, the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). This model learns sim</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Explorations in automatic thesaurus discovery. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>An Introduction to Functional Grammar (second edition). Edward</title>
<date>1994</date>
<location>Arnold, London.</location>
<contexts>
<context position="6269" citStr="Halliday (1994)" startWordPosition="993" endWordPosition="994">corpus and improved results when compared to a fully-supervised classifier. We will discuss their method in detail in section 5. 3 Semantic role labeling Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles). Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). An alternative approach to semantic role labeling is the framework developed 1See http://www.cnts.ua.ac.be/conll/ for an overview. by Halliday (1994) and implemented by Mehay et al. (2005). PropBank has thus far received the most attention of the research community, and is used in our work. 3.1 PropBank The goal of the PropBank project is to add semantic information to the syntactic nodes in the English Penn Treebank. The main motivation for this annotation is the preservation of semantic roles across different syntactic realizations. Take for instance the sentences 1. The window broke. 2. John broke the window. In both sentences the constituent “the window” is broken, although it occurs at different syntactic positions. The PropBank proje</context>
</contexts>
<marker>Halliday, 1994</marker>
<rawString>M.A.K. Halliday. 1994. An Introduction to Functional Grammar (second edition). Edward Arnold, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="13027" citStr="Harris (1954)" startWordPosition="2112" endWordPosition="2113">the main source of errors was incorrect labeling of a word because the word token did not occur, or occurred only a small number of times in the training set. We hypothesize that knowledge of semantic similar words could overcome this problem by associating words that occurred infrequently in the training set to similar words that occurred more frequently. Furthermore, we would like to learn these similarities automatically, to be independent of knowledge sources that might not be available for all languages or domains. The Distributional Hypothesis, supported by theoretical linguists such as Harris (1954), states that words that occur in the same contexts tend to have similar meanings. This suggests that one can learn the similarity between two words automatically by comparing their relative contexts in a large unlabeled corpus, which was confirmed by different researchers (e.g. (Lin, 1998; McDonald and Ramscar, 2001; Grefenstette, 1994)). Different methods for computing word similarities have been proposed, differing between methods to represent the context (using dependency relationship or a window of words) and between methods that, given a set of contexts, compute the similarity between di</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S He</author>
<author>D Gildea</author>
</authors>
<title>Self-training and Cotraining for Semantic Role Labeling: Primary Report.</title>
<date>2006</date>
<tech>Technical report. TR 891.</tech>
<contexts>
<context position="4977" citStr="He and Gildea (2006)" startWordPosition="791" endWordPosition="794">apelle et al., 2006; Zhu, 2005) for an overview), and has been applied successfully on a number of natural language processing tasks. Mann and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data. Koo et al. (2008) present an algorithm for dependency parsing that uses clusters of semantically related words, which were learned in an unsupervised manner. There has been little research on semi-supervised learning for SRL. We refer to He and Gildea (2006) who tested active learning and co-training methods, but found little or no gain from semi-supervised learning, and to Swier and Stevenson (2004), who achieved good results using semi-supervised methods, but tested their methods on a small number of VerbNet roles, which have not been used by other SRL systems. To the best of our knowledge no system was able to reproduce the successful results of (Swier and Stevenson, 2004) on the PropBank roleset. Our approach most closely resembles the work of Fürstenau and Lapata (2009) who automatically expand a small training set using an automatic depende</context>
</contexts>
<marker>He, Gildea, 2006</marker>
<rawString>S. He and D. Gildea. 2006. Self-training and Cotraining for Semantic Role Labeling: Primary Report. Technical report. TR 891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with propbank and nombank.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>183--187</pages>
<location>Manchester, England,</location>
<contexts>
<context position="4156" citStr="Johansson and Nugues, 2008" startWordPosition="661" endWordPosition="664">Jurafsky (2002) were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. This approach was soon followed by other researchers (Surdeanu et al., 2003; Pradhan et al., 2004; Xue and Palmer, 2004), focus21 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21–29, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 conferences1. The best system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1- measure of 81.65% on the workshop’s evaluation corpus. Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006; Zhu, 2005) for an overview), and has been applied successfully on a number of natural language processing tasks. Mann and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data. Koo et al. (2008) present an algorith</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with propbank and nombank. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 183–187, Manchester, England, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>595--603</pages>
<contexts>
<context position="4736" citStr="Koo et al. (2008)" startWordPosition="752" endWordPosition="755">t system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1- measure of 81.65% on the workshop’s evaluation corpus. Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006; Zhu, 2005) for an overview), and has been applied successfully on a number of natural language processing tasks. Mann and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data. Koo et al. (2008) present an algorithm for dependency parsing that uses clusters of semantically related words, which were learned in an unsupervised manner. There has been little research on semi-supervised learning for SRL. We refer to He and Gildea (2006) who tested active learning and co-training methods, but found little or no gain from semi-supervised learning, and to Swier and Stevenson (2004), who achieved good results using semi-supervised methods, but tested their methods on a small number of VerbNet roles, which have not been used by other SRL systems. To the best of our knowledge no system was able</context>
<context position="21770" citStr="Koo et al. (2008)" startWordPosition="3579" endWordPosition="3582">riminative model. More specifically, we append |V |extra values to the feature vector f(wj), containing the probability distribution over the |V | possible words for the hidden variable hi4. We call this the LWFeatures method. This method has the advantage that it is simple to implement and that many existing SRL systems can be easily extended by adding additional features. We also expect that this method can be employed almost effortless in other information extraction tasks, such as Named Entity Recognition or Part-Of-Speech labeling. We compare this approach to the semisupervised method in Koo et al. (2008) who employ clusters of related words constructed by the Brown clustering algorithm (Brown et al., 1992) for syntactic processing of texts. Interestingly, this clustering algorithm has a similar objective as LWLM since it tries to optimize a class-based language model in terms of perplexity on an unseen test text. We employ a slightly different clustering method here, the fullibmpredict method discussed in (Goodman, 2001). This method was shown to outperform the class based model proposed in (Brown et al., 1992) and can thus be expected to discover better clusters of words. We append the featu</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-H Lim</author>
<author>Y-S Hwang</author>
<author>S-Y Park</author>
<author>H-C Rim</author>
</authors>
<title>Semantic role labeling using maximum entropy model.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Conference on Computational Natural Language Learning,</booktitle>
<pages>122--125</pages>
<publisher>ACL.</publisher>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="8454" citStr="Lim et al., 2004" startWordPosition="1343" endWordPosition="1346"> fully expressed in a single sentence and thus do not try to instantiate roles across sentence boundaries. Although the original PropBank corpus assigned semantic roles to syntactic phrases (such as noun phrases), we use the CoNLL dataset, where the PropBank corpus was converted to a dependency representation, assigning semantic roles to single (head) words. 3.2 Features In this section we discuss the features used in the semantic role labeling system. All features but the 22 Split path feature are taken from existing semantic role labeling systems, see for example (Gildea and Jurafsky, 2002; Lim et al., 2004; Thompson et al., 2006). The number in brackets denotes the number of unique features for that type. Word We split every sentence in (unigram) word tokens, including punctuation. (37079) Stem We reduce the word tokens to their stem, e.g. “walks” -&gt; “walk”. (28690) POS The part-of-speech tag for every word, e.g. “NNP” (for a singular proper noun). (77) Neighbor POS’s The concatenated part-ofspeech tags of the word before and the word just after the current word, e.g. “RBS_JJR”. (1787) Path This important feature describes the path through the dependency tree from the current word to the positi</context>
<context position="10124" citStr="Lim et al., 2004" startWordPosition="1619" endWordPosition="1622">v”, “Troot”, “Jdep”, “Jnmod”, “Jpmod”. Note that the split path feature includes the POS feature, since the first component of the path is the POS tag for the current word. This feature has not been used previously for semantic role detection. (155) For every word wi in the training and test set we construct the feature vector f(wi), where at every position in this vector 1 indicates the presence for the corresponding feature and 0 the absence of that feature. 3.3 Discriminative model Discriminative models have been found to outperform generative models for many different tasks including SRL (Lim et al., 2004). For this reason we also employ discriminative models here. The structure of the model was inspired by a similar Figure 1: Discriminative model for SRL. Grey circles represent observed variables, white circles hidden variables and arrows directed dependencies. s ranges over all sentences in the corpus and j over the n words in the sentence. (although generative) model in (Thompson et al., 2006) where it was used for semantic frame classification. The model (fig. 1) assumes that the role label rij for the word wi is conditioned on the features fi and on the role label ri_1j of the previous wor</context>
</contexts>
<marker>Lim, Hwang, Park, Rim, 2004</marker>
<rawString>J.-H. Lim, Y.-S. Hwang, S.-Y. Park, and H.-C. Rim. 2004. Semantic role labeling using maximum entropy model. In Proceedings of the Eighth Conference on Computational Natural Language Learning, pages 122–125, Boston, Massachusetts, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>35</volume>
<pages>64--71</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="26526" citStr="Lin, 1997" startWordPosition="4347" endWordPosition="4348">3% 74.93% 78.65% LWFeatures 60.29% 72.88% 76.42% 80.98% CZusterFeatures 59.51% 66.70% 70.15% 72.62% AutomaticExpansionCOS 47.05% 53.72% 64.51% 70.52% AutomaticExpansionLW 45.40% 53.82% 65.39% 72.66% Table 1: Results (in F1-measure) on the CoNLL 2008 test set for the different methods, comparing the supervised method (Supervised) with the semi-supervised methods LWFeatures, CZusterFeatures, AutomaticExpansionCOS and AutomaticExpansionLW. See section 5 for details on the different methods. Best results are in bold. L(h6(i)). We then compute the semantic similarity measure as the Jensen-Shannon (Lin, 1997) divergence JS(L(hi)||L(h6(i))) = 1 [D(L(hi)||avg)+D(L(h6(i))||avg)� 2 where avg = (L(hi) + L(h6(i)))/2 is the average between the two distributions and D(L(hi)||avg) is the Kullback–Leiber divergence (Cover and Thomas, 2006). Although this change might appear only a slight deviation from the original model discussed in (Fürstenau and Lapata, 2009) it is potentially an important one, since an accurate semantic similarity measure will greatly influence the accuracy of the alignments, and thus of the accuracy of the automatic expansion. We call this method AutomaticExpansionLW. 6 Experiments We </context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>D. Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, volume 35, pages 64–71. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="13317" citStr="Lin, 1998" startWordPosition="2158" endWordPosition="2159">the training set to similar words that occurred more frequently. Furthermore, we would like to learn these similarities automatically, to be independent of knowledge sources that might not be available for all languages or domains. The Distributional Hypothesis, supported by theoretical linguists such as Harris (1954), states that words that occur in the same contexts tend to have similar meanings. This suggests that one can learn the similarity between two words automatically by comparing their relative contexts in a large unlabeled corpus, which was confirmed by different researchers (e.g. (Lin, 1998; McDonald and Ramscar, 2001; Grefenstette, 1994)). Different methods for computing word similarities have been proposed, differing between methods to represent the context (using dependency relationship or a window of words) and between methods that, given a set of contexts, compute the similarity between different words (ranging from cosine similarity to more complex metrics such as the Jaccard index). We refer to (Lin, 1998) for a comparison of the different similarity metrics. In the next section we propose a novel method to learn word similarities, the Latent Words Language Model (LWLM) (</context>
<context position="31092" citStr="Lin (1998)" startWordPosition="5080" endWordPosition="5081">atent words learned by the LWLM help for this complex information extraction task. Furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features. We would like to perform experiments on employing this model in other information extraction tasks, such as Word Sense Disambiguation or Named Entity Recognition. The current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates. Lin (1998) for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples. The other semi-supervised methods proposed here were less successful, although all improved on the supervised model for small training sizes. In the future we would like to improve the described automatic expansion methods, since we feel that their full potential has not yet been reached. More specifically we plan to experiment with more advanced methods to decide whether some automatically generated examples should be added to the training set. Acknow</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational Linguistics, pages 768–774. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>A McCallum</author>
</authors>
<title>Simple, robust, scalable semi-supervised learning via expectation regularization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>593--600</pages>
<publisher>ACM Press</publisher>
<location>New York, USA.</location>
<contexts>
<context position="4515" citStr="Mann and McCallum (2007)" startWordPosition="720" endWordPosition="723">sing, pages 21–29, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 conferences1. The best system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1- measure of 81.65% on the workshop’s evaluation corpus. Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006; Zhu, 2005) for an overview), and has been applied successfully on a number of natural language processing tasks. Mann and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data. Koo et al. (2008) present an algorithm for dependency parsing that uses clusters of semantically related words, which were learned in an unsupervised manner. There has been little research on semi-supervised learning for SRL. We refer to He and Gildea (2006) who tested active learning and co-training methods, but found little or no gain from semi-supervised learning, and to Swier and Stevenson</context>
</contexts>
<marker>Mann, McCallum, 2007</marker>
<rawString>G.S. Mann and A. McCallum. 2007. Simple, robust, scalable semi-supervised learning via expectation regularization. In Proceedings of the 24th International Conference on Machine Learning, pages 593–600. ACM Press New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="1971" citStr="Marcu et al., 2006" startWordPosition="298" endWordPosition="301">for a computer. Although some successful applications have been developed (see for instance (Chinchor, 1998)), implementing an automatic text analysis system is still a labour and time intensive task. Many applications would benefit from an intermediate representation of texts, where an automatic analysis is already performed which is sufficiently general to be useful in a wide range of applications. Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an example of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al., 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). A syntactic parse is however a representation that is very closely tied with the surface-form of natural language, in contrast to Semantic Role Labeling (SRL) which adds a layer of predicate-argument information that generalizes across different syntactic alternations (Palmer et al., 2005). SRL has received a lot of attention in the research community, and many systems have been developed (see section 2). Most of these systems rely on a large dataset for training that is manually annotated. In this paper we investigate whether </context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proceedings of the Conference on Empirical Methods for Natural Language Processing, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McDonald</author>
<author>M Ramscar</author>
</authors>
<title>Testing the distributional hypothesis: The influence of context on judgements of semantic similarity.</title>
<date>2001</date>
<booktitle>In Proceedings of the 23rd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>611--616</pages>
<contexts>
<context position="13345" citStr="McDonald and Ramscar, 2001" startWordPosition="2160" endWordPosition="2163">g set to similar words that occurred more frequently. Furthermore, we would like to learn these similarities automatically, to be independent of knowledge sources that might not be available for all languages or domains. The Distributional Hypothesis, supported by theoretical linguists such as Harris (1954), states that words that occur in the same contexts tend to have similar meanings. This suggests that one can learn the similarity between two words automatically by comparing their relative contexts in a large unlabeled corpus, which was confirmed by different researchers (e.g. (Lin, 1998; McDonald and Ramscar, 2001; Grefenstette, 1994)). Different methods for computing word similarities have been proposed, differing between methods to represent the context (using dependency relationship or a window of words) and between methods that, given a set of contexts, compute the similarity between different words (ranging from cosine similarity to more complex metrics such as the Jaccard index). We refer to (Lin, 1998) for a comparison of the different similarity metrics. In the next section we propose a novel method to learn word similarities, the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). </context>
</contexts>
<marker>McDonald, Ramscar, 2001</marker>
<rawString>S. McDonald and M. Ramscar. 2001. Testing the distributional hypothesis: The influence of context on judgements of semantic similarity. In Proceedings of the 23rd Annual Conference of the Cognitive Science Society, pages 611–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Mehay</author>
<author>Rik De Busser</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Labeling generic semantic roles.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Workshop on Computational Semantics.</booktitle>
<marker>Mehay, De Busser, Moens, 2005</marker>
<rawString>Dennis Mehay, Rik De Busser, and Marie-Francine Moens. 2005. Labeling generic semantic roles. In Proceedings of the Sixth International Workshop on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>MaltParser: A datadriven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="25631" citStr="Nivre et al., 2006" startWordPosition="4221" endWordPosition="4224">riment on a held-out set (disjoint from both training and test set). We adapt this approach by employing a different method for measuring semantic similarity. Given two words wi and wσ(i) we estimate the distribution of latent words, respectively L(hi) and 5Note that this is a syntactic role, not a semantic role as the ones discussed in this article. 6Subtypes are fine-grained distinctions made by the parser such as the underlying grammatical roles in passive constructions. 7The only major differences with (Fürstenau and Lapata, 2009) are the dependency parser which was used (the MALT parser (Nivre et al., 2006) instead of the RASP parser (Briscoe et al., 2006)) and the corpus employed to learn semantic similarities (the Reuters corpus instead of the British National Corpus). We expect that these differences will only influence the results minimally. 26 5% 20% 50% 100% Supervised 40.49% 67.23% 74.93% 78.65% LWFeatures 60.29% 72.88% 76.42% 80.98% CZusterFeatures 59.51% 66.70% 70.15% 72.62% AutomaticExpansionCOS 47.05% 53.72% 64.51% 70.52% AutomaticExpansionLW 45.40% 53.82% 65.39% 72.66% Table 1: Results (in F1-measure) on the CoNLL 2008 test set for the different methods, comparing the supervised meth</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A datadriven parser-generator for dependency parsing. In Proceedings of the Fifth International Conference on Language Resources and Evaluation, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2328" citStr="Palmer et al., 2005" startWordPosition="352" endWordPosition="355">l to be useful in a wide range of applications. Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an example of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al., 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). A syntactic parse is however a representation that is very closely tied with the surface-form of natural language, in contrast to Semantic Role Labeling (SRL) which adds a layer of predicate-argument information that generalizes across different syntactic alternations (Palmer et al., 2005). SRL has received a lot of attention in the research community, and many systems have been developed (see section 2). Most of these systems rely on a large dataset for training that is manually annotated. In this paper we investigate whether we can develop a system that achieves state-of-the-art semantic role labeling without relying on a large number of labeled examples. We aim to do so by employing the Latent Words Language Model that learns latent words from a large unlabeled corpus. Latent words are words that (unlike observed words) did not occur at a particular position in a text, but g</context>
<context position="6118" citStr="Palmer et al., 2005" startWordPosition="972" endWordPosition="975">(2009) who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences. This method was tested on the FrameNet corpus and improved results when compared to a fully-supervised classifier. We will discuss their method in detail in section 5. 3 Semantic role labeling Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles). Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). An alternative approach to semantic role labeling is the framework developed 1See http://www.cnts.ua.ac.be/conll/ for an overview. by Halliday (1994) and implemented by Mehay et al. (2005). PropBank has thus far received the most attention of the research community, and is used in our work. 3.1 PropBank The goal of the PropBank project is to add semantic information to the syntactic nodes in the English Penn Treebank. The main motivation for this annotation is the preservation of semantic roles across different syntactic realizations. Take for instance the sentences 1. The window broke. 2. J</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>K Hacioglu</author>
<author>J Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference/North American chapter of the Association of Computational Linguistics,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="3776" citStr="Pradhan et al., 2004" startWordPosition="598" endWordPosition="602">for SRL and section 4 discusses the Latent Words Language Model. In section 5 we will combine the two models for semisupervised role labeling. We will test the model on the standard PropBank dataset and compare it with state-of-the-art semi-supervised SRL systems in section 6 and finally in section 7 we draw conclusions and outline future work. 2 Related work Gildea and Jurafsky (2002) were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. This approach was soon followed by other researchers (Surdeanu et al., 2003; Pradhan et al., 2004; Xue and Palmer, 2004), focus21 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21–29, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 conferences1. The best system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1- measure of 81.65% on the workshop’s evaluation corpus. Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings of the Human Language Technology Conference/North American chapter of the Association of Computational Linguistics, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10945" citStr="Ratnaparkhi, 1996" startWordPosition="1766" endWordPosition="1767">circles hidden variables and arrows directed dependencies. s ranges over all sentences in the corpus and j over the n words in the sentence. (although generative) model in (Thompson et al., 2006) where it was used for semantic frame classification. The model (fig. 1) assumes that the role label rij for the word wi is conditioned on the features fi and on the role label ri_1j of the previous word and that the predicate label pj for word wj is conditioned on the role labels Rj and on the features fj. This model can be seen as an extension of the standard Maximum Entropy Markov Model (MEMM, see (Ratnaparkhi, 1996)) with an extra dependency on the predicate label, we will henceforth refer to this model as MEMM+pred. To estimate the parameters of the MEMM+pred model we turn to the successful Maximum Entropy (Berger et al., 1996) parameter estimation method. The Maximum Entropy principle states that the best model given the training data is the model such that the conditional distribution defined by the model has maximum entropy subject to the constraints represented by the training examples. There is no closed form solution to find this maximum and we thus turn to an iterative method. In this work we use</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133–142. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>S Harabagiu</author>
<author>J Williams</author>
<author>P Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="3754" citStr="Surdeanu et al., 2003" startWordPosition="594" endWordPosition="597"> supervised classifier for SRL and section 4 discusses the Latent Words Language Model. In section 5 we will combine the two models for semisupervised role labeling. We will test the model on the standard PropBank dataset and compare it with state-of-the-art semi-supervised SRL systems in section 6 and finally in section 7 we draw conclusions and outline future work. 2 Related work Gildea and Jurafsky (2002) were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. This approach was soon followed by other researchers (Surdeanu et al., 2003; Pradhan et al., 2004; Xue and Palmer, 2004), focus21 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21–29, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 conferences1. The best system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1- measure of 81.65% on the workshop’s evaluation corpus. Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see </context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Swier</author>
<author>S Stevenson</author>
</authors>
<title>Unsupervised semantic role labelling.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="5122" citStr="Swier and Stevenson (2004)" startWordPosition="814" endWordPosition="817">and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data. Koo et al. (2008) present an algorithm for dependency parsing that uses clusters of semantically related words, which were learned in an unsupervised manner. There has been little research on semi-supervised learning for SRL. We refer to He and Gildea (2006) who tested active learning and co-training methods, but found little or no gain from semi-supervised learning, and to Swier and Stevenson (2004), who achieved good results using semi-supervised methods, but tested their methods on a small number of VerbNet roles, which have not been used by other SRL systems. To the best of our knowledge no system was able to reproduce the successful results of (Swier and Stevenson, 2004) on the PropBank roleset. Our approach most closely resembles the work of Fürstenau and Lapata (2009) who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences. This method was tested on the FrameNet corpus and improved results when compared to a fully-supervised clas</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>R.S. Swier and S. Stevenson. 2004. Unsupervised semantic role labelling. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 95–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Thompson</author>
<author>R Levy</author>
<author>C Manning</author>
</authors>
<title>A generative model for FrameNet semantic role labeling .</title>
<date>2006</date>
<booktitle>In Proceedings of the 14th European Conference on Machine Learning,</booktitle>
<location>Cavtat-Dubrovnik, Croatia.</location>
<contexts>
<context position="8478" citStr="Thompson et al., 2006" startWordPosition="1347" endWordPosition="1350">n a single sentence and thus do not try to instantiate roles across sentence boundaries. Although the original PropBank corpus assigned semantic roles to syntactic phrases (such as noun phrases), we use the CoNLL dataset, where the PropBank corpus was converted to a dependency representation, assigning semantic roles to single (head) words. 3.2 Features In this section we discuss the features used in the semantic role labeling system. All features but the 22 Split path feature are taken from existing semantic role labeling systems, see for example (Gildea and Jurafsky, 2002; Lim et al., 2004; Thompson et al., 2006). The number in brackets denotes the number of unique features for that type. Word We split every sentence in (unigram) word tokens, including punctuation. (37079) Stem We reduce the word tokens to their stem, e.g. “walks” -&gt; “walk”. (28690) POS The part-of-speech tag for every word, e.g. “NNP” (for a singular proper noun). (77) Neighbor POS’s The concatenated part-ofspeech tags of the word before and the word just after the current word, e.g. “RBS_JJR”. (1787) Path This important feature describes the path through the dependency tree from the current word to the position of the predicate, e.g</context>
<context position="10522" citStr="Thompson et al., 2006" startWordPosition="1683" endWordPosition="1686">esence for the corresponding feature and 0 the absence of that feature. 3.3 Discriminative model Discriminative models have been found to outperform generative models for many different tasks including SRL (Lim et al., 2004). For this reason we also employ discriminative models here. The structure of the model was inspired by a similar Figure 1: Discriminative model for SRL. Grey circles represent observed variables, white circles hidden variables and arrows directed dependencies. s ranges over all sentences in the corpus and j over the n words in the sentence. (although generative) model in (Thompson et al., 2006) where it was used for semantic frame classification. The model (fig. 1) assumes that the role label rij for the word wi is conditioned on the features fi and on the role label ri_1j of the previous word and that the predicate label pj for word wj is conditioned on the role labels Rj and on the features fj. This model can be seen as an extension of the standard Maximum Entropy Markov Model (MEMM, see (Ratnaparkhi, 1996)) with an extra dependency on the predicate label, we will henceforth refer to this model as MEMM+pred. To estimate the parameters of the MEMM+pred model we turn to the successf</context>
</contexts>
<marker>Thompson, Levy, Manning, 2006</marker>
<rawString>C. Thompson, R. Levy, and C. Manning. 2006. A generative model for FrameNet semantic role labeling . In Proceedings of the 14th European Conference on Machine Learning, Cavtat-Dubrovnik, Croatia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>4</volume>
<contexts>
<context position="3799" citStr="Xue and Palmer, 2004" startWordPosition="603" endWordPosition="606">discusses the Latent Words Language Model. In section 5 we will combine the two models for semisupervised role labeling. We will test the model on the standard PropBank dataset and compare it with state-of-the-art semi-supervised SRL systems in section 6 and finally in section 7 we draw conclusions and outline future work. 2 Related work Gildea and Jurafsky (2002) were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. This approach was soon followed by other researchers (Surdeanu et al., 2003; Pradhan et al., 2004; Xue and Palmer, 2004), focus21 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21–29, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 conferences1. The best system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1- measure of 81.65% on the workshop’s evaluation corpus. Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006; Zhu, 2005) for an ove</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>N. Xue and M. Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2005</date>
<tech>Technical Report 1530,</tech>
<institution>Computer Sciences, University of Wisconsin-Madison.</institution>
<contexts>
<context position="4388" citStr="Zhu, 2005" startWordPosition="700" endWordPosition="701">Xue and Palmer, 2004), focus21 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21–29, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 conferences1. The best system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1- measure of 81.65% on the workshop’s evaluation corpus. Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006; Zhu, 2005) for an overview), and has been applied successfully on a number of natural language processing tasks. Mann and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data. Koo et al. (2008) present an algorithm for dependency parsing that uses clusters of semantically related words, which were learned in an unsupervised manner. There has been little research on semi-supervised learning for SRL. We refer to He and Gildea (2006) who tested</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>X. Zhu. 2005. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>