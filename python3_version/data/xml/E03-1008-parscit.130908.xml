<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.990828">
Bootstrapping Statistical Parsers from Small Datasets
</title>
<author confidence="0.998373">
Mark Steedman*, Miles Osborne*, Anoop Sarkar% Stephen Clark*, Rebecca Hwa.
Julia Hockenmaier*, Paul Ruhlent, Steven BakerI and Jeremiah Crimt
</author>
<affiliation confidence="0.8750042">
*Division of Informatics, University of Edinburgh
fsteedman,stephenc,julia,osbornel@cogsci.ed.ac.uk
&apos;F School of Computing Science, Simon Fraser University anoop@cs . sfu . ca
Institute for Advanced Computer Studies, University of Maryland hwa@umia c s . umd . edu
tCenter for Language and Speech Processing, Johns Hopkins University
</affiliation>
<email confidence="0.686481">
jcrim@jhu.edu,ruhlen@cs.jhu.edu
</email>
<affiliation confidence="0.981207">
IDepartment of Computer Science, Cornell University sdb2 2 @cornell .edu
</affiliation>
<sectionHeader confidence="0.994732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999735625">
We present a practical co-training
method for bootstrapping statistical
parsers using a small amount of manu-
ally parsed training material and a much
larger pool of raw sentences. Experi-
mental results show that unlabelled sen-
tences can be used to improve the per-
formance of statistical parsers. In addi-
tion, we consider the problem of boot-
strapping parsers when the manually
parsed training material is in a differ-
ent domain to either the raw sentences or
the testing material. We show that boot-
strapping continues to be useful, even
though no manually produced parses
from the target domain are used.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999668948717949">
In this paper we describe how co-training (Blum
and Mitchell, 1998) can be used to boot-
strap a pair of statistical parsers from a small
amount of annotated training data. Co-training
is a wealdy supervised learning algorithm in
which two (or more) learners are iteratively re-
trained on each other&apos;s output. It has been ap-
plied to problems such as word-sense disam-
biguation (Yarowsky, 1995), web-page classifica-
tion (Blum and Mitchell, 1998) and named-entity
recognition (Collins and Singer, 1999). However,
these tasks typically involved a small set of la-
bels (around 2-3) and a relatively small param-
eter space. It is therefore instructive to consider
co-training for more complex models. Compared
to these earlier models, a statistical parser has a
larger parameter space, and instead of class labels,
it produces recursively built parse trees as output.
Previous work in co-training statistical
parsers (Sarkar, 2001) used two components of
a single parsing framework (that is, a parser and
a supertagger for that parser). In contrast, this
paper considers co-training two diverse statistical
parsers: the Collins lexicalized PCFG parser and
a Lexicalized Tree Adjoining Grammar (LTAG)
parser.
Section 2 reviews co-training theory. Section 3
considers how co-training applied to training sta-
tistical parsers can be made computationally vi-
able. In Section 4 we show that co-training out-
performs self-training, and that co-training is most
beneficial when the seed set of manually created
parses is small. Section 4.4 shows that co-training
is possible even when the set of initially labelled
data is drawn from a different distribution to either
the unlabelled training material or the test set; that
is, we show that co-training can help in porting a
parser from one genre to another. Finally, section 5
reports summary results of our experiments.
</bodyText>
<sectionHeader confidence="0.961797" genericHeader="method">
2 Co-training: theory
</sectionHeader>
<bodyText confidence="0.631761">
Co-training can be informally described in the fol-
lowing manner (Blum and Mitchell, 1998):
</bodyText>
<page confidence="0.99548">
331
</page>
<listItem confidence="0.981885933333333">
• Pick two (or more) &amp;quot;views&amp;quot; of a classification
problem.
• Build separate models for each of these
&amp;quot;views&amp;quot; and train each model on a small set
of labelled data.
• Sample from an unlabelled data set to find
examples for each model to label indepen-
dently (Nigam and Ghani, 2000).
• Those examples labelled with high confi-
dence are selected to be new training exam-
ples (Collins and Singer, 1999; Goldman and
Zhou, 2000).
• The models are re-trained on the updated
training examples, and the procedure is iter-
ated until the unlabelled data is exhausted.
</listItem>
<bodyText confidence="0.999618307692308">
Effectively, by picking confidently labelled data
from each model to add to the training data, one
model is labelling data for the other. This is in
contrast to self-training, in which a model is re-
trained only on the labelled examples that it pro-
duces (Nigam and Ghani, 2000).
Blum and Mitchell prove that, when the
two views are conditionally independent given
the label, and each view is sufficient for
learning the task, co-training can improve
an initial weak learner using unlabelled data.
Dasgupta et al. (2002) extend the theory of co-
training by showing that, by maximising their
agreement over the unlabelled data, the two learn-
ers make few generalisation errors (under the same
independence assumption adopted by Blum and
Mitchell). Abney (2002) argues that this assump-
tion is extremely restrictive and typically violated
in the data, and he proposes a weaker indepen-
dence assumption. Abney also presents a greedy
algorithm that maximises agreement on unlabelled
data.
Goldman and Zhou (2000) show that, through
careful selection of newly labelled examples, co-
training can work even when the classifiers&apos; views
do not fully satisfy the independence assumption.
</bodyText>
<sectionHeader confidence="0.995978" genericHeader="method">
3 Co-training: practice
</sectionHeader>
<bodyText confidence="0.999981956521739">
To apply the theory of co-training to parsing, we
need to ensure that each parser is capable of learn-
ing the parsing task alone and that the two parsers
have different views. We could also attempt to
maximise the agreement of the two parsers over
unlabelled data, using a similar approach to that
given by Abney. This would be computation-
ally very expensive for parsers, however, and we
therefore propose some practical heuristics for de-
termining which labelled examples to add to the
training set for each parser.
Our approach is to decompose the problem into
two steps. First, each parser assigns a score for
every unlabelled sentence it parsed according to
some scoring function, f, estimating the reliabil-
ity of the label it assigned to the sentence (e.g.
the probability of the parse). Note that the scor-
ing functions used by the two parsers do not nec-
essarily have to be the same. Next, a selection
method decides which parser is retrained upon
which newly parsed sentences. Both scoring and
selection phases are controlled by a simple incre-
mental algorithm, which is detailed in section 3.2.
</bodyText>
<subsectionHeader confidence="0.999814">
3.1 Scoring functions and selection methods
</subsectionHeader>
<bodyText confidence="0.99959125">
An ideal scoring function would tell us the true ac-
curacy rates (e.g., combined labelled precision and
recall scores) of the trees that the parser produced.
In practice, we rely on computable scoring func-
tions that approximate the true accuracy scores,
such as measures of uncertainty. In this paper we
use the probability of the most likely parse as the
scoring function:
</bodyText>
<equation confidence="0.844516">
fprob(w) = max Pr (v,w) (1)
vcv
</equation>
<bodyText confidence="0.999844571428572">
where w is the sentence and V is the set of parses
produced by the parser for the sentence. Scor-
ing parses using parse probability is motivated by
the idea that parse probability should increase with
parse correctness.
During the selection phase, we pick a subset of
the newly labelled sentences to add to the training
sets of both parsers. That is, a subset of those sen-
tences labelled by the LTAG parser is added to the
training set of the Collins PCFG parser, and vice
versa. It is important to find examples that are re-
liably labelled by the teacher as training data for
the student. The term teacher refers to the parser
providing data, and student to the parser receiving
</bodyText>
<page confidence="0.996178">
332
</page>
<bodyText confidence="0.780174428571428">
A and B are two different parsers.
MA and ivriB are models of A and B at step i.
U is a large pool of unlabelled sentences.
Ui is a small cache holding subset of U at step i.
L is the manually labelled seed data.
L&apos;A and LiB are the labelled training examples
for A and B at step i.
</bodyText>
<figure confidence="0.708427142857143">
Initialise:
L°A &lt;— L°B L.
Mi°1 Train(A, LcA&apos; )
4 Train(B , L°B)
Loop:
U Add unlabeled sentences from U.
MiA and M parse the sentences in Ui
</figure>
<bodyText confidence="0.976758142857143">
and assign scores to them according to
their scoring functions JA and fB.
Select new parses {PA} and {PB}
according to some selection method S,
which uses the scores from fA and fB.
LiA+1- is LiA augmented with {PB}
L1- is LiB augmented with {PA}
</bodyText>
<equation confidence="0.8596775">
Train(A, L&apos;A+1)
Train(B , 4F1)
</equation>
<figureCaption confidence="0.9294365">
Figure 1: The pseudo-code for the co-training al-
gorithm
</figureCaption>
<bodyText confidence="0.9825762">
data. In the co-training process the two parsers
alternate between teacher and student. We use a
method which builds on this idea, Stop-n, which
chooses those sentences (using the teacher&apos;s la-
bels) that belong to the teacher&apos;s n-highest scored
sentences.
For this paper we have used a simple scoring
function and selection method, but there are alter-
natives. Other possible scoring functions include a
normalized version of fprob which does not penal-
ize longer sentences, and a scoring function based
on the entropy of the probability distribution over
all parses returned by the parser. Other possible
selection methods include selecting examples that
one parser scored highly and another parser scored
lowly, and methods based on disagreements on
the label between the two parsers. These meth-
ods build on the idea that the newly labelled data
should not only be reliably labelled by the teacher,
but also be as useful as possible for the student.
</bodyText>
<subsectionHeader confidence="0.904683">
3.2 Co-training algorithm
</subsectionHeader>
<bodyText confidence="0.999916111111111">
The pseudo-code for the co-training process is
given in Figure 1, and consists of two different
parsers and a central control that interfaces be-
tween the two parsers and the data. At each
co-training iteration, a small set of sentences is
drawn from a large pool of unlabelled sentences
and stored in a cache. Both parsers then attempt
to parse every sentence in the cache. Next, a sub-
set of the sentences newly labelled by one parser is
added to the training data of the other parser, and
vice versa.
The general control flow of our system is similar
to the algorithm described by Blum and Mitchell;
however, there are some differences in our treat-
ment of the training data. First, the cache is
flushed at each iteration: instead of only replac-
ing just those sentences moved from the cache, the
entire cache is refilled with new sentences. This
aims to ensure that the distribution of sentences in
the cache is representative of the entire pool and
also reduces the possibility of forcing the central
control to select training examples from an entire
set of unreliably labelled sentences. Second, we
do not require the two parsers to have the same
training sets. This allows us to explore several se-
lection schemes in addition to the one proposed by
Blum and Mitchell.
</bodyText>
<sectionHeader confidence="0.999124" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999854">
In order to conduct co-training experiments be-
tween statistical parsers, it was necessary to
choose two parsers that generate comparable out-
put but use different statistical models. We there-
fore chose the following parsers:
</bodyText>
<sectionHeader confidence="0.575648" genericHeader="method">
1. The Collins lexicalized PCFG
</sectionHeader>
<bodyText confidence="0.85872875">
parser (Collins, 1999), model 2. Some
code for (re)training this parser was added to
make the co-training experiments possible.
We refer to this parser as Collins-CFG.
</bodyText>
<listItem confidence="0.844115666666667">
2. The Lexicalized Tree Adjoining Grammar
(LTAG) parser of Sarkar (2001), which we
refer to as the LTAG parser.
</listItem>
<bodyText confidence="0.9863175">
In order to perform the co-training experiments
reported in this paper, LTAG derivation events
</bodyText>
<page confidence="0.97199">
333
</page>
<bodyText confidence="0.957103">
Collins-CFG LTAG
Bi-lexical dependencies are between Bi-lexical dependencies are between
lexicalized nonterminals elementary trees
Can produce novel elementary Can produce novel hi-lexical
trees for the LTAG parser dependencies for Collins-CFG
When using small amounts of seed data, When using small amounts of seed data,
abstains less often than LTAG abstains more often than Collins-CFG
</bodyText>
<figureCaption confidence="0.99819">
Figure 2: Summary of the different views given by the Collins-CFG parser and the LEAG parser
</figureCaption>
<bodyText confidence="0.999962617647059">
were extracted from the head-lexicalized parse
tree output produced by the Collins-CFG parser.
These events were used to retrain the statistical
model used in the LTAG parser. The output of the
LTAG parser was also modified in order to provide
input for the re-training phase in the Collins-CFG
parser. These steps ensured that the output of the
Collins-CFG parser could be used as new labelled
data to re-train the LTAG parser and vice versa.
The domains over which the two models op-
erate are quite distinct. The LTAG model uses
tree fragments of the final parse tree and com-
bines them together, while the Collins-CFG model
operates on a much smaller domain of individual
lexicalized non-terminals. This provides a mech-
anism to bootstrap information between these two
models when they are applied to unlabelled data.
LTAG can provide a larger domain over which
hi-lexical information is defined due to the arbi-
trary depth of the elementary trees it uses, and
hence can provide novel lexical relationships for
the Collins-CFG model, while the Collins-CFG
model can paste together novel elementary trees
for the LTAG model.
A summary of the differences between the two
models is given in Figure 2, which provides an in-
formal argument for why the two parsers provide
contrastive views for the co-training experiments.
Of course there is still the question of whether the
two parsers really are independent enough for ef-
fective co-training to be possible; in the results
section we show that the Collins-CFG parser is
able to learn useful information from the output
of the LTAG parser.
</bodyText>
<figure confidence="0.976642454545455">
Collins-CFG Learning Curve
90
88
86
84
82
80
78
76
100 5000 10000 15000 20000 25000 30000 35000 40000
Number of Sentences
</figure>
<figureCaption confidence="0.775717">
Figure 3: The learning curve for the Collins-CFG
parser in terms of F-scores for increasing amounts
of manually annotated training data. Performance
for sentences &lt; 40 words is plotted.
</figureCaption>
<subsectionHeader confidence="0.967563">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999887611111111">
Figure 3 shows how the performance of the
Collins-CFG parser varies as the amount of man-
ually annotated training data (from the Wall Street
Journal (WSJ) Penn Treebank (Marcus et al.,
1993)) is increased. The graph shows a rapid
growth in accuracy which tails off as increasing
amounts of training data are added. The learn-
ing curve shows that the maximum payoff from
co-training is likely to occur between 500 and
1,000 sentences. Therefore we used two sizes of
seed data: 500 and 1,000 sentences, to see if co-
training could improve parser performance using
these small amounts of labelled seed data. For
reference, Figure 4 shows a similar curve for the
LTAG parser.
Each parser was first initialized with some la-
belled seed data from the standard training split
(sections 2 to 21) of the WSJ Penn Treebank.
</bodyText>
<page confidence="0.991018">
334
</page>
<figure confidence="0.9849538">
LTAG Learning Curve
89 LNAG (&lt; 40micfsT•
88
87
ci 86
85
84
83
0 5000 10000 15000 20000 25000 30000 35000 40000
Number of Sentences
</figure>
<figureCaption confidence="0.8784365">
Figure 4: The learning curve for the LTAG parser
in terms of F-scores for increasing amounts of
training data. Performance when evaluated on sen-
tences of length &lt; 40 words is plotted.
</figureCaption>
<bodyText confidence="0.999918454545455">
Evaluation was in terms of Parseval (Black et al.,
1991), using a balanced F-score over labelled con-
stituents from section 0 of the Treebank. I The F-
score values are reported for each iteration of co-
training on the development set (section 0 of the
Treebank). Since we need to parse all sentences in
section 0 at each iteration, in the experiments re-
ported in this paper we only evaluated one of the
parsers, the Collins-CFG parser, at each iteration.
All results we mention (unless stated otherwise)
are F-scores for the Collins-CFG parser.
</bodyText>
<subsectionHeader confidence="0.992202">
4.2 Self-training experiments
</subsectionHeader>
<bodyText confidence="0.9999642">
Self-training experiments were conducted in
which each parser was retrained on its own out-
put. Self-training provides a useful comparison
with co-training because any difference in the re-
sults indicates how much the parsers are benefit-
ing from being trained on the output of another
parser. This experiment also gives us some insight
into the differences between the two parsing mod-
els. Self-training was used by Charniak (1997),
where a modest gain was reported after re-training
his parser on 30 million words.
The results are shown in Figure 5. Here, both
parsers were initialised with the first 500 sentences
from the standard training split (sections 2 to 21)
of the WSJ Penn Treebank. Subsequent unlabelled
</bodyText>
<footnote confidence="0.6904595">
2xLRxLP
1F-score = where LP is labelled precision and
</footnote>
<figure confidence="0.798972">
L,R+LP
LR is labelled recall.
Self-training results
10 20 30 40 50 60 70 80 90 100
Co-training rounds
</figure>
<figureCaption confidence="0.974728">
Figure 5: Self-training results for LTAG and
Collins-CFG. The upper curve is for Collins-CFG;
the lower curve is for LTAG.
</figureCaption>
<bodyText confidence="0.984401357142857">
sentences were also drawn from this split. Dur-
ing each round of self-training, 30 sentences were
parsed by each parser, and each parser was re-
trained upon the 20 self-labelled sentences which
it scored most highly (each parser using its own
joint probability (equation 1) as the score).
The results vary significantly between the
Collins-CFG and the LTAG parser, which lends
weight to the argument that the two parsers are
largely independent of each other. It also shows
that, at least for the Collins-CFG model, a minor
improvement in performance can be had from self-
training. The LTAG parser, by contrast, is hurt by
self-training
</bodyText>
<subsectionHeader confidence="0.99828">
4.3 Co-training experiments
</subsectionHeader>
<bodyText confidence="0.977067">
The first co-training experiment used the first 500
sentences from sections 2-21 of the Treebank as
seed data, and subsequent unlabelled sentences
were drawn from the remainder of these sections.
During each co-training round, the LTAG parser
parsed 30 sentences, and the 20 labelled sentences
with the highest scores (according to the LTAG
joint probability) were added to the training data
of the Collins-CFG parser. The training data of
the LTAG parser was augmented in the same way,
using the 20 highest scoring parses from the set
of 30, but using the Collins-CFG parser to label
the sentences and provide the joint probability for
scoring.
Figure 6 gives the results for the Collins-CFG
parser, and also shows the self-training curve for
</bodyText>
<figure confidence="0.96887075">
d,92 761
76
75.5
75
742
74
73.5
73
72.5
72
71.5
335
Co training versus self-training
Cross-genre co-training
20 30 40 50 60 70 BO 90 100
Co-training rounds
</figure>
<figureCaption confidence="0.998803">
Figure 6: Co-training compared with self-training.
</figureCaption>
<bodyText confidence="0.855115333333333">
The upper curve is for co-training between
Collins-CFG and LTAG; the lower curve is self-
training for Collins-CFG.
</bodyText>
<note confidence="0.42885">
The effect of seed size
</note>
<figureCaption confidence="0.836982">
Figure 7: The effect of varying seed size on CO-
training. The upper curve is for 1,000 sentences
labelled seed data; the lower curve is for 500 sen-
tences.
</figureCaption>
<bodyText confidence="0.999885125">
comparison.2 The graph shows that co-training
results in higher performance than self-training.
The graph also shows that co-training perfor-
mance levels out after around 80 rounds, and
then starts to degrade. The likely reason for
this dip is noise in the parse trees added by co-
training. Pierce and Cardie (2001) noted a similar
behaviour when they co-trained shallow parsers.
</bodyText>
<footnote confidence="0.985747833333333">
2Figures 6, 7 and 8 report the performance of the Collins-
CFG parser. We do not report the LTAG parser performance
in this paper as evaluating it at the end of each co-training
round was too time consuming. We did track LTAG perfor-
mance on a subset of the WSJ Section 0 and can confirm that
LTAG performance also improves as a result of co-training.
</footnote>
<figure confidence="0.9289075">
10 20 30 40 50 60 70 BO 90 100
Co-training rounds
</figure>
<figureCaption confidence="0.986318">
Figure 8: Cross-genre bootstrapping results. The
</figureCaption>
<bodyText confidence="0.989250931034483">
upper curve is for 1,000 sentences labelled data
from Brown plus 100 WSJ sentences; the lower
curve only uses 1,000 sentences from Brown.
The second co-training experiment was the
same as the first, except that more seed data was
used: the first 1,000 sentences from sections 2-21
of the Treebank. Figure 7 gives the results, and,
for comparison, also shows the previous perfor-
mance curve for the 500 seed set experiment. The
key observation is that the benefit of co-training is
greater when the amount of seed material is small.
Our hypothesis is that, when there is a paucity of
initial seed data, coverage is a major obstacle that
co-training can address. As the amount of seed
data increases, coverage becomes less of a prob-
lem, and the co-training advantage is diminished.
This means that, when most sentences in the test-
ing set can be parsed, subsequent changes in per-
formance come from better parameter estimates.
Although co-training boosts the performance of
the parser using the 500 seed sentences from 75%
to 77.8% (the performance level after 100 rounds
of co-training), it does not achieve the level of
performance of a parser trained on 1,000 seed
sentences. Some possible explanations are: that
the newly labelled sentences are not reliable (i.e.,
they contain too many errors); that the sentences
deemed reliable are not informative training exam-
ples; or a combination of both factors.
</bodyText>
<subsectionHeader confidence="0.997765">
4.4 Cross genre experiments
</subsectionHeader>
<bodyText confidence="0.989679">
This experiment examines whether co-training
can be used to boost performance when the un-
</bodyText>
<figure confidence="0.9992794">
20 30 40 50 60 70 80 90 100
Co-training rounds
80
79.5
79
78.5
78
77.5
77
76.5
76
75.5
75
74.5
0
</figure>
<page confidence="0.998135">
336
</page>
<bodyText confidence="0.999956578947368">
labelled data are taken from a different source
than the initial seed data. Previous experiments
in Gildea (2001) have shown that porting a statis-
tical parser from a source genre to a target genre is
a non-trivial task. Our two different sources were
the parsed section of the Brown corpus and the
Penn Treebank WSJ. Unlike the WSJ, the Brown
corpus does not contain newswire material, and so
the two sources differ from each other in terms of
vocabulary and syntactic constructs.
1,000 annotated sentences from the Brown sec-
tion of the Penn Treebank were used as the seed
data. Co-training then proceeds using the WSJ.3
Note that no manually created parses in the WSJ
domain are used by the parser, even though it is
evaluated using WSJ material. In Figure 8, the
lower curve shows performance for the Collins-
CFG parser (again evaluated on section 0). The
difference in corpus domain does not hinder co-
training. The parser performance is boosted from
75% to 77.3%. Note that most of the improvement
is within the first 5 iterations. This suggests that
the parsing model may be adapting to the vocabu-
lary of the new domain.
We also conducted an experiment in which the
initial seed data was supplemented with a tiny
amount of annotated data (100 manually anno-
tated WSJ sentences) from the domain of the un-
labelled data. This experiment simulates the situ-
ation where there is only a very limited amount of
labelled material in the novel domain. The upper
curve in Figure 8 shows the outcome of this ex-
periment. Not surprisingly, the 100 additional la-
belled WSJ sentences improved the initial perfor-
mance of the parser (to 76.7%). While the amount
of improvement in performance is less than the
previous case, co-training provides an additional
boost to the parsing performance, to 78.7%.
</bodyText>
<sectionHeader confidence="0.997074" genericHeader="method">
5 Experimental summary
</sectionHeader>
<bodyText confidence="0.999627833333333">
The various experiments are summarised in Ta-
ble 1. As is customary in the statistical parsing
literature, we view all our previous experiments
using section 0 of the Penn Treebank WSJ as con-
tributing towards development. Here we report on
system performance on unseen material (namely
</bodyText>
<footnote confidence="0.9940205">
3The Brown corpus was chosen as the seed data and the
WSJ as the unlabelled data for convenience.
</footnote>
<table confidence="0.999698666666667">
Experiment Before After
WSJ Self-training 74.4 74.3
WSJ (500) Co-training 74.4 76.9
WSJ (1k) Co-training 78.6 79.0
Brown co-training 73.6 76.8
Brown+ small WSJ co-training 75.4 78.2
</table>
<tableCaption confidence="0.9581275">
Table 1: Results on section 23 for the Collins-CFG
parser after co-training with the LTAG parser
</tableCaption>
<bodyText confidence="0.997142027027027">
section 23 of the WSJ). We give F-score results
for the Collins-CFG parser before and after co-
training for section 23.
The results show a modest improvement un-
der each co-training scenario, indicating that, for
the Collins-CFG parser, there is useful informa-
tion to be had from the output of the LTAG
parser. However, the results are not as dramatic
as those reported in other co-training papers, such
as Blum and Mitchell (1998) for web-page classi-
fication and Collins and Singer (1999) for named-
entity recognition. A possible reason is that pars-
ing is a much harder task than these problems.
An open question is whether co-training can
produce results that improve upon the state-of-the-
art in statistical parsing. Investigation of the con-
vergence curves (Figures 3 and 4) as the parsers
are trained upon more and more manually-created
treebank material suggests that, with the Penn
Treebank, the Collins-CFG parser has nearly con-
verged already. Given 40,000 sentences of la-
belled data, we can obtain a projected value of how
much performance can be improved with addi-
tional reliably labelled data. This projected value
was obtained by fitting a curve to the observed
convergence results using a least-squares method
from MAT LAB.
When training data is projected to a size of
400K manually created Treebank sentences, the
performance of the Collins-CFG parser is pro-
jected to be 89.2% with an absolute upper bound
of 89.3%. This suggests that there is very lit-
tle room for performance improvement for the
Collins-CFG parser by simply adding more la-
belled data (using co-training or other bootstrap-
ping methods or even manually). However, mod-
els whose parameters have not already converged
</bodyText>
<page confidence="0.994254">
337
</page>
<bodyText confidence="0.9996395">
might benefit from co-training For instance, when
training data is projected to a size of 400K manu-
ally created Treebank sentences, the performance
of the LTAG statistical parser would be 90.4%
with an absolute upper bound of 91.6%. Thus, a
bootstrapping method might improve performance
of the LTAG statistical parser beyond the current
state-of-the-art performance on the Treebank.
</bodyText>
<sectionHeader confidence="0.998483" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999957791666667">
In this paper, we presented an experimental study
in which a pair of statistical parsers were trained
on labelled and unlabelled data using co-training
Our results showed that simple heuristic methods
for choosing which newly parsed sentences to add
to the training data can be beneficial. We saw that
co-training outperformed self-training, that it was
most beneficial when the seed set was small, and
that co-training was possible even when the seed
material was from another distribution to both the
unlabelled material or the testing set. This final
result is significant as it bears upon the general
problem of having to build models when little or
no labelled training material is available for some
new domain.
Co-training performance may improve if we
consider co-training using sub-parses. This is be-
cause a parse tree is really a large collection of
individual decisions, and retraining upon an entire
tree means committing to all such decisions. Our
ongoing work is addressing this point, largely in
terms of re-ranked parsers. Finally, future work
will also track comparative performance between
the LTAG and Collins-CFG models.
</bodyText>
<sectionHeader confidence="0.991332" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999981833333333">
This work has been supported, in part, by the
NSF/DARPA funded 2002 Language Engineering
Workshop at Johns Hopkins University. We would
like to thank Michael Collins, Andrew McCallum,
and Fernando Pereira for helpful discussions, and
the reviewers for their comments on this paper.
</bodyText>
<sectionHeader confidence="0.999013" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999638339285714">
Steven Abney. 2002. Bootstrapping. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 360-367, Philadelphia, PA.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans,
M. Liberman, M. Marcus, S. Roukos, B. Santorini, and
T. Strzalkowski. 1991. A procedure for quantitatively
comparing the syntactic coverage of English grammars.
In Proceedings of DARPA Speech and Natural Language
Workshop, pages 306-311.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings of
the 11th Annual Conference on Computational Learning
Theory, pages 92-100, Madisson, WI.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of the
AAAL pages 598-603, Menlo Park, CA. AAAI Press/MIT
Press.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings of
the Empirical Methods in NLP Conference, pages 100-
110, University of Maryland, MD.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Sanjoy Dasgupta, Michael Littman, and David McAllester.
2002. PAC generalization bounds for co-training. In
T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,
Advances in Neural Information Processing Systems 14,
Cambridge, MA. MIT Press.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the Empirical Methods in NLP
Conference, Pittsburgh, PA.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings of the 17th
International Conference on Machine Learning, Stanford,
CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguis-
tics, 19(2): 313-330.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceedings
of the 9th International Conference on Information and
Knowledge Management, pages 86-93.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets.
In Proceedings of the Empirical Methods in NLP Confer-
ence, Pittsburgh, PA.
Anoop Sarluu-. 2001. Applying co-training methods to statis-
tical parsing. In Proceedings of the 2nd Annual Meeting
of the NAACL, pages 95-102, Pittsburgh, PA.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings of
the 33rd Annual Meeting of the Association for Computa-
tional Linguistics, pages 189-196, Cambridge, MA.
</reference>
<page confidence="0.99836">
338
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.371636">
<title confidence="0.9999">Bootstrapping Statistical Parsers from Small Datasets</title>
<author confidence="0.995868">Julia Hockenmaier</author>
<author confidence="0.995868">Paul Ruhlent</author>
<author confidence="0.995868">Steven BakerI</author>
<author confidence="0.995868">Jeremiah Crimt</author>
<affiliation confidence="0.997976">Division of Informatics, University of Edinburgh</affiliation>
<email confidence="0.941997">fsteedman,stephenc,julia,osbornel@cogsci.ed.ac.uk</email>
<abstract confidence="0.8904245">of Computing Science, Simon Fraser University . sfu . ca for Advanced Computer Studies, University of Maryland s . umd .</abstract>
<affiliation confidence="0.977795">tCenter for Language and Speech Processing, Johns Hopkins University</affiliation>
<email confidence="0.9894">jcrim@jhu.edu,ruhlen@cs.jhu.edu</email>
<author confidence="0.473024">of Computer Science</author>
<author confidence="0.473024">Cornell University cornell edu</author>
<abstract confidence="0.999776470588235">We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>360--367</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4534" citStr="Abney (2002)" startWordPosition="710" endWordPosition="711">is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000). Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can improve an initial weak learner using unlabelled data. Dasgupta et al. (2002) extend the theory of cotraining by showing that, by maximising their agreement over the unlabelled data, the two learners make few generalisation errors (under the same independence assumption adopted by Blum and Mitchell). Abney (2002) argues that this assumption is extremely restrictive and typically violated in the data, and he proposes a weaker independence assumption. Abney also presents a greedy algorithm that maximises agreement on unlabelled data. Goldman and Zhou (2000) show that, through careful selection of newly labelled examples, cotraining can work even when the classifiers&apos; views do not fully satisfy the independence assumption. 3 Co-training: practice To apply the theory of co-training to parsing, we need to ensure that each parser is capable of learning the parsing task alone and that the two parsers have di</context>
</contexts>
<marker>Abney, 2002</marker>
<rawString>Steven Abney. 2002. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 360-367, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickinger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of DARPA Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="14438" citStr="Black et al., 1991" startWordPosition="2381" endWordPosition="2384">mall amounts of labelled seed data. For reference, Figure 4 shows a similar curve for the LTAG parser. Each parser was first initialized with some labelled seed data from the standard training split (sections 2 to 21) of the WSJ Penn Treebank. 334 LTAG Learning Curve 89 LNAG (&lt; 40micfsT• 88 87 ci 86 85 84 83 0 5000 10000 15000 20000 25000 30000 35000 40000 Number of Sentences Figure 4: The learning curve for the LTAG parser in terms of F-scores for increasing amounts of training data. Performance when evaluated on sentences of length &lt; 40 words is plotted. Evaluation was in terms of Parseval (Black et al., 1991), using a balanced F-score over labelled constituents from section 0 of the Treebank. I The Fscore values are reported for each iteration of cotraining on the development set (section 0 of the Treebank). Since we need to parse all sentences in section 0 at each iteration, in the experiments reported in this paper we only evaluated one of the parsers, the Collins-CFG parser, at each iteration. All results we mention (unless stated otherwise) are F-scores for the Collins-CFG parser. 4.2 Self-training experiments Self-training experiments were conducted in which each parser was retrained on its o</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of DARPA Speech and Natural Language Workshop, pages 306-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<location>Madisson, WI.</location>
<contexts>
<context position="1327" citStr="Blum and Mitchell, 1998" startWordPosition="190" endWordPosition="193"> statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used. 1 Introduction In this paper we describe how co-training (Blum and Mitchell, 1998) can be used to bootstrap a pair of statistical parsers from a small amount of annotated training data. Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other&apos;s output. It has been applied to problems such as word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999). However, these tasks typically involved a small set of labels (around 2-3) and a relatively small parameter space. It is therefore instructive to consider co-training fo</context>
<context position="3222" citStr="Blum and Mitchell, 1998" startWordPosition="486" endWordPosition="489"> viable. In Section 4 we show that co-training outperforms self-training, and that co-training is most beneficial when the seed set of manually created parses is small. Section 4.4 shows that co-training is possible even when the set of initially labelled data is drawn from a different distribution to either the unlabelled training material or the test set; that is, we show that co-training can help in porting a parser from one genre to another. Finally, section 5 reports summary results of our experiments. 2 Co-training: theory Co-training can be informally described in the following manner (Blum and Mitchell, 1998): 331 • Pick two (or more) &amp;quot;views&amp;quot; of a classification problem. • Build separate models for each of these &amp;quot;views&amp;quot; and train each model on a small set of labelled data. • Sample from an unlabelled data set to find examples for each model to label independently (Nigam and Ghani, 2000). • Those examples labelled with high confidence are selected to be new training examples (Collins and Singer, 1999; Goldman and Zhou, 2000). • The models are re-trained on the updated training examples, and the procedure is iterated until the unlabelled data is exhausted. Effectively, by picking confidently labelle</context>
<context position="23199" citStr="Blum and Mitchell (1998)" startWordPosition="3854" endWordPosition="3857">WSJ (1k) Co-training 78.6 79.0 Brown co-training 73.6 76.8 Brown+ small WSJ co-training 75.4 78.2 Table 1: Results on section 23 for the Collins-CFG parser after co-training with the LTAG parser section 23 of the WSJ). We give F-score results for the Collins-CFG parser before and after cotraining for section 23. The results show a modest improvement under each co-training scenario, indicating that, for the Collins-CFG parser, there is useful information to be had from the output of the LTAG parser. However, the results are not as dramatic as those reported in other co-training papers, such as Blum and Mitchell (1998) for web-page classification and Collins and Singer (1999) for namedentity recognition. A possible reason is that parsing is a much harder task than these problems. An open question is whether co-training can produce results that improve upon the state-of-theart in statistical parsing. Investigation of the convergence curves (Figures 3 and 4) as the parsers are trained upon more and more manually-created treebank material suggests that, with the Penn Treebank, the Collins-CFG parser has nearly converged already. Given 40,000 sentences of labelled data, we can obtain a projected value of how mu</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92-100, Madisson, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a contextfree grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the AAAL</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press/MIT Press.</publisher>
<location>Menlo Park, CA.</location>
<contexts>
<context position="15382" citStr="Charniak (1997)" startWordPosition="2537" endWordPosition="2538">ted one of the parsers, the Collins-CFG parser, at each iteration. All results we mention (unless stated otherwise) are F-scores for the Collins-CFG parser. 4.2 Self-training experiments Self-training experiments were conducted in which each parser was retrained on its own output. Self-training provides a useful comparison with co-training because any difference in the results indicates how much the parsers are benefiting from being trained on the output of another parser. This experiment also gives us some insight into the differences between the two parsing models. Self-training was used by Charniak (1997), where a modest gain was reported after re-training his parser on 30 million words. The results are shown in Figure 5. Here, both parsers were initialised with the first 500 sentences from the standard training split (sections 2 to 21) of the WSJ Penn Treebank. Subsequent unlabelled 2xLRxLP 1F-score = where LP is labelled precision and L,R+LP LR is labelled recall. Self-training results 10 20 30 40 50 60 70 80 90 100 Co-training rounds Figure 5: Self-training results for LTAG and Collins-CFG. The upper curve is for Collins-CFG; the lower curve is for LTAG. sentences were also drawn from this </context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a contextfree grammar and word statistics. In Proceedings of the AAAL pages 598-603, Menlo Park, CA. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Empirical Methods in NLP Conference,</booktitle>
<pages>100--110</pages>
<institution>University of Maryland, MD.</institution>
<contexts>
<context position="1756" citStr="Collins and Singer, 1999" startWordPosition="258" endWordPosition="261">at bootstrapping continues to be useful, even though no manually produced parses from the target domain are used. 1 Introduction In this paper we describe how co-training (Blum and Mitchell, 1998) can be used to bootstrap a pair of statistical parsers from a small amount of annotated training data. Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other&apos;s output. It has been applied to problems such as word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999). However, these tasks typically involved a small set of labels (around 2-3) and a relatively small parameter space. It is therefore instructive to consider co-training for more complex models. Compared to these earlier models, a statistical parser has a larger parameter space, and instead of class labels, it produces recursively built parse trees as output. Previous work in co-training statistical parsers (Sarkar, 2001) used two components of a single parsing framework (that is, a parser and a supertagger for that parser). In contrast, this paper considers co-training two diverse statistical </context>
<context position="3620" citStr="Collins and Singer, 1999" startWordPosition="558" endWordPosition="561">help in porting a parser from one genre to another. Finally, section 5 reports summary results of our experiments. 2 Co-training: theory Co-training can be informally described in the following manner (Blum and Mitchell, 1998): 331 • Pick two (or more) &amp;quot;views&amp;quot; of a classification problem. • Build separate models for each of these &amp;quot;views&amp;quot; and train each model on a small set of labelled data. • Sample from an unlabelled data set to find examples for each model to label independently (Nigam and Ghani, 2000). • Those examples labelled with high confidence are selected to be new training examples (Collins and Singer, 1999; Goldman and Zhou, 2000). • The models are re-trained on the updated training examples, and the procedure is iterated until the unlabelled data is exhausted. Effectively, by picking confidently labelled data from each model to add to the training data, one model is labelling data for the other. This is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000). Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can </context>
<context position="23257" citStr="Collins and Singer (1999)" startWordPosition="3863" endWordPosition="3866">8 Brown+ small WSJ co-training 75.4 78.2 Table 1: Results on section 23 for the Collins-CFG parser after co-training with the LTAG parser section 23 of the WSJ). We give F-score results for the Collins-CFG parser before and after cotraining for section 23. The results show a modest improvement under each co-training scenario, indicating that, for the Collins-CFG parser, there is useful information to be had from the output of the LTAG parser. However, the results are not as dramatic as those reported in other co-training papers, such as Blum and Mitchell (1998) for web-page classification and Collins and Singer (1999) for namedentity recognition. A possible reason is that parsing is a much harder task than these problems. An open question is whether co-training can produce results that improve upon the state-of-theart in statistical parsing. Investigation of the convergence curves (Figures 3 and 4) as the parsers are trained upon more and more manually-created treebank material suggests that, with the Penn Treebank, the Collins-CFG parser has nearly converged already. Given 40,000 sentences of labelled data, we can obtain a projected value of how much performance can be improved with additional reliably la</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Empirical Methods in NLP Conference, pages 100-110, University of Maryland, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10499" citStr="Collins, 1999" startWordPosition="1731" endWordPosition="1732"> also reduces the possibility of forcing the central control to select training examples from an entire set of unreliably labelled sentences. Second, we do not require the two parsers to have the same training sets. This allows us to explore several selection schemes in addition to the one proposed by Blum and Mitchell. 4 Experiments In order to conduct co-training experiments between statistical parsers, it was necessary to choose two parsers that generate comparable output but use different statistical models. We therefore chose the following parsers: 1. The Collins lexicalized PCFG parser (Collins, 1999), model 2. Some code for (re)training this parser was added to make the co-training experiments possible. We refer to this parser as Collins-CFG. 2. The Lexicalized Tree Adjoining Grammar (LTAG) parser of Sarkar (2001), which we refer to as the LTAG parser. In order to perform the co-training experiments reported in this paper, LTAG derivation events 333 Collins-CFG LTAG Bi-lexical dependencies are between Bi-lexical dependencies are between lexicalized nonterminals elementary trees Can produce novel elementary Can produce novel hi-lexical trees for the LTAG parser dependencies for Collins-CFG</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Michael Littman</author>
<author>David McAllester</author>
</authors>
<title>PAC generalization bounds for co-training. In</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4297" citStr="Dasgupta et al. (2002)" startWordPosition="671" endWordPosition="674">on the updated training examples, and the procedure is iterated until the unlabelled data is exhausted. Effectively, by picking confidently labelled data from each model to add to the training data, one model is labelling data for the other. This is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000). Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can improve an initial weak learner using unlabelled data. Dasgupta et al. (2002) extend the theory of cotraining by showing that, by maximising their agreement over the unlabelled data, the two learners make few generalisation errors (under the same independence assumption adopted by Blum and Mitchell). Abney (2002) argues that this assumption is extremely restrictive and typically violated in the data, and he proposes a weaker independence assumption. Abney also presents a greedy algorithm that maximises agreement on unlabelled data. Goldman and Zhou (2000) show that, through careful selection of newly labelled examples, cotraining can work even when the classifiers&apos; vie</context>
</contexts>
<marker>Dasgupta, Littman, McAllester, 2002</marker>
<rawString>Sanjoy Dasgupta, Michael Littman, and David McAllester. 2002. PAC generalization bounds for co-training. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the Empirical Methods in NLP Conference,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="20411" citStr="Gildea (2001)" startWordPosition="3385" endWordPosition="3386">ormance of a parser trained on 1,000 seed sentences. Some possible explanations are: that the newly labelled sentences are not reliable (i.e., they contain too many errors); that the sentences deemed reliable are not informative training examples; or a combination of both factors. 4.4 Cross genre experiments This experiment examines whether co-training can be used to boost performance when the un20 30 40 50 60 70 80 90 100 Co-training rounds 80 79.5 79 78.5 78 77.5 77 76.5 76 75.5 75 74.5 0 336 labelled data are taken from a different source than the initial seed data. Previous experiments in Gildea (2001) have shown that porting a statistical parser from a source genre to a target genre is a non-trivial task. Our two different sources were the parsed section of the Brown corpus and the Penn Treebank WSJ. Unlike the WSJ, the Brown corpus does not contain newswire material, and so the two sources differ from each other in terms of vocabulary and syntactic constructs. 1,000 annotated sentences from the Brown section of the Penn Treebank were used as the seed data. Co-training then proceeds using the WSJ.3 Note that no manually created parses in the WSJ domain are used by the parser, even though i</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of the Empirical Methods in NLP Conference, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Goldman</author>
<author>Yan Zhou</author>
</authors>
<title>Enhancing supervised learning with unlabeled data.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="3645" citStr="Goldman and Zhou, 2000" startWordPosition="562" endWordPosition="565">rom one genre to another. Finally, section 5 reports summary results of our experiments. 2 Co-training: theory Co-training can be informally described in the following manner (Blum and Mitchell, 1998): 331 • Pick two (or more) &amp;quot;views&amp;quot; of a classification problem. • Build separate models for each of these &amp;quot;views&amp;quot; and train each model on a small set of labelled data. • Sample from an unlabelled data set to find examples for each model to label independently (Nigam and Ghani, 2000). • Those examples labelled with high confidence are selected to be new training examples (Collins and Singer, 1999; Goldman and Zhou, 2000). • The models are re-trained on the updated training examples, and the procedure is iterated until the unlabelled data is exhausted. Effectively, by picking confidently labelled data from each model to add to the training data, one model is labelling data for the other. This is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000). Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can improve an initial weak l</context>
</contexts>
<marker>Goldman, Zhou, 2000</marker>
<rawString>Sally Goldman and Yan Zhou. 2000. Enhancing supervised learning with unlabeled data. In Proceedings of the 17th International Conference on Machine Learning, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="13442" citStr="Marcus et al., 1993" startWordPosition="2205" endWordPosition="2208">that the Collins-CFG parser is able to learn useful information from the output of the LTAG parser. Collins-CFG Learning Curve 90 88 86 84 82 80 78 76 100 5000 10000 15000 20000 25000 30000 35000 40000 Number of Sentences Figure 3: The learning curve for the Collins-CFG parser in terms of F-scores for increasing amounts of manually annotated training data. Performance for sentences &lt; 40 words is plotted. 4.1 Experimental setup Figure 3 shows how the performance of the Collins-CFG parser varies as the amount of manually annotated training data (from the Wall Street Journal (WSJ) Penn Treebank (Marcus et al., 1993)) is increased. The graph shows a rapid growth in accuracy which tails off as increasing amounts of training data are added. The learning curve shows that the maximum payoff from co-training is likely to occur between 500 and 1,000 sentences. Therefore we used two sizes of seed data: 500 and 1,000 sentences, to see if cotraining could improve parser performance using these small amounts of labelled seed data. For reference, Figure 4 shows a similar curve for the LTAG parser. Each parser was first initialized with some labelled seed data from the standard training split (sections 2 to 21) of th</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2): 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Rayid Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In Proceedings of the 9th International Conference on Information and Knowledge Management,</booktitle>
<pages>86--93</pages>
<contexts>
<context position="3505" citStr="Nigam and Ghani, 2000" startWordPosition="538" endWordPosition="541">t distribution to either the unlabelled training material or the test set; that is, we show that co-training can help in porting a parser from one genre to another. Finally, section 5 reports summary results of our experiments. 2 Co-training: theory Co-training can be informally described in the following manner (Blum and Mitchell, 1998): 331 • Pick two (or more) &amp;quot;views&amp;quot; of a classification problem. • Build separate models for each of these &amp;quot;views&amp;quot; and train each model on a small set of labelled data. • Sample from an unlabelled data set to find examples for each model to label independently (Nigam and Ghani, 2000). • Those examples labelled with high confidence are selected to be new training examples (Collins and Singer, 1999; Goldman and Zhou, 2000). • The models are re-trained on the updated training examples, and the procedure is iterated until the unlabelled data is exhausted. Effectively, by picking confidently labelled data from each model to add to the training data, one model is labelling data for the other. This is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000). Blum and Mitchell prove that, when the two views</context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In Proceedings of the 9th International Conference on Information and Knowledge Management, pages 86-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of cotraining for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings of the Empirical Methods in NLP Conference,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="18164" citStr="Pierce and Cardie (2001)" startWordPosition="2998" endWordPosition="3001">ith self-training. The upper curve is for co-training between Collins-CFG and LTAG; the lower curve is selftraining for Collins-CFG. The effect of seed size Figure 7: The effect of varying seed size on COtraining. The upper curve is for 1,000 sentences labelled seed data; the lower curve is for 500 sentences. comparison.2 The graph shows that co-training results in higher performance than self-training. The graph also shows that co-training performance levels out after around 80 rounds, and then starts to degrade. The likely reason for this dip is noise in the parse trees added by cotraining. Pierce and Cardie (2001) noted a similar behaviour when they co-trained shallow parsers. 2Figures 6, 7 and 8 report the performance of the CollinsCFG parser. We do not report the LTAG parser performance in this paper as evaluating it at the end of each co-training round was too time consuming. We did track LTAG performance on a subset of the WSJ Section 0 and can confirm that LTAG performance also improves as a result of co-training. 10 20 30 40 50 60 70 BO 90 100 Co-training rounds Figure 8: Cross-genre bootstrapping results. The upper curve is for 1,000 sentences labelled data from Brown plus 100 WSJ sentences; the</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of cotraining for natural language learning from large datasets. In Proceedings of the Empirical Methods in NLP Conference, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarluu-</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Annual Meeting of the NAACL,</booktitle>
<pages>95--102</pages>
<location>Pittsburgh, PA.</location>
<marker>Sarluu-, 2001</marker>
<rawString>Anoop Sarluu-. 2001. Applying co-training methods to statistical parsing. In Proceedings of the 2nd Annual Meeting of the NAACL, pages 95-102, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="1649" citStr="Yarowsky, 1995" startWordPosition="246" endWordPosition="247">material is in a different domain to either the raw sentences or the testing material. We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used. 1 Introduction In this paper we describe how co-training (Blum and Mitchell, 1998) can be used to bootstrap a pair of statistical parsers from a small amount of annotated training data. Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other&apos;s output. It has been applied to problems such as word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999). However, these tasks typically involved a small set of labels (around 2-3) and a relatively small parameter space. It is therefore instructive to consider co-training for more complex models. Compared to these earlier models, a statistical parser has a larger parameter space, and instead of class labels, it produces recursively built parse trees as output. Previous work in co-training statistical parsers (Sarkar, 2001) used two components of a single parsing framework (that is, a parser</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189-196, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>