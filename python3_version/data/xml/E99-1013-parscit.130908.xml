<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.713324">
Proceedings of EACL &apos;99
</note>
<title confidence="0.999571">
Complementing WordNet with Roget&apos;s and Corpus-based
Thesauri for Information Retrieval
</title>
<author confidence="0.999745">
Rila Mandala, Takenobu Tokunaga and Hozumi Tanaka
</author>
<affiliation confidence="0.9997865">
Department of Computer Science
Tokyo Institute of Technology
</affiliation>
<address confidence="0.8825855">
2-12-1 Oookayama Meguro-Ku
Tokyo 152-8522 Japan
</address>
<email confidence="0.994095">
{rila,take,tanaka}@cs.titech.ac.jp
</email>
<bodyText confidence="0.979316">
Abstract expansion (Voorhees, 1994; Smeaton and Berrut,
1995), computing lexical cohesion (Stairmand,
1997), word sense disambiguation (Voorhees,
1993), and so on, but the results have not been
very successful.
Previously, we conducted query expansion ex-
periments using WordNet (Mandala et al., to ap-
pear 1999) and found limitations, which can be
summarized as follows:
</bodyText>
<listItem confidence="0.850667">
• Interrelated words may have different parts
of speech.
• Most domain-specific relationships between
</listItem>
<bodyText confidence="0.527638">
words are not found in WordNet.
</bodyText>
<sectionHeader confidence="0.998378" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987157016666667">
This paper proposes a method to over-
come the drawbacks of WordNet when
applied to information retrieval by com-
plementing it with Roget&apos;s thesaurus and
corpus-derived thesauri. Words and rela-
tions which are not included in WordNet
can be found in the corpus-derived the-
sauri. Effects of polysemy can be min-
imized with weighting method consider-
ing all query terms and all of the the-
sauri. Experimental results show that
our method enhances information re-
trieval performance significantly.
Information retrieval (IR) systems can be viewed
basically as a form of comparison between doc-
uments and queries. In traditional IR methods,
this comparison is done based on the use of com-
mon index terms in the document and the query
(Salton and McGill, 1983). The drawback of such
methods is that if semantically relevant docu-
ments do not contain the same terms as the query,
then they will be judged irrelevant by the IR sys-
tem. This occurs because the vocabulary that the
user uses is often not the same as the one used in
documents (Blair and Maron, 1985).
To avoid the above problem, several researchers
have suggested the addition of terms which have
similar or related meaning to the query, increasing
the chances of matching words in relevant docu-
ments. This method is called query expansion.
A thesaurus contains information pertaining to
paradigmatic semantic relations such as term syn-
onymy, hypernymy, and hyponymy (Aitchison and
Gilchrist, 1987). It is thus natural to use a the-
saurus as a source for query expansion.
Many researchers have used WordNet (Miller,
1990) in information retrieval as a tool for query
• Some kinds of words are not included in
WordNet, such as proper names.
To overcome all the above problems, we pro-
pose a method to enrich WordNet with Roget&apos;s
Thesaurus and corpus-based thesauri. The idea
underlying this method is that the automatically
constructed thesauri can counter all the above
drawbacks of WordNet. For example, as we stated
earlier, proper names and their interrelations are
not found in WordNet, but if proper names bear
some strong relationship with other terms, they
often cooccur in documents, as can be modelled
by a corpus-based thesaurus.
Polysemous words degrade the precision of in-
formation retrieval since all senses of the original
query term are considered for expansion. To over-
come the problem of polysemous words, we ap-
ply a restriction in that queries are expanded by
adding those terms that are most similar to the
entirety of the query, rather than selecting terms
that are similar to a single term in the query.
In the next section we describe the details of
our method.
</bodyText>
<page confidence="0.997572">
94
</page>
<note confidence="0.510614">
Proceedings of EACL &apos;99
</note>
<sectionHeader confidence="0.967602" genericHeader="keywords">
2 Thesauri
</sectionHeader>
<subsectionHeader confidence="0.968676">
2.1 WordNet
</subsectionHeader>
<bodyText confidence="0.963764214285714">
In WordNet, words are organized into taxonomies
where each node is a set of synonyms (a synset)
representing a single sense. There are 4 differ-
ent taxonomies based on distinct parts of speech
and many relationships defined within each. In
this paper we use only noun taxonomy with
hyponymy/hypernymy (or is-a) relations, which
relates more general and more specific senses
(Miller, 1988). Figure 1 shows a fragment of the
WordNet taxonomy.
The similarity between word w1 and w2 is de-
fined as the shortest path from each sense of
w1 to each sense of w2, as below (Leacock and
Chodorow, 1988; Resnik, 1995)
</bodyText>
<equation confidence="0.997468">
sim(wi,w2) = max[— log( eb-N )]
</equation>
<bodyText confidence="0.999626333333333">
where Np is the number of nodes in path p from
tvi to w2 and D is the maximum depth of the
taxonomy.
</bodyText>
<subsectionHeader confidence="0.99774">
2.2 Roget&apos;s Thesaurus
</subsectionHeader>
<bodyText confidence="0.999868142857143">
In Roget&apos;s Thesaurus (Chapman, 1977), words
are classified according to the ideas they express,
and these categories of ideas are numbered in se-
quence. The terms within a category are further
organized by part of speech (nouns, verbs, adjec-
tives, adverbs, prepositions, conjunctions, and in-
terjections). Figure 2 shows a fragment of Roget&apos;s
category.
In this case, our similarity measure treat all the
words in Roget as features. A word w possesses
the feature f if f and w belong to the same Ro-
get category. The similarity between two words
is then defined as the Dice coefficient of the two
feature vectors (Lin, 1998).
</bodyText>
<equation confidence="0.991919">
2IR(wi) n R(w2)I
sim(wi w2) IR(wi )1 IR(w2)1
</equation>
<bodyText confidence="0.999652">
where R(w) is the set of words that belong to
the same Roget category as w.
</bodyText>
<subsectionHeader confidence="0.955461">
2.3 Corpus-based Thesaurus
2.3.1 Co-occurrence-based Thesaurus
</subsectionHeader>
<bodyText confidence="0.9999265">
This method is based on the assumption that a
pair of words that frequently occur together in the
same document are related to the same subject.
Therefore word co-occurrence information can be
used to identify semantic relationships between
words (Schutze and Pederson, 1997; Schutze and
Pederson, 1994). We use mutual information as a
tool for computing similarity between words. Mu-
tual information compares the probability of the
co-occurence of words a and b with the indepen-
dent probabilities of occurrence of a and b (Church
and Hanks, 1990).
</bodyText>
<equation confidence="0.9996345">
P(a,b)
I(a,b) = log P(a)P(b)
</equation>
<bodyText confidence="0.999844285714286">
where the probabilities of P(a) and P(b) are esti-
mated by counting the number of occurrences of
a and b in documents and normalizing over the
size of vocabulary in the documents. The joint
probability is estimated by counting the number
of times that word a co-occurs with b and is also
normalized over the size of the vocabulary.
</bodyText>
<subsectionHeader confidence="0.675365">
2.3.2 Syntactically-based Thesaurus
</subsectionHeader>
<bodyText confidence="0.999932025641026">
In contrast to the previous section, this method
attempts to gather term relations on the ba-
sis of linguistic relations and not document co-
occurrence statistics. Words appearing in simi-
lar grammatical contexts are assumed to be sim-
ilar, and therefore classified into the same class
(Lin, 1998; Grefenstette, 1994; Grefenstette, 1992;
Ruge, 1992; Hindle, 1990).
First, all the documents are parsed using the
Apple Pie Parser. The Apple Pie Parser is a
natural language syntactic analyzer developed by
Satoshi Sekine at New York University (Sekine
and Grishman, 1995). The parser is a bottom-up
probabilistic chart parser which finds the parse
tree with the best score by way of the best-first
search algorithm. Its grammar is a semi-context
sensitive grammar with two non-terminals and
was automatically extracted from Penn Tree Bank
syntactically tagged corpus developed at the Uni-
versity of Pennsylvania. The parser generates a
syntactic tree in the manner of a Penn Tree Bank
bracketing. Figure 3 shows a parse tree produced
by this parser.
The main technique used by the parser is the
best-first search. Because the grammar is prob-
abilistic, it is enough to find only one parse
tree with highest possibility. During the parsing
process, the parser keeps the unexpanded active
nodes in a heap, and always expands the active
node with the best probability.
Unknown words are treated in a special man-
ner. If the tagging phase of the parser finds an
unknown word, it uses a list of parts-of-speech de-
fined in the parameter file. This information has
been collected from the Wall Street Journal cor-
pus and uses part of the corpus for training and
the rest for testing. Also, it has separate lists for
such information as special suffices like -1y, -y, -ed,
-d, and -s. The accuracy of this parser is reported
</bodyText>
<page confidence="0.991555">
95
</page>
<bodyText confidence="0.815383625">
Proceedings of EACL &apos;99
Synonyms/Hypernyms (Ordered by Frequency) of noun correlation
2 senses of correlation
Sense 1
correlation, correlativity
=&gt; reciprocality, reciprocity
=&gt; relation
=&gt; abstraction
</bodyText>
<figureCaption confidence="0.885539444444445">
Figure 1: An Example WordNet entry
9. Relation. -- N. relation, bearing, reference, connection,
concern,. cognation ; correlation c. 12; analogy; similarity c. 17;
affinity, homology, alliance, homogeneity, association; approximation c.
(nearness) 197; filiation c. (consanguinity) 11[obs3]; interest; relevancy
c. 23; dependency, relationship, relative position.
comparison c. 464; ratio, proportion.
link, tie, bond of union.
Figure 2: A fragment of a Roget&apos;s Thesaurus entry
</figureCaption>
<bodyText confidence="0.9992885">
as parseva1 recall 77.45 % and parseva1 precision
75.58 %.
Using the above parser, the following syntactic
structures are extracted :
</bodyText>
<listItem confidence="0.942228857142857">
• Subject-Verb
a noun is the subject of a verb.
• Verb-Object
a noun is the object of a verb.
• Adjective-Noun
an adjective modifies a noun.
• Noun-Noun
</listItem>
<bodyText confidence="0.921871">
a noun modifies a noun.
Each noun has a set of verbs, adjectives, and
nouns that it co-occurs with, and for each such
relationship, a mutual information value is calcu-
lated.
</bodyText>
<listItem confidence="0.870357625">
• /sub (vi , nj) = log cf..,(n, )/N.ub)(f vAcub)
where fsub(vi, n3) is the frequency of noun nj
occurring as the subject of verb vi, fsub(n.i)
is the frequency of the noun nj occurring as
subject of any verb, f(vi) is the frequency of
the verb vi, and Nsub is the number of subject
clauses.
• /°bj(vi, nj) = log (fob,(n3)1MAJ)(f(v.)1NobJ)
</listItem>
<bodyText confidence="0.985048666666667">
where fobj(v.i, nj) is the frequency of noun nj
occurring as the object of verb vi, .fobj(n3)
is the frequency of the noun nj occurring as
object of any verb, f(vi) is the frequency of
the verb vi, and Nsub is the number of object
clauses.
</bodyText>
<listItem confidence="0.964726">
• /ad.; (ai, ni) 1°g (f.di(fn*)iNn.j:r(411/N..d;)
</listItem>
<bodyText confidence="0.877188">
where gai, nj) is the frequency of noun nj
occurring as the argument of adjective ai,
fadj(nj) is the frequency of the noun nj oc-
curring as the argument of any adjective,
f(ai) is the frequency of the adjective ai, and
Nadi is the number of adjective clauses.
</bodyText>
<listItem confidence="0.708555">
• /noun(n,ni)
</listItem>
<equation confidence="0.621221333333333">
fooun (nj,ni)/Nnoun
log , ,
nOun xn,„,j J/••noun„ „ kni •.noun where
</equation>
<bodyText confidence="0.99937825">
f (ai, nj) is the frequency of noun nj occur-
ring as the argument of noun n, f0(n) is
the frequency of the noun nj occurring as the
argument of any noun, f(n2) is the frequency
of the noun ni, and Nuouu is the number of
noun clauses.
The similarity sim(w,w2) between two words
w1 and w2 can be computed as follows:
</bodyText>
<equation confidence="0.9539738">
cir(vl,w)+/,(1v2,w))
Eir(wi,w)± E ircw2,w)
(
(r,w)ET(wi) r,w)ET(w2)
Where r is the syntactic relation type, and w is
</equation>
<listItem confidence="0.9965245">
• a verb, if r is the subject-verb or object-verb
relation.
• an adjective, if r is the adjective-noun rela-
tion.
</listItem>
<figure confidence="0.900259">
fobj(nj,vi)Probi
96
Proceedings of EACL &apos;99
VP
VP
NP
VP
/\
ADJ
DT JJ NN VBZ JJ CC VBZ DT JJ NN
That quill Pen looks good and is a new product
</figure>
<figureCaption confidence="0.999797">
Figure 3: An example parse tree
</figureCaption>
<bodyText confidence="0.99753">
The similarity between a query q and a term tj
can be defined as belows :
</bodyText>
<equation confidence="0.957241">
simqt(q,ti) = E * sim(ti, ti)
tiEg
</equation>
<bodyText confidence="0.999955">
where the value of sim(ti, tj) is taken from the
combined thesauri as described above.
With respect to the query q, all the terms in the
collection can now be ranked according to their
simqt. Expansion terms are terms tj with high
simqt(q,ti).
The weight(q,ti) of an expansion term tj is de-
fined as a function of simqt(q,tj):
</bodyText>
<equation confidence="0.562555">
simqt(q,ti)
weight(q,t j)
</equation>
<listItem confidence="0.983608">
• a noun, if r is the noun-noun relation.
</listItem>
<bodyText confidence="0.496868">
and T(w) is the set of pairs (r, w&apos;) such that
/r (w, w&apos;) is positive.
</bodyText>
<sectionHeader confidence="0.890324" genericHeader="introduction">
3 Combination and Term
Expansion Method
</sectionHeader>
<bodyText confidence="0.999076833333333">
A query q is represented by the vector =
q27-•7 qn)7 where each qt is the weight of each
search term ti contained in query q. We used
SMART version 11.0 (Salton, 1971) to obtain the
initial query weight using the formula /tc as be-
lows:
</bodyText>
<equation confidence="0.995933">
(log(tfik) + 1.0) * log(N/nk)
ERlog(tfii + 1.0) * log(Ninj)]2
j.1
</equation>
<bodyText confidence="0.970237">
where tfik is the occurrrence frequency of term tk
in query qi, N is the total number of documents in
the collection, and nk is the number of documents
to which term tk is assigned.
Using the above weighting method, the weight
of initial query terms lies between 0 and 1. On
the other hand, the similarity in each type of the-
saurus does not have a fixed range. Hence, we
apply the following normalization strategy to each
type of thesaurus to bring the similarity value into
the range [0, 1].
sirnold — SiMmin
</bodyText>
<subsectionHeader confidence="0.884861">
SiMmax Sininzin
</subsectionHeader>
<bodyText confidence="0.961334045454545">
The similarity value between two terms in the
combined thesauri is defined as the average of
their similarity value over all types of thesaurus.
EtEg qi
where 0 &lt; weight(q,t j) &lt; 1.
The weight of an expansion term depends both
on all terms appearing in a query and on the sim-
ilarity between the terms, and ranges from 0 to 1.
The weight of an expansion term depends both on
the entire query and on the similarity between the
terms. The weight of an expansion term can be
interpreted mathematically as the weighted mean
of the similarities between the term tj and all the
query terms. The weight of the original query
terms are the weighting factors of those similari-
ties (Qiu and Frei, 1993).
Therefore the query q is expanded by adding
the following query :
cIe&gt; = (ai , a2, ar)
where aj is equal to weight(q,ti) if t3 belongs to
the top r ranked terms. Otherwise aj is equal to
0.
</bodyText>
<equation confidence="0.595406">
simne. =
</equation>
<page confidence="0.916453">
97
</page>
<bodyText confidence="0.876985777777778">
Proceedings of EACL &apos;99
The resulting expanded query is:
expanded = 4 o Cie
where the o is defined as the concatenation oper-
ator.
The method above can accommodate polysemy,
because an expansion term which is taken from a
different sense to the original query term is given
a very low weight.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999088666666667">
Experiments were carried out on the TREC-7 Col-
lection, which consists of 528,155 documents and
50 topics (Voorhees and Harman, to appear 1999).
TREC is currently de facto standard test collec-
tion in information retrieval community.
Table 1 shows topic-length statistics, Table 2
shows document statistics, and Figure 4 shows an
example topic.
We use the title, description, and combined ti-
tle+description+narrative of these topics. Note
that in the TREC-7 collection the description con-
tains all terms in the title section.
For our baseline, we used SMART version 11.0
(Salton, 1971) as information retrieval engine with
the lnc.ltc weighting method. SMART is an infor-
mation retrieval engine based on the vector space
model in which term weights are calculated based
on term frequency, inverse document frequency
and document length normalization.
Automatic indexing of a text in SMART system
involves the following steps :
</bodyText>
<listItem confidence="0.96072525">
• Tokenization : The text is first tokenized
into individual words and other tokens.
• Stop word removal : Common function
words (like the, of, an, etc.) also called stop
words, are removed from this list of tokens.
The SMART system uses a predefined list of
571 stop words.
• Stemming: Various morphological variants
of a word are normalized to the same stem.
SMART system uses the variant of Lovin
method to apply simple rules for suffix strip-
ping.
• Weighting : The term (word and phrase)
vector thus created for a text, is weighted us-
ing tf,idf, and length normalization consid-
erations.
</listItem>
<bodyText confidence="0.981215727272727">
Table 3 gives the average of non-interpolated
precision using SMART without expansion (base-
line), expansion using only WordNet, expansion
using only the corpus-based syntactic-relation-
based thesaurus, expansion using only the corpus-
based co-occurrence-based thesaurus, and expan-
sion using combined thesauri. For each method we
also give the relative improvement over the base-
line. We can see that the combined method out-
perform the isolated use of each type of thesaurus
significantly.
</bodyText>
<tableCaption confidence="0.997439">
Table 1: TREC-7 Topic length statistics
</tableCaption>
<table confidence="0.9957478">
Topic Section Min Max Mean
Title 1 3 2.5
Description 5 34 14.3
Narrative 14 92 40.8
All 31 114 57.6
</table>
<sectionHeader confidence="0.998612" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999951181818182">
In this section we discuss why our method using
WordNet is able to improve information retrieval
performance. The three types of thesaurus we
used have different characteristics. Automatically
constructed thesauri add not only new terms but
also new relationships not found in WordNet. If
two terms often co-occur in a document then those
two terms are likely to bear some relationship.
The reason why we should use not only auto-
matically constructed thesauri is that some rela-
tionships may be missing in them For example,
consider the words colour and color. These words
certainly share the same context, but would never
appear in the same document, at least not with
a frequency recognized by a co-occurrence-based
method. In general, different words used to de-
scribe similar concepts may never be used in the
same document, and are thus missed by cooccur-
rence methods. However their relationship may be
found in WordNet, Roget&apos;s, and the syntactically-
based thesaurus.
One may ask why we included Roget&apos;s The-
saurus here which is almost identical in nature to
WordNet. The reason is to provide more evidence
in the final weighting method. Including Roget&apos;s
as part of the combined thesaurus is better than
not including it, although the improvement is not
significant (4% for title, 2% for description and
0.9% for all terms in the query). One reason is
that the coverage of Roget&apos;s is very limited.
A second point is our weighting method. The
advantages of our weighting method can be sum-
marized as follows:
</bodyText>
<listItem confidence="0.743083">
• the weight of each expansion term considers
the similarity of that term to all terms in the
</listItem>
<page confidence="0.988016">
98
</page>
<tableCaption confidence="0.8314905">
Proceedings of EACL &apos;99
Table 2: TREC-7 Document statistics
</tableCaption>
<table confidence="0.892416818181818">
Source Size (Mb) # Docs Median # Mean #
Words/Doc Words/Doc
Disk 4
FT 564 210,158 316 412.7
FR94 395 55,630 588 644.7
Disk 5
FBIS 470 130,471 322 543.6
LA Times 475 131,896 351 526.5
Title :
ocean remote sensing
Description:
</table>
<bodyText confidence="0.9895999">
Identify documents discussing the development and application of spaceborne
ocean remote sensing.
Narrative:
Documents discussing the development and application of spaceborne ocean re-
mote sensing in oceanography, seabed prospecting and mining, or any marine-
science activity are relevant. Documents that discuss the application of satellite
remote sensing in geography, agriculture, forestry, mining and mineral prospect-
ing or any land-bound science are not relevant, nor are references to interna-
tional marketing or promotional advertizing of any remote-sensing technology.
Synthetic aperture radar (SAR) employed in ocean remote sensing is relevant.
</bodyText>
<figureCaption confidence="0.997815">
Figure 4: Topics Example
</figureCaption>
<bodyText confidence="0.970958785714286">
original query, rather than to just one query
term.
• the weight of an expansion term also depends
on its similarity within all types of thesaurus.
Our method can accommodate polysemy, be-
cause an expansion term taken from a different
sense to the original query term sense is given
very low weight. The reason for this is that the
weighting method depends on all query terms and
all of the thesauri. For example, the word bank
has many senses in WordNet. Two such senses are
the financial institution and river edge senses. In
a document collection relating to financial banks,
the river sense of bank will generally not be found
in the cooccurrence-based thesaurus because of a
lack of articles talking about rivers. Even though
(with small possibility) there may be some doc-
uments in the collection talking about rivers, if
the query contained the finance sense of bank then
the other terms in the query would also tend to be
concerned with finance and not rivers. Thus rivers
would only have a relationship with the bank term
and there would be no relations with other terms
in the original query, resulting in a low weight.
Since our weighting method depends on both the
query in its entirety and similarity over the three
thesauri, wrong sense expansion terms are given
very low weight.
</bodyText>
<sectionHeader confidence="0.998644" genericHeader="method">
6 Related Research
</sectionHeader>
<bodyText confidence="0.999845444444444">
Smeaton (1995) and Voorhees (1994; 1988) pro-
posed an expansion method using WordNet. Our
method differs from theirs in that we enrich the
coverage of WordNet using two methods of auto-
matic thesaurus construction, and we weight the
expansion term appropriately so that it can ac-
commodate polysemy.
Although Stairmand (1997) and Richardson
(1995) proposed the use of WordNet in informa-
tion retrieval, they did not use WordNet in the
query expansion framework.
Our syntactic-relation-based thesaurus is based
on the method proposed by Hindle (1990), al-
though Hindle did not apply it to information
retrieval. Hindle only extracted subject-verb
and object-verb relations, while we also extract
adjective-noun and noun-noun relations, in the
manner of Grefenstette (1994), who applied his
</bodyText>
<page confidence="0.998226">
99
</page>
<tableCaption confidence="0.9239855">
Proceedings of EACL &apos;99
Table 3: Average non-interpolated precision for expansion using single or combined thesauri.
</tableCaption>
<table confidence="0.998017222222222">
Expanded with
Topic Type Base WordNet Roget Syntac Cooccur Combined
only only only only method
Title 0.1175 0.1276 0.1236 0.1386 0.1457 0.2314
(+8.6%) (+5.2 %) (+17.9%) (+24.0%) (+96.9%)
Description 0.1428 0.1509 0.1477 0.1648 0.1693 0.2645
(+5.7%) (+3.4 %) (+15.4%) (+18.5%) (+85.2%)
All 0.1976 0.2010 0.1999 0.2131 0.2191 0.2724
(+1.7%) (+1.2%) (+7.8%) (+10.8%) (+37.8%)
</table>
<bodyText confidence="0.9996138125">
syntactically-based thesaurus to information re-
trieval with mixed results. Our system improves
on Grefenstette&apos;s results since we factor in the-
sauri which contain hierarchical information ab-
sent from his automatically derived thesaurus.
Our weighting method follows the Qiu and Frei
(1993) method, except that Qiu used it to expand
terms from a single automatically constructed the-
sarus and did not consider the use of more than
one thesaurus.
This paper is an extension of our previous work
(Mandala et al., to appear 1999) in which we ddid
not consider the effects of using Roget&apos;s Thesaurus
as one piece of evidence for expansion and used
the Tanimoto coefficient as similarity coefficient
instead of mutual information.
</bodyText>
<sectionHeader confidence="0.999044" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999984384615385">
We have proposed the use of different types of the-
saurus for query expansion. The basic idea under-
lying this method is that each type of thesaurus
has different characteristics and combining them
provides a valuable resource to expand the query.
Wrong expansion terms can be avoided by design-
ing a weighting term method in which the weight
of expansion terms not only depends on all query
terms, but also depends on their similarity values
in all type of thesaurus.
Future research will include the use of a parser
with better performance and the use of more re-
cent term weighting methods for indexing.
</bodyText>
<sectionHeader confidence="0.996574" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998546333333333">
The authors would like to thank Mr. Timothy
Baldwin (TIT, Japan) and three anonymous ref-
erees for useful comments on the earlier version
of this paper. We also thank Dr. Chris Buck-
ley (SabIR Research) for support with SMART,
and Dr. Satoshi Sekine (New York University)
for providing the Apple Pie Parser program. This
research is partially supported by JSPS project
number JSPS-RFTF96P00502.
</bodyText>
<sectionHeader confidence="0.997709" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998303970588235">
J. Aitchison and A. Gilchrist. 1987. Thesaurus
Construction: A Practical Manual. Aslib.
D.C. Blair and M.E. Maxon. 1985. An evalua-
tion of retrieval effectiveness. Communications
of the ACM, 28:289-299.
Robert L. Chapman. 1977. Roget&apos;s International
Thesaurus (Forth Edition). Harper and Row,
New York.
Kenneth Ward Church and Patrick Hanks. 1990.
Word association norms, mutual information
and lexicography. In Proceedings of the 27th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 76-83.
Gregory Grefenstette. 1992. Use of syntactic
context to produce term association lists for
text retrieval. In Proceedings of the 15th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval, pages 89-97.
Gregory Grefenstette. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer Aca-
demic Publisher.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings
of the 28th Annual Meeting of the Association
for Computational Linguistic, pages 268-275.
Claudia Leacock and Martin Chodorow. 1988.
Combining local context and WordNet similar-
ity for word sense identification. In Christiane
Fellbaum, editor, WordNet, An Electronic Lex-
ical Database, pages 265-283. MIT Press.
Dekang Lin. 1998. Automatic retrieval and clus-
tering of similar words. In Proceedings of the
COLING-ACL&apos;98, pages 768-773.
</reference>
<page confidence="0.873436">
100
</page>
<reference confidence="0.998502148148148">
Proceedings of EACL &apos;99
Rila Mandala, Takenobu Tokunaga, and Hozumi
Tanaka. to appear, 1999. Combining general
hand-made and automatically constructed the-
sauri for information retrieval. In Proceedings
of the 16th International Joint Conference on
Artificial Intelligence (IJCAI-99).
George A. Miller. 1988. Nouns in WordNet.
In Christiane Fellbaum, editor, WordNet, An
Electronic Lexical Database, pages 23-46. MIT
Press.
George A. Miller. 1990. Special issue, WordNet:
An on-line lexical database. International Jour-
nal of Lexicography, 3(4).
Yonggang Qiu and Hans-Peter Frei. 1993. Con-
cept based query expansion. In Proceedings
of the 16th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 160-169.
Philip Resnik. 1995. Using information content
to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th International Joint
Conference on Artificial Intelligence (IJCAI-
95), pages 448-453.
R. Richardson and Alan F. Smeaton. 1995. Using
WordNet in a knowledge-based approach to in-
formation retrieval. Technical Report CA-0395,
School of Computer Applications, Dublin City
University.
Gerda Ruge. 1992. Experiments on linguistically-
based term associations. Information Process-
ing and Management, 28(3):317-332.
Gerard Salton and M McGill. 1983. An In-
troduction to Modern Information Retrieval.
McGraw-Hill.
Gerard Salton. 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Pro-
cessing. Prentice-Hall.
Hinrich Schutze and Jan 0. Pederson. 1994. A
cooccurrence-based thesaurus and two applica-
tions to information retrieval. In Proceedings of
the RIAD 94 Conference.
Hinrich Schutze and Jan 0. Pederson. 1997. A
cooccurrence-based thesaurus and two applica-
tions to information retrieval. Information Pro-
cessing and Management, 33(3):307-318.
Satoshi Sekine and Ralph Grishman. 1995. A
corpus-based probabilistic grammar with only
two non-terminals. In Proceedings of the Inter-
national Workshop on Parsing Technologies.
Alan F. Smeaton and C. Berrut. 1995. Running
TREC-4 experiments: A chronological report of
query expansion experiments carried out as part
of TREC-4. In Proceedings of The Fourth Text
REtrieval Conference (TREC-4). NIST special
publication.
Mark A. Stairmand. 1997. Textual context anal-
ysis for information retrieval. In Proceedings
of the 20th Annual International ACM-SIGIR
Conference on Research and Development in
Information Retrieval, pages 140-147.
Ellen M. Voorhees and Donna Harman. to ap-
pear, 1999. Overview of the Seventh Text RE-
trieval Conference (TREC-7). In Proceedings of
the Seventh Text REtrieval Conference. NIST
Special Publication.
Ellen M. Voorhees. 1988. Using WordNet for text
retrieval. In Christiane Fellbaum, editor, Word-
Net, An Electronic Lexical Database, pages 285-
303. MIT Press.
Ellen M. Voorhees. 1993. Using wordnet to dis-
ambiguate word senses for text retrieval. In
Proceedings of the 16th Annual International
ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 171-
180.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of the
17th Annual International ACM-SIGIR Con-
ference on Research and Development in Infor-
mation Retrieval, pages 61-69.
</reference>
<page confidence="0.998604">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.114582">
<note confidence="0.829897">Proceedings of EACL &apos;99</note>
<title confidence="0.949927">Complementing WordNet with Roget&apos;s and Corpus-based Thesauri for Information Retrieval</title>
<author confidence="0.987114">Takenobu Tokunaga Tanaka Mandala</author>
<affiliation confidence="0.999965">Department of Computer Science Tokyo Institute of Technology</affiliation>
<address confidence="0.726229">2-12-1 Oookayama Meguro-Ku Tokyo 152-8522 Japan</address>
<email confidence="0.602444">rila@cs.titech.ac.jp</email>
<email confidence="0.602444">take@cs.titech.ac.jp</email>
<email confidence="0.602444">tanaka@cs.titech.ac.jp</email>
<note confidence="0.834704">(Voorhees, 1994; Smeaton and Berrut, 1995), computing lexical cohesion (Stairmand, 1997), word sense disambiguation (Voorhees,</note>
<abstract confidence="0.9435825">1993), and so on, but the results have not been very successful. Previously, we conducted query expansion experiments using WordNet (Mandala et al., to appear 1999) and found limitations, which can be summarized as follows: • Interrelated words may have different parts of speech. • Most domain-specific relationships between words are not found in WordNet.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aitchison</author>
<author>A Gilchrist</author>
</authors>
<title>Thesaurus Construction: A Practical Manual.</title>
<date>1987</date>
<publisher>Aslib.</publisher>
<contexts>
<context position="2269" citStr="Aitchison and Gilchrist, 1987" startWordPosition="346" endWordPosition="349">ents do not contain the same terms as the query, then they will be judged irrelevant by the IR system. This occurs because the vocabulary that the user uses is often not the same as the one used in documents (Blair and Maron, 1985). To avoid the above problem, several researchers have suggested the addition of terms which have similar or related meaning to the query, increasing the chances of matching words in relevant documents. This method is called query expansion. A thesaurus contains information pertaining to paradigmatic semantic relations such as term synonymy, hypernymy, and hyponymy (Aitchison and Gilchrist, 1987). It is thus natural to use a thesaurus as a source for query expansion. Many researchers have used WordNet (Miller, 1990) in information retrieval as a tool for query • Some kinds of words are not included in WordNet, such as proper names. To overcome all the above problems, we propose a method to enrich WordNet with Roget&apos;s Thesaurus and corpus-based thesauri. The idea underlying this method is that the automatically constructed thesauri can counter all the above drawbacks of WordNet. For example, as we stated earlier, proper names and their interrelations are not found in WordNet, but if pr</context>
</contexts>
<marker>Aitchison, Gilchrist, 1987</marker>
<rawString>J. Aitchison and A. Gilchrist. 1987. Thesaurus Construction: A Practical Manual. Aslib.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Blair</author>
<author>M E Maxon</author>
</authors>
<title>An evaluation of retrieval effectiveness.</title>
<date>1985</date>
<journal>Communications of the ACM,</journal>
<pages>28--289</pages>
<marker>Blair, Maxon, 1985</marker>
<rawString>D.C. Blair and M.E. Maxon. 1985. An evaluation of retrieval effectiveness. Communications of the ACM, 28:289-299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert L Chapman</author>
</authors>
<title>Roget&apos;s International Thesaurus (Forth Edition). Harper and Row,</title>
<date>1977</date>
<location>New York.</location>
<contexts>
<context position="4299" citStr="Chapman, 1977" startWordPosition="698" endWordPosition="699">of speech and many relationships defined within each. In this paper we use only noun taxonomy with hyponymy/hypernymy (or is-a) relations, which relates more general and more specific senses (Miller, 1988). Figure 1 shows a fragment of the WordNet taxonomy. The similarity between word w1 and w2 is defined as the shortest path from each sense of w1 to each sense of w2, as below (Leacock and Chodorow, 1988; Resnik, 1995) sim(wi,w2) = max[— log( eb-N )] where Np is the number of nodes in path p from tvi to w2 and D is the maximum depth of the taxonomy. 2.2 Roget&apos;s Thesaurus In Roget&apos;s Thesaurus (Chapman, 1977), words are classified according to the ideas they express, and these categories of ideas are numbered in sequence. The terms within a category are further organized by part of speech (nouns, verbs, adjectives, adverbs, prepositions, conjunctions, and interjections). Figure 2 shows a fragment of Roget&apos;s category. In this case, our similarity measure treat all the words in Roget as features. A word w possesses the feature f if f and w belong to the same Roget category. The similarity between two words is then defined as the Dice coefficient of the two feature vectors (Lin, 1998). 2IR(wi) n R(w2</context>
</contexts>
<marker>Chapman, 1977</marker>
<rawString>Robert L. Chapman. 1977. Roget&apos;s International Thesaurus (Forth Edition). Harper and Row, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="5615" citStr="Church and Hanks, 1990" startWordPosition="914" endWordPosition="917">ategory as w. 2.3 Corpus-based Thesaurus 2.3.1 Co-occurrence-based Thesaurus This method is based on the assumption that a pair of words that frequently occur together in the same document are related to the same subject. Therefore word co-occurrence information can be used to identify semantic relationships between words (Schutze and Pederson, 1997; Schutze and Pederson, 1994). We use mutual information as a tool for computing similarity between words. Mutual information compares the probability of the co-occurence of words a and b with the independent probabilities of occurrence of a and b (Church and Hanks, 1990). P(a,b) I(a,b) = log P(a)P(b) where the probabilities of P(a) and P(b) are estimated by counting the number of occurrences of a and b in documents and normalizing over the size of vocabulary in the documents. The joint probability is estimated by counting the number of times that word a co-occurs with b and is also normalized over the size of the vocabulary. 2.3.2 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appearing in similar grammatical c</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information and lexicography. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 76-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Use of syntactic context to produce term association lists for text retrieval.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>89--97</pages>
<contexts>
<context position="6345" citStr="Grefenstette, 1992" startWordPosition="1035" endWordPosition="1036">occurrences of a and b in documents and normalizing over the size of vocabulary in the documents. The joint probability is estimated by counting the number of times that word a co-occurs with b and is also normalized over the size of the vocabulary. 2.3.2 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appearing in similar grammatical contexts are assumed to be similar, and therefore classified into the same class (Lin, 1998; Grefenstette, 1994; Grefenstette, 1992; Ruge, 1992; Hindle, 1990). First, all the documents are parsed using the Apple Pie Parser. The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University (Sekine and Grishman, 1995). The parser is a bottom-up probabilistic chart parser which finds the parse tree with the best score by way of the best-first search algorithm. Its grammar is a semi-context sensitive grammar with two non-terminals and was automatically extracted from Penn Tree Bank syntactically tagged corpus developed at the University of Pennsylvania. The parser generates a syn</context>
</contexts>
<marker>Grefenstette, 1992</marker>
<rawString>Gregory Grefenstette. 1992. Use of syntactic context to produce term association lists for text retrieval. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 89-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publisher.</publisher>
<contexts>
<context position="6325" citStr="Grefenstette, 1994" startWordPosition="1033" endWordPosition="1034">nting the number of occurrences of a and b in documents and normalizing over the size of vocabulary in the documents. The joint probability is estimated by counting the number of times that word a co-occurs with b and is also normalized over the size of the vocabulary. 2.3.2 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appearing in similar grammatical contexts are assumed to be similar, and therefore classified into the same class (Lin, 1998; Grefenstette, 1994; Grefenstette, 1992; Ruge, 1992; Hindle, 1990). First, all the documents are parsed using the Apple Pie Parser. The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University (Sekine and Grishman, 1995). The parser is a bottom-up probabilistic chart parser which finds the parse tree with the best score by way of the best-first search algorithm. Its grammar is a semi-context sensitive grammar with two non-terminals and was automatically extracted from Penn Tree Bank syntactically tagged corpus developed at the University of Pennsylvania. The pa</context>
<context position="20097" citStr="Grefenstette (1994)" startWordPosition="3361" endWordPosition="3362">of WordNet using two methods of automatic thesaurus construction, and we weight the expansion term appropriately so that it can accommodate polysemy. Although Stairmand (1997) and Richardson (1995) proposed the use of WordNet in information retrieval, they did not use WordNet in the query expansion framework. Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval. Hindle only extracted subject-verb and object-verb relations, while we also extract adjective-noun and noun-noun relations, in the manner of Grefenstette (1994), who applied his 99 Proceedings of EACL &apos;99 Table 3: Average non-interpolated precision for expansion using single or combined thesauri. Expanded with Topic Type Base WordNet Roget Syntac Cooccur Combined only only only only method Title 0.1175 0.1276 0.1236 0.1386 0.1457 0.2314 (+8.6%) (+5.2 %) (+17.9%) (+24.0%) (+96.9%) Description 0.1428 0.1509 0.1477 0.1648 0.1693 0.2645 (+5.7%) (+3.4 %) (+15.4%) (+18.5%) (+85.2%) All 0.1976 0.2010 0.1999 0.2131 0.2191 0.2724 (+1.7%) (+1.2%) (+7.8%) (+10.8%) (+37.8%) syntactically-based thesaurus to information retrieval with mixed results. Our system imp</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistic,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="6372" citStr="Hindle, 1990" startWordPosition="1039" endWordPosition="1040">nts and normalizing over the size of vocabulary in the documents. The joint probability is estimated by counting the number of times that word a co-occurs with b and is also normalized over the size of the vocabulary. 2.3.2 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appearing in similar grammatical contexts are assumed to be similar, and therefore classified into the same class (Lin, 1998; Grefenstette, 1994; Grefenstette, 1992; Ruge, 1992; Hindle, 1990). First, all the documents are parsed using the Apple Pie Parser. The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University (Sekine and Grishman, 1995). The parser is a bottom-up probabilistic chart parser which finds the parse tree with the best score by way of the best-first search algorithm. Its grammar is a semi-context sensitive grammar with two non-terminals and was automatically extracted from Penn Tree Bank syntactically tagged corpus developed at the University of Pennsylvania. The parser generates a syntactic tree in the manner o</context>
<context position="19876" citStr="Hindle (1990)" startWordPosition="3330" endWordPosition="3331">sense expansion terms are given very low weight. 6 Related Research Smeaton (1995) and Voorhees (1994; 1988) proposed an expansion method using WordNet. Our method differs from theirs in that we enrich the coverage of WordNet using two methods of automatic thesaurus construction, and we weight the expansion term appropriately so that it can accommodate polysemy. Although Stairmand (1997) and Richardson (1995) proposed the use of WordNet in information retrieval, they did not use WordNet in the query expansion framework. Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval. Hindle only extracted subject-verb and object-verb relations, while we also extract adjective-noun and noun-noun relations, in the manner of Grefenstette (1994), who applied his 99 Proceedings of EACL &apos;99 Table 3: Average non-interpolated precision for expansion using single or combined thesauri. Expanded with Topic Type Base WordNet Roget Syntac Cooccur Combined only only only only method Title 0.1175 0.1276 0.1236 0.1386 0.1457 0.2314 (+8.6%) (+5.2 %) (+17.9%) (+24.0%) (+96.9%) Description 0.1428 0.1509 0.1477 0.1648 0.1693 0.2645 </context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun classification from predicate-argument structures. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistic, pages 268-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1988</date>
<booktitle>In Christiane Fellbaum, editor, WordNet, An Electronic Lexical Database,</booktitle>
<pages>265--283</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4092" citStr="Leacock and Chodorow, 1988" startWordPosition="657" endWordPosition="660">s of EACL &apos;99 2 Thesauri 2.1 WordNet In WordNet, words are organized into taxonomies where each node is a set of synonyms (a synset) representing a single sense. There are 4 different taxonomies based on distinct parts of speech and many relationships defined within each. In this paper we use only noun taxonomy with hyponymy/hypernymy (or is-a) relations, which relates more general and more specific senses (Miller, 1988). Figure 1 shows a fragment of the WordNet taxonomy. The similarity between word w1 and w2 is defined as the shortest path from each sense of w1 to each sense of w2, as below (Leacock and Chodorow, 1988; Resnik, 1995) sim(wi,w2) = max[— log( eb-N )] where Np is the number of nodes in path p from tvi to w2 and D is the maximum depth of the taxonomy. 2.2 Roget&apos;s Thesaurus In Roget&apos;s Thesaurus (Chapman, 1977), words are classified according to the ideas they express, and these categories of ideas are numbered in sequence. The terms within a category are further organized by part of speech (nouns, verbs, adjectives, adverbs, prepositions, conjunctions, and interjections). Figure 2 shows a fragment of Roget&apos;s category. In this case, our similarity measure treat all the words in Roget as features.</context>
</contexts>
<marker>Leacock, Chodorow, 1988</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1988. Combining local context and WordNet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet, An Electronic Lexical Database, pages 265-283. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL&apos;98,</booktitle>
<pages>768--773</pages>
<contexts>
<context position="4883" citStr="Lin, 1998" startWordPosition="798" endWordPosition="799"> Thesaurus (Chapman, 1977), words are classified according to the ideas they express, and these categories of ideas are numbered in sequence. The terms within a category are further organized by part of speech (nouns, verbs, adjectives, adverbs, prepositions, conjunctions, and interjections). Figure 2 shows a fragment of Roget&apos;s category. In this case, our similarity measure treat all the words in Roget as features. A word w possesses the feature f if f and w belong to the same Roget category. The similarity between two words is then defined as the Dice coefficient of the two feature vectors (Lin, 1998). 2IR(wi) n R(w2)I sim(wi w2) IR(wi )1 IR(w2)1 where R(w) is the set of words that belong to the same Roget category as w. 2.3 Corpus-based Thesaurus 2.3.1 Co-occurrence-based Thesaurus This method is based on the assumption that a pair of words that frequently occur together in the same document are related to the same subject. Therefore word co-occurrence information can be used to identify semantic relationships between words (Schutze and Pederson, 1997; Schutze and Pederson, 1994). We use mutual information as a tool for computing similarity between words. Mutual information compares the p</context>
<context position="6305" citStr="Lin, 1998" startWordPosition="1031" endWordPosition="1032">ated by counting the number of occurrences of a and b in documents and normalizing over the size of vocabulary in the documents. The joint probability is estimated by counting the number of times that word a co-occurs with b and is also normalized over the size of the vocabulary. 2.3.2 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appearing in similar grammatical contexts are assumed to be similar, and therefore classified into the same class (Lin, 1998; Grefenstette, 1994; Grefenstette, 1992; Ruge, 1992; Hindle, 1990). First, all the documents are parsed using the Apple Pie Parser. The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University (Sekine and Grishman, 1995). The parser is a bottom-up probabilistic chart parser which finds the parse tree with the best score by way of the best-first search algorithm. Its grammar is a semi-context sensitive grammar with two non-terminals and was automatically extracted from Penn Tree Bank syntactically tagged corpus developed at the University of </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the COLING-ACL&apos;98, pages 768-773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rila Mandala</author>
</authors>
<title>Takenobu Tokunaga, and Hozumi Tanaka. to appear,</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-99).</booktitle>
<marker>Mandala, 1999</marker>
<rawString>Rila Mandala, Takenobu Tokunaga, and Hozumi Tanaka. to appear, 1999. Combining general hand-made and automatically constructed thesauri for information retrieval. In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Nouns in WordNet.</title>
<date>1988</date>
<booktitle>In Christiane Fellbaum, editor, WordNet, An Electronic Lexical Database,</booktitle>
<pages>23--46</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3890" citStr="Miller, 1988" startWordPosition="620" endWordPosition="621">st similar to the entirety of the query, rather than selecting terms that are similar to a single term in the query. In the next section we describe the details of our method. 94 Proceedings of EACL &apos;99 2 Thesauri 2.1 WordNet In WordNet, words are organized into taxonomies where each node is a set of synonyms (a synset) representing a single sense. There are 4 different taxonomies based on distinct parts of speech and many relationships defined within each. In this paper we use only noun taxonomy with hyponymy/hypernymy (or is-a) relations, which relates more general and more specific senses (Miller, 1988). Figure 1 shows a fragment of the WordNet taxonomy. The similarity between word w1 and w2 is defined as the shortest path from each sense of w1 to each sense of w2, as below (Leacock and Chodorow, 1988; Resnik, 1995) sim(wi,w2) = max[— log( eb-N )] where Np is the number of nodes in path p from tvi to w2 and D is the maximum depth of the taxonomy. 2.2 Roget&apos;s Thesaurus In Roget&apos;s Thesaurus (Chapman, 1977), words are classified according to the ideas they express, and these categories of ideas are numbered in sequence. The terms within a category are further organized by part of speech (nouns,</context>
</contexts>
<marker>Miller, 1988</marker>
<rawString>George A. Miller. 1988. Nouns in WordNet. In Christiane Fellbaum, editor, WordNet, An Electronic Lexical Database, pages 23-46. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Special issue, WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="2391" citStr="Miller, 1990" startWordPosition="370" endWordPosition="371">hat the user uses is often not the same as the one used in documents (Blair and Maron, 1985). To avoid the above problem, several researchers have suggested the addition of terms which have similar or related meaning to the query, increasing the chances of matching words in relevant documents. This method is called query expansion. A thesaurus contains information pertaining to paradigmatic semantic relations such as term synonymy, hypernymy, and hyponymy (Aitchison and Gilchrist, 1987). It is thus natural to use a thesaurus as a source for query expansion. Many researchers have used WordNet (Miller, 1990) in information retrieval as a tool for query • Some kinds of words are not included in WordNet, such as proper names. To overcome all the above problems, we propose a method to enrich WordNet with Roget&apos;s Thesaurus and corpus-based thesauri. The idea underlying this method is that the automatically constructed thesauri can counter all the above drawbacks of WordNet. For example, as we stated earlier, proper names and their interrelations are not found in WordNet, but if proper names bear some strong relationship with other terms, they often cooccur in documents, as can be modelled by a corpus</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. Special issue, WordNet: An on-line lexical database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Qiu</author>
<author>Hans-Peter Frei</author>
</authors>
<title>Concept based query expansion.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>160--169</pages>
<contexts>
<context position="12787" citStr="Qiu and Frei, 1993" startWordPosition="2162" endWordPosition="2165">ed as the average of their similarity value over all types of thesaurus. EtEg qi where 0 &lt; weight(q,t j) &lt; 1. The weight of an expansion term depends both on all terms appearing in a query and on the similarity between the terms, and ranges from 0 to 1. The weight of an expansion term depends both on the entire query and on the similarity between the terms. The weight of an expansion term can be interpreted mathematically as the weighted mean of the similarities between the term tj and all the query terms. The weight of the original query terms are the weighting factors of those similarities (Qiu and Frei, 1993). Therefore the query q is expanded by adding the following query : cIe&gt; = (ai , a2, ar) where aj is equal to weight(q,ti) if t3 belongs to the top r ranked terms. Otherwise aj is equal to 0. simne. = 97 Proceedings of EACL &apos;99 The resulting expanded query is: expanded = 4 o Cie where the o is defined as the concatenation operator. The method above can accommodate polysemy, because an expansion term which is taken from a different sense to the original query term is given a very low weight. 4 Experiments Experiments were carried out on the TREC-7 Collection, which consists of 528,155 documents</context>
<context position="20897" citStr="Qiu and Frei (1993)" startWordPosition="3473" endWordPosition="3476"> Syntac Cooccur Combined only only only only method Title 0.1175 0.1276 0.1236 0.1386 0.1457 0.2314 (+8.6%) (+5.2 %) (+17.9%) (+24.0%) (+96.9%) Description 0.1428 0.1509 0.1477 0.1648 0.1693 0.2645 (+5.7%) (+3.4 %) (+15.4%) (+18.5%) (+85.2%) All 0.1976 0.2010 0.1999 0.2131 0.2191 0.2724 (+1.7%) (+1.2%) (+7.8%) (+10.8%) (+37.8%) syntactically-based thesaurus to information retrieval with mixed results. Our system improves on Grefenstette&apos;s results since we factor in thesauri which contain hierarchical information absent from his automatically derived thesaurus. Our weighting method follows the Qiu and Frei (1993) method, except that Qiu used it to expand terms from a single automatically constructed thesarus and did not consider the use of more than one thesaurus. This paper is an extension of our previous work (Mandala et al., to appear 1999) in which we ddid not consider the effects of using Roget&apos;s Thesaurus as one piece of evidence for expansion and used the Tanimoto coefficient as similarity coefficient instead of mutual information. 7 Conclusions We have proposed the use of different types of thesaurus for query expansion. The basic idea underlying this method is that each type of thesaurus has </context>
</contexts>
<marker>Qiu, Frei, 1993</marker>
<rawString>Yonggang Qiu and Hans-Peter Frei. 1993. Concept based query expansion. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 160-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI95),</booktitle>
<pages>448--453</pages>
<contexts>
<context position="4107" citStr="Resnik, 1995" startWordPosition="661" endWordPosition="662"> WordNet In WordNet, words are organized into taxonomies where each node is a set of synonyms (a synset) representing a single sense. There are 4 different taxonomies based on distinct parts of speech and many relationships defined within each. In this paper we use only noun taxonomy with hyponymy/hypernymy (or is-a) relations, which relates more general and more specific senses (Miller, 1988). Figure 1 shows a fragment of the WordNet taxonomy. The similarity between word w1 and w2 is defined as the shortest path from each sense of w1 to each sense of w2, as below (Leacock and Chodorow, 1988; Resnik, 1995) sim(wi,w2) = max[— log( eb-N )] where Np is the number of nodes in path p from tvi to w2 and D is the maximum depth of the taxonomy. 2.2 Roget&apos;s Thesaurus In Roget&apos;s Thesaurus (Chapman, 1977), words are classified according to the ideas they express, and these categories of ideas are numbered in sequence. The terms within a category are further organized by part of speech (nouns, verbs, adjectives, adverbs, prepositions, conjunctions, and interjections). Figure 2 shows a fragment of Roget&apos;s category. In this case, our similarity measure treat all the words in Roget as features. A word w posse</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI95), pages 448-453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Richardson</author>
<author>Alan F Smeaton</author>
</authors>
<title>Using WordNet in a knowledge-based approach to information retrieval.</title>
<date>1995</date>
<tech>Technical Report CA-0395,</tech>
<institution>School of Computer Applications, Dublin City University.</institution>
<marker>Richardson, Smeaton, 1995</marker>
<rawString>R. Richardson and Alan F. Smeaton. 1995. Using WordNet in a knowledge-based approach to information retrieval. Technical Report CA-0395, School of Computer Applications, Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerda Ruge</author>
</authors>
<title>Experiments on linguisticallybased term associations.</title>
<date>1992</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>28--3</pages>
<contexts>
<context position="6357" citStr="Ruge, 1992" startWordPosition="1037" endWordPosition="1038"> b in documents and normalizing over the size of vocabulary in the documents. The joint probability is estimated by counting the number of times that word a co-occurs with b and is also normalized over the size of the vocabulary. 2.3.2 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appearing in similar grammatical contexts are assumed to be similar, and therefore classified into the same class (Lin, 1998; Grefenstette, 1994; Grefenstette, 1992; Ruge, 1992; Hindle, 1990). First, all the documents are parsed using the Apple Pie Parser. The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University (Sekine and Grishman, 1995). The parser is a bottom-up probabilistic chart parser which finds the parse tree with the best score by way of the best-first search algorithm. Its grammar is a semi-context sensitive grammar with two non-terminals and was automatically extracted from Penn Tree Bank syntactically tagged corpus developed at the University of Pennsylvania. The parser generates a syntactic tree </context>
</contexts>
<marker>Ruge, 1992</marker>
<rawString>Gerda Ruge. 1992. Experiments on linguisticallybased term associations. Information Processing and Management, 28(3):317-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>M McGill</author>
</authors>
<title>An Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="1570" citStr="Salton and McGill, 1983" startWordPosition="231" endWordPosition="234">t with Roget&apos;s thesaurus and corpus-derived thesauri. Words and relations which are not included in WordNet can be found in the corpus-derived thesauri. Effects of polysemy can be minimized with weighting method considering all query terms and all of the thesauri. Experimental results show that our method enhances information retrieval performance significantly. Information retrieval (IR) systems can be viewed basically as a form of comparison between documents and queries. In traditional IR methods, this comparison is done based on the use of common index terms in the document and the query (Salton and McGill, 1983). The drawback of such methods is that if semantically relevant documents do not contain the same terms as the query, then they will be judged irrelevant by the IR system. This occurs because the vocabulary that the user uses is often not the same as the one used in documents (Blair and Maron, 1985). To avoid the above problem, several researchers have suggested the addition of terms which have similar or related meaning to the query, increasing the chances of matching words in relevant documents. This method is called query expansion. A thesaurus contains information pertaining to paradigmati</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and M McGill. 1983. An Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<date>1971</date>
<booktitle>The SMART Retrieval System: Experiments in Automatic Document Processing.</booktitle>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="11435" citStr="Salton, 1971" startWordPosition="1918" endWordPosition="1919">hesauri as described above. With respect to the query q, all the terms in the collection can now be ranked according to their simqt. Expansion terms are terms tj with high simqt(q,ti). The weight(q,ti) of an expansion term tj is defined as a function of simqt(q,tj): simqt(q,ti) weight(q,t j) • a noun, if r is the noun-noun relation. and T(w) is the set of pairs (r, w&apos;) such that /r (w, w&apos;) is positive. 3 Combination and Term Expansion Method A query q is represented by the vector = q27-•7 qn)7 where each qt is the weight of each search term ti contained in query q. We used SMART version 11.0 (Salton, 1971) to obtain the initial query weight using the formula /tc as belows: (log(tfik) + 1.0) * log(N/nk) ERlog(tfii + 1.0) * log(Ninj)]2 j.1 where tfik is the occurrrence frequency of term tk in query qi, N is the total number of documents in the collection, and nk is the number of documents to which term tk is assigned. Using the above weighting method, the weight of initial query terms lies between 0 and 1. On the other hand, the similarity in each type of thesaurus does not have a fixed range. Hence, we apply the following normalization strategy to each type of thesaurus to bring the similarity v</context>
<context position="13880" citStr="Salton, 1971" startWordPosition="2351" endWordPosition="2352">ery low weight. 4 Experiments Experiments were carried out on the TREC-7 Collection, which consists of 528,155 documents and 50 topics (Voorhees and Harman, to appear 1999). TREC is currently de facto standard test collection in information retrieval community. Table 1 shows topic-length statistics, Table 2 shows document statistics, and Figure 4 shows an example topic. We use the title, description, and combined title+description+narrative of these topics. Note that in the TREC-7 collection the description contains all terms in the title section. For our baseline, we used SMART version 11.0 (Salton, 1971) as information retrieval engine with the lnc.ltc weighting method. SMART is an information retrieval engine based on the vector space model in which term weights are calculated based on term frequency, inverse document frequency and document length normalization. Automatic indexing of a text in SMART system involves the following steps : • Tokenization : The text is first tokenized into individual words and other tokens. • Stop word removal : Common function words (like the, of, an, etc.) also called stop words, are removed from this list of tokens. The SMART system uses a predefined list of </context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>Gerard Salton. 1971. The SMART Retrieval System: Experiments in Automatic Document Processing. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
<author>Jan</author>
</authors>
<title>A cooccurrence-based thesaurus and two applications to information retrieval.</title>
<date>1994</date>
<booktitle>In Proceedings of the RIAD 94 Conference.</booktitle>
<marker>Schutze, Jan, 1994</marker>
<rawString>Hinrich Schutze and Jan 0. Pederson. 1994. A cooccurrence-based thesaurus and two applications to information retrieval. In Proceedings of the RIAD 94 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
<author>Jan</author>
</authors>
<title>A cooccurrence-based thesaurus and two applications to information retrieval.</title>
<date>1997</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>33--3</pages>
<marker>Schutze, Jan, 1997</marker>
<rawString>Hinrich Schutze and Jan 0. Pederson. 1997. A cooccurrence-based thesaurus and two applications to information retrieval. Information Processing and Management, 33(3):307-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>A corpus-based probabilistic grammar with only two non-terminals.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="6578" citStr="Sekine and Grishman, 1995" startWordPosition="1070" endWordPosition="1073"> size of the vocabulary. 2.3.2 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appearing in similar grammatical contexts are assumed to be similar, and therefore classified into the same class (Lin, 1998; Grefenstette, 1994; Grefenstette, 1992; Ruge, 1992; Hindle, 1990). First, all the documents are parsed using the Apple Pie Parser. The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University (Sekine and Grishman, 1995). The parser is a bottom-up probabilistic chart parser which finds the parse tree with the best score by way of the best-first search algorithm. Its grammar is a semi-context sensitive grammar with two non-terminals and was automatically extracted from Penn Tree Bank syntactically tagged corpus developed at the University of Pennsylvania. The parser generates a syntactic tree in the manner of a Penn Tree Bank bracketing. Figure 3 shows a parse tree produced by this parser. The main technique used by the parser is the best-first search. Because the grammar is probabilistic, it is enough to find</context>
</contexts>
<marker>Sekine, Grishman, 1995</marker>
<rawString>Satoshi Sekine and Ralph Grishman. 1995. A corpus-based probabilistic grammar with only two non-terminals. In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan F Smeaton</author>
<author>C Berrut</author>
</authors>
<title>Running TREC-4 experiments: A chronological report of query expansion experiments carried out as part of TREC-4.</title>
<date>1995</date>
<booktitle>In Proceedings of The Fourth Text REtrieval Conference (TREC-4). NIST special publication.</booktitle>
<marker>Smeaton, Berrut, 1995</marker>
<rawString>Alan F. Smeaton and C. Berrut. 1995. Running TREC-4 experiments: A chronological report of query expansion experiments carried out as part of TREC-4. In Proceedings of The Fourth Text REtrieval Conference (TREC-4). NIST special publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Stairmand</author>
</authors>
<title>Textual context analysis for information retrieval.</title>
<date>1997</date>
<booktitle>In Proceedings of the 20th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="19653" citStr="Stairmand (1997)" startWordPosition="3296" endWordPosition="3297">e bank term and there would be no relations with other terms in the original query, resulting in a low weight. Since our weighting method depends on both the query in its entirety and similarity over the three thesauri, wrong sense expansion terms are given very low weight. 6 Related Research Smeaton (1995) and Voorhees (1994; 1988) proposed an expansion method using WordNet. Our method differs from theirs in that we enrich the coverage of WordNet using two methods of automatic thesaurus construction, and we weight the expansion term appropriately so that it can accommodate polysemy. Although Stairmand (1997) and Richardson (1995) proposed the use of WordNet in information retrieval, they did not use WordNet in the query expansion framework. Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval. Hindle only extracted subject-verb and object-verb relations, while we also extract adjective-noun and noun-noun relations, in the manner of Grefenstette (1994), who applied his 99 Proceedings of EACL &apos;99 Table 3: Average non-interpolated precision for expansion using single or combined thesauri. Expanded with Topi</context>
</contexts>
<marker>Stairmand, 1997</marker>
<rawString>Mark A. Stairmand. 1997. Textual context analysis for information retrieval. In Proceedings of the 20th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, pages 140-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Donna Harman</author>
</authors>
<title>to appear,</title>
<date>1999</date>
<booktitle>Overview of the Seventh Text REtrieval Conference (TREC-7). In Proceedings of the Seventh Text REtrieval Conference. NIST Special Publication.</booktitle>
<marker>Voorhees, Harman, 1999</marker>
<rawString>Ellen M. Voorhees and Donna Harman. to appear, 1999. Overview of the Seventh Text REtrieval Conference (TREC-7). In Proceedings of the Seventh Text REtrieval Conference. NIST Special Publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Using WordNet for text retrieval.</title>
<date>1988</date>
<booktitle>WordNet, An Electronic Lexical Database,</booktitle>
<pages>285--303</pages>
<editor>In Christiane Fellbaum, editor,</editor>
<publisher>MIT Press.</publisher>
<marker>Voorhees, 1988</marker>
<rawString>Ellen M. Voorhees. 1988. Using WordNet for text retrieval. In Christiane Fellbaum, editor, WordNet, An Electronic Lexical Database, pages 285-303. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Using wordnet to disambiguate word senses for text retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>171--180</pages>
<marker>Voorhees, 1993</marker>
<rawString>Ellen M. Voorhees. 1993. Using wordnet to disambiguate word senses for text retrieval. In Proceedings of the 16th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, pages 171-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Query expansion using lexical-semantic relations.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>61--69</pages>
<contexts>
<context position="19364" citStr="Voorhees (1994" startWordPosition="3250" endWordPosition="3251">(with small possibility) there may be some documents in the collection talking about rivers, if the query contained the finance sense of bank then the other terms in the query would also tend to be concerned with finance and not rivers. Thus rivers would only have a relationship with the bank term and there would be no relations with other terms in the original query, resulting in a low weight. Since our weighting method depends on both the query in its entirety and similarity over the three thesauri, wrong sense expansion terms are given very low weight. 6 Related Research Smeaton (1995) and Voorhees (1994; 1988) proposed an expansion method using WordNet. Our method differs from theirs in that we enrich the coverage of WordNet using two methods of automatic thesaurus construction, and we weight the expansion term appropriately so that it can accommodate polysemy. Although Stairmand (1997) and Richardson (1995) proposed the use of WordNet in information retrieval, they did not use WordNet in the query expansion framework. Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval. Hindle only extracted subje</context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>Ellen M. Voorhees. 1994. Query expansion using lexical-semantic relations. In Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, pages 61-69.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>