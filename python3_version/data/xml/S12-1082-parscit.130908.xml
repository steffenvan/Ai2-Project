<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009209">
<title confidence="0.995124">
DeepPurple: Estimating Sentence Semantic Similarity using
N-gram Regression Models and Web Snippets
</title>
<author confidence="0.999274">
Nikos Malandrakis, Elias Iosif, Alexandros Potamianos
</author>
<affiliation confidence="0.997369">
Department of ECE, Technical University of Crete, 73100 Chania, Greece
</affiliation>
<email confidence="0.996239">
[nmalandrakis,iosife,potam]@telecom.tuc.gr
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999150882352941">
We estimate the semantic similarity between
two sentences using regression models with
features: 1) n-gram hit rates (lexical matches)
between sentences, 2) lexical semantic sim-
ilarity between non-matching words, and 3)
sentence length. Lexical semantic similarity is
computed via co-occurrence counts on a cor-
pus harvested from the web using a modified
mutual information metric. State-of-the-art re-
sults are obtained for semantic similarity com-
putation at the word level, however, the fusion
of this information at the sentence level pro-
vides only moderate improvement on Task 6
of SemEval’12. Despite the simple features
used, regression models provide good perfor-
mance, especially for shorter sentences, reach-
ing correlation of 0.62 on the SemEval test set.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993950020833334">
Recently, there has been significant research activ-
ity on the area of semantic similarity estimation
motivated both by abundance of relevant web data
and linguistic resources for this task. Algorithms
for computing semantic textual similarity (STS) are
relevant for a variety of applications, including in-
formation extraction (Szpektor and Dagan, 2008),
question answering (Harabagiu and Hickl, 2006)
and machine translation (Mirkin et al., 2009). Word-
or term-level STS (a special case of sentence level
STS) has also been successfully applied to the prob-
lem of grammar induction (Meng and Siu, 2002)
and affective text categorization (Malandrakis et al.,
2011). In this work, we built on previous research
on word-level semantic similarity estimation to de-
sign and implement a system for sentence-level STS
for Task6 of the SemEval’12 campaign.
Semantic similarity between words can be re-
garded as the graded semantic equivalence at the
lexeme level and is tightly related with the tasks of
word sense discovery and disambiguation (Agirre
and Edmonds, 2007). Metrics of word semantic sim-
ilarity can be divided into: (i) knowledge-based met-
rics (Miller, 1990; Budanitsky and Hirst, 2006) and
(ii) corpus-based metrics (Baroni and Lenci, 2010;
Iosif and Potamianos, 2010).
When more complex structures, such as phrases
and sentences, are considered, it is much harder
to estimate semantic equivalence due to the non-
compositional nature of sentence-level semantics
and the exponential explosion of possible interpre-
tations. STS is closely related to the problems of
paraphrasing, which is bidirectional and based on
semantic equivalence (Madnani and Dorr, 2010) and
textual entailment, which is directional and based
on relations between semantics (Dagan et al., 2006).
Related methods incorporate measurements of sim-
ilarity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al., 2009), and semantic (Rinaldi
et al., 2003; Bos and Markert, 2005). Measures
from machine translation evaluation are often used
to evaluate lexical level approaches (Finch et al.,
2005; Perez and Alfonseca, 2005), including BLEU
(Papineni et al., 2002), a metric based on word n-
gram hit rates.
Motivated by BLEU, we use n-gram hit rates and
word-level semantic similarity scores as features in
</bodyText>
<page confidence="0.991348">
565
</page>
<note confidence="0.52908">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9944484">
a linear regression model to estimate sentence level
semantic similarity. We also propose sigmoid scal-
ing of similarity scores and sentence-length depen-
dent modeling. The models are evaluated on the Se-
mEval’12 sentence similarity task.
</bodyText>
<sectionHeader confidence="0.632991" genericHeader="method">
2 Semantic similarity between words
</sectionHeader>
<bodyText confidence="0.999752676470588">
In this section, two different metrics of word simi-
larity are presented. The first is a language-agnostic,
corpus-based metric requiring no knowledge re-
sources, while the second metric relies on WordNet.
Corpus-based metric: Given a corpus, the se-
mantic similarity between two words, wi and wj,
is estimated as their pointwise mutual information
(Church and Hanks, 1990): I(i, j) = log ˆ�(i)ˆ�(j),
where p(i) and p(j) are the occurrence probabili-
ties of wi and wj, respectively, while the probability
of their co-occurrence is denoted by p(i, j). These
probabilities are computed according to maximum
likelihood estimation. The assumption of this met-
ric is that co-occurrence implies semantic similarity.
During the past decade the web has been used for
estimating the required probabilities (Turney, 2001;
Bollegala et al., 2007), by querying web search en-
gines and retrieving the number of hits required
to estimate the frequency of individual words and
their co-occurrence. However, these approaches
have failed to obtain state-of-the-art results (Bolle-
gala et al., 2007), unless “expensive” conjunctive
AND queries are used for harvesting a corpus and
then using this corpus to estimate similarity scores
(Iosif and Potamianos, 2010).
Recently, a scalable approach1 for harvesting a
corpus has been proposed where web snippets are
downloaded using individual queries for each word
(Iosif and Potamianos, 2012b). Semantic similar-
ity can then be estimated using the I(i, j) metric
and within-snippet word co-occurrence frequencies.
Under the maximum sense similarity assumption
(Resnik, 1995), it is relatively easy to show that a
(more) lexically-balanced corpus2 (as the one cre-
</bodyText>
<footnote confidence="0.993260142857143">
1The scalability of this approach has been demonstrated in
(Iosif and Potamianos, 2012b) for a 10K vocabulary, here we
extend it to the full 60K WordNet vocabulary.
2According to this assumption the semantic similarity of two
words can be estimated as the minimum pairwise similarity of
their senses. The gist of the argument is that although words
often co-occur with their closest senses, word occurrences cor-
</footnote>
<bodyText confidence="0.999196727272727">
ated above) can significantly reduce the semantic
similarity estimation error of the mutual information
metric I(i, j). This is also experimentally verified in
(Iosif and Potamianos, 2012c).
In addition, one can modify the mutual informa-
tion metric to further reduce estimation error (for
the theoretical foundation behind this see (Iosif and
Potamianos, 2012a)). Specifically, one may intro-
duce exponential weights α in order to reduce the
contribution of p(i) and p(j) in the similarity met-
ric. The modified metric Ia(i, j), is defined as:
</bodyText>
<equation confidence="0.997212">
Ia(i,j)=2 [logpα(i)�p(j) + logp(i)�pα(j)J (1)
</equation>
<bodyText confidence="0.99988965">
The weight α was estimated on the corpus of (Iosif
and Potamianos, 2012b) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of α = 0.8 was shown to significantly out-
perform I(i, j) and to achieve state-of-the-art results
on standard semantic similarity datasets (Rubenstein
and Goodenough, 1965; Miller and Charles, 1998;
Finkelstein et al., 2002). For more details see (Iosif
and Potamianos, 2012a).
WordIet-based metrics: For comparison pur-
poses, we evaluated various similarity metrics on
the task of word similarity computation on three
standard datasets (same as above). The best re-
sults were obtained by the Vector metric (Patward-
han and Pedersen, 2006), which exploits the lexical
information that is included in the WordNet glosses.
This metric was incorporated to our proposed ap-
proach. All metrics were computed using the Word-
Net::Similarity module (Pedersen, 2005).
</bodyText>
<sectionHeader confidence="0.991875" genericHeader="method">
3 I-gram Regression Models
</sectionHeader>
<bodyText confidence="0.945395333333333">
Inspired by BLEU (Papineni et al., 2002), we pro-
pose a simple regression model that combines evi-
dence from two sources: number of n-gram matches
and degree of similarity between non-matching
words between two sentences. In order to incorpo-
rate a word semantic similarity metric into BLEU,
we apply the following two-pass process: first lexi-
cal hits are identified and counted, and then the se-
mantic similarity between n-grams not matched dur-
respond to all senses, i.e., the denominator of I(i, j) is overes-
timated causing large underestimation error for similarities be-
tween polysemous words.
</bodyText>
<equation confidence="0.781773">
ˆ�(i,j)
</equation>
<page confidence="0.979352">
566
</page>
<bodyText confidence="0.99950835">
ing the first pass is estimated. All word similar-
ity metrics used are peak-to-peak normalized in the
[0,1] range, so they serve as a “degree-of-match”.
The semantic similarity scores from word pairs are
summed together (just like n-gram hits) to obtain
a BLEU-like semantic similarity score. The main
problem here is one of alignment, since we need
to compare each non-matched n-gram from the hy-
pothesis with an n-gram from the reference. We
use a simple approach: we iterate on the hypoth-
esis n-grams, left-to-right, and compare each with
the most similar non-matched n-gram in the refer-
ence. This modification to BLEU is only applied
to 1-grams, since semantic similarity scores for bi-
grams (or higher) were not available.
Thus, our list of features are the hit rates obtained
by BLEU (for 1-, 2-, 3-, 4-grams) and the total se-
mantic similarity (SS) score for 1-grams3. These
features are then combined using a multiple linear
regression model:
</bodyText>
<equation confidence="0.992504333333333">
4
DL = a0 + � an Bn + a5 M1, (2)
n=1
</equation>
<bodyText confidence="0.999942">
where DL is the estimated similarity, Bn is the
BLEU hit rate for n-grams, M1 is the total semantic
similarity score (SS) for non-matching 1-grams and
an are the trainable parameters of the model.
Motivated by evidence of cognitive scaling of
semantic similarity scores (Iosif and Potamianos,
2010), we propose the use of a sigmoid function to
scale DL sentence similarities. We have also ob-
served in the SemEval data that the way humans rate
sentence similarity is very much dependent on sen-
tence length4. To capture the effect of length and
cognitive scaling we propose next two modifications
to the linear regression model. The sigmoid fusion
scheme is described by the following equation:
</bodyText>
<equation confidence="0.998187333333333">
r 11 −1
DL L1 + exp Cas —1 IJ (3)
\ ag /
</equation>
<bodyText confidence="0.943380045454545">
where we assume that sentence length l (average
3Note that the features are computed twice on each sentence
in a forward and backward fashion (where the word order is
reversed), and then averaged between the two runs.
4We speculate that shorter sentences are mostly compared at
the lexical level using the short-term memory language buffers,
while longer sentences tend to be compared at a higher cogni-
tive level, where the non-compositional nature of sentence se-
mantics dominate.
length for each sentence pair, in words) acts as a
scaling factor for the linearly estimated similarity.
The hierarchical fusion scheme is actually a col-
lection of (overlapping) linear regression models,
each matching a range of sentence lengths. For ex-
ample, the first model DL1 is trained with sentences
with length up to l1, i.e., l &lt; l1, the second model
DL2 up to length l2 etc. During testing, sentences
with length l E [1,l1] are decoded with DL1, sen-
tences with length l E (l1, l2] with model DL2 etc.
Each of these partial models is a linear fusion model
as shown in (2). In this work, we use four models
with l1 = 10, l2 = 20, l3 = 30, l4 = 00.
</bodyText>
<sectionHeader confidence="0.996323" genericHeader="method">
4 Experimental Procedure and Results
</sectionHeader>
<bodyText confidence="0.99992990625">
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al., 2005; Toutanova et al.,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. The exact type
of pre-processing used depends on the metric used.
For the plain lexical BLEU, we use lemmatization,
stemming (of lemmas) and remove all non-content
words, keeping only nouns, adjectives, verbs and ad-
verbs. For computing semantic similarity scores, we
don’t use stemming and keep only noun words, since
we only have similarities between non-noun words.
For the computation of semantic similarity we have
created a dictionary containing all the single-word
nouns included in WordNet (approx. 60K) and then
downloaded snippets of the 500 top-ranked docu-
ments for each word by formulating single-word
queries and submitting them to the Yahoo! search
engine.
Next, results are reported in terms of correlation
between the automatically computed scores and the
ground truth, for each of the corpora in Task 6 of
SemEval’12 (paraphrase, video, europarl, WordNet,
news). Overall correlation (“Ovrl”) computed on the
join of the dataset, as well as, average (“Mean”) cor-
relation across all task is also reported. Training is
performed on a subset of the first three corpora and
testing on all five corpora.
Baseline BLEU: The first set of results in Ta-
ble 1, shows the correlation performance of the
plain BLEU hit rates (per training data set and over-
all/average). The best performing hit rate is the one
</bodyText>
<equation confidence="0.971951">
�DS = a6 DL + a7
</equation>
<page confidence="0.987042">
567
</page>
<tableCaption confidence="0.7987845">
calculated using unigrams.
Table 1: Correlation performance of BLEU hit rates.
</tableCaption>
<table confidence="0.9987044">
par vid euro Mean Ovrl
BLEU 1-grams 0.62 0.67 0.49 0.59 0.57
BLEU 2-grams 0.40 0.39 0.37 0.39 0.34
BLEU 3-grams 0.32 0.36 0.30 0.33 0.33
BLEU 4-grams 0.26 0.25 0.24 0.25 0.28
</table>
<bodyText confidence="0.992630538461539">
Semantic Similarity BLEU (Purple): The perfor-
mance of the modified version of BLEU that in-
corporates various word-level similarity metrics is
shown in Table 2. Here the BLEU hits (exact
matches) are summed together with the normalized
similarity scores (approximate matches) to obtain a
single B1+M1 (Purple) score5. As we can see, there
are definite benefits to using the modified version,
particularly with regards to mean correlation. Over-
all the best performers, when taking into account
both mean and overall correlation, are the WordNet-
based and I,,, metrics, with the I,,, metric winning by
a slight margin, earning a place in the final models.
</bodyText>
<tableCaption confidence="0.9973005">
Table 2: Correlation performance of 1-gram BLEU
scores with semantic similarity metrics (nouns-only).
</tableCaption>
<table confidence="0.9983198">
par vid euro Mean Ovrl
BLEU 0.54 0.60 0.39 0.51 0.58
SS-BLEU WordNet 0.56 0.64 0.41 0.54 0.58
SS-BLEU I(i, j) 0.56 0.63 0.39 0.53 0.59
SS-BLEU Ia(i, j) 0.57 0.64 0.40 0.54 0.58
</table>
<bodyText confidence="0.979511941176471">
Regression models (DeepPurple): Next, the per-
formance of the various regression models (fusion
schemes) is investigated. Each regression model is
evaluated by performing 10-fold cross-validation on
the SemEval training set. Correlation performance
is shown in Table 3 both with and without seman-
tic similarity. The baseline in this case is the Pur-
ple metric (corresponding to no fusion). Clearly
the use of regression models significantly improves
performance compared to the 1-gram BLEU and
Purple baselines for almost all datasets, and espe-
cially for the combined dataset (overall). Among
the fusion schemes, the hierarchical models perform
the best. Following fusion, the performance gain
from incorporating semantic similarity (SS) is much
smaller. Finally, in Table 4, correlation performance
of our submissions on the official SemEval test set is
</bodyText>
<footnote confidence="0.439254">
5It should be stressed that the plain BLEU unigram scores
shown in this table are not comparable to those in Table 1, since
here scores are calculated over only the nouns of each sentence.
</footnote>
<tableCaption confidence="0.996612333333333">
Table 3: Correlation performance of regression model
with (SS) and without semantic similarities on the train-
ing set (using 10-fold cross-validation).
</tableCaption>
<table confidence="0.997987625">
par vid euro Mean Ovrl
None (SS-BLEU Ia) 0.57 0.64 0.40 0.54 0.58
Linear (DL, a5 =0) 0.62 0.72 0.47 0.60 0.66
Sigmoid (DS, a5 =0) 0.64 0.73 0.42 0.60 0.73
Hierarchical 0.64 0.74 0.48 0.62 0.73
SS-Linear (DL) 0.64 0.73 0.47 0.61 0.66
SS-Sigmoid (DS) 0.65 0.74 0.42 0.60 0.74
SS-Hierarchical 0.65 0.74 0.48 0.62 0.73
</table>
<bodyText confidence="0.9996185">
shown. The overall correlation performance of the
Hierarchical model ranks somewhere in the middle
(43rd out of 89 systems), while the mean correla-
tion (weighted by number of samples per set) is no-
tably better: 23rd out of 89. Comparing the individ-
ual dataset results, our systems underperform for the
two datasets that originate from the machine transla-
tion (MT) literature (and contain longer sentences),
while we achieve good results for the rest (19th for
paraphrase, 37th for video and 29th for WN).
</bodyText>
<tableCaption confidence="0.99837">
Table 4: Correlation performance on test set.
</tableCaption>
<bodyText confidence="0.72539125">
par vid euro WN news Mean Ovrl
None 0.50 0.71 0.44 0.49 0.24 0.51 0.49
Sigm. 0.60 0.76 0.26 0.60 0.34 0.56 0.55
Hier. 0.60 0.77 0.43 0.65 0.37 0.60 0.62
</bodyText>
<sectionHeader confidence="0.996615" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99996752631579">
We have shown that: 1) a regression model that
combines counts of exact and approximate n-gram
matches provides good performance for sentence
similarity computation (especially for short and
medium length sentences), 2) the non-linear scal-
ing of hit-rates with respect to sentence length im-
proves performance, 3) incorporating word semantic
similarity scores (soft-match) into the model can im-
prove performance, and 4) web snippet corpus cre-
ation and the modified mutual information metric
is a language agnostic approach that can (at least)
match semantic similarity performance of the best
resource-based metrics for this task. Future work,
should involve the extension of this approach to
model larger lexical chunks, the incorporation of
compositional models of meaning, and in general
the phrase-level modeling of semantic similarity, in
order to compete with MT-based systems trained on
massive external parallel corpora.
</bodyText>
<page confidence="0.997042">
568
</page>
<sectionHeader confidence="0.931446" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997300048076923">
E. Agirre and P. Edmonds, editors. 2007. Word
Sense Disambiguation: Algorithms and Applications.
Springer.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673–721.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proc. of International Conference
on World Wide Web, pages 757–766.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:13–47.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22–29.
I. Dagan, O. Glickman, and B. Magnini. 2006.
The pascal recognising textual entailment challenge.
In Joaquin Quionero-Candela, Ido Dagan, Bernardo
Magnini, and Florence dAlch Buc, editors, Machine
Learning Challenges. Evaluating Predictive Uncer-
tainty, Visual Object Classification, and Recognising
Tectual Entailment, volume 3944 of Lecture Notes in
Computer Science, pages 177–190. Springer Berlin /
Heidelberg.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 363–370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116–131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings ofthe 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting ofthe Association for Computational Linguis-
tics, pages 905–912.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):1637–1647.
E. Iosif and A. Potamianos. 2012a. Minimum error se-
mantic similarity using text corpora constructed from
web queries. IEEE Transactions on Knowledge and
Data Engineering (submitted to).
E. Iosif and A. Potamianos. 2012b. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. Proc. of Eighth International Con-
ference on Language Resources and Evaluation (to ap-
pear).
E. Iosif and A. Potamianos. 2012c. Similarity com-
putation using semantic networks created from web-
harvested data. Natural Language Engineering (sub-
mitted to).
N. Madnani and B. J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341387.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42–47.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing ofAFNLP, pages 42–47.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977–2980.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172–181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1–28.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal ofLexicography, 3(4):235–312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings ofthe 47th Annual Meeting ofACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791–799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311–318.
</reference>
<page confidence="0.983914">
569
</page>
<reference confidence="0.99960475">
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based context vectors to estimate the semantic related-
ness of concepts. In Proc. of the EACL Workshop on
Making Sense of Sense: Bringing Computational Lin-
guistics and Psycholinguistics Together, pages 1–8.
T. Pedersen. 2005. WordNet::Similarity.
http://search.cpan.org/dist/
WordNet-Similarity/.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conferencefor Artificial Intelligence,
pages 448–453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 25–32.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627–633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849–856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173–180.
P. D. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proc. of the European
Conference on Machine Learning, pages 491–502.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
</reference>
<page confidence="0.996921">
570
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.790973">
<title confidence="0.998108">DeepPurple: Estimating Sentence Semantic Similarity using N-gram Regression Models and Web Snippets</title>
<author confidence="0.991228">Nikos Malandrakis</author>
<author confidence="0.991228">Elias Iosif</author>
<author confidence="0.991228">Alexandros Potamianos</author>
<affiliation confidence="0.890402">Department of ECE, Technical University of Crete, 73100 Chania, Greece</affiliation>
<email confidence="0.981262">[nmalandrakis,iosife,potam]@telecom.tuc.gr</email>
<abstract confidence="0.994952055555555">We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, and 3) sentence length. Lexical semantic similarity is computed via co-occurrence counts on a corpus harvested from the web using a modified mutual information metric. State-of-the-art results are obtained for semantic similarity computation at the word level, however, the fusion of this information at the sentence level provides only moderate improvement on Task 6 of SemEval’12. Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Word Sense Disambiguation: Algorithms and Applications.</title>
<date>2007</date>
<editor>E. Agirre and P. Edmonds, editors.</editor>
<publisher>Springer.</publisher>
<marker>2007</marker>
<rawString>E. Agirre and P. Edmonds, editors. 2007. Word Sense Disambiguation: Algorithms and Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="2295" citStr="Baroni and Lenci, 2010" startWordPosition="332" endWordPosition="335"> categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Edmonds, 2007). Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels:</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Measuring semantic similarity between words using web search engines.</title>
<date>2007</date>
<booktitle>In Proc. of International Conference on World Wide Web,</booktitle>
<pages>757--766</pages>
<contexts>
<context position="4662" citStr="Bollegala et al., 2007" startWordPosition="683" endWordPosition="686">pus-based metric: Given a corpus, the semantic similarity between two words, wi and wj, is estimated as their pointwise mutual information (Church and Hanks, 1990): I(i, j) = log ˆ�(i)ˆ�(j), where p(i) and p(j) are the occurrence probabilities of wi and wj, respectively, while the probability of their co-occurrence is denoted by p(i, j). These probabilities are computed according to maximum likelihood estimation. The assumption of this metric is that co-occurrence implies semantic similarity. During the past decade the web has been used for estimating the required probabilities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occurrence. However, these approaches have failed to obtain state-of-the-art results (Bollegala et al., 2007), unless “expensive” conjunctive AND queries are used for harvesting a corpus and then using this corpus to estimate similarity scores (Iosif and Potamianos, 2010). Recently, a scalable approach1 for harvesting a corpus has been proposed where web snippets are downloaded using individual queries for each word (Iosif and Potamianos, 2012b). Semantic simil</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2007</marker>
<rawString>D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Measuring semantic similarity between words using web search engines. In Proc. of International Conference on World Wide Web, pages 757–766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>K Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>628635</pages>
<contexts>
<context position="3059" citStr="Bos and Markert, 2005" startWordPosition="441" endWordPosition="444">ntic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics a linear regression model to estimate sentence level semantic similarity. We also propose sigmoid s</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>J. Bos and K. Markert. 2005. Recognising textual entailment with logical inference. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, page 628635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased measures of semantic distance.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<pages>32--13</pages>
<contexts>
<context position="2241" citStr="Budanitsky and Hirst, 2006" startWordPosition="324" endWordPosition="327">f grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Edmonds, 2007). Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods inc</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNetbased measures of semantic distance. Computational Linguistics, 32:13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="4202" citStr="Church and Hanks, 1990" startWordPosition="612" endWordPosition="615"> model to estimate sentence level semantic similarity. We also propose sigmoid scaling of similarity scores and sentence-length dependent modeling. The models are evaluated on the SemEval’12 sentence similarity task. 2 Semantic similarity between words In this section, two different metrics of word similarity are presented. The first is a language-agnostic, corpus-based metric requiring no knowledge resources, while the second metric relies on WordNet. Corpus-based metric: Given a corpus, the semantic similarity between two words, wi and wj, is estimated as their pointwise mutual information (Church and Hanks, 1990): I(i, j) = log ˆ�(i)ˆ�(j), where p(i) and p(j) are the occurrence probabilities of wi and wj, respectively, while the probability of their co-occurrence is denoted by p(i, j). These probabilities are computed according to maximum likelihood estimation. The assumption of this metric is that co-occurrence implies semantic similarity. During the past decade the web has been used for estimating the required probabilities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occur</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<editor>In Joaquin Quionero-Candela, Ido Dagan, Bernardo Magnini, and Florence dAlch Buc, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="2820" citStr="Dagan et al., 2006" startWordPosition="409" endWordPosition="412">iller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lex</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2006. The pascal recognising textual entailment challenge. In Joaquin Quionero-Candela, Ido Dagan, Bernardo Magnini, and Florence dAlch Buc, editors, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finch</author>
<author>S Y Hwang</author>
<author>E Sumita</author>
</authors>
<title>Using machine translation evaluation techniques to determine sentence-level semantic equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Workshop on Paraphrasing,</booktitle>
<pages>1724</pages>
<contexts>
<context position="3177" citStr="Finch et al., 2005" startWordPosition="458" endWordPosition="461">interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics a linear regression model to estimate sentence level semantic similarity. We also propose sigmoid scaling of similarity scores and sentence-length dependent modeling. The models are evaluated on the SemEval’12 sentenc</context>
</contexts>
<marker>Finch, Hwang, Sumita, 2005</marker>
<rawString>A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In Proceedings of the 3rd International Workshop on Paraphrasing, page 1724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="11070" citStr="Finkel et al., 2005" startWordPosition="1732" endWordPosition="1735">collection of (overlapping) linear regression models, each matching a range of sentence lengths. For example, the first model DL1 is trained with sentences with length up to l1, i.e., l &lt; l1, the second model DL2 up to length l2 etc. During testing, sentences with length l E [1,l1] are decoded with DL1, sentences with length l E (l1, l2] with model DL2 etc. Each of these partial models is a linear fusion model as shown in (2). In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 = 00. 4 Experimental Procedure and Results Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. The exact type of pre-processing used depends on the metric used. For the plain lexical BLEU, we use lemmatization, stemming (of lemmas) and remove all non-content words, keeping only nouns, adjectives, verbs and adverbs. For computing semantic similarity scores, we don’t use stemming and keep only noun words, since we only have similarities between non-noun words. For the computation of semantic similarity we have created a dictionary c</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. R. Finkel, T. Grenager, and C. D. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="6940" citStr="Finkelstein et al., 2002" startWordPosition="1034" endWordPosition="1037">oduce exponential weights α in order to reduce the contribution of p(i) and p(j) in the similarity metric. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 [logpα(i)�p(j) + logp(i)�pα(j)J (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012b) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordIet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best results were obtained by the Vector metric (Patwardhan and Pedersen, 2006), which exploits the lexical information that is included in the WordNet glosses. This metric was incorporated to our proposed approach. All metrics were computed using the WordNet::Similarity module (Pedersen, 2005). 3 I-gram Regression Models Inspired by BLEU (Papineni et al., 2002), w</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
</authors>
<title>Methods for Using Textual Entailment in Open-Domain Question Answering.</title>
<date>2006</date>
<booktitle>In Proceedings ofthe 21st International Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>905--912</pages>
<contexts>
<context position="1454" citStr="Harabagiu and Hickl, 2006" startWordPosition="199" endWordPosition="202">oderate improvement on Task 6 of SemEval’12. Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set. 1 Introduction Recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task. Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Wordor term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sen</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>S. Harabagiu and A. Hickl. 2006. Methods for Using Textual Entailment in Open-Domain Question Answering. In Proceedings ofthe 21st International Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages 905–912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Unsupervised semantic similarity computation between terms using web documents.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>22</volume>
<issue>11</issue>
<contexts>
<context position="2324" citStr="Iosif and Potamianos, 2010" startWordPosition="336" endWordPosition="339">akis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Edmonds, 2007). Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and An</context>
<context position="5069" citStr="Iosif and Potamianos, 2010" startWordPosition="744" endWordPosition="747">d estimation. The assumption of this metric is that co-occurrence implies semantic similarity. During the past decade the web has been used for estimating the required probabilities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occurrence. However, these approaches have failed to obtain state-of-the-art results (Bollegala et al., 2007), unless “expensive” conjunctive AND queries are used for harvesting a corpus and then using this corpus to estimate similarity scores (Iosif and Potamianos, 2010). Recently, a scalable approach1 for harvesting a corpus has been proposed where web snippets are downloaded using individual queries for each word (Iosif and Potamianos, 2012b). Semantic similarity can then be estimated using the I(i, j) metric and within-snippet word co-occurrence frequencies. Under the maximum sense similarity assumption (Resnik, 1995), it is relatively easy to show that a (more) lexically-balanced corpus2 (as the one cre1The scalability of this approach has been demonstrated in (Iosif and Potamianos, 2012b) for a 10K vocabulary, here we extend it to the full 60K WordNet vo</context>
<context position="9384" citStr="Iosif and Potamianos, 2010" startWordPosition="1433" endWordPosition="1436">emantic similarity scores for bigrams (or higher) were not available. Thus, our list of features are the hit rates obtained by BLEU (for 1-, 2-, 3-, 4-grams) and the total semantic similarity (SS) score for 1-grams3. These features are then combined using a multiple linear regression model: 4 DL = a0 + � an Bn + a5 M1, (2) n=1 where DL is the estimated similarity, Bn is the BLEU hit rate for n-grams, M1 is the total semantic similarity score (SS) for non-matching 1-grams and an are the trainable parameters of the model. Motivated by evidence of cognitive scaling of semantic similarity scores (Iosif and Potamianos, 2010), we propose the use of a sigmoid function to scale DL sentence similarities. We have also observed in the SemEval data that the way humans rate sentence similarity is very much dependent on sentence length4. To capture the effect of length and cognitive scaling we propose next two modifications to the linear regression model. The sigmoid fusion scheme is described by the following equation: r 11 −1 DL L1 + exp Cas —1 IJ (3) \ ag / where we assume that sentence length l (average 3Note that the features are computed twice on each sentence in a forward and backward fashion (where the word order </context>
</contexts>
<marker>Iosif, Potamianos, 2010</marker>
<rawString>E. Iosif and A. Potamianos. 2010. Unsupervised semantic similarity computation between terms using web documents. IEEE Transactions on Knowledge and Data Engineering, 22(11):1637–1647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Minimum error semantic similarity using text corpora constructed from web queries.</title>
<date>2012</date>
<journal>IEEE Transactions on Knowledge and Data Engineering</journal>
<note>(submitted to).</note>
<contexts>
<context position="5244" citStr="Iosif and Potamianos, 2012" startWordPosition="770" endWordPosition="773">ities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occurrence. However, these approaches have failed to obtain state-of-the-art results (Bollegala et al., 2007), unless “expensive” conjunctive AND queries are used for harvesting a corpus and then using this corpus to estimate similarity scores (Iosif and Potamianos, 2010). Recently, a scalable approach1 for harvesting a corpus has been proposed where web snippets are downloaded using individual queries for each word (Iosif and Potamianos, 2012b). Semantic similarity can then be estimated using the I(i, j) metric and within-snippet word co-occurrence frequencies. Under the maximum sense similarity assumption (Resnik, 1995), it is relatively easy to show that a (more) lexically-balanced corpus2 (as the one cre1The scalability of this approach has been demonstrated in (Iosif and Potamianos, 2012b) for a 10K vocabulary, here we extend it to the full 60K WordNet vocabulary. 2According to this assumption the semantic similarity of two words can be estimated as the minimum pairwise similarity of their senses. The gist of the argument is t</context>
<context position="6584" citStr="Iosif and Potamianos, 2012" startWordPosition="978" endWordPosition="981">uce the semantic similarity estimation error of the mutual information metric I(i, j). This is also experimentally verified in (Iosif and Potamianos, 2012c). In addition, one can modify the mutual information metric to further reduce estimation error (for the theoretical foundation behind this see (Iosif and Potamianos, 2012a)). Specifically, one may introduce exponential weights α in order to reduce the contribution of p(i) and p(j) in the similarity metric. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 [logpα(i)�p(j) + logp(i)�pα(j)J (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012b) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordIet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best result</context>
</contexts>
<marker>Iosif, Potamianos, 2012</marker>
<rawString>E. Iosif and A. Potamianos. 2012a. Minimum error semantic similarity using text corpora constructed from web queries. IEEE Transactions on Knowledge and Data Engineering (submitted to).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Semsim: Resources for normalized semantic similarity computation using lexical networks.</title>
<date>2012</date>
<booktitle>Proc. of Eighth International Conference on Language Resources and Evaluation</booktitle>
<note>(to appear).</note>
<contexts>
<context position="5244" citStr="Iosif and Potamianos, 2012" startWordPosition="770" endWordPosition="773">ities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occurrence. However, these approaches have failed to obtain state-of-the-art results (Bollegala et al., 2007), unless “expensive” conjunctive AND queries are used for harvesting a corpus and then using this corpus to estimate similarity scores (Iosif and Potamianos, 2010). Recently, a scalable approach1 for harvesting a corpus has been proposed where web snippets are downloaded using individual queries for each word (Iosif and Potamianos, 2012b). Semantic similarity can then be estimated using the I(i, j) metric and within-snippet word co-occurrence frequencies. Under the maximum sense similarity assumption (Resnik, 1995), it is relatively easy to show that a (more) lexically-balanced corpus2 (as the one cre1The scalability of this approach has been demonstrated in (Iosif and Potamianos, 2012b) for a 10K vocabulary, here we extend it to the full 60K WordNet vocabulary. 2According to this assumption the semantic similarity of two words can be estimated as the minimum pairwise similarity of their senses. The gist of the argument is t</context>
<context position="6584" citStr="Iosif and Potamianos, 2012" startWordPosition="978" endWordPosition="981">uce the semantic similarity estimation error of the mutual information metric I(i, j). This is also experimentally verified in (Iosif and Potamianos, 2012c). In addition, one can modify the mutual information metric to further reduce estimation error (for the theoretical foundation behind this see (Iosif and Potamianos, 2012a)). Specifically, one may introduce exponential weights α in order to reduce the contribution of p(i) and p(j) in the similarity metric. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 [logpα(i)�p(j) + logp(i)�pα(j)J (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012b) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordIet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best result</context>
</contexts>
<marker>Iosif, Potamianos, 2012</marker>
<rawString>E. Iosif and A. Potamianos. 2012b. Semsim: Resources for normalized semantic similarity computation using lexical networks. Proc. of Eighth International Conference on Language Resources and Evaluation (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Iosif</author>
<author>A Potamianos</author>
</authors>
<title>Similarity computation using semantic networks created from webharvested data. Natural Language Engineering</title>
<date>2012</date>
<note>(submitted to).</note>
<contexts>
<context position="5244" citStr="Iosif and Potamianos, 2012" startWordPosition="770" endWordPosition="773">ities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occurrence. However, these approaches have failed to obtain state-of-the-art results (Bollegala et al., 2007), unless “expensive” conjunctive AND queries are used for harvesting a corpus and then using this corpus to estimate similarity scores (Iosif and Potamianos, 2010). Recently, a scalable approach1 for harvesting a corpus has been proposed where web snippets are downloaded using individual queries for each word (Iosif and Potamianos, 2012b). Semantic similarity can then be estimated using the I(i, j) metric and within-snippet word co-occurrence frequencies. Under the maximum sense similarity assumption (Resnik, 1995), it is relatively easy to show that a (more) lexically-balanced corpus2 (as the one cre1The scalability of this approach has been demonstrated in (Iosif and Potamianos, 2012b) for a 10K vocabulary, here we extend it to the full 60K WordNet vocabulary. 2According to this assumption the semantic similarity of two words can be estimated as the minimum pairwise similarity of their senses. The gist of the argument is t</context>
<context position="6584" citStr="Iosif and Potamianos, 2012" startWordPosition="978" endWordPosition="981">uce the semantic similarity estimation error of the mutual information metric I(i, j). This is also experimentally verified in (Iosif and Potamianos, 2012c). In addition, one can modify the mutual information metric to further reduce estimation error (for the theoretical foundation behind this see (Iosif and Potamianos, 2012a)). Specifically, one may introduce exponential weights α in order to reduce the contribution of p(i) and p(j) in the similarity metric. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 [logpα(i)�p(j) + logp(i)�pα(j)J (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012b) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordIet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best result</context>
</contexts>
<marker>Iosif, Potamianos, 2012</marker>
<rawString>E. Iosif and A. Potamianos. 2012c. Similarity computation using semantic networks created from webharvested data. Natural Language Engineering (submitted to).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>B J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="2713" citStr="Madnani and Dorr, 2010" startWordPosition="393" endWordPosition="396">rre and Edmonds, 2007). Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use </context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>N. Madnani and B. J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
<author>I Androutsopoulos</author>
</authors>
<title>Learning textual entailment using svms and string similarity measures.</title>
<date>2007</date>
<booktitle>In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>42--47</pages>
<contexts>
<context position="2944" citStr="Malakasiotis and Androutsopoulos, 2007" startWordPosition="424" endWordPosition="427">d Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computatio</context>
</contexts>
<marker>Malakasiotis, Androutsopoulos, 2007</marker>
<rawString>P. Malakasiotis and I. Androutsopoulos. 2007. Learning textual entailment using svms and string similarity measures. In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
</authors>
<title>Paraphrase recognition using machine learning to combine similarity measures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing ofAFNLP,</booktitle>
<pages>42--47</pages>
<contexts>
<context position="2975" citStr="Malakasiotis, 2009" startWordPosition="429" endWordPosition="430">uch as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics a linear regres</context>
</contexts>
<marker>Malakasiotis, 2009</marker>
<rawString>P. Malakasiotis. 2009. Paraphrase recognition using machine learning to combine similarity measures. In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing ofAFNLP, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Malandrakis</author>
<author>A Potamianos</author>
<author>E Iosif</author>
<author>S Narayanan</author>
</authors>
<title>Kernel models for affective lexicon creation.</title>
<date>2011</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>2977--2980</pages>
<contexts>
<context position="1715" citStr="Malandrakis et al., 2011" startWordPosition="241" endWordPosition="244"> research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task. Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Wordor term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Edmonds, 2007). Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamian</context>
</contexts>
<marker>Malandrakis, Potamianos, Iosif, Narayanan, 2011</marker>
<rawString>N. Malandrakis, A. Potamianos, E. Iosif, and S. Narayanan. 2011. Kernel models for affective lexicon creation. In Proc. Interspeech, pages 2977–2980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Meng</author>
<author>K-C Siu</author>
</authors>
<title>Semi-automatic acquisition of semantic structures for understanding domainspecific natural language queries.</title>
<date>2002</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="1654" citStr="Meng and Siu, 2002" startWordPosition="233" endWordPosition="236">et. 1 Introduction Recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task. Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Wordor term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Edmonds, 2007). Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) cor</context>
</contexts>
<marker>Meng, Siu, 2002</marker>
<rawString>H. Meng and K.-C. Siu. 2002. Semi-automatic acquisition of semantic structures for understanding domainspecific natural language queries. IEEE Transactions on Knowledge and Data Engineering, 14(1):172–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>W Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1998</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="6913" citStr="Miller and Charles, 1998" startWordPosition="1030" endWordPosition="1033">Specifically, one may introduce exponential weights α in order to reduce the contribution of p(i) and p(j) in the similarity metric. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 [logpα(i)�p(j) + logp(i)�pα(j)J (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012b) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordIet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best results were obtained by the Vector metric (Patwardhan and Pedersen, 2006), which exploits the lexical information that is included in the WordNet glosses. This metric was incorporated to our proposed approach. All metrics were computed using the WordNet::Similarity module (Pedersen, 2005). 3 I-gram Regression Models Inspired by BLEU</context>
</contexts>
<marker>Miller, Charles, 1998</marker>
<rawString>G. Miller and W. Charles. 1998. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal ofLexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="2212" citStr="Miller, 1990" startWordPosition="322" endWordPosition="323"> the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Edmonds, 2007). Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An on-line lexical database. International Journal ofLexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mirkin</author>
<author>L Specia</author>
<author>N Cancedda</author>
<author>I Dagan</author>
<author>M Dymetman</author>
<author>S Idan</author>
</authors>
<title>Source-language entailment modeling for translating unknown terms.</title>
<date>2009</date>
<booktitle>In Proceedings ofthe 47th Annual Meeting ofACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP,</booktitle>
<pages>791--799</pages>
<contexts>
<context position="1500" citStr="Mirkin et al., 2009" startWordPosition="206" endWordPosition="209"> the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set. 1 Introduction Recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task. Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Wordor term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Ed</context>
</contexts>
<marker>Mirkin, Specia, Cancedda, Dagan, Dymetman, Idan, 2009</marker>
<rawString>S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymetman, and S. Idan. 2009. Source-language entailment modeling for translating unknown terms. In Proceedings ofthe 47th Annual Meeting ofACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP, pages 791–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="3245" citStr="Papineni et al., 2002" startWordPosition="468" endWordPosition="471">hrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics a linear regression model to estimate sentence level semantic similarity. We also propose sigmoid scaling of similarity scores and sentence-length dependent modeling. The models are evaluated on the SemEval’12 sentence similarity task. 2 Semantic similarity between words In this secti</context>
<context position="7537" citStr="Papineni et al., 2002" startWordPosition="1125" endWordPosition="1128">Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordIet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best results were obtained by the Vector metric (Patwardhan and Pedersen, 2006), which exploits the lexical information that is included in the WordNet glosses. This metric was incorporated to our proposed approach. All metrics were computed using the WordNet::Similarity module (Pedersen, 2005). 3 I-gram Regression Models Inspired by BLEU (Papineni et al., 2002), we propose a simple regression model that combines evidence from two sources: number of n-gram matches and degree of similarity between non-matching words between two sentences. In order to incorporate a word semantic similarity metric into BLEU, we apply the following two-pass process: first lexical hits are identified and counted, and then the semantic similarity between n-grams not matched durrespond to all senses, i.e., the denominator of I(i, j) is overestimated causing large underestimation error for similarities between polysemous words. ˆ�(i,j) 566 ing the first pass is estimated. Al</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>T Pedersen</author>
</authors>
<title>Using WordNetbased context vectors to estimate the semantic relatedness of concepts.</title>
<date>2006</date>
<booktitle>In Proc. of the EACL Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7252" citStr="Patwardhan and Pedersen, 2006" startWordPosition="1081" endWordPosition="1085">erage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordIet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best results were obtained by the Vector metric (Patwardhan and Pedersen, 2006), which exploits the lexical information that is included in the WordNet glosses. This metric was incorporated to our proposed approach. All metrics were computed using the WordNet::Similarity module (Pedersen, 2005). 3 I-gram Regression Models Inspired by BLEU (Papineni et al., 2002), we propose a simple regression model that combines evidence from two sources: number of n-gram matches and degree of similarity between non-matching words between two sentences. In order to incorporate a word semantic similarity metric into BLEU, we apply the following two-pass process: first lexical hits are id</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>S. Patwardhan and T. Pedersen. 2006. Using WordNetbased context vectors to estimate the semantic relatedness of concepts. In Proc. of the EACL Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<date>2005</date>
<note>WordNet::Similarity. http://search.cpan.org/dist/ WordNet-Similarity/.</note>
<contexts>
<context position="7468" citStr="Pedersen, 2005" startWordPosition="1116" endWordPosition="1117">s (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordIet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best results were obtained by the Vector metric (Patwardhan and Pedersen, 2006), which exploits the lexical information that is included in the WordNet glosses. This metric was incorporated to our proposed approach. All metrics were computed using the WordNet::Similarity module (Pedersen, 2005). 3 I-gram Regression Models Inspired by BLEU (Papineni et al., 2002), we propose a simple regression model that combines evidence from two sources: number of n-gram matches and degree of similarity between non-matching words between two sentences. In order to incorporate a word semantic similarity metric into BLEU, we apply the following two-pass process: first lexical hits are identified and counted, and then the semantic similarity between n-grams not matched durrespond to all senses, i.e., the denominator of I(i, j) is overestimated causing large underestimation error for similarities betw</context>
</contexts>
<marker>Pedersen, 2005</marker>
<rawString>T. Pedersen. 2005. WordNet::Similarity. http://search.cpan.org/dist/ WordNet-Similarity/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Perez</author>
<author>E Alfonseca</author>
</authors>
<title>Application of the bleu algorithm for recognizing textual entailments.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Worshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="3205" citStr="Perez and Alfonseca, 2005" startWordPosition="462" endWordPosition="465"> is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics a linear regression model to estimate sentence level semantic similarity. We also propose sigmoid scaling of similarity scores and sentence-length dependent modeling. The models are evaluated on the SemEval’12 sentence similarity task. 2 Semanti</context>
</contexts>
<marker>Perez, Alfonseca, 2005</marker>
<rawString>D. Perez and E. Alfonseca. 2005. Application of the bleu algorithm for recognizing textual entailments. In Proceedings of the PASCAL Challenges Worshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxanomy.</title>
<date>1995</date>
<booktitle>In Proc. of International Joint Conferencefor Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="5426" citStr="Resnik, 1995" startWordPosition="797" endWordPosition="798">ver, these approaches have failed to obtain state-of-the-art results (Bollegala et al., 2007), unless “expensive” conjunctive AND queries are used for harvesting a corpus and then using this corpus to estimate similarity scores (Iosif and Potamianos, 2010). Recently, a scalable approach1 for harvesting a corpus has been proposed where web snippets are downloaded using individual queries for each word (Iosif and Potamianos, 2012b). Semantic similarity can then be estimated using the I(i, j) metric and within-snippet word co-occurrence frequencies. Under the maximum sense similarity assumption (Resnik, 1995), it is relatively easy to show that a (more) lexically-balanced corpus2 (as the one cre1The scalability of this approach has been demonstrated in (Iosif and Potamianos, 2012b) for a 10K vocabulary, here we extend it to the full 60K WordNet vocabulary. 2According to this assumption the semantic similarity of two words can be estimated as the minimum pairwise similarity of their senses. The gist of the argument is that although words often co-occur with their closest senses, word occurrences corated above) can significantly reduce the semantic similarity estimation error of the mutual informati</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxanomy. In Proc. of International Joint Conferencefor Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rinaldi</author>
<author>J Dowdall</author>
<author>K Kaljurand</author>
<author>M Hess</author>
<author>D Molla</author>
</authors>
<title>Exploiting paraphrases in a question answering system.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd International Workshop on Paraphrasing,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="3035" citStr="Rinaldi et al., 2003" startWordPosition="437" endWordPosition="440">arder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics a linear regression model to estimate sentence level semantic similarity. W</context>
</contexts>
<marker>Rinaldi, Dowdall, Kaljurand, Hess, Molla, 2003</marker>
<rawString>F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and D. Molla. 2003. Exploiting paraphrases in a question answering system. In Proceedings of the 2nd International Workshop on Paraphrasing, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="6887" citStr="Rubenstein and Goodenough, 1965" startWordPosition="1026" endWordPosition="1029"> (Iosif and Potamianos, 2012a)). Specifically, one may introduce exponential weights α in order to reduce the contribution of p(i) and p(j) in the similarity metric. The modified metric Ia(i, j), is defined as: Ia(i,j)=2 [logpα(i)�p(j) + logp(i)�pα(j)J (1) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012b) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordIet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best results were obtained by the Vector metric (Patwardhan and Pedersen, 2006), which exploits the lexical information that is included in the WordNet glosses. This metric was incorporated to our proposed approach. All metrics were computed using the WordNet::Similarity module (Pedersen, 2005). 3 I-gram Regressi</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J. B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>I Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="1406" citStr="Szpektor and Dagan, 2008" startWordPosition="193" endWordPosition="196">formation at the sentence level provides only moderate improvement on Task 6 of SemEval’12. Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set. 1 Introduction Recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task. Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Wordor term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level a</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>I. Szpektor and I. Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="11095" citStr="Toutanova et al., 2003" startWordPosition="1736" endWordPosition="1739">pping) linear regression models, each matching a range of sentence lengths. For example, the first model DL1 is trained with sentences with length up to l1, i.e., l &lt; l1, the second model DL2 up to length l2 etc. During testing, sentences with length l E [1,l1] are decoded with DL1, sentences with length l E (l1, l2] with model DL2 etc. Each of these partial models is a linear fusion model as shown in (2). In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 = 00. 4 Experimental Procedure and Results Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. The exact type of pre-processing used depends on the metric used. For the plain lexical BLEU, we use lemmatization, stemming (of lemmas) and remove all non-content words, keeping only nouns, adjectives, verbs and adverbs. For computing semantic similarity scores, we don’t use stemming and keep only noun words, since we only have similarities between non-noun words. For the computation of semantic similarity we have created a dictionary containing all the single-</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the web for synonyms: PMIIR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proc. of the European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="4637" citStr="Turney, 2001" startWordPosition="681" endWordPosition="682">n WordNet. Corpus-based metric: Given a corpus, the semantic similarity between two words, wi and wj, is estimated as their pointwise mutual information (Church and Hanks, 1990): I(i, j) = log ˆ�(i)ˆ�(j), where p(i) and p(j) are the occurrence probabilities of wi and wj, respectively, while the probability of their co-occurrence is denoted by p(i, j). These probabilities are computed according to maximum likelihood estimation. The assumption of this metric is that co-occurrence implies semantic similarity. During the past decade the web has been used for estimating the required probabilities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occurrence. However, these approaches have failed to obtain state-of-the-art results (Bollegala et al., 2007), unless “expensive” conjunctive AND queries are used for harvesting a corpus and then using this corpus to estimate similarity scores (Iosif and Potamianos, 2010). Recently, a scalable approach1 for harvesting a corpus has been proposed where web snippets are downloaded using individual queries for each word (Iosif and Potamiano</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. D. Turney. 2001. Mining the web for synonyms: PMIIR versus LSA on TOEFL. In Proc. of the European Conference on Machine Learning, pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Zanzotto</author>
<author>M Pennacchiotti</author>
<author>A Moschitti</author>
</authors>
<title>A machine-learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="2999" citStr="Zanzotto et al., 2009" startWordPosition="431" endWordPosition="434">entences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics a linear regression model to estimate s</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>F. Zanzotto, M. Pennacchiotti, and A. Moschitti. 2009. A machine-learning approach to textual entailment recognition. Natural Language Engineering, 15(4):551582.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>