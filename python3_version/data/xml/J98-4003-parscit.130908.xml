<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.959813">
Machine Transliteration
</title>
<author confidence="0.999631">
Kevin Knight* Jonathan Graehlt
</author>
<affiliation confidence="0.993783">
University of Southern California University of Southern California
</affiliation>
<bodyText confidence="0.9989495">
It is challenging to translate names and technical terms across languages with different alphabets
and sound inventories. These items are commonly transliterated, i.e., replaced with approxi-
mate phonetic equivalents. For example, &amp;quot;computer&amp;quot; in English comes out as &amp;quot;konpyuutaa&amp;quot; in
Japanese. Translating such items from Japanese back to English is even more challenging, and
of practical interest, as transliterated items make up the bulk of text phrases not found in bilin-
gual dictionaries. We describe and evaluate a method for performing backwards transliterations
by machine. This method uses a generative model, incorporating several distinct stages in the
transliteration process.
</bodyText>
<sectionHeader confidence="0.990272" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999754952380952">
One of the most frequent problems translators must deal with is translating proper
names and technical terms. For language pairs like Spanish/English, this presents no
great challenge: a phrase like Antonio Gil usually gets translated as Antonio Gil. How-
ever, the situation is more complicated for language pairs that employ very different
alphabets and sound systems, such as Japanese/English and Arabic/English. Phonetic
translation across these pairs is called transliteration. We will look at Japanese/English
transliteration in this article.
Japanese frequently imports vocabulary from other languages, primarily (but not
exclusively) from English. It has a special phonetic alphabet called katakana, which is
used primarily (but not exclusively) to write down foreign names and loanwords. The
katakana symbols are shown in Figure 1, with their Japanese pronunciations. The two
symbols shown in the lower right corner ( —, ) are used to lengthen any Japanese
vowel or consonant.
To write a word like golfbag in katakana, some compromises must be made. For
example, Japanese has no distinct L and R. sounds: the two English sounds collapse
onto the same Japanese sound. A similar compromise must be struck for English H
and F. Also, Japanese generally uses an alternating consonant-vowel structure, making
it impossible to pronounce LFB without intervening vowels. Katakana writing is a
syllabary rather than an alphabet—there is one symbol for ga (If), another for gi
eV ), another for gu ( 7 ), etc. So the way to write golfbag in katakana is &apos;&apos;&apos;&apos;-&apos; 7 &apos;Z Y ,
roughly pronounced go-ru-hu-ba-ggu. Here are a few more examples:
</bodyText>
<note confidence="0.549699333333333">
* USC/Information Sciences Institute, Marina del Rey, CA 90292 and USC/Computer Science
Department, Los Angeles, CA 90089
t USC/Computer Science Department, Los Angeles, CA 90089
</note>
<table confidence="0.79315725">
C) 1998 Association for Computational Linguistics
Computational Linguistics Volume 24, Number 4
7 (a) )3 (ka) -13- (sa) (ta) -3- (na) Js (ha) --q (ma) , (ra)
4 (i) .* (ki) &apos;,/ (shi) 1- (chi) -= (ni) t (hi) :: (mi) 9 (ri)
9 (u) (ku) x (su) &apos;Y (tsu) (nu) 7 (hu) 1,, (mu) A, (ru)
-I- (e) r(ke) -t (se) t (te) 4- (ne) ^ (he) )z (me) I. (re)
21- (o) (ko) ) (so) I- (to) ./ (no) 71-c (ho) -- (mo) Li (ro)
-&amp;quot;c (ba) t• (ga) --z (pa) -if (za) Y (da) 7 (a) .-r (ya) -r (ya)
E&apos; (bi) k.- (gi) e (pi) z% (ji) ---. (de) 4 (i) 3 (yo) a (yo)
7* (bu) Y (gu) i (pu) .A (zu) 1: (do) ,, (u) -.- (yu) = (yu)
-&lt; (be) Y (ge) -&lt;&apos; (pe) -V (ze) :/ (n) (e) V (v) •2
7g (N)) =&apos;. (go) A-Z (po) I (zo) 1-* (chi) A- (o) 7 (wa) —
</table>
<figureCaption confidence="0.9337">
Figure 1
</figureCaption>
<bodyText confidence="0.2655115">
Katakana symbols and their Japanese pronunciations.
Angela Johnson New York Times ice cream
</bodyText>
<equation confidence="0.417808375">
• -; :/ • 4 7 4 9 —
(a n jira jyo n son) (nyu u yo o ku ta i mu zu) (a i suku ri i mu)
Omaha Beach pro soccer Tonya Harding
7* ia -9- — -V • &apos;` 51.
(omahabiitchi) (purosakkaa) (t oonya haadingu)
ramp lamp casual fashion learn leader
, /;7 , V 7° t :si 7A. t 7 y a Y /- — -A 9 — Y —
(r anpu) (ranpu) (kajyuaruhasshyon) (chiimuriidaa)
</equation>
<bodyText confidence="0.9999240625">
Notice how the transliteration is more phonetic than orthographic; the letter h in
Johnson does not produce any katakana. Also, a dot-separator (o) is used to sepa-
rate words, but not consistently. And transliteration is clearly an information-losing
operation: ranpu could come from either lamp or ramp, while aisukuriimu loses the
distinction between ice cream and I scream.
Transliteration is not trivial to automate, but we will be concerned with an even
more challenging problem—going from katakana back to English, i.e., back-translit-
eration. Human translators can often &amp;quot;sound out&amp;quot; a katakana phrase to guess an
appropriate translation. Automating this process has great practical importance in
Japanese/English machine translation. Katakana phrases are the largest source of text
phrases that do not appear in bilingual dictionaries or training corpora (a.k.a. &amp;quot;not-
found words&amp;quot;), but very little computational work has been done in this area. Yamron
et al. (1994) briefly mention a pattern-matching approach, while Arbabi et al. (1994)
discuss a hybrid neural-net/expert-system approach to (forward) transliteration.
The information-losing aspect of transliteration makes it hard to invert. Here are
some problem instances, taken from actual newspaper articles:
</bodyText>
<equation confidence="0.341791333333333">
? ? ?
7 — A -..&amp;quot;-- r?,— 1- • &apos;..., a — V • 1/ *— 1: -- — 1&apos; — -3- I v 1-
(aasudee) (robaato shyoon renaado) (masutaazutoonamento)
</equation>
<page confidence="0.989817">
600
</page>
<note confidence="0.849578">
Knight and Graehl Machine Transliteration
</note>
<bodyText confidence="0.972852333333333">
English translations appear later in this article.
Here are a few observations about back-transliteration that give an idea of the
difficulty of the task:
</bodyText>
<listItem confidence="0.856585944444444">
• Back-transliteration is less forgiving than transliteration. There are many
ways to write an English word like switch in katakana, all equally valid,
but we do not have this flexibility in the reverse direction. For example,
we cannot drop the t in switch, nor can we write arture when we mean
archer. Forward-direction flexibility wreaks havoc with dictionary-based
solutions, because no dictionary will contain all katakana variants.
• Back-transliteration is harder than romanization. A romanization scheme
simply sets down a method for writing a foreign script in roman letters.
For example, to romanize 7 , we look up each symbol in Figure 1
and substitute characters. This substitution gives us (romanized) anj
but not (translated) angela. Romanization schemes are usually
deterministic and invertible, although small ambiguities can arise. We
discuss some wrinkles in Section 3.4.
• Finally, not all katakana phrases can be &amp;quot;sounded out&amp;quot; by back-
transliteration. Some phrases are shorthand, e.g., 7 — 7 &amp;quot; (waapuro)
should be translated as word processing. Others are onomatopoetic and
difficult to translate. These cases must be solved by techniques other
than those described here.
</listItem>
<bodyText confidence="0.998575">
The most desirable feature of an automatic back-transliterator is accuracy. If pos-
sible, our techniques should also be:
</bodyText>
<listItem confidence="0.999241142857143">
• portable to new language pairs like Arabic/English with minimal effort,
possibly reusing resources.
• robust against errors introduced by optical character recognition.
• relevant to speech recognition situations in which the speaker has a
heavy foreign accent.
• able to take textual (topical/syntactic) context into account, or at least be
able to return a ranked list of possible English translations.
</listItem>
<bodyText confidence="0.9588452">
Like most problems in computational linguistics, this one requires full world
knowledge for a 100% solution. Choosing between Katarina and Catalina (both good
guesses for 9 ) might even require detailed knowledge of geography and figure
skating. At that level, human translators find the problem quite difficult as well, so
we only aim to match or possibly exceed their performance.
</bodyText>
<sectionHeader confidence="0.851199" genericHeader="method">
2. A Modular Learning Approach
</sectionHeader>
<bodyText confidence="0.9580944">
Bilingual glossaries contain many entries mapping katakana phrases onto English
phrases, e.g., (aircraft carrier 2 7 is t &amp;quot;I&apos; 9 7 ). It is possible to automatically
analyze such pairs to gain enough knowledge to accurately map new katakana phrases
that come along, and this learning approach travels well to other language pairs. A
naive approach to finding direct correspondences between English letters and katakana
</bodyText>
<page confidence="0.993207">
601
</page>
<note confidence="0.437244">
Computational Linguistics Volume 24, Number 4
</note>
<bodyText confidence="0.996376714285714">
symbols, however, suffers from a number of problems. One can easily wind up with
a system that proposes iskrym as a back-transliteration of aisukuriimu. Taking letter
frequencies into account improves this to a more plausible-looking isclim. Moving to
real words may give is crime: the i corresponds to ai, the s corresponds to su, etc.
Unfortunately, the correct answer here is ice cream.
After initial experiments along these lines, we stepped back and built a generative
model of the transliteration process, which goes like this:
</bodyText>
<listItem confidence="0.9989268">
1. An English phrase is written.
2. A translator pronounces it in English.
3. The pronunciation is modified to fit the Japanese sound inventory.
4. The sounds are converted into katakana.
5. Katakana is written.
</listItem>
<bodyText confidence="0.9931977">
This divides our problem into five subproblems. Fortunately, there are techniques
for coordinating solutions to such subproblems, and for using generative models in the
reverse direction. These techniques rely on probabilities and Bayes&apos; theorem. Suppose
we build an English phrase generator that produces word sequences according to
some probability distribution P(w). And suppose we build an English pronouncer that
takes a word sequence and assigns it a set of pronunciations, again probabilistically,
according to some P(pi w). Given a pronunciation p, we may want to search for the
word sequence w that maximizes P(w Ip). Bayes&apos; theorem lets us equivalently maximize
P(w) • P(plw), exactly the two distributions we have modeled.
Extending this notion, we settled down to build five probability distributions:
</bodyText>
<listItem confidence="0.996824666666667">
1. P(w) — generates written English word sequences.
2. P(elw) — pronounces English word sequences.
3. P(j1e) — converts English sounds into Japanese sounds.
4. P(k1j) — converts Japanese sounds to katakana writing.
5. P(olk) — introduces misspellings caused by optical character recognition
(OCR).
</listItem>
<bodyText confidence="0.9605715">
Given a katakana string o observed by OCR, we want to find the English word
sequence w that maximizes the sum, over all e, j, and k, of
</bodyText>
<equation confidence="0.936138">
P(w) • WeI w) • P(jle) • P(k1j) • P(01k)
</equation>
<listItem confidence="0.957140222222222">
Following Pereira and Riley (1997), we implement P(w) in a weighted finite-state ac-
ceptor (WFSA) and we implement the other distributions in weighted finite-state trans-
ducers (WFSTs). A WFSA is a state/transition diagram with weights and symbols on
the transitions, making some output sequences more likely than others. A WFST is a
WFSA with a pair of symbols on each transition, one input and one output. Inputs
and outputs may include the empty symbol E. Also following Pereira and Riley (1997),
we have implemented a general composition algorithm for constructing an integrated
model P(xlz) from models P(xly) and P(y1z), treating WFSAs as VVFSTs with identical
inputs and outputs. We use this to combine an observed katakana string with each
</listItem>
<page confidence="0.994352">
602
</page>
<note confidence="0.360985">
Knight and Graehl Machine Transliteration
</note>
<bodyText confidence="0.999563583333334">
of the models in turn. The result is a large WFSA containing all possible English
translations.
We have implemented two algorithms for extracting the best translations. The first
is Dijkstra&apos;s shortest-path graph algorithm (Dijkstra 1959). The second is a recently
discovered k-shortest-paths algorithm (Eppstein 1994) that makes it possible for us to
identify the top k translations in efficient 0(m + n log n + kn) time, where the WFSA
contains n states and m arcs.
The approach is modular. We can test each engine independently and be confident
that their results are combined correctly. We do no pruning, so the final WFSA contains
every solution, however unlikely. The only approximation is the Viterbi one, which
searches for the best path through a WFSA instead of the best sequence (i.e., the same
sequence does not receive bonus points for appearing more than once).
</bodyText>
<sectionHeader confidence="0.90996" genericHeader="method">
3. Probabilistic Models
</sectionHeader>
<bodyText confidence="0.9999385">
This section describes how we designed and built each of our five models. For consis-
tency, we continue to print written English word sequences in italics (golf ball), English
sound sequences in all capitals (G AA L F B AO L), Japanese sound sequences in lower
case (goruhubooru) and katakana sequences naturally
</bodyText>
<subsectionHeader confidence="0.99995">
3.1 Word Sequences
</subsectionHeader>
<bodyText confidence="0.969699571428571">
The first model generates scored word sequences, the idea being that ice cream should
score higher than ice creme, which should score higher than aice kreem. We adopted a
simple unigram scoring method that multiplies the scores of the known words and
phrases in a sequence. Our 262,000-entry frequency list draws its words and phrases
from the Wall Street Journal corpus, an on-line English name list, and an on-line
gazetteer of place names.&apos; A portion of the WFSA looks like this:
los / 0.000087
</bodyText>
<subsubsectionHeader confidence="0.475134">
angeles
</subsubsectionHeader>
<bodyText confidence="0.9869257">
month I 0.000992
An ideal word sequence model would look a bit different. It would prefer exactly
those strings which are actually grist for Japanese transliterators. For example, people
rarely transliterate auxiliary verbs, but surnames are often transliterated. We have
approximated such a model by removing high-frequency words like has, an, are, am,
were, their, and does, plus unlikely words corresponding to Japanese sound bites, like
coup and oh.
We also built a separate word sequence model containing only English first and
last names. If we know (from context) that the transliterated phrase is a personal name,
this model is more precise.
</bodyText>
<subsectionHeader confidence="0.99996">
3.2 Words to English Sounds
</subsectionHeader>
<bodyText confidence="0.9994455">
The next WFST converts English word sequences into English sound sequences. We
use the English phoneme inventory from the on-line CMU Pronunciation Dictio-
</bodyText>
<footnote confidence="0.338931">
1 Available from the ACL Data Collection Initiative.
</footnote>
<bodyText confidence="0.526677">
federal I 0.0013
</bodyText>
<page confidence="0.927099">
603
</page>
<note confidence="0.396599">
Computational Linguistics Volume 24, Number 4
</note>
<bodyText confidence="0.9997775">
nary, minus the stress marks.2 This gives a total of 40 sounds, including 14 vowel
sounds (e.g., AA, AE, UW), 25 consonant sounds (e.g., K, HH, R), plus one special symbol
(PAUSE). The dictionary has pronunciations for 110,000 words, and we organized a
tree-based WFST from it:
</bodyText>
<subsectionHeader confidence="0.456942">
E:E
</subsectionHeader>
<bodyText confidence="0.999459571428571">
Note that we insert an optional PAUSE between word pronunciations.
We originally thought to build a general letter-to-sound WFST (Divay and Vitale
1997), on the theory that while wrong (overgeneralized) pronunciations might occa-
sionally be generated, Japanese transliterators also mispronounce words. However,
our letter-to-sound WFST did not match the performance of Japanese transliterators,
and it turns out that mispronunciations are modeled adequately in the next stage of
the cascade.
</bodyText>
<subsectionHeader confidence="0.984776">
3.3 English Sounds to Japanese Sounds
</subsectionHeader>
<bodyText confidence="0.999991">
Next, we map English sound sequences onto Japanese sound sequences. This is an in-
herently information-losing process, as English R and L sounds collapse onto Japanese
r, the 14 English vowel sounds collapse onto the 5 Japanese vowel sounds, etc. We
face two immediate problems:
</bodyText>
<listItem confidence="0.999499">
1. What is the target Japanese sound inventory?
2. How can we build a WFST to perform the sequence mapping?
</listItem>
<bodyText confidence="0.996303333333333">
An obvious target inventory is the Japanese syllabary itself, written down in
katakana (e.g., -= ) or a roman equivalent (e.g., ni). With this approach, the English
sound K corresponds to one of t (ka), (ki), (ku), (ke), or (ko), depend-
ing on its context. Unfortunately, because katakana is a syllabary, we would be un-
able to express an obvious and useful generalization, namely that English K usually
corresponds to Japanese k, independent of context. Moreover, the correspondence of
Japanese katakana writing to Japanese sound sequences is not perfectly one-to-one (see
Section 3.4), so an independent sound inventory is well-motivated in any case. Our
Japanese sound inventory includes 39 symbols: 5 vowel sounds, 33 consonant sounds
(including doubled consonants like kk), and one special symbol (pause). An English
sound sequence like (P R OW PAUSE S AA K ER) might map onto a Japanese sound
sequence like (p u r o pause s a kk a a). Note that long Japanese vowel sounds
</bodyText>
<footnote confidence="0.818924">
2 The CMU Pronunciation Dictionary can be found on-line at
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
</footnote>
<page confidence="0.991382">
604
</page>
<bodyText confidence="0.949076545454545">
Knight and Graehl Machine Transliteration
are written with two symbols (a a) instead of just one (aa). This scheme is attractive
because Japanese sequences are almost always longer than English sequences.
Our WFST is learned automatically from 8,000 pairs of English/Japanese sound
sequences, e.g., ((S AA K ER) (s a kk a a)). We were able to produce these pairs
by manipulating a small English-katakana glossary. For each glossary entry, we con-
verted English words into English sounds using the model described in the previous
section, and we converted katakana words into Japanese sounds using the model we
describe in the next section. We then applied the estimation-maximization (EM) al-
gorithm (Baum 1972; Dempster, Laird, and Rubin 1977) to generate symbol-mapping
probabilities, shown in Figure 2. Our EM training goes like this:
</bodyText>
<listItem confidence="0.588193">
1. For each English/Japanese sequence pair, compute all possible
</listItem>
<bodyText confidence="0.99943">
alignments between their elements. In our case, an alignment is a
drawing that connects each English sound with one or more Japanese
sounds, such that all Japanese sounds are covered and no lines cross. For
example, there are two ways to align the pair ( (L OW) &lt;-&gt; (r o 0)):
</bodyText>
<equation confidence="0.981994666666667">
L OW L OW
/\
r o 0 r 0 0
</equation>
<bodyText confidence="0.7819522">
In this case, the alignment on the left is intuitively preferable. The
algorithm learns such preferences.
2. For each pair, assign an equal weight to each of its alignments, such that
those weights sum to 1. In the case above, each alignment gets a weight
of 0.5.
</bodyText>
<listItem confidence="0.944727923076923">
3. For each of the 40 English sounds, count up instances of its different
mappings, as observed in all alignments of all pairs. Each alignment
contributes counts in proportion to its own weight.
4. For each of the 40 English sounds, normalize the scores of the Japanese
sequences it maps to, so that the scores sum to 1. These are the
symbol-mapping probabilities shown in Figure 2.
5. Recompute the alignment scores. Each alignment is scored with the
product of the scores of the symbol mappings it contains. Figure 3 shows
sample alignments found automatically through EM training.
6. Normalize the alignment scores. Scores for each pair&apos;s alignments should
sum to 1.
7. Repeat 3-6 until the symbol-mapping probabilities converge.
We then build a WFST directly from the symbol-mapping probabilities:
</listItem>
<bodyText confidence="0.8071375">
PAUSE:pause
Our WFST has 99 states and 283 arcs.
</bodyText>
<page confidence="0.974602">
605
</page>
<figure confidence="0.926521277777778">
Computational Linguistics
e 3 Nile) e 3 Nile)
AA o 0.566 EY e e 0.641
a 0.382 a 0.122
a a 0.024 e 0.114
o o 0.018 e i 0.080
a i 0.014
AE a 0.942
y a 0.046
F h 0.623
h u 0.331
hh 0.019
a h u 0.010
All a 0.486
o 0.169
e 0.134
i 0.111
u 0.076
G g 0.598
g u 0.304
gg u 0.059
gg 0.010
AO o 0.671
o o 0.257
a 0.047 HH h 0.959
AW a u 0.830 w 0.014
a w .0.095 IH i 0.908
o o 0.027 e 0.071
a o 0.020 TY i i 0.573
a 0.014 i 0.317
AY a i 0.864 e 0.074
i 0.073 e e 0.016
a 0.018 JH j 0.329
a i y 0.018 j y 0.328
B b 0.802 j i 0.129
b u 0.185 jj i 0.066
</figure>
<table confidence="0.978319928571429">
CH ch y 0.277 e j i 0.057
ch 0.240 z 0.032
tch i 0.199 g 0.018
ch i 0.159 jj 0.012
tch 0.038 e 0.012
ch y u 0.021 K k 0.528
tch y 0.020 k u 0.238
D d 0.535 kk u 0.150
d o 0.329 kk 0.043
dd o 0.053 k i 0.015
j 0.032 k y 0.012
DH z 0.670 L r 0.621
z u 0.125 r u 0.362
j 0.125 M in 0.653
</table>
<figure confidence="0.8743565">
a z 0.080 is u 0.207
EH e 0.901 n 0.123
a 0.069 n is 0.011
ER a a 0.719 N n 0.978
a 0.081 NG n g u 0.743
a r 0.063 n 0.220
e r 0.042 n g 0.023
0 r 0.029
</figure>
<table confidence="0.99173629787234">
Volume 24, Number 4
3 P (2le) e Nile)
OW o 0.516 UH u 0.794
o o 0.456 u u 0.098
o u 0.011 dd 0.034
DY o i 0.828 a 0.030
ooi 0.057 0.026
0.029 UW u u 0.550
o y 0.029 0.302
o 0.027 y u u 0.109
o o y 0.014 y u 0.021
o o 0.014 V b 0.810
0.649 b u 0.150
p u 0.218 0.015
pp u 0.085 W w 0.693
pp 0.045 0.194
PAUSE pause 1.000 0.039
0.661 0.027
a 0.170 a 0.015
0.076 0.012
u 0.042 0.652
u r 0.016 0.220
a r 0.012 y u 0.050
s u 0.539 0.048
0.269 0.016
sh 0.109 Z z 0.296
0.028 z u 0.283
as 0.014 0.107
SH sh y 0.475 s u 0.103
sh 0.175 0.073
ssh y u 0.166 a 0.036
ssh y 0.088 0.018
sh i 0.029 0.015
ssh 0.027 0.013
sh y u 0.015 0.011
0.463 sh 0.011
t o 0.305 ZH j y 0.324
tt o 0.103 sh i 0.270
ch 0.043 j i 0.173
tt 0.021 0.135
ts 0.020 ajyu 0.027
ts u 0.011 sh y 0.027
TB s u 0.418 0.027
0.303 a j 1 0.016
sh 0.130
ch 0.038
0.029
</table>
<figureCaption confidence="0.965454">
Figure 2
</figureCaption>
<bodyText confidence="0.8802216">
English sounds (in capitals) with probabilistic mappings to Japanese sound sequences (in
lower case), as learned by estimation-maximization. Only mappings with conditional
probabilities greater than 1% are shown, so the figures may not sum to 1.
We have also built models that allow individual English sounds to be &amp;quot;swallowed&amp;quot;
(i.e., produce zero Japanese sounds). However, these models are expensive to compute
(many more alignments) and lead to a vast number of hypotheses during WFST com-
position. Furthermore, in disallowing &amp;quot;swallowing,&amp;quot; we were able to automatically
remove hundreds of potentially harmful pairs from our training set, e.g., ( (B AA R
B ER SH AA P) 4-* (b aab a a) ). Because no alignments are possible, such pairs
are skipped by the learning algorithm; cases like these must be solved by dictionary
</bodyText>
<page confidence="0.992099">
606
</page>
<figure confidence="0.991531">
Knight and Graehl Machine Transliteration
biscuit
English sound sequence: B IH S K AH T
/ X X
Japanese sound sequence: BISUKETT 0
divider
A
</figure>
<figureCaption confidence="0.953899">
Figure 3
</figureCaption>
<bodyText confidence="0.9653024">
Alignments between English and Japanese sound sequences, as determined by EM training.
Best alignments are shown for the English words biscuit, divider, and filter.
lookup anyway. Only two pairs failed to align when we wished they had—both in-
volved turning English Y UW into Japanese u, as in ((Y UW K AH L EY L 1Y) +-4 (u k
urere)).
Note also that our model translates each English sound without regard to context.
We have also built context-based models, using decision trees recoded as WFSTs. For
example, at the end of a word, English T is likely to come out as (t o) rather than (t).
However, context-based models proved unnecessary for back-transliteration. They are
more useful for English-to-Japanese forward transliteration.
</bodyText>
<subsectionHeader confidence="0.991922">
3.4 Japanese Sounds to Katakana
</subsectionHeader>
<bodyText confidence="0.997867714285714">
To map Japanese sound sequences like (m o ot a a) onto katakana sequences like
), we manually constructed two WFSTs. Composed together, they yield an
integrated WFST with 53 states and 303 arcs, producing a katakana inventory contain-
ing 81 symbols, including the dot-separator (.). The first WFST simply merges long
Japanese vowel sounds into new symbols aa, ii, uu, ee, and oo. The second WFST
maps Japanese sounds onto katakana symbols. The basic idea is to consume a whole
syllable worth of sounds before producing any katakana. For example:
</bodyText>
<figure confidence="0.989807444444444">
o: 3
English sound sequence: D IH V AY D
I / /X \
Japanese sound Sequence: DIBAIDAA
filter
English sound sequence: F IH L T ER
I I X
Japanese sound sequence: H I R UT A
ER
</figure>
<page confidence="0.652396">
607
</page>
<note confidence="0.478826">
Computational Linguistics Volume 24, Number 4
</note>
<bodyText confidence="0.9983496">
This fragment shows one kind of spelling variation in Japanese: long vowel sounds
(00) are usually written with a long vowel mark ( 21&amp;quot; ) but are sometimes written
with repeated katakana ( 71- 71.). We combined corpus analysis with guidelines from a
Japanese textbook (Jorden and Chaplin 1976) to turn up many spelling variations and
unusual katakana symbols:
</bodyText>
<listItem confidence="0.999237125">
• the sound sequence (j i) is usually written , but occasionally 1&amp;quot; .
• (g u a) is usually Y&apos;7 , but occasionally .
• (1.1 0 0) is variously , 9 * , or with a special old-style katakana
for wo.
• (y e) may be , 4 , or 4 .
• (w i) is either &amp;quot;or 9 4.
• (n y e) is a rare sound sequence, but is written when it occurs.
• (t y u) is rarer than (ch y u), but is written 5-- -&apos;- when it occurs.
</listItem>
<bodyText confidence="0.931735666666667">
and so on.
Spelling variation is clearest in cases where an English word like switch shows
up transliterated variously ( 4 ‘2 4 &apos;Y , 7 9 4 &apos;Y ) in different dictionaries.
Treating these variations as an equivalence class enables us to learn general sound
mappings even if our bilingual glossary adheres to a single narrow spelling conven-
tion. We do not, however, generate all katakana sequences with this model; for exam-
ple, we do not output strings that begin with a subscripted vowel katakana. So this
model also serves to filter out some ill-formed katakana sequences, possibly proposed
by optical character recognition.
</bodyText>
<subsectionHeader confidence="0.998636">
3.5 Katakana to OCR
</subsectionHeader>
<bodyText confidence="0.999610142857143">
Perhaps uncharitably, we can view optical character recognition (OCR) as a device that
garbles perfectly good katakana sequences. Typical confusions made by our commer-
cial OCR system include t: for 71. for , 7 for 7, and 7 for I. To generate
pre-OCR text, we collected 19,500 characters worth of katakana words, stored them in
a file, and printed them out. To generate post-OCR text, we OCR&apos;d the printouts. We
then ran the EM algorithm to determine symbol-mapping (&amp;quot;garbling&amp;quot;) probabilities.
Here is part of that table:
</bodyText>
<table confidence="0.937700875">
k o P(olk)
0.492
e 0.434
e 0.042
7 0.011
e e 1.000
,., , 0.964
1 , 0.036
</table>
<page confidence="0.983298">
608
</page>
<note confidence="0.660281">
Knight and Graehl Machine Transliteration
</note>
<bodyText confidence="0.9935865">
This model outputs a superset of the 81 katakana symbols, including spurious
quote marks, alphabetic symbols, and the numeral 7.3
</bodyText>
<subsectionHeader confidence="0.637634">
4. A Sample Back-transliteration
</subsectionHeader>
<bodyText confidence="0.999375">
We can now use the models to do a sample back-transliteration. We start with a
katakana phrase as observed by OCR. We then serially compose it with the models, in
reverse order. Each intermediate stage is a WFSA that encodes many possibilities. The
final stage contains all back-transliterations suggested by the models, and we finally
extract the best one.
We start with the masutaazutoonamento problem from Section 1. Our OCR ob-
serves:
</bodyText>
<equation confidence="0.551196">
Y
</equation>
<bodyText confidence="0.928849428571429">
This string has two recognition errors: (ku) for (t a), and (chi) for (ha).
We turn the string into a chained 12-state/11-arc WFSA and compose it with the P(Iclo)
model. This yields a fatter 12-state/15-arc WFSA, which accepts the correct spelling
at a lower probability. Next comes the POO model, which produces a 28-state/31-arc
WFSA whose highest-scoring sequence is:
masutaazutoochimento
Next comes P(elj), yielding a 62-state/241-arc WFSA whose best sequence is:
</bodyText>
<equation confidence="0.686392">
M AE S T AE AE DH UH T AO AO CH IH M EH N T AO
</equation>
<bodyText confidence="0.957790166666667">
Next to last comes P(w le), which results in a 2982-state/4601-arc WFSA whose best
sequence (out of roughly three hundred million) is:
masters tone am ent awe
This English string is closest phonetically to the Japanese, but we are willing to trade
phonetic proximity for more sensical English; we rescore this WFSA by composing it
with P(w) and extract the best translation:
</bodyText>
<subsectionHeader confidence="0.534523">
masters tournament
</subsectionHeader>
<bodyText confidence="0.9955898">
Other Section 1 examples (aasudee and robaato shyoon renaado) are translated cor-
rectly as earth day and robert sean leonard.
We may also be interested in the k best translations. In fact, after any composition,
we can inspect several high-scoring sequences using the algorithm of Eppstein (1994).
Given the following katakana input phrase:
</bodyText>
<footnote confidence="0.563057">
3 A more thorough OCR model would train on a wide variety of fonts and photocopy distortions. In
practice, such degradations can easily overwhelm even the better OCR systems.
</footnote>
<page confidence="0.993053">
609
</page>
<table confidence="0.8921196875">
Computational Linguistics Volume 24, Number 4
(pronounced anj irahoorasuterunaito), the top five English sound sequences are
P(kle)
AE N JH IH R AE HH AO AO R AE S T EH R UH N AE IH T AO 0.00753
AE N JH IH R AE HH AO AO R AE S T EH R UH N AY T AO 0.00742
AE N JH IH L AE HH AO AO R AE S T EH R UH N AE IH T AO 0.00735
AE N JH IH R AE HH AO AO R AE S T EH L UH N AE IH T AO 0.00735
AE N JH IH R AE HH AO AO L AE S T EH R UH N AE IH T AO 0.00735
Notice that different R and L combinations are visible in this list. The top five final
translations are:
angela forrestal knight P(w) * p(k1w)
angela forrester knight 3.6e-20
angela forest el knight 8.5e-21
angela forester knight 2.7e-21
angela forest air knight 2.5e-21
1.7e-21
</table>
<bodyText confidence="0.9976512">
Inspecting the k-best list is useful for diagnosing problems with the models. If the
right answer appears low in the list, then some numbers are probably off somewhere.
If the right answer does not appear at all, then one of the models may be missing a
word or suffer from some kind of brittleness. A k-best list can also be used as input
to a later context-based disambiguator, or as an aid to a human translator.
</bodyText>
<sectionHeader confidence="0.997672" genericHeader="method">
5. Experiments
</sectionHeader>
<bodyText confidence="0.993237181818182">
We have performed two large-scale experiments, one using a full-language P(w) model,
and one using a personal name language model.
In the first experiment, we extracted 1,449 unique katakana phrases from a corpus
of 100 short news articles. Of these, 222 were missing from an on-line 100,000-entry
bilingual dictionary. We back-transliterated these 222 phrases. Many of the translations
are perfect: technical program, sex scandal, omaha beach, new york times, ramon diaz. Others
are close: tanya harding, nickel simpson, danger washington, world cap. Some miss the
mark: nancy care again, plus occur, patriot miss rea1.4 While it is difficult to judge overall
accuracy—some of the phrases are onomatopoetic, and others are simply too hard even
for good human translators—it is easier to identify system weaknesses, and most of
these lie in the P(w) model. For example, nancy kerrigan should be preferred over nancy
care again.
In a second experiment, we took (non-OCR) katakana versions of the names of 100
U.S. politicians, e.g.: &apos; 1 (jyon.buroo), 7)1&amp;quot;1&amp;quot; 7 (aruhonsu.
damatto), and -Q. 4 &apos; 7 .s/ (maiku.dewain). We back-transliterated these by ma-
chine and asked four human subjects to do the same. These subjects were native
English speakers and news-aware; we gave them brief instructions. The results were
as in Table 1.
There is room for improvement on both sides. Being English speakers, the human
subjects were good at English name spelling and U.S. politics, but not at Japanese
phonetics. A native Japanese speaker might be expert at the latter but not the former.
People who are expert in all of these areas, however, are rare.
</bodyText>
<subsubsectionHeader confidence="0.411167">
4 Correct translations are tonya harding, nicole simpson, denzel washington, world cup, nancy kerrigan, pro
soccer, and patriot missile.
</subsubsectionHeader>
<page confidence="0.985557">
610
</page>
<note confidence="0.732754">
Knight and Graehl Machine Transliteration
</note>
<tableCaption confidence="0.7088435">
Table 1
Accuracy of back-transliteration by human subjects and machine.
</tableCaption>
<figure confidence="0.9707371">
correct
(e.g., spencer abraham / spencer abraham)
phonetically equivalent, but misspelled
(e.g., richard brian / richard bryan)
incorrect
(e.g., olin hatch / orren hatch)
Human Machine
27% 64%
7% 12%
66% 24%
</figure>
<bodyText confidence="0.9996694">
On the automatic side, many errors can be corrected. A first-name/last-name
model would rank richard bryan more highly than richard brian. A bigram model would
prefer orren hatch over olin hatch. Other errors are due to unigram training problems, or
more rarely, incorrect or brittle phonetic models. For example, Long occurs much more
often than Ron in newspaper text, and our word selection does not exclude phrases
like Long Island. So we get long wyden instead of ron wyden. One way to fix these prob-
lems is by manually changing unigram probabilities. Reducing P(long) by a factor of
ten solves the problem while maintaining a high score for P(long rongu).
Despite these problems, the machine&apos;s performance is impressive. When word
separators (p) are removed from the katakana phrases, rendering the task exceedingly
difficult for people, the machine&apos;s performance is unchanged. In other words, it offers
the same top-scoring translations whether or not the separators are present; how-
ever, their presence significantly cuts down on the number of alternatives considered,
improving efficiency. When we use OCR, 7% of katakana tokens are misrecognized,
affecting 50% of test strings, but translation accuracy only drops from 64% to 52%.
</bodyText>
<sectionHeader confidence="0.995954" genericHeader="discussions">
6. Discussion
</sectionHeader>
<bodyText confidence="0.999425941176471">
In a 1947 memorandum, Weaver (1955) wrote:
One naturally wonders if the problem of translation could conceivably
be treated as a problem of cryptography. When I look at an article in
Russian, I say: &amp;quot;This is really written in English, but it has been coded
in some strange symbols. I will now proceed to decode.&amp;quot; (p. 18)
Whether this is a useful perspective for machine translation is debatable (Brown et
al. 1993; Knoblock 1996)—however, it is a dead-on description of transliteration. Most
katakana phrases really are English, ready to be decoded.
We have presented a method for automatic back-transliteration which, while far
from perfect, is highly competitive. It also achieves the objectives outlined in Section 1.
It ports easily to new language pairs; the P(w) and P(ejw) models are entirely reusable,
while other models are learned automatically. It is robust against OCR noise, in a rare
example of high-level language processing being useful (necessary, even) in improving
low-level OCR.
There are several directions for improving accuracy. The biggest problem is that
raw English frequency counts are not the best indication of whether a word is a possi-
ble source for transliteration. Alternative data collection methods must be considered.
</bodyText>
<page confidence="0.992209">
611
</page>
<note confidence="0.70199">
Computational Linguistics Volume 24, Number 4
</note>
<bodyText confidence="0.999805631578947">
We may also consider changes to the model sequence itself. As we have pre-
sented it, our hypothetical human transliterator produces Japanese sounds from En-
glish sounds only, without regard for the original English spelling. This means that
English homonyms will produce exactly the same katakana strings. In reality, though,
transliterators will sometimes key off spelling, so that tonya and tanya produce toonya
and taanya. It might pay to carry along some spelling information in the English
pronunciation lattices.
Sentential context should be useful for determining correct translations. It is often
clear from a Japanese sentence whether a katakana phrase is a person, an institution,
or a place. In many cases it is possible to narrow things further—given the phrase
&amp;quot;such-and-such, Arizona,&amp;quot; we can restrict our P(w) model to include only those cities
and towns in Arizona.
It is also interesting to consider transliteration for other languages. In Arabic, for
example, it is more difficult to identify candidates for transliteration because there is
no distinct, explicit alphabet that marks them. Furthermore, Arabic is usually written
without vowels, so we must generate vowel sounds from scratch in order to produce
correct English.
Finally, it may be possible to embed phonetic-shift models inside speech recogniz-
ers, to explicitly adjust for heavy foreign accents.
</bodyText>
<sectionHeader confidence="0.991545" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.987180333333333">
We would like to thank Alton Earl Ingram,
Yolanda Gil, Bonnie Glover Stalls, Richard
Whitney, Kenji Yamada, and the anonymous
reviewers for their helpful comments. We
would also like to thank our sponsors at the
Department of Defense.
</bodyText>
<sectionHeader confidence="0.991163" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914372881356">
Arbabi, Mansur, Scott M. Fischthal,
Vincent C. Cheng, and Elizabeth Bart.
1994. Algorithms for Arabic name
transliteration. IBM Journal of Research and
Development, 38(2):183-193.
Baum, Leonard E. 1972. An inequality and
associated maximization technique in
statistical estimation of probabilistic
functions of a Markov process.
Inequalities, 3:1-8.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263-311.
Dempster, A. P., N. M. Laird, and D. B.
Rubin. 1977. Maximum likelihood from
incomplete via the EM algorithm. Journal
of the Royal Statistical Society, 39(B):1-38.
Dijkstra, Edsgar W. 1959. A note on two
problems in connexion with graphs.
Numerische Mathematik, 1:269-271.
Divay, Michel and Anthony J. Vitale. 1997.
Algorithms for grapheme-phoneme
translation for English and
French:Applications. Computational
Linguistics, 23(4):495-524.
Eppstein, David. 1994. Finding the k
shortest paths. In Proceedings of the
35thSymposium on the Foundations of
Computer Science, pages 154-165.
Jorden, Eleanor H. and Hamako I. Chaplin.
1976. Reading Japanese. Yale University
Press, New Haven.
Knoblock, Craig. 1996. Trends and
controversies: Statistical versus
knowledge-based machine translation.
IEEE Expert, 11(2):12-18.
Pereira, Fernando C. N. and Michael Riley.
1997. Speech recognition by composition
of weighted finite automata. In E. Roche
and Y. Schabes, editors, Finite-State
Language Processing, pages 431-453. MIT
Press.
Weaver, Warren. 1955. Translation. In
William N. Locke and A. Donald Booth,
editors, Machine Translation of Languages.
Technology Press of MIT and John Wiley
&amp; Sons, New York (1949 memorandum,
reprinted, quoting a 1947 letter from
Weaver to Norbert Wiener).
Yamron, Jonathan, James Cant, Anne
Demedts, Taiko Dietzel, and Yoshiko Ito.
1994. The automatic component of the
LINGSTAT machine-aided translation
system. In Proceedings of the ARPA
Workshop on Human Language Technology,
pages 163-168. Morgan Kaufmann.
</reference>
<page confidence="0.997499">
612
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982583">
<title confidence="0.999989">Machine Transliteration</title>
<author confidence="0.999982">Kevin Knight Jonathan Graehlt</author>
<affiliation confidence="0.998912">University of Southern California University of Southern California</affiliation>
<abstract confidence="0.99791175">It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. For example, &amp;quot;computer&amp;quot; in English comes out as &amp;quot;konpyuutaa&amp;quot; in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mansur Arbabi</author>
<author>Scott M Fischthal</author>
<author>Vincent C Cheng</author>
<author>Elizabeth Bart</author>
</authors>
<title>Algorithms for Arabic name transliteration.</title>
<date>1994</date>
<journal>IBM Journal of Research and Development,</journal>
<pages>38--2</pages>
<contexts>
<context position="4809" citStr="Arbabi et al. (1994)" startWordPosition="807" endWordPosition="810">t we will be concerned with an even more challenging problem—going from katakana back to English, i.e., back-transliteration. Human translators can often &amp;quot;sound out&amp;quot; a katakana phrase to guess an appropriate translation. Automating this process has great practical importance in Japanese/English machine translation. Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a. &amp;quot;notfound words&amp;quot;), but very little computational work has been done in this area. Yamron et al. (1994) briefly mention a pattern-matching approach, while Arbabi et al. (1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration. The information-losing aspect of transliteration makes it hard to invert. Here are some problem instances, taken from actual newspaper articles: ? ? ? 7 — A -..&amp;quot;-- r?,— 1- • &apos;..., a — V • 1/ *— 1: -- — 1&apos; — -3- I v 1- (aasudee) (robaato shyoon renaado) (masutaazutoonamento) 600 Knight and Graehl Machine Transliteration English translations appear later in this article. Here are a few observations about back-transliteration that give an idea of the difficulty of the task: • Back-transliteration is less forgiving t</context>
</contexts>
<marker>Arbabi, Fischthal, Cheng, Bart, 1994</marker>
<rawString>Arbabi, Mansur, Scott M. Fischthal, Vincent C. Cheng, and Elizabeth Bart. 1994. Algorithms for Arabic name transliteration. IBM Journal of Research and Development, 38(2):183-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<pages>3--1</pages>
<contexts>
<context position="16473" citStr="Baum 1972" startWordPosition="2656" endWordPosition="2657">a). This scheme is attractive because Japanese sequences are almost always longer than English sequences. Our WFST is learned automatically from 8,000 pairs of English/Japanese sound sequences, e.g., ((S AA K ER) (s a kk a a)). We were able to produce these pairs by manipulating a small English-katakana glossary. For each glossary entry, we converted English words into English sounds using the model described in the previous section, and we converted katakana words into Japanese sounds using the model we describe in the next section. We then applied the estimation-maximization (EM) algorithm (Baum 1972; Dempster, Laird, and Rubin 1977) to generate symbol-mapping probabilities, shown in Figure 2. Our EM training goes like this: 1. For each English/Japanese sequence pair, compute all possible alignments between their elements. In our case, an alignment is a drawing that connects each English sound with one or more Japanese sounds, such that all Japanese sounds are covered and no lines cross. For example, there are two ways to align the pair ( (L OW) &lt;-&gt; (r o 0)): L OW L OW /\ r o 0 r 0 0 In this case, the alignment on the left is intuitively preferable. The algorithm learns such preferences. </context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, Leonard E. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. Inequalities, 3:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="31388" citStr="Brown et al. 1993" startWordPosition="5382" endWordPosition="5385"> alternatives considered, improving efficiency. When we use OCR, 7% of katakana tokens are misrecognized, affecting 50% of test strings, but translation accuracy only drops from 64% to 52%. 6. Discussion In a 1947 memorandum, Weaver (1955) wrote: One naturally wonders if the problem of translation could conceivably be treated as a problem of cryptography. When I look at an article in Russian, I say: &amp;quot;This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.&amp;quot; (p. 18) Whether this is a useful perspective for machine translation is debatable (Brown et al. 1993; Knoblock 1996)—however, it is a dead-on description of transliteration. Most katakana phrases really are English, ready to be decoded. We have presented a method for automatic back-transliteration which, while far from perfect, is highly competitive. It also achieves the objectives outlined in Section 1. It ports easily to new language pairs; the P(w) and P(ejw) models are entirely reusable, while other models are learned automatically. It is robust against OCR noise, in a rare example of high-level language processing being useful (necessary, even) in improving low-level OCR. There are seve</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edsgar W Dijkstra</author>
</authors>
<title>A note on two problems in connexion with graphs.</title>
<date>1959</date>
<journal>Numerische Mathematik,</journal>
<pages>1--269</pages>
<contexts>
<context position="11032" citStr="Dijkstra 1959" startWordPosition="1781" endWordPosition="1782"> outputs may include the empty symbol E. Also following Pereira and Riley (1997), we have implemented a general composition algorithm for constructing an integrated model P(xlz) from models P(xly) and P(y1z), treating WFSAs as VVFSTs with identical inputs and outputs. We use this to combine an observed katakana string with each 602 Knight and Graehl Machine Transliteration of the models in turn. The result is a large WFSA containing all possible English translations. We have implemented two algorithms for extracting the best translations. The first is Dijkstra&apos;s shortest-path graph algorithm (Dijkstra 1959). The second is a recently discovered k-shortest-paths algorithm (Eppstein 1994) that makes it possible for us to identify the top k translations in efficient 0(m + n log n + kn) time, where the WFSA contains n states and m arcs. The approach is modular. We can test each engine independently and be confident that their results are combined correctly. We do no pruning, so the final WFSA contains every solution, however unlikely. The only approximation is the Viterbi one, which searches for the best path through a WFSA instead of the best sequence (i.e., the same sequence does not receive bonus </context>
</contexts>
<marker>Dijkstra, 1959</marker>
<rawString>Dijkstra, Edsgar W. 1959. A note on two problems in connexion with graphs. Numerische Mathematik, 1:269-271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Divay</author>
<author>Anthony J Vitale</author>
</authors>
<title>Algorithms for grapheme-phoneme translation for English and French:Applications.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--4</pages>
<contexts>
<context position="13917" citStr="Divay and Vitale 1997" startWordPosition="2249" endWordPosition="2252"> English phoneme inventory from the on-line CMU Pronunciation Dictio1 Available from the ACL Data Collection Initiative. federal I 0.0013 603 Computational Linguistics Volume 24, Number 4 nary, minus the stress marks.2 This gives a total of 40 sounds, including 14 vowel sounds (e.g., AA, AE, UW), 25 consonant sounds (e.g., K, HH, R), plus one special symbol (PAUSE). The dictionary has pronunciations for 110,000 words, and we organized a tree-based WFST from it: E:E Note that we insert an optional PAUSE between word pronunciations. We originally thought to build a general letter-to-sound WFST (Divay and Vitale 1997), on the theory that while wrong (overgeneralized) pronunciations might occasionally be generated, Japanese transliterators also mispronounce words. However, our letter-to-sound WFST did not match the performance of Japanese transliterators, and it turns out that mispronunciations are modeled adequately in the next stage of the cascade. 3.3 English Sounds to Japanese Sounds Next, we map English sound sequences onto Japanese sound sequences. This is an inherently information-losing process, as English R and L sounds collapse onto Japanese r, the 14 English vowel sounds collapse onto the 5 Japan</context>
</contexts>
<marker>Divay, Vitale, 1997</marker>
<rawString>Divay, Michel and Anthony J. Vitale. 1997. Algorithms for grapheme-phoneme translation for English and French:Applications. Computational Linguistics, 23(4):495-524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Eppstein</author>
</authors>
<title>Finding the k shortest paths.</title>
<date>1994</date>
<booktitle>In Proceedings of the 35thSymposium on the Foundations of Computer Science,</booktitle>
<pages>154--165</pages>
<contexts>
<context position="11112" citStr="Eppstein 1994" startWordPosition="1791" endWordPosition="1792">, we have implemented a general composition algorithm for constructing an integrated model P(xlz) from models P(xly) and P(y1z), treating WFSAs as VVFSTs with identical inputs and outputs. We use this to combine an observed katakana string with each 602 Knight and Graehl Machine Transliteration of the models in turn. The result is a large WFSA containing all possible English translations. We have implemented two algorithms for extracting the best translations. The first is Dijkstra&apos;s shortest-path graph algorithm (Dijkstra 1959). The second is a recently discovered k-shortest-paths algorithm (Eppstein 1994) that makes it possible for us to identify the top k translations in efficient 0(m + n log n + kn) time, where the WFSA contains n states and m arcs. The approach is modular. We can test each engine independently and be confident that their results are combined correctly. We do no pruning, so the final WFSA contains every solution, however unlikely. The only approximation is the Viterbi one, which searches for the best path through a WFSA instead of the best sequence (i.e., the same sequence does not receive bonus points for appearing more than once). 3. Probabilistic Models This section descr</context>
<context position="26242" citStr="Eppstein (1994)" startWordPosition="4501" endWordPosition="4502">ose best sequence (out of roughly three hundred million) is: masters tone am ent awe This English string is closest phonetically to the Japanese, but we are willing to trade phonetic proximity for more sensical English; we rescore this WFSA by composing it with P(w) and extract the best translation: masters tournament Other Section 1 examples (aasudee and robaato shyoon renaado) are translated correctly as earth day and robert sean leonard. We may also be interested in the k best translations. In fact, after any composition, we can inspect several high-scoring sequences using the algorithm of Eppstein (1994). Given the following katakana input phrase: 3 A more thorough OCR model would train on a wide variety of fonts and photocopy distortions. In practice, such degradations can easily overwhelm even the better OCR systems. 609 Computational Linguistics Volume 24, Number 4 (pronounced anj irahoorasuterunaito), the top five English sound sequences are P(kle) AE N JH IH R AE HH AO AO R AE S T EH R UH N AE IH T AO 0.00753 AE N JH IH R AE HH AO AO R AE S T EH R UH N AY T AO 0.00742 AE N JH IH L AE HH AO AO R AE S T EH R UH N AE IH T AO 0.00735 AE N JH IH R AE HH AO AO R AE S T EH L UH N AE IH T AO 0.0</context>
</contexts>
<marker>Eppstein, 1994</marker>
<rawString>Eppstein, David. 1994. Finding the k shortest paths. In Proceedings of the 35thSymposium on the Foundations of Computer Science, pages 154-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor H Jorden</author>
<author>Hamako I Chaplin</author>
</authors>
<title>Reading Japanese.</title>
<date>1976</date>
<publisher>Yale University Press,</publisher>
<location>New Haven.</location>
<contexts>
<context position="22695" citStr="Jorden and Chaplin 1976" startWordPosition="3878" endWordPosition="3881">s. The basic idea is to consume a whole syllable worth of sounds before producing any katakana. For example: o: 3 English sound sequence: D IH V AY D I / /X \ Japanese sound Sequence: DIBAIDAA filter English sound sequence: F IH L T ER I I X Japanese sound sequence: H I R UT A ER 607 Computational Linguistics Volume 24, Number 4 This fragment shows one kind of spelling variation in Japanese: long vowel sounds (00) are usually written with a long vowel mark ( 21&amp;quot; ) but are sometimes written with repeated katakana ( 71- 71.). We combined corpus analysis with guidelines from a Japanese textbook (Jorden and Chaplin 1976) to turn up many spelling variations and unusual katakana symbols: • the sound sequence (j i) is usually written , but occasionally 1&amp;quot; . • (g u a) is usually Y&apos;7 , but occasionally . • (1.1 0 0) is variously , 9 * , or with a special old-style katakana for wo. • (y e) may be , 4 , or 4 . • (w i) is either &amp;quot;or 9 4. • (n y e) is a rare sound sequence, but is written when it occurs. • (t y u) is rarer than (ch y u), but is written 5-- -&apos;- when it occurs. and so on. Spelling variation is clearest in cases where an English word like switch shows up transliterated variously ( 4 ‘2 4 &apos;Y , 7 9 4 &apos;Y ) </context>
</contexts>
<marker>Jorden, Chaplin, 1976</marker>
<rawString>Jorden, Eleanor H. and Hamako I. Chaplin. 1976. Reading Japanese. Yale University Press, New Haven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Knoblock</author>
</authors>
<title>Trends and controversies: Statistical versus knowledge-based machine translation.</title>
<date>1996</date>
<journal>IEEE Expert,</journal>
<pages>11--2</pages>
<contexts>
<context position="31404" citStr="Knoblock 1996" startWordPosition="5386" endWordPosition="5387">dered, improving efficiency. When we use OCR, 7% of katakana tokens are misrecognized, affecting 50% of test strings, but translation accuracy only drops from 64% to 52%. 6. Discussion In a 1947 memorandum, Weaver (1955) wrote: One naturally wonders if the problem of translation could conceivably be treated as a problem of cryptography. When I look at an article in Russian, I say: &amp;quot;This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.&amp;quot; (p. 18) Whether this is a useful perspective for machine translation is debatable (Brown et al. 1993; Knoblock 1996)—however, it is a dead-on description of transliteration. Most katakana phrases really are English, ready to be decoded. We have presented a method for automatic back-transliteration which, while far from perfect, is highly competitive. It also achieves the objectives outlined in Section 1. It ports easily to new language pairs; the P(w) and P(ejw) models are entirely reusable, while other models are learned automatically. It is robust against OCR noise, in a rare example of high-level language processing being useful (necessary, even) in improving low-level OCR. There are several directions f</context>
</contexts>
<marker>Knoblock, 1996</marker>
<rawString>Knoblock, Craig. 1996. Trends and controversies: Statistical versus knowledge-based machine translation. IEEE Expert, 11(2):12-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Speech recognition by composition of weighted finite automata.</title>
<date>1997</date>
<booktitle>Finite-State Language Processing,</booktitle>
<pages>431--453</pages>
<editor>In E. Roche and Y. Schabes, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10036" citStr="Pereira and Riley (1997)" startWordPosition="1624" endWordPosition="1627">two distributions we have modeled. Extending this notion, we settled down to build five probability distributions: 1. P(w) — generates written English word sequences. 2. P(elw) — pronounces English word sequences. 3. P(j1e) — converts English sounds into Japanese sounds. 4. P(k1j) — converts Japanese sounds to katakana writing. 5. P(olk) — introduces misspellings caused by optical character recognition (OCR). Given a katakana string o observed by OCR, we want to find the English word sequence w that maximizes the sum, over all e, j, and k, of P(w) • WeI w) • P(jle) • P(k1j) • P(01k) Following Pereira and Riley (1997), we implement P(w) in a weighted finite-state acceptor (WFSA) and we implement the other distributions in weighted finite-state transducers (WFSTs). A WFSA is a state/transition diagram with weights and symbols on the transitions, making some output sequences more likely than others. A WFST is a WFSA with a pair of symbols on each transition, one input and one output. Inputs and outputs may include the empty symbol E. Also following Pereira and Riley (1997), we have implemented a general composition algorithm for constructing an integrated model P(xlz) from models P(xly) and P(y1z), treating </context>
</contexts>
<marker>Pereira, Riley, 1997</marker>
<rawString>Pereira, Fernando C. N. and Michael Riley. 1997. Speech recognition by composition of weighted finite automata. In E. Roche and Y. Schabes, editors, Finite-State Language Processing, pages 431-453. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren Weaver</author>
</authors>
<title>memorandum, reprinted, quoting a 1947 letter from Weaver to Norbert Wiener).</title>
<date>1955</date>
<booktitle>Machine Translation of Languages. Technology Press of MIT</booktitle>
<editor>Translation. In William N. Locke and A. Donald Booth, editors,</editor>
<publisher>and John Wiley &amp; Sons,</publisher>
<location>New York</location>
<contexts>
<context position="31010" citStr="Weaver (1955)" startWordPosition="5318" endWordPosition="5319">roblems, the machine&apos;s performance is impressive. When word separators (p) are removed from the katakana phrases, rendering the task exceedingly difficult for people, the machine&apos;s performance is unchanged. In other words, it offers the same top-scoring translations whether or not the separators are present; however, their presence significantly cuts down on the number of alternatives considered, improving efficiency. When we use OCR, 7% of katakana tokens are misrecognized, affecting 50% of test strings, but translation accuracy only drops from 64% to 52%. 6. Discussion In a 1947 memorandum, Weaver (1955) wrote: One naturally wonders if the problem of translation could conceivably be treated as a problem of cryptography. When I look at an article in Russian, I say: &amp;quot;This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.&amp;quot; (p. 18) Whether this is a useful perspective for machine translation is debatable (Brown et al. 1993; Knoblock 1996)—however, it is a dead-on description of transliteration. Most katakana phrases really are English, ready to be decoded. We have presented a method for automatic back-transliteration which, while far from p</context>
</contexts>
<marker>Weaver, 1955</marker>
<rawString>Weaver, Warren. 1955. Translation. In William N. Locke and A. Donald Booth, editors, Machine Translation of Languages. Technology Press of MIT and John Wiley &amp; Sons, New York (1949 memorandum, reprinted, quoting a 1947 letter from Weaver to Norbert Wiener).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Yamron</author>
<author>James Cant</author>
<author>Anne Demedts</author>
<author>Taiko Dietzel</author>
<author>Yoshiko Ito</author>
</authors>
<title>The automatic component of the LINGSTAT machine-aided translation system.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>163--168</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="4737" citStr="Yamron et al. (1994)" startWordPosition="797" endWordPosition="800">n ice cream and I scream. Transliteration is not trivial to automate, but we will be concerned with an even more challenging problem—going from katakana back to English, i.e., back-transliteration. Human translators can often &amp;quot;sound out&amp;quot; a katakana phrase to guess an appropriate translation. Automating this process has great practical importance in Japanese/English machine translation. Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a. &amp;quot;notfound words&amp;quot;), but very little computational work has been done in this area. Yamron et al. (1994) briefly mention a pattern-matching approach, while Arbabi et al. (1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration. The information-losing aspect of transliteration makes it hard to invert. Here are some problem instances, taken from actual newspaper articles: ? ? ? 7 — A -..&amp;quot;-- r?,— 1- • &apos;..., a — V • 1/ *— 1: -- — 1&apos; — -3- I v 1- (aasudee) (robaato shyoon renaado) (masutaazutoonamento) 600 Knight and Graehl Machine Transliteration English translations appear later in this article. Here are a few observations about back-transliteration that give an idea o</context>
</contexts>
<marker>Yamron, Cant, Demedts, Dietzel, Ito, 1994</marker>
<rawString>Yamron, Jonathan, James Cant, Anne Demedts, Taiko Dietzel, and Yoshiko Ito. 1994. The automatic component of the LINGSTAT machine-aided translation system. In Proceedings of the ARPA Workshop on Human Language Technology, pages 163-168. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>