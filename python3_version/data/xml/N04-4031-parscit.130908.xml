<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.185013">
<title confidence="0.959447">
Computational Linkuistics: word triggers across hyperlinks
</title>
<author confidence="0.99109">
Dragomir R. Radev , Hong Qi , Daniel Tam , Adam Winkel
</author>
<affiliation confidence="0.9956135">
School of Information and Department of EECS
University of Michigan
</affiliation>
<address confidence="0.948854">
Ann Arbor, MI 48109-1092
</address>
<email confidence="0.998718">
radev,hqi,dtam,winkela @umich.edu
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911933333333">
It is known that context words tend to be self-
triggers, that is, the probability of a content
word to appear more than once in a document,
given that it already appears once, is signifi-
cantly higher than the probability of the first oc-
currence. We look at self-triggerability across
hyperlinks on the Web. We show that the prob-
ability of a word to appear in a Web docu-
ment depends on the presence of in doc-
uments pointing to . In Document Model-
ing, we will propose the use of a correction fac-
tor, , which indicates how much more likely
a word is to appear in a document given that
another document containing the same word is
linked to it.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998601272727273">
Given the size of the Web, it is intuitively very hard to find
a given page of interest by just following links. Classic
results have shown however, that the link structure of the
Web is not random. Various models have been proposed
including power law distributions (the “rich get richer”
model), and lexical models. In this paper, we will in-
vestigate how the presence of a given word in a given
Web document affects the presence of the same word
in documents linked to . We will use the term Compu-
tational Linkuistics to describe the study of hyperlinks for
Document Modeling and Information Retrieval purposes.
</bodyText>
<subsectionHeader confidence="0.999499">
1.1 Link structure of the Web
</subsectionHeader>
<bodyText confidence="0.999753115384616">
Random graphs have been studied by Erd˝os and R´enyi
(Erd˝os and R´enyi, 1960). In a random graph, edges are
added sequentially with both vertices of a new edge cho-
sen randomly.
The diameter of the Web (that is, the average number
of links from any given page to another) has been found to
be a constant (approximately , where
is the number of documents on the Web and is the
average document out-degree (i.e., the number of pages
linked from the document). This result was described in
(Barab´asi and Albert, 1999) and is based on a corpus of
800 M web pages). This estimate of would entail that
in a random graph model, the size of the Web would be
approximately which is 10 M times its actual size.
Clearly, a random graph model is not an appropriate de-
scription of the Web. Instead, it has been shown that due
to preferential attachment (Barab´asi and Albert, 1999),
the out-degree distribution follows a power law. The pref-
erential model makes it more likely that a new random
edge will connect two vertices that already have a high
degree. Specifically, the degree of pages is distributed
according to , where is a constant
strictly greater than 0. (Note this is different from ,
the distribution of out-degree on random graphs.) As a
result, random walks on the Web graph soon reach well-
connected nodes.
</bodyText>
<subsectionHeader confidence="0.999301">
1.2 Lexical structure of the Web
</subsectionHeader>
<bodyText confidence="0.998892117647059">
Davison (Davison, 2000) discusses the topical locality
hypothesis, namely that new edges are more likely to con-
nect pages that are semantically related. In Davison’s ex-
periment, semantic and link distances between pairs of
pages from a 100 K page corpus were computed. Davison
describes results associating TF*IDF cosine similarity
(Salton and McGill, 1983) and link hop distance. He re-
ports that the cosine similarity between pages selected at
random from his corpus is 0.02 whereas that number in-
creases significantly for topologically related pages: 0.31
for pages from the same Web domain, 0.23 for linked
pages, and 0.19 for sibling pages (pages pointed to by the
same page).
Menczer (Menczer, 2001) introduces the link-content
conjecture states that the semantic content of a web
page can be inferred from the pages that point to it.
Menczer uses a corpus of 373 K pages and employs a
non-linear least squares fit to come up with a seman-
tic model connecting cosine-based semantic similarity
and the link distance between two
pages and (the shortest directed distance on the hy-
pertext graph from to ). Menczer reports that and
are connected via a power law:
. represents noise level in similarity.
Menczer reports empirically determined values of the
parameters of the fit as follows: , , and
.
Menczer’s results further confirm Davison’s observa-
tions that pages adjacent in hyperlink space to a given
page are semantically connected.
Our idea has been to investigate the circumstances un-
der which the semantic similarity between linked pages
can be explained in terms of the presence of individual
words across links.
</bodyText>
<subsectionHeader confidence="0.994741">
1.3 Document modeling
</subsectionHeader>
<bodyText confidence="0.99985375">
In the computational linguistics and speech communities,
the notion of a language model is used to describe a prob-
ability distribution over words. Since a cluster of docu-
ments contains a subset of an entire language, a document
model is a special case of a language model. As such, it
can be expressed as a conditional probability distribution
indicating how likely a word is to appear in a document
given some context (e.g., other similar documents, the
topic of the document, etc.). Language models are used
in speech recognition (Chen and Goodman, 1996), docu-
ment indexing (Bookstein and Swanson, 1974; Croft and
Harper, 1979) and information retrieval (Ponte and Croft,
1998).
Document models are a special class of language mod-
els. One property of document models is that they can
be used to predict some lexical properties of textual doc-
uments, e.g., the frequency of a certain word. Mosteller
and Wallace (Mosteller and Wallace, 1984) discovered
that content words are ”bursty” - the appearance of a con-
tent word significantly increases the probability that the
word would appear again. Church and his colleagues
(Church and Gale, 1995; Church, 2000) describe doc-
ument models based on the distribution of the frequen-
cies of individual words over large document collections.
In (Church and Gale, 1995), Church and Gale compare
document models based on the Poisson distribution, the
2-Poisson distribution (Bookstein and Swanson, 1974),
as well as generic Poisson mixtures. A Poisson mix-
ture is described by , where
for a given integer non-negative value
of .
Church and Gale empirically show that Poisson mix-
tures are a more accurate model for describing the dis-
tribution of words in documents within a corpus. They
obtain the best fits with the Negative Binomial model and
the K-mixture (both special cases of Poisson mixtures)
(Church and Gale, 1995). In the Negative Binomial case,
(which is the Gamma distribution)
whereas in the K-mixture, ,
where is Dirac’s delta function.
Our study focuses on modeling across hyperlinks.
Documents linked across the web are often written by
people with different backgrounds and language usage
pattern.
</bodyText>
<subsectionHeader confidence="0.964938">
1.4 Link-based document models
</subsectionHeader>
<bodyText confidence="0.9999456">
In Church et al.’s experiments, the documents being mod-
eled do not have hyperlinks between them. When model-
ing hyperlinked corpora, it is important to decompose the
document model into link-free and link-dependent com-
ponents. The link-free component predicts the probabil-
ity of a word appearing in a document regardless
of the documents that point to . The link-dependent
part makes use of a particular incarnation of the link-
content conjecture, namely micro link-content depen-
dency (MLD), which we will propose in this paper.
</bodyText>
<subsectionHeader confidence="0.863073">
1.5 Our framework
</subsectionHeader>
<bodyText confidence="0.999962416666667">
In traditional Information Retrieval, the main object that
is represented, and searched, is the document. In our
setup, we will be looking at the hyperlink between two
documents as the main object to retrieve. If a page
points to page via link , then we will consider as
the object to index and the two pages that it links as fea-
tures describing the link.
For our experiments, we used the 2-Gigabyte wt2g cor-
pus (Hawking, 2002) which contains 247,491 Web docu-
ments connected with 3,118,248 links. These documents
contain 948,036 unique words (after Porter-style stem-
ming).
</bodyText>
<sectionHeader confidence="0.988173" genericHeader="method">
2 A link-based document model
</sectionHeader>
<bodyText confidence="0.998527108695652">
It is well known that the distributions of words in text de-
pend on many factors such as genre, topic, author, etc.
Certain words with high content has been found to ”trig-
ger” other words to appear. Interestingly, the hyperlinks
which connect the text on the Web may also affect the
word distributions in the hypertext. For example, if page
that contains education points to page , then we
would expect a higher probability of seeing education in
page than in a random page. This experiment was de-
signed to discover how the links between pages can trig-
ger words and change the word distributions.
For each stemmed word in wt2g, we compute the fol-
lowing numbers:
PagesContainingWord = how many pages in the col-
lection contain the word.
OutgoingLinks = the total number of outgoing links in
all the pages that contain the word.
LinkedPagesContainingWord = how many of the
linked pages contain the word.
For the latter two measures, only the links inside the
collection were considered.
The probability of a word appearing in a random
page is computed as
where Total Pages = 247,491. If contains the word
and points to a new page , then the probability of the
word appearing in is computed as
For instance, in the wt2g corpus there are 55,654 pages
that contain the word each, and these pages have a total
of 46,163 links pointing to the pages in the collection,
15,815 of which have the word each. Therefore, its prior
probability is , and its posterior probability
is .
We are interested in the ratio of posterior over prior
probability for each stemmed word and would like to see
if there is any interesting relationship between this ratio
and other linguistic features.
We will look at the ratio (the
link effect) which describes how much more likely a
linked page is to contain a given word than a random
page.
IDF (Inverse Document Frequency) values based on
the wt2g corpus are also computed. We compute IDF us-
ing the formula , where
is the document frequency (fraction of all docu-
ments containing ) and is the number of documents
in the collection.
</bodyText>
<subsectionHeader confidence="0.633021">
2.1 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99925992">
Table 1 shows the different measures for the 2000 words
with lowest IDF. Each line shows the average values on a
chunk of 100 words.
As one can see in the table, the posterior probabilities
are always higher than the prior probabilities. Hypoth-
esis testing shows that the difference between prior and
posterior is statistically significant, which verifies our as-
sumption. It is noticeable that the link effect has the same
trend as the IDF values. The correlation coefficient of
these two columns is 0.7112. It is customary to use IDF
as an indicator of words’ content. Low IDF usually im-
plies a low content value. We would like to investigate
whether link effect can be used instead of IDF for cer-
tain IR tasks. Let’s consider the sample words between
and american on table 1. Intuitively, american has more
content than between, but the later has an IDF of 2.37,
higher than that of the former (2.36). However, their link
effects agree with intuition: american: 1.97, one standard
deviation higher than between: 1.40.
Table 2 compares the link effects for two ranges of
sample words with roughly the same IDF values within
each range. It shows the words in the order of IDF and of
Link Effect ( ). As one can see, the link effect tends to
be high for content words when IDF value alone cannot
discriminate the words.
</bodyText>
<table confidence="0.989902">
sorted by sorted by sorted by sorted by
IDF link effect IDF link effect
word IDF word word IDF word
human 2.981 close 1.675 centuri 3.988 extend 2.085
accord 2.983 among 1.770 interact 3.990 beyond 2.477
perform 2.984 further 1.796 introduct 3.993 front 2.606
close 2.985 expect 1.864 front 3.994 centuri 2.713
press 2.992 accord 1.922 travel 3.997 elimin 2.753
applic 2.992 assist 1.962 elimin 4.009 damag 2.757
expect 2.997 human 2.093 opinion 4.013 introduct 2.843
among 2.998 perform 2.095 damag 4.017 opinion 2.984
assist 3.004 applic 2.203 beyond 4.019 travel 3.491
further 3.011 press 2.388 extend 4.021 interact 3.527
</table>
<tableCaption confidence="0.999636">
Table 2: IDF vs. link effect R
</tableCaption>
<bodyText confidence="0.998305727272727">
Figure 1 describes a linear fit of over the 2000 words
with the lowest IDF in our corpus. A very clear trend can
be observed, whereby over most words, the value of is
almost a constant. When we looked only at the top 100
or 200 words, the trend was even cleaner. However, with
2000 words one cannot help but notice that a number of
outliers appear in the left hand part of the figure. We ran a
K-Means c (with K=2) to identify two clusters of words.
The clusterer stopped after 32 iterations after identifying
the two clusters (Figures 2 and 3), each with a very clear
trend. Their means are 1.86 and 3.57, respectively.
</bodyText>
<figure confidence="0.997905818181818">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</figure>
<figureCaption confidence="0.850143">
Figure 1: Linear fit for 2000 lowest-IDF words. The X
axis represents the prior probability while the Y axis
corresponds to the posterior probability .
</figureCaption>
<sectionHeader confidence="0.990703" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.9268825">
In this paper we discussed some properties of hyperlinked
Web documents. We showed that the probability of a
word to appear in a Web document depends on the
presence of in documents pointing to .
</bodyText>
<table confidence="0.999447894736842">
Words Prior Posterior TDF link effect R Sample words
1-100 0.4047 0.5293 1.6761 1.3639 the, of, make, and
101-200 0.2141 0.3574 2.3803 1.6745 under, go, between, amlusterererican
201-300 0.1688 0.3209 2.6896 1.9047 market, subject, special, mean
301-400 0.1386 0.2876 2.9513 2.0750 administr, put, establish, ask
401-500 0.1192 0.2588 3.1548 2.1750 understand, social, hand, share
501-600 0.1046 0.2426 3.3326 2.3179 prevent, staff, risk, north
601-700 0.0934 0.2246 3.4879 2.4085 trade, class, size, california
701-800 0.0839 0.2201 3.6354 2.6233 global, drug, letter, softwar
801-900 0.0752 0.2004 3.7884 2.6668 sound, tool, monitor, transport
901-1000 0.0669 0.2024 3.9499 3.0200 permit, target, east, normal
1001-1100 0.0605 0.1823 4.0909 3.0149 approxim, telephon, danger, europ
1101-1200 0.0548 0.1710 4.2292 3.1213 favor, richard, map, pictur
1201-1300 0.0498 0.1752 4.3635 3.5210 professor, earth, english, republican
1301-1400 0.0454 0.1652 4.4934 3.6366 medicin, doctor, church, color
1401-1500 0.0416 0.1630 4.6166 3.9224 permiss, agenda, programm, prioriti
... ... ... ...
100001-100100 0.0000 0.0642 12.4774 363.7331 sinker, surmont, thong, undergrowth
500001-500100 0.0000 0.0215 16.9270 2658.9231 scheflin, schena, schendel, scheriff
</table>
<tableCaption confidence="0.93287">
Table 1: Some measurements over 20 sets of 100 words among the 2000 lowest-IDF words plus 2 sets of 100 words
among the words of higher IDF
</tableCaption>
<figure confidence="0.999349083333333">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
</figure>
<figureCaption confidence="0.99499025">
Figure 2: Linear fit for Cluster 1, which contains many
low-IDF words such as by, with, from, etc.
Figure 3: Linear Fit for Cluster 2. Sample words from
this cluster are photo, dream, path, etc.
</figureCaption>
<sectionHeader confidence="0.995935" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998436131578948">
Albert-L´aszl´o Barab´asi and R´eka Albert. 1999. Emer-
gence of scaling in random networks. Science,
286:509–512.
Abraham Bookstein and Don Swanson. 1974. Proba-
bilistic models for automatic indexing. Journal of the
American Society for Information Science, 25(5):118–
132.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In ACL-96, pages 310–318, Santa Cruz, CA.
ACL.
Kenneth Church and William Gale. 1995. Posson mix-
tures. Natural Language Engineering.
Kenneth Church. 2000. Empirical estimates of adapta-
tion: the chance of two noriegas is closer to than
. In COLING, Saarbruecken, Germany, August.
W. Bruce Croft and David J. Harper. 1979. Using proba-
bilistic models of document retrieval without relevance
information. Journal ofDocumentation, 35:285–295.
Brian Davison. 2000. Topical locality in the web. In
SIGIR 2000), Athens, Greece, July.
P. Erd˝os and A. R´enyi. 1960. On the evolution of random
graphs. Publications of the Mathematical Institute of
the Hungarian Academy of Sciences, 5:17–61.
David Hawking. 2002. Web research collections - trec
web track. http://www.ted.cmis.csiro.au/TRECWeb/.
Filippo Menczer. 2001. Links tell us about lexical and
semantic web content. http://arxiv.org/cs.IR/0108004.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference - The Case of
The Federalist Papers. Springer Series in Satistics,
Springer-Verlag.
Jay Ponte and Bruce Croft. 1998. A language model-
ing approach to information retrieval. In SIGIR 1998,
pages 275–281, Melbourne, Australia, August.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill, New
York, NY.
</reference>
<figure confidence="0.994455923076923">
1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923431">
<title confidence="0.995431">Computational Linkuistics: word triggers across hyperlinks</title>
<author confidence="0.999905">Hong Qi</author>
<affiliation confidence="0.9999515">School of Information and Department of EECS University of Michigan</affiliation>
<address confidence="0.999836">Ann Arbor, MI 48109-1092</address>
<email confidence="0.998644">radev,hqi,dtam,winkela@umich.edu</email>
<abstract confidence="0.9955121875">It is known that context words tend to be selftriggers, that is, the probability of a content word to appear more than once in a document, given that it already appears once, is significantly higher than the probability of the first occurrence. We look at self-triggerability across on the Web. We show that the probof a word to appear in a Web ment depends on the presence of in documents pointing to . In Document Modeling, we will propose the use of a correction factor, , which indicates how much more likely a word is to appear in a document given that another document containing the same word is linked to it.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Albert-L´aszl´o Barab´asi</author>
<author>R´eka Albert</author>
</authors>
<title>Emergence of scaling in random networks.</title>
<date>1999</date>
<journal>Science,</journal>
<pages>286--509</pages>
<marker>Barab´asi, Albert, 1999</marker>
<rawString>Albert-L´aszl´o Barab´asi and R´eka Albert. 1999. Emergence of scaling in random networks. Science, 286:509–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Bookstein</author>
<author>Don Swanson</author>
</authors>
<title>Probabilistic models for automatic indexing.</title>
<date>1974</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>25</volume>
<issue>5</issue>
<pages>132</pages>
<contexts>
<context position="5143" citStr="Bookstein and Swanson, 1974" startWordPosition="873" endWordPosition="876"> 1.3 Document modeling In the computational linguistics and speech communities, the notion of a language model is used to describe a probability distribution over words. Since a cluster of documents contains a subset of an entire language, a document model is a special case of a language model. As such, it can be expressed as a conditional probability distribution indicating how likely a word is to appear in a document given some context (e.g., other similar documents, the topic of the document, etc.). Language models are used in speech recognition (Chen and Goodman, 1996), document indexing (Bookstein and Swanson, 1974; Croft and Harper, 1979) and information retrieval (Ponte and Croft, 1998). Document models are a special class of language models. One property of document models is that they can be used to predict some lexical properties of textual documents, e.g., the frequency of a certain word. Mosteller and Wallace (Mosteller and Wallace, 1984) discovered that content words are ”bursty” - the appearance of a content word significantly increases the probability that the word would appear again. Church and his colleagues (Church and Gale, 1995; Church, 2000) describe document models based on the distribu</context>
</contexts>
<marker>Bookstein, Swanson, 1974</marker>
<rawString>Abraham Bookstein and Don Swanson. 1974. Probabilistic models for automatic indexing. Journal of the American Society for Information Science, 25(5):118– 132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In ACL-96,</booktitle>
<pages>310--318</pages>
<publisher>ACL.</publisher>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="5095" citStr="Chen and Goodman, 1996" startWordPosition="866" endWordPosition="869">e presence of individual words across links. 1.3 Document modeling In the computational linguistics and speech communities, the notion of a language model is used to describe a probability distribution over words. Since a cluster of documents contains a subset of an entire language, a document model is a special case of a language model. As such, it can be expressed as a conditional probability distribution indicating how likely a word is to appear in a document given some context (e.g., other similar documents, the topic of the document, etc.). Language models are used in speech recognition (Chen and Goodman, 1996), document indexing (Bookstein and Swanson, 1974; Croft and Harper, 1979) and information retrieval (Ponte and Croft, 1998). Document models are a special class of language models. One property of document models is that they can be used to predict some lexical properties of textual documents, e.g., the frequency of a certain word. Mosteller and Wallace (Mosteller and Wallace, 1984) discovered that content words are ”bursty” - the appearance of a content word significantly increases the probability that the word would appear again. Church and his colleagues (Church and Gale, 1995; Church, 2000</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In ACL-96, pages 310–318, Santa Cruz, CA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>Posson mixtures. Natural Language Engineering.</title>
<date>1995</date>
<contexts>
<context position="5681" citStr="Church and Gale, 1995" startWordPosition="960" endWordPosition="963">ecognition (Chen and Goodman, 1996), document indexing (Bookstein and Swanson, 1974; Croft and Harper, 1979) and information retrieval (Ponte and Croft, 1998). Document models are a special class of language models. One property of document models is that they can be used to predict some lexical properties of textual documents, e.g., the frequency of a certain word. Mosteller and Wallace (Mosteller and Wallace, 1984) discovered that content words are ”bursty” - the appearance of a content word significantly increases the probability that the word would appear again. Church and his colleagues (Church and Gale, 1995; Church, 2000) describe document models based on the distribution of the frequencies of individual words over large document collections. In (Church and Gale, 1995), Church and Gale compare document models based on the Poisson distribution, the 2-Poisson distribution (Bookstein and Swanson, 1974), as well as generic Poisson mixtures. A Poisson mixture is described by , where for a given integer non-negative value of . Church and Gale empirically show that Poisson mixtures are a more accurate model for describing the distribution of words in documents within a corpus. They obtain the best fits</context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>Kenneth Church and William Gale. 1995. Posson mixtures. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>Empirical estimates of adaptation: the chance of two noriegas is closer to than .</title>
<date>2000</date>
<booktitle>In COLING,</booktitle>
<location>Saarbruecken, Germany,</location>
<contexts>
<context position="5696" citStr="Church, 2000" startWordPosition="964" endWordPosition="965">odman, 1996), document indexing (Bookstein and Swanson, 1974; Croft and Harper, 1979) and information retrieval (Ponte and Croft, 1998). Document models are a special class of language models. One property of document models is that they can be used to predict some lexical properties of textual documents, e.g., the frequency of a certain word. Mosteller and Wallace (Mosteller and Wallace, 1984) discovered that content words are ”bursty” - the appearance of a content word significantly increases the probability that the word would appear again. Church and his colleagues (Church and Gale, 1995; Church, 2000) describe document models based on the distribution of the frequencies of individual words over large document collections. In (Church and Gale, 1995), Church and Gale compare document models based on the Poisson distribution, the 2-Poisson distribution (Bookstein and Swanson, 1974), as well as generic Poisson mixtures. A Poisson mixture is described by , where for a given integer non-negative value of . Church and Gale empirically show that Poisson mixtures are a more accurate model for describing the distribution of words in documents within a corpus. They obtain the best fits with the Negat</context>
</contexts>
<marker>Church, 2000</marker>
<rawString>Kenneth Church. 2000. Empirical estimates of adaptation: the chance of two noriegas is closer to than . In COLING, Saarbruecken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Bruce Croft</author>
<author>David J Harper</author>
</authors>
<title>Using probabilistic models of document retrieval without relevance information.</title>
<date>1979</date>
<journal>Journal ofDocumentation,</journal>
<pages>35--285</pages>
<contexts>
<context position="5168" citStr="Croft and Harper, 1979" startWordPosition="877" endWordPosition="880"> computational linguistics and speech communities, the notion of a language model is used to describe a probability distribution over words. Since a cluster of documents contains a subset of an entire language, a document model is a special case of a language model. As such, it can be expressed as a conditional probability distribution indicating how likely a word is to appear in a document given some context (e.g., other similar documents, the topic of the document, etc.). Language models are used in speech recognition (Chen and Goodman, 1996), document indexing (Bookstein and Swanson, 1974; Croft and Harper, 1979) and information retrieval (Ponte and Croft, 1998). Document models are a special class of language models. One property of document models is that they can be used to predict some lexical properties of textual documents, e.g., the frequency of a certain word. Mosteller and Wallace (Mosteller and Wallace, 1984) discovered that content words are ”bursty” - the appearance of a content word significantly increases the probability that the word would appear again. Church and his colleagues (Church and Gale, 1995; Church, 2000) describe document models based on the distribution of the frequencies o</context>
</contexts>
<marker>Croft, Harper, 1979</marker>
<rawString>W. Bruce Croft and David J. Harper. 1979. Using probabilistic models of document retrieval without relevance information. Journal ofDocumentation, 35:285–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Davison</author>
</authors>
<title>Topical locality in the web.</title>
<date>2000</date>
<booktitle>In SIGIR 2000),</booktitle>
<location>Athens, Greece,</location>
<contexts>
<context position="2916" citStr="Davison, 2000" startWordPosition="510" endWordPosition="511"> description of the Web. Instead, it has been shown that due to preferential attachment (Barab´asi and Albert, 1999), the out-degree distribution follows a power law. The preferential model makes it more likely that a new random edge will connect two vertices that already have a high degree. Specifically, the degree of pages is distributed according to , where is a constant strictly greater than 0. (Note this is different from , the distribution of out-degree on random graphs.) As a result, random walks on the Web graph soon reach wellconnected nodes. 1.2 Lexical structure of the Web Davison (Davison, 2000) discusses the topical locality hypothesis, namely that new edges are more likely to connect pages that are semantically related. In Davison’s experiment, semantic and link distances between pairs of pages from a 100 K page corpus were computed. Davison describes results associating TF*IDF cosine similarity (Salton and McGill, 1983) and link hop distance. He reports that the cosine similarity between pages selected at random from his corpus is 0.02 whereas that number increases significantly for topologically related pages: 0.31 for pages from the same Web domain, 0.23 for linked pages, and 0.</context>
</contexts>
<marker>Davison, 2000</marker>
<rawString>Brian Davison. 2000. Topical locality in the web. In SIGIR 2000), Athens, Greece, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Erd˝os</author>
<author>A R´enyi</author>
</authors>
<title>On the evolution of random graphs.</title>
<date>1960</date>
<booktitle>Publications of the Mathematical Institute of the Hungarian Academy of Sciences,</booktitle>
<pages>5--17</pages>
<marker>Erd˝os, R´enyi, 1960</marker>
<rawString>P. Erd˝os and A. R´enyi. 1960. On the evolution of random graphs. Publications of the Mathematical Institute of the Hungarian Academy of Sciences, 5:17–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hawking</author>
</authors>
<title>Web research collections - trec web track.</title>
<date>2002</date>
<note>http://www.ted.cmis.csiro.au/TRECWeb/.</note>
<contexts>
<context position="7692" citStr="Hawking, 2002" startWordPosition="1290" endWordPosition="1291">ink-dependent part makes use of a particular incarnation of the linkcontent conjecture, namely micro link-content dependency (MLD), which we will propose in this paper. 1.5 Our framework In traditional Information Retrieval, the main object that is represented, and searched, is the document. In our setup, we will be looking at the hyperlink between two documents as the main object to retrieve. If a page points to page via link , then we will consider as the object to index and the two pages that it links as features describing the link. For our experiments, we used the 2-Gigabyte wt2g corpus (Hawking, 2002) which contains 247,491 Web documents connected with 3,118,248 links. These documents contain 948,036 unique words (after Porter-style stemming). 2 A link-based document model It is well known that the distributions of words in text depend on many factors such as genre, topic, author, etc. Certain words with high content has been found to ”trigger” other words to appear. Interestingly, the hyperlinks which connect the text on the Web may also affect the word distributions in the hypertext. For example, if page that contains education points to page , then we would expect a higher probability o</context>
</contexts>
<marker>Hawking, 2002</marker>
<rawString>David Hawking. 2002. Web research collections - trec web track. http://www.ted.cmis.csiro.au/TRECWeb/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filippo Menczer</author>
</authors>
<title>Links tell us about lexical and semantic web content.</title>
<date>2001</date>
<note>http://arxiv.org/cs.IR/0108004.</note>
<contexts>
<context position="3597" citStr="Menczer, 2001" startWordPosition="619" endWordPosition="620">re more likely to connect pages that are semantically related. In Davison’s experiment, semantic and link distances between pairs of pages from a 100 K page corpus were computed. Davison describes results associating TF*IDF cosine similarity (Salton and McGill, 1983) and link hop distance. He reports that the cosine similarity between pages selected at random from his corpus is 0.02 whereas that number increases significantly for topologically related pages: 0.31 for pages from the same Web domain, 0.23 for linked pages, and 0.19 for sibling pages (pages pointed to by the same page). Menczer (Menczer, 2001) introduces the link-content conjecture states that the semantic content of a web page can be inferred from the pages that point to it. Menczer uses a corpus of 373 K pages and employs a non-linear least squares fit to come up with a semantic model connecting cosine-based semantic similarity and the link distance between two pages and (the shortest directed distance on the hypertext graph from to ). Menczer reports that and are connected via a power law: . represents noise level in similarity. Menczer reports empirically determined values of the parameters of the fit as follows: , , and . Menc</context>
</contexts>
<marker>Menczer, 2001</marker>
<rawString>Filippo Menczer. 2001. Links tell us about lexical and semantic web content. http://arxiv.org/cs.IR/0108004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David L Wallace</author>
</authors>
<title>Applied Bayesian and Classical Inference - The Case of The Federalist Papers. Springer Series in Satistics,</title>
<date>1984</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="5480" citStr="Mosteller and Wallace, 1984" startWordPosition="928" endWordPosition="931">ional probability distribution indicating how likely a word is to appear in a document given some context (e.g., other similar documents, the topic of the document, etc.). Language models are used in speech recognition (Chen and Goodman, 1996), document indexing (Bookstein and Swanson, 1974; Croft and Harper, 1979) and information retrieval (Ponte and Croft, 1998). Document models are a special class of language models. One property of document models is that they can be used to predict some lexical properties of textual documents, e.g., the frequency of a certain word. Mosteller and Wallace (Mosteller and Wallace, 1984) discovered that content words are ”bursty” - the appearance of a content word significantly increases the probability that the word would appear again. Church and his colleagues (Church and Gale, 1995; Church, 2000) describe document models based on the distribution of the frequencies of individual words over large document collections. In (Church and Gale, 1995), Church and Gale compare document models based on the Poisson distribution, the 2-Poisson distribution (Bookstein and Swanson, 1974), as well as generic Poisson mixtures. A Poisson mixture is described by , where for a given integer </context>
</contexts>
<marker>Mosteller, Wallace, 1984</marker>
<rawString>Frederick Mosteller and David L. Wallace. 1984. Applied Bayesian and Classical Inference - The Case of The Federalist Papers. Springer Series in Satistics, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Ponte</author>
<author>Bruce Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In SIGIR</booktitle>
<pages>275--281</pages>
<location>Melbourne, Australia,</location>
<contexts>
<context position="5218" citStr="Ponte and Croft, 1998" startWordPosition="884" endWordPosition="887">the notion of a language model is used to describe a probability distribution over words. Since a cluster of documents contains a subset of an entire language, a document model is a special case of a language model. As such, it can be expressed as a conditional probability distribution indicating how likely a word is to appear in a document given some context (e.g., other similar documents, the topic of the document, etc.). Language models are used in speech recognition (Chen and Goodman, 1996), document indexing (Bookstein and Swanson, 1974; Croft and Harper, 1979) and information retrieval (Ponte and Croft, 1998). Document models are a special class of language models. One property of document models is that they can be used to predict some lexical properties of textual documents, e.g., the frequency of a certain word. Mosteller and Wallace (Mosteller and Wallace, 1984) discovered that content words are ”bursty” - the appearance of a content word significantly increases the probability that the word would appear again. Church and his colleagues (Church and Gale, 1995; Church, 2000) describe document models based on the distribution of the frequencies of individual words over large document collections</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Jay Ponte and Bruce Croft. 1998. A language modeling approach to information retrieval. In SIGIR 1998, pages 275–281, Melbourne, Australia, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="3250" citStr="Salton and McGill, 1983" startWordPosition="559" endWordPosition="562">es is distributed according to , where is a constant strictly greater than 0. (Note this is different from , the distribution of out-degree on random graphs.) As a result, random walks on the Web graph soon reach wellconnected nodes. 1.2 Lexical structure of the Web Davison (Davison, 2000) discusses the topical locality hypothesis, namely that new edges are more likely to connect pages that are semantically related. In Davison’s experiment, semantic and link distances between pairs of pages from a 100 K page corpus were computed. Davison describes results associating TF*IDF cosine similarity (Salton and McGill, 1983) and link hop distance. He reports that the cosine similarity between pages selected at random from his corpus is 0.02 whereas that number increases significantly for topologically related pages: 0.31 for pages from the same Web domain, 0.23 for linked pages, and 0.19 for sibling pages (pages pointed to by the same page). Menczer (Menczer, 2001) introduces the link-content conjecture states that the semantic content of a web page can be inferred from the pages that point to it. Menczer uses a corpus of 373 K pages and employs a non-linear least squares fit to come up with a semantic model conn</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>