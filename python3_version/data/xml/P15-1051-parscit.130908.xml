<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.9886805">
Encoding Distributional Semantics into Triple-Based Knowledge Ranking
for Document Enrichment
</title>
<author confidence="0.999644">
Muyu Zhang1∗, Bing Qin1, Mao Zheng1, Graeme Hirst2, and Ting Liu1
</author>
<affiliation confidence="0.999328">
1Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, Harbin, China
2Department of Computer Science, University of Toronto, Toronto, ON, Canada
</affiliation>
<email confidence="0.9907185">
{myzhang,qinb,mzheng,tliu}@ir.hit.edu.cn
gh@cs.toronto.edu
</email>
<sectionHeader confidence="0.997354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992036">
Document enrichment focuses on retriev-
ing relevant knowledge from external re-
sources, which is essential because text is
generally replete with gaps. Since conven-
tional work primarily relies on special re-
sources, we instead use triples of Subject,
Predicate, Object as knowledge and in-
corporate distributional semantics to rank
them. Our model first extracts these triples
automatically from raw text and converts
them into real-valued vectors based on the
word semantics captured by Latent Dirich-
let Allocation. We then represent these
triples, together with the source document
that is to be enriched, as a graph of triples,
and adopt a global iterative algorithm to
propagate relevance weight from source
document to these triples so as to select the
most relevant ones. Evaluated as a rank-
ing problem, our model significantly out-
performs multiple strong baselines. More-
over, we conduct a task-based evaluation
by incorporating these triples as additional
features into document classification and
enhances the performance by 3.02%.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983928396226415">
Document enrichment is the task of acquiring rel-
evant background knowledge from external re-
sources for a given document. This task is essen-
tial because, during the writing of text, some ba-
sic but well-known information is usually omitted
by the author to make the document more concise.
For example, Baghdad is the capital of Iraq is
omitted in Figure 1a. A human will fill these gaps
automatically with the background knowledge in
his mind. However, the machine lacks both the
∗ This work was partly done while the first author was
visiting University of Toronto.
necessary background knowledge and the ability
to select. The task of document enrichment is pro-
posed to tackle this problem, and has been proved
helpful in many NLP tasks such as web search
(Pantel and Fuxman, 2011), coreference resolu-
tion (Bryl et al., 2010), document cluster (Hu et
al., 2009) and entity disambiguation (Sen, 2012).
We can classify previous work into two classes
according to the resources they rely on. The first
line of work uses Wikipedia, the largest on-line en-
cyclopedia, as a resource and introduces the con-
tent of Wikipedia pages as external knowledge
(Cucerzan, 2007; Kataria et al., 2011; He et al.,
2013). Most research in this area relies on the text
similarity (Zheng et al., 2010; Hoffart et al., 2011)
and structure information (Kulkarni et al., 2009;
Sen, 2012; He et al., 2013) between the mention
and the Wikipedia page. Despite the apparent suc-
cess of these methods, most Wikipedia pages con-
tain too much information, most of which is not
relevant enough to the source document, and this
causes a noise problem. Another line of work tries
to improve the accuracy by introducing ontolo-
gies (Fodeh et al., 2011; Kumar and Salim, 2012)
and structured knowledge bases such as WordNet
(Nastase et al., 2010), which provide semantic in-
formation about words such as synonym (Sun et
al., 2011) and antonym (Sansonnet and Bouchet,
2010). However, these methods primarily rely on
special resources constructed with supervision or
even manually, which are difficult to expand and
in turn limit their applications in practice.
In contrast, we wish to seek the benefits of both
coverage and accuracy from a better representa-
tion of background knowledge: triples of Subject,
Predicate, Object (SPO). According to Hoffart et
al. (2013), these triples, such as LeonardCohen,
wasBornIn, Montreal, can be extracted automat-
ically from Wikipedia and other sources, which
is compatible with the RDF data model (Staab
and Studer, 2009). Moreover, by extracting these
</bodyText>
<page confidence="0.970038">
524
</page>
<note confidence="0.985279333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 524–533,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.789314">
(a) Source document: air strike aiming at Saddam in Baghdad
</figure>
<figureCaption confidence="0.996962">
Figure 1: An example of document enrichment:
</figureCaption>
<bodyText confidence="0.999602946428572">
A source document about a U.S. air strike omit-
ting two important pieces of background knowl-
edge which are acquired by our framework.
triples from multiple sources, we also get better
coverage. Therefore, one can expect that this rep-
resentation is helpful for better document enrich-
ment by incorporating both accuracy and cover-
age. In fact, there is already evidence that this
representation is helpful. Zhang et al. (2014) pro-
posed a triple-based document enrichment frame-
work which uses triples of SPO as background
knowledge. They first proposed a search engine–
based method to evaluate the relatedness between
every pair of triples, and then an iterative propa-
gation algorithm was introduced to select the most
relevant triples to a given source document (see
Section 2), which achieved a good performance.
However, to evaluate the semantic relatedness
between two triples, Zhang et al. (2014) primar-
ily relied on the text of triples and used search
engines, which makes their method difficult to
re-implement and in turn limits its application in
practice. Moreover, they did not carry out any
task-based evaluation, which makes it uncertain
whether their method will be helpful in real appli-
cations. Therefore, we instead use topic models,
especially Latent Dirichlet Allocation (LDA), to
encode distributional semantics of words and con-
vert every triple into a real-valued vector, which
is then used to evaluate the relatedness between
a pair of triples. We then incorporate these triples
into the given source document and represent them
together as a graph of triples. Then a modified it-
erative propagation is carried out over the entire
graph to select the most relevant triples of back-
ground knowledge to the given source document.
To evaluate our model, we conduct two series of
experiments: (1) evaluation as a ranking problem,
and (2) task-based evaluation. We first treat this
task as a ranking problem which inputs one doc-
ument and outputs the top N most-relevant triples
of background knowledge. Second, we carry out a
task-based evaluation by incorporating these rele-
vant triples acquired by our model into the origi-
nal model of document classification as additional
features. We then perform a direct comparison be-
tween the classification models with and without
these triples, to determine whether they are help-
ful or not. On the first series of experiments, we
achieve a MAP of 0.6494 and a P@N of 0.5597 in
the best situation, which outperforms the strongest
baseline by 5.87% and 17.21%. In the task-based
evaluation, the enriched model derived from the
triples of background knowledge performs better
by 3.02%, which demonstrates the effectiveness of
our framework in real NLP applications.
</bodyText>
<sectionHeader confidence="0.993213" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.989378463414634">
The most closely related work in this area is our
own (Zhang et al., 2014), which used the triples
of SPO as background knowledge. In that work,
we first proposed a triple graph to represent the
source document and then used a search engine–
based iterative algorithm to rank all the triples. We
describe this work in detail below.
Triple graph Zhang et al. (2014) proposed the
triple graph as a document representation, where
the triples of SPO serve as nodes, and the edges
between nodes indicate their semantic relatedness.
There are two kinds of nodes in the triple graph:
(1) source document nodes (sd-nodes), which are
triples extracted from source documents, and (2)
background knowledge nodes (bk-nodes), which
are triples extracted from external sources. Both
of them are extracted automatically with Reverb, a
well-known Open Information Extraction system
(Etzioni et al., 2011). There are also two kinds
of edges: (1) an edge between a pair of sd-nodes,
and (2) an edge between one sd-node and another
bk-node, both of which are unidirectional. In the
original representation, there are no edges between
two bk-nodes because they treat the bk-nodes as
recipients of relevance weight only. In this paper,
we modify this setup and connect every pair of bk-
nodes with an edge, so the bk-nodes serve as in-
termediate nodes during the iterative propagation
process and contribute to the final performance too
as shown in our experiments (see Section 5.1).
S1: Th coaliton may never know if Iaqi president Saddam
Hussein survived a US. air strike yeserday
Hussein survived a U.S. air strike yesterday.
S A B1 bb dd f 2000d
S2: A B-1 bombe drpped four 2,000-pound bombs on a building
in a residential area of Baghdad
in a residential area of Baghdad .
S: They had received telligence rS3: They had received intelligence reports that senior official were
meeting thee, possibly including Saddam Hussein and his sons .
meeting there, possibly including Saddam Hussein and his sons .
Two omitted relevant pieces of background knowledge
</bodyText>
<figure confidence="0.9844148">
Saddam Hussen hasChild Qusay Hussein
k2:
k2:
Saddam Hussein
k1: B
k1:
Baghdad
Iraq
Capital
hasChild Qusay Hussein
</figure>
<page confidence="0.99663">
525
</page>
<bodyText confidence="0.9996872">
Relevance evaluation To compute the weight of
a edge, Zhang et al. (2014) evaluate the seman-
tic relatedness between two nodes with a search
engine–based method. They first convert every
node, which is a triple of SPO, into a query by
combining the text of Subject and Object together.
Then for every pair of nodes ti and tj, they con-
struct three queries: p, q, and p n q, which corre-
spond to the queries of ti, tj, and tj n tj, the com-
bination of ti and tj. All these queries are put into
a search engine to get H(p), H(q), and H(p n q),
the numbers of returned pages for query p, p, and
pnq. Then the WebJaccard Coefficient (Bollegala
et al., 2007) is used to evaluate r(i, j), the related-
ness between ti and tj, according to Formula 1.
</bodyText>
<equation confidence="0.9988684">
r(i, j) = WebJaccard(p,q) =
{ 0 if H(p n q) &lt; C
H(p n q) otherwise.
H(p) +H(q) — H(p n q)
(1)
</equation>
<bodyText confidence="0.998696">
Using r(i, j), Zhang et al. (2014) further define
p(i, j), the probability of ti and tj propagating to
each other, as shown in Formula 2. Here N is
the set of all nodes, and 3(i, j) denotes whether
an edge exists between two nodes or not.
</bodyText>
<equation confidence="0.982924">
r(i, j) x 3(i, j) (2)
p(i, j) = ∑ncN r(n, j) x 3(n, j)
</equation>
<bodyText confidence="0.998326444444444">
Iterative propagation Considering that the
source document D is represented as a graph of
sd-nodes, so the relevance of background knowl-
edge tb to D is naturally converted into that of tb to
the graph of sd-nodes. Zhang et al. (2014) evalu-
ate this relevance by propagating relevance weight
from sd-nodes to tb iteratively. After convergence,
the relevance weight of tb will be treated as the fi-
nal relevance to D. There are in total n x n pairs
of nodes, and their p(i, j) are stored in a matrix P.
Zhang et al. (2014) use W~ = (w1,w2,...,wn) to de-
note the relevance weights of nodes, where wi in-
dicates the relevance of ti to D. At the beginning,
each wi of bk-nodes is initialized to 0, and each
that of sd-nodes is initialized to its importance to
D. Then W~ is updated to ~W� after every iteration
according to Formula 3. They keep updating the
weights of both sd-nodes and bk-nodes until con-
</bodyText>
<equation confidence="0.798161166666667">
vergence and do not distinguish them explicitly.
~W� W~xP
= W~x p(1,1) p(1,2) ... p(1,n) (3)
p(2,1) p(2,2) ... p(2,n)
... ... ... ...
(n,1) p(n,2) ... p(n,n) 1
</equation>
<sectionHeader confidence="0.997268" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999960842105263">
The key idea behind this work is that every doc-
ument is composed of several units of informa-
tion, which can be extracted into triples automat-
ically. For every unit of background knowledge
b, the more units that are relevant to b and the
more relevant they are, the more relevant b will
be to the source document. Based on this intu-
ition, we first present both source document infor-
mation and background knowledge together as a
document-level triple graph as illustrated in Sec-
tion 2. Then we use LDA to capture the distribu-
tional semantics of a triple by representing it as a
vector of distributional probabilities over k topics
and evaluate the relatedness between two triples
with cosine-similarity. Finally, we propose a mod-
ified iterative process to propagate the relevance
score from the source document information to the
background knowledge and select the top n rele-
vant ones.
</bodyText>
<subsectionHeader confidence="0.999667">
3.1 Encoding distributional semantics
</subsectionHeader>
<bodyText confidence="0.999981833333333">
LDA LDA is a popular generative probabilistic
model, which was first introduced by Blei et al.
(2003). LDA views every document as a mixture
over underlying topics, and each topic as a distri-
bution over words. Both the document-topic and
the topic-word distributions are assumed to have a
Dirichlet prior. Given a set of documents and a
number of topics, the model returns 0d, the topic
distribution for each document d, and biz, the word
distribution for every topic z.
LDA assumes the following generative process
for each document in a corpus D:
</bodyText>
<listItem confidence="0.9898422">
1. Choose N — Poisson(4).
2. Choose 0 — Dir(a).
(a) Choose a topic zn — Multinomial(0).
(b) Choose a word wn from p(wn|zn,/3) con-
ditioned on the topic zn.
</listItem>
<bodyText confidence="0.99377">
Here the dimensionality k of the Dirichlet distribu-
tion (and thus the dimensionality of the topic vari-
</bodyText>
<page confidence="0.993829">
526
</page>
<bodyText confidence="0.958128375">
Figure 2: Graphical representation of LDA. The
boxes represents replicates, where the inner box
represents the repeated choice of N topics and
words within a document, while the outer one rep-
resents the repeated generation of M documents.
able z) is assumed to be known and fixed; 0 is a k-
dimensional Dirichlet random variable, where the
parameter a is a k-vector with components ai &gt; 0;
and the P indicates the word probabilities over
topics, which is a matrix with Pij = p(wj = 1|zi =
1). Figure 2 shows the representation of LDA as
a probabilistic graphical model with three levels.
There are two corpus-level parameters a and P ,
which are assumed to be sampled once in the pro-
cess of generating a corpus; one document-level
variable 0d, which is sampled once per document;
and two word-level variables zdn and wdn, which
are sampled once for each word in each document.
We employ the publicly available implementa-
tion of LDA, JGibbLDA21 (Phan et al., 2008),
which has two main execution methods: param-
eter estimation (model building) and inference for
new data (classification of a new document).
Relevance evaluation Given a set of documents
and the number of topics k, LDA will return biz,
the word distribution over the topic z. So for every
word wn, we get k distributional probabilities over
k topics. We use pwnzi to denote the probability
that wn appears in the ith topic zi, where i &lt; k, zi E
Z, the set of k topics. Then we combine these k
possibilities together as a real-valued vectorvwn to
represent wn as shown in Formula 4.
</bodyText>
<equation confidence="0.876869">
vwn = (pwnz1, pwnz2,..., pwnzk) (4)
</equation>
<bodyText confidence="0.999862571428571">
After getting the vectors of words, we employ
an intuitive method to compute the vector of a
triplet, by accumulating all the corresponding
vectors of words appearing in t according to For-
mula 5. Considering that the elements of this
newly generated vector indicate the distributional
probabilities of t over k topics, we then normalize
</bodyText>
<footnote confidence="0.918479">
1http://jgibblda.sourceforge.net/
</footnote>
<bodyText confidence="0.999724571428572">
it according to Formula 6 so that its elements sum
to 1. This gives us vt, the real-valued vector of
triplet, which captures its distributional probabil-
ities over k topics. Here t corresponds to a triple
of background knowledge or of source document,
ptzi indicates the possibility of t to appear in the ith
topic zi, and wn E t means that wn appears in t.
</bodyText>
<equation confidence="0.998254">
ptzi = E
wnEt pwnzi (5)
vt = (ptz1, ptz2,..., ptzk) (6)
Ek i=1 ptzi
</equation>
<bodyText confidence="0.999966363636364">
Using the vectors of triples, we can easily com-
pute the semantic relatedness between a pair of
triples as their cosine-similarity according to For-
mula 7. Here A, B correspond to the real-valued
vectors of two triples, r(A,B) denotes their se-
mantic relatedness, and k is the number of topics,
which is also the length of A (or B). A high value
of r(A,B) usually indicates a close relatedness be-
tween A and B, and thus a higher probability of
propagating to each other in the following modi-
fied iterative propagation illustrated in Section 3.2.
</bodyText>
<equation confidence="0.990403666666667">
AB
r(A,B) =cos(A,B) =
Eki=1 AiBi (7)
=
/ /
Ek i=1 (Ai)2 Ek i=1 (Bi)2
</equation>
<subsectionHeader confidence="0.997729">
3.2 Modified iterative propagation
</subsectionHeader>
<bodyText confidence="0.999972">
In this part, we propose a modified iterative prop-
agation based ranking model to select the most-
relevant triples of background knowledge. There
are three primary modifications to the original
model of Zhang et al. (2014), all of which are
shown more powerful in our experiments.
First of all, the original model (Zhang et al.,
2014) does not reset the relevance weight of sd-
nodes after every iteration. This results in a contin-
ued decrease of the relevance weight of sd-nodes,
which weakens the effect of sd-nodes during the
iterative propagation and in turn affects the fi-
nal performance. To tackle this problem, we de-
crease the relevance weight of bk-nodes and in-
crease that of sd-nodes according to a fixed ratio
after every iteration, so as to ensure that the to-
tal weight of sd-nodes is always higher than that
of bk-nodes. Note that although the relevance
weights of bk-nodes are changed after the redis-
tribution, the corresponding ranking of them is not
changed because the redistribution is carried out
</bodyText>
<sectionHeader confidence="0.242003" genericHeader="method">
IJAIJIJBIJ
</sectionHeader>
<page confidence="0.801992">
527
</page>
<figureCaption confidence="0.930056">
Figure 3: The edge between two bk-nodes helps
</figureCaption>
<bodyText confidence="0.99803945945946">
in the better evaluation of relatedness between the
bk-node Yoko Ono and the sd-node Beatles.
over all nodes accordingly. In our experiments, we
tried different ratios and finally chose 10:1, with
sd-nodes corresponding to 10 and bk-nodes to 1,
which achieved the best performance.
In addition, we also modify the triple graph, the
representation of a document illustrated in Section
2, by connecting every pair of bk-nodes with an
edge, which is not allowed in the original model.
This modification was motivated by the intuition
that the relatedness between bk-nodes also con-
tributes to the better evaluation of relevance to the
source document, because the bk-nodes can serve
as the intermediate nodes during the iterative prop-
agation over the entire graph. Figure 3 shows an
example, where the bk-node John Lennon is close
to both the sd-node Beatles and to another bk-
node Yoko Ono, so the relatedness between two
bk-nodes John Lennon and Yoko Ono helps in bet-
ter evaluation of the relatedness between the bk-
node Yoko Ono and the sd-node Beatles.
We also modify the definition of p(i, j), the
probability of two nodes ti and tj propagating to
each other. Zhang et al. (2014) compute this prob-
ability according to Formula 2, which highlights
the number of neighbors, but weakens the related-
ness between nodes, due to the normalization. For
instance, if a node tx has only one neighbor ty, no
matter how low their relatedness is, their p(x,y)
will still be equal to 1 in the original model, while
another node with two equally but closely related
neighbors will only get a probability of 0.5 for
each neighbor. We modify this setup by removing
the normalization process and computing p(i, j) as
the relatedness between ti and tj directly, which is
evaluated according to Formula 1 .
</bodyText>
<sectionHeader confidence="0.884388" genericHeader="method">
4 Encoding background knowledge into
document classification
</sectionHeader>
<bodyText confidence="0.989770592592592">
In this part, we demonstrate that the introduction
of relevant knowledge could be helpful to real
NLP applications. In particular, we choose the
document classification task as a demonstration,
which aims to classify documents into predefined
categories automatically (Sebastiani, 2002). We
choose this task for two reasons: (1) This task
has witnessed a booming interest in the last 20
years, due to the increased availability of docu-
ments in digital form and the ensuing need to orga-
nize them, so it is important in both research and
application. (2) The state-of-the-art performance
of this task is achieved by a series of topic model–
based methods, which rely on the same model as
we do, but make use of source document informa-
tion only. However, there is always some omitted
information and relevant knowledge, which can-
not be captured from the source document. In-
tuitively, the recovery of this information will be
helpful. If we can improve the performance by in-
troducing extra background knowledge into exist-
ing framework of document classification, we can
inference naturally that the improvement benefits
from the introduction of this knowledge.
Traditional methods primarily use topic models
to represent a document as a topic vector. Then a
SVM classifier takes this vector as input and out-
puts the class of the document. In this work, we
propose a new framework for document classifica-
tion to incorporate extra knowledge. Given a doc-
ument to be classified, we select the top N most-
relevant triples of background knowledge with our
model introduced in Section 3, all of which are
represented as vectors of ~vt = (ptz1, ptz2,..., ptzk).
Then we combine these N triples as a new vec-
tor vt
-4, which is then incorporated into the original
framework of document classification. Another
0
SVM classifier takes ~vt, together with the original
features extracted from the source document, as
input and outputs the category of the source doc-
ument. To combine N triples as one, we employ
an intuitive method by computing the average of
N corresponding vectors in every dimension.
One possible problem is how to decide N, the
number of triples to be introduced. We first intro-
duce a fixed amount of triples for every document.
Moreover, we also select the triples according to
their relevance weight to the source document (see
Section 3.2) by setting a threshold of relevance
weight first and selecting the triples whose weights
are higher than the threshold. We further discuss
the impact of different thresholds in Section 5.2.
</bodyText>
<figure confidence="0.98761">
sd-node
bk-node bk-node
??
John Lennon Yoko Ono
Beatles
</figure>
<page confidence="0.97796">
528
</page>
<sectionHeader confidence="0.998027" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999992727272727">
To evaluate our model, we conduct two series
of experiments: (1) We first treat this task as a
ranking problem, which takes a document as in-
put and outputs the ranked triples of background
knowledge, and evaluate the ranking performance
by computing the scores of MAP and P@N. (2)
We also conduct a task-based evaluation, where
document classification (see Section 4) is chosen
as a demonstration, by enriching the background
knowledge to the original framework as additional
features and performing a direct comparison.
</bodyText>
<subsectionHeader confidence="0.981349">
5.1 Evaluation as a ranking problem
</subsectionHeader>
<bodyText confidence="0.998837323529412">
Data preparation The data is composed of two
parts: source documents and background knowl-
edge. For source documents, we use a publicly
available Chinese corpus which consists of 17,199
documents and 13,719,428 tokens extracted from
Internet news2 including 9 topics: Finance, IT,
Health, Sports, Travel, Education, Jobs, Art, Mil-
itary. We then randomly but equally select 600
articles as the set of source documents from 9 top-
ics without data bias. We use all the other 16,599
documents of the same corpus as the source of
background knowledge, and then introduce a well-
known Chinese open source tool (Che et al., 2010)
to extract the triples of background knowledge
from the raw text automatically. So the back-
ground knowledge also distributes evenly across
the same 9 topics. We use the same tool to extract
the triples of source documents too.
Baseline systems As Zhang et al. (2014) argued,
it is difficult to use the methods in traditional
ranking tasks, such as information retrieval (Man-
ning et al., 2008) and entity linking (Han et al.,
2011; Sen, 2012), as baselines in this task, because
our model takes triples as basic input and thus
lacks some crucial information such as link struc-
ture. For better comparison, we implement three
methods as baselines, which have been proved ef-
fective in relevance evaluation: (1) Vector Space
Model (VSM), (2) Word Embedding (WE), and
(3) Latent Dirichlet Allocation (LDA). Note that
our model captures the distributional semantics of
triples with LDA, while WE serves as a baseline
only, where the word embeddings are acquired
over the same corpus mentioned previously with
</bodyText>
<footnote confidence="0.651532">
2http://www.sogou.com/labs/dl/c.html
</footnote>
<bodyText confidence="0.998918306122449">
the publicly available tool word2vec3.
Here we use ti, D, and wi to denote a triple of
background knowledge, a source document, and
the relevance of ti to D. For VSM, we represent
both ti and D with a tf-idf scheme first (Salton
and McGill, 1986) and compute wi as their cosine-
similarity. For WE, we first convert both ti and the
triples extracted from D into real-valued vectors
with WE and then compute wi by accumulating all
the cosine-similarities between ti and every triple
from D. For LDA, we represent ti as a vector with
our model introduced in Section 3.1 and get the
vector of D directly with LDA. Then we evaluate
their relevance of ti to D by computing the cosine-
similarity of two corresponding vectors.
Moreover, to determine whether our modified
iterative propagation is helpful or not, we also
compare our full model (Ours) against a simpli-
fied version without iterative propagation (Ours-
S). In Ours-S, we represent both ti and the triples
extracted from D as real-valued vectors with our
model introduced in Section 3.1. Then we com-
pute wi by accumulating all the cosine-similarities
between ti and the triples extracted from D. For all
the baselines, we rank the triples of background
knowledge according to wi, their relevance to D.
Experimental setup Previous research relies on
manual annotation to evaluate the ranking perfor-
mance (Zhang et al., 2014), which costs a lot,
and in which it is difficult to get high consistency.
In this paper, we carry out an automatic evalua-
tion. The corpus we used consists of 9 different
classes, from which we extract triples of back-
ground knowledge. So correspondingly, there will
be 9 sets of triples too. Then we randomly select
200 triples from every class and mix 200 × 9 =
1800 triples together as S, the set of triples of
background knowledge. For every document D
to be enriched, our model selects the top N most-
relevant triples from S and returns them to D as
enrichments. We treat a triple ti selected by our
model as positive only if ti is extracted from the
same class as D. We evaluate the performance of
our model with two well-known criteria in ranking
problem: MAP and P@N (Voorhees et al., 2005).
Statistically significant differences of performance
are determined using the two-tailed paired t-test
computed at a 95% confidence level based on the
average performance per source document.
</bodyText>
<footnote confidence="0.95267">
3https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.992983">
529
</page>
<table confidence="0.616294933333333">
Model MAP 5 P@5 MAP 10 P@10
VSM 0.4968 0.3435 0.4752 0.3841
WE 0.4356 0.3354 0.4624 0.3841
LDA 0.6134 0.4775 0.6071 0.5295
Ours-S 0.5325 0.3762 0.5012 0.4054
Ours 0.6494 0.5597 0.6338 0.5502
Value
0.35 0.45 0.55 0.65
● ●● ● ● ● ●
● ●
●
● MAP_5
P@5
MAP_10
P@10
</table>
<tableCaption confidence="0.86497275">
Table 1: The performance evaluated as a ranking
task. Here Ours corresponds to our full model,
while Ours-S is a simplified version of our model
without iterative propagation (see Section 3.2).
</tableCaption>
<figure confidence="0.873315">
1 2 3 4 5 6 7 8 9 10
Ratio (sd−nodes / bk−nodes)
</figure>
<figureCaption confidence="0.9416215">
Figure 4: The performance of our model with dif-
ferent ratios between sd-nodes and bk-nodes.
</figureCaption>
<bodyText confidence="0.999270666666667">
Results The performance of multiple models is
shown in Table 1. Overall, our full model Ours
outperforms all the baseline systems significantly
in every metric. When evaluating the top 10 triples
with the highest relevance weight, our framework
outperforms the best baseline LDA by 4.4% in
MAP and by 3.91% in P@N. When evaluating the
top 5 triples, our framework performs even better
and significantly outperforms the best baseline by
5.87% in MAP and by 17.21% in P@N.
To analyze the results further, Ours-S, the sim-
plified version of our model without iterative
propagation, outperforms two strong baselines
VSM and WE, which indicates the effectiveness
of encoding distributional semantics. However,
the performance of this simplified model is not as
good as that of LDA, because Ours-S evaluates the
relevance with simple accumulation, which fails
to capture the relatedness between multiple triples
from the source document. We tackle this prob-
lem by incorporating the modified iterative propa-
gation over the entire triple graph into Ours, which
achieves the best performance. One possible prob-
lem is why WE has a poor performance, the reason
of which lies in the setup of our evaluation, where
we label positive and negative instances according
to the class information of triples and documents.
This is better fit for topic model–based methods.
Discussion We further analyze the impact of the
three modifications we made to the original model
(see Section 3.2). We first focus on the impact
of decreasing the relevance weight of bk-nodes
and increasing that of sd-nodes after every itera-
tion. As mentioned previously, we change their
relevance weight according to a fixed ratio, which
is important to the performance. Figure 4 shows
the performance of models with different ratios.
With any increase of the ratio, our model improves
its performance in every metric, which shows the
effectiveness of this setup. The performance re-
mains stable from the value of 10:1, which is thus
chosen as the final value in our experiments. We
then turn to the other two modifications about the
edges between bk-nodes and the setup of propaga-
tion probability. Table 2 shows the performance of
our full model and the simplified models without
these two modifications. With the edges between
bk-nodes, our model improves the performance by
1.48% in MAP 5 and by 1.82% in P@5. With the
modified iterative propagation, we achieve a even
greater improvement of 13.99% in MAP 5 and
24.27% in P@5. All these improvements are sta-
tistically significant, which indicates the effective-
ness of these modifications to the original model.
</bodyText>
<table confidence="0.98780275">
Model MAP 5 P@5 MAP 10 P@10
Full 0.6494 0.5597 0.6338 0.5502
Full−bb 0.6399 0.5497 0.6254 0.5404
Full−p 0.5697 0.4504 0.5485 0.4409
</table>
<tableCaption confidence="0.962058">
Table 2: The performance of our full model (Full)
and two simplified models without modifications:
</tableCaption>
<listItem confidence="0.985053666666667">
(1) without edges between bk-nodes (Full−bb),
(2) without the newly proposed definition of prop-
agation probability between nodes (Full−p).
</listItem>
<subsectionHeader confidence="0.998134">
5.2 Task-based evaluation
</subsectionHeader>
<bodyText confidence="0.999436">
Data preparation To carry out the task-based
evaluation, we use the same Chinese corpus as that
in previous experiments, which consists of 17,199
documents extracted from Internet news in 9 top-
ics. We also use the same tool (Che et al., 2010) to
extract triples of both source document and back-
ground knowledge. For every document D to be
classified, we first use our model to get the top N
most-relevant triples to D, and then use them as
extra features for the original model. We conduct
a direct comparison between the models with and
</bodyText>
<page confidence="0.992967">
530
</page>
<table confidence="0.9970328">
Model P R F
VSM+one-hot 0.8214 0.8146 0.8168
VSM+tf-idf 0.8381 0.8333 0.8336
LDA+SVM 0.8512 0.8422 0.8436
LDA+SVM+Ours-S 0.8584 0.8489 0.8501
LDA+SVM+Ours 0.8748 0.8689 0.8691
● ● ●● ● ● ● ●
●
● P
R
F
●
●
0.85 0.86 0.87
Value
</table>
<tableCaption confidence="0.773879333333333">
Table 3: The performance of document classifica-
tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)
and without (others) background knowledge.
</tableCaption>
<bodyText confidence="0.99345158974359">
without background knowledge to evaluate the im-
pact of introducing background knowledge.
Baseline systems We first illustrate two base-
lines without background knowledge based on
VSM and LDA. For VSM, the test document D
is represented as a bag of words, where the word
distribution over candidate topics is trained on
the same corpus mentioned previously. Then we
evaluate the similarity between D and a candi-
date topic with cosine-similarity directly, where
the topic with the highest similarity will be chosen
as the final class. We use two setups: (1) VSM-
one-hot represents a word as 1 if it appears in a
document or topic, or 0 if not. (2) VSM-tf-idf rep-
resents a word as the value of tf-idf. For LDA,
we re-implement the state-of-the-art system as an-
other baseline, which represents D as a topic vec-
tor Vd in the parameter estimation step, and then
introduces a SVM classifier to takeVd as input and
decide the final class in the inference step.
We also evaluate the impact of knowledge qual-
ity by proposing two different models to introduce
background knowledge: our full model introduced
in Section 3 (Ours), and a simplified version of
our model without iterative propagation (Ours-S).
They have different performances on introducing
background knowledge as shown in previous ex-
periments (see Section 5.1). We then conduct a di-
rect comparison between the document classifica-
tion models with these conditions, whose differing
performances demonstrates the impact of different
qualities of background knowledge on this task.
Results Table 3 shows the results. We use P, R, F
to evaluate the performance, which are computed
as the micro-average over 9 topics. Both models
with background knowledge (LDA+SVM+Ours-
S, LDA+SVM+Ours) outperform systems without
knowledge, which shows that the introduction of
background knowledge helps in better classifica-
</bodyText>
<figure confidence="0.885937">
6.0 6.2 6.4 6.6 6.8 7.0
Threshold of relevance weight
</figure>
<figureCaption confidence="0.988068">
Figure 5: The performance of document classifica-
</figureCaption>
<bodyText confidence="0.986891448275862">
tion models with different thresholds. The knowl-
edge whose relevance weight to the source docu-
ment exceeds the threshold will be introduced as
background knowledge.
tion of documents. The system with the simpli-
fied version of our model without iterative prop-
agation (LDA+SVM+Ours-S) achieves a F-value
of 0.8501, which outperforms the other baselines
without knowledge too. Moreover, the system
with our full model (LDA+SVM+Ours) achieves
the best performance, a F-value of 0.8691, and
outperforms the best baseline LDA+SVM signif-
icantly. This shows that introducing better qual-
ity of background knowledge is helpful to the bet-
ter classification of documents. Statistical signif-
icance is also verified using the two-tailed paired
t-test computed at a 95% confidence level based
on the results of classification over the test set.
Discussion One important question here is how
much background knowledge to include. As men-
tioned in Section 4, we have tried two different
solutions: (1) introducing a fixed amount of back-
ground knowledge for every document, and (2)
setting a threshold and selecting knowledge whose
relevance weight exceeds the threshold. The re-
sults are shown in Table 4, where the systems
with threshold outperform that with fixed amount,
which shows that the threshold helps in better in-
troduction of background knowledge.
</bodyText>
<table confidence="0.9993376">
Model P R F
Ours-S+Top5 0.8522 0.8444 0.8456
Ours-S+ThreD 0.8584 0.8489 0.8501
Ours+Top5 0.8769 0.8667 0.8677
Ours+ThreD 0.8748 0.8689 0.8691
</table>
<tableCaption confidence="0.814543666666667">
Table 4: The performance of document classifica-
tion with the full model (Ours) and the simplified
model (Ours-S) to introduce knowledge.
</tableCaption>
<page confidence="0.995391">
531
</page>
<bodyText confidence="0.999996727272727">
We also evaluate the impact of different thresh-
olds as shown in Figure 5. The performance keeps
improving as the threshold increases up to 6.4 and
becomes steady from 6.4 to 6.7, while it begins to
decline sharply from 6.7. This is reasonable be-
cause at the beginning, as the threshold increases,
we recall more background knowledge and pro-
vide more information. However, with the further
increase of the threshold, we introduce more noise,
which decreases the performance. In our experi-
ments, we choose 6.4 as the final threshold.
</bodyText>
<sectionHeader confidence="0.998067" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99995156">
This study encodes distributional semantics into
the triple-based background knowledge ranking
model (Zhang et al., 2014) for better document
enrichment. We first use LDA to represent ev-
ery triple as a real-valued vector, which is used to
evaluate the relatedness between triples, and then
propose a modified iterative propagation model to
rank all the triples of background knowledge. For
evaluation, we conduct two series of experiments:
(1) evaluation as ranking problem, and (2) task-
based evaluation, especially for document classifi-
cation. In the first set of experiments, our model
outperforms multiple strong baselines based on
VSM, LDA, and WE. In the second set of exper-
iments, our full model with background knowl-
edge outperforms the state-of-the-art systems sig-
nificantly. Moreover, we also explore the impact
of knowledge quality and show its importance.
In our future work, we wish to explore a better
way to encode distributional semantics by propos-
ing a modified LDA for better triples representa-
tion. In addition, we also want to explore the ef-
fect of introducing background knowledge in con-
junction with other NLP tasks, especially with dis-
course parsing (Marcu, 2000; Pitler et al., 2009).
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992985">
We would like to thank our colleagues for
their great help. This work was partly sup-
ported by National Natural Science Foundation
of China via grant 61133012, the National 863
Leading Technology Research Project via grant
2015AA015407, and the National Natural Science
Foundation of China Surface Project via grant
61273321.
</bodyText>
<sectionHeader confidence="0.996061" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999730277777778">
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. Measuring semantic similarity be-
tween words using web search engines. Proceedings
of the 16th International Conference on World Wide
Web, 7:757–766.
Volha Bryl, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In
Proceedings of the 2010 Conference on ECAI 2010:
19th European Conference on Artificial Intelligence,
volume 10, pages 759–764.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A Chinese language technology platform. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, pages
13–16. Association for Computational Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, volume 7,
pages 708–716.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second genera-
tion. In Proceedings of the Twenty-Second inter-
national joint conference on Artificial Intelligence-
Volume Volume One, pages 3–10. AAAI Press.
Samah Fodeh, Bill Punch, and Pang-Ning Tan. 2011.
On ontology-driven document clustering using core
semantic features. Knowledge and Information Sys-
tems, 28(2):395–421.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765–774. ACM.
Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai
Zhang, and Houfeng Wang. 2013. Learning entity
representation for entity disambiguation. Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), August.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F¨urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 782–792. Association for Computational
Linguistics.
</reference>
<page confidence="0.970471">
532
</page>
<reference confidence="0.999886989690722">
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: a
spatially and temporally enhanced knowledge base
from Wikipedia. Artificial Intelligence, 194:28–61.
Xiaohua Hu, Xiaodan Zhang, Caimei Lu, Eun K Park,
and Xiaohua Zhou. 2009. Exploiting Wikipedia
as external knowledge for document clustering. In
Proceedings of the 15th International Conference on
Knowledge Discovery and Data Mining, pages 389–
396. ACM.
Saurabh S Kataria, Krishnan S Kumar, Rajeev R Ras-
togi, Prithviraj Sen, and Srinivasan H Sengamedu.
2011. Entity disambiguation with hierarchical topic
models. In Proceedings of the 17th International
Conference on Knowledge Discovery and Data Min-
ing, pages 1037–1045. ACM.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of Wikipedia entities in web text. In Proceed-
ings of the 15th International Conference on Knowl-
edge Discovery and Data Mining, pages 457–466.
ACM.
Yogan Jaya Kumar and Naomie Salim. 2012. Au-
tomatic multi document summarization approaches.
Journal of Computer Science, 8(1).
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395–448.
Vivi Nastase, Michael Strube, Benjamin B¨orschinger,
C¨acilia Zirn, and Anas Elghafari. 2010. Wikinet: A
very large scale multi-lingual concept network. In
Proceeding of the 7th International Conference on
Language Resources and Evaluation.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and lures:
Associating web queries with structured entities. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 83–92. Associ-
ation for Computational Linguistics.
Xuan-Hieu Phan, Le-Minh Nguyen, and Susumu
Horiguchi. 2008. Learning to classify short and
sparse text &amp; web with hidden topics from large-
scale data collections. In Proceedings of the 17th
International Conference on World Wide Web, pages
91–100. ACM.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 683–691. Association for Computational
Linguistics.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
Jean-Paul Sansonnet and Franc¸ois Bouchet. 2010.
Extraction of agent psychological behaviors from
glosses of WordNet personality adjectives. In Proc.
of the 8th European Workshop on Multi-Agent Sys-
tems (EUMAS10).
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34(1):1–47, March.
Prithviraj Sen. 2012. Collective context-aware topic
models for entity disambiguation. In Proceedings
of the 21st International Conference on World Wide
Web, pages 729–738. ACM.
Steffen Staab and Rudi Studer. 2009. Handbook on
Ontologies. Springer Publishing Company, Incor-
porated, 2nd edition.
Koun-Tem Sun, Yueh-Min Huang, and Ming-Chi
Liu. 2011. A WordNet-based near-synonyms and
similar-looking word learning system. Educational
Technology &amp; Society, 14(1):121–134.
Ellen M Voorhees, Donna K Harman, et al. 2005.
TREC: Experiment and evaluation in information
retrieval, volume 63. MIT press Cambridge.
Muyu Zhang, Bing Qin, Ting Liu, and Mao Zheng.
2014. Triple based background knowledge ranking
for document enrichment. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers, pages
917–927, Dublin, Ireland, August. Dublin City Uni-
versity and Association for Computational Linguis-
tics.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xi-
aoyan Zhu. 2010. Learning to link entities with
knowledge base. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 483–491. Association for
Computational Linguistics.
</reference>
<page confidence="0.998962">
533
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.556848">
<title confidence="0.997793">Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment</title>
<author confidence="0.940134">Bing Mao Graeme</author>
<author confidence="0.940134">Ting</author>
<affiliation confidence="0.891315666666667">Center for Social Computing and Information Harbin Institute of Technology, Harbin, of Computer Science, University of Toronto, Toronto, ON,</affiliation>
<email confidence="0.999744">gh@cs.toronto.edu</email>
<abstract confidence="0.995100961538462">enrichment on retrieving relevant knowledge from external resources, which is essential because text is generally replete with gaps. Since conventional work primarily relies on special rewe instead use triples of Object knowledge and incorporate distributional semantics to rank them. Our model first extracts these triples automatically from raw text and converts them into real-valued vectors based on the word semantics captured by Latent Dirichlet Allocation. We then represent these triples, together with the source document that is to be enriched, as a graph of triples, and adopt a global iterative algorithm to propagate relevance weight from source document to these triples so as to select the most relevant ones. Evaluated as a ranking problem, our model significantly outperforms multiple strong baselines. Moreover, we conduct a task-based evaluation by incorporating these triples as additional features into document classification and enhances the performance by 3.02%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="12484" citStr="Blei et al. (2003)" startWordPosition="2062" endWordPosition="2065">wledge together as a document-level triple graph as illustrated in Section 2. Then we use LDA to capture the distributional semantics of a triple by representing it as a vector of distributional probabilities over k topics and evaluate the relatedness between two triples with cosine-similarity. Finally, we propose a modified iterative process to propagate the relevance score from the source document information to the background knowledge and select the top n relevant ones. 3.1 Encoding distributional semantics LDA LDA is a popular generative probabilistic model, which was first introduced by Blei et al. (2003). LDA views every document as a mixture over underlying topics, and each topic as a distribution over words. Both the document-topic and the topic-word distributions are assumed to have a Dirichlet prior. Given a set of documents and a number of topics, the model returns 0d, the topic distribution for each document d, and biz, the word distribution for every topic z. LDA assumes the following generative process for each document in a corpus D: 1. Choose N — Poisson(4). 2. Choose 0 — Dir(a). (a) Choose a topic zn — Multinomial(0). (b) Choose a word wn from p(wn|zn,/3) conditioned on the topic z</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Measuring semantic similarity between words using web search engines.</title>
<date>2007</date>
<booktitle>Proceedings of the 16th International Conference on World Wide Web,</booktitle>
<pages>7--757</pages>
<contexts>
<context position="9913" citStr="Bollegala et al., 2007" startWordPosition="1592" endWordPosition="1595">mpute the weight of a edge, Zhang et al. (2014) evaluate the semantic relatedness between two nodes with a search engine–based method. They first convert every node, which is a triple of SPO, into a query by combining the text of Subject and Object together. Then for every pair of nodes ti and tj, they construct three queries: p, q, and p n q, which correspond to the queries of ti, tj, and tj n tj, the combination of ti and tj. All these queries are put into a search engine to get H(p), H(q), and H(p n q), the numbers of returned pages for query p, p, and pnq. Then the WebJaccard Coefficient (Bollegala et al., 2007) is used to evaluate r(i, j), the relatedness between ti and tj, according to Formula 1. r(i, j) = WebJaccard(p,q) = { 0 if H(p n q) &lt; C H(p n q) otherwise. H(p) +H(q) — H(p n q) (1) Using r(i, j), Zhang et al. (2014) further define p(i, j), the probability of ti and tj propagating to each other, as shown in Formula 2. Here N is the set of all nodes, and 3(i, j) denotes whether an edge exists between two nodes or not. r(i, j) x 3(i, j) (2) p(i, j) = ∑ncN r(n, j) x 3(n, j) Iterative propagation Considering that the source document D is represented as a graph of sd-nodes, so the relevance of bac</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2007</marker>
<rawString>Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2007. Measuring semantic similarity between words using web search engines. Proceedings of the 16th International Conference on World Wide Web, 7:757–766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volha Bryl</author>
<author>Claudio Giuliano</author>
<author>Luciano Serafini</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>Using background knowledge to support coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence,</booktitle>
<volume>10</volume>
<pages>759--764</pages>
<contexts>
<context position="2290" citStr="Bryl et al., 2010" startWordPosition="344" endWordPosition="347">information is usually omitted by the author to make the document more concise. For example, Baghdad is the capital of Iraq is omitted in Figure 1a. A human will fill these gaps automatically with the background knowledge in his mind. However, the machine lacks both the ∗ This work was partly done while the first author was visiting University of Toronto. necessary background knowledge and the ability to select. The task of document enrichment is proposed to tackle this problem, and has been proved helpful in many NLP tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Sen, 2012). We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. De</context>
</contexts>
<marker>Bryl, Giuliano, Serafini, Tymoshenko, 2010</marker>
<rawString>Volha Bryl, Claudio Giuliano, Luciano Serafini, and Kateryna Tymoshenko. 2010. Using background knowledge to support coreference resolution. In Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence, volume 10, pages 759–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
</authors>
<title>Ltp: A Chinese language technology platform.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations,</booktitle>
<pages>13--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22917" citStr="Che et al., 2010" startWordPosition="3827" endWordPosition="3830">ta preparation The data is composed of two parts: source documents and background knowledge. For source documents, we use a publicly available Chinese corpus which consists of 17,199 documents and 13,719,428 tokens extracted from Internet news2 including 9 topics: Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Military. We then randomly but equally select 600 articles as the set of source documents from 9 topics without data bias. We use all the other 16,599 documents of the same corpus as the source of background knowledge, and then introduce a wellknown Chinese open source tool (Che et al., 2010) to extract the triples of background knowledge from the raw text automatically. So the background knowledge also distributes evenly across the same 9 topics. We use the same tool to extract the triples of source documents too. Baseline systems As Zhang et al. (2014) argued, it is difficult to use the methods in traditional ranking tasks, such as information retrieval (Manning et al., 2008) and entity linking (Han et al., 2011; Sen, 2012), as baselines in this task, because our model takes triples as basic input and thus lacks some crucial information such as link structure. For better compari</context>
<context position="30212" citStr="Che et al., 2010" startWordPosition="5036" endWordPosition="5039">0 P@10 Full 0.6494 0.5597 0.6338 0.5502 Full−bb 0.6399 0.5497 0.6254 0.5404 Full−p 0.5697 0.4504 0.5485 0.4409 Table 2: The performance of our full model (Full) and two simplified models without modifications: (1) without edges between bk-nodes (Full−bb), (2) without the newly proposed definition of propagation probability between nodes (Full−p). 5.2 Task-based evaluation Data preparation To carry out the task-based evaluation, we use the same Chinese corpus as that in previous experiments, which consists of 17,199 documents extracted from Internet news in 9 topics. We also use the same tool (Che et al., 2010) to extract triples of both source document and background knowledge. For every document D to be classified, we first use our model to get the top N most-relevant triples to D, and then use them as extra features for the original model. We conduct a direct comparison between the models with and 530 Model P R F VSM+one-hot 0.8214 0.8146 0.8168 VSM+tf-idf 0.8381 0.8333 0.8336 LDA+SVM 0.8512 0.8422 0.8436 LDA+SVM+Ours-S 0.8584 0.8489 0.8501 LDA+SVM+Ours 0.8748 0.8689 0.8691 ● ● ●● ● ● ● ● ● ● P R F ● ● 0.85 0.86 0.87 Value Table 3: The performance of document classification with (LDA+SVM+Ours-S, </context>
</contexts>
<marker>Che, Li, Liu, 2010</marker>
<rawString>Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A Chinese language technology platform. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, pages 13–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<volume>7</volume>
<pages>708--716</pages>
<contexts>
<context position="2624" citStr="Cucerzan, 2007" startWordPosition="400" endWordPosition="401">versity of Toronto. necessary background knowledge and the ability to select. The task of document enrichment is proposed to tackle this problem, and has been proved helpful in many NLP tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Sen, 2012). We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontologies (Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledg</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, volume 7, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Anthony Fader</author>
<author>Janara Christensen</author>
<author>Stephen Soderland</author>
<author>Mausam Mausam</author>
</authors>
<title>Open information extraction: The second generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume One,</booktitle>
<pages>3--10</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="7999" citStr="Etzioni et al., 2011" startWordPosition="1254" endWordPosition="1257"> to rank all the triples. We describe this work in detail below. Triple graph Zhang et al. (2014) proposed the triple graph as a document representation, where the triples of SPO serve as nodes, and the edges between nodes indicate their semantic relatedness. There are two kinds of nodes in the triple graph: (1) source document nodes (sd-nodes), which are triples extracted from source documents, and (2) background knowledge nodes (bk-nodes), which are triples extracted from external sources. Both of them are extracted automatically with Reverb, a well-known Open Information Extraction system (Etzioni et al., 2011). There are also two kinds of edges: (1) an edge between a pair of sd-nodes, and (2) an edge between one sd-node and another bk-node, both of which are unidirectional. In the original representation, there are no edges between two bk-nodes because they treat the bk-nodes as recipients of relevance weight only. In this paper, we modify this setup and connect every pair of bknodes with an edge, so the bk-nodes serve as intermediate nodes during the iterative propagation process and contribute to the final performance too as shown in our experiments (see Section 5.1). S1: Th coaliton may never kn</context>
</contexts>
<marker>Etzioni, Fader, Christensen, Soderland, Mausam, 2011</marker>
<rawString>Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open information extraction: The second generation. In Proceedings of the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume One, pages 3–10. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samah Fodeh</author>
<author>Bill Punch</author>
<author>Pang-Ning Tan</author>
</authors>
<title>On ontology-driven document clustering using core semantic features.</title>
<date>2011</date>
<journal>Knowledge and Information Systems,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="3176" citStr="Fodeh et al., 2011" startWordPosition="494" endWordPosition="497">e content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontologies (Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge bases such as WordNet (Nastase et al., 2010), which provide semantic information about words such as synonym (Sun et al., 2011) and antonym (Sansonnet and Bouchet, 2010). However, these methods primarily rely on special resources constructed with supervision or even manually, which are difficult to expand and in turn limit their applications in practice. In contrast, we wish to seek the benefits of both coverage and accuracy from a better representation of background knowledge: triples of Subject, Predicate, Object (SPO). According to Hoffart e</context>
</contexts>
<marker>Fodeh, Punch, Tan, 2011</marker>
<rawString>Samah Fodeh, Bill Punch, and Pang-Ning Tan. 2011. On ontology-driven document clustering using core semantic features. Knowledge and Information Systems, 28(2):395–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Le Sun</author>
<author>Jun Zhao</author>
</authors>
<title>Collective entity linking in web text: a graph-based method.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,</booktitle>
<pages>765--774</pages>
<publisher>ACM.</publisher>
<marker>Han, Le Sun, Zhao, 2011</marker>
<rawString>Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective entity linking in web text: a graph-based method. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 765–774. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengyan He</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Longkai Zhang</author>
<author>Houfeng Wang</author>
</authors>
<title>Learning entity representation for entity disambiguation.</title>
<date>2013</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<contexts>
<context position="2664" citStr="He et al., 2013" startWordPosition="406" endWordPosition="409">nd knowledge and the ability to select. The task of document enrichment is proposed to tackle this problem, and has been proved helpful in many NLP tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Sen, 2012). We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontologies (Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge bases such as WordNet (Nastase et al.,</context>
</contexts>
<marker>He, Liu, Li, Zhou, Zhang, Wang, 2013</marker>
<rawString>Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai Zhang, and Houfeng Wang. 2013. Learning entity representation for entity disambiguation. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed Amir Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>782--792</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782–792. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Fabian M Suchanek</author>
<author>Klaus Berberich</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago2: a spatially and temporally enhanced knowledge base from Wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--28</pages>
<contexts>
<context position="3788" citStr="Hoffart et al. (2013)" startWordPosition="589" endWordPosition="592">al., 2011; Kumar and Salim, 2012) and structured knowledge bases such as WordNet (Nastase et al., 2010), which provide semantic information about words such as synonym (Sun et al., 2011) and antonym (Sansonnet and Bouchet, 2010). However, these methods primarily rely on special resources constructed with supervision or even manually, which are difficult to expand and in turn limit their applications in practice. In contrast, we wish to seek the benefits of both coverage and accuracy from a better representation of background knowledge: triples of Subject, Predicate, Object (SPO). According to Hoffart et al. (2013), these triples, such as LeonardCohen, wasBornIn, Montreal, can be extracted automatically from Wikipedia and other sources, which is compatible with the RDF data model (Staab and Studer, 2009). Moreover, by extracting these 524 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 524–533, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics (a) Source document: air strike aiming at Saddam in Baghdad Figure 1: An example of document enrichment: A sou</context>
</contexts>
<marker>Hoffart, Suchanek, Berberich, Weikum, 2013</marker>
<rawString>Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: a spatially and temporally enhanced knowledge base from Wikipedia. Artificial Intelligence, 194:28–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Hu</author>
<author>Xiaodan Zhang</author>
<author>Caimei Lu</author>
<author>Eun K Park</author>
<author>Xiaohua Zhou</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for document clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>389--396</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2326" citStr="Hu et al., 2009" startWordPosition="350" endWordPosition="353">author to make the document more concise. For example, Baghdad is the capital of Iraq is omitted in Figure 1a. A human will fill these gaps automatically with the background knowledge in his mind. However, the machine lacks both the ∗ This work was partly done while the first author was visiting University of Toronto. necessary background knowledge and the ability to select. The task of document enrichment is proposed to tackle this problem, and has been proved helpful in many NLP tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Sen, 2012). We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these </context>
</contexts>
<marker>Hu, Zhang, Lu, Park, Zhou, 2009</marker>
<rawString>Xiaohua Hu, Xiaodan Zhang, Caimei Lu, Eun K Park, and Xiaohua Zhou. 2009. Exploiting Wikipedia as external knowledge for document clustering. In Proceedings of the 15th International Conference on Knowledge Discovery and Data Mining, pages 389– 396. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saurabh S Kataria</author>
<author>Krishnan S Kumar</author>
<author>Rajeev R Rastogi</author>
<author>Prithviraj Sen</author>
<author>Srinivasan H Sengamedu</author>
</authors>
<title>Entity disambiguation with hierarchical topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>1037--1045</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2646" citStr="Kataria et al., 2011" startWordPosition="402" endWordPosition="405">to. necessary background knowledge and the ability to select. The task of document enrichment is proposed to tackle this problem, and has been proved helpful in many NLP tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Sen, 2012). We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontologies (Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge bases such as WordNe</context>
</contexts>
<marker>Kataria, Kumar, Rastogi, Sen, Sengamedu, 2011</marker>
<rawString>Saurabh S Kataria, Krishnan S Kumar, Rajeev R Rastogi, Prithviraj Sen, and Srinivasan H Sengamedu. 2011. Entity disambiguation with hierarchical topic models. In Proceedings of the 17th International Conference on Knowledge Discovery and Data Mining, pages 1037–1045. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of Wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>457--466</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2814" citStr="Kulkarni et al., 2009" startWordPosition="431" endWordPosition="434">LP tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Sen, 2012). We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontologies (Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge bases such as WordNet (Nastase et al., 2010), which provide semantic information about words such as synonym (Sun et al., 2011) and antonym (Sansonnet and Bouchet, 2010). However, these me</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of Wikipedia entities in web text. In Proceedings of the 15th International Conference on Knowledge Discovery and Data Mining, pages 457–466. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yogan Jaya Kumar</author>
<author>Naomie Salim</author>
</authors>
<title>Automatic multi document summarization approaches.</title>
<date>2012</date>
<journal>Journal of Computer Science,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="3200" citStr="Kumar and Salim, 2012" startWordPosition="498" endWordPosition="501">ia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontologies (Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge bases such as WordNet (Nastase et al., 2010), which provide semantic information about words such as synonym (Sun et al., 2011) and antonym (Sansonnet and Bouchet, 2010). However, these methods primarily rely on special resources constructed with supervision or even manually, which are difficult to expand and in turn limit their applications in practice. In contrast, we wish to seek the benefits of both coverage and accuracy from a better representation of background knowledge: triples of Subject, Predicate, Object (SPO). According to Hoffart et al. (2013), these trip</context>
</contexts>
<marker>Kumar, Salim, 2012</marker>
<rawString>Yogan Jaya Kumar and Naomie Salim. 2012. Automatic multi document summarization approaches. Journal of Computer Science, 8(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1.</title>
<date>2008</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The rhetorical parsing of unrestricted texts: A surface-based approach.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The rhetorical parsing of unrestricted texts: A surface-based approach. Computational Linguistics, 26(3):395–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
<author>Benjamin B¨orschinger</author>
<author>C¨acilia Zirn</author>
<author>Anas Elghafari</author>
</authors>
<title>Wikinet: A very large scale multi-lingual concept network.</title>
<date>2010</date>
<booktitle>In Proceeding of the 7th International Conference on Language Resources and Evaluation.</booktitle>
<marker>Nastase, Strube, B¨orschinger, Zirn, Elghafari, 2010</marker>
<rawString>Vivi Nastase, Michael Strube, Benjamin B¨orschinger, C¨acilia Zirn, and Anas Elghafari. 2010. Wikinet: A very large scale multi-lingual concept network. In Proceeding of the 7th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Ariel Fuxman</author>
</authors>
<title>Jigs and lures: Associating web queries with structured entities.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>83--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2246" citStr="Pantel and Fuxman, 2011" startWordPosition="337" endWordPosition="340">ng the writing of text, some basic but well-known information is usually omitted by the author to make the document more concise. For example, Baghdad is the capital of Iraq is omitted in Figure 1a. A human will fill these gaps automatically with the background knowledge in his mind. However, the machine lacks both the ∗ This work was partly done while the first author was visiting University of Toronto. necessary background knowledge and the ability to select. The task of document enrichment is proposed to tackle this problem, and has been proved helpful in many NLP tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Sen, 2012). We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) be</context>
</contexts>
<marker>Pantel, Fuxman, 2011</marker>
<rawString>Patrick Pantel and Ariel Fuxman. 2011. Jigs and lures: Associating web queries with structured entities. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 83–92. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
<author>Le-Minh Nguyen</author>
<author>Susumu Horiguchi</author>
</authors>
<title>Learning to classify short and sparse text &amp; web with hidden topics from largescale data collections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web,</booktitle>
<pages>91--100</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="14154" citStr="Phan et al., 2008" startWordPosition="2354" endWordPosition="2357">eter a is a k-vector with components ai &gt; 0; and the P indicates the word probabilities over topics, which is a matrix with Pij = p(wj = 1|zi = 1). Figure 2 shows the representation of LDA as a probabilistic graphical model with three levels. There are two corpus-level parameters a and P , which are assumed to be sampled once in the process of generating a corpus; one document-level variable 0d, which is sampled once per document; and two word-level variables zdn and wdn, which are sampled once for each word in each document. We employ the publicly available implementation of LDA, JGibbLDA21 (Phan et al., 2008), which has two main execution methods: parameter estimation (model building) and inference for new data (classification of a new document). Relevance evaluation Given a set of documents and the number of topics k, LDA will return biz, the word distribution over the topic z. So for every word wn, we get k distributional probabilities over k topics. We use pwnzi to denote the probability that wn appears in the ith topic zi, where i &lt; k, zi E Z, the set of k topics. Then we combine these k possibilities together as a real-valued vectorvwn to represent wn as shown in Formula 4. vwn = (pwnz1, pwnz</context>
</contexts>
<marker>Phan, Nguyen, Horiguchi, 2008</marker>
<rawString>Xuan-Hieu Phan, Le-Minh Nguyen, and Susumu Horiguchi. 2008. Learning to classify short and sparse text &amp; web with hidden topics from largescale data collections. In Proceedings of the 17th International Conference on World Wide Web, pages 91–100. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>683--691</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 683–691. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1986</date>
<publisher>McGrawHill, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="24203" citStr="Salton and McGill, 1986" startWordPosition="4038" endWordPosition="4041">proved effective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA). Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with 2http://www.sogou.com/labs/dl/c.html the publicly available tool word2vec3. Here we use ti, D, and wi to denote a triple of background knowledge, a source document, and the relevance of ti to D. For VSM, we represent both ti and D with a tf-idf scheme first (Salton and McGill, 1986) and compute wi as their cosinesimilarity. For WE, we first convert both ti and the triples extracted from D into real-valued vectors with WE and then compute wi by accumulating all the cosine-similarities between ti and every triple from D. For LDA, we represent ti as a vector with our model introduced in Section 3.1 and get the vector of D directly with LDA. Then we evaluate their relevance of ti to D by computing the cosinesimilarity of two corresponding vectors. Moreover, to determine whether our modified iterative propagation is helpful or not, we also compare our full model (Ours) agains</context>
</contexts>
<marker>Salton, McGill, 1986</marker>
<rawString>Gerard Salton and Michael J. McGill. 1986. Introduction to Modern Information Retrieval. McGrawHill, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Paul Sansonnet</author>
<author>Franc¸ois Bouchet</author>
</authors>
<title>Extraction of agent psychological behaviors from glosses of WordNet personality adjectives.</title>
<date>2010</date>
<booktitle>In Proc. of the 8th European Workshop on Multi-Agent Systems (EUMAS10).</booktitle>
<contexts>
<context position="3395" citStr="Sansonnet and Bouchet, 2010" startWordPosition="529" endWordPosition="532">nd structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontologies (Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge bases such as WordNet (Nastase et al., 2010), which provide semantic information about words such as synonym (Sun et al., 2011) and antonym (Sansonnet and Bouchet, 2010). However, these methods primarily rely on special resources constructed with supervision or even manually, which are difficult to expand and in turn limit their applications in practice. In contrast, we wish to seek the benefits of both coverage and accuracy from a better representation of background knowledge: triples of Subject, Predicate, Object (SPO). According to Hoffart et al. (2013), these triples, such as LeonardCohen, wasBornIn, Montreal, can be extracted automatically from Wikipedia and other sources, which is compatible with the RDF data model (Staab and Studer, 2009). Moreover, by</context>
</contexts>
<marker>Sansonnet, Bouchet, 2010</marker>
<rawString>Jean-Paul Sansonnet and Franc¸ois Bouchet. 2010. Extraction of agent psychological behaviors from glosses of WordNet personality adjectives. In Proc. of the 8th European Workshop on Multi-Agent Systems (EUMAS10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="19419" citStr="Sebastiani, 2002" startWordPosition="3248" endWordPosition="3249">h two equally but closely related neighbors will only get a probability of 0.5 for each neighbor. We modify this setup by removing the normalization process and computing p(i, j) as the relatedness between ti and tj directly, which is evaluated according to Formula 1 . 4 Encoding background knowledge into document classification In this part, we demonstrate that the introduction of relevant knowledge could be helpful to real NLP applications. In particular, we choose the document classification task as a demonstration, which aims to classify documents into predefined categories automatically (Sebastiani, 2002). We choose this task for two reasons: (1) This task has witnessed a booming interest in the last 20 years, due to the increased availability of documents in digital form and the ensuing need to organize them, so it is important in both research and application. (2) The state-of-the-art performance of this task is achieved by a series of topic model– based methods, which rely on the same model as we do, but make use of source document information only. However, there is always some omitted information and relevant knowledge, which cannot be captured from the source document. Intuitively, the r</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prithviraj Sen</author>
</authors>
<title>Collective context-aware topic models for entity disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st International Conference on World Wide Web,</booktitle>
<pages>729--738</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2364" citStr="Sen, 2012" startWordPosition="357" endWordPosition="358">r example, Baghdad is the capital of Iraq is omitted in Figure 1a. A human will fill these gaps automatically with the background knowledge in his mind. However, the machine lacks both the ∗ This work was partly done while the first author was visiting University of Toronto. necessary background knowledge and the ability to select. The task of document enrichment is proposed to tackle this problem, and has been proved helpful in many NLP tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Sen, 2012). We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain </context>
<context position="23359" citStr="Sen, 2012" startWordPosition="3904" endWordPosition="3905"> use all the other 16,599 documents of the same corpus as the source of background knowledge, and then introduce a wellknown Chinese open source tool (Che et al., 2010) to extract the triples of background knowledge from the raw text automatically. So the background knowledge also distributes evenly across the same 9 topics. We use the same tool to extract the triples of source documents too. Baseline systems As Zhang et al. (2014) argued, it is difficult to use the methods in traditional ranking tasks, such as information retrieval (Manning et al., 2008) and entity linking (Han et al., 2011; Sen, 2012), as baselines in this task, because our model takes triples as basic input and thus lacks some crucial information such as link structure. For better comparison, we implement three methods as baselines, which have been proved effective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA). Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with 2http://www.sogou.com/labs/dl/c.html th</context>
</contexts>
<marker>Sen, 2012</marker>
<rawString>Prithviraj Sen. 2012. Collective context-aware topic models for entity disambiguation. In Proceedings of the 21st International Conference on World Wide Web, pages 729–738. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Staab</author>
<author>Rudi Studer</author>
</authors>
<title>Handbook on Ontologies.</title>
<date>2009</date>
<publisher>Springer Publishing Company,</publisher>
<note>Incorporated, 2nd edition.</note>
<contexts>
<context position="3981" citStr="Staab and Studer, 2009" startWordPosition="618" endWordPosition="621">antonym (Sansonnet and Bouchet, 2010). However, these methods primarily rely on special resources constructed with supervision or even manually, which are difficult to expand and in turn limit their applications in practice. In contrast, we wish to seek the benefits of both coverage and accuracy from a better representation of background knowledge: triples of Subject, Predicate, Object (SPO). According to Hoffart et al. (2013), these triples, such as LeonardCohen, wasBornIn, Montreal, can be extracted automatically from Wikipedia and other sources, which is compatible with the RDF data model (Staab and Studer, 2009). Moreover, by extracting these 524 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 524–533, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics (a) Source document: air strike aiming at Saddam in Baghdad Figure 1: An example of document enrichment: A source document about a U.S. air strike omitting two important pieces of background knowledge which are acquired by our framework. triples from multiple sources, we also get better coverage. There</context>
</contexts>
<marker>Staab, Studer, 2009</marker>
<rawString>Steffen Staab and Rudi Studer. 2009. Handbook on Ontologies. Springer Publishing Company, Incorporated, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koun-Tem Sun</author>
<author>Yueh-Min Huang</author>
<author>Ming-Chi Liu</author>
</authors>
<title>A WordNet-based near-synonyms and similar-looking word learning system.</title>
<date>2011</date>
<pages>14--1</pages>
<publisher>Educational Technology &amp; Society,</publisher>
<contexts>
<context position="3353" citStr="Sun et al., 2011" startWordPosition="523" endWordPosition="526">, 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontologies (Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge bases such as WordNet (Nastase et al., 2010), which provide semantic information about words such as synonym (Sun et al., 2011) and antonym (Sansonnet and Bouchet, 2010). However, these methods primarily rely on special resources constructed with supervision or even manually, which are difficult to expand and in turn limit their applications in practice. In contrast, we wish to seek the benefits of both coverage and accuracy from a better representation of background knowledge: triples of Subject, Predicate, Object (SPO). According to Hoffart et al. (2013), these triples, such as LeonardCohen, wasBornIn, Montreal, can be extracted automatically from Wikipedia and other sources, which is compatible with the RDF data mo</context>
</contexts>
<marker>Sun, Huang, Liu, 2011</marker>
<rawString>Koun-Tem Sun, Yueh-Min Huang, and Ming-Chi Liu. 2011. A WordNet-based near-synonyms and similar-looking word learning system. Educational Technology &amp; Society, 14(1):121–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Donna K Harman</author>
</authors>
<title>TREC: Experiment and evaluation in information retrieval, volume 63.</title>
<date>2005</date>
<publisher>MIT press</publisher>
<location>Cambridge.</location>
<marker>Voorhees, Harman, 2005</marker>
<rawString>Ellen M Voorhees, Donna K Harman, et al. 2005. TREC: Experiment and evaluation in information retrieval, volume 63. MIT press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muyu Zhang</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Mao Zheng</author>
</authors>
<title>Triple based background knowledge ranking for document enrichment.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>917--927</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="4805" citStr="Zhang et al. (2014)" startWordPosition="743" endWordPosition="746">524–533, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics (a) Source document: air strike aiming at Saddam in Baghdad Figure 1: An example of document enrichment: A source document about a U.S. air strike omitting two important pieces of background knowledge which are acquired by our framework. triples from multiple sources, we also get better coverage. Therefore, one can expect that this representation is helpful for better document enrichment by incorporating both accuracy and coverage. In fact, there is already evidence that this representation is helpful. Zhang et al. (2014) proposed a triple-based document enrichment framework which uses triples of SPO as background knowledge. They first proposed a search engine– based method to evaluate the relatedness between every pair of triples, and then an iterative propagation algorithm was introduced to select the most relevant triples to a given source document (see Section 2), which achieved a good performance. However, to evaluate the semantic relatedness between two triples, Zhang et al. (2014) primarily relied on the text of triples and used search engines, which makes their method difficult to re-implement and in t</context>
<context position="7185" citStr="Zhang et al., 2014" startWordPosition="1126" endWordPosition="1129">l features. We then perform a direct comparison between the classification models with and without these triples, to determine whether they are helpful or not. On the first series of experiments, we achieve a MAP of 0.6494 and a P@N of 0.5597 in the best situation, which outperforms the strongest baseline by 5.87% and 17.21%. In the task-based evaluation, the enriched model derived from the triples of background knowledge performs better by 3.02%, which demonstrates the effectiveness of our framework in real NLP applications. 2 Background The most closely related work in this area is our own (Zhang et al., 2014), which used the triples of SPO as background knowledge. In that work, we first proposed a triple graph to represent the source document and then used a search engine– based iterative algorithm to rank all the triples. We describe this work in detail below. Triple graph Zhang et al. (2014) proposed the triple graph as a document representation, where the triples of SPO serve as nodes, and the edges between nodes indicate their semantic relatedness. There are two kinds of nodes in the triple graph: (1) source document nodes (sd-nodes), which are triples extracted from source documents, and (2) </context>
<context position="9337" citStr="Zhang et al. (2014)" startWordPosition="1480" endWordPosition="1483">A B1 bb dd f 2000d S2: A B-1 bombe drpped four 2,000-pound bombs on a building in a residential area of Baghdad in a residential area of Baghdad . S: They had received telligence rS3: They had received intelligence reports that senior official were meeting thee, possibly including Saddam Hussein and his sons . meeting there, possibly including Saddam Hussein and his sons . Two omitted relevant pieces of background knowledge Saddam Hussen hasChild Qusay Hussein k2: k2: Saddam Hussein k1: B k1: Baghdad Iraq Capital hasChild Qusay Hussein 525 Relevance evaluation To compute the weight of a edge, Zhang et al. (2014) evaluate the semantic relatedness between two nodes with a search engine–based method. They first convert every node, which is a triple of SPO, into a query by combining the text of Subject and Object together. Then for every pair of nodes ti and tj, they construct three queries: p, q, and p n q, which correspond to the queries of ti, tj, and tj n tj, the combination of ti and tj. All these queries are put into a search engine to get H(p), H(q), and H(p n q), the numbers of returned pages for query p, p, and pnq. Then the WebJaccard Coefficient (Bollegala et al., 2007) is used to evaluate r(i</context>
<context position="10623" citStr="Zhang et al. (2014)" startWordPosition="1736" endWordPosition="1739">i, j) = WebJaccard(p,q) = { 0 if H(p n q) &lt; C H(p n q) otherwise. H(p) +H(q) — H(p n q) (1) Using r(i, j), Zhang et al. (2014) further define p(i, j), the probability of ti and tj propagating to each other, as shown in Formula 2. Here N is the set of all nodes, and 3(i, j) denotes whether an edge exists between two nodes or not. r(i, j) x 3(i, j) (2) p(i, j) = ∑ncN r(n, j) x 3(n, j) Iterative propagation Considering that the source document D is represented as a graph of sd-nodes, so the relevance of background knowledge tb to D is naturally converted into that of tb to the graph of sd-nodes. Zhang et al. (2014) evaluate this relevance by propagating relevance weight from sd-nodes to tb iteratively. After convergence, the relevance weight of tb will be treated as the final relevance to D. There are in total n x n pairs of nodes, and their p(i, j) are stored in a matrix P. Zhang et al. (2014) use W~ = (w1,w2,...,wn) to denote the relevance weights of nodes, where wi indicates the relevance of ti to D. At the beginning, each wi of bk-nodes is initialized to 0, and each that of sd-nodes is initialized to its importance to D. Then W~ is updated to ~W� after every iteration according to Formula 3. They ke</context>
<context position="16435" citStr="Zhang et al. (2014)" startWordPosition="2752" endWordPosition="2755">relatedness, and k is the number of topics, which is also the length of A (or B). A high value of r(A,B) usually indicates a close relatedness between A and B, and thus a higher probability of propagating to each other in the following modified iterative propagation illustrated in Section 3.2. AB r(A,B) =cos(A,B) = Eki=1 AiBi (7) = / / Ek i=1 (Ai)2 Ek i=1 (Bi)2 3.2 Modified iterative propagation In this part, we propose a modified iterative propagation based ranking model to select the mostrelevant triples of background knowledge. There are three primary modifications to the original model of Zhang et al. (2014), all of which are shown more powerful in our experiments. First of all, the original model (Zhang et al., 2014) does not reset the relevance weight of sdnodes after every iteration. This results in a continued decrease of the relevance weight of sd-nodes, which weakens the effect of sd-nodes during the iterative propagation and in turn affects the final performance. To tackle this problem, we decrease the relevance weight of bk-nodes and increase that of sd-nodes according to a fixed ratio after every iteration, so as to ensure that the total weight of sd-nodes is always higher than that of b</context>
<context position="18465" citStr="Zhang et al. (2014)" startWordPosition="3095" endWordPosition="3098"> bk-nodes also contributes to the better evaluation of relevance to the source document, because the bk-nodes can serve as the intermediate nodes during the iterative propagation over the entire graph. Figure 3 shows an example, where the bk-node John Lennon is close to both the sd-node Beatles and to another bknode Yoko Ono, so the relatedness between two bk-nodes John Lennon and Yoko Ono helps in better evaluation of the relatedness between the bknode Yoko Ono and the sd-node Beatles. We also modify the definition of p(i, j), the probability of two nodes ti and tj propagating to each other. Zhang et al. (2014) compute this probability according to Formula 2, which highlights the number of neighbors, but weakens the relatedness between nodes, due to the normalization. For instance, if a node tx has only one neighbor ty, no matter how low their relatedness is, their p(x,y) will still be equal to 1 in the original model, while another node with two equally but closely related neighbors will only get a probability of 0.5 for each neighbor. We modify this setup by removing the normalization process and computing p(i, j) as the relatedness between ti and tj directly, which is evaluated according to Formu</context>
<context position="23184" citStr="Zhang et al. (2014)" startWordPosition="3872" endWordPosition="3875">Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Military. We then randomly but equally select 600 articles as the set of source documents from 9 topics without data bias. We use all the other 16,599 documents of the same corpus as the source of background knowledge, and then introduce a wellknown Chinese open source tool (Che et al., 2010) to extract the triples of background knowledge from the raw text automatically. So the background knowledge also distributes evenly across the same 9 topics. We use the same tool to extract the triples of source documents too. Baseline systems As Zhang et al. (2014) argued, it is difficult to use the methods in traditional ranking tasks, such as information retrieval (Manning et al., 2008) and entity linking (Han et al., 2011; Sen, 2012), as baselines in this task, because our model takes triples as basic input and thus lacks some crucial information such as link structure. For better comparison, we implement three methods as baselines, which have been proved effective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA). Note that our model captures the distributional semantics of trip</context>
<context position="25330" citStr="Zhang et al., 2014" startWordPosition="4226" endWordPosition="4229">dified iterative propagation is helpful or not, we also compare our full model (Ours) against a simplified version without iterative propagation (OursS). In Ours-S, we represent both ti and the triples extracted from D as real-valued vectors with our model introduced in Section 3.1. Then we compute wi by accumulating all the cosine-similarities between ti and the triples extracted from D. For all the baselines, we rank the triples of background knowledge according to wi, their relevance to D. Experimental setup Previous research relies on manual annotation to evaluate the ranking performance (Zhang et al., 2014), which costs a lot, and in which it is difficult to get high consistency. In this paper, we carry out an automatic evaluation. The corpus we used consists of 9 different classes, from which we extract triples of background knowledge. So correspondingly, there will be 9 sets of triples too. Then we randomly select 200 triples from every class and mix 200 × 9 = 1800 triples together as S, the set of triples of background knowledge. For every document D to be enriched, our model selects the top N mostrelevant triples from S and returns them to D as enrichments. We treat a triple ti selected by o</context>
<context position="35131" citStr="Zhang et al., 2014" startWordPosition="5821" endWordPosition="5824"> Figure 5. The performance keeps improving as the threshold increases up to 6.4 and becomes steady from 6.4 to 6.7, while it begins to decline sharply from 6.7. This is reasonable because at the beginning, as the threshold increases, we recall more background knowledge and provide more information. However, with the further increase of the threshold, we introduce more noise, which decreases the performance. In our experiments, we choose 6.4 as the final threshold. 6 Conclusion and Future Work This study encodes distributional semantics into the triple-based background knowledge ranking model (Zhang et al., 2014) for better document enrichment. We first use LDA to represent every triple as a real-valued vector, which is used to evaluate the relatedness between triples, and then propose a modified iterative propagation model to rank all the triples of background knowledge. For evaluation, we conduct two series of experiments: (1) evaluation as ranking problem, and (2) taskbased evaluation, especially for document classification. In the first set of experiments, our model outperforms multiple strong baselines based on VSM, LDA, and WE. In the second set of experiments, our full model with background kno</context>
</contexts>
<marker>Zhang, Qin, Liu, Zheng, 2014</marker>
<rawString>Muyu Zhang, Bing Qin, Ting Liu, and Mao Zheng. 2014. Triple based background knowledge ranking for document enrichment. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 917–927, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Zheng</author>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Learning to link entities with knowledge base.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>483--491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2742" citStr="Zheng et al., 2010" startWordPosition="420" endWordPosition="423">roposed to tackle this problem, and has been proved helpful in many NLP tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Sen, 2012). We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line encyclopedia, as a resource and introduces the content of Wikipedia pages as external knowledge (Cucerzan, 2007; Kataria et al., 2011; He et al., 2013). Most research in this area relies on the text similarity (Zheng et al., 2010; Hoffart et al., 2011) and structure information (Kulkarni et al., 2009; Sen, 2012; He et al., 2013) between the mention and the Wikipedia page. Despite the apparent success of these methods, most Wikipedia pages contain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontologies (Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge bases such as WordNet (Nastase et al., 2010), which provide semantic information about words such as synonym (Sun et</context>
</contexts>
<marker>Zheng, Li, Huang, Zhu, 2010</marker>
<rawString>Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010. Learning to link entities with knowledge base. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 483–491. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>