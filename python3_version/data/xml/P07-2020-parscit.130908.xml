<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006044">
<title confidence="0.9987845">
Ensemble Document Clustering
Using Weighted Hypergraph Generated by NMF
</title>
<author confidence="0.997497">
Hiroyuki Shinnou, Minoru Sasaki
</author>
<affiliation confidence="0.990147">
Ibaraki University,
</affiliation>
<address confidence="0.811385">
4-12-1 Nakanarusawa, Hitachi,
Ibaraki, Japan 316-8511
</address>
<email confidence="0.999673">
{shinnou,msasaki}@mx.ibaraki.ac.jp
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852777777778">
In this paper, we propose a new ensemble
document clustering method. The novelty
of our method is the use of Non-negative
Matrix Factorization (NMF) in the genera-
tion phase and a weighted hypergraph in the
integration phase. In our experiment, we
compared our method with some clustering
methods. Our method achieved the best re-
sults.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925090909091">
In this paper, we propose a new ensemble docu-
ment clustering method using Non-negative Matrix
Factorization (NMF) in the generation phase and a
weighted hypergraph in the integration phase.
Document clustering is the task of dividing a doc-
ument’s data set into groups based on document sim-
ilarity. This is the basic intelligent procedure, and
is important in text mining systems (M. W. Berry,
2003). As the specific application, relevant feed-
back in IR, where retrieved documents are clus-
tered, is actively researched (Hearst and Pedersen,
1996)(Kummamuru et al., 2004).
In document clustering, the document is repre-
sented as a vector, which typically uses the “bag
of word” model and the TF-IDF term weight. A
vector represented in this manner is highly dimen-
sional and sparse. Thus, in document clustering,
a dimensional reduction method such as PCA or
SVD is applied before actual clustering (Boley et al.,
1999)(Deerwester et al., 1990). Dimensional reduc-
tion maps data in a high-dimensional space into a
low-dimensional space, and improves both cluster-
ing accuracy and speed.
NMF is a dimensional reduction method (Xu et
al., 2003) that is based on the “aspect model” used
in the Probabilistic Latent Semantic Indexing (Hof-
mann, 1999). Because the axis in the reduced space
by NMF corresponds to a topic, the reduced vector
represents the clustering result. For a given term-
document matrix and cluster number, we can obtain
the NMF result with an iterative procedure (Lee and
Seung, 2000). However, this iteration does not al-
ways converge to a global optimum solution. That
is, NMF results depend on the initial value. The
standard countermeasure for this problem is to gen-
erate multiple clustering results by changing the ini-
tial value, and then select the best clustering result
estimated by an object function. However, this se-
lection often fails because the object function does
not always measure clustering accuracy.
To overcome this problem, we use ensemble clus-
tering, which combines multiple clustering results to
obtain an accurate clustering result.
Ensemble clustering consists of generation and
integration phases. The generation phase produces
multiple clustering results. Many strategies have
been proposed to achieve this goal, including ran-
dom initialization (Fred and Jain, 2002), feature ex-
traction based on random projection (Fern and Brod-
ley, 2003) and the combination of sets of “weak”
partitions (Topchy et al., 2003). The integration
phase, as the name implies, integrates multiple clus-
tering results to improve the accuracy of the final
clustering result. This phase primarily relies on two
methods. The first method constructs a new simi-
</bodyText>
<page confidence="0.988309">
77
</page>
<bodyText confidence="0.964507777777778">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 77–80,
Prague, June 2007. c�2007 Association for Computational Linguistics
larity matrix from multiple clustering results (Fred
and Jain, 2002). The second method constructs new
vectors for each instance data using multiple cluster-
ing results (Strehl and Ghosh, 2002). Both methods
apply the clustering procedure to the new object to
obtain the final clustering result.
Our method generates multiple clustering results
by random initialization of the NMF, and integrates
them with a weighted hypergraph instead of the stan-
dard hypergraph (Strehl and Ghosh, 2002). An ad-
vantage of our method is that the weighted hyper-
graph can be directly obtained from the NMF result.
In our experiment, we compared the k-means,
NMF, the ensemble method using a standard hyper-
graph and the ensemble method using a weighted
hypergraph. Our method achieved the best results.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="introduction">
2 NMF
</sectionHeader>
<bodyText confidence="0.984288818181818">
The NMF decomposes the m x n term-document
matrix X to the m x k matrix U and the transposed
matrix of the nxk matrix V (Xu et al., 2003), where
k is the number of clusters; that is,
X=UVT.
The i-th document di corresponds to the i-th row
vector of V; that is, di = (vi1, vi2, •••&apos; vik). The
cluster number is obtained from arg maxjE1:k Vij.
For a given term-document matrix X, we can ob-
tain U and V by the following iteration (Lee and
Seung, 2000):
</bodyText>
<equation confidence="0.986678666666667">
(�� )ij
�ij �ij (1)
(UV T V )ij
(�T�)ij
�ij �ij (2)
(V UT U)ij.
</equation>
<bodyText confidence="0.9782225">
Here, uij, vij and (X)ij represent the i-th row and
the j-th column element of U, V and X respectively.
After each iteration, U must be normalized as fol-
lows:
</bodyText>
<equation confidence="0.42576225">
uij
uij
Ti u
ij
</equation>
<bodyText confidence="0.9931635">
Either the fixed maximum iteration number, or the
distance J between X and UVT stops the iteration:
</bodyText>
<equation confidence="0.941922">
J = IIx — UV T IIF . (4)
</equation>
<bodyText confidence="0.999991857142857">
In NMF, the clustering result depends on the ini-
tial values. Generally, we conduct NMF several
times with random initialization, and then select the
clustering result with the smallest value of Eq.4. The
value of Eq.4 represents the NMF decomposition er-
ror and not the clustering error. Thus, we cannot al-
way select the best result.
</bodyText>
<sectionHeader confidence="0.997578" genericHeader="method">
3 Ensemble clustering
</sectionHeader>
<subsectionHeader confidence="0.99992">
3.1 Hypergraph data representation
</subsectionHeader>
<bodyText confidence="0.999984095238095">
To overcome the above mentioned problem, we
used ensemble clustering. Ensemble clustering con-
sists of generation and integration phases. The first
phase generates multiple clustering results with ran-
dom initialization of the NMF. We integrated them
with the hypergraph proposed in (Strehl and Ghosh,
2002).
Suppose that the generation phase produces m
clustering results, and each result has k clusters. In
this case, the dimension of the new vector is km.
The (k(i — 1) + c)-th dimensional value of the data
d is defined as follows: If the c-th cluster of the i-th
clustering result includes the data d, the value is 1.
Otherwise, the value is 0. Thus, the km dimensional
vector for the data d is constructed.
Consider a simple example, where k = 3, m = 4
and the data set is {d1, d2, • • •, d7}. We generate
four clustering results. Supposing that the first clus-
tering result is {d1, d2, d5}, {d3, d4}, {d6, d7}, we
can obtain the 1st, 2nd and 3rd column of the hy-
pergraph as follows:
</bodyText>
<equation confidence="0.984946142857143">
1 0 0
1 0 0
0 1 0
0 1 0
1 0 0
0 0 1
0 0 1
</equation>
<bodyText confidence="0.9864405">
Repeating the procedure produces a total of four
matrices from four clustering results. Connecting
these four partial matrices, we obtain the following
7 x 12 matrix, which is the hypergraph.
</bodyText>
<table confidence="0.952677285714286">
1 0 0 1 0 0 0 1 0 1 0 0
1 0 0 0 1 0 1 0 0 0 0 1
0 1 0 0 1 0 0 0 1 0 1 0
0 1 0 0 0 1 0 1 0 0 1 0
1 0 0 1 0 0 1 0 0 1 0 0
0 0 1 0 0 1 0 0 1 0 0 1
0 0 1 0 0 1 0 0 1 1 0 0
</table>
<figure confidence="0.907134125">
(3)
�
������
dl
d2
d3
d4
d5
d6
d7
�
� ����� �
�
������
dl
d2
</figure>
<page confidence="0.958211142857143">
d3
d4
d5
d6
d7
1
78
</page>
<subsectionHeader confidence="0.510247">
3.2 Weighted hypergraph vs. standard
hypergraph
</subsectionHeader>
<bodyText confidence="0.998274">
Each element of the hypergraph is 0 or 1. However,
the element value must be real because it represents
the membership degree for the corresponding clus-
ter.
Fortunately, the matrix V produced by NMF de-
scribes the membership degree. Thus, we assign the
real value described in V to the element of the hyper-
graph whose value is 1. Figure 1 shows an example
of this procedure. Our method uses this weighted
hypergraph, instead of a standard hypergraph for in-
tegration.
</bodyText>
<figureCaption confidence="0.99864">
Figure 1: Weighted hypergraph through the matrix
</figureCaption>
<bodyText confidence="0.455752">
V
</bodyText>
<sectionHeader confidence="0.997311" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999085571428571">
To confirm the effectiveness of our method, we com-
pared the k-means, NMF, the ensemble method us-
ing a standard hypergraph and the ensemble method
using a weighted hypergraph.
In our experiment, we use 18 document data
sets provided at http://glaros.dtc.umn.edu/
gkhome/cluto/cluto/download.
The document vector is not normalized for each
data set. We normalize them using TF-IDF.
Table 1 shows the result of the experiment 1. The
value in the table represents entropy, and the smaller
it is, the better the clustering result.
In NMF, we generated 20 clustering results us-
ing random initialization, and selected the cluster-
</bodyText>
<footnote confidence="0.8947305">
1We used the clustering toolkit CLUTO for clustering the
hypergraph.
</footnote>
<bodyText confidence="0.999886375">
ing result with the smallest decomposition error.
The selected clustering result is shown as “NMF”
in Table 1. “NMF means” in Table 1 is the average
of 20 entropy values for 20 clustering results. The
“standard hypergraph” and “weighted hypergraph”
in Table 1 show the results of the ensemble method
obtained using the two hypergraph types. Table 1
shows the effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="method">
5 Related works
</sectionHeader>
<bodyText confidence="0.999938696969697">
When we generate multiple clustering results, the
number of clusters in each clustering is fixed to the
number of clusters in the final clustering result. This
is not a limitation of our ensemble method. Any
number is available for each clustering. Experience
shows that the ensemble clustering using k-means
succeeds when each clustering has many clusters,
and they are combined into fewer clusters, which is
a heuristics that has been reported (Fred and Jain,
2002), and is available for our method
Our method uses the weighted hypergraph, which
is constructed by changing the value 1 in the stan-
dard hypergraph to the corresponding real value in
the matrix V . Taking this idea one step further,
it may be good to change the value 0 in the stan-
dard hypergraph to its real value. In this case,
the weighted hypergraph is constructed by only
connecting multiple V s. We tested this complete
weighted hypergraph, and the results are shown as
“hypergraph V” in Table 1.
“Hypergraph V” was better than the standard hy-
pergraph, but worse than our method. Further-
more, the value 0 may be useful because we can use
the graph spectrum clustering method (Ding et al.,
2001), which is a powerful clustering method for the
spare hypergraph.
In clustering, the cluster label is unassigned.
However, if cluster labeling is possible, we can use
many techniques in the ensemble learning (Breiman,
1996). Cluster labeling is not difficult when there
are two or three clusters. We plan to study this ap-
proach of the labeling cluster first and then using the
techniques from ensemble learning.
</bodyText>
<sectionHeader confidence="0.996974" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.992901">
This paper proposed a new ensemble document clus-
tering method. The novelty of our method is the use
</bodyText>
<figure confidence="0.998130358490566">
d
d
normalize
d
0.723 0. 1 50 0. 127
d
0.960 0.01 5 0.025
d
d
0.3 1 3 0.556 0. 13 1
41 1
0.
0.438
0. 1 5 1
1 0 0
1 0 0
0 1 0
0 1 0
1 0 0
0 0 1
0 0 1
0.723 0 0
0.960 0 0
0 0.556 0
0 0.438 0
Standard
Hyper Graph
d d d d d d d
NMF
V
d
d
0.508 0.230 0.262
0. 1 1 5 0.163 0.722
0.001 0. 190 0.809
Weighted
Hyper Graph
0.508 0 0
0 0 0.722
0 0 0.809
d
d
d
d
d
d
d
d
d
d
d
d
d
</figure>
<page confidence="0.991469">
79
</page>
<tableCaption confidence="0.99911">
Table 1: Document data sets and Experiment results
</tableCaption>
<table confidence="0.996889095238095">
Data # of # of # of k-means NMF NMF Standard Weighted Hypergraph
doc. terms classes means hypergraph hypergraph V
cacmcisi 4663 41681 2 0.750 0.817 0.693 0.691 0.690 0.778
cranmed 2431 41681 2 0.113 0.963 0.792 0.750 0.450 0.525
fbis 2463 2000 17 0.610 0.393 0.406 0.408 0.381 0.402
hitech 2301 126373 6 0.585 0.679 0.705 0.683 0.684 0.688
k1a 2340 21839 20 0.374 0.393 0.377 0.386 0.351 0.366
k1b 2340 21839 6 0.221 0.259 0.238 0.456 0.216 0.205
la1 3204 31472 6 0.641 0.464 0.515 0.458 0.459 0.491
la2 3075 31472 6 0.620 0.576 0.551 0.548 0.468 0.486
re0 1504 2886 13 0.368 0.419 0.401 0.383 0.379 0.378
re1 1657 3758 25 0.374 0.364 0.346 0.334 0.325 0.337
reviews 4069 126373 5 0.364 0.398 0.538 0.416 0.408 0.391
tr11 414 6429 9 0.349 0.338 0.311 0.300 0.304 0.280
tr12 313 5804 8 0.493 0.332 0.375 0.308 0.307 0.316
tr23 204 5832 6 0.527 0.485 0.489 0.493 0.521 0.474
tr31 927 10128 7 0.385 0.402 0.383 0.343 0.334 0.310
tr41 878 7454 10 0.277 0.358 0.299 0.245 0.270 0.340
tr45 690 8261 10 0.397 0.345 0.328 0.277 0.274 0.380
wap 1560 6460 20 0.408 0.371 0.374 0.336 0.327 0.344
Average 1946.2 27874.5 9.9 0.436 0.464 0.451 0.434 0.397 0.416
</table>
<bodyText confidence="0.997595714285714">
of NMF in the generation phase and a weighted hy-
pergraph in the integration phase. One advantage of
our method is that the weighted hypergraph can be
obtained directly from the NMF results. Our exper-
iment showed the effectiveness of our method using
18 document data sets. In the future, we will use an
ensemble learning technique by labeling clusters.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999958833333333">
D. Boley, M. L. Gini, R. Gross, E. Han, K. Hastings,
G. Karypis, V. Kumar, B. Mobasher, and J. Moore.
1999. Document categorization and query generation
on the world wide web using webace. Artificial Intel-
ligence Review, 13(5-6):365–391.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123–140.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391–407.
C. Ding, X. He, H. Zha, M. Gu, and H. Simon. 2001.
Spectral Min-max Cut for Graph Partitioning and Data
Clustering. In Lawrence Berkeley National Lab. Tech.
report 47848.
X. Z. Fern and C. E. Brodley. 2003. Random Projec-
tion for High Dimensional Data Clustering: A Cluster
Ensemble Approach. In the 20th International Con-
ference ofMachine Learning (ICML-03).
A.L.N. Fred and A. K. Jain. 2002. Data Clustering Us-
ing Evidence Accumulation. In the 16th international
conference on pattern recognition, pages 276–280.
M. A. Hearst and J. O. Pedersen. 1996. Reexamining the
cluster hypothesis: Scatter/gather on retrieval results.
In Proceedings of SIGIR-96, pages 76–84.
T. Hofmann. 1999. Probabilistic Latent Semantic Index-
ing. In Proceedings of the 22nd Annual ACM Con-
ference on Research and Development in Information
Retrieval, pages 50–57.
K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and
R. Krishnapuram. 2004. A Hierarchical Monothetic
Document Clustering Algorithm for Summarization
and Browsing Search Results. In Proceedings of
WWW-04, pages 658–665.
D. D. Lee and H. S. Seung. 2000. Algorithms for non-
negative matrix factorization. In NIPS, pages 556–
562.
M. W. Berry, editor. 2003. Survey of Text Mining: Clus-
tering, Classification, and Retrieval. Springer.
A. Strehl and J. Ghosh. 2002. Cluster Ensembles - A
Knowledge Reuse Framework for Combining Multi-
ple Partitions. In Conference on Artificial Intelligence
(AAAI-2002), pages 93–98.
A. Topchy, A. K. Jain, and W. Punch. 2003. Combining
Multiple Weak Clusterings.
W. Xu, X. Liu, and Y. Gong. 2003. Document clus-
tering based on non-negative matrix factorization. In
Proceedings of SIGIR-03, pages 267–273.
</reference>
<page confidence="0.998248">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.879074">
<title confidence="0.9995515">Ensemble Document Clustering Using Weighted Hypergraph Generated by NMF</title>
<author confidence="0.99404">Hiroyuki Shinnou</author>
<author confidence="0.99404">Minoru Sasaki</author>
<affiliation confidence="0.999999">Ibaraki University,</affiliation>
<address confidence="0.9941955">4-12-1 Nakanarusawa, Hitachi, Ibaraki, Japan 316-8511</address>
<abstract confidence="0.9895091">In this paper, we propose a new ensemble document clustering method. The novelty of our method is the use of Non-negative Matrix Factorization (NMF) in the generation phase and a weighted hypergraph in the integration phase. In our experiment, we compared our method with some clustering methods. Our method achieved the best results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Boley</author>
<author>M L Gini</author>
<author>R Gross</author>
<author>E Han</author>
<author>K Hastings</author>
<author>G Karypis</author>
<author>V Kumar</author>
<author>B Mobasher</author>
<author>J Moore</author>
</authors>
<title>Document categorization and query generation on the world wide web using webace.</title>
<date>1999</date>
<journal>Artificial Intelligence Review,</journal>
<pages>13--5</pages>
<contexts>
<context position="1487" citStr="Boley et al., 1999" startWordPosition="224" endWordPosition="227">. This is the basic intelligent procedure, and is important in text mining systems (M. W. Berry, 2003). As the specific application, relevant feedback in IR, where retrieved documents are clustered, is actively researched (Hearst and Pedersen, 1996)(Kummamuru et al., 2004). In document clustering, the document is represented as a vector, which typically uses the “bag of word” model and the TF-IDF term weight. A vector represented in this manner is highly dimensional and sparse. Thus, in document clustering, a dimensional reduction method such as PCA or SVD is applied before actual clustering (Boley et al., 1999)(Deerwester et al., 1990). Dimensional reduction maps data in a high-dimensional space into a low-dimensional space, and improves both clustering accuracy and speed. NMF is a dimensional reduction method (Xu et al., 2003) that is based on the “aspect model” used in the Probabilistic Latent Semantic Indexing (Hofmann, 1999). Because the axis in the reduced space by NMF corresponds to a topic, the reduced vector represents the clustering result. For a given termdocument matrix and cluster number, we can obtain the NMF result with an iterative procedure (Lee and Seung, 2000). However, this iterat</context>
</contexts>
<marker>Boley, Gini, Gross, Han, Hastings, Karypis, Kumar, Mobasher, Moore, 1999</marker>
<rawString>D. Boley, M. L. Gini, R. Gross, E. Han, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore. 1999. Document categorization and query generation on the world wide web using webace. Artificial Intelligence Review, 13(5-6):365–391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="9902" citStr="Breiman, 1996" startWordPosition="1719" endWordPosition="1720">value. In this case, the weighted hypergraph is constructed by only connecting multiple V s. We tested this complete weighted hypergraph, and the results are shown as “hypergraph V” in Table 1. “Hypergraph V” was better than the standard hypergraph, but worse than our method. Furthermore, the value 0 may be useful because we can use the graph spectrum clustering method (Ding et al., 2001), which is a powerful clustering method for the spare hypergraph. In clustering, the cluster label is unassigned. However, if cluster labeling is possible, we can use many techniques in the ensemble learning (Breiman, 1996). Cluster labeling is not difficult when there are two or three clusters. We plan to study this approach of the labeling cluster first and then using the techniques from ensemble learning. 6 Conclusion This paper proposed a new ensemble document clustering method. The novelty of our method is the use d d normalize d 0.723 0. 1 50 0. 127 d 0.960 0.01 5 0.025 d d 0.3 1 3 0.556 0. 13 1 41 1 0. 0.438 0. 1 5 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0.723 0 0 0.960 0 0 0 0.556 0 0 0.438 0 Standard Hyper Graph d d d d d d d NMF V d d 0.508 0.230 0.262 0. 1 1 5 0.163 0.722 0.001 0. 190 0.809 Weight</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1512" citStr="Deerwester et al., 1990" startWordPosition="227" endWordPosition="230">intelligent procedure, and is important in text mining systems (M. W. Berry, 2003). As the specific application, relevant feedback in IR, where retrieved documents are clustered, is actively researched (Hearst and Pedersen, 1996)(Kummamuru et al., 2004). In document clustering, the document is represented as a vector, which typically uses the “bag of word” model and the TF-IDF term weight. A vector represented in this manner is highly dimensional and sparse. Thus, in document clustering, a dimensional reduction method such as PCA or SVD is applied before actual clustering (Boley et al., 1999)(Deerwester et al., 1990). Dimensional reduction maps data in a high-dimensional space into a low-dimensional space, and improves both clustering accuracy and speed. NMF is a dimensional reduction method (Xu et al., 2003) that is based on the “aspect model” used in the Probabilistic Latent Semantic Indexing (Hofmann, 1999). Because the axis in the reduced space by NMF corresponds to a topic, the reduced vector represents the clustering result. For a given termdocument matrix and cluster number, we can obtain the NMF result with an iterative procedure (Lee and Seung, 2000). However, this iteration does not always conve</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ding</author>
<author>X He</author>
<author>H Zha</author>
<author>M Gu</author>
<author>H Simon</author>
</authors>
<title>Spectral Min-max Cut for Graph Partitioning and Data Clustering. In Lawrence Berkeley National Lab.</title>
<date>2001</date>
<tech>Tech. report 47848.</tech>
<contexts>
<context position="9679" citStr="Ding et al., 2001" startWordPosition="1683" endWordPosition="1686">is constructed by changing the value 1 in the standard hypergraph to the corresponding real value in the matrix V . Taking this idea one step further, it may be good to change the value 0 in the standard hypergraph to its real value. In this case, the weighted hypergraph is constructed by only connecting multiple V s. We tested this complete weighted hypergraph, and the results are shown as “hypergraph V” in Table 1. “Hypergraph V” was better than the standard hypergraph, but worse than our method. Furthermore, the value 0 may be useful because we can use the graph spectrum clustering method (Ding et al., 2001), which is a powerful clustering method for the spare hypergraph. In clustering, the cluster label is unassigned. However, if cluster labeling is possible, we can use many techniques in the ensemble learning (Breiman, 1996). Cluster labeling is not difficult when there are two or three clusters. We plan to study this approach of the labeling cluster first and then using the techniques from ensemble learning. 6 Conclusion This paper proposed a new ensemble document clustering method. The novelty of our method is the use d d normalize d 0.723 0. 1 50 0. 127 d 0.960 0.01 5 0.025 d d 0.3 1 3 0.556</context>
</contexts>
<marker>Ding, He, Zha, Gu, Simon, 2001</marker>
<rawString>C. Ding, X. He, H. Zha, M. Gu, and H. Simon. 2001. Spectral Min-max Cut for Graph Partitioning and Data Clustering. In Lawrence Berkeley National Lab. Tech. report 47848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Z Fern</author>
<author>C E Brodley</author>
</authors>
<title>Random Projection for High Dimensional Data Clustering: A Cluster Ensemble Approach.</title>
<date>2003</date>
<booktitle>In the 20th International Conference ofMachine Learning (ICML-03).</booktitle>
<contexts>
<context position="2946" citStr="Fern and Brodley, 2003" startWordPosition="450" endWordPosition="454"> select the best clustering result estimated by an object function. However, this selection often fails because the object function does not always measure clustering accuracy. To overcome this problem, we use ensemble clustering, which combines multiple clustering results to obtain an accurate clustering result. Ensemble clustering consists of generation and integration phases. The generation phase produces multiple clustering results. Many strategies have been proposed to achieve this goal, including random initialization (Fred and Jain, 2002), feature extraction based on random projection (Fern and Brodley, 2003) and the combination of sets of “weak” partitions (Topchy et al., 2003). The integration phase, as the name implies, integrates multiple clustering results to improve the accuracy of the final clustering result. This phase primarily relies on two methods. The first method constructs a new simi77 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 77–80, Prague, June 2007. c�2007 Association for Computational Linguistics larity matrix from multiple clustering results (Fred and Jain, 2002). The second method constructs new vectors for each instance data using multiple clustering results </context>
</contexts>
<marker>Fern, Brodley, 2003</marker>
<rawString>X. Z. Fern and C. E. Brodley. 2003. Random Projection for High Dimensional Data Clustering: A Cluster Ensemble Approach. In the 20th International Conference ofMachine Learning (ICML-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L N Fred</author>
<author>A K Jain</author>
</authors>
<title>Data Clustering Using Evidence Accumulation.</title>
<date>2002</date>
<booktitle>In the 16th international conference on pattern recognition,</booktitle>
<pages>276--280</pages>
<contexts>
<context position="2874" citStr="Fred and Jain, 2002" startWordPosition="439" endWordPosition="442">e multiple clustering results by changing the initial value, and then select the best clustering result estimated by an object function. However, this selection often fails because the object function does not always measure clustering accuracy. To overcome this problem, we use ensemble clustering, which combines multiple clustering results to obtain an accurate clustering result. Ensemble clustering consists of generation and integration phases. The generation phase produces multiple clustering results. Many strategies have been proposed to achieve this goal, including random initialization (Fred and Jain, 2002), feature extraction based on random projection (Fern and Brodley, 2003) and the combination of sets of “weak” partitions (Topchy et al., 2003). The integration phase, as the name implies, integrates multiple clustering results to improve the accuracy of the final clustering result. This phase primarily relies on two methods. The first method constructs a new simi77 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 77–80, Prague, June 2007. c�2007 Association for Computational Linguistics larity matrix from multiple clustering results (Fred and Jain, 2002). The second method construc</context>
<context position="8980" citStr="Fred and Jain, 2002" startWordPosition="1558" endWordPosition="1561"> Table 1 show the results of the ensemble method obtained using the two hypergraph types. Table 1 shows the effectiveness of our method. 5 Related works When we generate multiple clustering results, the number of clusters in each clustering is fixed to the number of clusters in the final clustering result. This is not a limitation of our ensemble method. Any number is available for each clustering. Experience shows that the ensemble clustering using k-means succeeds when each clustering has many clusters, and they are combined into fewer clusters, which is a heuristics that has been reported (Fred and Jain, 2002), and is available for our method Our method uses the weighted hypergraph, which is constructed by changing the value 1 in the standard hypergraph to the corresponding real value in the matrix V . Taking this idea one step further, it may be good to change the value 0 in the standard hypergraph to its real value. In this case, the weighted hypergraph is constructed by only connecting multiple V s. We tested this complete weighted hypergraph, and the results are shown as “hypergraph V” in Table 1. “Hypergraph V” was better than the standard hypergraph, but worse than our method. Furthermore, th</context>
</contexts>
<marker>Fred, Jain, 2002</marker>
<rawString>A.L.N. Fred and A. K. Jain. 2002. Data Clustering Using Evidence Accumulation. In the 16th international conference on pattern recognition, pages 276–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
<author>J O Pedersen</author>
</authors>
<title>Reexamining the cluster hypothesis: Scatter/gather on retrieval results.</title>
<date>1996</date>
<booktitle>In Proceedings of SIGIR-96,</booktitle>
<pages>76--84</pages>
<contexts>
<context position="1117" citStr="Hearst and Pedersen, 1996" startWordPosition="163" endWordPosition="166">th some clustering methods. Our method achieved the best results. 1 Introduction In this paper, we propose a new ensemble document clustering method using Non-negative Matrix Factorization (NMF) in the generation phase and a weighted hypergraph in the integration phase. Document clustering is the task of dividing a document’s data set into groups based on document similarity. This is the basic intelligent procedure, and is important in text mining systems (M. W. Berry, 2003). As the specific application, relevant feedback in IR, where retrieved documents are clustered, is actively researched (Hearst and Pedersen, 1996)(Kummamuru et al., 2004). In document clustering, the document is represented as a vector, which typically uses the “bag of word” model and the TF-IDF term weight. A vector represented in this manner is highly dimensional and sparse. Thus, in document clustering, a dimensional reduction method such as PCA or SVD is applied before actual clustering (Boley et al., 1999)(Deerwester et al., 1990). Dimensional reduction maps data in a high-dimensional space into a low-dimensional space, and improves both clustering accuracy and speed. NMF is a dimensional reduction method (Xu et al., 2003) that is </context>
</contexts>
<marker>Hearst, Pedersen, 1996</marker>
<rawString>M. A. Hearst and J. O. Pedersen. 1996. Reexamining the cluster hypothesis: Scatter/gather on retrieval results. In Proceedings of SIGIR-96, pages 76–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual ACM Conference on Research and Development in Information Retrieval,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="1811" citStr="Hofmann, 1999" startWordPosition="276" endWordPosition="278"> a vector, which typically uses the “bag of word” model and the TF-IDF term weight. A vector represented in this manner is highly dimensional and sparse. Thus, in document clustering, a dimensional reduction method such as PCA or SVD is applied before actual clustering (Boley et al., 1999)(Deerwester et al., 1990). Dimensional reduction maps data in a high-dimensional space into a low-dimensional space, and improves both clustering accuracy and speed. NMF is a dimensional reduction method (Xu et al., 2003) that is based on the “aspect model” used in the Probabilistic Latent Semantic Indexing (Hofmann, 1999). Because the axis in the reduced space by NMF corresponds to a topic, the reduced vector represents the clustering result. For a given termdocument matrix and cluster number, we can obtain the NMF result with an iterative procedure (Lee and Seung, 2000). However, this iteration does not always converge to a global optimum solution. That is, NMF results depend on the initial value. The standard countermeasure for this problem is to generate multiple clustering results by changing the initial value, and then select the best clustering result estimated by an object function. However, this select</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>T. Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of the 22nd Annual ACM Conference on Research and Development in Information Retrieval, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kummamuru</author>
<author>R Lotlikar</author>
<author>S Roy</author>
<author>K Singal</author>
<author>R Krishnapuram</author>
</authors>
<title>A Hierarchical Monothetic Document Clustering Algorithm for Summarization and Browsing Search Results.</title>
<date>2004</date>
<booktitle>In Proceedings of WWW-04,</booktitle>
<pages>658--665</pages>
<contexts>
<context position="1141" citStr="Kummamuru et al., 2004" startWordPosition="166" endWordPosition="169"> Our method achieved the best results. 1 Introduction In this paper, we propose a new ensemble document clustering method using Non-negative Matrix Factorization (NMF) in the generation phase and a weighted hypergraph in the integration phase. Document clustering is the task of dividing a document’s data set into groups based on document similarity. This is the basic intelligent procedure, and is important in text mining systems (M. W. Berry, 2003). As the specific application, relevant feedback in IR, where retrieved documents are clustered, is actively researched (Hearst and Pedersen, 1996)(Kummamuru et al., 2004). In document clustering, the document is represented as a vector, which typically uses the “bag of word” model and the TF-IDF term weight. A vector represented in this manner is highly dimensional and sparse. Thus, in document clustering, a dimensional reduction method such as PCA or SVD is applied before actual clustering (Boley et al., 1999)(Deerwester et al., 1990). Dimensional reduction maps data in a high-dimensional space into a low-dimensional space, and improves both clustering accuracy and speed. NMF is a dimensional reduction method (Xu et al., 2003) that is based on the “aspect mod</context>
</contexts>
<marker>Kummamuru, Lotlikar, Roy, Singal, Krishnapuram, 2004</marker>
<rawString>K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram. 2004. A Hierarchical Monothetic Document Clustering Algorithm for Summarization and Browsing Search Results. In Proceedings of WWW-04, pages 658–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lee</author>
<author>H S Seung</author>
</authors>
<title>Algorithms for nonnegative matrix factorization.</title>
<date>2000</date>
<booktitle>In NIPS,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="2065" citStr="Lee and Seung, 2000" startWordPosition="318" endWordPosition="321">ore actual clustering (Boley et al., 1999)(Deerwester et al., 1990). Dimensional reduction maps data in a high-dimensional space into a low-dimensional space, and improves both clustering accuracy and speed. NMF is a dimensional reduction method (Xu et al., 2003) that is based on the “aspect model” used in the Probabilistic Latent Semantic Indexing (Hofmann, 1999). Because the axis in the reduced space by NMF corresponds to a topic, the reduced vector represents the clustering result. For a given termdocument matrix and cluster number, we can obtain the NMF result with an iterative procedure (Lee and Seung, 2000). However, this iteration does not always converge to a global optimum solution. That is, NMF results depend on the initial value. The standard countermeasure for this problem is to generate multiple clustering results by changing the initial value, and then select the best clustering result estimated by an object function. However, this selection often fails because the object function does not always measure clustering accuracy. To overcome this problem, we use ensemble clustering, which combines multiple clustering results to obtain an accurate clustering result. Ensemble clustering consist</context>
<context position="4614" citStr="Lee and Seung, 2000" startWordPosition="734" endWordPosition="737">compared the k-means, NMF, the ensemble method using a standard hypergraph and the ensemble method using a weighted hypergraph. Our method achieved the best results. 2 NMF The NMF decomposes the m x n term-document matrix X to the m x k matrix U and the transposed matrix of the nxk matrix V (Xu et al., 2003), where k is the number of clusters; that is, X=UVT. The i-th document di corresponds to the i-th row vector of V; that is, di = (vi1, vi2, •••&apos; vik). The cluster number is obtained from arg maxjE1:k Vij. For a given term-document matrix X, we can obtain U and V by the following iteration (Lee and Seung, 2000): (�� )ij �ij �ij (1) (UV T V )ij (�T�)ij �ij �ij (2) (V UT U)ij. Here, uij, vij and (X)ij represent the i-th row and the j-th column element of U, V and X respectively. After each iteration, U must be normalized as follows: uij uij Ti u ij Either the fixed maximum iteration number, or the distance J between X and UVT stops the iteration: J = IIx — UV T IIF . (4) In NMF, the clustering result depends on the initial values. Generally, we conduct NMF several times with random initialization, and then select the clustering result with the smallest value of Eq.4. The value of Eq.4 represents the N</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>D. D. Lee and H. S. Seung. 2000. Algorithms for nonnegative matrix factorization. In NIPS, pages 556– 562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Berry</author>
<author>editor</author>
</authors>
<date>2003</date>
<booktitle>Survey of Text Mining: Clustering, Classification, and Retrieval.</booktitle>
<publisher>Springer.</publisher>
<marker>Berry, editor, 2003</marker>
<rawString>M. W. Berry, editor. 2003. Survey of Text Mining: Clustering, Classification, and Retrieval. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Strehl</author>
<author>J Ghosh</author>
</authors>
<title>Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions.</title>
<date>2002</date>
<booktitle>In Conference on Artificial Intelligence (AAAI-2002),</booktitle>
<pages>93--98</pages>
<contexts>
<context position="3570" citStr="Strehl and Ghosh, 2002" startWordPosition="547" endWordPosition="550">and the combination of sets of “weak” partitions (Topchy et al., 2003). The integration phase, as the name implies, integrates multiple clustering results to improve the accuracy of the final clustering result. This phase primarily relies on two methods. The first method constructs a new simi77 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 77–80, Prague, June 2007. c�2007 Association for Computational Linguistics larity matrix from multiple clustering results (Fred and Jain, 2002). The second method constructs new vectors for each instance data using multiple clustering results (Strehl and Ghosh, 2002). Both methods apply the clustering procedure to the new object to obtain the final clustering result. Our method generates multiple clustering results by random initialization of the NMF, and integrates them with a weighted hypergraph instead of the standard hypergraph (Strehl and Ghosh, 2002). An advantage of our method is that the weighted hypergraph can be directly obtained from the NMF result. In our experiment, we compared the k-means, NMF, the ensemble method using a standard hypergraph and the ensemble method using a weighted hypergraph. Our method achieved the best results. 2 NMF The </context>
<context position="5675" citStr="Strehl and Ghosh, 2002" startWordPosition="919" endWordPosition="922">we conduct NMF several times with random initialization, and then select the clustering result with the smallest value of Eq.4. The value of Eq.4 represents the NMF decomposition error and not the clustering error. Thus, we cannot alway select the best result. 3 Ensemble clustering 3.1 Hypergraph data representation To overcome the above mentioned problem, we used ensemble clustering. Ensemble clustering consists of generation and integration phases. The first phase generates multiple clustering results with random initialization of the NMF. We integrated them with the hypergraph proposed in (Strehl and Ghosh, 2002). Suppose that the generation phase produces m clustering results, and each result has k clusters. In this case, the dimension of the new vector is km. The (k(i — 1) + c)-th dimensional value of the data d is defined as follows: If the c-th cluster of the i-th clustering result includes the data d, the value is 1. Otherwise, the value is 0. Thus, the km dimensional vector for the data d is constructed. Consider a simple example, where k = 3, m = 4 and the data set is {d1, d2, • • •, d7}. We generate four clustering results. Supposing that the first clustering result is {d1, d2, d5}, {d3, d4}, </context>
</contexts>
<marker>Strehl, Ghosh, 2002</marker>
<rawString>A. Strehl and J. Ghosh. 2002. Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions. In Conference on Artificial Intelligence (AAAI-2002), pages 93–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Topchy</author>
<author>A K Jain</author>
<author>W Punch</author>
</authors>
<title>Combining Multiple Weak Clusterings.</title>
<date>2003</date>
<contexts>
<context position="3017" citStr="Topchy et al., 2003" startWordPosition="463" endWordPosition="466">r, this selection often fails because the object function does not always measure clustering accuracy. To overcome this problem, we use ensemble clustering, which combines multiple clustering results to obtain an accurate clustering result. Ensemble clustering consists of generation and integration phases. The generation phase produces multiple clustering results. Many strategies have been proposed to achieve this goal, including random initialization (Fred and Jain, 2002), feature extraction based on random projection (Fern and Brodley, 2003) and the combination of sets of “weak” partitions (Topchy et al., 2003). The integration phase, as the name implies, integrates multiple clustering results to improve the accuracy of the final clustering result. This phase primarily relies on two methods. The first method constructs a new simi77 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 77–80, Prague, June 2007. c�2007 Association for Computational Linguistics larity matrix from multiple clustering results (Fred and Jain, 2002). The second method constructs new vectors for each instance data using multiple clustering results (Strehl and Ghosh, 2002). Both methods apply the clustering procedure t</context>
</contexts>
<marker>Topchy, Jain, Punch, 2003</marker>
<rawString>A. Topchy, A. K. Jain, and W. Punch. 2003. Combining Multiple Weak Clusterings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
<author>X Liu</author>
<author>Y Gong</author>
</authors>
<title>Document clustering based on non-negative matrix factorization.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR-03,</booktitle>
<pages>267--273</pages>
<contexts>
<context position="1708" citStr="Xu et al., 2003" startWordPosition="258" endWordPosition="261">earst and Pedersen, 1996)(Kummamuru et al., 2004). In document clustering, the document is represented as a vector, which typically uses the “bag of word” model and the TF-IDF term weight. A vector represented in this manner is highly dimensional and sparse. Thus, in document clustering, a dimensional reduction method such as PCA or SVD is applied before actual clustering (Boley et al., 1999)(Deerwester et al., 1990). Dimensional reduction maps data in a high-dimensional space into a low-dimensional space, and improves both clustering accuracy and speed. NMF is a dimensional reduction method (Xu et al., 2003) that is based on the “aspect model” used in the Probabilistic Latent Semantic Indexing (Hofmann, 1999). Because the axis in the reduced space by NMF corresponds to a topic, the reduced vector represents the clustering result. For a given termdocument matrix and cluster number, we can obtain the NMF result with an iterative procedure (Lee and Seung, 2000). However, this iteration does not always converge to a global optimum solution. That is, NMF results depend on the initial value. The standard countermeasure for this problem is to generate multiple clustering results by changing the initial </context>
<context position="4303" citStr="Xu et al., 2003" startWordPosition="674" endWordPosition="677">erates multiple clustering results by random initialization of the NMF, and integrates them with a weighted hypergraph instead of the standard hypergraph (Strehl and Ghosh, 2002). An advantage of our method is that the weighted hypergraph can be directly obtained from the NMF result. In our experiment, we compared the k-means, NMF, the ensemble method using a standard hypergraph and the ensemble method using a weighted hypergraph. Our method achieved the best results. 2 NMF The NMF decomposes the m x n term-document matrix X to the m x k matrix U and the transposed matrix of the nxk matrix V (Xu et al., 2003), where k is the number of clusters; that is, X=UVT. The i-th document di corresponds to the i-th row vector of V; that is, di = (vi1, vi2, •••&apos; vik). The cluster number is obtained from arg maxjE1:k Vij. For a given term-document matrix X, we can obtain U and V by the following iteration (Lee and Seung, 2000): (�� )ij �ij �ij (1) (UV T V )ij (�T�)ij �ij �ij (2) (V UT U)ij. Here, uij, vij and (X)ij represent the i-th row and the j-th column element of U, V and X respectively. After each iteration, U must be normalized as follows: uij uij Ti u ij Either the fixed maximum iteration number, or th</context>
</contexts>
<marker>Xu, Liu, Gong, 2003</marker>
<rawString>W. Xu, X. Liu, and Y. Gong. 2003. Document clustering based on non-negative matrix factorization. In Proceedings of SIGIR-03, pages 267–273.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>