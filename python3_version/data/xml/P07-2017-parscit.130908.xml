<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003546">
<title confidence="0.9693945">
An Approximate Approach for Training Polynomial Kernel SVMs in
Linear Time
</title>
<author confidence="0.999135">
Yu-Chieh Wu Jie-Chi Yang Yue-Shi Lee
</author>
<affiliation confidence="0.978256">
Dept. of Computer Science and Graduate Institute of Net- Dept. of Computer Science and
Information Engineering work Learning Technology Information Engineering
National Central University National Central University Ming Chuan University
Taoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwan
</affiliation>
<email confidence="0.993534">
bcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw lees@mcu.edu.tw
</email>
<sectionHeader confidence="0.997334" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901619047619">
Kernel methods such as support vector ma-
chines (SVMs) have attracted a great deal
of popularity in the machine learning and
natural language processing (NLP) com-
munities. Polynomial kernel SVMs showed
very competitive accuracy in many NLP
problems, like part-of-speech tagging and
chunking. However, these methods are
usually too inefficient to be applied to large
dataset and real time purpose. In this paper,
we propose an approximate method to
analogy polynomial kernel with efficient
data mining approaches. To prevent expo-
nential-scaled testing time complexity, we
also present a new method for speeding up
SVM classifying which does independent
to the polynomial degree d. The experi-
mental results showed that our method is
16.94 and 450 times faster than traditional
polynomial kernel in terms of training and
testing respectively.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998675">
Kernel methods, for example support vector
machines (SVM) (Vapnik, 1995) are successfully
applied to many natural language processing (NLP)
problems. They yielded very competitive and
satisfactory performance in many classification
tasks, such as part-of-speech (POS) tagging
(Gimenez and Marquez, 2003), shallow parsing
(Kudo and Matsumoto, 2001, 2004; Lee and Wu,
2007), named entity recognition (Isozaki and
Kazawa, 2002), and parsing (Nivre et al., 2006).
In particular, the use of polynomial kernel SVM
implicitly takes the feature combinations into ac-
</bodyText>
<page confidence="0.995625">
65
</page>
<bodyText confidence="0.991314">
count instead of explicitly combines features. By
setting with polynomial kernel degree (i.e., d), dif-
ferent number of feature conjunctions can be im-
plicitly computed. In this way, polynomial kernel
SVM is often better than linear kernel which did
not use feature conjunctions. However, the training
and testing time costs for polynomial kernel SVM
is far slow than the linear kernel. For example, it
took one day to train the CoNLL-2000 task with
polynomial kernel SVM, while the testing speed is
merely 20-30 words per second (Kudo and Ma-
tsumoto, 2001). Although the author provided the
solution for fast classifying with polynomial kernel
(Kudo and Matsumoto, 2004), the training time is
still inefficient. Nevertheless, the testing time of
their method exponentially scales with polynomial
kernel degree d, i.e., O(|X|d) where |X |denotes as
the length of example X.
On the contrary, even the linear kernel SVM
simply disregards the effect of feature combina-
tions during training and testing, it performs not
only more efficient than polynomial kernel, but
also can be improved through directly appending
features derived from the set of feature combina-
tions. Examples include bigram, trigram, etc. Nev-
ertheless, selecting the feature conjunctions was
manually and heuristically encoded and should
perform amount of validation trials to discover
which is useful or not. In recent years, several
studies had reported that the training time of linear
kernel SVM can be reduced to linear time
(Joachims, 2006; Keerthi and DeCoste, 2005). But
they did not and difficult to be extent to polyno-
mial kernels.
In this paper, we propose an approximate ap-
proach to extend the linear kernel SVM toward
polynomial. By introducing the well-known se-
quential pattern mining approach (Pei et al., 2004),
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 65–68,
Prague, June 2007. c�2007 Association for Computational Linguistics
frequent feature conjunctions, namely patterns
could be discovered and also kept as expand fea-
ture space. We then adopt the mined patterns to re-
represent the training/testing examples. Subse-
quently, we use the off-the-shelf linear kernel
SVM algorithm to perform training and testing.
Besides, to exponential-scaled testing time com-
plexity, we propose a new classification method
for speeding up the SVM testing. Rather than
enumerating all patterns for each example, our
method requires O(Favg*Navg) which is independent
to the polynomial kernel degree. Favg is the average
number of frequent features per example, while the
Navg is the average number of patterns per feature.
</bodyText>
<sectionHeader confidence="0.954259" genericHeader="introduction">
2 SVM and Kernel Methods
</sectionHeader>
<bodyText confidence="0.9978355">
Suppose we have the training instance set for bi-
nary classification problem:
</bodyText>
<equation confidence="0.9966745">
( 1, 1), ( 2, 2),..., ( , ),
x y x y xn yn xi ∈ ℜ i ∈ + −
D, { 1, 1}
y
</equation>
<bodyText confidence="0.9995196">
where xi is a feature vector in D-dimension
space of the i-th example, and yi is the label of xi
either positive or negative. The training of SVMs
involves in minimize the following object (primal
form, soft-margin) (Vapnik, 1995):
</bodyText>
<equation confidence="0.994978625">
n
1
minimize: (α)
W = ⋅ +
W W C Loss Wxi yi
( , )
∑=
2 i 1
</equation>
<bodyText confidence="0.999605666666667">
The loss function indicates the loss of training
error. Usually, the hinge-loss is used (Keerthi and
DeCoste, 2005). The factor C in (1) is a parameter
that allows one to trade off training error and mar-
gin. A small value for C will increase the number
of training errors.
To determine the class (+1 or -1) of an example
x can be judged by computing the following equa-
tion.
</bodyText>
<equation confidence="0.997005714285714">
y x
( ) sign(( ∑ ( , )) )
= α y K x x b
+
i i i
x SVs
i∈ (2)
</equation>
<bodyText confidence="0.9999114">
αi is the weight of training example xi (αi&gt;0),
and b denotes as a threshold. Here the xi should be
the support vectors (SVs), and are representative of
training examples. The kernel function K is the
kernel mapping function, which might map from
</bodyText>
<equation confidence="0.601382333333333">
ℜ to D&apos;
D ℜ (usually D&lt;&lt;D’). The natural linear ker-
nel simply uses the dot-product as (3).
K (x, xi) = dot (x, xi) (3)
A polynomial kernel of degree d is given by (4).
K(x, xi) = (1 + dot (x, xi )) d (4)
</equation>
<bodyText confidence="0.999888090909091">
One can design or employ off-the-shelf kernel
types for particular applications. In particular to the
use of polynomial kernel-based SVM, it was
shown to be the most successful kernels for many
natural language processing (NLP) problems
(Kudo and Matsumoto, 2001; Isozaki and Kazawa,
2002; Nivre et al., 2006).
It is known that the dot-product (linear form)
represents the most efficient kernel computing
which can produce the output value by linearly
combining all support vectors such as
</bodyText>
<equation confidence="0.988959">
y(x) = sign(dot(x, w) + b) where w= ∑ αy x
i i i
(5)
</equation>
<bodyText confidence="0.936149">
an example of x using the polynomial kernel can
be shown as follows.
</bodyText>
<equation confidence="0.999098714285714">
y(x) sign((
= ∑ αy dot x x
( ( , ) 1) ) )
d
i i i + + b
x SVs
i ∈
</equation>
<bodyText confidence="0.9559028">
support vector
The situation is even worse when
the number of support vectors become huge (Kudo
and Matsumoto, 2004). Therefore, whether in
training or testing phrase, the cost of kernel com-
putations is far more expensive than linear kern
xi.
el.
By combining (2) and (4), the determination of
Usually, degree d is set more than 1. When d is
set as 1, the polynomial kernel backs-off to linear
kernel. Although the effectiveness of polynomial
kernel, it can not be shown to linearly combine all
support vectors into one weight vector whereas it
requires computing the kernel function (4) for each
</bodyText>
<sectionHeader confidence="0.996156" genericHeader="method">
3 Approximate Polynomial Kernel
</sectionHeader>
<bodyText confidence="0.989134208333333">
In 2004, Kudo and Matsumoto (2004) derived both
implicitly (6) and explicitly form of polynomial
kernel. They indicated that the use of explicitly
enumerate the feature combinations is equivalent
to the polynomial kernel (see Lemma 1 and Exam-
ple 1, Kudo and Matsumoto, 2004) which shared
the same view of
and Roth, 2003).
We follow the similar idea of the above studies
that requires explicitly enumerated all feature com-
binations. To meet with our problem, we employ
the well-known sequential pattern mining algo-
rithm, namely PrefixSpan (Pei et al., 2004) to effi-
cient mine the frequent patterns. However, directly
adopt the algorithm is not a good idea. To fit with
SVM, we modify the original PrefixSpan algo-
rithm according to the following constraints.
Given a set features, the PrefixSpan mines the
frequent patterns which occurs more than prede-
fined minimum support in the training set and lim-
ited in the length of predefined d, which is equiva-
lent to the polynomial kern
(Cumby
el degree d. For exam-
</bodyText>
<equation confidence="0.938513">
(1)
x SVs
i ∈
(6)
</equation>
<page confidence="0.860608">
66
</page>
<bodyText confidence="0.983580194444445">
ple, if the minimum support is 5, and d=2, then a
feature combination (fi, fj) must appear more than 5
times in set of x.
Definition 1 (Frequent single-item sequence):
Given a set of feature vectors x, minimum support,
and d, mining the frequent patterns (feature combi-
nations) is to mine the patterns in the single-item
sequence database.
Lemma 2 (Ordered feature vector):
For each example, the feature vector could be
transformed into an ordered item (feature) list, i.e.,
f1&lt;f2&lt;...&lt;fmax where fmax is the highest dimension of
the example.
Proof. It is very easy to sort an unordered feature
vector into the ordered list with conventional sort-
ing algorithm.
Definition 3 (Uniqueness of the features per ex-
ample):
Given the set of mined patterns, for any feature fi,
it is impossible to appear more than once in the
same pattern.
Different from conventional sequential pattern
mining method, in feature combination mining for
SVM only contains a set of feature vectors each of
which is independently treated. In other words, no
compound features in the vector. If it exists, one
can simply expand the compound features as an-
other new feature.
By means of the above constraints, mining the
frequent patterns can be reduced to mining the lim-
ited length of frequent patterns in the single-item
database (set of ordered vectors). Furthermore,
during each phase, we need only focus on finding
the “frequent single features” to expand previous
phase. More detail implementation issues can refer
(Pei et al., 2004).
</bodyText>
<subsectionHeader confidence="0.997802">
3.1 Speed-up Testing
</subsectionHeader>
<bodyText confidence="0.999953230769231">
To efficiently expand new features for the original
feature vectors, we propose a new method to fast
discovery patterns. Essentially, the PrefixSpan al-
gorithm gradually expands one item from previous
result which can be viewed as a tree growing. An
example can be found in Figure 1.
Each node in Figure 1 is the associate feature of
root. The whole patterns expanded by fj can be rep-
resented as the path from root to each node. For
example, pattern (fj, fk, fm, fr) can be found via trav-
ersing the tree starting from fj. In this way, we can
re-expand the original feature vector via visiting
corresponding trees for each feature.
</bodyText>
<figureCaption confidence="0.993402">
Figure 1: The tree representation of feature fj
</figureCaption>
<tableCaption confidence="0.916334">
Table 1: Encoding frequent patterns with DFS array
representation
</tableCaption>
<bodyText confidence="0.997671571428571">
Level 0 1 2 3 2 1 2 1 2 2
Label Root k m r p m p o p q
Item fj fk fm fr fp fm fp fo fp fq
However, traversing arrays is much more effi-
cient than visiting trees. Therefore, we adopt the l2-
sequences encoding method based on the DFS
(depth-first-search) sequence as (Wang et al., 2004)
to represent the trees. An l2-sequence does not only
store the label information but also take the node
level into account. Examples can be found in Table
1.
Theorem 4 (Uniqueness of l2-sequence): Given
trees T1, and T2, their l2-sequences are identical if
and only if T1 and T2 are isomorphic, i.e., there
exists a one-to-one mapping for set of nodes, node
labels, edges, and root nodes.
Proof. see theorem 1 in (Wang et al., 2004).
Definition 5 (Ascend-descend relation):
Given a node k of feature fk in l2-sequence, all of
the descendant of k that rooted by k have the
greater feature numbers than fk.
</bodyText>
<subsectionHeader confidence="0.807566">
Definition 6 (Limited visiting space):
</subsectionHeader>
<bodyText confidence="0.999968866666667">
Given the highest feature fmax of vector X, and fk
rooted l2-sequence, if fmax&lt;fk, then we can not find
any pattern that prefix by fk.
Both definitions 5 and 6 strictly follow lemma 2
that kept the ordered relations among features. For
example, once node k could be found in X, it is
unnecessary to visit its children. More specifically,
to determine whether a frequent pattern is in X, we
need to compare feature vector of X and l2-
sequence database. It is clearly that the time com-
plexity of our method is O(Favg*Navg) where Favg is
the average number of frequent features per exam-
ple, while the Navg is the average length of l2-
sequence. In other words, our method does not de-
pendent on the polynomial kernel degree.
</bodyText>
<page confidence="0.999505">
67
</page>
<sectionHeader confidence="0.999243" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999905933333333">
To evaluate our method, we examine the well-
known shallow parsing task which is the task of
CoNLL-20001. We also adopted the released perl-
evaluator to measure the recall/precision/f1 rates.
The used feature consists of word, POS, ortho-
graphic, affix(2-4 prefix/suffix letters), and previ-
ous chunk tags in the two words context window
size (the same as (Lee and Wu, 2007)). We limited
the features should at least appear more than twice
in the training set.
For the learning algorithm, we replicate the
modified finite Newton SVM as learner which can
be trained in linear time (Keerthi and DeCoste,
2005). We also compare our method with the stan-
dard linear and polynomial kernels with SVMlight 2.
</bodyText>
<sectionHeader confidence="0.834783" genericHeader="evaluation">
4.1 Results
</sectionHeader>
<bodyText confidence="0.9689314">
Table 2 lists the experimental results on the
CoNLL-2000 shallow parsing task. Table 3 com-
pares the testing speed of different feature expan-
sion techniques, namely, array visiting (our method)
and enumeration.
</bodyText>
<tableCaption confidence="0.995754">
Table 2: Experimental results for CoNLL-2000 shal-
</tableCaption>
<table confidence="0.9730208">
low parsing task
CoNLL-2000 F1 Mining Training Testing
Time Time Time
Linear Kernel 93.15 N/A 0.53hr 2.57s
Polynomial(d=2) 94.19 N/A 11.52hr 3189.62s
Polynomial(d=3) 93.95 N/A 19.43hr 6539.75s
Our Method 93.71 &lt;10s 0.68hr 6.54s
(d=2,sup=0.01)
Our Method 93.46 &lt;15s 0.79hr 9.95s
(d=3,sup=0.01)
</table>
<tableCaption confidence="0.997127">
Table 3: Classification time performance of enu-
meration and array visiting techniques
</tableCaption>
<table confidence="0.9921198">
CoNLL-2000 Array visiting Enumeration
d=2 d=3 d=2 d=3
Testing time 6.54s 9.95s 4.79s 11.73s
Chunking speed 7244.19 4761.50 9890.81 4038.95
(words/sec)
</table>
<bodyText confidence="0.9997824">
It is not surprising that the best performance was
obtained by the classical polynomial kernel. But
the limitation is that the slow in training and test-
ing time costs. The most efficient method is linear
kernel SVM but it does not as accurate as polyno-
mial kernel. However, our method stands for both
efficiency and accuracy in this experiment. In
terms of training time, it slightly slower than the
linear kernel, while it is 16.94 and ~450 times
faster than polynomial kernel in training and test-
</bodyText>
<footnote confidence="0.996909">
1 http://www.cnts.ua.ac.be/conll2000/chunking/
2 http://svmlight.joachims.org/
</footnote>
<bodyText confidence="0.99825275">
ing. Besides, the pattern mining time is far smaller
than SVM training.
As listed in Table 3, we can see that our method
provide a more efficient solution to feature expan-
sion when d is set more than two. Also it demon-
strates that when d is small, the enumerate-based
method is a better choice (see PKE in (Kudo and
Matsumoto, 2004)).
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999995285714286">
This paper presents an approximate method for
extending linear kernel SVM to analogy polyno-
mial-like computing. The advantage of this method
is that it does not require maintaining the cost of
support vectors in training, while achieves satisfac-
tory result. On the other hand, we also propose a
new method for speeding up classification which is
independent to the polynomial kernel degree. The
experimental results showed that our method close
to the performance of polynomial kernel SVM and
better than the linear kernel. In terms of efficiency,
our method did not only improve 16.94 times
faster in training and 450 times in testing, but also
faster than previous similar studies.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859433333333">
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. International Conference on Machine
Learning, pages 104-114.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient support
vector classifiers for named entity recognition. Interna-
tional Conference on Computational Linguistics, pages 1-7.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong Wang,
Helen Pinto, Qiming Chen, Umeshwar Dayal and Mei-
Chun Hsu. 2004. Mining Sequential Patterns by Pattern-
Growth: The Prefix Span Approach. IEEE Trans. on
Knowledge and Data Engineering, 16(11): 1424-1440.
Sathiya Keerthi and Dennis DeCoste. 2005. A modified finite
Newton method for fast solution of large scale linear SVMs.
Journal of Machine Learning Research. 6: 341-361.
Taku Kudo and Yuji Matsumoto. 2001. Fast methods for
kernel-based text analysis. Annual Meeting of the Associa-
tion for Computational Linguistics, pages 24-31.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with sup-
port vector machines. Annual Meetings of the North
American Chapter and the Association for the Computa-
tional Linguistics.
Yue-Shi Lee and Yu-Chieh Wu. 2007. A Robust Multilingual
Portable Phrase Chunking System. Expert Systems with
Applications, 33(3): 1-26.
Vladimir N. Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer.
Chen Wang, Mingsheng Hong, Jian Pei, Haofeng Zhou, Wei
Wang and Baile Shi. 2004. Efficient Pattern-Growth
Methods for Frequent Tree Pattern Mining. Pacific knowl-
edge discovery in database (PAKDD).
</reference>
<page confidence="0.999446">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823629">
<title confidence="0.978564">An Approximate Approach for Training Polynomial Kernel SVMs in Linear Time</title>
<author confidence="0.9982">Yu-Chieh Wu Jie-Chi Yang Yue-Shi Lee</author>
<affiliation confidence="0.997166666666667">Dept. of Computer Science and Graduate Institute of Net- Dept. of Computer Science and Information Engineering work Learning Technology Information Engineering National Central University National Central University Ming Chuan University</affiliation>
<address confidence="0.987567">Taoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwan</address>
<email confidence="0.898768">bcbb@db.csie.ncu.edu.twyang@cl.ncu.edu.twlees@mcu.edu.tw</email>
<abstract confidence="0.998962954545455">Kernel methods such as support vector machines (SVMs) have attracted a great deal of popularity in the machine learning and natural language processing (NLP) communities. Polynomial kernel SVMs showed very competitive accuracy in many NLP problems, like part-of-speech tagging and chunking. However, these methods are usually too inefficient to be applied to large dataset and real time purpose. In this paper, we propose an approximate method to analogy polynomial kernel with efficient data mining approaches. To prevent exponential-scaled testing time complexity, we also present a new method for speeding up SVM classifying which does independent to the polynomial degree d. The experimental results showed that our method is 16.94 and 450 times faster than traditional polynomial kernel in terms of training and testing respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>Kernel methods for relational learning.</title>
<date>2003</date>
<booktitle>International Conference on Machine Learning,</booktitle>
<pages>104--114</pages>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. Kernel methods for relational learning. International Conference on Machine Learning, pages 104-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient support vector classifiers for named entity recognition.</title>
<date>2002</date>
<booktitle>International Conference on Computational Linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="1744" citStr="Isozaki and Kazawa, 2002" startWordPosition="241" endWordPosition="244"> the polynomial degree d. The experimental results showed that our method is 16.94 and 450 times faster than traditional polynomial kernel in terms of training and testing respectively. 1 Introduction Kernel methods, for example support vector machines (SVM) (Vapnik, 1995) are successfully applied to many natural language processing (NLP) problems. They yielded very competitive and satisfactory performance in many classification tasks, such as part-of-speech (POS) tagging (Gimenez and Marquez, 2003), shallow parsing (Kudo and Matsumoto, 2001, 2004; Lee and Wu, 2007), named entity recognition (Isozaki and Kazawa, 2002), and parsing (Nivre et al., 2006). In particular, the use of polynomial kernel SVM implicitly takes the feature combinations into ac65 count instead of explicitly combines features. By setting with polynomial kernel degree (i.e., d), different number of feature conjunctions can be implicitly computed. In this way, polynomial kernel SVM is often better than linear kernel which did not use feature conjunctions. However, the training and testing time costs for polynomial kernel SVM is far slow than the linear kernel. For example, it took one day to train the CoNLL-2000 task with polynomial kerne</context>
<context position="6123" citStr="Isozaki and Kazawa, 2002" startWordPosition="1001" endWordPosition="1004">tors (SVs), and are representative of training examples. The kernel function K is the kernel mapping function, which might map from ℜ to D&apos; D ℜ (usually D&lt;&lt;D’). The natural linear kernel simply uses the dot-product as (3). K (x, xi) = dot (x, xi) (3) A polynomial kernel of degree d is given by (4). K(x, xi) = (1 + dot (x, xi )) d (4) One can design or employ off-the-shelf kernel types for particular applications. In particular to the use of polynomial kernel-based SVM, it was shown to be the most successful kernels for many natural language processing (NLP) problems (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002; Nivre et al., 2006). It is known that the dot-product (linear form) represents the most efficient kernel computing which can produce the output value by linearly combining all support vectors such as y(x) = sign(dot(x, w) + b) where w= ∑ αy x i i i (5) an example of x using the polynomial kernel can be shown as follows. y(x) sign(( = ∑ αy dot x x ( ( , ) 1) ) ) d i i i + + b x SVs i ∈ support vector The situation is even worse when the number of support vectors become huge (Kudo and Matsumoto, 2004). Therefore, whether in training or testing phrase, the cost of kernel computations is far mor</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. International Conference on Computational Linguistics, pages 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Pei</author>
<author>Jiawei Han</author>
<author>Behzad Mortazavi-Asl</author>
<author>Jianyong Wang</author>
<author>Helen Pinto</author>
</authors>
<title>Qiming Chen, Umeshwar Dayal and MeiChun Hsu.</title>
<date>2004</date>
<journal>IEEE Trans. on Knowledge and Data Engineering,</journal>
<volume>16</volume>
<issue>11</issue>
<pages>1424--1440</pages>
<contexts>
<context position="3670" citStr="Pei et al., 2004" startWordPosition="547" endWordPosition="550">include bigram, trigram, etc. Nevertheless, selecting the feature conjunctions was manually and heuristically encoded and should perform amount of validation trials to discover which is useful or not. In recent years, several studies had reported that the training time of linear kernel SVM can be reduced to linear time (Joachims, 2006; Keerthi and DeCoste, 2005). But they did not and difficult to be extent to polynomial kernels. In this paper, we propose an approximate approach to extend the linear kernel SVM toward polynomial. By introducing the well-known sequential pattern mining approach (Pei et al., 2004), Proceedings of the ACL 2007 Demo and Poster Sessions, pages 65–68, Prague, June 2007. c�2007 Association for Computational Linguistics frequent feature conjunctions, namely patterns could be discovered and also kept as expand feature space. We then adopt the mined patterns to rerepresent the training/testing examples. Subsequently, we use the off-the-shelf linear kernel SVM algorithm to perform training and testing. Besides, to exponential-scaled testing time complexity, we propose a new classification method for speeding up the SVM testing. Rather than enumerating all patterns for each exam</context>
<context position="7698" citStr="Pei et al., 2004" startWordPosition="1282" endWordPosition="1285">kernel function (4) for each 3 Approximate Polynomial Kernel In 2004, Kudo and Matsumoto (2004) derived both implicitly (6) and explicitly form of polynomial kernel. They indicated that the use of explicitly enumerate the feature combinations is equivalent to the polynomial kernel (see Lemma 1 and Example 1, Kudo and Matsumoto, 2004) which shared the same view of and Roth, 2003). We follow the similar idea of the above studies that requires explicitly enumerated all feature combinations. To meet with our problem, we employ the well-known sequential pattern mining algorithm, namely PrefixSpan (Pei et al., 2004) to efficient mine the frequent patterns. However, directly adopt the algorithm is not a good idea. To fit with SVM, we modify the original PrefixSpan algorithm according to the following constraints. Given a set features, the PrefixSpan mines the frequent patterns which occurs more than predefined minimum support in the training set and limited in the length of predefined d, which is equivalent to the polynomial kern (Cumby el degree d. For exam(1) x SVs i ∈ (6) 66 ple, if the minimum support is 5, and d=2, then a feature combination (fi, fj) must appear more than 5 times in set of x. Definit</context>
<context position="9677" citStr="Pei et al., 2004" startWordPosition="1615" endWordPosition="1618">, in feature combination mining for SVM only contains a set of feature vectors each of which is independently treated. In other words, no compound features in the vector. If it exists, one can simply expand the compound features as another new feature. By means of the above constraints, mining the frequent patterns can be reduced to mining the limited length of frequent patterns in the single-item database (set of ordered vectors). Furthermore, during each phase, we need only focus on finding the “frequent single features” to expand previous phase. More detail implementation issues can refer (Pei et al., 2004). 3.1 Speed-up Testing To efficiently expand new features for the original feature vectors, we propose a new method to fast discovery patterns. Essentially, the PrefixSpan algorithm gradually expands one item from previous result which can be viewed as a tree growing. An example can be found in Figure 1. Each node in Figure 1 is the associate feature of root. The whole patterns expanded by fj can be represented as the path from root to each node. For example, pattern (fj, fk, fm, fr) can be found via traversing the tree starting from fj. In this way, we can re-expand the original feature vecto</context>
</contexts>
<marker>Pei, Han, Mortazavi-Asl, Wang, Pinto, 2004</marker>
<rawString>Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal and MeiChun Hsu. 2004. Mining Sequential Patterns by PatternGrowth: The Prefix Span Approach. IEEE Trans. on Knowledge and Data Engineering, 16(11): 1424-1440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sathiya Keerthi</author>
<author>Dennis DeCoste</author>
</authors>
<title>A modified finite Newton method for fast solution of large scale linear SVMs.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research.</journal>
<volume>6</volume>
<pages>341--361</pages>
<contexts>
<context position="3417" citStr="Keerthi and DeCoste, 2005" startWordPosition="504" endWordPosition="507">SVM simply disregards the effect of feature combinations during training and testing, it performs not only more efficient than polynomial kernel, but also can be improved through directly appending features derived from the set of feature combinations. Examples include bigram, trigram, etc. Nevertheless, selecting the feature conjunctions was manually and heuristically encoded and should perform amount of validation trials to discover which is useful or not. In recent years, several studies had reported that the training time of linear kernel SVM can be reduced to linear time (Joachims, 2006; Keerthi and DeCoste, 2005). But they did not and difficult to be extent to polynomial kernels. In this paper, we propose an approximate approach to extend the linear kernel SVM toward polynomial. By introducing the well-known sequential pattern mining approach (Pei et al., 2004), Proceedings of the ACL 2007 Demo and Poster Sessions, pages 65–68, Prague, June 2007. c�2007 Association for Computational Linguistics frequent feature conjunctions, namely patterns could be discovered and also kept as expand feature space. We then adopt the mined patterns to rerepresent the training/testing examples. Subsequently, we use the </context>
<context position="5064" citStr="Keerthi and DeCoste, 2005" startWordPosition="791" endWordPosition="794">is the average number of patterns per feature. 2 SVM and Kernel Methods Suppose we have the training instance set for binary classification problem: ( 1, 1), ( 2, 2),..., ( , ), x y x y xn yn xi ∈ ℜ i ∈ + − D, { 1, 1} y where xi is a feature vector in D-dimension space of the i-th example, and yi is the label of xi either positive or negative. The training of SVMs involves in minimize the following object (primal form, soft-margin) (Vapnik, 1995): n 1 minimize: (α) W = ⋅ + W W C Loss Wxi yi ( , ) ∑= 2 i 1 The loss function indicates the loss of training error. Usually, the hinge-loss is used (Keerthi and DeCoste, 2005). The factor C in (1) is a parameter that allows one to trade off training error and margin. A small value for C will increase the number of training errors. To determine the class (+1 or -1) of an example x can be judged by computing the following equation. y x ( ) sign(( ∑ ( , )) ) = α y K x x b + i i i x SVs i∈ (2) αi is the weight of training example xi (αi&gt;0), and b denotes as a threshold. Here the xi should be the support vectors (SVs), and are representative of training examples. The kernel function K is the kernel mapping function, which might map from ℜ to D&apos; D ℜ (usually D&lt;&lt;D’). The </context>
<context position="12714" citStr="Keerthi and DeCoste, 2005" startWordPosition="2153" endWordPosition="2156">egree. 67 4 Experiments To evaluate our method, we examine the wellknown shallow parsing task which is the task of CoNLL-20001. We also adopted the released perlevaluator to measure the recall/precision/f1 rates. The used feature consists of word, POS, orthographic, affix(2-4 prefix/suffix letters), and previous chunk tags in the two words context window size (the same as (Lee and Wu, 2007)). We limited the features should at least appear more than twice in the training set. For the learning algorithm, we replicate the modified finite Newton SVM as learner which can be trained in linear time (Keerthi and DeCoste, 2005). We also compare our method with the standard linear and polynomial kernels with SVMlight 2. 4.1 Results Table 2 lists the experimental results on the CoNLL-2000 shallow parsing task. Table 3 compares the testing speed of different feature expansion techniques, namely, array visiting (our method) and enumeration. Table 2: Experimental results for CoNLL-2000 shallow parsing task CoNLL-2000 F1 Mining Training Testing Time Time Time Linear Kernel 93.15 N/A 0.53hr 2.57s Polynomial(d=2) 94.19 N/A 11.52hr 3189.62s Polynomial(d=3) 93.95 N/A 19.43hr 6539.75s Our Method 93.71 &lt;10s 0.68hr 6.54s (d=2,su</context>
</contexts>
<marker>Keerthi, DeCoste, 2005</marker>
<rawString>Sathiya Keerthi and Dennis DeCoste. 2005. A modified finite Newton method for fast solution of large scale linear SVMs. Journal of Machine Learning Research. 6: 341-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2001</date>
<booktitle>Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>24--31</pages>
<contexts>
<context position="1666" citStr="Kudo and Matsumoto, 2001" startWordPosition="229" endWordPosition="232">resent a new method for speeding up SVM classifying which does independent to the polynomial degree d. The experimental results showed that our method is 16.94 and 450 times faster than traditional polynomial kernel in terms of training and testing respectively. 1 Introduction Kernel methods, for example support vector machines (SVM) (Vapnik, 1995) are successfully applied to many natural language processing (NLP) problems. They yielded very competitive and satisfactory performance in many classification tasks, such as part-of-speech (POS) tagging (Gimenez and Marquez, 2003), shallow parsing (Kudo and Matsumoto, 2001, 2004; Lee and Wu, 2007), named entity recognition (Isozaki and Kazawa, 2002), and parsing (Nivre et al., 2006). In particular, the use of polynomial kernel SVM implicitly takes the feature combinations into ac65 count instead of explicitly combines features. By setting with polynomial kernel degree (i.e., d), different number of feature conjunctions can be implicitly computed. In this way, polynomial kernel SVM is often better than linear kernel which did not use feature conjunctions. However, the training and testing time costs for polynomial kernel SVM is far slow than the linear kernel. F</context>
<context position="6097" citStr="Kudo and Matsumoto, 2001" startWordPosition="997" endWordPosition="1000"> should be the support vectors (SVs), and are representative of training examples. The kernel function K is the kernel mapping function, which might map from ℜ to D&apos; D ℜ (usually D&lt;&lt;D’). The natural linear kernel simply uses the dot-product as (3). K (x, xi) = dot (x, xi) (3) A polynomial kernel of degree d is given by (4). K(x, xi) = (1 + dot (x, xi )) d (4) One can design or employ off-the-shelf kernel types for particular applications. In particular to the use of polynomial kernel-based SVM, it was shown to be the most successful kernels for many natural language processing (NLP) problems (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002; Nivre et al., 2006). It is known that the dot-product (linear form) represents the most efficient kernel computing which can produce the output value by linearly combining all support vectors such as y(x) = sign(dot(x, w) + b) where w= ∑ αy x i i i (5) an example of x using the polynomial kernel can be shown as follows. y(x) sign(( = ∑ αy dot x x ( ( , ) 1) ) ) d i i i + + b x SVs i ∈ support vector The situation is even worse when the number of support vectors become huge (Kudo and Matsumoto, 2004). Therefore, whether in training or testing phrase, the cost of kern</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Fast methods for kernel-based text analysis. Annual Meeting of the Association for Computational Linguistics, pages 24-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>Annual Meetings of the North American Chapter and the Association for the Computational Linguistics.</booktitle>
<contexts>
<context position="1666" citStr="Kudo and Matsumoto, 2001" startWordPosition="229" endWordPosition="232">resent a new method for speeding up SVM classifying which does independent to the polynomial degree d. The experimental results showed that our method is 16.94 and 450 times faster than traditional polynomial kernel in terms of training and testing respectively. 1 Introduction Kernel methods, for example support vector machines (SVM) (Vapnik, 1995) are successfully applied to many natural language processing (NLP) problems. They yielded very competitive and satisfactory performance in many classification tasks, such as part-of-speech (POS) tagging (Gimenez and Marquez, 2003), shallow parsing (Kudo and Matsumoto, 2001, 2004; Lee and Wu, 2007), named entity recognition (Isozaki and Kazawa, 2002), and parsing (Nivre et al., 2006). In particular, the use of polynomial kernel SVM implicitly takes the feature combinations into ac65 count instead of explicitly combines features. By setting with polynomial kernel degree (i.e., d), different number of feature conjunctions can be implicitly computed. In this way, polynomial kernel SVM is often better than linear kernel which did not use feature conjunctions. However, the training and testing time costs for polynomial kernel SVM is far slow than the linear kernel. F</context>
<context position="6097" citStr="Kudo and Matsumoto, 2001" startWordPosition="997" endWordPosition="1000"> should be the support vectors (SVs), and are representative of training examples. The kernel function K is the kernel mapping function, which might map from ℜ to D&apos; D ℜ (usually D&lt;&lt;D’). The natural linear kernel simply uses the dot-product as (3). K (x, xi) = dot (x, xi) (3) A polynomial kernel of degree d is given by (4). K(x, xi) = (1 + dot (x, xi )) d (4) One can design or employ off-the-shelf kernel types for particular applications. In particular to the use of polynomial kernel-based SVM, it was shown to be the most successful kernels for many natural language processing (NLP) problems (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002; Nivre et al., 2006). It is known that the dot-product (linear form) represents the most efficient kernel computing which can produce the output value by linearly combining all support vectors such as y(x) = sign(dot(x, w) + b) where w= ∑ αy x i i i (5) an example of x using the polynomial kernel can be shown as follows. y(x) sign(( = ∑ αy dot x x ( ( , ) 1) ) ) d i i i + + b x SVs i ∈ support vector The situation is even worse when the number of support vectors become huge (Kudo and Matsumoto, 2004). Therefore, whether in training or testing phrase, the cost of kern</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. Annual Meetings of the North American Chapter and the Association for the Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue-Shi Lee</author>
<author>Yu-Chieh Wu</author>
</authors>
<title>A Robust Multilingual Portable Phrase Chunking System. Expert Systems with Applications,</title>
<date>2007</date>
<volume>33</volume>
<issue>3</issue>
<pages>1--26</pages>
<contexts>
<context position="1691" citStr="Lee and Wu, 2007" startWordPosition="234" endWordPosition="237"> up SVM classifying which does independent to the polynomial degree d. The experimental results showed that our method is 16.94 and 450 times faster than traditional polynomial kernel in terms of training and testing respectively. 1 Introduction Kernel methods, for example support vector machines (SVM) (Vapnik, 1995) are successfully applied to many natural language processing (NLP) problems. They yielded very competitive and satisfactory performance in many classification tasks, such as part-of-speech (POS) tagging (Gimenez and Marquez, 2003), shallow parsing (Kudo and Matsumoto, 2001, 2004; Lee and Wu, 2007), named entity recognition (Isozaki and Kazawa, 2002), and parsing (Nivre et al., 2006). In particular, the use of polynomial kernel SVM implicitly takes the feature combinations into ac65 count instead of explicitly combines features. By setting with polynomial kernel degree (i.e., d), different number of feature conjunctions can be implicitly computed. In this way, polynomial kernel SVM is often better than linear kernel which did not use feature conjunctions. However, the training and testing time costs for polynomial kernel SVM is far slow than the linear kernel. For example, it took one d</context>
<context position="12481" citStr="Lee and Wu, 2007" startWordPosition="2114" endWordPosition="2117"> of our method is O(Favg*Navg) where Favg is the average number of frequent features per example, while the Navg is the average length of l2- sequence. In other words, our method does not dependent on the polynomial kernel degree. 67 4 Experiments To evaluate our method, we examine the wellknown shallow parsing task which is the task of CoNLL-20001. We also adopted the released perlevaluator to measure the recall/precision/f1 rates. The used feature consists of word, POS, orthographic, affix(2-4 prefix/suffix letters), and previous chunk tags in the two words context window size (the same as (Lee and Wu, 2007)). We limited the features should at least appear more than twice in the training set. For the learning algorithm, we replicate the modified finite Newton SVM as learner which can be trained in linear time (Keerthi and DeCoste, 2005). We also compare our method with the standard linear and polynomial kernels with SVMlight 2. 4.1 Results Table 2 lists the experimental results on the CoNLL-2000 shallow parsing task. Table 3 compares the testing speed of different feature expansion techniques, namely, array visiting (our method) and enumeration. Table 2: Experimental results for CoNLL-2000 shallo</context>
</contexts>
<marker>Lee, Wu, 2007</marker>
<rawString>Yue-Shi Lee and Yu-Chieh Wu. 2007. A Robust Multilingual Portable Phrase Chunking System. Expert Systems with Applications, 33(3): 1-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="1392" citStr="Vapnik, 1995" startWordPosition="195" endWordPosition="196"> are usually too inefficient to be applied to large dataset and real time purpose. In this paper, we propose an approximate method to analogy polynomial kernel with efficient data mining approaches. To prevent exponential-scaled testing time complexity, we also present a new method for speeding up SVM classifying which does independent to the polynomial degree d. The experimental results showed that our method is 16.94 and 450 times faster than traditional polynomial kernel in terms of training and testing respectively. 1 Introduction Kernel methods, for example support vector machines (SVM) (Vapnik, 1995) are successfully applied to many natural language processing (NLP) problems. They yielded very competitive and satisfactory performance in many classification tasks, such as part-of-speech (POS) tagging (Gimenez and Marquez, 2003), shallow parsing (Kudo and Matsumoto, 2001, 2004; Lee and Wu, 2007), named entity recognition (Isozaki and Kazawa, 2002), and parsing (Nivre et al., 2006). In particular, the use of polynomial kernel SVM implicitly takes the feature combinations into ac65 count instead of explicitly combines features. By setting with polynomial kernel degree (i.e., d), different num</context>
<context position="4888" citStr="Vapnik, 1995" startWordPosition="754" endWordPosition="755">our method requires O(Favg*Navg) which is independent to the polynomial kernel degree. Favg is the average number of frequent features per example, while the Navg is the average number of patterns per feature. 2 SVM and Kernel Methods Suppose we have the training instance set for binary classification problem: ( 1, 1), ( 2, 2),..., ( , ), x y x y xn yn xi ∈ ℜ i ∈ + − D, { 1, 1} y where xi is a feature vector in D-dimension space of the i-th example, and yi is the label of xi either positive or negative. The training of SVMs involves in minimize the following object (primal form, soft-margin) (Vapnik, 1995): n 1 minimize: (α) W = ⋅ + W W C Loss Wxi yi ( , ) ∑= 2 i 1 The loss function indicates the loss of training error. Usually, the hinge-loss is used (Keerthi and DeCoste, 2005). The factor C in (1) is a parameter that allows one to trade off training error and margin. A small value for C will increase the number of training errors. To determine the class (+1 or -1) of an example x can be judged by computing the following equation. y x ( ) sign(( ∑ ( , )) ) = α y K x x b + i i i x SVs i∈ (2) αi is the weight of training example xi (αi&gt;0), and b denotes as a threshold. Here the xi should be the </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Wang</author>
<author>Mingsheng Hong</author>
<author>Jian Pei</author>
<author>Haofeng Zhou</author>
<author>Wei Wang</author>
<author>Baile Shi</author>
</authors>
<title>Efficient Pattern-Growth Methods for Frequent Tree Pattern Mining. Pacific knowledge discovery in database (PAKDD).</title>
<date>2004</date>
<contexts>
<context position="10728" citStr="Wang et al., 2004" startWordPosition="1809" endWordPosition="1812">oot to each node. For example, pattern (fj, fk, fm, fr) can be found via traversing the tree starting from fj. In this way, we can re-expand the original feature vector via visiting corresponding trees for each feature. Figure 1: The tree representation of feature fj Table 1: Encoding frequent patterns with DFS array representation Level 0 1 2 3 2 1 2 1 2 2 Label Root k m r p m p o p q Item fj fk fm fr fp fm fp fo fp fq However, traversing arrays is much more efficient than visiting trees. Therefore, we adopt the l2- sequences encoding method based on the DFS (depth-first-search) sequence as (Wang et al., 2004) to represent the trees. An l2-sequence does not only store the label information but also take the node level into account. Examples can be found in Table 1. Theorem 4 (Uniqueness of l2-sequence): Given trees T1, and T2, their l2-sequences are identical if and only if T1 and T2 are isomorphic, i.e., there exists a one-to-one mapping for set of nodes, node labels, edges, and root nodes. Proof. see theorem 1 in (Wang et al., 2004). Definition 5 (Ascend-descend relation): Given a node k of feature fk in l2-sequence, all of the descendant of k that rooted by k have the greater feature numbers tha</context>
</contexts>
<marker>Wang, Hong, Pei, Zhou, Wang, Shi, 2004</marker>
<rawString>Chen Wang, Mingsheng Hong, Jian Pei, Haofeng Zhou, Wei Wang and Baile Shi. 2004. Efficient Pattern-Growth Methods for Frequent Tree Pattern Mining. Pacific knowledge discovery in database (PAKDD).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>