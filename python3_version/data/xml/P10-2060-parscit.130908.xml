<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011968">
<title confidence="0.9977575">
Optimizing Informativeness and Readability
for Sentiment Summarization
</title>
<author confidence="0.965701">
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro Kikui
</author>
<affiliation confidence="0.764355">
NTT Cyber Space Laboratories, NTT Corporation
</affiliation>
<address confidence="0.783211">
1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan
{ nishikawa.hitoshi, hasegawa.takaaki
</address>
<email confidence="0.901399">
@lab.ntt.co.jp
matsuo.yoshihiro, kikui.genichiro
</email>
<sectionHeader confidence="0.995534" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999870625">
We propose a novel algorithm for senti-
ment summarization that takes account of
informativeness and readability, simulta-
neously. Our algorithm generates a sum-
mary by selecting and ordering sentences
taken from multiple review texts according
to two scores that represent the informa-
tiveness and readability of the sentence or-
der. The informativeness score is defined
by the number of sentiment expressions
and the readability score is learned from
the target corpus. We evaluate our method
by summarizing reviews on restaurants.
Our method outperforms an existing al-
gorithm as indicated by its ROUGE score
and human readability experiments.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.935158833333333">
The Web holds a massive number of reviews de-
scribing the sentiments of customers about prod-
ucts and services. These reviews can help the user
reach purchasing decisions and guide companies’
business activities such as product improvements.
It is, however, almost impossible to read all re-
views given their sheer number.
These reviews are best utilized by the devel-
opment of automatic text summarization, partic-
ularly sentiment summarization. It enables us to
efficiently grasp the key bits of information. Senti-
ment summarizers are divided into two categories
in terms of output style. One outputs lists of
sentences (Hu and Liu, 2004; Blair-Goldensohn
et al., 2008; Titov and McDonald, 2008), the
other outputs texts consisting of ordered sentences
(Carenini et al., 2006; Carenini and Cheung, 2008;
Lerman et al., 2009; Lerman and McDonald,
2009). Our work lies in the latter category, and
a typical summary is shown in Figure 1. Although
visual representations such as bar or rader charts
This restaurant offers customers delicious foods and a
relaxing atmosphere. The staff are very friendly but the
price is a little high.
</bodyText>
<figureCaption confidence="0.999624">
Figure 1: A typical summary.
</figureCaption>
<bodyText confidence="0.999962617647059">
are helpful, such representations necessitate some
simplifications of information to presentation. In
contrast, text can present complex information that
can’t readily be visualized, so in this paper we fo-
cus on producing textual summaries.
One crucial weakness of existing text-oriented
summarizers is the poor readability of their results.
Good readability is essential because readability
strongly affects text comprehension (Barzilay et
al., 2002).
To achieve readable summaries, the extracted
sentences must be appropriately ordered (Barzilay
et al., 2002; Lapata, 2003; Barzilay and Lee, 2004;
Barzilay and Lapata, 2005). Barzilay et al. (2002)
proposed an algorithm for ordering sentences ac-
cording to the dates of the publications from which
the sentences were extracted. Lapata (2003) pro-
posed an algorithm that computes the probability
of two sentences being adjacent for ordering sen-
tences. Both methods delink sentence extraction
from sentence ordering, so a sentence can be ex-
tracted that cannot be ordered naturally with the
other extracted sentences.
To solve this problem, we propose an algorithm
that chooses sentences and orders them simulta-
neously in such a way that the ordered sentences
maximize the scores of informativeness and read-
ability. Our algorithm efficiently searches for the
best sequence of sentences by using dynamic pro-
gramming and beam search. We verify that our
method generates summaries that are significantly
better than the baseline results in terms of ROUGE
score (Lin, 2004) and subjective readability mea-
sures. As far as we know, this is the first work to
</bodyText>
<page confidence="0.99098">
325
</page>
<note confidence="0.507122">
Proceedings of the ACL 2010 Conference Short Papers, pages 325–330,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999801714285714">
simultaneously achieve both informativeness and
readability in the area of multi-document summa-
rization.
This paper is organized as follows: Section 2
describes our summarization method. Section 3
reports our evaluation experiments. We conclude
this paper in Section 4.
</bodyText>
<sectionHeader confidence="0.85734" genericHeader="method">
2 Optimizing Sentence Sequence
</sectionHeader>
<bodyText confidence="0.996805333333333">
Formally, we define a summary S* =
(s0, ss, ... , sn, sn+s) as a sequence consist-
ing of n sentences where s0 and sn+s are symbols
indicating the beginning and ending of the se-
quence, respectively. Summary S* is also defined
as follows:
</bodyText>
<equation confidence="0.997305333333333">
S* = argmax [Info(S) + ARead(S)] (1)
SET
s.t. length(S) G K
</equation>
<bodyText confidence="0.9999109">
where Info(S) indicates the informativeness
score of S, Read(S) indicates the readability
score of S, T indicates possible sequences com-
posed of sentences in the target documents, A
is a weight parameter balancing informativeness
against readability, length(S) is the length of S,
and K is the maximum size of the summary.
We introduce the informativeness score and the
readability score, then describe how to optimize a
sequence.
</bodyText>
<subsectionHeader confidence="0.967777">
2.1 Informativeness Score
</subsectionHeader>
<bodyText confidence="0.9998805">
Since we attempt to summarize reviews, we as-
sume that a good summary must involve as many
sentiments as possible. Therefore, we define the
informativeness score as follows:
</bodyText>
<equation confidence="0.9986375">
Info(S) = ∑ f(e) (2)
eEE(S)
</equation>
<bodyText confidence="0.999982444444444">
where e indicates sentiment e = (a, p) as the tu-
ple of aspect a and polarity p = {−1, 0, 11, E(S)
is the set of sentiments contained S, and f(e) is the
score of sentiment e. Aspect a represents a stand-
point for evaluating products and services. With
regard to restaurants, aspects include food, atmo-
sphere and staff. Polarity represents whether the
sentiment is positive or negative. In this paper, we
define p = −1 as negative, p = 0 as neutral and
p = 1 as positive sentiment.
Notice that Equation 2 defines the informative-
ness score of a summary as the sum of the score
of the sentiments contained in S. To avoid du-
plicative sentences, each sentiment is counted only
once for scoring. In addition, the aspects are clus-
tered and similar aspects (e.g. air, ambience) are
treated as the same aspect (e.g. atmosphere). In
this paper we define f(e) as the frequency of e in
the target documents.
Sentiments are extracted using a sentiment lex-
icon and pattern matched from dependency trees
of sentences. The sentiment lexicons consists of
pairs of sentiment expressions and their polarities,
for example, delicious, friendly and good are pos-
itive sentiment expressions, bad and expensive are
negative sentiment expressions.
To extract sentiments from given sentences,
first, we identify sentiment expressions among
words consisting of parsed sentences. For ex-
ample, in the case of the sentence “This restau-
rant offers customers delicious foods and a relax-
ing atmosphere.” in Figure 1, delicious and re-
laxing are identified as sentiment expressions. If
the sentiment expressions are identified, the ex-
pressions and its aspects are extracted as aspect-
sentiment expression pairs from dependency tree
using some rules. In the case of the example sen-
tence, foods and delicious, atmosphere and relax-
ing are extracted as aspect-sentiment expression
pairs. Finally extracted sentiment expressions are
converted to polarities, we acquire the set of sen-
timents from sentences, for example, ( foods, 1)
and ( atmosphere, 1).
Note that since our method relies on only senti-
ment lexicon, extractable aspects are unlimited.
</bodyText>
<subsectionHeader confidence="0.99884">
2.2 Readability Score
</subsectionHeader>
<bodyText confidence="0.936876368421053">
Readability consists of various elements such as
conciseness, coherence, and grammar. Since it
is difficult to model all of them, we approximate
readability as the natural order of sentences.
To order sentences, Barzilay et al. (2002)
used the publication dates of documents to catch
temporally-ordered events, but this approach is not
really suitable for our goal because reviews focus
on entities rather than events. Lapata (2003) em-
ployed the probability of two sentences being ad-
jacent as determined from a corpus. If the cor-
pus consists of reviews, it is expected that this ap-
proach would be effective for sentiment summa-
rization. Therefore, we adopt and improve Lap-
ata’s approach to order sentences. We define the
&apos;Since we aim to summarize Japanese reviews, we utilize
Japanese sentiment lexicon (Asano et al., 2008). However,
our method is, except for sentiment extraction, language in-
dependent.
</bodyText>
<page confidence="0.989714">
326
</page>
<bodyText confidence="0.796219">
readability score as follows:
</bodyText>
<equation confidence="0.993009333333333">
n
Read(S) = ∑ wTφ(si, si+1) (3)
i=0
</equation>
<bodyText confidence="0.999324833333333">
where, given two adjacent sentences si and
si+1, wTφ(si, si+1), which measures the connec-
tivity of the two sentences, is the inner product of
w and φ(si, si+1), w is a parameter vector and
φ(si, si+1) is a feature vector of the two sentences.
That is, the readability score of sentence sequence
S is the sum of the connectivity of all adjacent sen-
tences in the sequence.
As the features, Lapata (2003) proposed the
Cartesian product of content words in adjacent
sentences. To this, we add named entity tags (e.g.
LOC, ORG) and connectives. We observe that the
first sentence of a review of a restaurant frequently
contains named entities indicating location. We
aim to reproduce this characteristic in the order-
ing.
We also define feature vector b(S) of the entire
sequence S = (s0, s1, ... , sn, sn+1) as follows:
</bodyText>
<equation confidence="0.995083333333333">
n
b(S) = ∑ φ(si,si+1) (4)
i=0
</equation>
<bodyText confidence="0.999978153846154">
Therefore, the score of sequence S is wTb(S).
Given a training set, if a trained parameter w as-
signs a score wTb(S+) to an correct order S+
that is higher than a score wTb(S−) to an incor-
rect order S−, it is expected that the trained pa-
rameter will give higher score to naturally ordered
sentences than to unnaturally ordered sentences.
We use Averaged Perceptron (Collins, 2002) to
find w. Averaged Perceptron requires an argmax
operation for parameter estimation. Since we at-
tempt to order a set of sentences, the operation is
regarded as solving the Traveling Salesman Prob-
lem; that is, we locate the path that offers maxi-
mum score through all n sentences as s0 and sn+1
are starting and ending points, respectively. Thus
the operation is NP-hard and it is difficult to find
the global optimal solution. To alleviate this, we
find an approximate solution by adopting the dy-
namic programming technique of the Held and
Karp Algorithm (Held and Karp, 1962) and beam
search.
We show the search procedure in Figure 2. S
indicates intended sentences and M is a distance
matrix of the readability scores of adjacent sen-
tence pairs. Hi(C, j) indicates the score of the
hypothesis that has covered the set of i sentences
</bodyText>
<equation confidence="0.88699375">
C and has the sentence j at the end of the path,
Sentences: S = {s1, ... , sn}
Distance matrix: M = [ai,j]i=0...n+1,j=0...n+1
1: H0({s0}, s0) = 0
</equation>
<listItem confidence="0.934540833333333">
2: for i : 0 ... n − 1
3: for j : 1 ... n
4: foreach Hi(C\{j}, k) E b
5: Hi+1(C, j) = maxHi(C\1j1,k)Eb Hi(C\{j}, k)
6: +Mk,j
7: H* = maxHn(C,k) Hn(C, k) + Mk,n+1
</listItem>
<figureCaption confidence="0.998008">
Figure 2: Held and Karp Algorithm.
</figureCaption>
<bodyText confidence="0.999751466666667">
i.e. the last sentence of the summary being gener-
ated. For example, H2({s0, s2, s5}, s2) indicates
a hypothesis that covers s0, s2, s5 and the last sen-
tence is s2. Initially, H0({s0}, s0) is assigned the
score of 0, and new sentences are then added one
by one. In the search procedure, our dynamic pro-
gramming based algorithm retains just the hypoth-
esis with maximum score among the hypotheses
that have the same sentences and the same last sen-
tence. Since this procedure is still computationally
hard, only the top b hypotheses are expanded.
Note that our method learns w from texts auto-
matically annotated by a POS tagger and a named
entity tagger. Thus manual annotation isn’t re-
quired.
</bodyText>
<subsectionHeader confidence="0.990235">
2.3 Optimization
</subsectionHeader>
<bodyText confidence="0.999996470588235">
The argmax operation in Equation 1 also involves
search, which is NP-hard as described in Section
2.2. Therefore, we adopt the Held and Karp Algo-
rithm and beam search to find approximate solu-
tions. The search algorithm is basically the same
as parameter estimation, except for its calculation
of the informativeness score and size limitation.
Therefore, when a new sentence is added to a hy-
pothesis, both the informativeness and the read-
ability scores are calculated. The size of the hy-
pothesis is also calculated and if the size exceeds
the limit, the sentence can’t be added. A hypoth-
esis that can’t accept any more sentences is re-
moved from the search procedure and preserved
in memory. After all hypotheses are removed,
the best hypothesis is chosen from among the pre-
served hypotheses as the solution.
</bodyText>
<sectionHeader confidence="0.999798" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999743333333333">
This section evaluates our method in terms of
ROUGE score and readability. We collected 2,940
reviews of 100 restaurants from a website. The
</bodyText>
<page confidence="0.995776">
327
</page>
<table confidence="0.998180166666667">
R-2 R-SU4 R-SU9
Baseline 0.089 0.068 0.062
Method1 0.157 0.096 0.089
Method2 0.172 0.107 0.098
Method3 0.180 0.110 0.101
Human 0.258 0.143 0.131
</table>
<tableCaption confidence="0.995437">
Table 1: Automatic ROUGE evaluation.
</tableCaption>
<table confidence="0.951611">
Numbers
Baseline 1.76
Method1 4.32
Method2 10.41
Method3 10.18
Human 4.75
</table>
<tableCaption confidence="0.999226">
Table 2: Unique sentiment numbers.
</tableCaption>
<bodyText confidence="0.999885666666667">
average size of each document set (corresponds to
one restaurant) was 5,343 bytes. We attempted
to generate 300 byte summaries, so the summa-
rization rate was about 6%. We used CRFs-
based Japanese dependency parser (Imamura et
al., 2007) and named entity recognizer (Suzuki et
al., 2006) for sentiment extraction and construct-
ing feature vectors for readability score, respec-
tively.
</bodyText>
<subsectionHeader confidence="0.94649">
3.1 ROUGE
</subsectionHeader>
<bodyText confidence="0.999088788461539">
We used ROUGE (Lin, 2004) for evaluating the
content of summaries. We chose ROUGE-2,
ROUGE-SU4 and ROUGE-SU9. We prepared
four reference summaries for each document set.
To evaluate the effects of the informativeness
score, the readability score and the optimization,
we compared the following five methods.
Baseline: employs MMR (Carbonell and Gold-
stein, 1998). We designed the score of a sentence
as term frequencies of the content words in a doc-
ument set.
Method1: uses optimization without the infor-
mativeness score or readability score. It also used
term frequencies to score sentences.
Method2: uses the informativeness score and
optimization without the readability score.
Method3: the proposed method. Following
Equation 1, the summarizer searches for a se-
quence with high informativeness and readability
score. The parameter vector w was trained on the
same 2,940 reviews in 5-fold cross validation fash-
ion. A was set to 6,000 using a development set.
Human is the reference summaries. To com-
pare our summarizer to human summarization, we
calculated ROUGE scores between each reference
and the other references, and averaged them.
The results of these experiments are shown in
Table 1. ROUGE scores increase in the order of
Method1, Method2 and Method3 but no method
could match the performance of Human. The
methods significantly outperformed Baseline ac-
cording to the Wilcoxon signed-rank test.
We discuss the contribution of readability to
ROUGE scores. Comparing Method2 to Method3,
ROUGE scores of the latter were higher for all cri-
teria. It is interesting that the readability criterion
also improved ROUGE scores.
We also evaluated our method in terms of sen-
timents. We extracted sentiments from the sum-
maries using the above sentiment extractor, and
averaged the unique sentiment numbers. Table 2
shows the results.
The references (Human) have fewer sentiments
than the summaries generated by our method. In
other words, the references included almost as
many other sentences (e.g. reasons for the senti-
ments) as those expressing sentiments. Carenini
et al. (2006) pointed out that readers wanted “de-
tailed information” in summaries, and the reasons
are one of such piece of information. Including
them in summaries would greatly improve sum-
marizer appeal.
</bodyText>
<subsectionHeader confidence="0.998042">
3.2 Readability
</subsectionHeader>
<bodyText confidence="0.998101473684211">
Readability was evaluated by human judges.
Three different summarizers generated summaries
for each document set. Ten judges evaluated the
thirty summaries for each. Before the evalua-
tion the judges read evaluation criteria and gave
points to summaries using a five-point scale. The
judges weren’t informed of which method gener-
ated which summary.
We compared three methods; Ordering sen-
tences according to publication dates and posi-
tions in which sentences appear after sentence
extraction (Method2), Ordering sentences us-
ing the readability score after sentence extrac-
tion (Method2+) and searching a document set
to discover the sequence with the highest score
(Method3).
Table 3 shows the results of the experiment.
Readability increased in the order of Method2,
Method2+ and Method3. According to the
</bodyText>
<page confidence="0.994912">
328
</page>
<table confidence="0.955993">
Readability point
Method2 3.45
Method2+ 3.54
Method3 3.74
</table>
<tableCaption confidence="0.999872">
Table 3: Readability evaluation.
</tableCaption>
<bodyText confidence="0.999815636363636">
Wilcoxon signed-rank test, there was no signifi-
cance difference between Method2 and Method2+
but the difference between Method2 and Method3
was significant, p &lt; 0.10.
One important factor behind the higher read-
ability of Method3 is that it yields longer sen-
tences on average (6.52). Method2 and Method2+
yielded averages of 7.23 sentences. The difference
is significant as indicated by p &lt; 0.01. That is,
Method2 and Method2+ tended to select short sen-
tences, which made their summaries less readable.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999978166666667">
This paper proposed a novel algorithm for senti-
ment summarization that takes account of infor-
mativeness and readability, simultaneously. To
summarize reviews, the informativeness score is
based on sentiments and the readability score is
learned from a corpus of reviews. The preferred
sequence is determined by using dynamic pro-
gramming and beam search. Experiments showed
that our method generated better summaries than
the baseline in terms of ROUGE score and read-
ability.
One future work is to include important infor-
mation other than sentiments in the summaries.
We also plan to model the order of sentences glob-
ally. Although the ordering model in this paper is
local since it looks at only adjacent sentences, a
model that can evaluate global order is important
for better summaries.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999978">
We would like to sincerely thank Tsutomu Hirao
for his comments and discussions. We would also
like to thank the reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.999321" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998532275862069">
Hisako Asano, Toru Hirano, Nozomi Kobayashi and
Yoshihiro Matsuo. 2008. Subjective Information In-
dexing Technology Analyzing Word-of-mouth Con-
tent on the Web. NTT Technical Review, Vol.6, No.9.
Regina Barzilay, Noemie Elhadad and Kathleen McK-
eown. 2002. Inferring Strategies for Sentence Or-
dering in Multidocument Summarization. Journal of
Artificial Intelligence Research (JAIR), Vol.17, pp.
35–55.
Regina Barzilay and Lillian Lee. 2004. Catching the
Drift: Probabilistic Content Models, with Applica-
tions to Generation and Summarization. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pp. 113–120.
Regina Barzilay and Mirella Lapata. 2005. Modeling
Local Coherence: An Entity-based Approach. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pp.
141–148.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A. Reis and Jeff Rey-
nar. 2008. Building a Sentiment Summarizer for Lo-
cal Service Reviews. WWW Workshop NLP Chal-
lenges in the Information Explosion Era (NLPIX).
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval (SIGIR), pp. 335–356.
Giuseppe Carenini, Raymond Ng and Adam Pauls.
2006. Multi-Document Summarization of Evalua-
tive Text. In Proceedings of the 11th European
Chapter of the Association for Computational Lin-
guistics (EACL), pp. 305–312.
Giuseppe Carenini and Jackie Chi Kit Cheung. 2008.
Extractive vs. NLG-based Abstractive Summariza-
tion of Evaluative Text: The Effect of Corpus Con-
troversiality. In Proceedings of the 5th International
Natural Language Generation Conference (INLG),
pp. 33–41.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the 2002 Conference on Empirical Methods on
Natural Language Processing (EMNLP), pp. 1–8.
Michael Held and Richard M. Karp. 1962. A dy-
namic programming approach to sequencing prob-
lems. Journal of the Society for Industrial and Ap-
plied Mathematics (SIAM), Vol.10, No.1, pp. 196–
210.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD), pp. 168–
177.
</reference>
<page confidence="0.989112">
329
</page>
<reference confidence="0.999829825">
Kenji Imamura, Genichiro Kikui and Norihito Yasuda.
2007. Japanese Dependency Parsing Using Sequen-
tial Labeling for Semi-spoken Language. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL) Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pp. 225–228.
Mirella Lapata. 2003. Probabilistic Text Structuring:
Experiments with Sentence Ordering. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 545–552.
Kevin Lerman, Sasha Blair-Goldensohn and Ryan Mc-
Donald. 2009. Sentiment Summarization: Evalu-
ating and Learning User Preferences. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pp. 514–522.
Kevin Lerman and Ryan McDonald. 2009. Contrastive
Summarization: An Experiment with Consumer Re-
views. In Proceedings of Human Language Tech-
nologies: the 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT), Companion Vol-
ume: Short Papers, pp. 113–116.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
pp. 74–81.
Jun Suzuki, Erik McDermott and Hideki Isozaki. 2006.
Training Conditional Random Fields with Multi-
variate Evaluation Measures. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the ACL
(COLING-ACL), pp. 217–224.
Ivan Titov and Ryan McDonald. 2008. A Joint Model
of Text and Aspect Ratings for Sentiment Summa-
rization. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT),
pp. 308–316.
</reference>
<page confidence="0.998376">
330
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.846759">
<title confidence="0.9989915">Optimizing Informativeness and Readability for Sentiment Summarization</title>
<author confidence="0.985728">Takaaki Hasegawa Nishikawa</author>
<author confidence="0.985728">Yoshihiro Matsuo Kikui</author>
<affiliation confidence="0.998458">NTT Cyber Space Laboratories, NTT Corporation</affiliation>
<address confidence="0.86571">1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan</address>
<abstract confidence="0.999606470588235">We propose a novel algorithm for sentiment summarization that takes account of informativeness and readability, simultaneously. Our algorithm generates a summary by selecting and ordering sentences taken from multiple review texts according to two scores that represent the informativeness and readability of the sentence order. The informativeness score is defined by the number of sentiment expressions and the readability score is learned from the target corpus. We evaluate our method by summarizing reviews on restaurants. Our method outperforms an existing algorithm as indicated by its ROUGE score and human readability experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hisako Asano</author>
<author>Toru Hirano</author>
<author>Nozomi Kobayashi</author>
<author>Yoshihiro Matsuo</author>
</authors>
<title>Subjective Information Indexing Technology Analyzing Word-of-mouth Content on the Web. NTT Technical Review,</title>
<date>2008</date>
<location>Vol.6, No.9.</location>
<contexts>
<context position="8115" citStr="Asano et al., 2008" startWordPosition="1263" endWordPosition="1266">tences, Barzilay et al. (2002) used the publication dates of documents to catch temporally-ordered events, but this approach is not really suitable for our goal because reviews focus on entities rather than events. Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. If the corpus consists of reviews, it is expected that this approach would be effective for sentiment summarization. Therefore, we adopt and improve Lapata’s approach to order sentences. We define the &apos;Since we aim to summarize Japanese reviews, we utilize Japanese sentiment lexicon (Asano et al., 2008). However, our method is, except for sentiment extraction, language independent. 326 readability score as follows: n Read(S) = ∑ wTφ(si, si+1) (3) i=0 where, given two adjacent sentences si and si+1, wTφ(si, si+1), which measures the connectivity of the two sentences, is the inner product of w and φ(si, si+1), w is a parameter vector and φ(si, si+1) is a feature vector of the two sentences. That is, the readability score of sentence sequence S is the sum of the connectivity of all adjacent sentences in the sequence. As the features, Lapata (2003) proposed the Cartesian product of content words</context>
</contexts>
<marker>Asano, Hirano, Kobayashi, Matsuo, 2008</marker>
<rawString>Hisako Asano, Toru Hirano, Nozomi Kobayashi and Yoshihiro Matsuo. 2008. Subjective Information Indexing Technology Analyzing Word-of-mouth Content on the Web. NTT Technical Review, Vol.6, No.9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Inferring Strategies for Sentence Ordering in Multidocument Summarization.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<volume>17</volume>
<pages>35--55</pages>
<contexts>
<context position="2604" citStr="Barzilay et al., 2002" startWordPosition="377" endWordPosition="380">harts This restaurant offers customers delicious foods and a relaxing atmosphere. The staff are very friendly but the price is a little high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on producing textual summaries. One crucial weakness of existing text-oriented summarizers is the poor readability of their results. Good readability is essential because readability strongly affects text comprehension (Barzilay et al., 2002). To achieve readable summaries, the extracted sentences must be appropriately ordered (Barzilay et al., 2002; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005). Barzilay et al. (2002) proposed an algorithm for ordering sentences according to the dates of the publications from which the sentences were extracted. Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. Both methods delink sentence extraction from sentence ordering, so a sentence can be extracted that cannot be ordered naturally with the other extrac</context>
<context position="7526" citStr="Barzilay et al. (2002)" startWordPosition="1167" endWordPosition="1170">ntence, foods and delicious, atmosphere and relaxing are extracted as aspect-sentiment expression pairs. Finally extracted sentiment expressions are converted to polarities, we acquire the set of sentiments from sentences, for example, ( foods, 1) and ( atmosphere, 1). Note that since our method relies on only sentiment lexicon, extractable aspects are unlimited. 2.2 Readability Score Readability consists of various elements such as conciseness, coherence, and grammar. Since it is difficult to model all of them, we approximate readability as the natural order of sentences. To order sentences, Barzilay et al. (2002) used the publication dates of documents to catch temporally-ordered events, but this approach is not really suitable for our goal because reviews focus on entities rather than events. Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. If the corpus consists of reviews, it is expected that this approach would be effective for sentiment summarization. Therefore, we adopt and improve Lapata’s approach to order sentences. We define the &apos;Since we aim to summarize Japanese reviews, we utilize Japanese sentiment lexicon (Asano et al., 2008). However, </context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Regina Barzilay, Noemie Elhadad and Kathleen McKeown. 2002. Inferring Strategies for Sentence Ordering in Multidocument Summarization. Journal of Artificial Intelligence Research (JAIR), Vol.17, pp. 35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>113--120</pages>
<contexts>
<context position="2751" citStr="Barzilay and Lee, 2004" startWordPosition="398" endWordPosition="401">e 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on producing textual summaries. One crucial weakness of existing text-oriented summarizers is the poor readability of their results. Good readability is essential because readability strongly affects text comprehension (Barzilay et al., 2002). To achieve readable summaries, the extracted sentences must be appropriately ordered (Barzilay et al., 2002; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005). Barzilay et al. (2002) proposed an algorithm for ordering sentences according to the dates of the publications from which the sentences were extracted. Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. Both methods delink sentence extraction from sentence ordering, so a sentence can be extracted that cannot be ordered naturally with the other extracted sentences. To solve this problem, we propose an algorithm that chooses sentences and orders them simultaneously in such a way that the ordered </context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pp. 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling Local Coherence: An Entity-based Approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>141--148</pages>
<contexts>
<context position="2779" citStr="Barzilay and Lapata, 2005" startWordPosition="402" endWordPosition="405">are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on producing textual summaries. One crucial weakness of existing text-oriented summarizers is the poor readability of their results. Good readability is essential because readability strongly affects text comprehension (Barzilay et al., 2002). To achieve readable summaries, the extracted sentences must be appropriately ordered (Barzilay et al., 2002; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005). Barzilay et al. (2002) proposed an algorithm for ordering sentences according to the dates of the publications from which the sentences were extracted. Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. Both methods delink sentence extraction from sentence ordering, so a sentence can be extracted that cannot be ordered naturally with the other extracted sentences. To solve this problem, we propose an algorithm that chooses sentences and orders them simultaneously in such a way that the ordered sentences maximize the score</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling Local Coherence: An Entity-based Approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pp. 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
<author>Tyler Neylon</author>
<author>George A Reis</author>
<author>Jeff Reynar</author>
</authors>
<title>Building a Sentiment Summarizer for Local Service Reviews.</title>
<date>2008</date>
<booktitle>WWW Workshop NLP Challenges in the Information Explosion Era (NLPIX).</booktitle>
<contexts>
<context position="1661" citStr="Blair-Goldensohn et al., 2008" startWordPosition="235" endWordPosition="238">views describing the sentiments of customers about products and services. These reviews can help the user reach purchasing decisions and guide companies’ business activities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts This restaurant offers customers delicious foods and a relaxing atmosphere. The staff are very friendly but the price is a little high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contra</context>
</contexts>
<marker>Blair-Goldensohn, Hannan, McDonald, Neylon, Reis, Reynar, 2008</marker>
<rawString>Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDonald, Tyler Neylon, George A. Reis and Jeff Reynar. 2008. Building a Sentiment Summarizer for Local Service Reviews. WWW Workshop NLP Challenges in the Information Explosion Era (NLPIX).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR),</booktitle>
<pages>335--356</pages>
<contexts>
<context position="13385" citStr="Carbonell and Goldstein, 1998" startWordPosition="2165" endWordPosition="2169">ries, so the summarization rate was about 6%. We used CRFsbased Japanese dependency parser (Imamura et al., 2007) and named entity recognizer (Suzuki et al., 2006) for sentiment extraction and constructing feature vectors for readability score, respectively. 3.1 ROUGE We used ROUGE (Lin, 2004) for evaluating the content of summaries. We chose ROUGE-2, ROUGE-SU4 and ROUGE-SU9. We prepared four reference summaries for each document set. To evaluate the effects of the informativeness score, the readability score and the optimization, we compared the following five methods. Baseline: employs MMR (Carbonell and Goldstein, 1998). We designed the score of a sentence as term frequencies of the content words in a document set. Method1: uses optimization without the informativeness score or readability score. It also used term frequencies to score sentences. Method2: uses the informativeness score and optimization without the readability score. Method3: the proposed method. Following Equation 1, the summarizer searches for a sequence with high informativeness and readability score. The parameter vector w was trained on the same 2,940 reviews in 5-fold cross validation fashion. A was set to 6,000 using a development set. </context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), pp. 335–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Adam Pauls</author>
</authors>
<title>Multi-Document Summarization of Evaluative Text.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>305--312</pages>
<contexts>
<context position="1768" citStr="Carenini et al., 2006" startWordPosition="251" endWordPosition="254">chasing decisions and guide companies’ business activities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts This restaurant offers customers delicious foods and a relaxing atmosphere. The staff are very friendly but the price is a little high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on pro</context>
<context position="15105" citStr="Carenini et al. (2006)" startWordPosition="2436" endWordPosition="2439">cores. Comparing Method2 to Method3, ROUGE scores of the latter were higher for all criteria. It is interesting that the readability criterion also improved ROUGE scores. We also evaluated our method in terms of sentiments. We extracted sentiments from the summaries using the above sentiment extractor, and averaged the unique sentiment numbers. Table 2 shows the results. The references (Human) have fewer sentiments than the summaries generated by our method. In other words, the references included almost as many other sentences (e.g. reasons for the sentiments) as those expressing sentiments. Carenini et al. (2006) pointed out that readers wanted “detailed information” in summaries, and the reasons are one of such piece of information. Including them in summaries would greatly improve summarizer appeal. 3.2 Readability Readability was evaluated by human judges. Three different summarizers generated summaries for each document set. Ten judges evaluated the thirty summaries for each. Before the evaluation the judges read evaluation criteria and gave points to summaries using a five-point scale. The judges weren’t informed of which method generated which summary. We compared three methods; Ordering sentenc</context>
</contexts>
<marker>Carenini, Ng, Pauls, 2006</marker>
<rawString>Giuseppe Carenini, Raymond Ng and Adam Pauls. 2006. Multi-Document Summarization of Evaluative Text. In Proceedings of the 11th European Chapter of the Association for Computational Linguistics (EACL), pp. 305–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Jackie Chi Kit Cheung</author>
</authors>
<title>Extractive vs. NLG-based Abstractive Summarization of Evaluative Text: The Effect of Corpus Controversiality.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Natural Language Generation Conference (INLG),</booktitle>
<pages>33--41</pages>
<contexts>
<context position="1795" citStr="Carenini and Cheung, 2008" startWordPosition="255" endWordPosition="258">uide companies’ business activities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts This restaurant offers customers delicious foods and a relaxing atmosphere. The staff are very friendly but the price is a little high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on producing textual summaries. O</context>
</contexts>
<marker>Carenini, Cheung, 2008</marker>
<rawString>Giuseppe Carenini and Jackie Chi Kit Cheung. 2008. Extractive vs. NLG-based Abstractive Summarization of Evaluative Text: The Effect of Corpus Controversiality. In Proceedings of the 5th International Natural Language Generation Conference (INLG), pp. 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods on Natural Language Processing (EMNLP),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="9490" citStr="Collins, 2002" startWordPosition="1505" endWordPosition="1506">tly contains named entities indicating location. We aim to reproduce this characteristic in the ordering. We also define feature vector b(S) of the entire sequence S = (s0, s1, ... , sn, sn+1) as follows: n b(S) = ∑ φ(si,si+1) (4) i=0 Therefore, the score of sequence S is wTb(S). Given a training set, if a trained parameter w assigns a score wTb(S+) to an correct order S+ that is higher than a score wTb(S−) to an incorrect order S−, it is expected that the trained parameter will give higher score to naturally ordered sentences than to unnaturally ordered sentences. We use Averaged Perceptron (Collins, 2002) to find w. Averaged Perceptron requires an argmax operation for parameter estimation. Since we attempt to order a set of sentences, the operation is regarded as solving the Traveling Salesman Problem; that is, we locate the path that offers maximum score through all n sentences as s0 and sn+1 are starting and ending points, respectively. Thus the operation is NP-hard and it is difficult to find the global optimal solution. To alleviate this, we find an approximate solution by adopting the dynamic programming technique of the Held and Karp Algorithm (Held and Karp, 1962) and beam search. We sh</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the 2002 Conference on Empirical Methods on Natural Language Processing (EMNLP), pp. 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Held</author>
<author>Richard M Karp</author>
</authors>
<title>A dynamic programming approach to sequencing problems.</title>
<date>1962</date>
<journal>Journal of the Society for Industrial and Applied Mathematics (SIAM),</journal>
<volume>10</volume>
<pages>196--210</pages>
<contexts>
<context position="10067" citStr="Held and Karp, 1962" startWordPosition="1601" endWordPosition="1604"> We use Averaged Perceptron (Collins, 2002) to find w. Averaged Perceptron requires an argmax operation for parameter estimation. Since we attempt to order a set of sentences, the operation is regarded as solving the Traveling Salesman Problem; that is, we locate the path that offers maximum score through all n sentences as s0 and sn+1 are starting and ending points, respectively. Thus the operation is NP-hard and it is difficult to find the global optimal solution. To alleviate this, we find an approximate solution by adopting the dynamic programming technique of the Held and Karp Algorithm (Held and Karp, 1962) and beam search. We show the search procedure in Figure 2. S indicates intended sentences and M is a distance matrix of the readability scores of adjacent sentence pairs. Hi(C, j) indicates the score of the hypothesis that has covered the set of i sentences C and has the sentence j at the end of the path, Sentences: S = {s1, ... , sn} Distance matrix: M = [ai,j]i=0...n+1,j=0...n+1 1: H0({s0}, s0) = 0 2: for i : 0 ... n − 1 3: for j : 1 ... n 4: foreach Hi(C\{j}, k) E b 5: Hi+1(C, j) = maxHi(C\1j1,k)Eb Hi(C\{j}, k) 6: +Mk,j 7: H* = maxHn(C,k) Hn(C, k) + Mk,n+1 Figure 2: Held and Karp Algorithm</context>
</contexts>
<marker>Held, Karp, 1962</marker>
<rawString>Michael Held and Richard M. Karp. 1962. A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics (SIAM), Vol.10, No.1, pp. 196– 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and Summarizing Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>168--177</pages>
<contexts>
<context position="1630" citStr="Hu and Liu, 2004" startWordPosition="231" endWordPosition="234">ssive number of reviews describing the sentiments of customers about products and services. These reviews can help the user reach purchasing decisions and guide companies’ business activities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts This restaurant offers customers delicious foods and a relaxing atmosphere. The staff are very friendly but the price is a little high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of informa</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and Summarizing Customer Reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 168– 177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
<author>Genichiro Kikui</author>
<author>Norihito Yasuda</author>
</authors>
<title>Japanese Dependency Parsing Using Sequential Labeling for Semi-spoken Language.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL) Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>225--228</pages>
<contexts>
<context position="12868" citStr="Imamura et al., 2007" startWordPosition="2089" endWordPosition="2092">E score and readability. We collected 2,940 reviews of 100 restaurants from a website. The 327 R-2 R-SU4 R-SU9 Baseline 0.089 0.068 0.062 Method1 0.157 0.096 0.089 Method2 0.172 0.107 0.098 Method3 0.180 0.110 0.101 Human 0.258 0.143 0.131 Table 1: Automatic ROUGE evaluation. Numbers Baseline 1.76 Method1 4.32 Method2 10.41 Method3 10.18 Human 4.75 Table 2: Unique sentiment numbers. average size of each document set (corresponds to one restaurant) was 5,343 bytes. We attempted to generate 300 byte summaries, so the summarization rate was about 6%. We used CRFsbased Japanese dependency parser (Imamura et al., 2007) and named entity recognizer (Suzuki et al., 2006) for sentiment extraction and constructing feature vectors for readability score, respectively. 3.1 ROUGE We used ROUGE (Lin, 2004) for evaluating the content of summaries. We chose ROUGE-2, ROUGE-SU4 and ROUGE-SU9. We prepared four reference summaries for each document set. To evaluate the effects of the informativeness score, the readability score and the optimization, we compared the following five methods. Baseline: employs MMR (Carbonell and Goldstein, 1998). We designed the score of a sentence as term frequencies of the content words in a</context>
</contexts>
<marker>Imamura, Kikui, Yasuda, 2007</marker>
<rawString>Kenji Imamura, Genichiro Kikui and Norihito Yasuda. 2007. Japanese Dependency Parsing Using Sequential Labeling for Semi-spoken Language. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL) Companion Volume Proceedings of the Demo and Poster Sessions, pp. 225–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic Text Structuring: Experiments with Sentence Ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>545--552</pages>
<contexts>
<context position="2727" citStr="Lapata, 2003" startWordPosition="396" endWordPosition="397">le high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on producing textual summaries. One crucial weakness of existing text-oriented summarizers is the poor readability of their results. Good readability is essential because readability strongly affects text comprehension (Barzilay et al., 2002). To achieve readable summaries, the extracted sentences must be appropriately ordered (Barzilay et al., 2002; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005). Barzilay et al. (2002) proposed an algorithm for ordering sentences according to the dates of the publications from which the sentences were extracted. Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. Both methods delink sentence extraction from sentence ordering, so a sentence can be extracted that cannot be ordered naturally with the other extracted sentences. To solve this problem, we propose an algorithm that chooses sentences and orders them simultaneously in such</context>
<context position="7724" citStr="Lapata (2003)" startWordPosition="1199" endWordPosition="1200">from sentences, for example, ( foods, 1) and ( atmosphere, 1). Note that since our method relies on only sentiment lexicon, extractable aspects are unlimited. 2.2 Readability Score Readability consists of various elements such as conciseness, coherence, and grammar. Since it is difficult to model all of them, we approximate readability as the natural order of sentences. To order sentences, Barzilay et al. (2002) used the publication dates of documents to catch temporally-ordered events, but this approach is not really suitable for our goal because reviews focus on entities rather than events. Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. If the corpus consists of reviews, it is expected that this approach would be effective for sentiment summarization. Therefore, we adopt and improve Lapata’s approach to order sentences. We define the &apos;Since we aim to summarize Japanese reviews, we utilize Japanese sentiment lexicon (Asano et al., 2008). However, our method is, except for sentiment extraction, language independent. 326 readability score as follows: n Read(S) = ∑ wTφ(si, si+1) (3) i=0 where, given two adjacent sentences si and si+1, wTφ(si, s</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic Text Structuring: Experiments with Sentence Ordering. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pp. 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lerman</author>
<author>Sasha Blair-Goldensohn</author>
<author>Ryan McDonald</author>
</authors>
<title>Sentiment Summarization: Evaluating and Learning User Preferences.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>514--522</pages>
<contexts>
<context position="1816" citStr="Lerman et al., 2009" startWordPosition="259" endWordPosition="262">tivities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts This restaurant offers customers delicious foods and a relaxing atmosphere. The staff are very friendly but the price is a little high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on producing textual summaries. One crucial weakness o</context>
</contexts>
<marker>Lerman, Blair-Goldensohn, McDonald, 2009</marker>
<rawString>Kevin Lerman, Sasha Blair-Goldensohn and Ryan McDonald. 2009. Sentiment Summarization: Evaluating and Learning User Preferences. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pp. 514–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lerman</author>
<author>Ryan McDonald</author>
</authors>
<title>Contrastive Summarization: An Experiment with Consumer Reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), Companion Volume: Short Papers,</booktitle>
<pages>113--116</pages>
<contexts>
<context position="1844" citStr="Lerman and McDonald, 2009" startWordPosition="263" endWordPosition="266">uct improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts This restaurant offers customers delicious foods and a relaxing atmosphere. The staff are very friendly but the price is a little high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on producing textual summaries. One crucial weakness of existing text-oriented sum</context>
</contexts>
<marker>Lerman, McDonald, 2009</marker>
<rawString>Kevin Lerman and Ryan McDonald. 2009. Contrastive Summarization: An Experiment with Consumer Reviews. In Proceedings of Human Language Technologies: the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), Companion Volume: Short Papers, pp. 113–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="3670" citStr="Lin, 2004" startWordPosition="544" endWordPosition="545">methods delink sentence extraction from sentence ordering, so a sentence can be extracted that cannot be ordered naturally with the other extracted sentences. To solve this problem, we propose an algorithm that chooses sentences and orders them simultaneously in such a way that the ordered sentences maximize the scores of informativeness and readability. Our algorithm efficiently searches for the best sequence of sentences by using dynamic programming and beam search. We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGE score (Lin, 2004) and subjective readability measures. As far as we know, this is the first work to 325 Proceedings of the ACL 2010 Conference Short Papers, pages 325–330, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics simultaneously achieve both informativeness and readability in the area of multi-document summarization. This paper is organized as follows: Section 2 describes our summarization method. Section 3 reports our evaluation experiments. We conclude this paper in Section 4. 2 Optimizing Sentence Sequence Formally, we define a summary S* = (s0, ss, ... , sn, sn+s) a</context>
<context position="13049" citStr="Lin, 2004" startWordPosition="2119" endWordPosition="2120">thod3 0.180 0.110 0.101 Human 0.258 0.143 0.131 Table 1: Automatic ROUGE evaluation. Numbers Baseline 1.76 Method1 4.32 Method2 10.41 Method3 10.18 Human 4.75 Table 2: Unique sentiment numbers. average size of each document set (corresponds to one restaurant) was 5,343 bytes. We attempted to generate 300 byte summaries, so the summarization rate was about 6%. We used CRFsbased Japanese dependency parser (Imamura et al., 2007) and named entity recognizer (Suzuki et al., 2006) for sentiment extraction and constructing feature vectors for readability score, respectively. 3.1 ROUGE We used ROUGE (Lin, 2004) for evaluating the content of summaries. We chose ROUGE-2, ROUGE-SU4 and ROUGE-SU9. We prepared four reference summaries for each document set. To evaluate the effects of the informativeness score, the readability score and the optimization, we compared the following five methods. Baseline: employs MMR (Carbonell and Goldstein, 1998). We designed the score of a sentence as term frequencies of the content words in a document set. Method1: uses optimization without the informativeness score or readability score. It also used term frequencies to score sentences. Method2: uses the informativeness</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of the Workshop on Text Summarization Branches Out, pp. 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Erik McDermott</author>
<author>Hideki Isozaki</author>
</authors>
<title>Training Conditional Random Fields with Multivariate Evaluation Measures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL (COLING-ACL),</booktitle>
<pages>217--224</pages>
<contexts>
<context position="12918" citStr="Suzuki et al., 2006" startWordPosition="2097" endWordPosition="2100"> of 100 restaurants from a website. The 327 R-2 R-SU4 R-SU9 Baseline 0.089 0.068 0.062 Method1 0.157 0.096 0.089 Method2 0.172 0.107 0.098 Method3 0.180 0.110 0.101 Human 0.258 0.143 0.131 Table 1: Automatic ROUGE evaluation. Numbers Baseline 1.76 Method1 4.32 Method2 10.41 Method3 10.18 Human 4.75 Table 2: Unique sentiment numbers. average size of each document set (corresponds to one restaurant) was 5,343 bytes. We attempted to generate 300 byte summaries, so the summarization rate was about 6%. We used CRFsbased Japanese dependency parser (Imamura et al., 2007) and named entity recognizer (Suzuki et al., 2006) for sentiment extraction and constructing feature vectors for readability score, respectively. 3.1 ROUGE We used ROUGE (Lin, 2004) for evaluating the content of summaries. We chose ROUGE-2, ROUGE-SU4 and ROUGE-SU9. We prepared four reference summaries for each document set. To evaluate the effects of the informativeness score, the readability score and the optimization, we compared the following five methods. Baseline: employs MMR (Carbonell and Goldstein, 1998). We designed the score of a sentence as term frequencies of the content words in a document set. Method1: uses optimization without </context>
</contexts>
<marker>Suzuki, McDermott, Isozaki, 2006</marker>
<rawString>Jun Suzuki, Erik McDermott and Hideki Isozaki. 2006. Training Conditional Random Fields with Multivariate Evaluation Measures. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL (COLING-ACL), pp. 217–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A Joint Model of Text and Aspect Ratings for Sentiment Summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),</booktitle>
<pages>308--316</pages>
<contexts>
<context position="1688" citStr="Titov and McDonald, 2008" startWordPosition="239" endWordPosition="242"> of customers about products and services. These reviews can help the user reach purchasing decisions and guide companies’ business activities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts This restaurant offers customers delicious foods and a relaxing atmosphere. The staff are very friendly but the price is a little high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present comple</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. A Joint Model of Text and Aspect Ratings for Sentiment Summarization. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pp. 308–316.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>