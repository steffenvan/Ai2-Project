<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.99911">
Evaluation of Phrase-Representation Summarization
based on Information Retrieval Task
</title>
<author confidence="0.974133">
Mamiko OKA Yoshihiro UEDA
</author>
<affiliation confidence="0.965662">
Industry Solutions Company,
Fuji Xerox Co., Ltd.
</affiliation>
<address confidence="0.978887">
430 Sakai, Nakai-machi, Ashigarakami-gun, Kanagawa, Japan, 259-0157
</address>
<email confidence="0.997854">
oka.mamiko@fujixerox.co.jp Ueda.Yoshihiro@fujixerox.co.jp
</email>
<sectionHeader confidence="0.993863" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962583333333">
We have developed an improved task-based
evaluation method of summarization, the
accuracy of which is increased by specifying
the details of the task including background
stories, and by assigning ten subjects per
summary sample. The method also serves
precision/recall pairs for a variety of situa-
tions by introducing multiple levels of
relevance assessment. The method is applied
to prove phrase-represented summary is
most effective to select relevant documents
from information retrieval results.
</bodyText>
<sectionHeader confidence="0.961456" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999933146341463">
Summaries are often used to select relevant
documents from information retrieval results.
The goal of summarization for such &amp;quot;indicative&amp;quot;
use is to serve fast and accurate judgement. We
have developed the concept of the &amp;quot;at-a-glance&amp;quot;
summary, and its realization in the Japanese
language - &amp;quot;phrase-representation surnmariza-
tioh&amp;quot; - to achieve this goal (Ueda, et al. 2000).
We have conducted an evaluation experiment to
verify the effectiveness of this summarization
method.
There are two strategies for evaluating
summarization systems: intrinsic and extrinsic
(Jing, et al. 1998). Intrinsic methods measure a
system&apos;s quality mainly by comparing the
system&apos;s output with an &amp;quot;ideal&amp;quot; summary.
Extrinsic methods measure a system&apos;s perfor-
mance in a particular task. The aim of the
phrase-representation summarization method is
fast and accurate judgement in selecting
documents in information retrieval. Thus, we
adopted a task-based method to evaluate
whether the goal was achieved. Task-based
evaluation has recently drawn the attention in
the summarization field, because the assumption
&apos; that there is only one &amp;quot;ideal&amp;quot; summary is
considered to be incorrect, and some experi-
ments on information retrieval were reported
(Jing, et al. 1998) (Mani, et al. 1998) (Mochizu-
ki and Okunura 1999). However, there is no
standard evaluation method, and we consider
that there are some shortcomings in the existing
methods. Thus, we have developed an improved
evaluation method and carried out a relatively
large experiment.
In this paper, we first give an overview of the
phrase-representation summarization method.
We then consider the evaluation method and
show the result of an experiment based on the
improved method to demonstrate the effective-
ness of phrase-representation summarization.
</bodyText>
<sectionHeader confidence="0.989545" genericHeader="method">
1 Phrase-Representation Summarization
</sectionHeader>
<bodyText confidence="0.999872">
Most automatic summarization systems adopt
the &amp;quot;sentence extraction&amp;quot; method,. which gives a
score to every sentence based on such charac-
teristics as the frequency of a word or the
position where it appears, and selects sentences
with high scores. In such a way, long and
complex sentences tend to be extracted.
However, a long and complex sentence is
difficult to read and understand, and therefore it
is not a suitable unit to compose a summary for
use in selecting documents.
To avoid the burden of reading such long and
complex sentences, we have developed the
phrase-representation summarization method,
which represents the outline of a document by a
series of short and simple expressions
(&amp;quot;phrases&amp;quot;) that contain key concepts. We use
the word &amp;quot;phrase&amp;quot; to represent&amp;quot; the simplicity
</bodyText>
<page confidence="0.997897">
59
</page>
<bodyText confidence="0.974613666666667">
characteristic&apos; in a word.
The phrase-represented summary has the
following characteristics.
</bodyText>
<listItem confidence="0.852693">
(1) At-a-glance comprehension
Because each unit is short and simple, the user
is able to grasp the meaning at a glance.
(2) Adequate informativeness
</listItem>
<bodyText confidence="0.938123">
Unlike extracted sentences, phrases created by
this method are not accompanied by informa-
tion unnecessary for relevance judgement.
</bodyText>
<listItem confidence="0.973523">
(3) Wide coverage of topics
</listItem>
<bodyText confidence="0.977276">
Units composing a summary are relatively
short, and point various positions of the
original text. Therefore, even a generic
summary includes various topics written in a
document.
A phrase-represented summary is generated
as follows.
I. Syntactic analysis to extract the relation-
ships between words
</bodyText>
<listItem confidence="0.997802">
2. Selection of an important relation (two
word sequences connected by an arc) as a
&amp;quot;core&amp;quot;
3. Addition of relations necessary for the
unity of the phrase&apos;s meaning (e.g., essen-
tial cases)
4. Generation of the surface phrase from the
selected relations
</listItem>
<bodyText confidence="0.995136142857143">
An important relation is selected by
considering both the importance of a word and
that of a relation between words. For example,
predicate-argument relations are considered
important and noun-modifier relations are given
low importance scores. Steps [2] to [4] are
repeated until specified amount of phrases are.
obtained. Before selecting a new &amp;quot;core,&amp;quot; the
seores for the already selected words are
decreased to suppress overuse of the same
words.
Fig. 1 shows a sample summary created from
a news article2 put on WWW. The underlined
words constitute the core relation of each phrase.
</bodyText>
<footnote confidence="0.86875925">
The word &amp;quot;phrase&amp;quot; as used here is not used in the
linguistic sense, but an expression for &amp;quot;short&amp;quot; and
&amp;quot;simple.&amp;quot; In Japanese, there is no rigid linguistic
distinction between a &amp;quot;phrase&amp;quot; and a &amp;quot;clause.&amp;quot;
2 The original text in Japanese and its outline in
English can be seen in the following URL.
httpl/www fujixerox.co jp/release/2000/0224._purcha
,se html (in Japanese)
</footnote>
<bodyText confidence="0.972824909090909">
... acquire chemical toner business3
Fuji Xerox ... acquires chemical toner
business of Nippon Carbide Industries Co.,
Inc,
... new chemical toner that contributes to
reduce cost in laser printers and to lower
energy consumption ...
...
strengthen... supplies business ...
manufacturing facilities of Hayatsuki Plant, ...
mifum... each particle ...
</bodyText>
<sectionHeader confidence="0.78136375" genericHeader="method">
Fig.1: A sample summary
2 Evaluation Method
2.1 Summarization Methods to be
Compared
</sectionHeader>
<bodyText confidence="0.999693857142857">
In this experiment, we compare the effectiveness
of phrase-represented summaries to summaries
created by other commonly used summarization
methods. From the viewpoint of the phrase-
represented summary, we focus the comparison
of the units that constitute summaries. The units
to be compared with phrases are sentences
(created by the sentence extraction method) and
words (by the keyword enumeration method).
We also compare &amp;quot;leading fixed-length
characters,&amp;quot; which are often used as substitutes
for summaries by WWW search engines. The
generation method for each summary is
described as follows.
</bodyText>
<listItem confidence="0.926857769230769">
(A) Leading fixed-length characters: extract
the first 80 characters of the document
body.
(B) Sentence extraction summarization: selectâ€¢
important sentences from a document.
The importance score of each sentence is
calculated from the simple sum of the im-
portance scores of the words in a sentence
(Zechner 1996).
(C) Phrase-representation summarization:
described in Chapter 1.
(D) Keyword enumeration summarization: list
up important words or compound nouns.
</listItem>
<footnote confidence="0.9991062">
http://www.fujixerox.co.jp/headline/2000/0308_nton
erbi7.html (in English)
3 This phrase lacks the subject because the original
sentence lacks it. Cases are usually omitted in
Japanese if they can be easily inferred.
</footnote>
<page confidence="0.998924">
60
</page>
<bodyText confidence="0.999106222222222">
In (B), (C), and (D), the same method of
calculating the importance scores of words is
used in common, and lengths of summaries are
kept to be 60 to 80 characters.
As you can see each summary is generic, Le.
not created for any specific queries. Because the
phrase-representation summarization method is
applied to Japanese, we examine the effective-
ness of these four methods in Japanese.
</bodyText>
<subsectionHeader confidence="0.995809">
2.2 Previous Work
</subsectionHeader>
<bodyText confidence="0.9997203125">
The best-known example of task-based
evaluation on information retrieval is the ad hoc
task in the TIPSTER Text Summarization
Evaluation Conference (SUMMAC) (Mani, et al.
1998). Hand (1997) details the proposed task-
based evaluation under TIPSTER. Jing, et al.
(1998) describe how various parameters affect
the evaluation result through a relatively large
task-based experiment. Evaluation conferences
like SUMMAC are not yet held for Japanese
summarization systems4. Mochizuki and
Okumura (1999) applied , the SUMMAC
methodology to Japanese summarization
methods for the first time. Most previous
experiments are concerned with SUMMAC,
accordingly the methods resemble each other.
</bodyText>
<subsectionHeader confidence="0.999856">
2.3 Framework of Evaluation
</subsectionHeader>
<bodyText confidence="0.99980275">
The framework of task-based evaluation on
information retrieval is shown in Fig. 2.
Task-based evaluation in general consists of
the following three steps:
</bodyText>
<listItem confidence="0.870424307692308">
(1) Data preparation: Assume an information
need, create a query for the information
need, and prepare simulated search results
- with different types of summaries.
(2) Relevance assessment: Using the summa-
ries, human subjects assess the relevance
of the search results to the assumed in-
formation needs.
(3) Measuring performance: Measure the
accuracy of the subjects&apos; assessment by
comparing the subjects&apos; judgement with
the correct relevance. The assessment
process is also timed.
</listItem>
<footnote confidence="0.95490875">
4 It is planning to be held in 2000. Further
information is in the following URL.
http://vvww.rd nacsis.acjp/-ntcadrn/workshop/ann2p-
en. htm I
</footnote>
<figure confidence="0.312105">
(3) Measuring Performance
</figure>
<figureCaption confidence="0.535056">
Fig.2: Framework of Task-Based Evaluation
</figureCaption>
<bodyText confidence="0.999977375">
We designed our evaluation method through
detailed examination of previous work. The
consideration points are compared to the
SUMMAC ad hoc task (Table 1). A section
number will be found in the &amp;quot;*&amp;quot; column if we
made an improvement. Details will be discussed
in the section indicated by the number in the
next chapter.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="method">
3 Improvements
</sectionHeader>
<subsectionHeader confidence="0.999723">
3.1 Description of Questions
</subsectionHeader>
<bodyText confidence="0.99827">
To assess the relevance accurately, the situation
of information retrieval should be realistic
enough for the subjects to feel as if they really
want to know about a given question. The
previous experiments gave only a short
description of a topic. We consider it is not
sufficiently specific and the interpretation of a
question must varied with the subjects.
We selected two topics (&amp;quot;moon cake&amp;quot; and
&amp;quot;journey in Malay. Peninsula&amp;quot;) and assumed
three questions. To indicate to the subjects, we
set detailed situation including the motivation to
know about that or the use of the information
obtained for each question. This method satisfies
the restriction &amp;quot;to limit the variation in
assessment between readers&amp;quot; in the MLUCE
Protocol (Minel, et al. 1997).
</bodyText>
<figure confidence="0.99863125">
(1) Data Preparation
Sspjne4-1
rifirmation,
Docurnetits
sawE _ â€¢
Relevant:
Relevant
Irrelevant
at
R Result
Time !I
, Accuracy
</figure>
<page confidence="0.993531">
61
</page>
<bodyText confidence="0.998034423076923">
For each topic, ten documents are selected
from search results by major WWW search
engines, so that more than five relevant
documents are included for each question. The
topics, the outline of the questions, the queries
for WWW search, and the number of relevant
documents are shown in Table 2. The descrip-
tion of Question-a2 that was given to the
subjects is shown in Fig. 3.
One day just after the mid-autumn festival, my
colleague Mr. A brought some moon cakes to
the office. He said that one of his Chinese
friends had given them to him. They looked so
new to us that we shared and ate them at a
coffee break. Chinese eat moon cakes at the
mid-autumn festival while Japanese have
dumplings then. Someone asked a question
why Chinese ate moon cakes, to-which nobody
gave the answer. Some cakes tasted sweet as
we expected; some were stuffed with salty
fillings like roasted pork. Ms. B said that there
were over fifty kinds of filling. Her story made
me think of a question:
What kinds of filling are there for moon
cakes sold at the mid-autumn festival in
Chinese society?
</bodyText>
<figureCaption confidence="0.952035">
Fig. 3: An example of question (Question-a2)
</figureCaption>
<subsectionHeader confidence="0.997491">
3.2 Number of Subjects per Summary
Sample
</subsectionHeader>
<bodyText confidence="0.999999125">
In the previous experiments, one to three
subjects were assigned to each summary sample.
Because the judgement must vary with the
subjects even if a detailed situation is given, we
assigned ten subjects per summary sample to
reduce the influence of each person&apos;s assessment.
The only requirement for subjects is that they
should be familiar with WWW search process.
</bodyText>
<subsectionHeader confidence="0.998364">
3.3 Relevance Levels
</subsectionHeader>
<bodyText confidence="0.921563736842105">
In the previous experiments, a subject reads a
summary and judges whether it is relevant or
irrelevant. However, a summary sometimes does
not give enough information for relevance
judgement. In actual information retrieval
situations, selecting criteria vary depending on
the question, the motivation, and other
circumstances. We will not examine dubious
documents if sufficient information is obtained
or we do not have sufficient time, and we will
examine dubious documents when an exhaustive
survey is required. Thus, here we introduce four
relevance levels LO to L3 to simulate various
cases in the experiment. L3, L2, and LI are
considered relevant, the confidence becomes
lower in order. To reduce the variance of
interpretation by subjects, we define each level
as follows.
L3: The answer to the given question is found
,in a summary.
L2: A clue to the answer is found in a sum-
mary.
Ll: Apparent clues are not found, but it is
probable that the answer is contained in the
whole document.
LO: A summary is not relevant to the question
at all.
If these are applied to the case of the fare of
the Malay Railway, the criteria will be
interpreted as follows.
L3: An expression like &amp;quot;the berth charge of
the second class is about RM15&amp;quot; is in a
summary.
L2: An expression like &amp;quot;I looked into the fare
of the train&amp;quot; is in a summary.
LI: A summary describes about a trip by the
Malay Railway, but the fare is not referred in
it.
</bodyText>
<subsectionHeader confidence="0.997063">
3.4 Measures of Accuracy
</subsectionHeader>
<bodyText confidence="0.9999367">
In the previous experiments, precision and recall
are used to measure accuracy. There are two
drawbacks to these measurements: (1) the
variance of the subjects&apos; assessment makes the
measure inaccurate, and (2) performance of each
summary sample is not measured.
Precision and recall are widely used to
measure information retrieval performance. In
the evaluation of summarization, they are
calculated as follows.
</bodyText>
<page confidence="0.99632">
62
</page>
<bodyText confidence="0.998263842105263">
In the previous experiments, the assessment
standard was not fixed, and some subjects
tended to make the relevant set broader and
others narrower. The variance reduces the
significance of the average precision and recall
value. Because we introduced four relevance
levels and showed the assessment criteria to the
subjects, we can assume three kinds of relevance
document sets: L3 only, L3 + L2, and L3 + L2 +
LI. The set composed only of The documents
with L3 assessment should have a high precision
score. This case represents a user wants to know
only high-probability information, for example,
the user is hurried, or just one answer is
sufficient. The set including Li documents
should get a high recall score. This case
represents a user wants to know any information
concerned with a specific question.
Precision and recall represent the perfor-
mance of a summarization method for certain
question, however they do not indicate the
reason why the method presents higher or lower
performance. To find the reasons and improve a
summarization method based on them, it is
useful to analyze quality and performance
connected together for each summary sample.
Measuring each summary&apos;s performance is
necessary for such analysis. Therefore, we
introduce the relevance score, which represents
the correspondence between the subject
judgement and the correct document relevance.
The score of each pair of subject judgement and
document relevance is shown in Table 3.
By averaging scores of all subjects for every
sample, summary&apos;s performances are compared.
By averaging scores of all summary samples for
every summarization method, method&apos;s
performances are compared.
</bodyText>
<table confidence="0.906666">
Precision = Documents that are actually
relevant in S
Documents that are assessed
relevant by a subject (S)
Documents that are assessed
relevant by a subject
Recall =
Relevant documents
</table>
<tableCaption confidence="0.990981">
Table 1: Experimental Method
</tableCaption>
<table confidence="0.999889217391304">
09.n. WI A Yad hoc, Â§ --rOuriiiet od
.
. l) Data preparation ,phase .
Document source Newspaper WWW
(TREC collection)
Question Selected from TREC topics Newly created, including 3.1
the detailed situation
Number of questions 20 3
Number of documents per 50 â€¢ 10
question
Summary type User-focused summary Generic summary
Summarization systems or 11 systems 4 methods that utilize
methods different units
&apos;T4 elqianCe assessment ipwe, ,. â€¢ . ,
Subject 21 information analysts 40 persons who usually
use WWW search
Number of subjects assigned to 1 or 2 - 10 3.2
each summary sample
Relevance levels 2 levels 4 levels 3.3
(Relevant or irrelevant) (LO, LI, L2, L3)
&apos; (3) Performance Measuring phase
Measure of accuracy I Precision and recall I Precision and recall 1 3.4
I Relevance score
</table>
<page confidence="0.989291">
63
</page>
<tableCaption confidence="0.9999515">
Table 2: Topics and Questions
Table 3: Relevance Score
</tableCaption>
<figure confidence="0.893888333333334">
Document Relevant Relevant Relevant Relevant Irrelevant Irrelevant Irrelevant Irrelevant
It et L3 L2 LI LO LO LI L2 L3
, It gftiOnt
1Scor 10 8 5 -2 2 -5 -8 -10
utline of question
Q-a 1
Q-a2
Q-b
What is the origin of the Chinese custom to have moon
cakes in the mid-autumn?
What kinds of fillings are there in moon cakes?
About the train between Singapore and Bankok:
How much does it cost?
How long does it take?
What is the difference in the equipment by the class?
(A document containing one of these information is
regarded as relevant.)
moon cake
mid-autumn
5
6
Singapore
Bankok
railway
7
Moon
cake
Journey in
Malay
Peninsula
</figure>
<sectionHeader confidence="0.883385" genericHeader="method">
4 Experiment Results
</sectionHeader>
<subsectionHeader confidence="0.976354">
4.1 Accuracy
</subsectionHeader>
<subsubsectionHeader confidence="0.987929">
4.1.1 Precision and Recall
</subsubsectionHeader>
<bodyText confidence="0.99988275">
The precision and recall are shown in Fig. 4, and
the F-measure is shown in Fig. 5. The F-measure
is the balanced score of precision and recall,
calculated as follows:
</bodyText>
<equation confidence="0.6915525">
F-measure â€”
precision + recall
</equation>
<bodyText confidence="0.988833">
Figures 4 and 5 show that the phrase-
represented summary (C) presents the highest
performance. It satisfies both the high precision
and the high recall requirements. Because there
are various situations in WWW searches,
phrase-representation summarization is
considered suitable in any cases.
</bodyText>
<subsubsectionHeader confidence="0.977263">
4.1.2 Relevance Score
</subsubsectionHeader>
<bodyText confidence="0.999939571428571">
The relevance score for each question is shown
in Fig. 6. The phrase-represented summary (C)
gets the highest score on average, and the best in
Question-a2 and Question-b. For Question-al,
though all summaries get poor scores, the
sentence extraction summary (B) is the best
among them.
</bodyText>
<subsectionHeader confidence="0.973043">
4.2 Time
</subsectionHeader>
<bodyText confidence="0.999940875">
The time required to assess relevance is shown
in Fig. 7. The time for Question-a is a sum of the
&apos; times for Questions al and a2. In the Question-a
case, phrase-represented summary (C) requires
the shortest time. For Question-b, leading fixed-
length characters (A) requires the shortest time,
and this result is different from the intuition.
This requires further examination.
</bodyText>
<equation confidence="0.912482">
2 * precision * recall
</equation>
<page confidence="0.975688">
64
</page>
<figure confidence="0.9976945">
L3 + L2
â€¢
0
1
0.5
Recall
Only L3
_
1
0.9 07-
0.8
0.7
0_6
0.5
OA
0
â€”*â€” A
--Aâ€” B
â€¢ C
- D
0
0.8-
0.7Fa
0.6-
0.5-
0.4-
0.2
lilA
â€¢ B
â–ª C
â€¢ D
0.1 L-I
Cl
High presicion High recall
</figure>
<figureCaption confidence="0.9982656">
Fig. 4: Precision &amp; Recall
Fig. 5: F-measure
A BCD
Fig. 6: Relevance Score
Fig. 7: Time
</figureCaption>
<figure confidence="0.98937085">
Relevance Score
350-1
300h,
2504-F
2011-
15q
100
A
â€¢ Q-b
â€¢ Ave ,
rage
Table 4: Summaries Containing Clues
CJD
A
C
A
BIC
A
7
7.73
8 6.43
Q-aI
Q-b
Q-a2
5
6
0
2
0
5
3
0
5
6
3
1
3
6
7.2 6.77
9.43 6.83 5.56 -
</figure>
<page confidence="0.625478">
5.4
</page>
<sectionHeader confidence="0.996324" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.996622571428571">
Here we analyze the experiment result from
multiple viewpoints: the constituent unit of
summaries and .the characteristics of questions
and documents in Section 5.1 and 5.2. We then
discuss advantages of our experimental method
in Section 5.3, and language dependency of the
experiment result in Section 5.4.
</bodyText>
<page confidence="0.99892">
65
</page>
<subsectionHeader confidence="0.997894">
5.1 Comparison of Constituent Unit
</subsectionHeader>
<bodyText confidence="0.999990125">
The units that constitute a summary may affect
the judging process; if the unit is long, the
number of units appeared in a summary may
decrease and the summary contains fewer key
concepts in the original document. We counted
the number of the summaries that contain the
clues to the given questions (see Table 4). The
average numbers are 0.3, 2.0, 4.3 and 4.7 for (A)
fixed-length characters, (B) sentences, (C)
phrases and (D) words, respectively. The phrase-
represented summary (C) and the keyword
enumeration summary (D) widely cover the
topics, and they are about twice as wide as the
sentence extraction summary (B). The leading
fixed-length characters (A) contain very few
clues and this fact supports that this summary
presents the worst performance (see Section
4.1).
In order to compare a summary&apos;s perfor-
mance with a summary&apos;s quality, we calculate
the average relevance score of summaries that
contain clues. These scores are also shown in
Table 4. The average score represents the
informativeness of each summary. Table 4
shows that the sentence extraction summary (B)
and the phrase-represented summary (C) get
relatively high scores, but vary with the question.
This is because sentences and phrases are
sufficiently informative in most cases, but
sentences tend to contain unnecessary informa-
tion, and phrases tend to lack necessary
information. The keyword enumeration
summary (D) gets a relatively low score. This is
because a word is not sufficiently informative to
enable judgement of whether it is clue to the
answer, and relations among words are lacked.
These analyses support the two characteris-
tics of the phrase-represented summaries
described in Chapter 1, that is, adequate
informativeness and wide coverage of topics.
</bodyText>
<subsectionHeader confidence="0.998959">
5.2 Influence of Question and Document
</subsectionHeader>
<bodyText confidence="0.999981648148148">
The most suitable summarization method may
depend on the type of question and/or document.
In the experiment results (see Section 4.1.2), the
sentence extraction summary (B) and the phrase-
represented summary (C) get the highest
relevance score. Therefore, here we focus on
those two summarization methods and consider
the influence of questions and documents.
In selecting questions, we consider two
factors may affect performance. One is which
unit an answer is expressed in. Another is
whether clue words easily come to mind.
If an answer is expressed as a relation of a
predicate to its arguments, the phrase-
representation summarization may be suitable.
Question-a2 and Question-b are of this case. If
an answer is expressed as compound relations,
e.g., reason-consequence relations or cause-
result relations, the sentence extraction
summarization may be required. And, if an
answer is expressed in complex relations of
sentences, any summarization method of the
four is not suitable. Questions that ask historical
background or complicated procedures are
examples of this kind, e.g., Question-al.
As for another factor, if clue words easily
come to mind, the phrase-represented summary
is suitable for any unit in which an answer is
expressed. This is because the clues are found
more easily in short phrases than in long
sentences.
In selecting documents, whether a question is
relevant to the main topic of a document affects
the performance, because we use generic
summaries. By sentence extraction summariza-
tion, the answer is extracted as a summary only
when the question is relevant to the main topic.
Phrase-represented summary is able to cover
topics more widely, for example, one of the
main topics or detailed description of each topic
(see Section 5.1). Because the characteristic of
the document is independent of the question,
which summaries cannot be predicted, and thus
the phrase-represented summary will give better
results.
Through these discussions, we conclude that
the phrase-representation summarization is
suitable for various cases, while the sentence
extraction summarization is for only some
restricted cases. Though the samples of
questions and documents are relatively few in
our experiment, it is sufficient to show the
effectiveness of the phrase-representation
summarization.
</bodyText>
<page confidence="0.956521">
66
</page>
<subsectionHeader confidence="0.833347">
5.3 , Advantages of our Experimental
Method
</subsectionHeader>
<bodyText confidence="0.997563">
Our experimental method has the following
advantages.
</bodyText>
<listItem confidence="0.746746">
(1) More exact assessment
</listItem>
<bodyText confidence="0.99295175">
(2)Serves precision/recall pairs for a variety of
situations
(3)Helps further analysis of problems of a
summarization method
</bodyText>
<subsubsectionHeader confidence="0.760619">
5.3.1 More Exact Assessment
</subsubsectionHeader>
<bodyText confidence="0.9868625">
Our experimental method provides more exact
relevance assessment in the following ways.
</bodyText>
<listItem confidence="0.479507">
(a) More detailed description of a question
</listItem>
<bodyText confidence="0.999072105263158">
We asked the subjects to assess the relevance of
full documents to each question after the
experiment. Result shows that 93% of the
subject judgements match the assumed relevance,
while only 69% match in the same kind of
assessment in SUMMAC. The percentage that
all judgements per document agreed the
assumed relevance is 33%, while only 17% in
SUMMAC. This is because the subjects
comprehended the questions correctly by given
detailed information about the situation.
(b) More subjects assigned per summary sample
We assigned ten subjects to each summary
sample, while only one or two subjects were
used in SUMMAC. We examined the difference
of judgement between the average of ten
subjects and the first subject of the ten. Result
&apos;shows that 47% of the first subject&apos;s judgement
differ more than one level from the average.
</bodyText>
<listItem confidence="0.989385">
â€¢ This proves that the assessment varies from one
subject to another, even if a detailed situation is
given.
(c) Finer levels of relevance
</listItem>
<bodyText confidence="0.998645333333333">
We introduced four levels of relevance, by
which ambiguity of relevance can be expressed
better.
</bodyText>
<subsubsectionHeader confidence="0.755076">
5.3.2 Serves precision/recall pairs for a variety
</subsubsectionHeader>
<bodyText confidence="0.9693873">
of situations
According to the four levels of relevance, we
assume three kinds of relevance document sets.
This enables to plot the PR curve.
In evaluation conferences like SUMMAC,
various summarization methods that are
developed for different purposes must be
compared. Using such a PR curve, each method
can be compared in a criterion that matches its
purpose.
</bodyText>
<subsubsectionHeader confidence="0.632997">
5.3.3 Helps further analysis of problems of a
</subsubsectionHeader>
<bodyText confidence="0.986911">
summarization method
We have introduced the relevance score, which
allows each summary to be evaluated. Using this
score, we can analyze the extrinsic evaluation
result and the intrinsic evaluation result
connected together, for example, an evaluation
result based on information retrieval task and
that based on Q &amp; A task using the same
questions. Through such analyses, the text
quality of summaries or the adequate informa-
tiveness can be examined. We ourselves got a lot
of benefit from the analysis to find problems and
improve the quality of the summary.
</bodyText>
<subsectionHeader confidence="0.990885">
5.4 Language dependency
</subsectionHeader>
<bodyText confidence="0.999949304347826">
Though experiment method may be applied to
any other languages, we must consider the
possibility that our result depends on the
language characteristics. Japanese text is written
by mixing several kinds of characters; Kana
characters (Hiragana and Katakana) and Kanji
(Chinese) characters, and alphabetic characters
are also used. Kanji characters are mainly used
to represent concept words and Hiragana
characters are used for function words. The fact
that they play the different roles makes it easy to
find the full words. Also Kanji is a kind of
ideogram and each character has its own
meaning. Thus, most words can be expressed by
1 to 3 Kanji characters to make short phrases (15
- 20 characters) sufficiently informative.
Though the basic algorithm to create phrase-
represented summary itself can be applied to
other languages by replacing its analysis
component and generation component, similarâ€¢
experiment in that language is required to prove
the effectiveness of the phrase-represented
summary.
</bodyText>
<sectionHeader confidence="0.978581" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999943714285714">
We proposed an improved method of task-based
evaluation on information retrieval. This method
can be used to evaluate the performance of
summarization methods more accurately than is
possible by the methods used in previous work.
We carried out a relatively large experiment
using this method, the results of which show that
</bodyText>
<page confidence="0.997882">
67
</page>
<bodyText confidence="0.997772333333333">
phrase-representation summarization is effective
to select relevant documents from information
retrieval results.
</bodyText>
<sectionHeader confidence="0.992141" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999990666666667">
We would like to thank our company members
who gave valuable suggestions and participated
in the experiment.
</bodyText>
<sectionHeader confidence="0.999182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915416666667">
Hand, T. F. (1997). &amp;quot;A Proposal for Task-based
Evaluation of Text Summarization Systems.&amp;quot; In
Proceedings of the ACL/EACL&apos;97 Workshop on
Intelligent Scalable Text Summarization, pp31-38.
Jing, H., Barzilay, R., McKeown, K. and Elhadad, M.
(1998). &amp;quot;Summarization Evaluation Methods:
Experiments and Analysis.&amp;quot; In Intelligent Text
Summarization. pp51-59. AAAI Press.
Mani, I., House, D., Klein,G., Hirschman, L.,Obrst,
L., Firmin, T., Chizanowski, M., and Sundheim, B.
(1998). &amp;quot;The TIPSTER SUMMAC Text Summari-
zation Evaluation.&amp;quot; Technical Report MIR
98W0000138, MITRE Technical Report.
Minel, J.-L., Nugier, S. and Piat, G. (1997). &amp;quot;How to
Appreciate the Quality of Automatic Text Summa-
rization? Examples of FAN and MLUCE Protocols
and their Results on SERAPH1N.&amp;quot; In Proc. of the
ACL/EACL&apos;97 Workshop on Intelligent Scalable
Text Summarization, pp.25-30.
Mochizuki, H and Okumura, M. (1999). &amp;quot;Evaluation
of Summarization Methods based on Information
Retrieval Task.&amp;quot; In Notes of S1GNL of the
Information Processing Society of Japan, 99-NL-
132, pp41-48. (In Japanese)
Salton, G. (1989). Automatic Text Processing: The
â€¢ Transformation, Analysis, and Retrieval of
Information by Computer. Addison-Wesley
Publishing Company, Inc.
Ueda, Y., Oka, M., Koyama, T. and Miyauchi, T.
(2000). &amp;quot;Toward the &amp;quot;At-a-glance&amp;quot; Summary:
Phrase-representation Summarization Method.&amp;quot;
submitted to COLING2000.
Zechner, K. (1996). &amp;quot;Fast Generation of Abstracts
from General Domain Text Corpora by Extracting
Relevant Sentences.&amp;quot; In Proc. of COLING-96, pp.
986-989.
</reference>
<page confidence="0.999445">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.187164">
<title confidence="0.997753">Evaluation of Phrase-Representation based on Information Retrieval Task</title>
<author confidence="0.644547">Mamiko OKA Yoshihiro UEDA</author>
<affiliation confidence="0.537797">Industry Solutions Fuji Xerox Co., Ltd.</affiliation>
<address confidence="0.972806">430 Sakai, Nakai-machi, Ashigarakami-gun, Kanagawa, Japan, 259-0157</address>
<email confidence="0.594656">oka.mamiko@fujixerox.co.jpUeda.Yoshihiro@fujixerox.co.jp</email>
<abstract confidence="0.997499846153846">We have developed an improved task-based evaluation method of summarization, the accuracy of which is increased by specifying the details of the task including background stories, and by assigning ten subjects per summary sample. The method also serves precision/recall pairs for a variety of situations by introducing multiple levels of relevance assessment. The method is applied to prove phrase-represented summary is most effective to select relevant documents from information retrieval results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T F Hand</author>
</authors>
<title>A Proposal for Task-based Evaluation of Text Summarization Systems.&amp;quot;</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="7637" citStr="Hand (1997)" startWordPosition="1136" endWordPosition="1137">y inferred. 60 In (B), (C), and (D), the same method of calculating the importance scores of words is used in common, and lengths of summaries are kept to be 60 to 80 characters. As you can see each summary is generic, Le. not created for any specific queries. Because the phrase-representation summarization method is applied to Japanese, we examine the effectiveness of these four methods in Japanese. 2.2 Previous Work The best-known example of task-based evaluation on information retrieval is the ad hoc task in the TIPSTER Text Summarization Evaluation Conference (SUMMAC) (Mani, et al. 1998). Hand (1997) details the proposed taskbased evaluation under TIPSTER. Jing, et al. (1998) describe how various parameters affect the evaluation result through a relatively large task-based experiment. Evaluation conferences like SUMMAC are not yet held for Japanese summarization systems4. Mochizuki and Okumura (1999) applied , the SUMMAC methodology to Japanese summarization methods for the first time. Most previous experiments are concerned with SUMMAC, accordingly the methods resemble each other. 2.3 Framework of Evaluation The framework of task-based evaluation on information retrieval is shown in Fig.</context>
</contexts>
<marker>Hand, 1997</marker>
<rawString>Hand, T. F. (1997). &amp;quot;A Proposal for Task-based Evaluation of Text Summarization Systems.&amp;quot; In Proceedings of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization, pp31-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>R Barzilay</author>
<author>K McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Summarization Evaluation Methods: Experiments and Analysis.&amp;quot;</title>
<date>1998</date>
<booktitle>In Intelligent Text Summarization.</booktitle>
<pages>51--59</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1390" citStr="Jing, et al. 1998" startWordPosition="184" endWordPosition="187">ation retrieval results. Introduction Summaries are often used to select relevant documents from information retrieval results. The goal of summarization for such &amp;quot;indicative&amp;quot; use is to serve fast and accurate judgement. We have developed the concept of the &amp;quot;at-a-glance&amp;quot; summary, and its realization in the Japanese language - &amp;quot;phrase-representation surnmarizatioh&amp;quot; - to achieve this goal (Ueda, et al. 2000). We have conducted an evaluation experiment to verify the effectiveness of this summarization method. There are two strategies for evaluating summarization systems: intrinsic and extrinsic (Jing, et al. 1998). Intrinsic methods measure a system&apos;s quality mainly by comparing the system&apos;s output with an &amp;quot;ideal&amp;quot; summary. Extrinsic methods measure a system&apos;s performance in a particular task. The aim of the phrase-representation summarization method is fast and accurate judgement in selecting documents in information retrieval. Thus, we adopted a task-based method to evaluate whether the goal was achieved. Task-based evaluation has recently drawn the attention in the summarization field, because the assumption &apos; that there is only one &amp;quot;ideal&amp;quot; summary is considered to be incorrect, and some experiments </context>
<context position="7714" citStr="Jing, et al. (1998)" startWordPosition="1146" endWordPosition="1149">he importance scores of words is used in common, and lengths of summaries are kept to be 60 to 80 characters. As you can see each summary is generic, Le. not created for any specific queries. Because the phrase-representation summarization method is applied to Japanese, we examine the effectiveness of these four methods in Japanese. 2.2 Previous Work The best-known example of task-based evaluation on information retrieval is the ad hoc task in the TIPSTER Text Summarization Evaluation Conference (SUMMAC) (Mani, et al. 1998). Hand (1997) details the proposed taskbased evaluation under TIPSTER. Jing, et al. (1998) describe how various parameters affect the evaluation result through a relatively large task-based experiment. Evaluation conferences like SUMMAC are not yet held for Japanese summarization systems4. Mochizuki and Okumura (1999) applied , the SUMMAC methodology to Japanese summarization methods for the first time. Most previous experiments are concerned with SUMMAC, accordingly the methods resemble each other. 2.3 Framework of Evaluation The framework of task-based evaluation on information retrieval is shown in Fig. 2. Task-based evaluation in general consists of the following three steps: (</context>
</contexts>
<marker>Jing, Barzilay, McKeown, Elhadad, 1998</marker>
<rawString>Jing, H., Barzilay, R., McKeown, K. and Elhadad, M. (1998). &amp;quot;Summarization Evaluation Methods: Experiments and Analysis.&amp;quot; In Intelligent Text Summarization. pp51-59. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>D House</author>
<author>G Klein</author>
<author>L Hirschman</author>
<author>L Obrst</author>
<author>T Firmin</author>
<author>M Chizanowski</author>
<author>B Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC Text Summarization Evaluation.&amp;quot;</title>
<date>1998</date>
<tech>Technical Report MIR 98W0000138, MITRE Technical Report.</tech>
<contexts>
<context position="2068" citStr="Mani, et al. 1998" startWordPosition="286" endWordPosition="289">paring the system&apos;s output with an &amp;quot;ideal&amp;quot; summary. Extrinsic methods measure a system&apos;s performance in a particular task. The aim of the phrase-representation summarization method is fast and accurate judgement in selecting documents in information retrieval. Thus, we adopted a task-based method to evaluate whether the goal was achieved. Task-based evaluation has recently drawn the attention in the summarization field, because the assumption &apos; that there is only one &amp;quot;ideal&amp;quot; summary is considered to be incorrect, and some experiments on information retrieval were reported (Jing, et al. 1998) (Mani, et al. 1998) (Mochizuki and Okunura 1999). However, there is no standard evaluation method, and we consider that there are some shortcomings in the existing methods. Thus, we have developed an improved evaluation method and carried out a relatively large experiment. In this paper, we first give an overview of the phrase-representation summarization method. We then consider the evaluation method and show the result of an experiment based on the improved method to demonstrate the effectiveness of phrase-representation summarization. 1 Phrase-Representation Summarization Most automatic summarization systems </context>
<context position="7624" citStr="Mani, et al. 1998" startWordPosition="1132" endWordPosition="1135">if they can be easily inferred. 60 In (B), (C), and (D), the same method of calculating the importance scores of words is used in common, and lengths of summaries are kept to be 60 to 80 characters. As you can see each summary is generic, Le. not created for any specific queries. Because the phrase-representation summarization method is applied to Japanese, we examine the effectiveness of these four methods in Japanese. 2.2 Previous Work The best-known example of task-based evaluation on information retrieval is the ad hoc task in the TIPSTER Text Summarization Evaluation Conference (SUMMAC) (Mani, et al. 1998). Hand (1997) details the proposed taskbased evaluation under TIPSTER. Jing, et al. (1998) describe how various parameters affect the evaluation result through a relatively large task-based experiment. Evaluation conferences like SUMMAC are not yet held for Japanese summarization systems4. Mochizuki and Okumura (1999) applied , the SUMMAC methodology to Japanese summarization methods for the first time. Most previous experiments are concerned with SUMMAC, accordingly the methods resemble each other. 2.3 Framework of Evaluation The framework of task-based evaluation on information retrieval is </context>
</contexts>
<marker>Mani, House, Klein, Hirschman, Obrst, Firmin, Chizanowski, Sundheim, 1998</marker>
<rawString>Mani, I., House, D., Klein,G., Hirschman, L.,Obrst, L., Firmin, T., Chizanowski, M., and Sundheim, B. (1998). &amp;quot;The TIPSTER SUMMAC Text Summarization Evaluation.&amp;quot; Technical Report MIR 98W0000138, MITRE Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-L Minel</author>
<author>S Nugier</author>
<author>G Piat</author>
</authors>
<title>How to Appreciate the Quality of Automatic Text Summarization? Examples of FAN and MLUCE Protocols and their Results on SERAPH1N.&amp;quot;</title>
<date>1997</date>
<booktitle>In Proc. of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="10121" citStr="Minel, et al. 1997" startWordPosition="1511" endWordPosition="1514">hey really want to know about a given question. The previous experiments gave only a short description of a topic. We consider it is not sufficiently specific and the interpretation of a question must varied with the subjects. We selected two topics (&amp;quot;moon cake&amp;quot; and &amp;quot;journey in Malay. Peninsula&amp;quot;) and assumed three questions. To indicate to the subjects, we set detailed situation including the motivation to know about that or the use of the information obtained for each question. This method satisfies the restriction &amp;quot;to limit the variation in assessment between readers&amp;quot; in the MLUCE Protocol (Minel, et al. 1997). (1) Data Preparation Sspjne4-1 rifirmation, Docurnetits sawE _ â€¢ Relevant: Relevant Irrelevant at R Result Time !I , Accuracy 61 For each topic, ten documents are selected from search results by major WWW search engines, so that more than five relevant documents are included for each question. The topics, the outline of the questions, the queries for WWW search, and the number of relevant documents are shown in Table 2. The description of Question-a2 that was given to the subjects is shown in Fig. 3. One day just after the mid-autumn festival, my colleague Mr. A brought some moon cakes to th</context>
</contexts>
<marker>Minel, Nugier, Piat, 1997</marker>
<rawString>Minel, J.-L., Nugier, S. and Piat, G. (1997). &amp;quot;How to Appreciate the Quality of Automatic Text Summarization? Examples of FAN and MLUCE Protocols and their Results on SERAPH1N.&amp;quot; In Proc. of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization, pp.25-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mochizuki</author>
<author>M Okumura</author>
</authors>
<title>Evaluation of Summarization Methods based on Information Retrieval Task.&amp;quot;</title>
<date>1999</date>
<booktitle>In Notes of S1GNL of the Information Processing Society of Japan, 99-NL132,</booktitle>
<pages>41--48</pages>
<note>(In Japanese)</note>
<contexts>
<context position="7943" citStr="Mochizuki and Okumura (1999)" startWordPosition="1176" endWordPosition="1179">ation summarization method is applied to Japanese, we examine the effectiveness of these four methods in Japanese. 2.2 Previous Work The best-known example of task-based evaluation on information retrieval is the ad hoc task in the TIPSTER Text Summarization Evaluation Conference (SUMMAC) (Mani, et al. 1998). Hand (1997) details the proposed taskbased evaluation under TIPSTER. Jing, et al. (1998) describe how various parameters affect the evaluation result through a relatively large task-based experiment. Evaluation conferences like SUMMAC are not yet held for Japanese summarization systems4. Mochizuki and Okumura (1999) applied , the SUMMAC methodology to Japanese summarization methods for the first time. Most previous experiments are concerned with SUMMAC, accordingly the methods resemble each other. 2.3 Framework of Evaluation The framework of task-based evaluation on information retrieval is shown in Fig. 2. Task-based evaluation in general consists of the following three steps: (1) Data preparation: Assume an information need, create a query for the information need, and prepare simulated search results - with different types of summaries. (2) Relevance assessment: Using the summaries, human subjects ass</context>
</contexts>
<marker>Mochizuki, Okumura, 1999</marker>
<rawString>Mochizuki, H and Okumura, M. (1999). &amp;quot;Evaluation of Summarization Methods based on Information Retrieval Task.&amp;quot; In Notes of S1GNL of the Information Processing Society of Japan, 99-NL132, pp41-48. (In Japanese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing:</title>
<date>1989</date>
<publisher>The</publisher>
<marker>Salton, 1989</marker>
<rawString>Salton, G. (1989). Automatic Text Processing: The</rawString>
</citation>
<citation valid="false">
<authors>
<author>Transformation</author>
</authors>
<title>Analysis, and Retrieval of Information by Computer.</title>
<publisher>Addison-Wesley Publishing Company, Inc.</publisher>
<marker>Transformation, </marker>
<rawString>â€¢ Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley Publishing Company, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ueda</author>
<author>M Oka</author>
<author>T Koyama</author>
<author>T Miyauchi</author>
</authors>
<title>Toward the &amp;quot;At-a-glance&amp;quot; Summary: Phrase-representation Summarization Method.&amp;quot;</title>
<date>2000</date>
<note>submitted to COLING2000.</note>
<contexts>
<context position="1181" citStr="Ueda, et al. 2000" startWordPosition="155" endWordPosition="158">ll pairs for a variety of situations by introducing multiple levels of relevance assessment. The method is applied to prove phrase-represented summary is most effective to select relevant documents from information retrieval results. Introduction Summaries are often used to select relevant documents from information retrieval results. The goal of summarization for such &amp;quot;indicative&amp;quot; use is to serve fast and accurate judgement. We have developed the concept of the &amp;quot;at-a-glance&amp;quot; summary, and its realization in the Japanese language - &amp;quot;phrase-representation surnmarizatioh&amp;quot; - to achieve this goal (Ueda, et al. 2000). We have conducted an evaluation experiment to verify the effectiveness of this summarization method. There are two strategies for evaluating summarization systems: intrinsic and extrinsic (Jing, et al. 1998). Intrinsic methods measure a system&apos;s quality mainly by comparing the system&apos;s output with an &amp;quot;ideal&amp;quot; summary. Extrinsic methods measure a system&apos;s performance in a particular task. The aim of the phrase-representation summarization method is fast and accurate judgement in selecting documents in information retrieval. Thus, we adopted a task-based method to evaluate whether the goal was </context>
</contexts>
<marker>Ueda, Oka, Koyama, Miyauchi, 2000</marker>
<rawString>Ueda, Y., Oka, M., Koyama, T. and Miyauchi, T. (2000). &amp;quot;Toward the &amp;quot;At-a-glance&amp;quot; Summary: Phrase-representation Summarization Method.&amp;quot; submitted to COLING2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Fast Generation of Abstracts from General Domain Text Corpora by Extracting Relevant Sentences.&amp;quot;</title>
<date>1996</date>
<booktitle>In Proc. of COLING-96,</booktitle>
<pages>986--989</pages>
<contexts>
<context position="6672" citStr="Zechner 1996" startWordPosition="992" endWordPosition="993">hrases are sentences (created by the sentence extraction method) and words (by the keyword enumeration method). We also compare &amp;quot;leading fixed-length characters,&amp;quot; which are often used as substitutes for summaries by WWW search engines. The generation method for each summary is described as follows. (A) Leading fixed-length characters: extract the first 80 characters of the document body. (B) Sentence extraction summarization: selectâ€¢ important sentences from a document. The importance score of each sentence is calculated from the simple sum of the importance scores of the words in a sentence (Zechner 1996). (C) Phrase-representation summarization: described in Chapter 1. (D) Keyword enumeration summarization: list up important words or compound nouns. http://www.fujixerox.co.jp/headline/2000/0308_nton erbi7.html (in English) 3 This phrase lacks the subject because the original sentence lacks it. Cases are usually omitted in Japanese if they can be easily inferred. 60 In (B), (C), and (D), the same method of calculating the importance scores of words is used in common, and lengths of summaries are kept to be 60 to 80 characters. As you can see each summary is generic, Le. not created for any spe</context>
</contexts>
<marker>Zechner, 1996</marker>
<rawString>Zechner, K. (1996). &amp;quot;Fast Generation of Abstracts from General Domain Text Corpora by Extracting Relevant Sentences.&amp;quot; In Proc. of COLING-96, pp. 986-989.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>