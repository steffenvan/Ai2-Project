<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.133194">
<title confidence="0.999267">
Good Seed Makes a Good Crop:
Accelerating Active Learning Using Language Modeling
</title>
<author confidence="0.997859">
Dmitriy Dligach Martha Palmer
</author>
<affiliation confidence="0.9936535">
Department of Computer Science Department of Linguistics
University of Colorado at Boulder University of Colorado at Boulder
</affiliation>
<email confidence="0.995915">
Dmitriy.Dligach@colorado.edu Martha.Palmer@colorado.edu
</email>
<sectionHeader confidence="0.993698" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999043769230769">
Active Learning (AL) is typically initialized
with a small seed of examples selected ran-
domly. However, when the distribution of
classes in the data is skewed, some classes
may be missed, resulting in a slow learning
progress. Our contribution is twofold: (1) we
show that an unsupervised language modeling
based technique is effective in selecting rare
class examples, and (2) we use this technique
for seeding AL and demonstrate that it leads
to a higher learning rate. The evaluation is
conducted in the context of word sense disam-
biguation.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992324947368421">
Active learning (AL) (Settles, 2009) has become a
popular research field due to its potential benefits: it
can lead to drastic reductions in the amount of anno-
tation that is necessary for training a highly accurate
statistical classifier. Unlike in a random sampling
approach, where unlabeled data is selected for anno-
tation randomly, AL delegates the selection of un-
labeled data to the classifier. In a typical AL setup,
a classifier is trained on a small sample of the data
(usually selected randomly), known as the seed ex-
amples. The classifier is subsequently applied to a
pool of unlabeled data with the purpose of selecting
additional examples that the classifier views as infor-
mative. The selected data is annotated and the cycle
is repeated, allowing the learner to quickly refine the
decision boundary between the classes.
Unfortunately, AL is susceptible to a shortcom-
ing known as the missed cluster effect (Sch¨utze et
al., 2006) and its special case called the missed class
</bodyText>
<page confidence="0.779002">
6
</page>
<bodyText confidence="0.999616617647059">
effect (Tomanek et al., 2009). The missed cluster ef-
fect is a consequence of the fact that seed examples
influence the direction the learner takes in its ex-
ploration of the instance space. Whenever the seed
does not contain the examples of a certain cluster
that is representative of a group of examples in the
data, the learner may become overconfident about
the class membership of this cluster (particularly if it
lies far from the decision boundary). As a result, the
learner spends a lot of time exploring one region of
the instance space at the expense of missing another.
This problem can become especially severe, when
the class distribution in the data is skewed: a ran-
domly selected seed may not adequately represent
all the classes or even miss certain classes altogether.
Consider a binary classification task where rare class
examples constitute 5% of the data (a frequent sce-
nario in e.g. word sense disambiguation). If 10
examples are chosen randomly for seeding AL, the
probability that none of the rare class examples will
make it to the seed is 60% 1. Thus, there is a high
probability that AL would stall, selecting only the
examples of the predominant class over the course
of many iterations. At the same time, if we had a
way to ensure that examples of the rare class were
present in the seed, AL would be able to select the
examples of both classes, efficiently clarifying the
decision boundary and ultimately producing an ac-
curate classifier.
Tomanek et al. (2009) simulated these scenarios
using manually constructed seed sets. They demon-
strated that seeding AL with a data set that is artifi-
cially enriched with rare class examples indeed leads
to a higher learning rate comparing to randomly
</bodyText>
<footnote confidence="0.699333">
1Calculated using Binomial distribution
</footnote>
<note confidence="0.613928">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 6–10,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.996499826086957">
sampled and predominant class enriched seeds. In place, strengthen, wait, wonder. The average num-
this paper, we propose a simple automatic approach ber of examples for these verbs is 232. In supervised
for selecting the seeds that are rich in the examples word sense disambiguation, a single model per word
of the rare class. We then demonstrate that this ap- is typically trained and that is the approach we take.
proach to seed selection accelerates AL. Finally, we Thus, we conduct our evaluation using 25 different
analyze the mechanism of this acceleration. data sets. We report the averages across these 25
2 Approach data sets. In our evaluation, we use a state-of-the-
Language Model (LM) Sampling is a simple unsu- art word sense disambiguation system (Dligach and
pervised technique for selecting unlabeled data that Palmer, 2008), that utilizes rich linguistic features to
is enriched with rare class examples. LM sampling capture the contexts of ambiguous words.
involves training a LM on a corpus of unlabeled can- 3.2 Rare Sense Retrieval
didate examples and selecting the examples with low The success of our approach to seeding AL hinges
LM probability. Dligach and Palmer (2009) used on the ability of LM sampling to discover rare class
this technique in the context of word sense disam- examples better than random sampling. In this ex-
biguation and showed that rare sense examples tend periment, we demonstrate that LM sampling outper-
to concentrate among the examples with low prob- forms random sampling for every selection size. For
ability. Unfortunately these authors provided a lim- each verb we conduct an experiment in which we
ited evaluation of this technique: they looked at its select the instances of this verb using both methods.
effectiveness only at a single selection size. We pro- We measure the recall of the rare sense, which we
vide a more convincing evaluation in which the ef- calculate as the ratio of the number of selected rare
fectiveness of this approach is examined for all sizes sense examples to the total number of rare sense ex-
of the selected data. amples for this verb.
Seed Selection for AL is typically done ran- We train a LM (Stolcke, 2002) on the corpora
domly. However, for datasets with a skewed dis- from which OntoNotes data originates: the Wall
tribution of classes, rare class examples may end Street Journal, English Broadcast News, English
up being underrepresented. We propose to use LM Conversation, and the Brown corpus. For each verb,
sampling for seed selection, which captures more we compute the LM probability for each instance of
examples of rare classes than random selection, thus this verb and sort the instances by probability. In
leading to a faster learning progress. the course of the experiment, we select one example
3 Evaluation with the smallest probability and move it to the set
3.1 Data of selected examples. We then measure the recall of
For our evaluation, we needed a dataset that is the rare sense for the selected examples. We con-
characterized by a skewed class distribution. This tinue in this fashion until all the examples have been
phenomenon is pervasive in word sense data. A selected. We use random sampling as a baseline,
large word sense annotated corpus has recently which is obtained by continuously selecting a single
been released by the OntoNotes (Hovy et al., 2006; example randomly. We continue until all the exam-
Weischedel et al., 2009) project. For clarity of eval- ples have been selected. At the end of the exper-
uation, we identify a set of verbs that satisfy three iment, we have produced two recall curves, which
criteria: (1) the number of senses is two, (2) the measure the recall of the rare sense retrieval for this
number of annotated examples is at least 100, (3) the verb at various sizes of selected data. Due to the
proportion of the rare sense is at most 20%. The fol- lack of space, we do not show the plots that display
lowing 25 verbs satisfy these criteria: account, add, these curves for individual verbs. Instead, in Figure
admit, allow, announce, approve, compare, demand, 1 we display the curves that are averaged across all
exist, expand, expect, explain, focus, include, invest, verbs. At every selection size, LM sampling results
issue, point, promote, protect, receive, remain, re- in a higher recall of the rare sense. The average dif-
7 ference across all selection sizes is 11%.
</bodyText>
<figureCaption confidence="0.9790145">
Figure 1: Average recall of rare sense retrieval for LM
and random sampling by relative size of training set
</figureCaption>
<subsectionHeader confidence="0.991936">
3.3 Classic and Selectively Seeded AL
</subsectionHeader>
<bodyText confidence="0.999017903846154">
In this experiment, we seed AL using LM sampling
and compare how this selectively seeded AL per-
forms in comparison with classic (randomly-seeded)
AL. Our experimental setup is typical for an active
learning study. We split the set of annotated exam-
ples for a verb into 90% and 10% parts. The 90%
part is used as a pool of unlabeled data. The 10%
part is used as a test set. We begin classic AL by
randomly selecting 10% of the examples from the
pool to use as seeds. We train a maximum entropy
model (Le, 2004) using these seeds. We then repeat-
edly apply the model to the remaining examples in
the pool: on each iteration of AL, we draw a sin-
gle most informative example from the pool. The
informativeness is estimated using prediction mar-
gin (Schein and Ungar, 2007), which is computed as
|P(c1|x) − P(c2|x)|, where c1 and c2 are the two
most probable classes of example x according to the
model. The selected example is moved to the train-
ing set. On each iteration, we also keep track of how
accurately the current model classifies the held out
test set.
In parallel, we conduct a selectively seeded AL
experiment that is identical to the classic one but
with one crucial difference: instead of selecting the
seed examples randomly, we select them using LM
sampling by identifying 10% of the examples from
the pool with the smallest LM probability. We also
produce a random sampling curve to be used as a
baseline. At the end of this experiment we have ob-
tained three learning curves: for classic AL, for se-
lectively seeded AL, and for the random sampling
baseline. The final learning curves for each verb are
produced by averaging the learning curves from ten
different trials.
Figure 2 presents the average accuracy of selec-
tively seeded AL (top curve), classic AL (middle
curve) and the random sampling baseline (bottom
curve) at various fractions of the total size of the
training set. The size of zero corresponds to a train-
ing set consisting only of the seed examples. The
size of one corresponds to a training set consisting
of all the examples in the pool labeled. The accuracy
at a given size was averaged across all 25 verbs.
It is clear that LM-seeded AL accelerates learn-
ing: it reaches the same performance as classic AL
with less training data. LM-seeded AL also reaches
a higher classification accuracy (if stopped at its
peak). We will analyze this somewhat surprising be-
havior in the next section. The difference between
the classic and LM-seeded curves is statistically sig-
nificant (p = 0.0174) 2.
</bodyText>
<figureCaption confidence="0.9836055">
Figure 2: Randomly and LM-seeded AL. Random sam-
pling baseline is also shown.
</figureCaption>
<subsectionHeader confidence="0.987646">
3.4 Why LM Seeding Produces Better Results
</subsectionHeader>
<bodyText confidence="0.942979428571428">
For random sampling, the system achieves its best
accuracy, 94.4%, when the entire pool of unlabeled
examples is labeled. The goal of a typical AL study
is to demonstrate that the same accuracy can be
2We compute the average area under the curve for each type
of AL and use Wilcoxon signed rank test to test whether the
difference between the averages is significant.
</bodyText>
<page confidence="0.997656">
8
</page>
<figureCaption confidence="0.996891413793104">
achieved with less labeled data. For example, in our
case, classic AL reaches the best random sampling
accuracy with only about 5% of the data. However,
it is interesting to notice that LM-seeded AL actually
reaches a higher accuracy, 95%, during early stages
of learning (at 15% of the total training set size). We
believe this phenomenon takes place due to overfit-
ting the predominant class: as the model receives
new data (and therefore more and more examples of
the predominant class), it begins to mislabel more
and more examples of the rare class. A similar idea
has been expressed in literature (Weiss, 1995; Kubat
and Matwin, 1997; Japkowicz, 2001; Weiss, 2004;
Chen et al., 2006), however it has never been veri-
fied in the context of AL.
To verify our hypothesis, we conduct an experi-
ment. The experimental setup is the same as in sec-
tion 3.3. However, instead of measuring the accu-
racy on the test set, we resort to different metrics
that reflect how accurately the classifier labels the in-
stances of the rare class in the held out test set. These
metrics are the recall and precision for the rare class.
Recall is the ratio of the correctly labeled examples
of the rare class and the total number of instances of
the rare class. Precision is the ratio of the correctly
labeled examples of the rare class and the number of
instances labeled as that class. Results are in Figures
3 and 4.
Figure 4: Rare sense classification precision
</figureCaption>
<bodyText confidence="0.978403083333333">
This is consistent with our hypothesis that the clas-
sifier overfits the predominant class. When all the
data is labeled, the recall decreases from about 13%
to only 7%, an almost 50% drop. The reason that
the system achieved a higher level of recall at first is
due to the fact that AL was seeded with LM selected
data, which has a higher content of rare classes (as
we demonstrated in the first experiment). The avail-
ability of the extra examples of the rare class allows
the classifier to label the instances of this class in
the test set more accurately, which in turn boosts the
overall accuracy.
4 Conclusion and Future Work
We introduced a novel approach to seeding AL, in
which the seeds are selected from the examples with
low LM probability. This approach selects more rare
class examples than random sampling, resulting in
more rapid learning and, more importantly, leading
to a classifier that performs better on rare class ex-
amples. As a consequence of this, the overall classi-
fication accuracy is higher than that for classic AL.
Our plans for future work include improving our
LM by incorporating syntactic information such as
POS tags. This should result in better performance
on the rare classes, which is currently still low.
We also plan to experiment with other unsupervised
techniques, such as clustering and outlier detection,
that can lead to better retrieval of rare classes. Fi-
nally, we plan to investigate the applicability of our
approach to a multi-class scenario.
Figure 3: Rare sense classification recall
Observe that for LM-seeded AL, the recall peaks
at first and begins to decline later. Thus the clas-
sifier makes progressively more errors on the rare
class as more labeled examples are being received.
9
</bodyText>
<sectionHeader confidence="0.962218" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999274">
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambiguation,
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-C-0022, a subcontract from the BBN-AGILE
Team. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.998916" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735815789474">
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha
Palmer. 2006. An empirical study of the behavior
of active learning for word sense disambiguation. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 120–127, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
HLT ’08: Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies, pages 29–32, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Dmitriy Dligach and Martha. Palmer. 2009. Using lan-
guage modeling to select useful annotation data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Student Research Workshop and
Doctoral Consortium, pages 25–30. Association for
Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In NAACL ’06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57–60, Morristown, NJ, USA. Association for
Computational Linguistics.
Nathalie Japkowicz. 2001. Concept-learning in the pres-
ence of between-class and within-class imbalances. In
AI ’01: Proceedings of the 14th Biennial Conference
of the Canadian Society on Computational Studies
of Intelligence, pages 67–77, London, UK. Springer-
Verlag.
M. Kubat and S. Matwin. 1997. Addressing the curse of
imbalanced training sets: one-sided selection. In Pro-
ceedings of the Fourteenth International Conference
on Machine Learning, pages 179–186. Citeseer.
Zhang Le, 2004. Maximum Entropy Modeling Toolkit for
Python and C++.
A.I. Schein and L.H. Ungar. 2007. Active learning for
logistic regression: an evaluation. Machine Learning,
68(3):235–265.
H. Sch¨utze, E. Velipasaoglu, and J.O. Pedersen. 2006.
Performance thresholding in practical text classifica-
tion. In Proceedings of the 15th ACM international
conference on Information and knowledge manage-
ment, pages 662–671. ACM.
Burr Settles. 2009. Active learning literature survey. In
Computer Sciences Technical Report 1648 University
of Wisconsin-Madison.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, Denver, Colorado., pages
901–904.
Katrin Tomanek, Florian Laws, Udo Hahn, and Hinrich
Sch¨utze. 2009. On proper unit selection in active
learning: co-selection effects for named entity recog-
nition. In HLT ’09: Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 9–17, Morristown, NJ, USA.
Association for Computational Linguistics.
R. Weischedel, E. Hovy, M. Marcus, M. Palmer,
R Belvin, S Pradan, L. Ramshaw, and N. Xue, 2009.
OntoNotes: A Large Training Corpus for Enhanced
Processing, chapter in Global Automatic Language
Exploitation, pages 54–63. Springer Verglag.
G.M. Weiss. 1995. Learning with rare cases and small
disjuncts. In Proceedings of the Twelfth International
Conference on Machine Learning, pages 558–565.
Citeseer.
G.M. Weiss. 2004. Mining with rarity: a unifying
framework. ACM SIGKDD Explorations Newsletter,
6(1):7–19.
</reference>
<page confidence="0.99777">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.447547">
<title confidence="0.993365">Good Seed Makes a Good Accelerating Active Learning Using Language Modeling</title>
<author confidence="0.9994">Dmitriy Dligach Martha Palmer</author>
<affiliation confidence="0.9983695">Department of Computer Science Department of Linguistics University of Colorado at Boulder University of Colorado at</affiliation>
<email confidence="0.6354">Dmitriy.Dligach@colorado.eduMartha.Palmer@colorado.edu</email>
<abstract confidence="0.979441785714286">Active Learning (AL) is typically initialized with a small seed of examples selected randomly. However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. The evaluation is conducted in the context of word sense disambiguation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jinying Chen</author>
<author>Andrew Schein</author>
<author>Lyle Ungar</author>
<author>Martha Palmer</author>
</authors>
<title>An empirical study of the behavior of active learning for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>120--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="12064" citStr="Chen et al., 2006" startWordPosition="2015" endWordPosition="2018">reaches the best random sampling accuracy with only about 5% of the data. However, it is interesting to notice that LM-seeded AL actually reaches a higher accuracy, 95%, during early stages of learning (at 15% of the total training set size). We believe this phenomenon takes place due to overfitting the predominant class: as the model receives new data (and therefore more and more examples of the predominant class), it begins to mislabel more and more examples of the rare class. A similar idea has been expressed in literature (Weiss, 1995; Kubat and Matwin, 1997; Japkowicz, 2001; Weiss, 2004; Chen et al., 2006), however it has never been verified in the context of AL. To verify our hypothesis, we conduct an experiment. The experimental setup is the same as in section 3.3. However, instead of measuring the accuracy on the test set, we resort to different metrics that reflect how accurately the classifier labels the instances of the rare class in the held out test set. These metrics are the recall and precision for the rare class. Recall is the ratio of the correctly labeled examples of the rare class and the total number of instances of the rare class. Precision is the ratio of the correctly labeled </context>
</contexts>
<marker>Chen, Schein, Ungar, Palmer, 2006</marker>
<rawString>Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer. 2006. An empirical study of the behavior of active learning for word sense disambiguation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 120–127, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Novel semantic features for verb sense disambiguation.</title>
<date>2008</date>
<booktitle>In HLT ’08: Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies,</booktitle>
<pages>29--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Dligach, Palmer, 2008</marker>
<rawString>Dmitriy Dligach and Martha Palmer. 2008. Novel semantic features for verb sense disambiguation. In HLT ’08: Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies, pages 29–32, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Palmer</author>
</authors>
<title>Using language modeling to select useful annotation data.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium,</booktitle>
<pages>25--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5000" citStr="Palmer (2009)" startWordPosition="802" endWordPosition="803">ort the averages across these 25 2 Approach data sets. In our evaluation, we use a state-of-theLanguage Model (LM) Sampling is a simple unsu- art word sense disambiguation system (Dligach and pervised technique for selecting unlabeled data that Palmer, 2008), that utilizes rich linguistic features to is enriched with rare class examples. LM sampling capture the contexts of ambiguous words. involves training a LM on a corpus of unlabeled can- 3.2 Rare Sense Retrieval didate examples and selecting the examples with low The success of our approach to seeding AL hinges LM probability. Dligach and Palmer (2009) used on the ability of LM sampling to discover rare class this technique in the context of word sense disam- examples better than random sampling. In this exbiguation and showed that rare sense examples tend periment, we demonstrate that LM sampling outperto concentrate among the examples with low prob- forms random sampling for every selection size. For ability. Unfortunately these authors provided a lim- each verb we conduct an experiment in which we ited evaluation of this technique: they looked at its select the instances of this verb using both methods. effectiveness only at a single sel</context>
</contexts>
<marker>Palmer, 2009</marker>
<rawString>Dmitriy Dligach and Martha. Palmer. 2009. Using language modeling to select useful annotation data. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium, pages 25–30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In NAACL ’06: Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7176" citStr="Hovy et al., 2006" startWordPosition="1166" endWordPosition="1169">rse of the experiment, we select one example 3 Evaluation with the smallest probability and move it to the set 3.1 Data of selected examples. We then measure the recall of For our evaluation, we needed a dataset that is the rare sense for the selected examples. We concharacterized by a skewed class distribution. This tinue in this fashion until all the examples have been phenomenon is pervasive in word sense data. A selected. We use random sampling as a baseline, large word sense annotated corpus has recently which is obtained by continuously selecting a single been released by the OntoNotes (Hovy et al., 2006; example randomly. We continue until all the examWeischedel et al., 2009) project. For clarity of eval- ples have been selected. At the end of the experuation, we identify a set of verbs that satisfy three iment, we have produced two recall curves, which criteria: (1) the number of senses is two, (2) the measure the recall of the rare sense retrieval for this number of annotated examples is at least 100, (3) the verb at various sizes of selected data. Due to the proportion of the rare sense is at most 20%. The fol- lack of space, we do not show the plots that display lowing 25 verbs satisfy t</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In NAACL ’06: Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX, pages 57–60, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathalie Japkowicz</author>
</authors>
<title>Concept-learning in the presence of between-class and within-class imbalances.</title>
<date>2001</date>
<booktitle>In AI ’01: Proceedings of the 14th Biennial Conference of the Canadian Society on Computational Studies of Intelligence,</booktitle>
<pages>67--77</pages>
<publisher>SpringerVerlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="12031" citStr="Japkowicz, 2001" startWordPosition="2011" endWordPosition="2012">mple, in our case, classic AL reaches the best random sampling accuracy with only about 5% of the data. However, it is interesting to notice that LM-seeded AL actually reaches a higher accuracy, 95%, during early stages of learning (at 15% of the total training set size). We believe this phenomenon takes place due to overfitting the predominant class: as the model receives new data (and therefore more and more examples of the predominant class), it begins to mislabel more and more examples of the rare class. A similar idea has been expressed in literature (Weiss, 1995; Kubat and Matwin, 1997; Japkowicz, 2001; Weiss, 2004; Chen et al., 2006), however it has never been verified in the context of AL. To verify our hypothesis, we conduct an experiment. The experimental setup is the same as in section 3.3. However, instead of measuring the accuracy on the test set, we resort to different metrics that reflect how accurately the classifier labels the instances of the rare class in the held out test set. These metrics are the recall and precision for the rare class. Recall is the ratio of the correctly labeled examples of the rare class and the total number of instances of the rare class. Precision is th</context>
</contexts>
<marker>Japkowicz, 2001</marker>
<rawString>Nathalie Japkowicz. 2001. Concept-learning in the presence of between-class and within-class imbalances. In AI ’01: Proceedings of the 14th Biennial Conference of the Canadian Society on Computational Studies of Intelligence, pages 67–77, London, UK. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kubat</author>
<author>S Matwin</author>
</authors>
<title>Addressing the curse of imbalanced training sets: one-sided selection.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning,</booktitle>
<pages>179--186</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="12014" citStr="Kubat and Matwin, 1997" startWordPosition="2007" endWordPosition="2010">ss labeled data. For example, in our case, classic AL reaches the best random sampling accuracy with only about 5% of the data. However, it is interesting to notice that LM-seeded AL actually reaches a higher accuracy, 95%, during early stages of learning (at 15% of the total training set size). We believe this phenomenon takes place due to overfitting the predominant class: as the model receives new data (and therefore more and more examples of the predominant class), it begins to mislabel more and more examples of the rare class. A similar idea has been expressed in literature (Weiss, 1995; Kubat and Matwin, 1997; Japkowicz, 2001; Weiss, 2004; Chen et al., 2006), however it has never been verified in the context of AL. To verify our hypothesis, we conduct an experiment. The experimental setup is the same as in section 3.3. However, instead of measuring the accuracy on the test set, we resort to different metrics that reflect how accurately the classifier labels the instances of the rare class in the held out test set. These metrics are the recall and precision for the rare class. Recall is the ratio of the correctly labeled examples of the rare class and the total number of instances of the rare class</context>
</contexts>
<marker>Kubat, Matwin, 1997</marker>
<rawString>M. Kubat and S. Matwin. 1997. Addressing the curse of imbalanced training sets: one-sided selection. In Proceedings of the Fourteenth International Conference on Machine Learning, pages 179–186. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Le</author>
</authors>
<title>Maximum Entropy Modeling Toolkit for Python and C++.</title>
<date>2004</date>
<contexts>
<context position="8878" citStr="Le, 2004" startWordPosition="1469" endWordPosition="1470"> for LM and random sampling by relative size of training set 3.3 Classic and Selectively Seeded AL In this experiment, we seed AL using LM sampling and compare how this selectively seeded AL performs in comparison with classic (randomly-seeded) AL. Our experimental setup is typical for an active learning study. We split the set of annotated examples for a verb into 90% and 10% parts. The 90% part is used as a pool of unlabeled data. The 10% part is used as a test set. We begin classic AL by randomly selecting 10% of the examples from the pool to use as seeds. We train a maximum entropy model (Le, 2004) using these seeds. We then repeatedly apply the model to the remaining examples in the pool: on each iteration of AL, we draw a single most informative example from the pool. The informativeness is estimated using prediction margin (Schein and Ungar, 2007), which is computed as |P(c1|x) − P(c2|x)|, where c1 and c2 are the two most probable classes of example x according to the model. The selected example is moved to the training set. On each iteration, we also keep track of how accurately the current model classifies the held out test set. In parallel, we conduct a selectively seeded AL exper</context>
</contexts>
<marker>Le, 2004</marker>
<rawString>Zhang Le, 2004. Maximum Entropy Modeling Toolkit for Python and C++.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A I Schein</author>
<author>L H Ungar</author>
</authors>
<title>Active learning for logistic regression: an evaluation.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<volume>68</volume>
<issue>3</issue>
<contexts>
<context position="9135" citStr="Schein and Ungar, 2007" startWordPosition="1512" endWordPosition="1515">) AL. Our experimental setup is typical for an active learning study. We split the set of annotated examples for a verb into 90% and 10% parts. The 90% part is used as a pool of unlabeled data. The 10% part is used as a test set. We begin classic AL by randomly selecting 10% of the examples from the pool to use as seeds. We train a maximum entropy model (Le, 2004) using these seeds. We then repeatedly apply the model to the remaining examples in the pool: on each iteration of AL, we draw a single most informative example from the pool. The informativeness is estimated using prediction margin (Schein and Ungar, 2007), which is computed as |P(c1|x) − P(c2|x)|, where c1 and c2 are the two most probable classes of example x according to the model. The selected example is moved to the training set. On each iteration, we also keep track of how accurately the current model classifies the held out test set. In parallel, we conduct a selectively seeded AL experiment that is identical to the classic one but with one crucial difference: instead of selecting the seed examples randomly, we select them using LM sampling by identifying 10% of the examples from the pool with the smallest LM probability. We also produce </context>
</contexts>
<marker>Schein, Ungar, 2007</marker>
<rawString>A.I. Schein and L.H. Ungar. 2007. Active learning for logistic regression: an evaluation. Machine Learning, 68(3):235–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
<author>E Velipasaoglu</author>
<author>J O Pedersen</author>
</authors>
<title>Performance thresholding in practical text classification.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM international conference on Information and knowledge management,</booktitle>
<pages>662--671</pages>
<publisher>ACM.</publisher>
<marker>Sch¨utze, Velipasaoglu, Pedersen, 2006</marker>
<rawString>H. Sch¨utze, E. Velipasaoglu, and J.O. Pedersen. 2006. Performance thresholding in practical text classification. In Proceedings of the 15th ACM international conference on Information and knowledge management, pages 662–671. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey.</title>
<date>2009</date>
<booktitle>In Computer Sciences</booktitle>
<tech>Technical Report 1648</tech>
<institution>University of Wisconsin-Madison.</institution>
<contexts>
<context position="898" citStr="Settles, 2009" startWordPosition="130" endWordPosition="131">rado.edu Abstract Active Learning (AL) is typically initialized with a small seed of examples selected randomly. However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. The evaluation is conducted in the context of word sense disambiguation. 1 Introduction Active learning (AL) (Settles, 2009) has become a popular research field due to its potential benefits: it can lead to drastic reductions in the amount of annotation that is necessary for training a highly accurate statistical classifier. Unlike in a random sampling approach, where unlabeled data is selected for annotation randomly, AL delegates the selection of unlabeled data to the classifier. In a typical AL setup, a classifier is trained on a small sample of the data (usually selected randomly), known as the seed examples. The classifier is subsequently applied to a pool of unlabeled data with the purpose of selecting additi</context>
</contexts>
<marker>Settles, 2009</marker>
<rawString>Burr Settles. 2009. Active learning literature survey. In Computer Sciences Technical Report 1648 University of Wisconsin-Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado.,</location>
<contexts>
<context position="6000" citStr="Stolcke, 2002" startWordPosition="974" endWordPosition="975"> authors provided a lim- each verb we conduct an experiment in which we ited evaluation of this technique: they looked at its select the instances of this verb using both methods. effectiveness only at a single selection size. We pro- We measure the recall of the rare sense, which we vide a more convincing evaluation in which the ef- calculate as the ratio of the number of selected rare fectiveness of this approach is examined for all sizes sense examples to the total number of rare sense exof the selected data. amples for this verb. Seed Selection for AL is typically done ran- We train a LM (Stolcke, 2002) on the corpora domly. However, for datasets with a skewed dis- from which OntoNotes data originates: the Wall tribution of classes, rare class examples may end Street Journal, English Broadcast News, English up being underrepresented. We propose to use LM Conversation, and the Brown corpus. For each verb, sampling for seed selection, which captures more we compute the LM probability for each instance of examples of rare classes than random selection, thus this verb and sort the instances by probability. In leading to a faster learning progress. the course of the experiment, we select one exam</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In International Conference on Spoken Language Processing, Denver, Colorado., pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Florian Laws</author>
<author>Udo Hahn</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>On proper unit selection in active learning: co-selection effects for named entity recognition.</title>
<date>2009</date>
<booktitle>In HLT ’09: Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing,</booktitle>
<pages>9--17</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Tomanek, Laws, Hahn, Sch¨utze, 2009</marker>
<rawString>Katrin Tomanek, Florian Laws, Udo Hahn, and Hinrich Sch¨utze. 2009. On proper unit selection in active learning: co-selection effects for named entity recognition. In HLT ’09: Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing, pages 9–17, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>R Belvin</author>
<author>S Pradan</author>
<author>L Ramshaw</author>
<author>N Xue</author>
</authors>
<title>OntoNotes: A Large Training Corpus for Enhanced Processing, chapter in Global Automatic Language Exploitation,</title>
<date>2009</date>
<pages>54--63</pages>
<publisher>Springer Verglag.</publisher>
<contexts>
<context position="7250" citStr="Weischedel et al., 2009" startWordPosition="1177" endWordPosition="1181">smallest probability and move it to the set 3.1 Data of selected examples. We then measure the recall of For our evaluation, we needed a dataset that is the rare sense for the selected examples. We concharacterized by a skewed class distribution. This tinue in this fashion until all the examples have been phenomenon is pervasive in word sense data. A selected. We use random sampling as a baseline, large word sense annotated corpus has recently which is obtained by continuously selecting a single been released by the OntoNotes (Hovy et al., 2006; example randomly. We continue until all the examWeischedel et al., 2009) project. For clarity of eval- ples have been selected. At the end of the experuation, we identify a set of verbs that satisfy three iment, we have produced two recall curves, which criteria: (1) the number of senses is two, (2) the measure the recall of the rare sense retrieval for this number of annotated examples is at least 100, (3) the verb at various sizes of selected data. Due to the proportion of the rare sense is at most 20%. The fol- lack of space, we do not show the plots that display lowing 25 verbs satisfy these criteria: account, add, these curves for individual verbs. Instead, i</context>
</contexts>
<marker>Weischedel, Hovy, Marcus, Palmer, Belvin, Pradan, Ramshaw, Xue, 2009</marker>
<rawString>R. Weischedel, E. Hovy, M. Marcus, M. Palmer, R Belvin, S Pradan, L. Ramshaw, and N. Xue, 2009. OntoNotes: A Large Training Corpus for Enhanced Processing, chapter in Global Automatic Language Exploitation, pages 54–63. Springer Verglag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G M Weiss</author>
</authors>
<title>Learning with rare cases and small disjuncts.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<pages>558--565</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="11990" citStr="Weiss, 1995" startWordPosition="2005" endWordPosition="2006">ieved with less labeled data. For example, in our case, classic AL reaches the best random sampling accuracy with only about 5% of the data. However, it is interesting to notice that LM-seeded AL actually reaches a higher accuracy, 95%, during early stages of learning (at 15% of the total training set size). We believe this phenomenon takes place due to overfitting the predominant class: as the model receives new data (and therefore more and more examples of the predominant class), it begins to mislabel more and more examples of the rare class. A similar idea has been expressed in literature (Weiss, 1995; Kubat and Matwin, 1997; Japkowicz, 2001; Weiss, 2004; Chen et al., 2006), however it has never been verified in the context of AL. To verify our hypothesis, we conduct an experiment. The experimental setup is the same as in section 3.3. However, instead of measuring the accuracy on the test set, we resort to different metrics that reflect how accurately the classifier labels the instances of the rare class in the held out test set. These metrics are the recall and precision for the rare class. Recall is the ratio of the correctly labeled examples of the rare class and the total number of ins</context>
</contexts>
<marker>Weiss, 1995</marker>
<rawString>G.M. Weiss. 1995. Learning with rare cases and small disjuncts. In Proceedings of the Twelfth International Conference on Machine Learning, pages 558–565. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G M Weiss</author>
</authors>
<title>Mining with rarity: a unifying framework.</title>
<date>2004</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="12044" citStr="Weiss, 2004" startWordPosition="2013" endWordPosition="2014">, classic AL reaches the best random sampling accuracy with only about 5% of the data. However, it is interesting to notice that LM-seeded AL actually reaches a higher accuracy, 95%, during early stages of learning (at 15% of the total training set size). We believe this phenomenon takes place due to overfitting the predominant class: as the model receives new data (and therefore more and more examples of the predominant class), it begins to mislabel more and more examples of the rare class. A similar idea has been expressed in literature (Weiss, 1995; Kubat and Matwin, 1997; Japkowicz, 2001; Weiss, 2004; Chen et al., 2006), however it has never been verified in the context of AL. To verify our hypothesis, we conduct an experiment. The experimental setup is the same as in section 3.3. However, instead of measuring the accuracy on the test set, we resort to different metrics that reflect how accurately the classifier labels the instances of the rare class in the held out test set. These metrics are the recall and precision for the rare class. Recall is the ratio of the correctly labeled examples of the rare class and the total number of instances of the rare class. Precision is the ratio of th</context>
</contexts>
<marker>Weiss, 2004</marker>
<rawString>G.M. Weiss. 2004. Mining with rarity: a unifying framework. ACM SIGKDD Explorations Newsletter, 6(1):7–19.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>