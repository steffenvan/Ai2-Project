<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000737">
<title confidence="0.995561">
Towards Relational POMDPs for Adaptive Dialogue Management
</title>
<author confidence="0.928831">
Pierre Lison
</author>
<affiliation confidence="0.690428666666667">
Language Technology Lab
German Research Centre for Artificial Intelligence (DFKI GmbH)
Saarbr¨ucken, Germany
</affiliation>
<sectionHeader confidence="0.978968" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819428571429">
Open-ended spoken interactions are typi-
cally characterised by both structural com-
plexity and high levels of uncertainty,
making dialogue management in such set-
tings a particularly challenging problem.
Traditional approaches have focused on
providing theoretical accounts for either
the uncertainty or the complexity of spo-
ken dialogue, but rarely considered the
two issues simultaneously. This paper de-
scribes ongoing work on a new approach
to dialogue management which attempts
to fill this gap. We represent the interac-
tion as a Partially Observable Markov De-
cision Process (POMDP) over a rich state
space incorporating both dialogue, user,
and environment models. The tractability
of the resulting POMDP can be preserved
using a mechanism for dynamically con-
straining the action space based on prior
knowledge over locally relevant dialogue
structures. These constraints are encoded
in a small set of general rules expressed as
a Markov Logic network. The first-order
expressivity of Markov Logic enables us
to leverage the rich relational structure of
the problem and efficiently abstract over
large regions of the state and action spaces.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999631">
The development of spoken dialogue systems for
rich, open-ended interactions raises a number of
challenges, one of which is dialogue management.
The role of dialogue management is to determine
which communicative actions to take (i.e. what to
say) given a goal and particular observations about
the interaction and the current situation.
Dialogue managers have to face several issues.
First, spoken dialogue systems must usually deal
with high levels of noise and uncertainty. These
uncertainties may arise from speech recognition
errors, limited grammar coverage, or from various
linguistic and pragmatic ambiguities.
Second, open-ended dialogue is characteristi-
cally complex, and exhibits rich relational struc-
tures. Natural interactions should be adaptive to
a variety of factors dependent on the interaction
history, the general context, and the user prefer-
ences. As a consequence, the state space necessary
to model the dynamics of the environment tends to
be large and sparsely populated.
These two problems have typically been ad-
dressed separately in the literature. On the one
hand, the issue of uncertainty in speech under-
standing is usually dealt using a range of proba-
bilistic models combined with decision-theoretic
planning. Among these, Partially Observable
Markov Decision Process (POMDP) models have
recently emerged as a unifying mathematical
framework for dialogue management (Williams
and Young, 2007; Lemon and Pietquin, 2007).
POMDPs provide an explicit account for a wide
range of uncertainties related to partial observabil-
ity (noisy, incomplete spoken inputs) and stochas-
tic action effects (the world may evolve in unpre-
dictable ways after executing an action).
On the other hand, structural complexity is
typically addressed with logic-based approaches.
Some investigated topics in this paradigm are
pragmatic interpretation (Thomason et al., 2006),
dialogue structure (Asher and Lascarides, 2003),
or collaborative planning (Kruijff et al., 2008).
These approaches are able to model sophisticated
dialogue behaviours, but at the expense of robust-
ness and adaptivity. They generally assume com-
plete observability and provide only a very limited
account (if any) of uncertainties.
We are currently developing an hybrid approach
which simultaneously tackles the uncertainty and
complexity of dialogue management, based on a
</bodyText>
<page confidence="0.992047">
7
</page>
<note confidence="0.605012">
Proceedings of the ACL 2010 Student Research Workshop, pages 7–12,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999596633333333">
POMDP framework. We present here our ongo-
ing work on this issue. In this paper, we more
specifically describe a new mechanism for dy-
namically constraining the space of possible ac-
tions available at a given time. Our aim is to use
such mechanism to significantly reduce the search
space and therefore make the planning problem
globally more tractable. This is performed in two
consecutive steps. We first structure the state space
using Markov Logic Networks, a first-order prob-
abilistic language. Prior pragmatic knowledge
about dialogue structure is then exploited to derive
the set of dialogue actions which are locally ad-
missible or relevant, and prune all irrelevant ones.
The first-order expressivity of Markov Logic Net-
works allows us to easily specify the constraints
via a small set of general rules which abstract over
large regions of the state and action spaces.
Our long-term goal is to develop an unified
framework for adaptive dialogue management in
rich, open-ended interactional settings.
This paper is structured as follows. Section 2
lays down the formal foundations of our work,
by describing dialogue management as a POMDP
problem. We then describe in Section 3 our ap-
proach to POMDP planning with control knowl-
edge using Markov Logic rules. Section 4 dis-
cusses some further aspects of our approach and
its relation to existing work, followed by the con-
clusion in Section 5.
</bodyText>
<sectionHeader confidence="0.998145" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999344">
2.1 Partially Observable Markov Decision
Processes (POMDPs)
</subsectionHeader>
<bodyText confidence="0.997935272727273">
POMDPs are a mathematical model for sequential
decision-making in partially observable environ-
ments. It provides a powerful framework for con-
trol problems which combine partial observability,
uncertain action effects, incomplete knowledge of
the environment dynamics and multiple, poten-
tially conflicting objectives.
Via reinforcement learning, it is possible to
automatically learn near-optimal action policies
given a POMDP model combined with real or sim-
ulated user data (Schatzmann et al., 2007).
</bodyText>
<subsectionHeader confidence="0.544872">
2.1.1 Formal definition
</subsectionHeader>
<bodyText confidence="0.949431">
A POMDP is a tuple (S, A, i, T, Q, R), where:
</bodyText>
<listItem confidence="0.991995666666667">
• S is the state space, which is the model of
the world from the agent’s viewpoint. It is
defined as a set of mutually exclusive states.
</listItem>
<figureCaption confidence="0.951007">
Figure 1: Bayesian decision network correspond-
</figureCaption>
<bodyText confidence="0.878755111111111">
ing to the POMDP model. Hidden variables are
greyed. Actions are represented as rectangles to
stress that they are system actions rather than ob-
served variables. Arcs into circular nodes express
influence, whereas arcs into squared nodes are in-
formational. For readability, only one state is
shown at each time step, but it should be noted
that the policy π is function of the full belief state
rather than a single (unobservable) state.
</bodyText>
<listItem confidence="0.9446656">
• A is the action space: the set of possible ac-
tions at the disposal of the agent.
• i is the observation space: the set of obser-
vations which can be captured by the agent.
They correspond to features of the environ-
ment which can be directly perceived by the
agent’s sensors.
• T is the transition function, defined as T :
S x A x S → [0, 1], where T (s, a, s&apos;) =
P(s&apos;|s, a) is the probability of reaching state
s&apos; from state s if action a is performed.
• Q is the observation function, defined as
Q : i x A x S → [0, 1], with Q(z, a, s&apos;) =
P(z|a, s&apos;), i.e. the probability of observing z
after performing a and being now in state s&apos;.
• R is the reward function, defined as R :
S x A → R, R(s, a) encodes the utility for
the agent to perform the action a while in
state s. It is therefore a model for the goals or
preferences of the agent.
</listItem>
<bodyText confidence="0.99516725">
A graphical illustration of a POMDP model as
a Bayesian decision network is provided in Fig. 1.
In addition, a POMDP can include additional
parameters such as the horizon of the agent (num-
</bodyText>
<equation confidence="0.543969454545455">
r(at, st) r(at+1, st+1)
t
st
st+1 st+2
π
π
at
at+1
zt
zt+1
zt+2
</equation>
<page confidence="0.956314">
8
</page>
<bodyText confidence="0.982563">
ber of look-ahead steps), and the discount factor
(weighting scheme for non-immediate rewards).
</bodyText>
<subsubsectionHeader confidence="0.670219">
2.1.2 Beliefs and belief update
</subsubsectionHeader>
<bodyText confidence="0.999978857142857">
A key idea of POMDP is the assumption that the
state of the world is not directly accessible, and
can only be inferred via observation. Such uncer-
tainty is expressed in the belief state b, which is
a probability distribution over possible states, that
is: b : S —* [0, 1]. The belief state for a state
space of cardinality n is therefore represented in a
real-valued simplex of dimension (n−1).
This belief state is dynamically updated before
executing each action. The belief state update op-
erates as follows. At a given time step t, the agent
is in some unobserved state st = s E S. The
probability of being in state s at time t is writ-
ten as bt(s). Based on the current belief state bt,
the agent selects an action at, receives a reward
R(s, at) and transitions to a new (unobserved)
state st+1 = s0 , where st+1 depends only on st
and at. The agent then receives a new observation
ot+1 which is dependent on st+1 and at.
Finally, the belief distribution bt is updated,
based on ot+1 and at as follows1.
</bodyText>
<equation confidence="0.992892666666667">
bt+1(s0)= P(s0|ot+1, at, bt) (1)
P(ot+1|s0, at, bt)P(s0|at, bt)
= (2)
P(ot+1|at, bt)
P(ot+1|s0, at) Es∈S P(s0|at, s)P(s|at, bt)
= (3)
P(ot+1|at, bt)
�= α Ω(ot+1, s0, at) T(s, at, s0)bt(s) (4)
s∈S
</equation>
<bodyText confidence="0.999777333333333">
where α is a normalisation constant. An initial
belief state b0 must be specified at runtime as a
POMDP parameter when initialising the system.
</bodyText>
<subsectionHeader confidence="0.71773">
2.1.3 POMDP policies
</subsectionHeader>
<bodyText confidence="0.999273125">
Given a POMDP model (S, A, Z, T, Z, R), the
agent should execute at each time-step the action
which maximises its expected cumulative reward
over the horizon. The function π : B —* A defines
a policy, which determines the action to perform
for each point of the belief space.
The expected reward for policy π starting from
belief b is defined as:
</bodyText>
<equation confidence="0.970102">
h
Jπ(b) = E I t=0 ]γtR(st, at)  |b, π (5)
</equation>
<footnote confidence="0.919033">
1As a notational shorthand, we write P(st=s) as P(s)
and P(st+1=s&apos;) as P(s).
</footnote>
<bodyText confidence="0.991645">
The optimal policy π∗ is then obtained by optimiz-
ing the long-term reward, starting from b0:
</bodyText>
<equation confidence="0.9994905">
π∗ = argmax
π Jπ(b0) (6)
</equation>
<bodyText confidence="0.99938">
The optimal policy π∗ yields the highest expected
reward value for each possible belief state. This
value is compactly represented by the optimal
value function, noted V ∗, which is a solution to
the Bellman optimality equation (Bellman, 1957).
Numerous algorithms for (offline) policy opti-
misation and (online) planning are available. For
large spaces, exact optimisation is impossible and
approximate methods must be used, see for in-
stance grid-based (Thomson and Young, 2009) or
point-based (Pineau et al., 2006) techniques.
</bodyText>
<subsectionHeader confidence="0.986676">
2.2 POMDP-based dialogue management
</subsectionHeader>
<bodyText confidence="0.999929235294117">
Dialogue management can be easily cast as a
POMDP problem, with the state space being a
compact representation of the interaction, the ac-
tion space being a set of dialogue moves, the ob-
servation space representing speech recognition
hypotheses, the transition function defining the
dynamics of the interaction (which user reaction
is to be expected after a particular dialogue move),
and the observation function describing a “sensor
model” between observed speech recognition hy-
potheses and actual utterances. Finally, the reward
function encodes the utility of dialogue policies –
it typically assigns a big positive reward if a long-
term goal has been reached (e.g. the retrieval of
some important information), and small negative
rewards for minor “inconveniences” (e.g. prompt-
ing the user to repeat or asking for confirmations).
Our long-term aim is to apply such POMDP
framework to a rich dialogue domain for human-
robot interaction (Kruijff et al., 2010). These inter-
actions are typically open-ended, relatively long,
include high levels of noise, and require complex
state and action spaces. Furthemore, the dialogue
system also needs to be adaptive to its user (at-
tributed beliefs and intentions, attitude, attentional
state) and to the current situation (currently per-
ceived entities and events).
As a consequence, the state space must be ex-
panded to include these knowledge sources. Be-
lief monitoring is then used to continuously update
the belief state based on perceptual inputs (see
also (Bohus and Horvitz, 2009) for an overview of
techniques to extract such information). These re-
quirements can only be fullfilled if we address the
</bodyText>
<page confidence="0.989356">
9
</page>
<bodyText confidence="0.999168333333333">
“curse of dimensionality” characteristic of tradi-
tional POMDP models. The next section provides
a tentative answer.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="method">
3 Approach
</sectionHeader>
<subsectionHeader confidence="0.999788">
3.1 Control knowledge
</subsectionHeader>
<bodyText confidence="0.999978944444444">
Classical approaches to POMDP planning oper-
ate directly on the full action space and select the
next action to perform based on the maximisation
of the expected cumulative reward over the spec-
ified horizon. Such approaches can be used in
small-scale domains with a limited action space,
but quickly become intractable for larger ones, as
the planning time increases exponentially with the
size of the action space. Significant planning time
is therefore spend on actions which should be di-
rectly discarded as irrelevant2. Dismissing these
actions before planning could therefore provide
important computational gains.
Instead of a direct policy optimisation over the
full action space, our approach formalises action
selection as a two-step process. As a first step, a
set of relevant dialogue moves is constructed from
the full action space. The POMDP planner then
computes the optimal (highest-reward) action on
this reduced action space in a second step.
Such an approach is able to significantly reduce
the dimensionality of the dialogue management
problem by taking advantage of prior knowledge
about the expected relational structure of spoken
dialogue. This prior knowledge is to be encoded
in a set of general rules describing the admissible
dialogue moves in a particular situation.
How can we express such rules? POMDPs are
usually modeled with Bayesian networks which
are inherently propositional. Encoding such rules
in a propositional framework requires a distinct
rule for every possible state and action instance.
This is not a feasible approach. We therefore need
a first order (probabilistic) language able to ex-
press generalities over large regions of the state
action spaces. Markov Logic is such a language.
</bodyText>
<subsectionHeader confidence="0.999154">
3.2 Markov Logic Networks (MLNs)
</subsectionHeader>
<bodyText confidence="0.938342103448276">
Markov Logic combines first-order logic and
probabilistic graphical models in a unified repre-
sentation (Richardson and Domingos, 2006). A
2For instance, an agent hearing a user command such as
“Please take the mug on your left” might spent a lot of plan-
ning time calculating the expected future reward of dialogue
moves such as “Is the box green?” or “Your name is John”, which
are irrelevant to the situation.
Markov Logic Network L is a set of pairs (Fi, wi),
where Fi is a formula in first-order logic and wi is
a real number representing the formula weight.
A Markov Logic Network L can be seen as
a template for constructing markov networks3.
To construct a markov network from L, one has
to provide an additional set of constants C =
{c1, c2, ..., c,C,}. The resulting markov network
is called a ground markov network and is written
ML,C. The ground markov network contains one
feature for each possible grounding of a first-order
formula in L, with the corresponding weight. The
technical details of the construction of ML,C from
the two sets L and C is explained in several pa-
pers, see e.g. (Richardson and Domingos, 2006).
Once the markov network ML,C is constructed,
it can be exploited to perform inference over ar-
bitrary queries. Efficient probabilistic inference
algorithms such as Markov Chain Monte Carlo
(MCMC) or other sampling techniques can then
be used to this end (Poon and Domingos, 2006).
</bodyText>
<subsectionHeader confidence="0.994202">
3.3 States and actions as relational structures
</subsectionHeader>
<bodyText confidence="0.985332869565217">
The specification of Markov Logic rules apply-
ing over complete regions of the state and action
spaces (instead of over single instances) requires
an explicit relational structure over these spaces.
This is realised by factoring the state and ac-
tion spaces into a set of distinct, conditionally in-
dependent features. A state s can be expanded into
a tuple (f1, f2, ...fn), where each sub-state fi is
assigned a value from a set {v1, v2, ...vm}. Such
structure can be expressed in first-order logic with
a binary predicate fi(s, vj) for each sub-state fi,
where vj is the value of the sub-state fi in s. The
same type of structure can be defined over actions.
This factoring leads to a relational structure of ar-
bitrary complexity, compactly represented by a set
of unary and binary predicates.
For instance, (Young et al., 2010) factors each
dialogue state into three independent parts s =
(su, au, sd), where su is the user goal, au the last
user move, and sd the dialogue history. These
can be expressed in Markov Logic with predicates
such as UserGoal(s, su), LastUserMove(s, au),
or History(s, sd).
</bodyText>
<footnote confidence="0.940496">
3Markov networks are undirected graphical models.
</footnote>
<page confidence="0.996627">
10
</page>
<subsectionHeader confidence="0.970422">
3.4 Relevant action space
</subsectionHeader>
<bodyText confidence="0.92386">
For a given state s, the relevant action space
RelMoves(A, s) is defined as:
</bodyText>
<equation confidence="0.678473">
{a„t : a„t E A n RelevantMove(am, s)} (7)
</equation>
<bodyText confidence="0.942892545454546">
The truth-value of the predicate
RelevantMove(am, s) is determined using a
set of Markov Logic rules dependent on both the
state s and the action a„t. For a given state s,
the relevant action space is constructed via prob-
abilistic inference, by estimating the probability
P(RelevantMove(am, s)) for each action a„t,
and selecting the subset of actions for which the
probability is above a given threshold.
Eq. 8 provides a simple example of such
Markov Logic rule:
</bodyText>
<equation confidence="0.983915">
LastUserMove(s, au) n PolarQuestion(au) n
YesNoAnswer(am) —* RelevantMove(am, s) (8)
</equation>
<bodyText confidence="0.999926">
It defines an admissible dialogue move for a situ-
ation where the user asks a polar question to the
agent (e.g. “do you see my hand?”). The rule speci-
fies that, if a state s contains a,, as last user move,
and if a,, is a polar question, then an answer a„t
of type yes-no is a relevant dialogue move for the
agent. This rule is (implicitly) universally quanti-
fied over s, a,, and a„t.
Each of these Markov Logic rules has a weight
attached to it, expressing the strength of the im-
plication. A rule with infinite weight and satisfied
premises will lead to a relevant move with prob-
ability 1. Softer weights can be used to describe
moves which are less relevant but still possible in
a particular context. These weights can either be
encoded by hand or learned from data (how to per-
form this efficiently remains an open question).
</bodyText>
<subsectionHeader confidence="0.992651">
3.5 Rules application on POMDP belief state
</subsectionHeader>
<bodyText confidence="0.9999938">
The previous section assumed that the state s is
known. But the real state of a POMDP is never di-
rectly accessible. The rules we just described must
therefore be applied on the belief state. Ultimately,
we want to define a function Rel : Rn —* P(A),
which takes as input a point in the belief space
and outputs a set of relevant moves. For efficiency
reasons, this function can be precomputed offline,
by segmenting the state space into distinct regions
and assigning a set of relevant moves to each re-
gion. The function can then be directly called at
runtime by the planning algorithm.
Due to the high dimensionality of the belief
space, the above function must be approximated
to remain tractable. One way to perform this ap-
proximation is to extract, for belief state b, a set
S„t of m most likely states, and compute the set
of relevant moves for each of them. We then de-
fine the global probability estimate of a being a
relevant move given b as such:
</bodyText>
<equation confidence="0.955963">
P(RelevantMove(a)  |b, a) �
� P(RelevantMove(a, s)  |s, a) x b(s) (9)
S∈S,
</equation>
<bodyText confidence="0.9997985">
In the limit where m —* |S|, the error margin on
the approximation tends to zero.
</bodyText>
<sectionHeader confidence="0.999981" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.993708">
4.1 General comments
</subsectionHeader>
<bodyText confidence="0.999948882352941">
It is worth noting that the mechanism we just
outlined does not intend to replace the existing
POMDP planning and optimisation algorithms,
but rather complements them. Each step serves a
different purpose: the action space reduction pro-
vides an answer to the question “Is this action rel-
evant?”, while the policy optimisation seeks to an-
swer “Is this action useful?”. We believe that such
distinction between relevance and usefulness is
important and will prove to be beneficial in terms
of tractability.
It is also useful to notice that the Markov Logic
rules we described provides a “positive” definition
of the action space. The rules were applied to pro-
duce an exhaustive list of all admissible actions
given a state, all actions outside this list being de
facto labelled as non-admissible. But the rules can
also provide a “negative” definition of the action
space. That is, instead of generating an exhaustive
list of possible actions, the dialogue system can
initially consider all actions as admissible, and the
rules can then be used to prune this action space
by removing irrelevant moves.
The choice of action filter depends mainly on
the size of the dialogue domain and the availabil-
ity of prior domain knowledge. A “positive” filter
is a necessity for large dialogue domains, as the
action space is likely to grow exponentially with
the domain size and become untractable. But the
positive definition of the action space is also sig-
nificantly more expensive for the dialogue devel-
oper. There is therefore a trade-off between the
costs of tractability issues, and the costs of dia-
logue domain modelling.
</bodyText>
<page confidence="0.998819">
11
</page>
<sectionHeader confidence="0.96049" genericHeader="related work">
4.2 Related Work
</sectionHeader>
<bodyText confidence="0.999991333333333">
There is a substantial body of existing work in
the POMDP literature about the exploitation of
the problem structure to tackle the curse of di-
mensionality (Poupart, 2005; Young et al., 2010),
but the vast majority of these approaches retain
a propositional structure. A few more theoreti-
cal papers also describe first-order MDPs (Wang
et al., 2007), and recent work on Markov Logic
has extended the MLN formalism to include some
decision-theoretic concepts (Nath and Domingos,
2009). To the author’s knowledge, none of these
ideas have been applied to dialogue management.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999980266666667">
This paper described a new approach to exploit re-
lational models of dialogue structure for control-
ling the action space in POMDPs. This approach
is part of an ongoing work to develop a unified
framework for adaptive dialogue management in
rich, open-ended interactional settings. The dia-
logue manager is being implemented as part of a
larger cognitive architecture for talking robots.
Besides the implementation, future work will
focus on refining the theoretical foundations of
relational POMDPs for dialogue (including how
to specify the transition, observation and reward
functions in such a relational framework), as well
as investigating the use of reinforcement learning
for policy optimisation based on simulated data.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999790636363637">
N. Asher and A. Lascarides. 2003. Logics of Conver-
sation. Cambridge University Press.
R. Bellman. 1957. Dynamic Programming. Princeton
University Press.
Dan Bohus and Eric Horvitz. 2009. Dialog in the open
world: platform and applications. In ICMI-MLMI
’09: Proceedings of the 2009 international confer-
ence on Multimodal interfaces, pages 31–38, New
York, NY, USA. ACM.
G.J.M. Kruijff, M. Brenner, and N.A. Hawes. 2008.
Continual planning for cross-modal situated clarifi-
cation in human-robot interaction. In Proceedings of
the 17th International Symposium on Robot and Hu-
man Interactive Communication (RO-MAN 2008),
Munich, Germany.
G.-J. M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson,
H. Zender, and I. Kruijff-Korbayova. 2010. Situated
dialogue processing for human-robot interaction. In
H. I. Christensen, A. Sloman, G.-J. M. Kruijff, and
J. Wyatt, editors, Cognitive Systems. Springer Ver-
lag. (in press).
O. Lemon and O. Pietquin. 2007. Machine learn-
ing for spoken dialogue systems. In Proceedings
of the European Conference on Speech Commu-
nication and Technologies (Interspeech’07), pages
2685–2688, Anvers (Belgium), August.
A. Nath and P. Domingos. 2009. A language for rela-
tional decision theory. In Proceedings of the Inter-
national Workshop on Statistical Relational Learn-
ing.
J. Pineau, G. Gordon, and S. Thrun. 2006. Anytime
point-based approximations for large pomdps. Arti-
ficial Intelligence Research, 27(1):335–380.
H. Poon and P. Domingos. 2006. Sound and effi-
cient inference with probabilistic and deterministic
dependencies. In AAAI’06: Proceedings of the 21st
national conference on Artificial intelligence, pages
458–463. AAAI Press.
P. Poupart. 2005. Exploiting structure to efficiently
solve large scale partially observable markov deci-
sion processes. Ph.D. thesis, University of Toronto,
Toronto, Canada.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1-2):107–136.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a POMDP dia-
logue system. In HLT ’07: Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies, pages 149–152, Rochester, New York, April.
Association for Computational Linguistics.
R. Thomason, M. Stone, and D. DeVault. 2006. En-
lightened update: A computational architecture for
presupposition and other pragmatic phenomena. In
Donna Byron, Craige Roberts, and Scott Schwenter,
editors, Presupposition Accommodation. Ohio State
Pragmatics Initiative.
B. Thomson and S. Young. 2009. Bayesian update
of dialogue state: A pomdp framework for spoken
dialogue systems. Computer Speech &amp; Language,
August.
Ch. Wang, S. Joshi, and R. Khardon. 2007. First order
decision diagrams for relational mdps. In IJCAI’07:
Proceedings of the 20th international joint confer-
ence on Artifical intelligence, pages 1095–1100, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
J. Williams and S. Young. 2007. Partially observable
markov decision processes for spoken dialog sys-
tems. Computer Speech and Language, 21(2):231–
422.
S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
pomdp-based spoken dialogue management. Com-
puter Speech &amp; Language, 24(2):150–174.
</reference>
<page confidence="0.998448">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.971300">
<title confidence="0.999618">Towards Relational POMDPs for Adaptive Dialogue Management</title>
<author confidence="0.996653">Pierre Lison</author>
<affiliation confidence="0.99445">Language Technology Lab German Research Centre for Artificial Intelligence (DFKI GmbH)</affiliation>
<address confidence="0.987755">Saarbr¨ucken, Germany</address>
<abstract confidence="0.999892586206896">Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions of the state and action spaces.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>A Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3271" citStr="Asher and Lascarides, 2003" startWordPosition="475" endWordPosition="478"> (POMDP) models have recently emerged as a unifying mathematical framework for dialogue management (Williams and Young, 2007; Lemon and Pietquin, 2007). POMDPs provide an explicit account for a wide range of uncertainties related to partial observability (noisy, incomplete spoken inputs) and stochastic action effects (the world may evolve in unpredictable ways after executing an action). On the other hand, structural complexity is typically addressed with logic-based approaches. Some investigated topics in this paradigm are pragmatic interpretation (Thomason et al., 2006), dialogue structure (Asher and Lascarides, 2003), or collaborative planning (Kruijff et al., 2008). These approaches are able to model sophisticated dialogue behaviours, but at the expense of robustness and adaptivity. They generally assume complete observability and provide only a very limited account (if any) of uncertainties. We are currently developing an hybrid approach which simultaneously tackles the uncertainty and complexity of dialogue management, based on a 7 Proceedings of the ACL 2010 Student Research Workshop, pages 7–12, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics POMDP framework. We presen</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>N. Asher and A. Lascarides. 2003. Logics of Conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bellman</author>
</authors>
<title>Dynamic Programming.</title>
<date>1957</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="9921" citStr="Bellman, 1957" startWordPosition="1630" endWordPosition="1631">mines the action to perform for each point of the belief space. The expected reward for policy π starting from belief b is defined as: h Jπ(b) = E I t=0 ]γtR(st, at) |b, π (5) 1As a notational shorthand, we write P(st=s) as P(s) and P(st+1=s&apos;) as P(s). The optimal policy π∗ is then obtained by optimizing the long-term reward, starting from b0: π∗ = argmax π Jπ(b0) (6) The optimal policy π∗ yields the highest expected reward value for each possible belief state. This value is compactly represented by the optimal value function, noted V ∗, which is a solution to the Bellman optimality equation (Bellman, 1957). Numerous algorithms for (offline) policy optimisation and (online) planning are available. For large spaces, exact optimisation is impossible and approximate methods must be used, see for instance grid-based (Thomson and Young, 2009) or point-based (Pineau et al., 2006) techniques. 2.2 POMDP-based dialogue management Dialogue management can be easily cast as a POMDP problem, with the state space being a compact representation of the interaction, the action space being a set of dialogue moves, the observation space representing speech recognition hypotheses, the transition function defining t</context>
</contexts>
<marker>Bellman, 1957</marker>
<rawString>R. Bellman. 1957. Dynamic Programming. Princeton University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Eric Horvitz</author>
</authors>
<title>Dialog in the open world: platform and applications. In</title>
<date>2009</date>
<booktitle>ICMI-MLMI ’09: Proceedings of the 2009 international conference on Multimodal interfaces,</booktitle>
<pages>31--38</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11767" citStr="Bohus and Horvitz, 2009" startWordPosition="1911" endWordPosition="1914">e domain for humanrobot interaction (Kruijff et al., 2010). These interactions are typically open-ended, relatively long, include high levels of noise, and require complex state and action spaces. Furthemore, the dialogue system also needs to be adaptive to its user (attributed beliefs and intentions, attitude, attentional state) and to the current situation (currently perceived entities and events). As a consequence, the state space must be expanded to include these knowledge sources. Belief monitoring is then used to continuously update the belief state based on perceptual inputs (see also (Bohus and Horvitz, 2009) for an overview of techniques to extract such information). These requirements can only be fullfilled if we address the 9 “curse of dimensionality” characteristic of traditional POMDP models. The next section provides a tentative answer. 3 Approach 3.1 Control knowledge Classical approaches to POMDP planning operate directly on the full action space and select the next action to perform based on the maximisation of the expected cumulative reward over the specified horizon. Such approaches can be used in small-scale domains with a limited action space, but quickly become intractable for larger</context>
</contexts>
<marker>Bohus, Horvitz, 2009</marker>
<rawString>Dan Bohus and Eric Horvitz. 2009. Dialog in the open world: platform and applications. In ICMI-MLMI ’09: Proceedings of the 2009 international conference on Multimodal interfaces, pages 31–38, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J M Kruijff</author>
<author>M Brenner</author>
<author>N A Hawes</author>
</authors>
<title>Continual planning for cross-modal situated clarification in human-robot interaction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Symposium on Robot and Human Interactive Communication</booktitle>
<location>RO-MAN</location>
<contexts>
<context position="3321" citStr="Kruijff et al., 2008" startWordPosition="482" endWordPosition="485">ematical framework for dialogue management (Williams and Young, 2007; Lemon and Pietquin, 2007). POMDPs provide an explicit account for a wide range of uncertainties related to partial observability (noisy, incomplete spoken inputs) and stochastic action effects (the world may evolve in unpredictable ways after executing an action). On the other hand, structural complexity is typically addressed with logic-based approaches. Some investigated topics in this paradigm are pragmatic interpretation (Thomason et al., 2006), dialogue structure (Asher and Lascarides, 2003), or collaborative planning (Kruijff et al., 2008). These approaches are able to model sophisticated dialogue behaviours, but at the expense of robustness and adaptivity. They generally assume complete observability and provide only a very limited account (if any) of uncertainties. We are currently developing an hybrid approach which simultaneously tackles the uncertainty and complexity of dialogue management, based on a 7 Proceedings of the ACL 2010 Student Research Workshop, pages 7–12, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics POMDP framework. We present here our ongoing work on this issue. In this pap</context>
</contexts>
<marker>Kruijff, Brenner, Hawes, 2008</marker>
<rawString>G.J.M. Kruijff, M. Brenner, and N.A. Hawes. 2008. Continual planning for cross-modal situated clarification in human-robot interaction. In Proceedings of the 17th International Symposium on Robot and Human Interactive Communication (RO-MAN 2008), Munich, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G-J M Kruijff</author>
<author>P Lison</author>
<author>T Benjamin</author>
<author>H Jacobsson</author>
<author>H Zender</author>
<author>I Kruijff-Korbayova</author>
</authors>
<title>Situated dialogue processing for human-robot interaction.</title>
<date>2010</date>
<editor>In H. I. Christensen, A. Sloman, G.-J. M. Kruijff, and J. Wyatt, editors, Cognitive Systems.</editor>
<publisher>Springer Verlag. (in press).</publisher>
<contexts>
<context position="11201" citStr="Kruijff et al., 2010" startWordPosition="1823" endWordPosition="1826">e expected after a particular dialogue move), and the observation function describing a “sensor model” between observed speech recognition hypotheses and actual utterances. Finally, the reward function encodes the utility of dialogue policies – it typically assigns a big positive reward if a longterm goal has been reached (e.g. the retrieval of some important information), and small negative rewards for minor “inconveniences” (e.g. prompting the user to repeat or asking for confirmations). Our long-term aim is to apply such POMDP framework to a rich dialogue domain for humanrobot interaction (Kruijff et al., 2010). These interactions are typically open-ended, relatively long, include high levels of noise, and require complex state and action spaces. Furthemore, the dialogue system also needs to be adaptive to its user (attributed beliefs and intentions, attitude, attentional state) and to the current situation (currently perceived entities and events). As a consequence, the state space must be expanded to include these knowledge sources. Belief monitoring is then used to continuously update the belief state based on perceptual inputs (see also (Bohus and Horvitz, 2009) for an overview of techniques to </context>
</contexts>
<marker>Kruijff, Lison, Benjamin, Jacobsson, Zender, Kruijff-Korbayova, 2010</marker>
<rawString>G.-J. M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, H. Zender, and I. Kruijff-Korbayova. 2010. Situated dialogue processing for human-robot interaction. In H. I. Christensen, A. Sloman, G.-J. M. Kruijff, and J. Wyatt, editors, Cognitive Systems. Springer Verlag. (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>O Pietquin</author>
</authors>
<title>Machine learning for spoken dialogue systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the European Conference on Speech Communication and Technologies (Interspeech’07),</booktitle>
<pages>2685--2688</pages>
<location>Anvers (Belgium),</location>
<contexts>
<context position="2795" citStr="Lemon and Pietquin, 2007" startWordPosition="407" endWordPosition="410">the general context, and the user preferences. As a consequence, the state space necessary to model the dynamics of the environment tends to be large and sparsely populated. These two problems have typically been addressed separately in the literature. On the one hand, the issue of uncertainty in speech understanding is usually dealt using a range of probabilistic models combined with decision-theoretic planning. Among these, Partially Observable Markov Decision Process (POMDP) models have recently emerged as a unifying mathematical framework for dialogue management (Williams and Young, 2007; Lemon and Pietquin, 2007). POMDPs provide an explicit account for a wide range of uncertainties related to partial observability (noisy, incomplete spoken inputs) and stochastic action effects (the world may evolve in unpredictable ways after executing an action). On the other hand, structural complexity is typically addressed with logic-based approaches. Some investigated topics in this paradigm are pragmatic interpretation (Thomason et al., 2006), dialogue structure (Asher and Lascarides, 2003), or collaborative planning (Kruijff et al., 2008). These approaches are able to model sophisticated dialogue behaviours, bu</context>
</contexts>
<marker>Lemon, Pietquin, 2007</marker>
<rawString>O. Lemon and O. Pietquin. 2007. Machine learning for spoken dialogue systems. In Proceedings of the European Conference on Speech Communication and Technologies (Interspeech’07), pages 2685–2688, Anvers (Belgium), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nath</author>
<author>P Domingos</author>
</authors>
<title>A language for relational decision theory.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Workshop on Statistical Relational Learning.</booktitle>
<contexts>
<context position="21244" citStr="Nath and Domingos, 2009" startWordPosition="3499" endWordPosition="3502">per. There is therefore a trade-off between the costs of tractability issues, and the costs of dialogue domain modelling. 11 4.2 Related Work There is a substantial body of existing work in the POMDP literature about the exploitation of the problem structure to tackle the curse of dimensionality (Poupart, 2005; Young et al., 2010), but the vast majority of these approaches retain a propositional structure. A few more theoretical papers also describe first-order MDPs (Wang et al., 2007), and recent work on Markov Logic has extended the MLN formalism to include some decision-theoretic concepts (Nath and Domingos, 2009). To the author’s knowledge, none of these ideas have been applied to dialogue management. 5 Conclusions This paper described a new approach to exploit relational models of dialogue structure for controlling the action space in POMDPs. This approach is part of an ongoing work to develop a unified framework for adaptive dialogue management in rich, open-ended interactional settings. The dialogue manager is being implemented as part of a larger cognitive architecture for talking robots. Besides the implementation, future work will focus on refining the theoretical foundations of relational POMDP</context>
</contexts>
<marker>Nath, Domingos, 2009</marker>
<rawString>A. Nath and P. Domingos. 2009. A language for relational decision theory. In Proceedings of the International Workshop on Statistical Relational Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pineau</author>
<author>G Gordon</author>
<author>S Thrun</author>
</authors>
<title>Anytime point-based approximations for large pomdps.</title>
<date>2006</date>
<journal>Artificial Intelligence Research,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="10193" citStr="Pineau et al., 2006" startWordPosition="1668" endWordPosition="1671">olicy π∗ is then obtained by optimizing the long-term reward, starting from b0: π∗ = argmax π Jπ(b0) (6) The optimal policy π∗ yields the highest expected reward value for each possible belief state. This value is compactly represented by the optimal value function, noted V ∗, which is a solution to the Bellman optimality equation (Bellman, 1957). Numerous algorithms for (offline) policy optimisation and (online) planning are available. For large spaces, exact optimisation is impossible and approximate methods must be used, see for instance grid-based (Thomson and Young, 2009) or point-based (Pineau et al., 2006) techniques. 2.2 POMDP-based dialogue management Dialogue management can be easily cast as a POMDP problem, with the state space being a compact representation of the interaction, the action space being a set of dialogue moves, the observation space representing speech recognition hypotheses, the transition function defining the dynamics of the interaction (which user reaction is to be expected after a particular dialogue move), and the observation function describing a “sensor model” between observed speech recognition hypotheses and actual utterances. Finally, the reward function encodes the</context>
</contexts>
<marker>Pineau, Gordon, Thrun, 2006</marker>
<rawString>J. Pineau, G. Gordon, and S. Thrun. 2006. Anytime point-based approximations for large pomdps. Artificial Intelligence Research, 27(1):335–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Sound and efficient inference with probabilistic and deterministic dependencies.</title>
<date>2006</date>
<booktitle>In AAAI’06: Proceedings of the 21st national conference on Artificial intelligence,</booktitle>
<pages>458--463</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="15210" citStr="Poon and Domingos, 2006" startWordPosition="2467" endWordPosition="2470">ork is called a ground markov network and is written ML,C. The ground markov network contains one feature for each possible grounding of a first-order formula in L, with the corresponding weight. The technical details of the construction of ML,C from the two sets L and C is explained in several papers, see e.g. (Richardson and Domingos, 2006). Once the markov network ML,C is constructed, it can be exploited to perform inference over arbitrary queries. Efficient probabilistic inference algorithms such as Markov Chain Monte Carlo (MCMC) or other sampling techniques can then be used to this end (Poon and Domingos, 2006). 3.3 States and actions as relational structures The specification of Markov Logic rules applying over complete regions of the state and action spaces (instead of over single instances) requires an explicit relational structure over these spaces. This is realised by factoring the state and action spaces into a set of distinct, conditionally independent features. A state s can be expanded into a tuple (f1, f2, ...fn), where each sub-state fi is assigned a value from a set {v1, v2, ...vm}. Such structure can be expressed in first-order logic with a binary predicate fi(s, vj) for each sub-state </context>
</contexts>
<marker>Poon, Domingos, 2006</marker>
<rawString>H. Poon and P. Domingos. 2006. Sound and efficient inference with probabilistic and deterministic dependencies. In AAAI’06: Proceedings of the 21st national conference on Artificial intelligence, pages 458–463. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Poupart</author>
</authors>
<title>Exploiting structure to efficiently solve large scale partially observable markov decision processes.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Toronto,</institution>
<location>Toronto, Canada.</location>
<contexts>
<context position="20931" citStr="Poupart, 2005" startWordPosition="3452" endWordPosition="3453">bility of prior domain knowledge. A “positive” filter is a necessity for large dialogue domains, as the action space is likely to grow exponentially with the domain size and become untractable. But the positive definition of the action space is also significantly more expensive for the dialogue developer. There is therefore a trade-off between the costs of tractability issues, and the costs of dialogue domain modelling. 11 4.2 Related Work There is a substantial body of existing work in the POMDP literature about the exploitation of the problem structure to tackle the curse of dimensionality (Poupart, 2005; Young et al., 2010), but the vast majority of these approaches retain a propositional structure. A few more theoretical papers also describe first-order MDPs (Wang et al., 2007), and recent work on Markov Logic has extended the MLN formalism to include some decision-theoretic concepts (Nath and Domingos, 2009). To the author’s knowledge, none of these ideas have been applied to dialogue management. 5 Conclusions This paper described a new approach to exploit relational models of dialogue structure for controlling the action space in POMDPs. This approach is part of an ongoing work to develop</context>
</contexts>
<marker>Poupart, 2005</marker>
<rawString>P. Poupart. 2005. Exploiting structure to efficiently solve large scale partially observable markov decision processes. Ph.D. thesis, University of Toronto, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="13934" citStr="Richardson and Domingos, 2006" startWordPosition="2244" endWordPosition="2247">in a particular situation. How can we express such rules? POMDPs are usually modeled with Bayesian networks which are inherently propositional. Encoding such rules in a propositional framework requires a distinct rule for every possible state and action instance. This is not a feasible approach. We therefore need a first order (probabilistic) language able to express generalities over large regions of the state action spaces. Markov Logic is such a language. 3.2 Markov Logic Networks (MLNs) Markov Logic combines first-order logic and probabilistic graphical models in a unified representation (Richardson and Domingos, 2006). A 2For instance, an agent hearing a user command such as “Please take the mug on your left” might spent a lot of planning time calculating the expected future reward of dialogue moves such as “Is the box green?” or “Your name is John”, which are irrelevant to the situation. Markov Logic Network L is a set of pairs (Fi, wi), where Fi is a formula in first-order logic and wi is a real number representing the formula weight. A Markov Logic Network L can be seen as a template for constructing markov networks3. To construct a markov network from L, one has to provide an additional set of constant</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov logic networks. Machine Learning, 62(1-2):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Karl Weilhammer</author>
<author>Hui Ye</author>
<author>Steve Young</author>
</authors>
<title>Agenda-based user simulation for bootstrapping a POMDP dialogue system.</title>
<date>2007</date>
<booktitle>In HLT ’07: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies,</booktitle>
<pages>149--152</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="5812" citStr="Schatzmann et al., 2007" startWordPosition="864" endWordPosition="867">ting work, followed by the conclusion in Section 5. 2 Background 2.1 Partially Observable Markov Decision Processes (POMDPs) POMDPs are a mathematical model for sequential decision-making in partially observable environments. It provides a powerful framework for control problems which combine partial observability, uncertain action effects, incomplete knowledge of the environment dynamics and multiple, potentially conflicting objectives. Via reinforcement learning, it is possible to automatically learn near-optimal action policies given a POMDP model combined with real or simulated user data (Schatzmann et al., 2007). 2.1.1 Formal definition A POMDP is a tuple (S, A, i, T, Q, R), where: • S is the state space, which is the model of the world from the agent’s viewpoint. It is defined as a set of mutually exclusive states. Figure 1: Bayesian decision network corresponding to the POMDP model. Hidden variables are greyed. Actions are represented as rectangles to stress that they are system actions rather than observed variables. Arcs into circular nodes express influence, whereas arcs into squared nodes are informational. For readability, only one state is shown at each time step, but it should be noted that </context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007. Agenda-based user simulation for bootstrapping a POMDP dialogue system. In HLT ’07: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies, pages 149–152, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Thomason</author>
<author>M Stone</author>
<author>D DeVault</author>
</authors>
<title>Enlightened update: A computational architecture for presupposition and other pragmatic phenomena.</title>
<date>2006</date>
<editor>In Donna Byron, Craige Roberts, and Scott Schwenter, editors,</editor>
<contexts>
<context position="3222" citStr="Thomason et al., 2006" startWordPosition="469" endWordPosition="472">Partially Observable Markov Decision Process (POMDP) models have recently emerged as a unifying mathematical framework for dialogue management (Williams and Young, 2007; Lemon and Pietquin, 2007). POMDPs provide an explicit account for a wide range of uncertainties related to partial observability (noisy, incomplete spoken inputs) and stochastic action effects (the world may evolve in unpredictable ways after executing an action). On the other hand, structural complexity is typically addressed with logic-based approaches. Some investigated topics in this paradigm are pragmatic interpretation (Thomason et al., 2006), dialogue structure (Asher and Lascarides, 2003), or collaborative planning (Kruijff et al., 2008). These approaches are able to model sophisticated dialogue behaviours, but at the expense of robustness and adaptivity. They generally assume complete observability and provide only a very limited account (if any) of uncertainties. We are currently developing an hybrid approach which simultaneously tackles the uncertainty and complexity of dialogue management, based on a 7 Proceedings of the ACL 2010 Student Research Workshop, pages 7–12, Uppsala, Sweden, 13 July 2010. c�2010 Association for Com</context>
</contexts>
<marker>Thomason, Stone, DeVault, 2006</marker>
<rawString>R. Thomason, M. Stone, and D. DeVault. 2006. Enlightened update: A computational architecture for presupposition and other pragmatic phenomena. In Donna Byron, Craige Roberts, and Scott Schwenter, editors, Presupposition Accommodation. Ohio State Pragmatics Initiative.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Thomson</author>
<author>S Young</author>
</authors>
<title>Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems.</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<contexts>
<context position="10156" citStr="Thomson and Young, 2009" startWordPosition="1662" endWordPosition="1665">(s) and P(st+1=s&apos;) as P(s). The optimal policy π∗ is then obtained by optimizing the long-term reward, starting from b0: π∗ = argmax π Jπ(b0) (6) The optimal policy π∗ yields the highest expected reward value for each possible belief state. This value is compactly represented by the optimal value function, noted V ∗, which is a solution to the Bellman optimality equation (Bellman, 1957). Numerous algorithms for (offline) policy optimisation and (online) planning are available. For large spaces, exact optimisation is impossible and approximate methods must be used, see for instance grid-based (Thomson and Young, 2009) or point-based (Pineau et al., 2006) techniques. 2.2 POMDP-based dialogue management Dialogue management can be easily cast as a POMDP problem, with the state space being a compact representation of the interaction, the action space being a set of dialogue moves, the observation space representing speech recognition hypotheses, the transition function defining the dynamics of the interaction (which user reaction is to be expected after a particular dialogue move), and the observation function describing a “sensor model” between observed speech recognition hypotheses and actual utterances. Fin</context>
</contexts>
<marker>Thomson, Young, 2009</marker>
<rawString>B. Thomson and S. Young. 2009. Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems. Computer Speech &amp; Language, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Joshi Wang</author>
<author>R Khardon</author>
</authors>
<title>First order decision diagrams for relational mdps.</title>
<date>2007</date>
<booktitle>In IJCAI’07: Proceedings of the 20th international joint conference on Artifical intelligence,</booktitle>
<pages>1095--1100</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<marker>Wang, Khardon, 2007</marker>
<rawString>Ch. Wang, S. Joshi, and R. Khardon. 2007. First order decision diagrams for relational mdps. In IJCAI’07: Proceedings of the 20th international joint conference on Artifical intelligence, pages 1095–1100, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Partially observable markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>422</pages>
<contexts>
<context position="2768" citStr="Williams and Young, 2007" startWordPosition="403" endWordPosition="406"> the interaction history, the general context, and the user preferences. As a consequence, the state space necessary to model the dynamics of the environment tends to be large and sparsely populated. These two problems have typically been addressed separately in the literature. On the one hand, the issue of uncertainty in speech understanding is usually dealt using a range of probabilistic models combined with decision-theoretic planning. Among these, Partially Observable Markov Decision Process (POMDP) models have recently emerged as a unifying mathematical framework for dialogue management (Williams and Young, 2007; Lemon and Pietquin, 2007). POMDPs provide an explicit account for a wide range of uncertainties related to partial observability (noisy, incomplete spoken inputs) and stochastic action effects (the world may evolve in unpredictable ways after executing an action). On the other hand, structural complexity is typically addressed with logic-based approaches. Some investigated topics in this paradigm are pragmatic interpretation (Thomason et al., 2006), dialogue structure (Asher and Lascarides, 2003), or collaborative planning (Kruijff et al., 2008). These approaches are able to model sophistica</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>J. Williams and S. Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech and Language, 21(2):231– 422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>M Gaˇsi´c</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The hidden information state model: A practical framework for pomdp-based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu. 2010. The hidden information state model: A practical framework for pomdp-based spoken dialogue management. Computer Speech &amp; Language, 24(2):150–174.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>