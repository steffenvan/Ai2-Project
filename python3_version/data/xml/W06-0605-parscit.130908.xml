<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000045">
<title confidence="0.993662">
Frontiers in Linguistic Annotation for Lower-Density Languages
</title>
<author confidence="0.997484">
Mike Maxwell
</author>
<affiliation confidence="0.9967615">
Center for Advanced Study of Language
University of Maryland
</affiliation>
<email confidence="0.998298">
mmaxwell@casl.umd.edu
</email>
<author confidence="0.996754">
Baden Hughes
</author>
<affiliation confidence="0.998052">
Department of Computer Science
The University of Melbourne
</affiliation>
<email confidence="0.996474">
badenh@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.993831" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998718125">
The languages that are most commonly
subject to linguistic annotation on a large
scale tend to be those with the largest pop-
ulations or with recent histories of lin-
guistic scholarship. In this paper we dis-
cuss the problems associated with lower-
density languages in the context of the de-
velopment of linguistically annotated re-
sources. We frame our work with three
key questions regarding the definition of
lower-density languages; increasing avail-
able resources and reducing data require-
ments. A number of steps forward are
identified for increasing the number lower-
density language corpora with linguistic
annotations.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955339285714">
The process for selecting a target language for re-
search activity in corpus linguistics, natural lan-
guage processing or computational linguistics is
largely arbitrary. To some extent, the motivation
for a specific choice is based on one or more of a
range of factors: the number of speakers of a given
language; the economic and social dominance of
the speakers; the extent to which computational
and/or lexical resources already exist; the avail-
ability of these resources in a manner conducive to
research activity; the level of geopolitical support
for language-specific activity, or the sensitivity of
the language in the political arena; the degree to
which the researchers are likely to be appreciated
by the speakers of the language simply because
of engagement; and the potential scientific returns
from working on the language in question (includ-
ing the likelihood that the language exhibits inter-
esting or unique phenomena). Notably, these fac-
tors are also significant in determining whether a
language is worked on for documentary and de-
scriptive purposes, although an additional factor
in this particular area is also the degree of endan-
germent (which can perhaps be contrasted with the
likelihood of economic returns for computational
endeavour).
As a result of these influencing factors, it is
clear that languages which exhibit positive effects
in one or more of these areas are likely to be the
target of computational research. If we consider
the availability of computationally tractable lan-
guage resources, we find, unsuprisingly that major
languages such as English, German, French and
Japanese are dominant; and research on computa-
tional approaches to linguistic analysis tends to be
farthest advanced in these languages.
However, renewed interest in the annotation of
lower-density languages has arisen for a number
of reasons, both theoretical and practical. In this
paper we discuss the problems associated with
lower-density languages in the context of the de-
velopment of linguistically annotated resources.
The structure of this paper is as follows. First
we define the lower-density languages and lin-
guistically annotated resources, thus defining the
scope of our interest. We review some related
work in the area of linguistically annotated cor-
pora for lower-density languages. Next we pose
three questions which frame the body of this pa-
per: What is the current status of in terms of lower-
density languages which have linguistically anno-
tated corpora? How can we more efficiently create
this particular type of data for lower-density lan-
guages? Can existing analytical methods methods
perform reliably with less data? A number of steps
are identified for advancing the agenda of linguis-
</bodyText>
<page confidence="0.987006">
29
</page>
<note confidence="0.777728">
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 29–37,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.620009666666667">
tically annotated resources for lower-density lan- • Text marked for word boundaries (for those
guages, and finally we draw conclusions. scripts which, like Thai, do not mark most
word boundaries)
</bodyText>
<sectionHeader confidence="0.989195" genericHeader="introduction">
2 Lower-Density Languages
</sectionHeader>
<bodyText confidence="0.999983863636363">
It should be noted from the outset that in this pa-
per we interpret ‘density’ to refer to the amount
of computational resources available, rather than
the number of speakers any given language might
have.
The fundamental problem for annotation of
lower-density languages is that they are lower-
density. While on the surface, this is a tautol-
ogy, it in fact is the problem. For a few lan-
guages of the world (such as English, Chinese
and Modern Standard Arabic, and a few West-
ern European languages), resources are abundant;
these are the high-density Languages. For a few
more languages (other European languages, for
the most part), resources are, if not exactly abun-
dant, at least existent, and growing; these may be
considered medium-density languages. Together,
high-density and medium-density languages ac-
count for perhaps 20 or 30 languages, although of
course the boundaries are arbitrary. For all other
languages, resources are scarce and hence they fall
into our specific area of interest.
</bodyText>
<sectionHeader confidence="0.958796" genericHeader="method">
3 Linguistically Annotated Resources
</sectionHeader>
<bodyText confidence="0.998396571428572">
While the scarcity of language resources for
lower-density languages is apparent for all re-
source types (with the possible exception of mono-
lingual text ), it is particularly true of linguistically
annotated texts. By annotated texts, we include
the following sorts of computational linguistic re-
sources:
</bodyText>
<listItem confidence="0.992792863636363">
• Parallel text aligned with another language at
the sentence level (and/or at finer levels of
parallelism, including morpheme-level gloss-
ing)
• Text annotated for named entities at various
levels of granularity
• Morphologically analyzed text (for non-
isolating languages; at issue here is particu-
larly inflectional morphology, and to a lesser
degree of importance for most computational
purposes, derivational morphology); also a
morphological tag schema appropriate to the
particular language
• POS tagged text, and a POS tag schema ap-
propriate to the particular language
• Treebanked (syntactically annotated and
parsed) text
• Semantically tagged text (semantic roles) cf.
Propbank (Palmer et al., 2005), or frames cf.
Framenet1
• Electronic dictionaries and other lexical re-
sources, such as Wordnet2
</listItem>
<bodyText confidence="0.887807071428572">
There are numerous dimensions for linguisti-
cally annotated resources, and a range of research
projects have attempted to identify the core prop-
erties of interest. While concepts such as the Ba-
sic Language Resource Kit (BLARK; (Krauwer,
2003; Mapelli and Choukri, 2003)) have gained
considerable currency in higher-density language
resource creation projects, it is clear that the base-
line requirements of such schemes are signifi-
cantly more advanced than we can hope for for
lower-density languages in the short to medium
term. Notably, the concept of a reduced BLARK
(‘BLARKette’) has recently gained some currency
in various forums.
</bodyText>
<sectionHeader confidence="0.986522" genericHeader="method">
4 Key Questions
</sectionHeader>
<bodyText confidence="0.999619714285714">
Given that the vast majority of the more than seven
thousand languages documented in the Ethno-
logue (Gordon, 2005) fall into the class of lower-
density languages, what should we do? Equally
important, what can we realistically do? We pose
three questions by which to frame the remainder
of this paper.
</bodyText>
<listItem confidence="0.993989625">
1. Status Indicators: How do we know where
we are? How do we keep track of what lan-
guages are high-density or medium-density,
and which are lower-density?
2. Increasing Available Resources: How (or
can) we encourage the movement of lan-
guages up the scale from lower-density to
medium-density or high-density?
</listItem>
<footnote confidence="0.9999275">
1http://framenet.icsi.berkeley.edu/
2http://wordnet.princeton.edu
</footnote>
<page confidence="0.998369">
30
</page>
<listItem confidence="0.455463">
3. Reducing Data Requirements: Given that
</listItem>
<bodyText confidence="0.9831696">
some languages will always be relatively
lower-density, can language processing ap-
plications be made smarter, so that they don’t
require largely unattainable resources in or-
der to perform adequately?
</bodyText>
<sectionHeader confidence="0.97617" genericHeader="method">
5 Status Indicators
</sectionHeader>
<bodyText confidence="0.999093586206896">
We have been deliberately vague up to this point
about how many lower-density languages there
are, or the simpler question, how my high and
medium density languages there are. Of course
one reason for this is that the boundary between
low density and medium or high density is inher-
ently vague. Another reason is that the situation
is constantly changing; many Central and East-
ern European languages which were lower-density
languages a decade or so ago are now arguably
medium density, if not high density. (The stan-
dard for high vs. low density changes, too; the bar
is considerably higher now than it was ten years
ago.)
But the primary reason for being vague about
how many – and which – languages are low den-
sity today is that no is keeping track of what re-
sources are available for most languages. So we
simply have no idea which languages are low den-
sity, and more importantly (since we can guess that
in the absence of evidence to the contrary, a lan-
guage is likely to be low density), we don’t know
which resource types most languages do or do not
have.
This lack of knowledge is not for lack of trying,
although perhaps we have not been trying hard
enough. The following are a few of the catalogs
of information about languages and their resources
that are available:
</bodyText>
<listItem confidence="0.9962341">
• The Ethnologue3: This is the standard list-
ing of the living languages of the world, but
contains little or no information about what
resources exist for each language.
• LDC catalog4 and ELDA catalog5: The
Linguistic Data Consortium (LDC) and the
European Language Resources Distribution
Agency (ELDA) have been among the largest
distributors of annotated language data. Their
catalogs, naturally, cover only those corpora
</listItem>
<footnote confidence="0.999816">
3http://www.ethnologue.org
4http://www.ldc.upenn.edu/Catalog/
5http://www.elda.org/rubrique6.html
</footnote>
<bodyText confidence="0.923275214285714">
distributed by each organization, and these
include only a small number of languages.
Naturally, the economically important lan-
guages constitute the majority of the holdings
of the LDC and ELDA.
• AILLA (Archive of the Indigenous Lan-
guages of Latin America6), and numerous
other language archiving sites: Such sites
maintain archives of linguistic data for lan-
guages, often with a specialization, such as
indigenous languages of a country or region.
The linguistic data ranges from unannotated
speech recordings to morphologically ana-
lyzed texts glossed at the morpheme level.
</bodyText>
<listItem confidence="0.992765">
• OLAC (Open Archives Language Commu-
nity7): Given that many of the above re-
sources (particularly those of the many lan-
guage archives) are hard to find, OLAC is
an attempt to be a meta-catalog (or aggre-
gator)of such resources. It allows lookup of
data by type, language etc. for all data repos-
itories that ‘belong to’ OLAC. In fact, all the
above resources are listed in the OLAC union
catalogue.
• Web-based catalogs of additional resources:
</listItem>
<bodyText confidence="0.99940105">
There is a huge number of additional web-
sites which catalog information about lan-
guages, ranging from electronic and print
dictionaries (e.g. yourDictionary8), to dis-
cussion groups about particular languages9.
Most such sites do little vetting of the re-
sources, and dead links abound. Neverthe-
less, such sites (or a simple search with an
Internet search engine) can often turn up use-
ful information (such as grammatical descrip-
tions of minority languages). Very few of
these web sites are cataloged in OLAC, al-
though recent efforts (Hughes et al., 2006a)
are slowly addressing the inclusion of web-
based low density language resources in such
indexes.
None of the above catalogs is in any sense com-
plete, and indeed the very notion of completeness
is moot when it comes to cataloging Internet re-
sources. But more to the point of this paper, it
</bodyText>
<footnote confidence="0.999993">
6http://www.ailla.utexas.org
7http://www.language-archives.org
8http://www.yourdictionary.com
9http://dir.groups.yahoo.com/dir/Cultures Community/By Language
</footnote>
<page confidence="0.999876">
31
</page>
<bodyText confidence="0.992506846153846">
is difficult, if not impossible, to get a picture of
the state of language resources in general. How
many languages have sufficient bitext (and in what
genre), for example, that one could put together a
statistical machine translation system? What lan-
guages have morphological parsers (and for what
languages is such a parser more or less irrele-
vant, because the language is relatively isolating)?
Where can one find character encoding converters
for the Ge’ez family of fonts for languages written
in Ethiopic script?
The answer to such questions is important for
several reasons:
</bodyText>
<listItem confidence="0.891614583333333">
1. If there were a crisis that involved an arbitrary
language of the world, what resources could
be deployed? An example of such a situa-
tion might be another tsunami near Indone-
sia, which could affect dozens, if not hun-
dreds of minority languages. (The Decem-
ber 26, 2004 tsunami was particularly felt in
the Aceh province of Indonesia, where one of
the main languages is Aceh, spoken by three
million people. Aceh is a lower-density lan-
guage.)
2. Which languages could, with a relatively
</listItem>
<bodyText confidence="0.909478818181818">
small amount of effort, move from lower-
density status to medium-density or high-
density status? For example, where parallel
text is harvestable, a relatively small amount
of work might suffice to produce many appli-
cations, or other resources (e.g. by projecting
syntactic annotation across languages). On
the other hand, where the writing system of
a language is in flux, or the language is po-
litically oppressed, a great deal more effort
might be necessary.
</bodyText>
<listItem confidence="0.9198498">
3. For which low density languages might re-
lated languages provide the leverage needed
to build at least first draft resources? For ex-
ample, one might think of using Turkish (ar-
guably at least a medium-density language)
as a sort of pivot language to build lexicons
and morphological parsers for such low den-
sity Turkic languages as Uzbek or Uyghur.
4. For which low density languages are there
extensive communities of speakers living in
</listItem>
<bodyText confidence="0.979319358974359">
other countries, who might be better able to
build language resources than speakers living
in the perhaps less economically developed
home countries? (Expatriate communities
may also be motivated by a desire to main-
tain their language among younger speakers,
born abroad.)
5. Which languages would require more work
(and funding) to build resources, but are still
plausible candidates for short term efforts?
To our knowledge, there is no general, on-going
effort to collect the sort of data that would make
answers to these questions possible. A survey was
done at the Linguistic Data Consortium several
years ago (Strassel et al., 2003) , for text-based re-
sources for the three hundred or so languages hav-
ing at least a million speakers (an arbitrary cutoff,
to be sure, but necessary for the survey to have had
at least some chance of success). It was remark-
ably successful, considering that it was done by
two linguists who did not know the vast majority
of the languages surveyed. The survey was funded
long enough to ‘finish’ about 150 languages, but
no subsequent update was ever done.
A better model for such a survey might be an
edited book: one or more computational linguists
would serve as ‘editors’, responsible for the over-
all framework, and training of other participants.
Section ‘editors’ would be responsible for a lan-
guage family, or for the languages of a geographic
region or country. Individual language experts
would receive a small amount of training to enable
them to answer the survey questions for their lan-
guage, and then paid to do the initial survey, plus
periodic updates. The model provided by the Eth-
nologue (Gordon, 2005) may serve as a starting
point, although for the level of detail that would
be useful in assessing language resource availabil-
ity will make wholesale adoption unsuitable.
</bodyText>
<sectionHeader confidence="0.990473" genericHeader="method">
6 Increasing Available Resources
</sectionHeader>
<bodyText confidence="0.999986916666667">
Given that a language significantly lacks compu-
tational linguistic resources (and in the context of
this paper and the associated workshop, annotated
text resources), so that it falls into the class of
lower-density languages (however that might be
defined), what then?
Most large-scale collections of computational
linguistics resources have been funded by govern-
ment agencies, either the US government (typi-
cally the Department of Defense) or by govern-
ments of countries where the languages in ques-
tion are spoken (primarily European, but also a
</bodyText>
<page confidence="0.996451">
32
</page>
<bodyText confidence="0.9999387">
few other financially well-off countries). In some
cases, governments have sponsored collections for
languages which are not indigenous to the coun-
try in question (e.g. the EMILLE project10, see
(McEnery et al., 2000)).
In most such projects, production of resources
for lower-density languages have been the work of
a very small team which oversees the effort, to-
gether with paid annotators and translators. More
specifically, collection and processing of monolin-
gual text can be done by a linguist who need not
know the language (although it helps to have a
speaker of the language who can be called on to
do language identification, etc.). Dictionary col-
lection from on-line dictionaries can also be done
by a linguist; but if it takes much more effort than
that – for example, if the dictionary needs to be
converted from print format to electronic format –
it is again preferable to have a language speaker
available.
Annotating text (e.g. for named entities) is dif-
ferent: it can only be done by a speaker of the lan-
guage (more accurately, a reader: for Punjabi, for
instance, it can be difficult to find fluent readers of
the Gurmukhi script). Preferably the annotator is
familiar enough with current events in the country
where the language is spoken that they can inter-
pret cross-references in the text. If two or more an-
notators are available, the work can be done some-
what more quickly. More importantly, there can be
some checking for inter-annotator agreement (and
revision taking into account such differences as are
found).
Earlier work on corpus collection from the web
(e.g. (Resnik and Smith, 2003)) gave some hope
that reasonably large quantities of parallel text
could be found on the web, so that a bitext collec-
tion could be built for interesting language pairs
(with one member of the pair usually being En-
glish) relatively cheaply. Subsequent experience
with lower-density languages has not born that
hope out; parallel text on the web seems rela-
tively rare for most languages. It is unclear why
this should be. Certainly in countries like India,
there are large amounts of news text in English and
many of the target languages (such as Hindi). Nev-
ertheless, very little of that text seems to be gen-
uinely parallel, although recent work (Munteanu
and Marcu, 2005) indicates that true parallelism
may not be required for some tasks, eg machine
</bodyText>
<footnote confidence="0.494296">
10http://bowland-files.lancs.ac.uk/corplang/emille/
</footnote>
<bodyText confidence="0.999763239130435">
translation, in order to gain acceptable results.
Because bitext was so difficult to find for lower-
density languages, corpus creation efforts rely
largely, if not exclusively, on contracting out text
for translation. In most cases, source text is har-
vested from news sites in the target language, and
then translated into English by commercial trans-
lation agencies, at a rate usually in the neighbor-
hood of US$0.25 per word. In theory, one could
reduce this cost by dealing directly with trans-
lators, avoiding the middleman agencies. Since
many translators are in the Third World, this might
result in considerable cost savings. Nevertheless,
quality control issues loom large. The more pro-
fessional agencies do quality control of their trans-
lations; even so, one may need to reject transla-
tions in some cases (and the agencies themselves
may have difficulty in dealing with translators for
languages for which there is comparatively little
demand). Obviously this overall cost is high; it
means that a 100k word quantity of parallel text
will cost in the neighborhood of US$25K.
Other sources of parallel text might include
government archives (but apart from parliamen-
tary proceedings where these are published bilin-
gually, such as the Hansards, these are usually not
open), and the archives of translation companies
(but again, these are seldom if ever open, because
the agencies must guard the privacy of those who
contracted the translations).
Finally, there is the possibility that parallel text
– and indeed, other forms of annotation – could be
produced in an open source fashion. Wikipedia11
is perhaps the most obvious instance of this, as
there are parallel articles in English and other lan-
guages. Unfortunately, the quantity of such par-
allel text at the Wikipedia is very small for all
but a few languages. At present (May 2006),
there are over 100,000 articles in German, Span-
ish, French, Italian, Japanese, Dutch, Polish, Por-
tuguese and Swedish.12 Languages with over
10,000 articles include Arabic, Bulgarian, Cata-
lan, Czech, Danish, Estonian, Esperanto and Ido
(both constructed languages), Persian, Galician,
Hebrew, Croatian), Bahasa Indonesian, Korean,
Lithuanian, Hungarian, Bahasa Malay, Norwegian
</bodyText>
<footnote confidence="0.997345166666667">
11http://en.wikipedia.org
12Probably some of these articles are non-parallel. Indeed,
a random check of Cebuano articles in Wikipedia revealed
that many were stubs (a term used in the Wikipedia to refer to
“a short article in need of expansion”), or were simply links to
Internet blogs, many of which were monolingual in English.
</footnote>
<page confidence="0.999416">
33
</page>
<bodyText confidence="0.999940638297872">
(Bokm´al and Nynorsk), Romanian, Russian, Slo-
vak, Slovenian, Serbian, Finnish, Thai, Turkish,
Ukrainian, and Chinese. The dominance of Euro-
pean languages in these lists is obvious.
During a TIDES exercise in 2003, researchers
at Johns Hopkins University explored an innova-
tive approach to the creation of bitext (parallel En-
glish and Hindi text, aligned at the sentence level):
they elicited translations into English of Hindi sen-
tences they posted on an Internet web page (Oard,
2003; Yarowsky, 2003). Participants were paid for
the best translations in Amazon.com gift certifi-
cates, with the quality of a twenty percent subset
of the translations automatically evaluated using
BLEU scores against highly scored translations of
the same sentences from previous rounds. This
pool of high-quality translations was initialized to
a set of known quality translations. A valuable
side effect of the use of previously translated texts
for evaluation is that this created a pool of multiply
translated texts.
The TIDES translation exercise quickly pro-
duced a large body of translated text: 300K words,
in five days, at a cost of about two cents per word.
This approach to resource creation is similar to
numerous open source projects, in the sense that
the work is being done by the public. It differed
in that the results of this work were not made
publicly available; the use of an explicit qual-
ity control method; and of course the payments
to (some) participants. While the quality control
aspect may be essential to producing useful lan-
guage resources, hiding those resources not cur-
rently being used for evaluation is not essential to
the methodology.
Open source resource creation efforts are of
course common, with the Wikipedia13 being the
best known. Other such projects include Ama-
zon.com’s Mechanical Turk14, LiTgloss15, The
ESP Game16, and the Wiktionary17. Clearly some
forms of annotation will be easier to do using
an open source methodology than others will.
For example, translation and possibly named en-
tity annotation might be fairly straightforward,
while morphological analysis is probably more
difficult, particularly for morphologically complex
languages.
</bodyText>
<footnote confidence="0.9998168">
13http://www.wikipedia.org
14http://www.mturk.com/mturk/
15http://litgloss.buffalo.edu/
16http://www.espgame.org/
17http://wiktionary.org/
</footnote>
<bodyText confidence="0.999792117647059">
Other researchers have experimented with the
automatic creation of corpora using web data
(Ghani et al., 2001) Some of these corpora have
grown to reasonable sizes; (Scannell, 2003; Scan-
nell, 2006) has corpora derived from web crawling
which are measured in tens of millions of words
for a variety of lower-density languages. However
it should be noted that in these cases, the type of
linguistic resource created is often not linguisti-
cally annotated, but rather a lexicon or collection
of primary texts in a given language.
Finally, we may mention efforts to create cer-
tain kinds of resources by computer-directed elic-
itation. Examples of projects sharing this focus
include BOAS (Nirenburg and Raskin, 1998), and
the AVENUE project (Probst et al., 2002), (Lavie
et al., 2003).
</bodyText>
<sectionHeader confidence="0.979598" genericHeader="method">
7 Reducing Data Requirements
</sectionHeader>
<bodyText confidence="0.999970375">
Creating more annotated resources is the obvious
way to approach the problem of the lack of re-
sources for lower-density languages. A comple-
mentary approach is to improve the way the infor-
mation in smaller resources is used, for example
by developing machine translation systems that re-
quire less parallel text.
How much reduction in the required amount of
resources might be enough? An interesting ex-
periment, which to our knowledge has never been
tried, would be for a linguist to attempt as a test
case what we hope that computers can do. That
is, a linguist could take a ‘small’ quantity of paral-
lel text, and extract as much lexical and grammat-
ical information from that as possible. The lin-
guist might then take a previously unseen text in
the target language and translate it into English,
or perform some other useful task on target lan-
guage texts. One might argue over whether this
experiment would constitute an upper bound on
how much information could be extracted, but it
would probably be more information than current
computational approaches extract.
Naturally, this approach partially shifts the
problem from the research community interested
in linguistically annotated corpora to the research
community interested in algorithms. Much ef-
fort has been invested in scaling algorithmic ap-
proaches upwards, that is, leveraging every last
available data point in pursuit of small perfor-
mance improvements. We argue that scaling down
(ie using less training data) poses an equally sig-
</bodyText>
<page confidence="0.997629">
34
</page>
<bodyText confidence="0.9999849">
nificant challenge. The basic question of whether
methods which are data-rich can scale down to im-
poverished data has been the focus of a number of
recent papers in areas such as machine translation
(Somers, 1997; Somers, 1998), language identifi-
cation (Hughes et al., 2006b) etc. However, tasks
which have lower-density language at their core
have yet to become mainstream in shared evalua-
tion tasks which drive much of the algorithmic im-
provements in computational linguistics and natu-
ral language processing.
Another approach to data reduction is to change
the type of data required for a given task. For
many lower-density languages a significant vol-
ume of linguistically annotated data exists, but not
in the form of the curated, standardised corpora
to which language technologists are accustomed.
Neverthless for extremely low density languages,
a degree of standardisation is apparent by virtue of
documentary linguistic practice. Consider for ex-
ample, the number of Shoebox lexicons and cor-
responding interlinear texts which are potentially
available from documentary sources: while not
being the traditional resource types on which sys-
tems are trained, they are reasonably accessible,
and cover a larger number of languages. Bible
translations are another form of parallel text avail-
able in nearly every written language (see (Resnik
et al., 1999)). There are of course issues of quality,
not to mention vocabulary, that arise from using
the Bible as a source of parallel text, but for some
purposes – such as morphology learning – Bible
translations might be a very good source of data.
Similarly, a different compromise may be found
in the ratio of the number of words in a corpus
to the richness of linguistic annotation. In many
high-density corpora development projects, an ar-
bitrary (and high) target for the number of words is
often set in advance, and subsequent linguistic an-
notation is layered over this base corpus in a pro-
gressively more granular fashion. It may be that
this corpus development model could be modified
for lower-density language resource development:
we argue that in many cases, the richness of lin-
guistic annotation over a given set of data is more
important than the raw quantity of the data set.
A related issue is different standards for an-
notating linguistic concepts We already see this
in larger languages (consider the difference in
morpho-syntactic tagging between the Penn Tree-
bank and other corpora), but has there is a higher
diversity of standards in lower-density languages.
Solutions may include ontologies for linguistic
concepts e.g. General Ontology for Linguistic
Description18 and the ISO Data Category Reg-
istry (Ide and Romary, 2004), which allow cross-
resource navigation based on common semantics.
Of course, cross-language and cross-cultural se-
mantics is a notoriously difficult subject.
Finally, it may be that development of web
based corpora can act as the middle ground: there
are plenty of documents on the web in lower-
density languages, and efforts such as projects by
Scannell19 and Lewis20 indicate these can be cu-
rated reasonably efficiently, even though the out-
comes may be slightly different to that which we
are accustomed. Is it possible to make use of XML
or HTML markup directly in these cases? Some-
day, the semantic web may help us with this type
of approach.
</bodyText>
<sectionHeader confidence="0.904741" genericHeader="method">
8 Moving Forward
</sectionHeader>
<bodyText confidence="0.909278076923077">
Having considered the status of linguistically-
annotated resources for lower-density languages,
and two broad strategies for improving this situ-
ation (innovative approaches to data creation, and
scaling down of resource requirements for existing
techniques), we now turn to the question of where
to go from here. We believe that there are a num-
ber of practical steps which can be taken in order
to increase the number of linguistically-annotated
lower-density language resources available to the
research community:
• Encouraging the publication of electronic
corpora of lower-density languages: most
economic incentives for corpus creation only
exhibit return on investment because of the
focus on higher-density languages; new mod-
els of funding and commercializing corpora
for lower-density languages are required.
• Engaging in research on bootstrapping from
higher density language resources to lower-
density surrogates: it seems obvious that
at least for related languages adopting a
derivational approach to the generation of
linguistically annotated corpora for lower-
density languages by using automated an-
notation tools trained on higher-density lan-
</bodyText>
<footnote confidence="0.999860333333333">
18http://www.linguistics-ontology.org
19http://borel.slu.edu/crubadan/stadas.html
20http://www.csufresno.edu/odin
</footnote>
<page confidence="0.998656">
35
</page>
<bodyText confidence="0.862590333333333">
guages may at least reduce the human effort
required.
• Scaling down (through data requirement re-
duction) of state of the art algorithms: there
has been little work in downscaling state of
the art algorithms for tasks such as named
entity recognition, POS tagging and syntac-
tic parsing, yet (considerably) reducing the
training data requirement seems like one of
the few ways that existing analysis technolo-
gies can be applied to lower-density lan-
guages.
</bodyText>
<listItem confidence="0.613215">
• Shared evaluation tasks which include lower-
</listItem>
<bodyText confidence="0.960156408163265">
density languages or smaller amounts of data:
most shared evaluation tasks are construed
as exercises in cross-linguistic scalability (eg
CLEF) or data intensivity (eg TREC) or both
(eg NTCIR). Within these constructs there
is certainly room for the inclusion of lower-
density languages as targets, although no-
tably the overhead here is not in the provi-
sion of the language data, but the derivatives
(eg query topics) on which these exercises are
based.
• Promotion of multilingual corpora which in-
clude lower-density languages: as multi-
lingual corpora emerge, there is opportu-
nity to include lower-density languages at
minimal opportunity cost e.g. EuroGOV
(Sigurbj¨onsson et al., 2005) or JRC-Acquis
(Steinberger et al., 2006), which are based on
web data from the EU, includes a number of
lower-density languages by virtue of the cor-
pus creation mechanism not being language-
specific.
• Language specific strategies: collectively we
have done well at developing formal strate-
gies for high density languages e.g. in EU
roadmaps, but not so well at strategies for
medium-density or lower-density languages.
The models for medium to long term strate-
gies of language resource development may
be adopted for lower density languages. Re-
cently this has been evidenced through events
such as the LREC 2006 workshop on African
language resources and the development of a
corresponding roadmap.
• Moving towards interoperability between an-
notation schemes which dominate the higher-
density languages (eg Penn Treebank tag-
ging conventions) and the relatively ad-hoc
schemes often exhibited by lower-density
languages, through means such as markup
ontologies like the General Ontology for Lin-
guistic Description or the ISO Data Category
Registry.
Many of these steps are not about to be realised
in the short term. However, developing a cohesive
strategy for addressing the need for linguistically
annotated corpora is a first step in ensuring com-
mittment from interested researchers to a common
roadmap.
</bodyText>
<sectionHeader confidence="0.997855" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999985208333333">
It is clear that the number of linguistically-
annotated resources for any language will in-
evitably be less than optimal. Regardless of the
density of the language under consideration, the
cost of producing linguistically annotated corpora
of a substantial size is significant, Inevitably, lan-
guages which do not have a strong political, eco-
nomic or social status will be less well resourced.
Certain avenues of investigation e.g. collect-
ing language specific web content, or building ap-
proximate bitexts web data are being explored, but
other areas (such as rich morphosyntactic annota-
tion) are not particularly evidenced.
However, there is considerable research inter-
est in the development of linguistically annotated
resources for languages of lower density. We are
encouraged by the steady rate at which academic
papers emerge reporting the development of re-
sources for lower-density language targets. We
have proposed a number of steps by which the is-
sue of language resources for lower-density lan-
guages may be more efficiently created and look
forward with anticipation as to how these ideas
motivate future work.
</bodyText>
<sectionHeader confidence="0.997566" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.959715444444444">
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2001.
Mining the web to create minority language corpora.
In Proceedings of 2001 ACM International Con-
ference on Knowledge Management (CIKM2001),
pages 279–286. Association for Computing Machin-
ery.
Raymond G. Gordon. 2005. Ethnologue: Languages
of the World (15th Edition). SIL International: Dal-
las.
</reference>
<page confidence="0.975567">
36
</page>
<reference confidence="0.999483656565657">
Baden Hughes, Timothy Baldwin, and Steven Bird.
2006a. Collecting low-density language data on the
web. In Proceedings of the 12th Australasian Web
Conference (AusWeb06). Southern Cross University.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006b. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC2006). European Language Resources Asso-
ciation: Paris.
Nancy Ide and Laurent Romary. 2004. A registry of
standard data categories for linguistic annotation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004,
pages 135–139. European Language Resources As-
sociation: Paris.
Steven Krauwer. 2003. The basic language resource
kit (BLARK) as the first milestone for the lan-
guage resources roadmap. In Proceedings of 2nd
International Conference on Speech and Computer
(SPECOM2003).
A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst,
A. Font Llitjos, R. Reynolds, J. Carbonell, and
R. Cohen. 2003. Experiments with a hindi-to-
english transfer-based mt system under a miserly
data scenario. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 2(2).
Valerie Mapelli and Khalid Choukri. 2003. Report
on a monimal set of language resources to be made
available for as many languages as possible, and a
map of the actual gaps. ENABLER internal project
report (Deliverable 5.1).
Tony McEnery, Paul Baker, and Lou Burnard. 2000.
Corpus resources and minority language engineer-
ing. In Proceedings of the 2nd International Con-
ference on Language Resources and Evaluation
(LREC2002). European Language Resources Asso-
ciation: Paris.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477–504.
Sergei Nirenburg and Victor Raskin. 1998. Univer-
sal grammar and lexis for quick ramp-up of mt. In
Proceedings of 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 975–979. Association for Computational Lin-
guistics.
Douglas W. Oard. 2003. The surprise language exer-
cises. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 2(2):79–84.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
K. Probst, L. Levin, E. Peterson, A. Lavie, and J. Car-
bonell. 2002. Mt for minority languages using
elicitation-based learning of syntactic transfer rules.
Machine Translation, 17(4):245–270.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380.
Philip Resnik, Mari Broman Olsen, and Mona Diab.
1999. The Bible as a parallel corpus: Annotating
the ‘Book of 2000 Tongues’. Computers and the
Humanities, 33(1-2):129–153.
Kevin Scannell. 2003. Automatic thesaurus genera-
tion for minority languages: an irish example. In
Actes des Traitement Automatique des Langues Mi-
noritaires et des Petites Langues, volume 2, pages
203–212.
Kevin Scannell. 2006. Machine translation for
closely related language pairs. In Proceedings of the
LREC2006 Workshop on Strategies for developing
machine translation for minority languages. Euro-
pean Language Resources Association: Paris.
B. Sigurbj¨onsson, J. Kamps, and M. de Rijke. 2005.
Blueprint of a cross-lingual web collection. Journal
of Digital Information Management, 3(1):9–13.
Harold Somers. 1997. Machine translation and minor-
ity languages. Translating and the Computer, 19:1–
13.
Harold Somers. 1998. Language resources and minor-
ity languages. Language Today, 5:20–24.
R. Steinberger, B. Pouliquen, A. Widger, C. Ignat,
T. Erjavec, D. Tufis, and D. Varga. 2006. The jrc-
acquis: A multilingual aligned parallel corpus with
20+ languages. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Eval-
uation (LREC2006). European Language Resources
Association: Paris.
Stephanie Strassel, Mike Maxwell, and Christopher
Cieri. 2003. Linguistic resource creation for re-
search and technology development: A recent exper-
iment. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 2(2):101–117.
David Yarowsky. 2003. Scalable elicitation of training
data for machine translation. Team Tides, 4.
</reference>
<sectionHeader confidence="0.988833" genericHeader="acknowledgments">
10 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999045142857143">
The authors are grateful to Kathryn L. Baker for
her comments on earlier drafts of this paper.
Portions of the research in this paper were sup-
ported by the Australian Research Council Spe-
cial Research Initiative (E-Research) grant num-
ber SR0567353 “An Intelligent Search Infrastruc-
ture for Language Resources on the Web.”
</bodyText>
<page confidence="0.999257">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.834147">
<title confidence="0.999635">Frontiers in Linguistic Annotation for Lower-Density Languages</title>
<author confidence="0.99365">Mike</author>
<affiliation confidence="0.997052">Center for Advanced Study of University of</affiliation>
<email confidence="0.996389">mmaxwell@casl.umd.edu</email>
<author confidence="0.894928">Baden</author>
<affiliation confidence="0.998162">Department of Computer The University of</affiliation>
<email confidence="0.979235">badenh@csse.unimelb.edu.au</email>
<abstract confidence="0.998146235294118">The languages that are most commonly subject to linguistic annotation on a large scale tend to be those with the largest populations or with recent histories of linguistic scholarship. In this paper we discuss the problems associated with lowerdensity languages in the context of the development of linguistically annotated resources. We frame our work with three key questions regarding the definition of lower-density languages; increasing available resources and reducing data requirements. A number of steps forward are identified for increasing the number lowerdensity language corpora with linguistic annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rayid Ghani</author>
<author>Rosie Jones</author>
<author>Dunja Mladenic</author>
</authors>
<title>Mining the web to create minority language corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of 2001 ACM International Conference on Knowledge Management (CIKM2001),</booktitle>
<pages>279--286</pages>
<publisher>Association for Computing Machinery.</publisher>
<contexts>
<context position="23337" citStr="Ghani et al., 2001" startWordPosition="3670" endWordPosition="3673">al Turk14, LiTgloss15, The ESP Game16, and the Wiktionary17. Clearly some forms of annotation will be easier to do using an open source methodology than others will. For example, translation and possibly named entity annotation might be fairly straightforward, while morphological analysis is probably more difficult, particularly for morphologically complex languages. 13http://www.wikipedia.org 14http://www.mturk.com/mturk/ 15http://litgloss.buffalo.edu/ 16http://www.espgame.org/ 17http://wiktionary.org/ Other researchers have experimented with the automatic creation of corpora using web data (Ghani et al., 2001) Some of these corpora have grown to reasonable sizes; (Scannell, 2003; Scannell, 2006) has corpora derived from web crawling which are measured in tens of millions of words for a variety of lower-density languages. However it should be noted that in these cases, the type of linguistic resource created is often not linguistically annotated, but rather a lexicon or collection of primary texts in a given language. Finally, we may mention efforts to create certain kinds of resources by computer-directed elicitation. Examples of projects sharing this focus include BOAS (Nirenburg and Raskin, 1998)</context>
</contexts>
<marker>Ghani, Jones, Mladenic, 2001</marker>
<rawString>Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2001. Mining the web to create minority language corpora. In Proceedings of 2001 ACM International Conference on Knowledge Management (CIKM2001), pages 279–286. Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond G Gordon</author>
</authors>
<date>2005</date>
<booktitle>Ethnologue: Languages of the World (15th Edition). SIL International:</booktitle>
<location>Dallas.</location>
<contexts>
<context position="6914" citStr="Gordon, 2005" startWordPosition="1059" endWordPosition="1060"> interest. While concepts such as the Basic Language Resource Kit (BLARK; (Krauwer, 2003; Mapelli and Choukri, 2003)) have gained considerable currency in higher-density language resource creation projects, it is clear that the baseline requirements of such schemes are significantly more advanced than we can hope for for lower-density languages in the short to medium term. Notably, the concept of a reduced BLARK (‘BLARKette’) has recently gained some currency in various forums. 4 Key Questions Given that the vast majority of the more than seven thousand languages documented in the Ethnologue (Gordon, 2005) fall into the class of lowerdensity languages, what should we do? Equally important, what can we realistically do? We pose three questions by which to frame the remainder of this paper. 1. Status Indicators: How do we know where we are? How do we keep track of what languages are high-density or medium-density, and which are lower-density? 2. Increasing Available Resources: How (or can) we encourage the movement of languages up the scale from lower-density to medium-density or high-density? 1http://framenet.icsi.berkeley.edu/ 2http://wordnet.princeton.edu 30 3. Reducing Data Requirements: Give</context>
<context position="15187" citStr="Gordon, 2005" startWordPosition="2395" endWordPosition="2396">languages, but no subsequent update was ever done. A better model for such a survey might be an edited book: one or more computational linguists would serve as ‘editors’, responsible for the overall framework, and training of other participants. Section ‘editors’ would be responsible for a language family, or for the languages of a geographic region or country. Individual language experts would receive a small amount of training to enable them to answer the survey questions for their language, and then paid to do the initial survey, plus periodic updates. The model provided by the Ethnologue (Gordon, 2005) may serve as a starting point, although for the level of detail that would be useful in assessing language resource availability will make wholesale adoption unsuitable. 6 Increasing Available Resources Given that a language significantly lacks computational linguistic resources (and in the context of this paper and the associated workshop, annotated text resources), so that it falls into the class of lower-density languages (however that might be defined), what then? Most large-scale collections of computational linguistics resources have been funded by government agencies, either the US gov</context>
</contexts>
<marker>Gordon, 2005</marker>
<rawString>Raymond G. Gordon. 2005. Ethnologue: Languages of the World (15th Edition). SIL International: Dallas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baden Hughes</author>
<author>Timothy Baldwin</author>
<author>Steven Bird</author>
</authors>
<title>Collecting low-density language data on the web.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th Australasian Web Conference (AusWeb06).</booktitle>
<institution>Cross University.</institution>
<location>Southern</location>
<contexts>
<context position="11113" citStr="Hughes et al., 2006" startWordPosition="1732" endWordPosition="1735">LAC union catalogue. • Web-based catalogs of additional resources: There is a huge number of additional websites which catalog information about languages, ranging from electronic and print dictionaries (e.g. yourDictionary8), to discussion groups about particular languages9. Most such sites do little vetting of the resources, and dead links abound. Nevertheless, such sites (or a simple search with an Internet search engine) can often turn up useful information (such as grammatical descriptions of minority languages). Very few of these web sites are cataloged in OLAC, although recent efforts (Hughes et al., 2006a) are slowly addressing the inclusion of webbased low density language resources in such indexes. None of the above catalogs is in any sense complete, and indeed the very notion of completeness is moot when it comes to cataloging Internet resources. But more to the point of this paper, it 6http://www.ailla.utexas.org 7http://www.language-archives.org 8http://www.yourdictionary.com 9http://dir.groups.yahoo.com/dir/Cultures Community/By Language 31 is difficult, if not impossible, to get a picture of the state of language resources in general. How many languages have sufficient bitext (and in w</context>
<context position="25807" citStr="Hughes et al., 2006" startWordPosition="4073" endWordPosition="4076">rested in linguistically annotated corpora to the research community interested in algorithms. Much effort has been invested in scaling algorithmic approaches upwards, that is, leveraging every last available data point in pursuit of small performance improvements. We argue that scaling down (ie using less training data) poses an equally sig34 nificant challenge. The basic question of whether methods which are data-rich can scale down to impoverished data has been the focus of a number of recent papers in areas such as machine translation (Somers, 1997; Somers, 1998), language identification (Hughes et al., 2006b) etc. However, tasks which have lower-density language at their core have yet to become mainstream in shared evaluation tasks which drive much of the algorithmic improvements in computational linguistics and natural language processing. Another approach to data reduction is to change the type of data required for a given task. For many lower-density languages a significant volume of linguistically annotated data exists, but not in the form of the curated, standardised corpora to which language technologists are accustomed. Neverthless for extremely low density languages, a degree of standard</context>
</contexts>
<marker>Hughes, Baldwin, Bird, 2006</marker>
<rawString>Baden Hughes, Timothy Baldwin, and Steven Bird. 2006a. Collecting low-density language data on the web. In Proceedings of the 12th Australasian Web Conference (AusWeb06). Southern Cross University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baden Hughes</author>
<author>Timothy Baldwin</author>
<author>Steven Bird</author>
<author>Jeremy Nicholson</author>
<author>Andrew MacKinlay</author>
</authors>
<title>Reconsidering language identification for written language resources.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC2006). European Language Resources Association:</booktitle>
<location>Paris.</location>
<contexts>
<context position="11113" citStr="Hughes et al., 2006" startWordPosition="1732" endWordPosition="1735">LAC union catalogue. • Web-based catalogs of additional resources: There is a huge number of additional websites which catalog information about languages, ranging from electronic and print dictionaries (e.g. yourDictionary8), to discussion groups about particular languages9. Most such sites do little vetting of the resources, and dead links abound. Nevertheless, such sites (or a simple search with an Internet search engine) can often turn up useful information (such as grammatical descriptions of minority languages). Very few of these web sites are cataloged in OLAC, although recent efforts (Hughes et al., 2006a) are slowly addressing the inclusion of webbased low density language resources in such indexes. None of the above catalogs is in any sense complete, and indeed the very notion of completeness is moot when it comes to cataloging Internet resources. But more to the point of this paper, it 6http://www.ailla.utexas.org 7http://www.language-archives.org 8http://www.yourdictionary.com 9http://dir.groups.yahoo.com/dir/Cultures Community/By Language 31 is difficult, if not impossible, to get a picture of the state of language resources in general. How many languages have sufficient bitext (and in w</context>
<context position="25807" citStr="Hughes et al., 2006" startWordPosition="4073" endWordPosition="4076">rested in linguistically annotated corpora to the research community interested in algorithms. Much effort has been invested in scaling algorithmic approaches upwards, that is, leveraging every last available data point in pursuit of small performance improvements. We argue that scaling down (ie using less training data) poses an equally sig34 nificant challenge. The basic question of whether methods which are data-rich can scale down to impoverished data has been the focus of a number of recent papers in areas such as machine translation (Somers, 1997; Somers, 1998), language identification (Hughes et al., 2006b) etc. However, tasks which have lower-density language at their core have yet to become mainstream in shared evaluation tasks which drive much of the algorithmic improvements in computational linguistics and natural language processing. Another approach to data reduction is to change the type of data required for a given task. For many lower-density languages a significant volume of linguistically annotated data exists, but not in the form of the curated, standardised corpora to which language technologists are accustomed. Neverthless for extremely low density languages, a degree of standard</context>
</contexts>
<marker>Hughes, Baldwin, Bird, Nicholson, MacKinlay, 2006</marker>
<rawString>Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy Nicholson, and Andrew MacKinlay. 2006b. Reconsidering language identification for written language resources. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC2006). European Language Resources Association: Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Laurent Romary</author>
</authors>
<title>A registry of standard data categories for linguistic annotation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC2004,</booktitle>
<pages>135--139</pages>
<location>Paris.</location>
<contexts>
<context position="28228" citStr="Ide and Romary, 2004" startWordPosition="4458" endWordPosition="4461">rce development: we argue that in many cases, the richness of linguistic annotation over a given set of data is more important than the raw quantity of the data set. A related issue is different standards for annotating linguistic concepts We already see this in larger languages (consider the difference in morpho-syntactic tagging between the Penn Treebank and other corpora), but has there is a higher diversity of standards in lower-density languages. Solutions may include ontologies for linguistic concepts e.g. General Ontology for Linguistic Description18 and the ISO Data Category Registry (Ide and Romary, 2004), which allow crossresource navigation based on common semantics. Of course, cross-language and cross-cultural semantics is a notoriously difficult subject. Finally, it may be that development of web based corpora can act as the middle ground: there are plenty of documents on the web in lowerdensity languages, and efforts such as projects by Scannell19 and Lewis20 indicate these can be curated reasonably efficiently, even though the outcomes may be slightly different to that which we are accustomed. Is it possible to make use of XML or HTML markup directly in these cases? Someday, the semantic</context>
</contexts>
<marker>Ide, Romary, 2004</marker>
<rawString>Nancy Ide and Laurent Romary. 2004. A registry of standard data categories for linguistic annotation. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC2004, pages 135–139. European Language Resources Association: Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Krauwer</author>
</authors>
<title>The basic language resource kit (BLARK) as the first milestone for the language resources roadmap.</title>
<date>2003</date>
<booktitle>In Proceedings of 2nd International Conference on Speech and Computer (SPECOM2003).</booktitle>
<contexts>
<context position="6389" citStr="Krauwer, 2003" startWordPosition="977" endWordPosition="978">orphological tag schema appropriate to the particular language • POS tagged text, and a POS tag schema appropriate to the particular language • Treebanked (syntactically annotated and parsed) text • Semantically tagged text (semantic roles) cf. Propbank (Palmer et al., 2005), or frames cf. Framenet1 • Electronic dictionaries and other lexical resources, such as Wordnet2 There are numerous dimensions for linguistically annotated resources, and a range of research projects have attempted to identify the core properties of interest. While concepts such as the Basic Language Resource Kit (BLARK; (Krauwer, 2003; Mapelli and Choukri, 2003)) have gained considerable currency in higher-density language resource creation projects, it is clear that the baseline requirements of such schemes are significantly more advanced than we can hope for for lower-density languages in the short to medium term. Notably, the concept of a reduced BLARK (‘BLARKette’) has recently gained some currency in various forums. 4 Key Questions Given that the vast majority of the more than seven thousand languages documented in the Ethnologue (Gordon, 2005) fall into the class of lowerdensity languages, what should we do? Equally </context>
</contexts>
<marker>Krauwer, 2003</marker>
<rawString>Steven Krauwer. 2003. The basic language resource kit (BLARK) as the first milestone for the language resources roadmap. In Proceedings of 2nd International Conference on Speech and Computer (SPECOM2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>S Vogel</author>
<author>L Levin</author>
<author>E Peterson</author>
<author>K Probst</author>
<author>A Font Llitjos</author>
<author>R Reynolds</author>
<author>J Carbonell</author>
<author>R Cohen</author>
</authors>
<title>Experiments with a hindi-toenglish transfer-based mt system under a miserly data scenario.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="24005" citStr="Lavie et al., 2003" startWordPosition="3779" endWordPosition="3782">izes; (Scannell, 2003; Scannell, 2006) has corpora derived from web crawling which are measured in tens of millions of words for a variety of lower-density languages. However it should be noted that in these cases, the type of linguistic resource created is often not linguistically annotated, but rather a lexicon or collection of primary texts in a given language. Finally, we may mention efforts to create certain kinds of resources by computer-directed elicitation. Examples of projects sharing this focus include BOAS (Nirenburg and Raskin, 1998), and the AVENUE project (Probst et al., 2002), (Lavie et al., 2003). 7 Reducing Data Requirements Creating more annotated resources is the obvious way to approach the problem of the lack of resources for lower-density languages. A complementary approach is to improve the way the information in smaller resources is used, for example by developing machine translation systems that require less parallel text. How much reduction in the required amount of resources might be enough? An interesting experiment, which to our knowledge has never been tried, would be for a linguist to attempt as a test case what we hope that computers can do. That is, a linguist could ta</context>
</contexts>
<marker>Lavie, Vogel, Levin, Peterson, Probst, Llitjos, Reynolds, Carbonell, Cohen, 2003</marker>
<rawString>A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst, A. Font Llitjos, R. Reynolds, J. Carbonell, and R. Cohen. 2003. Experiments with a hindi-toenglish transfer-based mt system under a miserly data scenario. ACM Transactions on Asian Language Information Processing (TALIP), 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valerie Mapelli</author>
<author>Khalid Choukri</author>
</authors>
<title>Report on a monimal set of language resources to be made available for as many languages as possible, and a map of the actual gaps. ENABLER internal project report</title>
<date>2003</date>
<journal>Deliverable</journal>
<volume>5</volume>
<contexts>
<context position="6417" citStr="Mapelli and Choukri, 2003" startWordPosition="979" endWordPosition="982">g schema appropriate to the particular language • POS tagged text, and a POS tag schema appropriate to the particular language • Treebanked (syntactically annotated and parsed) text • Semantically tagged text (semantic roles) cf. Propbank (Palmer et al., 2005), or frames cf. Framenet1 • Electronic dictionaries and other lexical resources, such as Wordnet2 There are numerous dimensions for linguistically annotated resources, and a range of research projects have attempted to identify the core properties of interest. While concepts such as the Basic Language Resource Kit (BLARK; (Krauwer, 2003; Mapelli and Choukri, 2003)) have gained considerable currency in higher-density language resource creation projects, it is clear that the baseline requirements of such schemes are significantly more advanced than we can hope for for lower-density languages in the short to medium term. Notably, the concept of a reduced BLARK (‘BLARKette’) has recently gained some currency in various forums. 4 Key Questions Given that the vast majority of the more than seven thousand languages documented in the Ethnologue (Gordon, 2005) fall into the class of lowerdensity languages, what should we do? Equally important, what can we reali</context>
</contexts>
<marker>Mapelli, Choukri, 2003</marker>
<rawString>Valerie Mapelli and Khalid Choukri. 2003. Report on a monimal set of language resources to be made available for as many languages as possible, and a map of the actual gaps. ENABLER internal project report (Deliverable 5.1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony McEnery</author>
<author>Paul Baker</author>
<author>Lou Burnard</author>
</authors>
<title>Corpus resources and minority language engineering.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC2002). European Language Resources Association:</booktitle>
<location>Paris.</location>
<contexts>
<context position="16159" citStr="McEnery et al., 2000" startWordPosition="2542" endWordPosition="2545">ed text resources), so that it falls into the class of lower-density languages (however that might be defined), what then? Most large-scale collections of computational linguistics resources have been funded by government agencies, either the US government (typically the Department of Defense) or by governments of countries where the languages in question are spoken (primarily European, but also a 32 few other financially well-off countries). In some cases, governments have sponsored collections for languages which are not indigenous to the country in question (e.g. the EMILLE project10, see (McEnery et al., 2000)). In most such projects, production of resources for lower-density languages have been the work of a very small team which oversees the effort, together with paid annotators and translators. More specifically, collection and processing of monolingual text can be done by a linguist who need not know the language (although it helps to have a speaker of the language who can be called on to do language identification, etc.). Dictionary collection from on-line dictionaries can also be done by a linguist; but if it takes much more effort than that – for example, if the dictionary needs to be conver</context>
</contexts>
<marker>McEnery, Baker, Burnard, 2000</marker>
<rawString>Tony McEnery, Paul Baker, and Lou Burnard. 2000. Corpus resources and minority language engineering. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC2002). European Language Resources Association: Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="18229" citStr="Munteanu and Marcu, 2005" startWordPosition="2893" endWordPosition="2896">ies of parallel text could be found on the web, so that a bitext collection could be built for interesting language pairs (with one member of the pair usually being English) relatively cheaply. Subsequent experience with lower-density languages has not born that hope out; parallel text on the web seems relatively rare for most languages. It is unclear why this should be. Certainly in countries like India, there are large amounts of news text in English and many of the target languages (such as Hindi). Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine 10http://bowland-files.lancs.ac.uk/corplang/emille/ translation, in order to gain acceptable results. Because bitext was so difficult to find for lowerdensity languages, corpus creation efforts rely largely, if not exclusively, on contracting out text for translation. In most cases, source text is harvested from news sites in the target language, and then translated into English by commercial translation agencies, at a rate usually in the neighborhood of US$0.25 per word. In theory, one could reduce this cost by de</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Nirenburg</author>
<author>Victor Raskin</author>
</authors>
<title>Universal grammar and lexis for quick ramp-up of mt.</title>
<date>1998</date>
<booktitle>In Proceedings of 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>975--979</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23937" citStr="Nirenburg and Raskin, 1998" startWordPosition="3767" endWordPosition="3770">b data (Ghani et al., 2001) Some of these corpora have grown to reasonable sizes; (Scannell, 2003; Scannell, 2006) has corpora derived from web crawling which are measured in tens of millions of words for a variety of lower-density languages. However it should be noted that in these cases, the type of linguistic resource created is often not linguistically annotated, but rather a lexicon or collection of primary texts in a given language. Finally, we may mention efforts to create certain kinds of resources by computer-directed elicitation. Examples of projects sharing this focus include BOAS (Nirenburg and Raskin, 1998), and the AVENUE project (Probst et al., 2002), (Lavie et al., 2003). 7 Reducing Data Requirements Creating more annotated resources is the obvious way to approach the problem of the lack of resources for lower-density languages. A complementary approach is to improve the way the information in smaller resources is used, for example by developing machine translation systems that require less parallel text. How much reduction in the required amount of resources might be enough? An interesting experiment, which to our knowledge has never been tried, would be for a linguist to attempt as a test c</context>
</contexts>
<marker>Nirenburg, Raskin, 1998</marker>
<rawString>Sergei Nirenburg and Victor Raskin. 1998. Universal grammar and lexis for quick ramp-up of mt. In Proceedings of 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 975–979. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas W Oard</author>
</authors>
<title>The surprise language exercises.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="21398" citStr="Oard, 2003" startWordPosition="3386" endWordPosition="3387">a short article in need of expansion”), or were simply links to Internet blogs, many of which were monolingual in English. 33 (Bokm´al and Nynorsk), Romanian, Russian, Slovak, Slovenian, Serbian, Finnish, Thai, Turkish, Ukrainian, and Chinese. The dominance of European languages in these lists is obvious. During a TIDES exercise in 2003, researchers at Johns Hopkins University explored an innovative approach to the creation of bitext (parallel English and Hindi text, aligned at the sentence level): they elicited translations into English of Hindi sentences they posted on an Internet web page (Oard, 2003; Yarowsky, 2003). Participants were paid for the best translations in Amazon.com gift certificates, with the quality of a twenty percent subset of the translations automatically evaluated using BLEU scores against highly scored translations of the same sentences from previous rounds. This pool of high-quality translations was initialized to a set of known quality translations. A valuable side effect of the use of previously translated texts for evaluation is that this created a pool of multiply translated texts. The TIDES translation exercise quickly produced a large body of translated text: </context>
</contexts>
<marker>Oard, 2003</marker>
<rawString>Douglas W. Oard. 2003. The surprise language exercises. ACM Transactions on Asian Language Information Processing (TALIP), 2(2):79–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="6051" citStr="Palmer et al., 2005" startWordPosition="922" endWordPosition="925">vels of parallelism, including morpheme-level glossing) • Text annotated for named entities at various levels of granularity • Morphologically analyzed text (for nonisolating languages; at issue here is particularly inflectional morphology, and to a lesser degree of importance for most computational purposes, derivational morphology); also a morphological tag schema appropriate to the particular language • POS tagged text, and a POS tag schema appropriate to the particular language • Treebanked (syntactically annotated and parsed) text • Semantically tagged text (semantic roles) cf. Propbank (Palmer et al., 2005), or frames cf. Framenet1 • Electronic dictionaries and other lexical resources, such as Wordnet2 There are numerous dimensions for linguistically annotated resources, and a range of research projects have attempted to identify the core properties of interest. While concepts such as the Basic Language Resource Kit (BLARK; (Krauwer, 2003; Mapelli and Choukri, 2003)) have gained considerable currency in higher-density language resource creation projects, it is clear that the baseline requirements of such schemes are significantly more advanced than we can hope for for lower-density languages in </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Probst</author>
<author>L Levin</author>
<author>E Peterson</author>
<author>A Lavie</author>
<author>J Carbonell</author>
</authors>
<title>Mt for minority languages using elicitation-based learning of syntactic transfer rules.</title>
<date>2002</date>
<journal>Machine Translation,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="23983" citStr="Probst et al., 2002" startWordPosition="3775" endWordPosition="3778">e grown to reasonable sizes; (Scannell, 2003; Scannell, 2006) has corpora derived from web crawling which are measured in tens of millions of words for a variety of lower-density languages. However it should be noted that in these cases, the type of linguistic resource created is often not linguistically annotated, but rather a lexicon or collection of primary texts in a given language. Finally, we may mention efforts to create certain kinds of resources by computer-directed elicitation. Examples of projects sharing this focus include BOAS (Nirenburg and Raskin, 1998), and the AVENUE project (Probst et al., 2002), (Lavie et al., 2003). 7 Reducing Data Requirements Creating more annotated resources is the obvious way to approach the problem of the lack of resources for lower-density languages. A complementary approach is to improve the way the information in smaller resources is used, for example by developing machine translation systems that require less parallel text. How much reduction in the required amount of resources might be enough? An interesting experiment, which to our knowledge has never been tried, would be for a linguist to attempt as a test case what we hope that computers can do. That i</context>
</contexts>
<marker>Probst, Levin, Peterson, Lavie, Carbonell, 2002</marker>
<rawString>K. Probst, L. Levin, E. Peterson, A. Lavie, and J. Carbonell. 2002. Mt for minority languages using elicitation-based learning of syntactic transfer rules. Machine Translation, 17(4):245–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="17558" citStr="Resnik and Smith, 2003" startWordPosition="2779" endWordPosition="2782"> done by a speaker of the language (more accurately, a reader: for Punjabi, for instance, it can be difficult to find fluent readers of the Gurmukhi script). Preferably the annotator is familiar enough with current events in the country where the language is spoken that they can interpret cross-references in the text. If two or more annotators are available, the work can be done somewhat more quickly. More importantly, there can be some checking for inter-annotator agreement (and revision taking into account such differences as are found). Earlier work on corpus collection from the web (e.g. (Resnik and Smith, 2003)) gave some hope that reasonably large quantities of parallel text could be found on the web, so that a bitext collection could be built for interesting language pairs (with one member of the pair usually being English) relatively cheaply. Subsequent experience with lower-density languages has not born that hope out; parallel text on the web seems relatively rare for most languages. It is unclear why this should be. Certainly in countries like India, there are large amounts of news text in English and many of the target languages (such as Hindi). Nevertheless, very little of that text seems to</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Mari Broman Olsen</author>
<author>Mona Diab</author>
</authors>
<title>The Bible as a parallel corpus: Annotating the ‘Book of</title>
<date>1999</date>
<booktitle>Tongues’. Computers and the Humanities,</booktitle>
<pages>33--1</pages>
<contexts>
<context position="26891" citStr="Resnik et al., 1999" startWordPosition="4238" endWordPosition="4241">andardised corpora to which language technologists are accustomed. Neverthless for extremely low density languages, a degree of standardisation is apparent by virtue of documentary linguistic practice. Consider for example, the number of Shoebox lexicons and corresponding interlinear texts which are potentially available from documentary sources: while not being the traditional resource types on which systems are trained, they are reasonably accessible, and cover a larger number of languages. Bible translations are another form of parallel text available in nearly every written language (see (Resnik et al., 1999)). There are of course issues of quality, not to mention vocabulary, that arise from using the Bible as a source of parallel text, but for some purposes – such as morphology learning – Bible translations might be a very good source of data. Similarly, a different compromise may be found in the ratio of the number of words in a corpus to the richness of linguistic annotation. In many high-density corpora development projects, an arbitrary (and high) target for the number of words is often set in advance, and subsequent linguistic annotation is layered over this base corpus in a progressively mo</context>
</contexts>
<marker>Resnik, Olsen, Diab, 1999</marker>
<rawString>Philip Resnik, Mari Broman Olsen, and Mona Diab. 1999. The Bible as a parallel corpus: Annotating the ‘Book of 2000 Tongues’. Computers and the Humanities, 33(1-2):129–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Scannell</author>
</authors>
<title>Automatic thesaurus generation for minority languages: an irish example.</title>
<date>2003</date>
<booktitle>In Actes des Traitement Automatique des Langues Minoritaires et des Petites Langues,</booktitle>
<volume>2</volume>
<pages>203--212</pages>
<contexts>
<context position="23407" citStr="Scannell, 2003" startWordPosition="3683" endWordPosition="3684">orms of annotation will be easier to do using an open source methodology than others will. For example, translation and possibly named entity annotation might be fairly straightforward, while morphological analysis is probably more difficult, particularly for morphologically complex languages. 13http://www.wikipedia.org 14http://www.mturk.com/mturk/ 15http://litgloss.buffalo.edu/ 16http://www.espgame.org/ 17http://wiktionary.org/ Other researchers have experimented with the automatic creation of corpora using web data (Ghani et al., 2001) Some of these corpora have grown to reasonable sizes; (Scannell, 2003; Scannell, 2006) has corpora derived from web crawling which are measured in tens of millions of words for a variety of lower-density languages. However it should be noted that in these cases, the type of linguistic resource created is often not linguistically annotated, but rather a lexicon or collection of primary texts in a given language. Finally, we may mention efforts to create certain kinds of resources by computer-directed elicitation. Examples of projects sharing this focus include BOAS (Nirenburg and Raskin, 1998), and the AVENUE project (Probst et al., 2002), (Lavie et al., 2003). </context>
</contexts>
<marker>Scannell, 2003</marker>
<rawString>Kevin Scannell. 2003. Automatic thesaurus generation for minority languages: an irish example. In Actes des Traitement Automatique des Langues Minoritaires et des Petites Langues, volume 2, pages 203–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Scannell</author>
</authors>
<title>Machine translation for closely related language pairs.</title>
<date>2006</date>
<booktitle>In Proceedings of the LREC2006 Workshop on Strategies for</booktitle>
<location>Paris.</location>
<contexts>
<context position="23424" citStr="Scannell, 2006" startWordPosition="3685" endWordPosition="3687">on will be easier to do using an open source methodology than others will. For example, translation and possibly named entity annotation might be fairly straightforward, while morphological analysis is probably more difficult, particularly for morphologically complex languages. 13http://www.wikipedia.org 14http://www.mturk.com/mturk/ 15http://litgloss.buffalo.edu/ 16http://www.espgame.org/ 17http://wiktionary.org/ Other researchers have experimented with the automatic creation of corpora using web data (Ghani et al., 2001) Some of these corpora have grown to reasonable sizes; (Scannell, 2003; Scannell, 2006) has corpora derived from web crawling which are measured in tens of millions of words for a variety of lower-density languages. However it should be noted that in these cases, the type of linguistic resource created is often not linguistically annotated, but rather a lexicon or collection of primary texts in a given language. Finally, we may mention efforts to create certain kinds of resources by computer-directed elicitation. Examples of projects sharing this focus include BOAS (Nirenburg and Raskin, 1998), and the AVENUE project (Probst et al., 2002), (Lavie et al., 2003). 7 Reducing Data R</context>
</contexts>
<marker>Scannell, 2006</marker>
<rawString>Kevin Scannell. 2006. Machine translation for closely related language pairs. In Proceedings of the LREC2006 Workshop on Strategies for developing machine translation for minority languages. European Language Resources Association: Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sigurbj¨onsson</author>
<author>J Kamps</author>
<author>M de Rijke</author>
</authors>
<title>Blueprint of a cross-lingual web collection.</title>
<date>2005</date>
<journal>Journal of Digital Information Management,</journal>
<volume>3</volume>
<issue>1</issue>
<marker>Sigurbj¨onsson, Kamps, de Rijke, 2005</marker>
<rawString>B. Sigurbj¨onsson, J. Kamps, and M. de Rijke. 2005. Blueprint of a cross-lingual web collection. Journal of Digital Information Management, 3(1):9–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Somers</author>
</authors>
<title>Machine translation and minority languages.</title>
<date>1997</date>
<journal>Translating and the Computer,</journal>
<volume>19</volume>
<contexts>
<context position="25746" citStr="Somers, 1997" startWordPosition="4066" endWordPosition="4067">ly shifts the problem from the research community interested in linguistically annotated corpora to the research community interested in algorithms. Much effort has been invested in scaling algorithmic approaches upwards, that is, leveraging every last available data point in pursuit of small performance improvements. We argue that scaling down (ie using less training data) poses an equally sig34 nificant challenge. The basic question of whether methods which are data-rich can scale down to impoverished data has been the focus of a number of recent papers in areas such as machine translation (Somers, 1997; Somers, 1998), language identification (Hughes et al., 2006b) etc. However, tasks which have lower-density language at their core have yet to become mainstream in shared evaluation tasks which drive much of the algorithmic improvements in computational linguistics and natural language processing. Another approach to data reduction is to change the type of data required for a given task. For many lower-density languages a significant volume of linguistically annotated data exists, but not in the form of the curated, standardised corpora to which language technologists are accustomed. Neverthl</context>
</contexts>
<marker>Somers, 1997</marker>
<rawString>Harold Somers. 1997. Machine translation and minority languages. Translating and the Computer, 19:1– 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Somers</author>
</authors>
<title>Language resources and minority languages. Language Today,</title>
<date>1998</date>
<pages>5--20</pages>
<contexts>
<context position="25761" citStr="Somers, 1998" startWordPosition="4068" endWordPosition="4069">problem from the research community interested in linguistically annotated corpora to the research community interested in algorithms. Much effort has been invested in scaling algorithmic approaches upwards, that is, leveraging every last available data point in pursuit of small performance improvements. We argue that scaling down (ie using less training data) poses an equally sig34 nificant challenge. The basic question of whether methods which are data-rich can scale down to impoverished data has been the focus of a number of recent papers in areas such as machine translation (Somers, 1997; Somers, 1998), language identification (Hughes et al., 2006b) etc. However, tasks which have lower-density language at their core have yet to become mainstream in shared evaluation tasks which drive much of the algorithmic improvements in computational linguistics and natural language processing. Another approach to data reduction is to change the type of data required for a given task. For many lower-density languages a significant volume of linguistically annotated data exists, but not in the form of the curated, standardised corpora to which language technologists are accustomed. Neverthless for extreme</context>
</contexts>
<marker>Somers, 1998</marker>
<rawString>Harold Somers. 1998. Language resources and minority languages. Language Today, 5:20–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>B Pouliquen</author>
<author>A Widger</author>
<author>C Ignat</author>
<author>T Erjavec</author>
<author>D Tufis</author>
<author>D Varga</author>
</authors>
<title>The jrcacquis: A multilingual aligned parallel corpus with 20+ languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC2006). European Language Resources Association:</booktitle>
<location>Paris.</location>
<contexts>
<context position="31389" citStr="Steinberger et al., 2006" startWordPosition="4934" endWordPosition="4937">in cross-linguistic scalability (eg CLEF) or data intensivity (eg TREC) or both (eg NTCIR). Within these constructs there is certainly room for the inclusion of lowerdensity languages as targets, although notably the overhead here is not in the provision of the language data, but the derivatives (eg query topics) on which these exercises are based. • Promotion of multilingual corpora which include lower-density languages: as multilingual corpora emerge, there is opportunity to include lower-density languages at minimal opportunity cost e.g. EuroGOV (Sigurbj¨onsson et al., 2005) or JRC-Acquis (Steinberger et al., 2006), which are based on web data from the EU, includes a number of lower-density languages by virtue of the corpus creation mechanism not being languagespecific. • Language specific strategies: collectively we have done well at developing formal strategies for high density languages e.g. in EU roadmaps, but not so well at strategies for medium-density or lower-density languages. The models for medium to long term strategies of language resource development may be adopted for lower density languages. Recently this has been evidenced through events such as the LREC 2006 workshop on African language</context>
</contexts>
<marker>Steinberger, Pouliquen, Widger, Ignat, Erjavec, Tufis, Varga, 2006</marker>
<rawString>R. Steinberger, B. Pouliquen, A. Widger, C. Ignat, T. Erjavec, D. Tufis, and D. Varga. 2006. The jrcacquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC2006). European Language Resources Association: Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Strassel</author>
<author>Mike Maxwell</author>
<author>Christopher Cieri</author>
</authors>
<title>Linguistic resource creation for research and technology development: A recent experiment.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="14170" citStr="Strassel et al., 2003" startWordPosition="2218" endWordPosition="2221"> who might be better able to build language resources than speakers living in the perhaps less economically developed home countries? (Expatriate communities may also be motivated by a desire to maintain their language among younger speakers, born abroad.) 5. Which languages would require more work (and funding) to build resources, but are still plausible candidates for short term efforts? To our knowledge, there is no general, on-going effort to collect the sort of data that would make answers to these questions possible. A survey was done at the Linguistic Data Consortium several years ago (Strassel et al., 2003) , for text-based resources for the three hundred or so languages having at least a million speakers (an arbitrary cutoff, to be sure, but necessary for the survey to have had at least some chance of success). It was remarkably successful, considering that it was done by two linguists who did not know the vast majority of the languages surveyed. The survey was funded long enough to ‘finish’ about 150 languages, but no subsequent update was ever done. A better model for such a survey might be an edited book: one or more computational linguists would serve as ‘editors’, responsible for the overa</context>
</contexts>
<marker>Strassel, Maxwell, Cieri, 2003</marker>
<rawString>Stephanie Strassel, Mike Maxwell, and Christopher Cieri. 2003. Linguistic resource creation for research and technology development: A recent experiment. ACM Transactions on Asian Language Information Processing (TALIP), 2(2):101–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Scalable elicitation of training data for machine translation.</title>
<date>2003</date>
<journal>Team Tides,</journal>
<volume>4</volume>
<contexts>
<context position="21415" citStr="Yarowsky, 2003" startWordPosition="3388" endWordPosition="3389">cle in need of expansion”), or were simply links to Internet blogs, many of which were monolingual in English. 33 (Bokm´al and Nynorsk), Romanian, Russian, Slovak, Slovenian, Serbian, Finnish, Thai, Turkish, Ukrainian, and Chinese. The dominance of European languages in these lists is obvious. During a TIDES exercise in 2003, researchers at Johns Hopkins University explored an innovative approach to the creation of bitext (parallel English and Hindi text, aligned at the sentence level): they elicited translations into English of Hindi sentences they posted on an Internet web page (Oard, 2003; Yarowsky, 2003). Participants were paid for the best translations in Amazon.com gift certificates, with the quality of a twenty percent subset of the translations automatically evaluated using BLEU scores against highly scored translations of the same sentences from previous rounds. This pool of high-quality translations was initialized to a set of known quality translations. A valuable side effect of the use of previously translated texts for evaluation is that this created a pool of multiply translated texts. The TIDES translation exercise quickly produced a large body of translated text: 300K words, in fi</context>
</contexts>
<marker>Yarowsky, 2003</marker>
<rawString>David Yarowsky. 2003. Scalable elicitation of training data for machine translation. Team Tides, 4.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>