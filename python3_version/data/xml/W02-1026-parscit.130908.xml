<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000081">
<note confidence="0.443928666666667">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 196-203.
Association for Computational Linguistics.
</note>
<title confidence="0.995297">
Manipulating Large Corpora for Text Classification
</title>
<author confidence="0.996752">
Fumiyo Fukumoto and Yoshimi Suzuki
</author>
<affiliation confidence="0.9978685">
Department of Computer Science and Media Engineering,
Yamanashi University
</affiliation>
<address confidence="0.987079">
4-3-11 Takeda, Kofu 400-8511 Japan
</address>
<email confidence="0.999532">
fukumoto@skye.esb.yamanashi.ac.jp ysuzuki@alps1.esi.yamanashi.ac.jp
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946722222222">
In this paper, we address the problem of
dealing with a large collection of data
and propose a method for text classifi-
cation which manipulates data using two
well-known machine learning techniques,
Naive Bayes(NB) and Support Vector Ma-
chines(SVMs). NB is based on the as-
sumption of word independence in a text,
which makes the computation of it far
more efficient. SVMs, on the other hand,
have the potential to handle large feature
spaces, which makes it possible to pro-
duce better performance. The training
data for SVMs are extracted using NB
classifiers according to the category hier-
archies, which makes it possible to reduce
the amount of computation necessary for
classification without sacrificing accuracy.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994465">
As the volume of online documents has drastically
increased, text classification has become more im-
portant, and a growing number of statistical and ma-
chine learning techniques have been applied to the
task(Lewis, 1992), (Yang and Wilbur, 1995), (Baker
and McCallum, 1998), (Lam and Ho, 1998), (Mc-
Callum, 1999), (Dumais and Chen, 2000). Most of
them use the Reuters-21578 articles1 in the evalu-
</bodyText>
<footnote confidence="0.95465075">
1The Reuters-21578, distribution 1.0, is comprised of
21,578 documents, representing what remains of the original
Reuters-22173 corpus after the elimination of 595 duplicates
by Steve Lynch and David Lewis in 1996.
</footnote>
<bodyText confidence="0.998372673684211">
ations of their methods, since the corpus has be-
come a benchmark, and their results are thus eas-
ily compared with other results. It is generally
agreed that these methods using statistical and ma-
chine learning techniques are effective for classifi-
cation task, since most of them showed significant
improvement (the performance over 0.85 F1 score)
for Reuters-21578(Joachims, 1998), (Dumais et al.,
1998), (Yang and Liu, 1999).
More recently, some researchers have applied
their techniques to larger corpora such as web
pages in Internet applications(Mladenic and Grobel-
nik, 1998), (McCallum, 1999), (Dumais and Chen,
2000). The increasing number of documents and
categories, however, often hampers the develop-
ment of practical classification systems, mainly due
to statistical, computational, and representational
problems(Dietterich, 2000). There are at least two
strategies for solving these problems. One is to
use category hierarchies. The idea behind this is
that when humans organize extensive data sets into
fine-grained categories, category hierarchies are of-
ten employed to make the large collection of cate-
gories more manageable. McCallum et. al. pre-
sented a method called ‘shrinkage’ to improve pa-
rameter estimates by taking advantage of a hierar-
chy(McCallum, 1999). They tested their method us-
ing three different real-world datasets: 20,000 ar-
ticles from UseNet, 6,440 web pages from the in-
dustry sector, and 14,831 pages from Yahoo, and
showed improved performance. Dumais et. al.
used SVMs and classified hierarchical web content
consisting of 50,078 web pages for training, and
10,024 for testing, with promising results(Dumais
and Chen, 2000).
The other is to use methods which are
learning algorithms that construct a set of classifiers
and then classify new data by taking a (weighted)
vote of their predictions(Dietterich, 2000). One
of the methods for constructing ensembles manipu-
lates the training examples to generate multiple hy-
potheses. The most straightforward way is called
. It presents the learning algorithm with a
training set that consists of a sample of examples
drawn randomly with replacement from the original
training set. The second method is to construct the
training sets by leaving out disjoint subsets of the
training data. The third is illustrated by the AD-
ABOOST algorithm(Freund and Schapire, 1996).
Dietterich has compared these methods(Dietterich,
2000). He reported that in low-noise data, AD-
ABOOST performs well, while in high-noise cases,
it yields overfitting because ADABOOST puts a
large amount of weight on the mislabeled examples.
Bagging works well on both the noisy and the noise-
free data because it focuses on the statistical prob-
lem which arises when the amount of training data
available is too small, and noise increases this sta-
tistical problem. However, it is not clear whether
‘works well’ means that it exponentially reduces the
amount of computation necessary for classification,
while sacrificing only a small amount of accuracy,
or whether it is statistically significantly better than
other methods.
In this paper, we address the problem of dealing
with a large collection of data and report on an em-
pirical study for text classification which manipu-
lates data using two well-known machine learning
techniques, Naive Bayes(NB) and Support Vector
Machines(SVMs). NB probabilistic classifiers are
based on the assumption of word independence in a
text which makes the computation of the NB classi-
fiers far more efficient. SVMs, on the other hand,
have the potential to handle large feature spaces,
since SVMs use overfitting protection which does
not necessarily depend on the number of features,
and thus makes it possible to produce better perfor-
mance. The basic idea of our approach is quite sim-
ple: We solve simple classification problems using
NB and more complex and difficult problems using
SVMs. As in previous research, we use category
hierarchies. We use all the training data for NB.
The training data for SVMs, on the other hand, is
extracted using NB classifiers. The training data is
learned by NB using cross-validation according to
the hierarchical structure of categories, and only the
documents which could not classify correctly by NB
classifiers in each category level are extracted as the
training data of SVMs.
The rest of the paper is organized as follows. The
next section provides the basic framework of NB and
SVMs. We then describe our classification method.
Finally, we report some experiments using 279,303
documents in the Reuters 1996 corpus with a discus-
sion of evaluation.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="introduction">
2 Classifiers
</sectionHeader>
<subsectionHeader confidence="0.536112">
2.1 NB
</subsectionHeader>
<bodyText confidence="0.977278">
Naive Bayes(NB) probabilistic classifiers are com-
monly studied in machine learning(Mitchell, 1996).
The basic idea in NB approaches is to use the joint
probabilities of words and categories to estimate the
probabilities of categories given a document. The
NB assumption is that all the words in a text are
conditionally independent given the value of a clas-
sification variable. There are several versions of the
NB classifiers. Recent studies on a Naive Bayes
classifier which is proposed by McCallum et. al.
reported high performance over some other com-
monly used versions of NB on several data collec-
tions(McCallum et al., 1998). We use the model of
NB by McCallum et. al. which is shown in formula
(1).
refers to the number of vocabularies, de-
notes the number of labeled training documents, and
shows the number of categories. denotes
document length. is the word in position of
</bodyText>
<equation confidence="0.701541">
(1)
</equation>
<bodyText confidence="0.9716745">
document , where the subscript of ,indicates
an index into the vocabulary. denotes the
number of times word occurs in document , and
is defined by 0,1 .
</bodyText>
<subsectionHeader confidence="0.997576">
2.2 SVMs
</subsectionHeader>
<bodyText confidence="0.999610545454545">
SVMs are introduced by Vapnik(Vapnik, 1995) for
solving two-class pattern recognition problems. It
is defined over a vector space where the problem is
to find a decision surface(classifier) that ‘best’ sep-
arates a set of positive examples from a set of nega-
tive examples by introducing the maximum ‘margin’
between two sets. The margin is defined by the dis-
tance from the hyperplane to the nearest of the pos-
itive and negative examples. The decision surface
produced by SVMs for linearly separable space is a
hyperplane which can be written as + = 0 (
</bodyText>
<figureCaption confidence="0.999631">
Figure 1: The decision surface of a linear SVMs
</figureCaption>
<bodyText confidence="0.9952405">
In the linearly separable case maximizing the margin
can be expressed as an optimization problem:
</bodyText>
<footnote confidence="0.782249">
2We focused on linear hypotheses in this work, while SVMs
</footnote>
<bodyText confidence="0.964645958333333">
can handle nonlinear hypotheses using functions.
) corresponds to each word in the training ex-
amples, and the larger value of is,
the more the word features positive examples.
We note that SVMs are basically introduced for
solving binary classification, while text classifica-
tion is a multi-class, multi-label classification prob-
lem. Several methods using SVMs which were in-
tended for multi-class, multi-label data have been
proposed(Weston and Watkins, 1998). We use -
- - version of the SVMs model in
the work. A time complexity of SVMs is known
as , where is the number of train-
ing data. We consider a time complexity of -
- - method. Let be the number
of training data with categories. The average
size of the training data per category is . Let
also be the time needed to train all cat-
egories, where represents the time for learn-
ing one binary classifier using training data, and
is the number of binary classifier. The time for
learning one binary classifier, is represented as
, where is a constant. - -
- method is thus done in time .
</bodyText>
<sectionHeader confidence="0.99716" genericHeader="method">
3 System Design
</sectionHeader>
<subsectionHeader confidence="0.999842">
3.1 Hierarchical classification
</subsectionHeader>
<bodyText confidence="0.999938444444444">
A well-known technique for classifying a large, het-
erogeneous collection such as web content is to use
category hierarchies. Following the approaches of
Koller and Sahami(Koller and Sahami, 1997), and
Dumais’s(Dumais and Chen, 2000), we employ a hi-
erarchy by learning separate classifiers at each in-
ternal node of the tree, and then labeling a docu-
ment using these classifiers to greedily select sub-
branches until a leaf is reached.
</bodyText>
<subsectionHeader confidence="0.999976">
3.2 Manipulating training data
</subsectionHeader>
<bodyText confidence="0.999989">
Our hypothesis regarding NB is that it can work well
for documents which are assigned to only one cate-
gory within the same category level in the hierar-
chical structure. We base this on some recent papers
claiming that NB methods perform surprisingly well
for an ‘accuracy’ measure which is equivalent to
the standard precision under the one-category-per-
document assumption on classifiers and also equiva-
lent to the standard recall, assuming that each docu-
ment has one and only one correct category per cat-
</bodyText>
<figure confidence="0.989572666666667">
Origin
Negative examples
Positive examples
w
−
b
w
Margin
s.t
</figure>
<bodyText confidence="0.998950333333333">
where = ( , , ) is the -th training example
and is a label corresponding the -th training ex-
ample. In formula (3), each element of w, (1
,
,), where is an arbitrary data point,
and = ( , , ) and are learned from a train-
ing set of linearly separable data. Figure 1 shows an
example of a simple two-dimensional problem that
is linearly separable2.
egory level(Lewis and Ringuette, 1994), (Koller and
Sahami, 1997). SVMs, on the other hand, have the
potential to handle more complex problems without
sacrificing accuracy, even though the computation of
the SVM classifiers is far less efficient than NB. We
thus use NB for simple classification problems and
SVMs for more complex data, i.e., the data which
cannot be classified correctly by NB classifiers. We
use ten-fold cross validation: All of the training data
were randomly shuffled and divided into ten equal
folds. Nine folds were used to train the NB clas-
sifiers while the remaining fold(held-out test data)
was used to evaluate the accuracy of the classifica-
tion. For each category level, we apply the following
procedures. Let be the total number of nine folds
training documents, and be the number of the re-
maining fold in each class level. Figure 2 illustrates
the flow of our system.
</bodyText>
<figureCaption confidence="0.933096">
Figure 2: Flow of our system
</figureCaption>
<sectionHeader confidence="0.293608" genericHeader="method">
1. Extracting training data using NB
</sectionHeader>
<bodyText confidence="0.973396714285714">
1-1 NB is applied to the documents, and clas-
sifiers for each category are induced. They are
evaluated using the held-out test data, the
documents.
1-2 This process is repeated ten times so that each
fold serves as the source of the test data once.
The threshold, the probability value which pro-
duces the most accurate classifier through ten
runs, is selected.
1-3 The held-out test data which could not be clas-
sified correctly by NB classifiers with the opti-
mal parameters are extracted ( in Figure
2). They are used to train SVMs.
The procedure is applied to each category level.
</bodyText>
<sectionHeader confidence="0.958275" genericHeader="method">
2. Classifying test data
</sectionHeader>
<bodyText confidence="0.9968942">
2-1 We use all the training data, + , to train
NB classifiers and the data which is produced
by procedure 1-3 to train SVMs.
2-2 NB classifiers are applied to the test data. The
test data is judged to be the category whose
probability is larger than the threshold which is
obtained by 1-2.
2-3 If the test data is not assigned to any one of the
categories, the test data is classified by SVMs
classifiers. The test data is judged to be the cat-
egory whose distance is larger than zero.
We employ the hierarchy by learning separate classi-
fiers at each internal node of the tree and then assign
categories to a document by using these classifiers to
greedily select sub-branches until a leaf is reached.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.997726">
4.1 Data and Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999571476190476">
We evaluated the method using the 1996 Reuters
corpus recently made available. The corpus from
20th Aug. to 31st Dec. consists of 279,303 doc-
uments. These documents are organized into 126
categories with a four level hierarchy. We selected
102 categories which have at least one document in
the training set and the test set. The number of cate-
gories in each level is 25 top level, 33 second level,
43 third level, and 1 fourth level, respectively. Table
1 shows the number of documents in each top level
category.
After eliminating unlabelled documents, we ob-
tained 271,171 documents. We divided these docu-
ments into two sets: a training set from 20th Aug.
to 31th Oct. which consists of 150,939 documents,
and test set from 1th Nov. to 31st Dec. which
consists of 120,242 documents. We obtained a vo-
cabulary of 183,400 unique words after eliminating
words which occur only once, stemming by a part-
of-speech tagger(Schmid, 1995), and stop word re-
moval. Figure 3 illustrates the category distribution
</bodyText>
<figure confidence="0.982569578947368">
1 Extracting training documents using NB
test
documents
Nb documents
incorrect documents by NB
estimating
parameters
Nerror documents
NB
training documents
Na documents
2 Classifying test documents
output
YES
SVM
NB
category
assigned?
NO
</figure>
<table confidence="0.969621620689655">
Na+Nb documents Nerror documents
Nt documents
Category name Training Test
Corporate/Industrial 69,975 56,100
Economics 22,214 18,694
Government/social 45,618 36,923
Crime 6,248 4,865
Defence 1,646 1,408
International relations 7,523 6,278
Disasters 1,644 1,383
Arts 771 602
Environment 1,170 876
Fashion 71 14
Health 1,232 961
Labour issues 3,314 2,827
Obituaries 123 124
Human interest 479 418
Domestic politics 11,528 9,022
Biographies 1,115 1,041
Religion 618 418
Science and technology 359 410
Sports 5,807 4,998
Travel and tourism 149 142
War 7,064 5,228
Elections 3,070 1,944
Weather 784 474
Welfare 359 260
Markets 34,901 28,484
Total 227,782 183,894
</table>
<tableCaption confidence="0.999744">
Table 1: Top level categories
</tableCaption>
<figureCaption confidence="0.78893">
in the training set. The number of categories per
document is 3.2 on average.
Figure 3: Category distribution in Reuters 1996
</figureCaption>
<bodyText confidence="0.9909044">
We use ten-fold cross validation for learning NB
parameters. For evaluating the effectiveness of cate-
gory assignments, we use the standard recall, preci-
sion, and measures. Recall denotes the ratio of
correct assignments by the system divided by the to-
tal number of correct assignments. Precision is the
ratio of correct assignments by the system divided
by the total number of the system’s assignments.
The measure which combines recall ( ) and pre-
cision ( ) with an equal weight is .
</bodyText>
<subsectionHeader confidence="0.390337">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.68096">
The result is shown in Table 2.
</bodyText>
<table confidence="0.999710875">
category Performance
miR miP miF1
NB all 0.684 0.419 0.519
parts 0.565 0.523 0.543
SVMs all 0.318 0.258 0.285
parts 0.795 0.554 0.653
Manipulating all 0.703 0.704 0.704
data parts 0.720 0.692 0.700
</table>
<tableCaption confidence="0.999302">
Table 2: Categorization accuracy
</tableCaption>
<bodyText confidence="0.999886303030303">
‘NB’, ‘SVMs’, and ‘Manipulating data’ denotes the
result using Naive Bayes, SVMs classifiers, and our
method, respectively. ‘miR’, ‘miP’, and ‘miF1’
refers to the micro-averaged recall, precision, and
F1, respectively. ‘all’ in Table 2 shows the results
of all 102 categories. The micro-averaged F1 score
of our method in ‘all’ (0.704) is higher than the NB
(0.519) and SVMs scores (0.285). We note that the
F1 score of SVMs (0.285) is significantly lower than
other models. This is because we could not obtain a
classifier to judge the category ‘corporate/industrial’
in the top level within 10 days using a standard 2.4
GHz Pentium IV PC with 1,500 MB of RAM. We
thus eliminated the category and its child categories
from the 102 categories. The number of the remain-
ing categories in each level is 24 top, 14 second,
29 third, and 1 fourth level. ‘Parts’ in Table 2 de-
notes the results. There is no significant difference
between ‘all’ and ‘parts’ in our method, as the F1
score of ‘all’ was 0.704 and ‘parts’ was 0.700. The
F1 of our method in ‘parts’ is also higher than the
NB and SVMs scores.
Table 3 denotes the amount of training data used
to train NB and SVMs in our method and test data
judged by each classifier. We can see that our
method makes the computation of the SVMs more
efficient, since the data trained by SVMs is only
23,243 from 150,939 documents.
Table 4 illustrates the results of three methods ac-
cording to each category level. ‘Training’ in ‘Ma-
nipulating data’ denotes the number of documents
used to train SVMs. The overall F1 value of NB,
SVMs, and our method for the 25 top-level cate-
</bodyText>
<table confidence="0.971124833333333">
Manipulating # of selected documents miR miP miF1
data
training test
NB 150,939 76,650 0.798 0.674 0.730
SVMs 23,243 43,592 0.789 0.588 0.674
Total performance 0.703 0.704 0.704
</table>
<tableCaption confidence="0.991507">
Table 3: # of selected documents and categorization accuracy
</tableCaption>
<table confidence="0.999947375">
Top level(25 categories)
training miR miP miF1
NB 147,576 0.877 0.573 0.693
SVMs 147,576 0.358 0.325 0.341
Manipulating 22,528 0.836 0.679 0.715
data
Second level(33 categories)
training miR miP miF1
NB 129,130 0.559 0.529 0.543
SVMs 129,130 0.327 0.302 0.314
Manipulating 17,667 0.833 0.478 0.608
data
Third level(43 categories)
training miR miP miF1
NB 92,320 0.609 0.383 0.471
SVMs 92,320 0.318 0.258 0.258
Manipulating 12,482 0.820 0.481 0.606
data
Fourth level(1 category)
training miR miP miF1
NB 150,939 0.397 0.184 0.251
SVMs 150,939 0.318 0.258 0.258
Manipulating 150,939 0.297 0.241 0.265
data
</table>
<tableCaption confidence="0.999698">
Table 4: Categorization accuracy by category level
</tableCaption>
<bodyText confidence="0.997586403508772">
gories is 0.693, 0.341, and 0.715, respectively. Clas-
sifying large corpora with similar categories is a
difficult task, so we did not expect to have excep-
tionally high accuracy like Reuters-21578 (0.85 F1
score). Performance on the original training set us-
ing SVMs is 0.285 and using NB is 0.519, so this is
a difficult learning task and generalization to the test
set is quite reasonable.
There is no significant difference between the
overall F1 value of the second(0.608) and third level
categories(0.606) in our method, while the accuracy
of the other methods drops when classifiers select
sub-branches, in third level categories. As Dumais
et. al. mentioned, the results of our experiment
show that performance varies widely across cate-
gories. The highest F1 score is 0.864 (‘Commodity
markets’ category), and the lowest is 0.284 (‘Eco-
nomic performance’ category).
The overall F1 values obtained by three methods
for the fourth level category (‘Annual result’) are
low. This is because there is only one category in
the level, and we thus used all of the training data,
150,939 documents, to learn models.
The contribution of the hierarchical structure is
best explained by looking at the results with and
without category hierarchies, as illustrated in Table
5. It is interesting to note that the results of both
NB and our method clearly demonstrate that incor-
porating category hierarchies into the classification
method improves performance, whereas hierarchies
degraded the performance of SVMs. This shows that
the separation of one top level category(C) from the
set of the other 24 top level categories is more dif-
ficult than separating C from the set of all the other
101 categories in SVMs.
Table 6 illustrates sample words which have the
highest weighted value calculated using formula (3).
Recall that in SVMs each value of word (1
) is calculated using formula (3), and the
larger value of is, the more the word fea-
tures positive examples. Table 6 denotes the results
of two binary classifiers. One is a classifier that
separates documents assigned the ‘Economics’ cat-
egory(positive examples) from documents assigned
a set of the other 24 top level categories, i.e. ‘hier-
archy’. The other is a classifier that separates doc-
uments with the ‘Economics’ category from doc-
uments with a set of the other 101 categories,
i.e., ‘non-hierarchy’. Table 6 shows that in ‘Non-
hierarchy’, words such as ‘economic’, ‘economy’
and ‘company’ which feature the category ‘Eco-
nomics’ have a high weighted value, while in ‘hi-
erarchy’, words such as ‘year’ and ‘month’ which
do not feature the category have a high weighted
value. Further research using various subsets of the
top level categories is necessary to fully understand
the influence of the hierarchical structure created by
</bodyText>
<table confidence="0.998136">
Non-hierarchy Hierarchy
miR miP miF1 miR miP miF1
NB 0.667 0.407 0.506 0.684 0.419 0.519
SVMs 0.655 0.524 0.582 0.318 0.258 0.258
Manipulating data 0.772 0.485 0.596 0.703 0.704 0.704
</table>
<tableCaption confidence="0.973851">
Table 5: Non-hierarchical v.s. Hierarchical categorization accuracy
</tableCaption>
<table confidence="0.959498">
humans.
Economics
Hierarchy access, Ford, Japan, Internet, econ-
omy, year, sale, service, month,
market
Non-hierarchy economic, economy, industry, ltd.,
company, Hollywood, business,
service, Internet, access
</table>
<tableCaption confidence="0.996252">
Table 6: Sample words
</tableCaption>
<bodyText confidence="0.997193891891892">
Finally, we compare our results with a well-
known technique, strategies. In the ex-
periment using ensemble, we divided a training set
into ten folds for each category level. Once the indi-
vidual classifiers are trained by SVMs they are used
to classify test data. Each classifier votes and the test
data is assigned to the category that receives more
than 6 votes3. The result is illustrated in Table 7.
In Table 7, ‘Non-hierarchy’ and ‘Hierarchy’ denotes
the result of the 102 categories treated as a flat non-
hierarchical problem, and the result using hierarchi-
cal structure, respectively. We can find that the re-
sult of with hierarchy(0.704 F1) outper-
forms the result with non-hierarchy(0.532 F1). A
necessary and sufficient condition for an ensemble
of classifiers to be more accurate than any of its in-
dividual members is if the classifiers are
and (Hansen and Salamon, 1990). An ac-
curate classifier is one that has an error rate bet-
ter than random guessing on new test data. Two
classifiers are diverse if they make different errors
on new data points. Given our result, it may be
safely said, at least regarding the Reuters 1996 cor-
pus, that hierarchical structure is more effective for
constructing ensembles, i.e., an ensemble of clas-
sifiers which are constructed by the training data
with fewer than 30 categories in each level is more
and . Table 7 shows that our
method and perform equally (0.704 F1
36 votes was the best results among 10 different voting
schemes in the experiment.
score) when we use hierarchical structure. How-
ever, the computation of the former is far more ef-
ficient than the latter. Furthermore, we see that
our method (0.596 F1 score) slightly outperforms
(0.532 F1 score) when the 102 categories
are treated as a flat non-hierarchical problem.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99982124">
We have reported an approach to text classifica-
tion which manipulates large corpora using NB and
SVMs. Our main conclusions are:
Our method outperforms the baselines, since
the micro-averaged score of our method
was 0.704 and the baselines were 0.519 for NB
and 0.285 for SVMs.
As shown in previous researches, hierarchical
structure is effective for classification, since the
result of our method using hierarchical struc-
ture led to as much as a 10.8% reduction in er-
ror rates, and up to 1.3% with NB.
There is no significant difference between the
F1 scores of our method and the
method with hierarchical structure. However,
the computation of our method is more efficient
than the method in the experiment.
Future work includes (i) extracting features which
discriminate between categories within the same
top-level category, (ii) investigating other machine
learning techniques to obtain further advantages in
efficiency in the manipulating data approach, and
(iii) evaluating the manipulating data approach us-
ing automatically generating hierarchies(Sanderson
and Croft, 1999).
</bodyText>
<sectionHeader confidence="0.998615" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.965998">
We would like to thank Prof. Virginia Teller of
Hunter College CUNY for her valuable comments
</bodyText>
<table confidence="0.99590525">
Non-hierarchy Hierarchy
miR miP miF1 miR miP miF1
0.625 0.464 0.532 0.704 0.705 0.704
Manipulating data 0.772 0.485 0.596 0.704 0.703 0.704
</table>
<tableCaption confidence="0.999663">
Table 7: Performance of v.s. Manipulating data
</tableCaption>
<bodyText confidence="0.998913">
and the anonymous reviewers for their helpful sug-
gestions. We also would like to express many thanks
to the Research and Standards Group of Reuters who
provided us the corpora.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998592784810127">
L. D. Baker and A. K. McCallum. 1998. Distributional
Clustering of Words for Text Classification. In Proc.
of the 22nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 96–103.
T. G. Dietterich. 2000. Ensemble Methods in Machine
Learning. In Proc. of the 1st International Workshop
on Multiple Classifier Systems.
S. Dumais and H. Chen. 2000. Hierarchical Classifi-
cation of Web Content. In Proc. of the 23rd Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 256–
263.
S. Dumais, J. Platt, D. Heckerman, and M. Sahami. 1998.
Inductive Learning Algorithm and Representations for
Text Categorization. In Proc. ofACM-CIKM98, pages
148–155.
Y. Freund and R. E. Schapire. 1996. Experiments with
a New Boosting Algorithm. In Proc. of the 13th In-
ternational Conference on Machine Learning, pages
148–156.
L. Hansen and P. Salamon. 1990. Neural Network En-
sembles. IEEETrans. Pattern Analysis and Machine
Intell., 12:993–1001.
T. Joachims. 1998. Text Categorization with Support
Vector Machines: Learning with Many Relevant Fea-
tures. In Proc. of the Conference on Machine Learn-
ing, pages 96–103.
D. Koller and M. Sahami. 1997. Hierarchically Classi-
fying Documents using Very Few Words. In Proc. of
the 14th International Conference on Machine Learn-
ing, pages 170–178.
W. Lam and C. Y. Ho. 1998. Using a Generalized In-
stance Set for Automatic Text Categorization. In Proc.
of the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 81–89.
D. D. Lewis and M. Ringuette. 1994. Comparison
of Two Learning Algorithms for Text Categorization.
In Proc. of the 3rd Annual Symposium on Document
analysis and Information Retrieval.
D. D. Lewis. 1992. An Evaluation of Phrasal and Clus-
tered Representations on a Text Categorization Task.
In Proc. of the 15th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 37–50.
A. K. McCallum, R. Rosenfeld, T. Mitchell, and A. Ng.
1998. Improving Text Classification by Shrinkage in
a Hierarchy of Classes. In Proc. of the 15th Interna-
tional Conference on Machine Learning, pages 359–
367.
A. K. McCallum. 1999. Multi-Label Text Classification
with a Mixture Model Trained by EM. In Revised ver-
sion ofpaper appearing in AAAI’99 Workshop on Text
Learning.
T. Mitchell. 1996. Machine Learning. McGraw Hill.
D. Mladenic and M. Grobelnik. 1998. Feature Selection
for Classification based on Text Hierarchy. In Proc. of
the Workshop on Learning from Text and the Web.
M. Sanderson and B. Croft. 1999. Deriving Concept Hi-
erarchies from Text. In Proc. ofthe 22nd AnnualInter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 206–213.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proc. of the
EACL SIGDAT Workshop.
V. Vapnik. 1995. The Nature ofStatistical Learning The-
ory. Springer.
J. Weston and C. Watkins. 1998. Multi-Class Support
Vector Machines. In Technical Report CSD-TR-98-
04.
Y. Yang and X. Liu. 1999. A Re-Examination of Text
Categorization Methods. In Proc. of the 22nd Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 42–
49.
Y. Yang and W. J. Wilbur. 1995. Using Corpus Statistics
to Remove Redundant Words in Text Categorization.
Journal ofAmerican Society Information Science.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.316986">
<note confidence="0.948466666666667">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 196-203. Association for Computational Linguistics.</note>
<title confidence="0.997883">Manipulating Large Corpora for Text Classification</title>
<author confidence="0.989128">Fukumoto</author>
<affiliation confidence="0.7521385">Department of Computer Science and Media Yamanashi</affiliation>
<address confidence="0.900324">4-3-11 Takeda, Kofu 400-8511 Japan</address>
<email confidence="0.762421">fukumoto@skye.esb.yamanashi.ac.jpysuzuki@alps1.esi.yamanashi.ac.jp</email>
<abstract confidence="0.999162263157895">In this paper, we address the problem of dealing with a large collection of data and propose a method for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs). NB is based on the assumption of word independence in a text, which makes the computation of it far more efficient. SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L D Baker</author>
<author>A K McCallum</author>
</authors>
<title>Distributional Clustering of Words for Text Classification.</title>
<date>1998</date>
<booktitle>In Proc. of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="1448" citStr="Baker and McCallum, 1998" startWordPosition="204" endWordPosition="207"> other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy. 1 Introduction As the volume of online documents has drastically increased, text classification has become more important, and a growing number of statistical and machine learning techniques have been applied to the task(Lewis, 1992), (Yang and Wilbur, 1995), (Baker and McCallum, 1998), (Lam and Ho, 1998), (McCallum, 1999), (Dumais and Chen, 2000). Most of them use the Reuters-21578 articles1 in the evalu1The Reuters-21578, distribution 1.0, is comprised of 21,578 documents, representing what remains of the original Reuters-22173 corpus after the elimination of 595 duplicates by Steve Lynch and David Lewis in 1996. ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, sin</context>
</contexts>
<marker>Baker, McCallum, 1998</marker>
<rawString>L. D. Baker and A. K. McCallum. 1998. Distributional Clustering of Words for Text Classification. In Proc. of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Ensemble Methods in Machine Learning.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st International Workshop on Multiple Classifier Systems.</booktitle>
<contexts>
<context position="2625" citStr="Dietterich, 2000" startWordPosition="379" endWordPosition="380">e effective for classification task, since most of them showed significant improvement (the performance over 0.85 F1 score) for Reuters-21578(Joachims, 1998), (Dumais et al., 1998), (Yang and Liu, 1999). More recently, some researchers have applied their techniques to larger corpora such as web pages in Internet applications(Mladenic and Grobelnik, 1998), (McCallum, 1999), (Dumais and Chen, 2000). The increasing number of documents and categories, however, often hampers the development of practical classification systems, mainly due to statistical, computational, and representational problems(Dietterich, 2000). There are at least two strategies for solving these problems. One is to use category hierarchies. The idea behind this is that when humans organize extensive data sets into fine-grained categories, category hierarchies are often employed to make the large collection of categories more manageable. McCallum et. al. presented a method called ‘shrinkage’ to improve parameter estimates by taking advantage of a hierarchy(McCallum, 1999). They tested their method using three different real-world datasets: 20,000 articles from UseNet, 6,440 web pages from the industry sector, and 14,831 pages from Y</context>
<context position="4185" citStr="Dietterich, 2000" startWordPosition="622" endWordPosition="623"> taking a (weighted) vote of their predictions(Dietterich, 2000). One of the methods for constructing ensembles manipulates the training examples to generate multiple hypotheses. The most straightforward way is called . It presents the learning algorithm with a training set that consists of a sample of examples drawn randomly with replacement from the original training set. The second method is to construct the training sets by leaving out disjoint subsets of the training data. The third is illustrated by the ADABOOST algorithm(Freund and Schapire, 1996). Dietterich has compared these methods(Dietterich, 2000). He reported that in low-noise data, ADABOOST performs well, while in high-noise cases, it yields overfitting because ADABOOST puts a large amount of weight on the mislabeled examples. Bagging works well on both the noisy and the noisefree data because it focuses on the statistical problem which arises when the amount of training data available is too small, and noise increases this statistical problem. However, it is not clear whether ‘works well’ means that it exponentially reduces the amount of computation necessary for classification, while sacrificing only a small amount of accuracy, or </context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>T. G. Dietterich. 2000. Ensemble Methods in Machine Learning. In Proc. of the 1st International Workshop on Multiple Classifier Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>H Chen</author>
</authors>
<title>Hierarchical Classification of Web Content.</title>
<date>2000</date>
<booktitle>In Proc. of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="1511" citStr="Dumais and Chen, 2000" startWordPosition="215" endWordPosition="218">ich makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy. 1 Introduction As the volume of online documents has drastically increased, text classification has become more important, and a growing number of statistical and machine learning techniques have been applied to the task(Lewis, 1992), (Yang and Wilbur, 1995), (Baker and McCallum, 1998), (Lam and Ho, 1998), (McCallum, 1999), (Dumais and Chen, 2000). Most of them use the Reuters-21578 articles1 in the evalu1The Reuters-21578, distribution 1.0, is comprised of 21,578 documents, representing what remains of the original Reuters-22173 corpus after the elimination of 595 duplicates by Steve Lynch and David Lewis in 1996. ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them showed significant improvement (the performance</context>
<context position="3443" citStr="Dumais and Chen, 2000" startWordPosition="505" endWordPosition="508">es, category hierarchies are often employed to make the large collection of categories more manageable. McCallum et. al. presented a method called ‘shrinkage’ to improve parameter estimates by taking advantage of a hierarchy(McCallum, 1999). They tested their method using three different real-world datasets: 20,000 articles from UseNet, 6,440 web pages from the industry sector, and 14,831 pages from Yahoo, and showed improved performance. Dumais et. al. used SVMs and classified hierarchical web content consisting of 50,078 web pages for training, and 10,024 for testing, with promising results(Dumais and Chen, 2000). The other is to use methods which are learning algorithms that construct a set of classifiers and then classify new data by taking a (weighted) vote of their predictions(Dietterich, 2000). One of the methods for constructing ensembles manipulates the training examples to generate multiple hypotheses. The most straightforward way is called . It presents the learning algorithm with a training set that consists of a sample of examples drawn randomly with replacement from the original training set. The second method is to construct the training sets by leaving out disjoint subsets of the trainin</context>
<context position="9506" citStr="Dumais and Chen, 2000" startWordPosition="1506" endWordPosition="1509"> size of the training data per category is . Let also be the time needed to train all categories, where represents the time for learning one binary classifier using training data, and is the number of binary classifier. The time for learning one binary classifier, is represented as , where is a constant. - - - method is thus done in time . 3 System Design 3.1 Hierarchical classification A well-known technique for classifying a large, heterogeneous collection such as web content is to use category hierarchies. Following the approaches of Koller and Sahami(Koller and Sahami, 1997), and Dumais’s(Dumais and Chen, 2000), we employ a hierarchy by learning separate classifiers at each internal node of the tree, and then labeling a document using these classifiers to greedily select subbranches until a leaf is reached. 3.2 Manipulating training data Our hypothesis regarding NB is that it can work well for documents which are assigned to only one category within the same category level in the hierarchical structure. We base this on some recent papers claiming that NB methods perform surprisingly well for an ‘accuracy’ measure which is equivalent to the standard precision under the one-category-perdocument assump</context>
</contexts>
<marker>Dumais, Chen, 2000</marker>
<rawString>S. Dumais and H. Chen. 2000. Hierarchical Classification of Web Content. In Proc. of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 256– 263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>J Platt</author>
<author>D Heckerman</author>
<author>M Sahami</author>
</authors>
<title>Inductive Learning Algorithm and Representations for Text Categorization. In</title>
<date>1998</date>
<booktitle>Proc. ofACM-CIKM98,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="2188" citStr="Dumais et al., 1998" startWordPosition="319" endWordPosition="322">lu1The Reuters-21578, distribution 1.0, is comprised of 21,578 documents, representing what remains of the original Reuters-22173 corpus after the elimination of 595 duplicates by Steve Lynch and David Lewis in 1996. ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them showed significant improvement (the performance over 0.85 F1 score) for Reuters-21578(Joachims, 1998), (Dumais et al., 1998), (Yang and Liu, 1999). More recently, some researchers have applied their techniques to larger corpora such as web pages in Internet applications(Mladenic and Grobelnik, 1998), (McCallum, 1999), (Dumais and Chen, 2000). The increasing number of documents and categories, however, often hampers the development of practical classification systems, mainly due to statistical, computational, and representational problems(Dietterich, 2000). There are at least two strategies for solving these problems. One is to use category hierarchies. The idea behind this is that when humans organize extensive dat</context>
</contexts>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>S. Dumais, J. Platt, D. Heckerman, and M. Sahami. 1998. Inductive Learning Algorithm and Representations for Text Categorization. In Proc. ofACM-CIKM98, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Experiments with a New Boosting Algorithm.</title>
<date>1996</date>
<booktitle>In Proc. of the 13th International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="4128" citStr="Freund and Schapire, 1996" startWordPosition="614" endWordPosition="617"> that construct a set of classifiers and then classify new data by taking a (weighted) vote of their predictions(Dietterich, 2000). One of the methods for constructing ensembles manipulates the training examples to generate multiple hypotheses. The most straightforward way is called . It presents the learning algorithm with a training set that consists of a sample of examples drawn randomly with replacement from the original training set. The second method is to construct the training sets by leaving out disjoint subsets of the training data. The third is illustrated by the ADABOOST algorithm(Freund and Schapire, 1996). Dietterich has compared these methods(Dietterich, 2000). He reported that in low-noise data, ADABOOST performs well, while in high-noise cases, it yields overfitting because ADABOOST puts a large amount of weight on the mislabeled examples. Bagging works well on both the noisy and the noisefree data because it focuses on the statistical problem which arises when the amount of training data available is too small, and noise increases this statistical problem. However, it is not clear whether ‘works well’ means that it exponentially reduces the amount of computation necessary for classificatio</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Y. Freund and R. E. Schapire. 1996. Experiments with a New Boosting Algorithm. In Proc. of the 13th International Conference on Machine Learning, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hansen</author>
<author>P Salamon</author>
</authors>
<date>1990</date>
<booktitle>Neural Network Ensembles. IEEETrans. Pattern Analysis and Machine Intell.,</booktitle>
<pages>12--993</pages>
<contexts>
<context position="22447" citStr="Hansen and Salamon, 1990" startWordPosition="3654" endWordPosition="3657">est data. Each classifier votes and the test data is assigned to the category that receives more than 6 votes3. The result is illustrated in Table 7. In Table 7, ‘Non-hierarchy’ and ‘Hierarchy’ denotes the result of the 102 categories treated as a flat nonhierarchical problem, and the result using hierarchical structure, respectively. We can find that the result of with hierarchy(0.704 F1) outperforms the result with non-hierarchy(0.532 F1). A necessary and sufficient condition for an ensemble of classifiers to be more accurate than any of its individual members is if the classifiers are and (Hansen and Salamon, 1990). An accurate classifier is one that has an error rate better than random guessing on new test data. Two classifiers are diverse if they make different errors on new data points. Given our result, it may be safely said, at least regarding the Reuters 1996 corpus, that hierarchical structure is more effective for constructing ensembles, i.e., an ensemble of classifiers which are constructed by the training data with fewer than 30 categories in each level is more and . Table 7 shows that our method and perform equally (0.704 F1 36 votes was the best results among 10 different voting schemes in t</context>
</contexts>
<marker>Hansen, Salamon, 1990</marker>
<rawString>L. Hansen and P. Salamon. 1990. Neural Network Ensembles. IEEETrans. Pattern Analysis and Machine Intell., 12:993–1001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>In Proc. of the Conference on Machine Learning,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="2165" citStr="Joachims, 1998" startWordPosition="317" endWordPosition="318">ticles1 in the evalu1The Reuters-21578, distribution 1.0, is comprised of 21,578 documents, representing what remains of the original Reuters-22173 corpus after the elimination of 595 duplicates by Steve Lynch and David Lewis in 1996. ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them showed significant improvement (the performance over 0.85 F1 score) for Reuters-21578(Joachims, 1998), (Dumais et al., 1998), (Yang and Liu, 1999). More recently, some researchers have applied their techniques to larger corpora such as web pages in Internet applications(Mladenic and Grobelnik, 1998), (McCallum, 1999), (Dumais and Chen, 2000). The increasing number of documents and categories, however, often hampers the development of practical classification systems, mainly due to statistical, computational, and representational problems(Dietterich, 2000). There are at least two strategies for solving these problems. One is to use category hierarchies. The idea behind this is that when humans</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In Proc. of the Conference on Machine Learning, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>M Sahami</author>
</authors>
<title>Hierarchically Classifying Documents using Very Few Words.</title>
<date>1997</date>
<booktitle>In Proc. of the 14th International Conference on Machine Learning,</booktitle>
<pages>170--178</pages>
<contexts>
<context position="9469" citStr="Koller and Sahami, 1997" startWordPosition="1501" endWordPosition="1504">ining data with categories. The average size of the training data per category is . Let also be the time needed to train all categories, where represents the time for learning one binary classifier using training data, and is the number of binary classifier. The time for learning one binary classifier, is represented as , where is a constant. - - - method is thus done in time . 3 System Design 3.1 Hierarchical classification A well-known technique for classifying a large, heterogeneous collection such as web content is to use category hierarchies. Following the approaches of Koller and Sahami(Koller and Sahami, 1997), and Dumais’s(Dumais and Chen, 2000), we employ a hierarchy by learning separate classifiers at each internal node of the tree, and then labeling a document using these classifiers to greedily select subbranches until a leaf is reached. 3.2 Manipulating training data Our hypothesis regarding NB is that it can work well for documents which are assigned to only one category within the same category level in the hierarchical structure. We base this on some recent papers claiming that NB methods perform surprisingly well for an ‘accuracy’ measure which is equivalent to the standard precision unde</context>
<context position="10716" citStr="Koller and Sahami, 1997" startWordPosition="1719" endWordPosition="1722">ocument assumption on classifiers and also equivalent to the standard recall, assuming that each document has one and only one correct category per catOrigin Negative examples Positive examples w − b w Margin s.t where = ( , , ) is the -th training example and is a label corresponding the -th training example. In formula (3), each element of w, (1 , ,), where is an arbitrary data point, and = ( , , ) and are learned from a training set of linearly separable data. Figure 1 shows an example of a simple two-dimensional problem that is linearly separable2. egory level(Lewis and Ringuette, 1994), (Koller and Sahami, 1997). SVMs, on the other hand, have the potential to handle more complex problems without sacrificing accuracy, even though the computation of the SVM classifiers is far less efficient than NB. We thus use NB for simple classification problems and SVMs for more complex data, i.e., the data which cannot be classified correctly by NB classifiers. We use ten-fold cross validation: All of the training data were randomly shuffled and divided into ten equal folds. Nine folds were used to train the NB classifiers while the remaining fold(held-out test data) was used to evaluate the accuracy of the classi</context>
</contexts>
<marker>Koller, Sahami, 1997</marker>
<rawString>D. Koller and M. Sahami. 1997. Hierarchically Classifying Documents using Very Few Words. In Proc. of the 14th International Conference on Machine Learning, pages 170–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lam</author>
<author>C Y Ho</author>
</authors>
<title>Using a Generalized Instance Set for Automatic Text Categorization.</title>
<date>1998</date>
<booktitle>In Proc. of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>81--89</pages>
<contexts>
<context position="1468" citStr="Lam and Ho, 1998" startWordPosition="208" endWordPosition="211">ial to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy. 1 Introduction As the volume of online documents has drastically increased, text classification has become more important, and a growing number of statistical and machine learning techniques have been applied to the task(Lewis, 1992), (Yang and Wilbur, 1995), (Baker and McCallum, 1998), (Lam and Ho, 1998), (McCallum, 1999), (Dumais and Chen, 2000). Most of them use the Reuters-21578 articles1 in the evalu1The Reuters-21578, distribution 1.0, is comprised of 21,578 documents, representing what remains of the original Reuters-22173 corpus after the elimination of 595 duplicates by Steve Lynch and David Lewis in 1996. ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them show</context>
</contexts>
<marker>Lam, Ho, 1998</marker>
<rawString>W. Lam and C. Y. Ho. 1998. Using a Generalized Instance Set for Automatic Text Categorization. In Proc. of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 81–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>M Ringuette</author>
</authors>
<title>Comparison of Two Learning Algorithms for Text Categorization.</title>
<date>1994</date>
<booktitle>In Proc. of the 3rd Annual Symposium on Document analysis and Information Retrieval.</booktitle>
<contexts>
<context position="10689" citStr="Lewis and Ringuette, 1994" startWordPosition="1715" endWordPosition="1718">n under the one-category-perdocument assumption on classifiers and also equivalent to the standard recall, assuming that each document has one and only one correct category per catOrigin Negative examples Positive examples w − b w Margin s.t where = ( , , ) is the -th training example and is a label corresponding the -th training example. In formula (3), each element of w, (1 , ,), where is an arbitrary data point, and = ( , , ) and are learned from a training set of linearly separable data. Figure 1 shows an example of a simple two-dimensional problem that is linearly separable2. egory level(Lewis and Ringuette, 1994), (Koller and Sahami, 1997). SVMs, on the other hand, have the potential to handle more complex problems without sacrificing accuracy, even though the computation of the SVM classifiers is far less efficient than NB. We thus use NB for simple classification problems and SVMs for more complex data, i.e., the data which cannot be classified correctly by NB classifiers. We use ten-fold cross validation: All of the training data were randomly shuffled and divided into ten equal folds. Nine folds were used to train the NB classifiers while the remaining fold(held-out test data) was used to evaluate</context>
</contexts>
<marker>Lewis, Ringuette, 1994</marker>
<rawString>D. D. Lewis and M. Ringuette. 1994. Comparison of Two Learning Algorithms for Text Categorization. In Proc. of the 3rd Annual Symposium on Document analysis and Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>An Evaluation of Phrasal and Clustered Representations on a Text Categorization Task.</title>
<date>1992</date>
<booktitle>In Proc. of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>37--50</pages>
<contexts>
<context position="1395" citStr="Lewis, 1992" startWordPosition="198" endWordPosition="199">n of it far more efficient. SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy. 1 Introduction As the volume of online documents has drastically increased, text classification has become more important, and a growing number of statistical and machine learning techniques have been applied to the task(Lewis, 1992), (Yang and Wilbur, 1995), (Baker and McCallum, 1998), (Lam and Ho, 1998), (McCallum, 1999), (Dumais and Chen, 2000). Most of them use the Reuters-21578 articles1 in the evalu1The Reuters-21578, distribution 1.0, is comprised of 21,578 documents, representing what remains of the original Reuters-22173 corpus after the elimination of 595 duplicates by Steve Lynch and David Lewis in 1996. ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning </context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>D. D. Lewis. 1992. An Evaluation of Phrasal and Clustered Representations on a Text Categorization Task. In Proc. of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
<author>R Rosenfeld</author>
<author>T Mitchell</author>
<author>A Ng</author>
</authors>
<title>Improving Text Classification by Shrinkage in a Hierarchy of Classes.</title>
<date>1998</date>
<booktitle>In Proc. of the 15th International Conference on Machine Learning,</booktitle>
<pages>359--367</pages>
<contexts>
<context position="7009" citStr="McCallum et al., 1998" startWordPosition="1075" endWordPosition="1079">yes(NB) probabilistic classifiers are commonly studied in machine learning(Mitchell, 1996). The basic idea in NB approaches is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document. The NB assumption is that all the words in a text are conditionally independent given the value of a classification variable. There are several versions of the NB classifiers. Recent studies on a Naive Bayes classifier which is proposed by McCallum et. al. reported high performance over some other commonly used versions of NB on several data collections(McCallum et al., 1998). We use the model of NB by McCallum et. al. which is shown in formula (1). refers to the number of vocabularies, denotes the number of labeled training documents, and shows the number of categories. denotes document length. is the word in position of (1) document , where the subscript of ,indicates an index into the vocabulary. denotes the number of times word occurs in document , and is defined by 0,1 . 2.2 SVMs SVMs are introduced by Vapnik(Vapnik, 1995) for solving two-class pattern recognition problems. It is defined over a vector space where the problem is to find a decision surface(clas</context>
</contexts>
<marker>McCallum, Rosenfeld, Mitchell, Ng, 1998</marker>
<rawString>A. K. McCallum, R. Rosenfeld, T. Mitchell, and A. Ng. 1998. Improving Text Classification by Shrinkage in a Hierarchy of Classes. In Proc. of the 15th International Conference on Machine Learning, pages 359– 367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>Multi-Label Text Classification with a Mixture Model Trained by EM.</title>
<date>1999</date>
<booktitle>In Revised version ofpaper appearing in AAAI’99 Workshop on Text Learning.</booktitle>
<contexts>
<context position="1486" citStr="McCallum, 1999" startWordPosition="212" endWordPosition="214">feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy. 1 Introduction As the volume of online documents has drastically increased, text classification has become more important, and a growing number of statistical and machine learning techniques have been applied to the task(Lewis, 1992), (Yang and Wilbur, 1995), (Baker and McCallum, 1998), (Lam and Ho, 1998), (McCallum, 1999), (Dumais and Chen, 2000). Most of them use the Reuters-21578 articles1 in the evalu1The Reuters-21578, distribution 1.0, is comprised of 21,578 documents, representing what remains of the original Reuters-22173 corpus after the elimination of 595 duplicates by Steve Lynch and David Lewis in 1996. ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them showed significant imp</context>
<context position="3061" citStr="McCallum, 1999" startWordPosition="447" endWordPosition="449">d categories, however, often hampers the development of practical classification systems, mainly due to statistical, computational, and representational problems(Dietterich, 2000). There are at least two strategies for solving these problems. One is to use category hierarchies. The idea behind this is that when humans organize extensive data sets into fine-grained categories, category hierarchies are often employed to make the large collection of categories more manageable. McCallum et. al. presented a method called ‘shrinkage’ to improve parameter estimates by taking advantage of a hierarchy(McCallum, 1999). They tested their method using three different real-world datasets: 20,000 articles from UseNet, 6,440 web pages from the industry sector, and 14,831 pages from Yahoo, and showed improved performance. Dumais et. al. used SVMs and classified hierarchical web content consisting of 50,078 web pages for training, and 10,024 for testing, with promising results(Dumais and Chen, 2000). The other is to use methods which are learning algorithms that construct a set of classifiers and then classify new data by taking a (weighted) vote of their predictions(Dietterich, 2000). One of the methods for cons</context>
</contexts>
<marker>McCallum, 1999</marker>
<rawString>A. K. McCallum. 1999. Multi-Label Text Classification with a Mixture Model Trained by EM. In Revised version ofpaper appearing in AAAI’99 Workshop on Text Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
</authors>
<title>Machine Learning.</title>
<date>1996</date>
<booktitle>In Proc. of the Workshop on Learning from Text and the Web.</booktitle>
<publisher>McGraw</publisher>
<contexts>
<context position="6477" citStr="Mitchell, 1996" startWordPosition="989" endWordPosition="990">B using cross-validation according to the hierarchical structure of categories, and only the documents which could not classify correctly by NB classifiers in each category level are extracted as the training data of SVMs. The rest of the paper is organized as follows. The next section provides the basic framework of NB and SVMs. We then describe our classification method. Finally, we report some experiments using 279,303 documents in the Reuters 1996 corpus with a discussion of evaluation. 2 Classifiers 2.1 NB Naive Bayes(NB) probabilistic classifiers are commonly studied in machine learning(Mitchell, 1996). The basic idea in NB approaches is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document. The NB assumption is that all the words in a text are conditionally independent given the value of a classification variable. There are several versions of the NB classifiers. Recent studies on a Naive Bayes classifier which is proposed by McCallum et. al. reported high performance over some other commonly used versions of NB on several data collections(McCallum et al., 1998). We use the model of NB by McCallum et. al. which is shown in formu</context>
</contexts>
<marker>Mitchell, 1996</marker>
<rawString>T. Mitchell. 1996. Machine Learning. McGraw Hill. D. Mladenic and M. Grobelnik. 1998. Feature Selection for Classification based on Text Hierarchy. In Proc. of the Workshop on Learning from Text and the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sanderson</author>
<author>B Croft</author>
</authors>
<title>Deriving Concept Hierarchies from Text.</title>
<date>1999</date>
<booktitle>In Proc. ofthe 22nd AnnualInternational ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>206--213</pages>
<marker>Sanderson, Croft, 1999</marker>
<rawString>M. Sanderson and B. Croft. 1999. Deriving Concept Hierarchies from Text. In Proc. ofthe 22nd AnnualInternational ACM SIGIR Conference on Research and Development in Information Retrieval, pages 206–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Improvements in Part-of-Speech Tagging with an Application to German.</title>
<date>1995</date>
<booktitle>In Proc. of the EACL SIGDAT Workshop.</booktitle>
<contexts>
<context position="13901" citStr="Schmid, 1995" startWordPosition="2273" endWordPosition="2274">test set. The number of categories in each level is 25 top level, 33 second level, 43 third level, and 1 fourth level, respectively. Table 1 shows the number of documents in each top level category. After eliminating unlabelled documents, we obtained 271,171 documents. We divided these documents into two sets: a training set from 20th Aug. to 31th Oct. which consists of 150,939 documents, and test set from 1th Nov. to 31st Dec. which consists of 120,242 documents. We obtained a vocabulary of 183,400 unique words after eliminating words which occur only once, stemming by a partof-speech tagger(Schmid, 1995), and stop word removal. Figure 3 illustrates the category distribution 1 Extracting training documents using NB test documents Nb documents incorrect documents by NB estimating parameters Nerror documents NB training documents Na documents 2 Classifying test documents output YES SVM NB category assigned? NO Na+Nb documents Nerror documents Nt documents Category name Training Test Corporate/Industrial 69,975 56,100 Economics 22,214 18,694 Government/social 45,618 36,923 Crime 6,248 4,865 Defence 1,646 1,408 International relations 7,523 6,278 Disasters 1,644 1,383 Arts 771 602 Environment 1,17</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>H. Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proc. of the EACL SIGDAT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature ofStatistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="7470" citStr="Vapnik, 1995" startWordPosition="1160" endWordPosition="1161">s proposed by McCallum et. al. reported high performance over some other commonly used versions of NB on several data collections(McCallum et al., 1998). We use the model of NB by McCallum et. al. which is shown in formula (1). refers to the number of vocabularies, denotes the number of labeled training documents, and shows the number of categories. denotes document length. is the word in position of (1) document , where the subscript of ,indicates an index into the vocabulary. denotes the number of times word occurs in document , and is defined by 0,1 . 2.2 SVMs SVMs are introduced by Vapnik(Vapnik, 1995) for solving two-class pattern recognition problems. It is defined over a vector space where the problem is to find a decision surface(classifier) that ‘best’ separates a set of positive examples from a set of negative examples by introducing the maximum ‘margin’ between two sets. The margin is defined by the distance from the hyperplane to the nearest of the positive and negative examples. The decision surface produced by SVMs for linearly separable space is a hyperplane which can be written as + = 0 ( Figure 1: The decision surface of a linear SVMs In the linearly separable case maximizing t</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature ofStatistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weston</author>
<author>C Watkins</author>
</authors>
<title>Multi-Class Support Vector Machines. In</title>
<date>1998</date>
<tech>Technical Report CSD-TR-98-04.</tech>
<contexts>
<context position="8642" citStr="Weston and Watkins, 1998" startWordPosition="1350" endWordPosition="1353">near SVMs In the linearly separable case maximizing the margin can be expressed as an optimization problem: 2We focused on linear hypotheses in this work, while SVMs can handle nonlinear hypotheses using functions. ) corresponds to each word in the training examples, and the larger value of is, the more the word features positive examples. We note that SVMs are basically introduced for solving binary classification, while text classification is a multi-class, multi-label classification problem. Several methods using SVMs which were intended for multi-class, multi-label data have been proposed(Weston and Watkins, 1998). We use - - - version of the SVMs model in the work. A time complexity of SVMs is known as , where is the number of training data. We consider a time complexity of - - - method. Let be the number of training data with categories. The average size of the training data per category is . Let also be the time needed to train all categories, where represents the time for learning one binary classifier using training data, and is the number of binary classifier. The time for learning one binary classifier, is represented as , where is a constant. - - - method is thus done in time . 3 System Design </context>
</contexts>
<marker>Weston, Watkins, 1998</marker>
<rawString>J. Weston and C. Watkins. 1998. Multi-Class Support Vector Machines. In Technical Report CSD-TR-98-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>X Liu</author>
</authors>
<title>A Re-Examination of Text Categorization Methods.</title>
<date>1999</date>
<booktitle>In Proc. of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="2210" citStr="Yang and Liu, 1999" startWordPosition="323" endWordPosition="326">istribution 1.0, is comprised of 21,578 documents, representing what remains of the original Reuters-22173 corpus after the elimination of 595 duplicates by Steve Lynch and David Lewis in 1996. ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them showed significant improvement (the performance over 0.85 F1 score) for Reuters-21578(Joachims, 1998), (Dumais et al., 1998), (Yang and Liu, 1999). More recently, some researchers have applied their techniques to larger corpora such as web pages in Internet applications(Mladenic and Grobelnik, 1998), (McCallum, 1999), (Dumais and Chen, 2000). The increasing number of documents and categories, however, often hampers the development of practical classification systems, mainly due to statistical, computational, and representational problems(Dietterich, 2000). There are at least two strategies for solving these problems. One is to use category hierarchies. The idea behind this is that when humans organize extensive data sets into fine-grain</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Y. Yang and X. Liu. 1999. A Re-Examination of Text Categorization Methods. In Proc. of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 42– 49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>W J Wilbur</author>
</authors>
<title>Using Corpus Statistics to Remove Redundant Words in Text Categorization.</title>
<date>1995</date>
<journal>Journal ofAmerican Society Information Science.</journal>
<contexts>
<context position="1420" citStr="Yang and Wilbur, 1995" startWordPosition="200" endWordPosition="203">e efficient. SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy. 1 Introduction As the volume of online documents has drastically increased, text classification has become more important, and a growing number of statistical and machine learning techniques have been applied to the task(Lewis, 1992), (Yang and Wilbur, 1995), (Baker and McCallum, 1998), (Lam and Ho, 1998), (McCallum, 1999), (Dumais and Chen, 2000). Most of them use the Reuters-21578 articles1 in the evalu1The Reuters-21578, distribution 1.0, is comprised of 21,578 documents, representing what remains of the original Reuters-22173 corpus after the elimination of 595 duplicates by Steve Lynch and David Lewis in 1996. ations of their methods, since the corpus has become a benchmark, and their results are thus easily compared with other results. It is generally agreed that these methods using statistical and machine learning techniques are effective </context>
</contexts>
<marker>Yang, Wilbur, 1995</marker>
<rawString>Y. Yang and W. J. Wilbur. 1995. Using Corpus Statistics to Remove Redundant Words in Text Categorization. Journal ofAmerican Society Information Science.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>