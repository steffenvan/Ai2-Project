<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.95412">
The Effects of Lexical Specialization on
the Growth Curve of the Vocabulary
</title>
<author confidence="0.968701">
R. Harald Baayen*
</author>
<affiliation confidence="0.5395245">
Max Planck Institute for
Psycholinguistics
</affiliation>
<bodyText confidence="0.997629818181818">
The number of different words expected on the basis of the urn model to appear in, for example,
the first half of a text, is known to overestimate the observed number of different words. This
paper examines the source of this overestimation bias. It is shown that this bias does not arise
due to sentence-bound syntactic constraints, but that it is a direct consequence of topic cohesion
in discourse. The nonrandom, clustered appearance of lexically specialized words, often the key
words of the text, explains the main trends in the overestimation bias both quantitatively and
qualitatively. The effects of nonrandomness are so strong that they introduce an overestimation
bias in distributions of units derived from words, such as syllables and digrams. Nonrandom
word usage also affects the accuracy of the Good-Turing frequency estimates which, for the lowest
frequencies, reveal a strong underestimation bias. A heuristic adjusted frequency estimate is
proposed that, at least for novel-sized texts, is considerably more accurate.
</bodyText>
<sectionHeader confidence="0.981268" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99997">
When reading through a text, word token by word token, the number of different word
types encountered increases, quickly at first, and ever more slowly as one progresses
through the text. The number of different word types encountered after reading N
tokens, the vocabulary size V(N), is a function of N. Analytical expressions for V(N)
based on the urn model are available. A classic problem in word frequency stud-
ies is, however, that these analytical expressions tend to overestimate the observed
vocabulary size, irrespective of whether these expressions are nonparametric (Good
1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986;
Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature.
Although the theoretical or expected vocabulary size E[V (N)] generally is of the
same order of magnitude as the observed vocabulary size, the lack of precision one
observes time and again casts serious doubt on the reliability of a number of mea-
sures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and
Renouf (1996) exploit the Good-Turing estimate for the probability of sampling un-
seen types (Good 1953) to develop measures for the degree of productivity of affixes,
Baayen and Sproat (to appear) apply this Good-Turing estimate to obtain enhanced
estimates of lexical priors for unseen words, and the Good-Turing estimates also play
an important role for estimating population probabilities (Church and Gale 1991). If a
simple random variable such as the vocabulary size reveals consistent and significant
deviation from its expectation, the accuracy of the Good-Turing estimates is also called
into question. The aim of this paper is to understand why this deviation between the-
</bodyText>
<footnote confidence="0.419752">
* Wundtlaan 1, 6525 XD Nijmegen, The Netherlands. E-mail: baayen@mpi.n1
</footnote>
<note confidence="0.801928">
© 1996 Association for Computational Linguistics
Computational Linguistics Volume 22, Number 4
</note>
<bodyText confidence="0.997371">
ory and observation arises in word frequency distributions, and in this light evaluate
applications of the Good-Turing results.
The remainder of this paper is structured as follows. In Section 2, I introduce some
basic notation and the expressions for the growth curve of the vocabulary with which
we will be concerned throughout, including a model proposed by Hubert and Labbe
(1988), which, by introducing a smoothing parameter, leads to much-improved fits. Un-
fortunately, this model is based on a series of unrealistic simplifications, and cannot
serve as an explanation for the divergence between the observed and expected vocab-
ulary size. In Section 3, therefore, I consider a number of possible sources for the misfit
in greater detail: nonrandomness at the sentence level due to syntactic structure, non-
randomness due to the discourse structure of the text as a whole, and noru-andomness
due to thematic cohesion in restricted sequences of sentences (paragraphs). Section 4
traces the implications of the results obtained for distributions of units derived from
words, such as syllables and digrams, and examines the accuracy of the Good-Turing
frequency estimates. A list of symbols is provided at the end of the paper.
</bodyText>
<sectionHeader confidence="0.892292" genericHeader="method">
2. The Growth Curve of the Vocabulary
</sectionHeader>
<bodyText confidence="0.999882857142857">
Let N be the size of a text in word tokens, and let V denote the total number of different
word types observed among the N word tokens. Roughly half of the word types occur
only once, the so-called hapax legomena, others occur with higher frequencies.1 Let
V(N, 1) denote the number of once-occurring types among N tokens, and, similarly,
let V(N,f) denote the number of types occurring f times after sampling N tokens. The
expected number of different types E[V(M)] for M &lt; N conditional on the frequency
spectrum {V(N,f)},f = 1, 2, 3, ... can be estimated by
</bodyText>
<equation confidence="0.999307">
E[V(M)] = V — M f
V(N,f) (1 — K1) . (1)
</equation>
<bodyText confidence="0.998909923076923">
A proof for (1) is presented in the appendix.
Figure 1 illustrates the problems that arise when (1) is applied to three texts, Alice
in Wonderland, by Lewis Carroll (upper panels), Moby Dick by Herman Melville (middle
panels), and Max Havelaar by Multatuli (the pseudonym of Eduard Douwes Dekker,
bottom panels).2 All panels show the sample size N on the horizontal axis. Thus the
horizontal axis can be viewed as displaying the &amp;quot;text time&amp;quot; measured in word tokens.
The vertical axis of the left-hand panels shows the number of observed word types
(dotted line) and the number of types predicted by the model (solid line) obtained
using (1). These panels reveal that the expected vocabulary size overestimates the
observed vocabulary size for almost all of the 40 equidistant measurement points. To
the eye, the overestimation seems fairly small. Nevertheless, in absolute terms the
expectation may be several hundreds of types too high, and may run up to 5% of the
total vocabulary size.
</bodyText>
<footnote confidence="0.6321665">
1 The type definition I have used throughout is based on the orthographic word form: house and houses
are counted as two different types, houses and houses as two tokens of the same type. No lemmatization
has been attempted, first, because the probabilistic aspects of the problem considered here are not
affected by whether or not lemmatization is carried out, and second, because it is of interest to
ascertain how much information can be extracted from texts with minimal preprocessing.
2 These texts were obtained by anonymous ftp from the Project Gutenberg at obi.std.com. The header of
the electronic version of Moby Dick requires mention of E.F. Tray at the University of Colorado,
Boulder, who prepared the text on the basis of the Hendricks House Edition.
</footnote>
<page confidence="0.997379">
456
</page>
<figure confidence="0.92915225">
Baayen Lexical Specialization
Lewis Carroll Alice in Wonderland
• ....
• 4,
5000 10000 15000 20000 25000 5000 10000 15000 20000 25000
Herman Melville
Moby Dick
50000 100000 150000 200000 50000 100000 150000 200000
Multatuli
Max Havelaar
20000 40000 60000 80000 100000
20000 40000 60000 80000 100000
</figure>
<figureCaption confidence="0.9407316">
Figure 1
The growth curve of the vocabulary. Observed vocabulary size V(N) (dotted lines) and
expected vocabulary size E[V(N)] (solid lines) for three novels (left-hand panels) and the
corresponding overstimation errors E[V(N)] — V(N) (dotted lines) and their
sentence-randomized versions (&amp;quot;+&amp;quot;-lines, see Section 3.1) (right-hand panels).
</figureCaption>
<bodyText confidence="0.999857">
The right-hand panels of Figure 1 show the overestimation error functions E[V(N)]
— V(N) corresponding to the left-hand panels using dotted lines. For the first 20 mea-
surement points, the instances for which E[V(N)] diverges significantly from V(N)
are shown in bold.3 Clearly, the divergence is significant for almost all of the first 20
measurement points. This suggests informally that the discrepancy between E[V(N)]
and V(N) is significant over a wide range of sample sizes.
</bodyText>
<subsectionHeader confidence="0.973119">
2.1 The Model Proposed by Hubert and Labbe
</subsectionHeader>
<bodyText confidence="0.8597774">
The problem of the systematic estimation error of E[V(N)] has been pointed out by
Muller (1979) and Brunet (1978), who hypothesize that lexical specialization is at issue.
In any text, there are words the use of which is mainly or even exclusively restricted
to a given subsection of that text. Such locally concentrated clusters of words are at
odds with the randomness assumption underlying the derivation of (1), and may be
the cause of the divergence illustrated in Figure 1. Following this line of reasoning,
Hubert and Labbe (1988) propose a model according to which (1) should be modified
3 Since the expression for an estimate of the variance of V(N) figuring in the Z-scores used here requires
knowledge of E[V(2N)], the significance of the divergence for the second 20 measurement points is not
available. For technical details, see Chitashvili and Baayen (1993).
</bodyText>
<page confidence="0.991829">
457
</page>
<note confidence="0.867694">
Computational Linguistics Volume 22, Number 4
</note>
<bodyText confidence="0.96489">
as follows (see the appendix for further details):
</bodyText>
<equation confidence="0.989393">
EHL[V(M)] = p-NV + (1 – p)V – (1 – p) E v(N,f) -N (2)
</equation>
<bodyText confidence="0.99973525">
Hubert and Labbe&apos;s model contains one free parameter, the coefficient of vocabu-
lary partition p, an estimate of the proportion of specialized words in the vocabulary.
Given K different text sizes for which the observed and expected vocabulary sizes are
known, p can be estimated by minimizing the mean squared error (MSE)
</bodyText>
<equation confidence="0.988979">
Elk( i(V(Mk) – E[V(Mk)[)2
or the chi-square statistic
E (v(mk) _ E[17040])2
E{v(mk)]
k=1
</equation>
<bodyText confidence="0.993775">
(conveniently ignoring that the variance of V(M) increases with M; see Chitashvili
and Baayen [19931). For Alice in Wonderland, minimalization of (4) for K = 40 leads to
p = 0.16, and according to this rough estimate of goodness-of-fit the revised model
fits the data very well indeed (Xq39) = 3.58, p&gt; 0.5). For Moby Dick, however, the chi-
squared statistic suggests a significant difference between the observed and expected
vocabulary sizes (.,q39) = 172.93,p &lt; 0.001), even though the value of the p parame-
ter (0.12) leads to a fit that is much improved with respect to the unadjusted growth
curve (X239) = 730.47). Closer inspection of the error pattern of the adjusted estimate
(
reveals the source of the misfit: for the first 12 measurement points, the observed
vocabulary size is consistently overestimated. From the 14th observation onwards,
the Hubert-Labbe model consistently underestimates the real vocabulary size. Appar-
ently, the development of the vocabulary in Moby Dick can be modeled globally, but
local fluctuations introducing additional points of inflection into the growth curve are
outside its scope—a more detailed study of the development of lexical specialization
in the narrative is required if the appearance of these points of inflection are to be
understood.
In spite of this deficiency, the Hubert-Labbe curve appears to be an optimal
smoother, and this suggests that the value obtained for the coefficient of vocabulary
partition p is a fairly reliable estimate of the extent to which a text is characterized by
lexical specialization. In this light, the evaluation by Holmes (1994), who suggests that
p might be a useful discriminant for authorship attribution studies, is understandable.
Unfortunately, the assumptions underlying (2) are overly simplistic, and seriously call
into question the reliability of p as a measure of lexical specialization, and the same
holds for the explanatory value of this model for the inaccuracy of E[V(N)].
</bodyText>
<subsectionHeader confidence="0.999261">
2.2 Problems with the Hubert and Labbe Model
</subsectionHeader>
<bodyText confidence="0.999810571428571">
One highly questionable simplification underlying the derivation of (2) spelled out in
the appendix is that specialized words are assumed to occur in a single text slice only.
Consider Figure 2, which plots the number of times Ahab appears in 40 successive,
equally sized text slices that jointly constitute the full text of Moby Dick. The dotted
line reveals the main developmental pattern (time-series smoothing using running
medians). Even though Ahab is one of the main characters in Moby Dick, and even
though his name certainly belongs to the specialized vocabulary of the novel, Ahab is
</bodyText>
<page confidence="0.995662">
458
</page>
<figure confidence="0.9839025">
Baayen Lexical Specialization
lb 20
30
text slice
</figure>
<figureCaption confidence="0.969545">
Figure 2
</figureCaption>
<bodyText confidence="0.981067071428572">
Nonrandom word usage illustrated for Ahab in Moby Dick. The horizontal axis plots the 40
equally sized text slices, the vertical axis the frequency of Ahab in these text slices. The dotted
line represents a time-series smoother using running medians (Tukey 1977).
not mentioned by name in one text slice only, as the Hubert-Labbe model would have.
What we find is that he is not mentioned at all in the first five text slices. Following
this we observe a series of text slices in which he appears frequently. These are in turn
succeeded by slices in which Ahab is hardly mentioned, but he reappears in the last
part of the book, and as the book draws to its dramatic close, the frequency of Ahab
increases to its maximum. This is an illustration of what Indefrey and Baayen (1994)
refer to as inter-textual cohesion: the word Ahab enjoys specialized use, but it occurs
in a series of subtexts within the novel as a whole, contributing to its overall cohesion.
Within text slices where Ahab is frequently mentioned, the intra-textual cohesion may
similarly be strengthened. For instance, Ahab appears to be a specialized word in text
slice 23, but he is mentioned only in passing in text slice 25. His appearance in the two
text slices strengthens the intertextual cohesion of the whole novel, but it is only the
intra-textual cohesion of slice 23 that is raised. The presence of inter-textual cohesion in
addition to intra-textual cohesion and the concomitant phenomenon of global lexical
specialization suggest that in order to understand the discrepancy between V(N) and
its expectation, a more fine-grained approach is required.
A second question concerns how lexical specialization affects the empirical growth
curve of the vocabulary. Inspection of plots such as those presented in Figure 1 for Al-
ice in Wonderland suggests that the effects of lexical specialization appear in the central
sections of the text, as it is there that the largest differences between the expected and
the observed vocabulary are to be observed—differences that are highly penalized by
the MSE and chi-squared techniques used to estimate the proportion of specialized
words in the vocabulary. Unfortunately, the central sections are not necessarily the
ones characterized by the highest degree of lexical specialization. To see this, consider
Figure 3, which plots the difference between the expected number of new types using
</bodyText>
<page confidence="0.99913">
459
</page>
<figureCaption confidence="0.842269">
Figure 3
</figureCaption>
<bodyText confidence="0.8904455">
Error scores for the influx of new types in Alice in Wonderland. The k = 1,2, ... ,40 text slices
are displayed on the horizontal axis, the progressive difference scores D(k) are shown on the
vertical axis. The dashed line represents a nonparametric scatterplot smoother (Cleveland
1979), the dotted line a least squares regression line (the negative slope is significant,
F(1,38) -= 11.07,p &lt; .002).
(1) and the observed number of new types for the successive text slices of Alice in Won-
derland. More precisely, for each text slice k, k -=- 1, ,40, we calculate the progressive
difference error scores D(k), k = 1. . . 40:
</bodyText>
<equation confidence="0.997786">
D(k) = {E[V(Mk)] - E[V(Mk_i)]1 - {V(Mk) - 1/(Mk-i)}• (5)
</equation>
<bodyText confidence="0.998188166666667">
Note that in addition to positive difference scores, which should be present given that
E[V(Mk)] &gt; V(Mk) for most, or, as in Alice in Wonderland, for all values of k, we also
have negative difference scores. Text slices containing more types than expected under
chance conditions are necessarily present given the existence of text slices k for which
E[V(Mk)] - V(Mk) &gt; 0: the total number of types accumulated over the 40 text slices has
to sum up to V(N). Figure 3 shows that the expected numbers of new word types are
overestimated for the initial part of the novel, that the theoretical estimates are fairly
reliable for the middle section of the novel, while the final chapters show a slightly
greater increase in the number of new types than expected under chance conditions.
If lexical specialization affects the influx of new types, its effects appear not in the
central sections of the novel as suggested by Figure 1, but rather in the beginning and
perhaps at the end. This finding seriously questions the appropriateness of using the
growth curve of the vocabulary for deriving a measure of lexical specialization.
A third question arises with respect to how one&apos;s measure of lexical concentration
is affected by the number of text slices K. In Hubert and Labbe&apos;s model, the optimal
value of the p parameter is independent of the number of text slices K for not-too-
small K (K&gt; 10). Since the expected growth curve and the observed growth curve
are completely fixed and independent of K—the former is fully determined by the fre-
</bodyText>
<figure confidence="0.995663142857143">
Volume 22, Number 4
Computational Linguistics
0
cJ
0
10
30
20
40
•
0
C■1
2
0
</figure>
<page confidence="0.990315">
460
</page>
<subsectionHeader confidence="0.617393">
Baayen Lexical Specialization
</subsectionHeader>
<bodyText confidence="0.999890823529412">
quency spectrum of the complete text, the latter is determined by the text itself—the
choice of K influences only the number of points at which the divergence between
the two curves is measured. Increasing the number of measurement points increases
the degrees of freedom along with the deviance, and the optimal value of the p pa-
rameter remains virtually unchanged. But is this a desirable property for a measure
of lexical specialization? Even without taking the effects of inter-textual cohesion into
account, and concentrating solely on local specialization and intra-textual cohesion,
formulating lexical specialization in terms of concentration at a particular point in the
text is unrealistic: it is absurd to assume that all tokens of a specialized word appear in
one chunk without any other intervening words. A more realistic definition of (local)
lexical specialization is the concentration of the tokens of a given word within a par-
ticular text slice. In such an approach, however, the size of the text slice is of crucial
importance. A word appearing only in the first half of a book enjoys some specialized
use, but to a far lesser extent than a word with the same frequency that occurs in the
first half of the first chapter only In other words, an approach to lexical specialization
in terms of concentration of use is incomplete without a specification of the unit of
concentration itself.
</bodyText>
<subsectionHeader confidence="0.78952">
3. Sources of Nonrandomness
</subsectionHeader>
<bodyText confidence="0.999927">
To avoid these problems, I will now sketch a somewhat more fine-grained approach to
understanding why V(N) and its expectation diverge, adopting Hubert and Labbe&apos;s
central insight that lexical specialization can be modeled in terms of local concentra-
tion. Consider again the potential sources for violation of the randomness assumption
underlying the derivation of E[V(N)]. At least three possibilities suggest themselves:
syntactic constraints on word usage within sentences, global discourse organization,
and local repetition. I will consider these possibilities in turn.
</bodyText>
<subsectionHeader confidence="0.999862">
3.1 Syntactic Constraints
</subsectionHeader>
<bodyText confidence="0.999989888888889">
Syntactic constraints at the level of the sentence introduce many restrictions on the
occurrence of words. For instance, in normal written English, following the determiner
the the appearance of a second instance of the same determiner (as in this sentence), is
extremely unlikely. According to the urn model, however, such a sequence is likely to
occur once every 278 words (the relative frequency of the in English is approximately
0.06), say once every two pages. This is not what we normally find. Clearly, syntax
imposes severe constraints on the occurrence of words. Does this imply that the urn
model is wrong? For individual sentences, the answer is undoubtedly yes. But for more
global textual properties such as vocabulary size, a motivated answer is less easy to
give. According to Herdan (1960, 40), reacting to Halle&apos;s criticism of the urn model as
a realistic model for language, there is no problem, since statistics is concerned with
form, not content.4 Whatever the force of this argument may be, Figure 1 demonstrates
clearly that the urn model lacks precision for our data.
In order to ascertain the potential relevance of syntactic constraints referred to by
Halle, we may proceed as follows: If sentence-level syntax underlies the misfit between
the observed and the expected vocabulary size, then this misfit should remain visible
for randomized versions of the text in which the sentences have been left unchanged,
but in which the order of the sentences has been permuted. If the misfit disappears,
</bodyText>
<page confidence="0.892148">
4 M. Halle, &amp;quot;In defence of the number two,&amp;quot; in Studies Presented to J. Whatmough, The Hague, 1957, quoted
in Herdan, 1960, page 40.
461
</page>
<note confidence="0.422098">
Computational Linguistics Volume 22, Number 4
</note>
<bodyText confidence="0.998246826086957">
we know that constraints the domain of which are restricted to the sentence can be
ruled out.
The results of this randomization test applied to Alice in Wonderland, Moby Dick,
and Max Havelaar are shown in the right-hand panels of Figure 1 by means of &amp;quot;+&amp;quot;
symbols. What we find is that following sentence randomization, all traces of a sig-
nificant divergence between the observed and expected vocabulary size disappear.
The differences between E[V(N)] and V(N) are substantially reduced and may remain
slightly negative, as in Alice in Wonderland, or slightly positive, as for Moby Dick, or
they may fluctuate around zero in an unpredictable way, as in Max Havelaar. Since
we are left with variation that is probably to be attributed to the particularities of the
individual randomization orders, we may conclude that at the global level of the text
as an (unordered) aggregate of sentences, the randomness assumption remains rea-
sonable. The nonrandomness at the level of sentence structure does not influence the
expected vocabulary size. As a global text characteristic, it is probably insensitive to
the strictly local constraints imposed by syntax. Apparently, it is the sequential order
in which sentences actually appear that crucially determines the bias of our theoretical
estimates. There are at least two domains where this sequential order might be rele-
vant: the global domain of the discourse structure of the text as a whole, and the more
local domain of relatively small sequences of sentences sharing a particular topic.
To explore these two potential explanatory domains in detail, we need a method
for linking topical discourse structure and local topic continuity with word usage.
Lexical specialization, informally defined as topic-linked concentrated word usage,
and formalized in terms of underdispersion, provides us with the required tool.
</bodyText>
<subsectionHeader confidence="0.999987">
3.2 Lexical Specialization
</subsectionHeader>
<bodyText confidence="0.999978086956522">
Recall that the word Ahab is unevenly distributed in Moby Dick. Given its high fre-
quency (510), one would expect it to occur in all 40 text slices, but it does not. In fact,
there are 11 text slices where Ahab is not mentioned at all. Technically speaking, Ahab
is underdispersed. If there are many such words, and if these underdispersed words
cluster together, the resulting deviations from randomness may be substantial enough
to become visible as a divergence between the observed and theoretical growth curves
of the vocabulary.
In order to explore this intuition, we need a reliable way to ascertain whether a
word is underdispersed. Let the dispersion d, of a word co, be the number of different
text slices in which co, appears. Analytical expressions for E[c/,1 and VAR[di] are avail-
able (Johnson and Kotz 1977, 113-114), so that in principle Z-scores can be calculated.
These Z-scores can then be used to ascertain which words are significantly underdis-
persed in that they occur in significantly too few text slices given the urn model (cf.
Baayen, 1996). Unfortunately, dispersions deviate substantially from normality, so that
Z-scores remain somewhat impressionistic. I have therefore used a randomization test
to ascertain which words are significantly underdispersed.
The randomization test proceeded as follows: The sequence of words of a text was
randomized 1,000 times. For each permutation, the dispersion of each word type in
that particular permutation was obtained. For each word, we calculated the propor-
tion of permutations for which the dispersion was lower than or equal to the empirical
dispersion. For Ahab, all 1,000 permutations revealed full dispersion (d = 40), which
suggests that the probability that the low empirical dispersion of Ahab (d = 28) is due
to chance is (much) less than .001.5 The content words singled out as being signifi-
</bodyText>
<sectionHeader confidence="0.328794" genericHeader="method">
5 I am indebted to an anonymous referee for pointing out to me that Z-scores are imprecise. I am
</sectionHeader>
<page confidence="0.997006">
462
</page>
<subsectionHeader confidence="0.461749">
Baayen Lexical Specialization
</subsectionHeader>
<bodyText confidence="0.99995845">
cantly underdispersed at the 1% level (the significance level I will use throughout this
study for determining underdispersion) reveal a strong tendency to be key words. For
instance, for Moby Dick, the ten most frequent underdispersed content words are Ahab,
boat, captain, said, white, Stubb, whales, men, sperm, and Queequeg. The five most frequent
underdispersed function words are you, ye, such, her, and any.&apos;
The number of chunks in which an underdispersed word appears, and the fre-
quencies with which such a word appears in the various chunks, cannot be predicted
on the basis of the urn model. (Instead of the binomial or Poisson models, the neg-
ative binomial has been found to be a good model for such words, see, e.g., Church
and Gale [1995]). Before studying how these words appear in texts and how they af-
fect the growth curve of the vocabulary, it is useful to further refine our definition of
underdispersion.
Consider again the distribution of the word Ahab in Figure 2. In text slice 25,
Ahab occurs only once. Although this single occurrence contributes to the inter-textual
cohesion of the novel as a whole, it can hardly be said to be a key word within text
slice 25. In order to eliminate such spurious instances of key words, it is useful to set
a frequency threshold. The threshold used here is that the frequency of the word in a
given text slice should be at least equal to the mean frequency of the word calculated
for the text slices in which the word appears. More formally, let f,,k be the frequency of
the i-th word type in the k-th text slice, and define the indicator variable d,,k as follows:
</bodyText>
<equation confidence="0.870444666666667">
di* = 1 iff L &gt; f k and wi underdispersed (6)
d, —
0 otherwise.
</equation>
<bodyText confidence="0.999858">
The number of underdispersed types in text slice k, VU(k), and the corresponding
number of underdispersed tokens, NU(k), can now be defined as
</bodyText>
<equation confidence="0.999974">
VU(k) = (7)
NU(k) = di,k • fi,k. (8)
</equation>
<subsectionHeader confidence="0.999983">
3.3 Lexical Specialization and Discourse Structure
</subsectionHeader>
<bodyText confidence="0.941084882352941">
We are now in a position to investigate where underdispersed words appear and
how they influence the observed growth curve of the vocabulary. First consider Fig-
ure 4, which summarizes a number of diagnostic functions for Alice in Wonderland.
The upper panels plot VU(k) (left) and NU(k) (right), the numbers of underdispersed
types and tokens appearing in the successive text chunks. Over sampling time, we
observe a slight increase in both the numbers of tokens and the numbers of types.
Both trends are significant according to least squares regressions, represented by dot-
ted lines (F(1,38) = 6.591,p &lt; .02 for VU(k); F(1,38) = 16.58,p &lt; .001 for NU(k)). A
time-series smoother using running medians (Tukey 1977), represented by solid lines,
similarly indebted to Fiona Tweedie, who suggested the use of the randomization test. Comparison of
the results based on Z-scores (see Baayen, to appear) and the results based on the randomization test,
however, reveal only minor differences that leave the main patterns in the data unaffected.
6 The present method of finding underdispersed words appears to be fairly robust with respect to the
number of text slices K. For different numbers of text chunks, virtually the same high-frequency words
appear to be underdispersed. The number of text chunks exploited in this paper, 40, has been chosen to
allow patterns in &amp;quot;sampling time&amp;quot; to become visible without leading to overly small text slices for the
smaller texts.
</bodyText>
<page confidence="0.999232">
463
</page>
<figure confidence="0.999752757575758">
Computational Linguistics
Volume 22, Number 4
Alice in Wonderland
0
-SC CV
&gt;0
• • • • .... .. t ....
........ ... •
• ••
0 10 20 30 40 10 20 30 40
Series: VU Series: NU
5 10 15
Lag
30 40
u_ cc!
o
I I I
5 10 15
Lag
4cg
60
10 20 30 40
9
0
10
20
2
a
0
10
40
30
20
</figure>
<figureCaption confidence="0.997937">
Figure 4
</figureCaption>
<bodyText confidence="0.997338923076923">
Diagnostic functions for Alice in Wonderland. VU(k) and NU(k): numbers of underdispersed
types and tokens in text slice k; ACF: auto-correlation function; Pr(U, type) and Pr(U, token):
proportions of underdispersed types and tokens; D(k) and DU(k): progressive difference
scores for the overall vocabulary and the underdispersed words.
suggests a slightly oscillating pattern. At least for a time lag of 1, this finds some sup-
port in the autocorrelation functions, shown in the second line of panels of Figure 4.
Clearly, key words are not uniformly distributed in Alice in Wonderland. Not only does
the use of key words in one text slice appear to influence the intensity with which key
words are used in the immediately neighboring text slices, but as the novel proceeds
key words appear with increasing frequency.
How does this nonrandom organization of key words in the discourse as a whole
influence V(N)? To answer this question, it is convenient to investigate the nature of
the new types that arrive with the successive text slices. Let
</bodyText>
<equation confidence="0.991078">
V(Mk) = V(Mk) - V(Mk-i) (9)
denote the number of new types observed in text slice k, and let
VU(Mk) = VU(Mk) - VU(Mk_i) (10)
</equation>
<bodyText confidence="0.988222">
denote the number of new underdispersed types for text slice k. The proportion of new
underdispersed types in text slice k on the total number of new types, Pr(U, type, k)
is given by
</bodyText>
<equation confidence="0.990759">
AVU(k)
Pr(U, type, k) = (11)
AV(k) •
</equation>
<bodyText confidence="0.9962135">
The plot of Pr(U, types, k) is shown on the third row of Figure 4 (left-hand panel).
According to a least squares regression (dotted line), there is a significant increase in
</bodyText>
<page confidence="0.997948">
464
</page>
<subsectionHeader confidence="0.461949">
Baayen Lexical Specialization
</subsectionHeader>
<bodyText confidence="0.999643947368421">
the proportion of underdispersed new types as k increases (F(1, 38) = 5.804, p &lt; .05).
The right-hand side counterpart shows a similar trend for the word tokens that is also
supported by a least squares regression (F(1, 38) = 5.681, p &lt; .05). Here, the proportion
of new underdispersed tokens on the total number of new tokens is defined as
with Pr( U, token, k) = y-• n &apos; k ,
and 2.4 ni,k
k-1
f f iff Etn=ir 0
n l&apos;k =I,. 0 otherwise,
= F. k iff ,m
= 0 and di,k = 1
0 otherwise.
The increase in the proportions of new underdispersed types and tokens shows that
the pattern observed for the absolute numbers of types and tokens observed in the
top panels of Figure 4 persists with respect to the new types and tokens.
We can now test to what extent the underdispersed types are responsible for the
divergence of E[V(N)] and its expectation by comparing the progressive difference
scores D(k) defined in (5) with the progressive difference scores for the subset of the
underdispersed words DU(k), defined as
</bodyText>
<equation confidence="0.995365">
DU(k) =-- E[VU(k)] – E[VU(k – 1)] – AVU(k). (15)
</equation>
<bodyText confidence="0.998685769230769">
The two progressive difference score functions are shown in the bottom left panel of
Figure 4, and the residuals D(k) – DU(k) are plotted in the bottom right-hand panel.
The residuals do not reveal any significant trend (F(1, 38) &lt; 1), which suggests that the
underdispersed vocabulary is indeed responsible for the main trend in the progressive
difference scores D(k) of the vocabulary and hence for the divergence between E[V(N)]
and V(N). In the next section, I will argue that intra-textual cohesion is in large part
responsible for the general downward curvature of DU(k). In what follows, I will first
present an attempt to understand the differences in the error scores E[V(N)] – V(N)
shown in Figure 1 as a function of differences in the use of key words at the discourse
level.
In Alice in Wonderland, key words are relatively rare in the initial text slices. As
a result, these text slices reveal fewer types than expected under chance conditions.
Consequently, V(N) is smaller than E[V(N)]. For increasing k, as shown in the up-
per right panel of Figure 1, the divergence between V(N) and its expectation first
increases—the initial text slices contain the lowest numbers of underdispersed types
and tokens—and then decreases as more and more underdispersed words appear.
Thus the semi-circular shape of the error scores E[V(N)] – V(N) shown in Figure 1 is
a direct consequence of the topical structure at discourse level of Alice in Wonderland.
The error scores E[V(N)] – V(N) for Moby Dick and Max Havelaar shown in Fig-
ure 1 reveal a different developmental profile. In these novels, the maximal diver-
gence appears early on in the text, after which the divergence decreases until, just
before the end, V(N) becomes even slightly larger than its expectation. Is it possible
to understand this qualitatively different pattern in terms of the discourse structure
of these novels? First, consider Moby Dick. A series of diagnostic plots is shown in
Figure 5. The numbers of underdispersed types and tokens VU(k) and NU(k) reveal
some variation, but unlike in Alice in Wonderland, there is only a nonsignificant trend
</bodyText>
<page confidence="0.998378">
465
</page>
<figure confidence="0.999803618181818">
Computational Linguistics Volume 22, Number 4
Moby Dick
10
20
30
40
40
10 20 30
I [Ahab]
0
_Nc
9 0
-Y 0
0
10
20
30
40
Do
&gt; 03
0
(00
CD 0
Z•ci
c‘i
0
_0
°
-C
o
-2ti
•—•
0
---- 0
_Y 0
• . • • • • • •
• • •
• •
10 20 30 40
• • • . • • • • •
• •
••
0 10 20 30 40
• • . • •
.
10 20 30 40
0 10 20 30 40
•• •
• • • •
•
• . __ .
, ..., ............... f
• • •• • •
. . •
k
</figure>
<figureCaption confidence="0.9812166">
Figure 5
Diagnostic functions for Moby Dick. VU(k) and NU(k): numbers of underdispersed types and
tokens in text slice k; Pr(U, type) and Pr(U, token): proportions of underdispersed types and
tokens; D(k) and DU(k): progressive difference scores for the overall vocabulary and the
underdispersed words; f[Ahab](k): frequency of Ahab in text slice k.
</figureCaption>
<bodyText confidence="0.999744238095238">
(F(1,38) = 2.11, p &gt; .15 for VU(k), F(1,38) = 1.98,p &gt; .15 for NU(k)) for underdisper-
sion to occur more often as the novel progresses. The absence of a trend is supported
by the proportions of underdispersed types and tokens, shown in the second row of
panels (F &lt; 1 for both types and tokens). In the last text slices, underdispersed words
are even underrepresented. The bottom panels show that the progressive difference
scores DU(k) for the underdispersed words capture the main trend in the progressive
difference scores of the total vocabulary D(k) quite well: The residuals D(k) — DU(k)
do not reveal a significant trend (F(1, 38) = 1.08, p&gt; .3).
Interestingly, the use of underdispersed words in Moby Dick is to some extent
correlated with the frequency of the word Ahab, with respect to both types and tokens
(F(1,38) = 4.61,p &lt; .04,r2 = .11 for VU(k); F(1,38) = 10.77,p &lt; .003,r2 = .22 for
NU(k). The panels on the third row of Figure 5 show the frequencies of Ahab (left)
and VU(k) as a function of the frequency of Ahab (right). A nonparametric time series
smoother (solid line) supports the least squares regression line (dotted line). In other
words, the key figure of Moby Dick induces a somewhat more intensive use of the key
words of the novel.
The nonuniform distribution of Ahab sheds some light on the details of the shape
of the difference function E[V(N)] — V(N) shown in Figure 1. The initial sections do
not mention Ahab, it is here that D(k) reveals its highest values, and here too we find
the largest discrepancies between E[V(N)] and V(N). By text slice 20, Ahab has been
firmly established as a principal character in the novel, and the main key words have
</bodyText>
<page confidence="0.998505">
466
</page>
<figure confidence="0.957741545454545">
Baayen Lexical Specialization
Max Havelaar
&apos;2 2-8 •
&gt;0 D • • •
Z 0 • * •
Lr) • .&amp;quot;
0 10 20 30 40 10 20 30 40
Series : VU Series: NU
CO •
0
&lt;
0 5 10 15
Lag
LL
0 0
•:/*
0
0 5 10
20liii
Lag
I I I &amp;quot;
15
CD 0 0
D o
11- a 7
2 0
D °
0
, 0
. . • • • • .. • Nheft
10 20 30 40 0 10 20 30 40
.. ..........
o 10 20 30 40 0 10 20 30 40
</figure>
<figureCaption confidence="0.943693">
Figure 6
</figureCaption>
<bodyText confidence="0.95994236">
Diagnostic functions for Max Havelaar. VU(k) and NU(k): numbers of underdispersed types
and tokens in text slice k; ACF: auto-correlation function; Pr(U, type) and Pr(U, token):
proportions of underdispersed types and tokens; D(k) and DU(k): progressive difference
scores for the overall vocabulary and the underdispersed words.
appeared. The overestimation of the vocabulary is substantially reduced. As the novel
draws to its dramatic end, the frequency of Ahab increases to its maximum. The plots
on the first row of Figure 5 suggest that underdispersed types and tokens are also
used more intensively in the last text slices. However, the proportions plots on the
second row show a final dip, suggesting that at the very end of the novel, a more
than average number of normally dispersed new types appears. Considered together,
this may explain why at the very end of the novel the expected vocabulary slightly
underestimates the observed vocabulary size, as shown in Figure 1.
Finally, consider the diagnostic plots for Max Havelaar, shown in Figure 6. The time
series smoother (solid line) for the absolute numbers of underdispersed types (VU(k))
and tokens (NU(k)) suggests an oscillating use of key words without any increase in the
use of key words over time (the dotted lines represent the least squares regression lines,
neither of which are significant: F &lt; 1 in both cases). This oscillatory structure receives
some support from the autocorrelation functions shown in the second row of panels.
Especially in the token analysis, there is some evidence for positive autocorrelation
at lag 1, and for a negative polarity at time lags 8 and 9. No trend emerges from
the proportions of new underdispersed types and tokens (third row, F &lt; 1 in both
analyses). A comparison of the progressive difference scores D(k) and DU(k) (bottom
row) shows that the underdispersed words are again largely responsible for the large
values of D(k) for small k. No significant trend remains in the residuals D(k) — DU(k)
(F(1,38) = 1.848, p&gt; .15).
</bodyText>
<page confidence="0.995376">
467
</page>
<figure confidence="0.612174">
Computational Linguistics Volume 22, Number 4
</figure>
<figureCaption confidence="0.788959">
Figure 1 revealed that E[V(N)] — V(N) is largest around text slices 3 to 7, but
</figureCaption>
<bodyText confidence="0.7615745">
becomes negative for roughly the last third of the novel. This pattern may be due
to the oscillating use of key words in Max Havelaar. Although there is a fair number
of key words in the first few text chunks, the intensity of key words drops quickly,
only to rise again around chunk 20. Thus, key words are slightly underrepresented in
the first part of the novel, allowing the largest divergence between the expected and
observed vocabulary size to emerge there.
</bodyText>
<subsectionHeader confidence="0.997069">
3.4 The Paragraph as the Domain of Topic Continuity
</subsectionHeader>
<bodyText confidence="0.993121833333333">
The preceding analyses all revealed violations of the randomness assumption under-
lying the urn model that originate in the topical structure of the narrative as a whole.
I have argued that a detailed analysis of the distribution of key word tokens and
types may shed some light on why the theoretical vocabulary size sometimes over-
estimates and sometimes underestimates the observed vocabulary size. We are left
with the question of to what extent repeated use of words within relatively short se-
quences of sentences, henceforth for ease of reference paragraphs, affects the accuracy
of E[V(N)]. I therefore carried out two additional analyses, one using five issues of the
Dutch newspaper Trouw, and one using the random samples of the Dutch newspaper
De Telegraaf available in the Uit den Boogaart (1975) corpus. For both texts, no overall
topical discourse structure is at issue, so that we can obtain a better view of the effects
of intra-textual cohesion by itself.
For each newspaper, the available texts were brought together in one large cor-
pus, preserving chronological order. Each corpus was divided into 40 equally large text
slices. The upper left panel of Figure 7 shows that in the consecutive issues of Trouw
(March 1994) the expected vocabulary size differs significantly from the observed vo-
cabulary size for all of the first 20 measurement points, the domain for which signif-
icance can be ascertained (see footnote 3). The upper right panel reveals that for the
chronologically ordered series of samples from De Telegraaf in the Uit den Boogaart
corpus (268 randomly sampled text fragments with on average 75 word tokens) only
3 text chunks reveal a significant difference between E[V(N)] and V(N). The bottom
panels of Figure 7 show the corresponding plots of the progressive difference scores
for the complete vocabulary (D(k), &amp;quot;.&amp;quot;) and underdispersed words (DU(k), &amp;quot;+&amp;quot;). The
least squares regression lines (dotted) for D(k), supported by nonparametric scatter-
plot smoothers (solid lines), reveal a significant negative slope (F(1, 38) = 6.89, p &lt; .02
for Trouw, F(1,38) = 10.99, p &lt; .001 for De Telegraaf). The residuals D(k) — DU(k) do
not reveal any significant trends (F &lt; 1 for both newspapers). Note that for De Tele-
graaf DU(k) does not capture the downward curvature of D(k) as well as it should
for large k. This may be due to the relatively small number of words that emerge as
significantly underdispersed for this corpus.
Figure 7 shows that intra-textual cohesion within paragraphs is sufficient to give
rise to substantial deviation between E[V(N)] and V(N) in texts with no overall dis-
course organization. Within successive issues of a newspaper, in which a given topic is
often discussed on several pages within the same newspaper, and in which a topic may
reappear in subsequent issues, strands of inter-textual cohesion may still contribute
significantly to the large divergence between the observed and expected vocabulary
size. It is only by randomly sampling short text fragments, as for the data from the Uit
den Boogaart corpus, which contains samples evenly spread out over a period of one
year, that a substantial reduction in overestimation is obtained. Note, however, that
even for the corpus data we again find that the expectation of V(N) is consistently too
high. Within paragraphs, words tend to be reused more often than expected under
change conditions. This reuse pre-empts the use of other word tokens, among which
</bodyText>
<page confidence="0.998884">
468
</page>
<figure confidence="0.533984">
Baayen Lexical Specialization
</figure>
<figureCaption confidence="0.830876">
Figure 7
Diagnostic plots for two Dutch newspapers. The difference between the expected and
observed vocabulary size for the Trouw data (five issues from March 1994) and the random
samples of De Telegraaf in the Uit den Boogaart coipus (upper panels; significant differences
are highlighted for the first 20 measurement points). The bottom panels show the progressive
difference error scores for the total vocabulary (113(k)) and for the subset of underdispersed
words (D1,1(k)). The dotted line is a least squares regression, the solid line a nonparametric
scatterplot smoother.
</figureCaption>
<bodyText confidence="0.999384">
tokens of types that have not been observed among the preceding tokens, and leads
to a decrease in type richness. Since intra-textual cohesion is also present in the texts
of novels, we may conclude that the overestimation bias in novels is determined by a
combination of intra-textual and inter-textual cohesion.
</bodyText>
<sectionHeader confidence="0.940459" genericHeader="method">
4. Implications
</sectionHeader>
<bodyText confidence="0.999413666666667">
We have seen that intra-textual and inter-textual cohesion lead to a significant differ-
ence between the expected and observed vocabulary size for a wide range of sample
sizes. This section addresses two additional questions. First, to what extent does the
nonrandomness of word occurrences affect distributions of units selected or derived
from words? Second, how does cohesive word usage affect the Good-Turing frequency
estimates?
</bodyText>
<subsectionHeader confidence="0.981045">
4.1 Word-derived Units
</subsectionHeader>
<bodyText confidence="0.956274333333333">
First consider the effect of nonrandomrtess on the frequency distributions of mor-
phological categories. The upper panels of Figure 8 plot the difference between the
expected and observed vocabulary size for the morphological category of words with
</bodyText>
<figure confidence="0.997959222222222">
10
10
20
30
20
40
30
40
Trouw (full issues)
0 50000 100000 150000 200000 250000
Trouw (full issues)
De Telegraaf (samples)
5000 10000 15000 20000
De Telegraaf (samples)
0
c‘i
0
0
„_.... 0
0
z
z 0
\ •
&apos; \
I.
• \
• \
</figure>
<page confidence="0.813195">
469
</page>
<figure confidence="0.998822928571429">
Computational Linguistics Volume 22, Number 4
-heid in &apos;Max Havelaar&apos; -heid in Trouw
10 \ 0
\ /\.... ,/\. C•1
A&apos;••
\
\
0
10 20 30 40 5 10 15 20
Syllables in Trouw Digraphs in &apos;Alice in Wonderland&apos;
10
Z
0
0
^ 01
Z 0
Lu 0
10
20
30
40
10
20
30
40
\
i
I •• •s.
</figure>
<figureCaption confidence="0.994542">
Figure 8
</figureCaption>
<bodyText confidence="0.9727152">
Diagnostic plots for affixes, syllables, and digraphs. The difference between the expected and
observed vocabulary size for the morphological category of words with the Dutch suffix -heid
&apos;-ness&apos; in Max Havelaar (upper left) and in Trouw (upper right), for syllables in Trouw (lower
left), and for digraphs in Alice in Wonderland. Significant differences are shown in bold for the
first half of the tokens.
the Dutch suffix -heid, which, like -ness in English, is used to coin abstract nouns from
adjectives (e.g., snelheid, &apos;speed&apos;, from snel, &apos;quick&apos;). The plots are based on samples
consisting of all and only those words occurring in Max Havelaar (upper left) and
Trouw (upper right) that belong to the morphological category of -heid, ignoring all
other words, and preserving their order of appearance in the original texts. The sample
of -heid words in Max Havelaar consisted of 640 tokens representing 260 types, of which
146 hapax legomena. From Trouw, 1145 tokens representing 394 types were extracted,
among which 246 hapax legomena.
In Max Havelaar, a number of words in -heid, such as waarheid &apos;truth&apos; and vrijheid
&apos;freedom&apos;, are underdispersed key words. Not surprisingly, this affects the growth
curve of -heid itself. For small values of k, we observe a significant divergence between
E[V(N)] and V (N). In the newspaper Trouw, where -heid words do not play a central
role in an overall discourse, no significant divergence emerges. Nevertheless, we again
observe a consistent trend for the expected vocabulary size to overestimate the actual
vocabulary size.
</bodyText>
<figureCaption confidence="0.9464816">
Figure 8 also plots the development of the vocabulary of syllables in Trouw (bot-
tom left), and the development of the vocabulary of digraphs in Alice in Wonderland
(bottom right). The &amp;quot;texts&amp;quot; of syllables and digraphs preserve the linear order of the
texts from which they were derived. For both digraphs (80,870 tokens representing
398 types, of which 30 hapax legomena) and syllables (470,520 tokens, 6,748 types,
</figureCaption>
<page confidence="0.990875">
470
</page>
<note confidence="0.444753">
Baayen Lexical Specialization
</note>
<bodyText confidence="0.997104666666667">
and 1,909 hapax legomena), Figure 8 reveals significant deviation in the first half of
both texts. This suggests that the nonrandomness observed for words carries over to
word-based units such as digraphs and syllables.
</bodyText>
<subsectionHeader confidence="0.999728">
4.2 Accuracy of Good-Turing Estimates
</subsectionHeader>
<bodyText confidence="0.999911166666667">
Samples of words generally contain—often small—subsets of all the different types
available in the population. The probability mass of the unseen types is generally large
enough to significantly bias population probabilities estimated from sample relative
frequencies. Good (1953) introduced an adjusted frequency estimate (which he credits
to Turing) to correct this bias. Instead of estimating the probability of a word with
frequency f by its sample relative frequency
</bodyText>
<equation confidence="0.96031475">
P1= (16)
Good suggests the use of the adjusted estimate
1 (f + 1)E[V(N,f + 1)] (17)
1(m )1; N E[V(N,f )] •
</equation>
<bodyText confidence="0.966114">
A closely related statistic is the probability P(N) of sampling a new, unseen type after
N word tokens have been sampled:
</bodyText>
<equation confidence="0.999431">
P(N) = E[V(N,1)] . (18)
</equation>
<bodyText confidence="0.99995864">
These estimates are in wide use (see, e.g., Church and Gale [1991] for application
to bigrams, Bod [1995] for application to syntax, and Baayen [1992] and Baayen and
Sproat [1996] for application to morphology). Hence, it is useful to consider in some
detail how their accuracy is affected by inter-textual and intra-textual cohesion. To this
end, I carried out a short series of experiments of the following kind.
Assume that the Trouw data used in the previous section constitute a population
of N = 265,360 word tokens from which we sample the first N/2 = 132,680 words. For
the Trouw data, this is a matter of stipulation; but for texts such as Moby Dick or Alice
in Wonderland, an argument can be made that the novel is the true population rather
than a sample from a population. For the present purposes, the crucial point is that
we now have defined a population for which we know exactly what the population
probabilities—the relative frequencies in the complete texts—are.
First consider how accurately we can estimate the vocabulary size of the popu-
lation from the sample. The expression for E[V(N)] given in (1) that we have used
thus far does not allow us to extrapolate to larger sample sizes. However, analytical
expressions that allow both interpolation (in the sense of estimating V(N) on the basis
of the frequency spectrum for sample sizes M &lt; N) and extrapolation (in the sense of
estimating V(M) for M &gt; N) are available (for a review, see Chitashvili and Baayen
[19931). Here, I will make use of a smoother developed by Sichel (1986). The three
parameters of this smoother are estimated by requiring that E[V(N)] = V(N), that
E[V(N, 1)] = V(N, 1), and by minimizing the chi-square statistic for a given span of
frequency ranks.
The upper left panel of Figure 9 shows that it was possible to select the param-
eters of Sichel&apos;s model such that the observed frequencies of the first 20 frequency
ranks (V(N,f),f = 1, ,20) do not differ significantly from their model-dependent
</bodyText>
<page confidence="0.995561">
471
</page>
<figure confidence="0.771705588235294">
Computational Linguistics Volume 22, Number 4
Frequency spectrum at N = 132680 Interpolation from N = 132680
o
0
Z 0
— Lc)
0
0
Lo
r•-•
.4; • •
.4.•
4, •
5 10 15 20 20 40 60 80 100 120
N (*1000)
Interpolation and extrapolation from N = 132680 Estimation errors conditional on N = 132680
6 a
CV c,.&apos;&apos;&apos; ■-• u,
1- 0
■,
g zf, .
&gt;u, &gt;L,
,,.... -
?... U.&apos; 5- •-&apos;
W 0
N (&amp;quot; 000)
Probability mass errors (Good-Turing)
0
S 6
0
7 c.3
=-
o
0
</figure>
<figureCaption confidence="0.951954">
Figure 9
</figureCaption>
<bodyText confidence="0.75727475">
Interpolation and extrapolation from sample (the first half of the Trouw data) to population
(the complete Trouw data). E[V(N,f)] and V(N,f): expected and observed frequency spectrum;
E[V(N)] and V(N): expected and observed numbers of types; Mp(f): population probability
mass of the types with frequency f in the sample; MGT(f): Good-Turing estimate of Mp(f);
Ms(f): unadjusted sample estimate of Mp(f).
expectations Es[V(N,f)1.7 The upper right panel shows that interpolation on the basis
of Sichel&apos;s model (dashed line) is virtually indistinguishable from interpolation using
(1) (dotted line). The observed vocabulary sizes are represented by large dots. As ex-
pected, both (1) and the parametric smoother reveal the characteristic overestimation
pattern.
The center panels of Figure 9 show that the overestimation characteristic for inter-
polation is reversed when extrapolating to larger samples. For extrapolation, under-
estimation is typical. The dotted line in the left-hand panel represents the observed
vocabulary size of the complete Trouw text, the solid line shows the result from in-
terpolation and extrapolation from N = 132,680. The right-hand panel highlights the
corresponding difference scores. For N = 265,360, the error is large: 5.5% of the actual
vocabulary size.
Having established that E[V(N)] underestimates V(N) when extrapolating, the
question is how well the Good-Turing estimates perform. To determine this, I will
consider the probability mass of the frequency classes V(M,f) for f = 1 ... 40. Let
</bodyText>
<equation confidence="0.889969">
MGT (f, M) = V(M,f) . pit (M) (19)
</equation>
<page confidence="0.933048">
7 The fit (X2(18) = 9.93,p &gt; .9) was obtained for the parameter values a = 0.291,-y = —0.7, and
</page>
<figure confidence="0.988375222222222">
b = 0.011.
50 100 150 200 250
10
20
30
40
.. .s.
L&apos;u 0 50 100 150 200 250
N ( 000)
Probability mass errors (unadjusted)
0
• 0
c9
0 10
........
20
30
40
</figure>
<page confidence="0.972837">
472
</page>
<note confidence="0.446073">
Baayen Lexical Specialization
</note>
<bodyText confidence="0.998935666666667">
be the joint Good-Turing probability mass of all types with frequency f in the sample
of M = 132,680 tokens, and let M(f) be the joint probability mass of exactly the same
word types, but now in the population (N = 265,360 tokens):
</bodyText>
<equation confidence="0.971748">
M(f) Ei .f(i, N) (20)
</equation>
<bodyText confidence="0.998774">
with f(i, X) the frequency of the i-th type in a sample of X tokens. The bottom left
panel of Figure 9 shows that for the first frequency ranks f, the Good-Turing estimate
MGT (f, M) underestimates the probability mass of the frequency class in the population.
For the higher-frequency ranks, the estimates are fairly reliable. The bottom right
panel of Figure 9 plots the corresponding errors for the unadjusted sample probability
estimate
</bodyText>
<equation confidence="0.998307">
Ms(f,M) = (21)
</equation>
<bodyText confidence="0.999172">
which overestimates the population values. Surprisingly, the unadjusted estimates
overestimate the population values to roughly the same extent that the adjusted esti-
mates lead to underestimation. A heuristic estimate,
</bodyText>
<equation confidence="0.9983015">
V(M,f) + 1)Es [V(M,f +1)] + fEs[1704,f)]
Mh(f,M) Es[v(m,f)]2M (22)
</equation>
<bodyText confidence="0.993516275862069">
the mean of Ms(f, M) and MGT(f,M), appears to approximate the population relative
class frequencies Mp (f) reasonably well, as shown in Table 1 for the Trouw data as
well as for Alice in Wonderland, Moby Dick, and Max Havelaar. For f &gt; 5, as shown in
Figure 10, the heuristic estimate remains a reasonable compromise.
We have seen that both E[V(N)] and the Good-Turing estimates MGT(f,M) (es-
pecially for f &lt; 5) lead to underestimation of population values. Interestingly, P(M)
overestimates the probability mass of unseen types. For the Trouw data, at M = 132,680
we count 11,363 hapax legomena, hence P(M) = 0.0856. However, the probability mass
of the types that do not appear among the first 132,680 tokens, M(0), is much smaller:
0.0609. Table 1 shows that P(M) similarly leads to overestimation for Alice in Wonder-
land, Moby Dick, and Max Havelaar. To judge from Table 1, the Good-Turing estimate
MGT (1, M) is an approximate lower bound and the unadjusted estimate M5(1, M) a
strict upper bound for M(0).
It is easy to see why P(N) is an upper bound for coherent text by focusing on
its interpretation. Given the urn model, the probability that the first token sampled
represents a type that will not be represented by any other token equals V(N,1)/N.
By symmetry, this probability is identical to the probability that the very last token
sampled will represent an unseen type. This probability approximates the probability
that, after N tokens have been sampled, the next token sampled will be a new type.
However, this interpretation hinges on the random selection of word tokens, and
this paper presents ample evidence that once a word has been used it is much more
likely to be used again than the urn model predicts. Hence, the probability that after
sampling N tokens the next token represents an unseen type is less than V(N,1)/N.
Due to intra-textual and inter-textual cohesion, the V(N) — V(N, 1) types that have
already been observed have a slightly higher probability of appearing than expected
under chance conditions, and consequently the unseen types have a lower probability.
Summing up, the Good-Turing frequency estimates are severely effected by the
cohesive use of words in normal text. In the absence of probabilistic models that take
cohesive word usage into account, estimates of (relative) frequencies remain heuristic
</bodyText>
<page confidence="0.998668">
473
</page>
<note confidence="0.613604">
Computational Linguistics Volume 22, Number 4
</note>
<tableCaption confidence="0.99607">
Table 1
</tableCaption>
<table confidence="0.966788828571429">
Comparison of probability mass estimates for frequencies f = 1, , 5 using the smoother
Es[V (N , f )] of Sichel (1986). The probability mass of unseen types, Mp (0), is also tabulated.
Notation: MGT (f , M): Good-Turing estimate; M, (f, M): sample estimate; M h (f, M): heuristic
estimate; Mp(f): population mass. For Max Havelaar, a sample comprising the first third of the
novel was used, for the other texts, a sample consisting of the first half of the tokens was
selected.
V (N , f ) Es [V (N, f )] MGT , M) Ms(f , M) Mh(f M) M(f)
Alice in Wonderland
0 0.0630
1 885 885.00 0.0411 0.0668 0.0540 0.0560
2 287 272.27 0.0328 0.0434 0.0381 0.0372
3 147 137.27 0.0277 0.0333 0.0305 0.0293
4 97 85.52 0.0255 0.0293 0.0274 0.0289
5 68 59.55 0.0230 0.0257 0.0243 0.0228
Moby Dick
0 0.0350
1 5,914 5,914.04 0.0366 0.0553 0.0460 0.0472
2 2,035 1,958.22 0.0272 0.0381 0.0326 0.0331
3 990 932.22 0.0218 0.0278 0.0248 0.0251
4 601 549.44 0.0187 0.0225 0.0206 0.0210
5 416 366.22 0.0168 0.0195 0.0181 0.0179
Max Havelaar
0 0.0921
1 3,513 3,513.01 0.0494 0.1058 0.0776 0.0692
2 908 821.04 0.0362 0.0547 0.0455 0.0411
3 346 362.65 0.0240 0.0313 0.0276 0.0246
4 214 208.95 0.0213 0.0258 0.0235 0.0216
5 157 137.84 0.0203 0.0236 0.0220 0.0189
Trouw
0 0.0609
1 11,363 11,363.05 0.0431 0.0856 0.0644 0.0639
2 2,941 2,856.65 0.0297 0.0443 0.0370 0.0359
3 1,338 1,276.07 0.0233 0.0303 0.0268 0.0256
4 826 737.69 0.0206 0.0249 0.0227 0.0219
5 532 487.51 0.0172 0.0200 0.0186 0.0190
</table>
<bodyText confidence="0.9965685">
in nature. For the frequencies of types occurring at least once in the sample, the
average of the sample and Good-Turing adjusted frequencies is a useful heuristic. For
estimates of the probability of unseen types, the sample and Good-Turing estimates
provide approximate upper and lower bounds.
</bodyText>
<sectionHeader confidence="0.971357" genericHeader="method">
5. Discussion
</sectionHeader>
<bodyText confidence="0.99922">
Words do not occur randomly in texts. This simple fact is difficult to take into account
in statistical models of word frequency distributions. Hence, it is often ignored, in
the hope that violations of the randomness assumption will not seriously affect the
accuracy of quantitative measures and estimates.
The goal of this paper has been to explore in detail the consequences of intra-
</bodyText>
<page confidence="0.995942">
474
</page>
<figure confidence="0.962554">
Baayen Lexical Specialization
0 10 20 30 40
</figure>
<figureCaption confidence="0.939045">
Figure 10
</figureCaption>
<bodyText confidence="0.98725092">
Frequency class probability mass estimates for the first 40 frequency ranks in a sample of
M = 132,680 of the Trouw data. The dots denote the probability mass Mr(f) in the full text
(N = 265,360) of the words with frequency f in the sample. The Good-Turing estimates
MGT , M) are represented by &amp;quot;e,&amp;quot; the sample estimates Ms (f, M) by &amp;quot;s,&amp;quot; and the heuristic
estimate Mh (f , M) by &amp;quot;+&amp;quot;.
textual and inter-textual cohesion on the accuracy of theoretical estimates of vocab-
ulary size, the growth rate of the vocabulary, and Good-Turing adjusted frequency
estimates, in the belief that knowledge of how nonrandomness might affect these
measures ultimately leads to a better understanding of the conditions under which
these measures may, or may not, be reliable.
Analyses of three novels, five consecutive issues of the Dutch newspaper Trouw,
and the chronologically ordered samples of the Dutch newspaper De Telegraaf in
the Uit den Boogaart corpus, all revealed systematic overestimation for the expected
vocabulary size. Further analyses of subsets of derived words, syllables, and digrams
showed that the overestimation bias reappears in units derived from words when
these words occur in normal, cohesive text.
The overestimation bias disappears when the order of the sentences is random-
ized. This indicates that the bias should not be attributed to syntactic and semantic
constraints on word usage operating within the sentence. Instead, the bias arises due
to intra-textual and inter-textual cohesion. In sequences of sentences, words are more
likely to be reused than expected under chance conditions. Coherent discourse requires
local topic continuity This intra-textual cohesion gives rise to a substantial part of the
overestimation bias, a bias that leads to significant deviations even when small text
fragments of some 75 words are selected randomly from a newspaper.
In addition to intra-textual cohesion, there are words that contribute to the cohe-
</bodyText>
<page confidence="0.996247">
475
</page>
<note confidence="0.691928">
Computational Linguistics Volume 22, Number 4
</note>
<bodyText confidence="0.999992342857143">
sion of the discourse as a whole. Detailed analyses of how these key words appear
over sampling time in the novels reveal marked differences in their distributions.
These differences in turn shed light on the details of the differences in the patterns of
estimation errors E[V(N)] – V(N) that characterize the texts. The progressive difference
scores of the key words, the deviation scores for the expected and observed numbers
of new types appearing in the successive text slices, reveal a pattern that is highly
similar to the same scores for the vocabulary as a whole, both qualitatively and quan-
titatively. This supports the hypothesis that the key words are primarily responsible
for the deviation of the expected vocabulary size from its expectation.
Nonrandomness in word usage not only introduces a bias with respect to the
expected vocabulary size—overestimation when interpolating and underestimation
when extrapolating, it also affects the accuracy of the Good-Turing estimates. To correct
for an overestimation bias, Good (1953) introduced adjusted estimates, building on
the assumption that word usage is to all practical purposes random. These adjusted
estimates, however, appear to overshoot their mark for continuous text in that they
underestimate the population relative frequencies to roughly the same extent that the
unadjusted probabilities lead to overestimation, especially for the lowest frequencies.
Again, the effect of inter-textual and intra-textual cohesion manifests itself. Once used,
words tend to be used again, and this leads to a somewhat higher relative population
frequency than expected. The other side of the same coin is that Good&apos;s estimate for
the probability mass of unseen types, P (N), is an upper bound. The words that have
already been used have a raised probability of being used again. Hence, the probability
for unseen types to appear is lowered.
There are two major ways to deal with the effects of nonrandomness in word usage
on the accuracy of statistical estimates. First, by randomly sampling individual sen-
tences instead of sequences of sentences, the effects of intra-textual and inter-textual
cohesion will be largely eliminated. With the increasingly large corpora that are becom-
ing available at present, enhanced sampling methods should pose no serious problem.
For literary studies, however, the discourse structure of a text is part and parcel of the
object of study itself. Here, the use of the heuristically adjusted estimates proposed in
Section 4.2 may prove to be useful.
Finally, the investigation of the distribution of key words may turn out to be a
useful tool for investigating the structure of literary texts, a tool that may lead to an
improved understanding of the role of lexical specialization in shaping the quantitative
developmental structure of the vocabulary.
</bodyText>
<sectionHeader confidence="0.933763" genericHeader="method">
Appendix
</sectionHeader>
<bodyText confidence="0.995995666666667">
Equation (1) can be derived as follows; see Good 1953; Good and Toulmin 1956; Kalinin
1965: Let f(i,M) denote the frequency of co, in a sample of M tokens (M &lt; N), and
define
</bodyText>
<equation confidence="0.77722">
xi = f 1 if f (i, M) = m
(23)
0 otherwise.
</equation>
<bodyText confidence="0.957717">
Denoting the probability of co, by pi, the expected total number of word types with
frequency m in a sample of M tokens, E[V(M, m)], is given by
E[V(M, m)] = E [E Xi]
</bodyText>
<sectionHeader confidence="0.666656" genericHeader="method">
= E E[X]
</sectionHeader>
<page confidence="0.99175">
476
</page>
<note confidence="0.34386">
Baayen Lexical Specialization
</note>
<equation confidence="0.5324965">
E Pr(Xi = 1) + 0 • Pr(Xi = 0)
Emm prino. )M-m (24)
</equation>
<bodyText confidence="0.998939666666667">
where we assume that the frequencies f(i,M) are independently and identically bi-
nomially (M, p1) distributed. The expected overall number of different types in the
sample, irrespective of their frequency, follows immediately:
</bodyText>
<equation confidence="0.987899">
E[V(M)] = E[ V(M, m)]
m&gt;i
PI)m-m
E(1 _ _ (25)
</equation>
<bodyText confidence="0.997443">
For large M and small p, binomial probabilities can be approximated by Poisson prob-
abilities, leading to the simplified expressions
</bodyText>
<equation confidence="0.483911">
, (26)
E[V(M, in)] = ()M)m e
E[V(M)] = E(1 _ e-A,A4).
</equation>
<bodyText confidence="0.9763145">
Conditional on a given frequency spectrum {V(N,f),f = 1, 2, ...}, the vocabulary size
E[V(M)] for sample size M &lt; N equals
</bodyText>
<equation confidence="0.991231">
V(N)
E[V(M)] =E (1
i=1
V(N)
E(1 _ e-f*A4)
=- V(N) - E V(N,f)e- (27)
f=1
</equation>
<bodyText confidence="0.999818636363636">
In the last step, all V(N,f) types sharing the same frequency f have been grouped
together. Note that when the N tokens themselves constitute a sample from a larger
population, E[V(M)] is in fact an estimate.
The derivation of (27) uses an urn model in which words are sampled with re-
placement. A model in which words are sampled without replacement is more precise.
For instance, for a randomly reordered text, the likelihood that a hapax-legomenon
in the full text that appears in the first M tokens will reappear among the remaining
N — M tokens is greater than zero in a model that assumes constant probabilities, con-
trary to fact. For large N and M, however, the binomial probabilities (sampling with
replacement) are a good approximation of the hypergeometric probabilities (sampling
without replacement).
</bodyText>
<page confidence="0.992248">
477
</page>
<note confidence="0.688354">
Computational Linguistics Volume 22, Number 4
</note>
<bodyText confidence="0.99468475">
Finally note that (27) suggests that, under randomness, and conditional on the
words appearing in the sample of N tokens, f(i,M) can alternatively be viewed as a
binomially distributed random variable with parameters M/N and f (i,N) for f (i,N) &lt;
M,N (Muller 1977):
</bodyText>
<equation confidence="0.999543666666667">
E[V(M)] = V - E V(N,f)e-114f
V - M (28)
V(N,f) (1 - .
</equation>
<bodyText confidence="0.952793916666667">
The modification of (28) proposed by Hubert and Labbe (1988) requires the as-
sumption that all the tokens of a word type with specialized use occur in a single
text slice. Let the total number of words in the set S of types with specialized use be
pV, and also assume that the text slices in which these specialized words appear are
randomly distributed over the text. Let
if wi E S and wi occurs in Pi (29)
otherwise,
and let
y = 01 if w, S and w occurs in Pi
otherwise.
The overall number of types in Pi is E, x, + &gt; Y,. If w, E S, its&apos;&apos;, tokens (f, &lt; M) will
all appear in the same part of the text. The probability that they will appear in Pi is
</bodyText>
<equation confidence="0.932363071428572">
M Hence
N •
(30)
EHL[V(M)]
Yj
Pr(X, = 1) +
wiEs
E + E (1
co,ES wlfz&apos;S
pV —N + (1- p)V
Pr(Yi = 1)
pV + (1- p)V - E(1_ p)V(N,f) (1 -
f
pV + (1- p)V - E(1_ p)V(N,f)e-W
</equation>
<bodyText confidence="0.9336695">
Note the implicit assumption that the same proportion of the V(N,n word types with
frequency f is specialized, irrespective of the value off.
</bodyText>
<figure confidence="0.9604975">
My.
N)
(31)
478
Baayen Lexical Specialization
List of Symbols
di
di,k
D(k)
DU(k)
V(k)
VU(k)
E[X]
EHL[V (M)]
ES[V(N,f)]
f(i,m)
M(f)
mGT(f,m)
ms(f,m)
mh(f,m)
ni,k
nUi,k
NU(k)
Pf
pi
(M)
</figure>
<equation confidence="0.957292714285714">
Pr( U, token, k)
Pr(U, type, k)
P(N)
V(N)
V (N,f)
V(Mk)
VU(k)
</equation>
<bodyText confidence="0.999380432432432">
dispersion of word type i
indicator variable for underdispersion of type i in chunk k
progressive difference score for text slice k
progressive difference score of underdispersed words at chunk k
number of new types at k
number of new underdispersed types at k
expectation of X
expectation of V(M) in the Hubert-Labbe model
expectation of V(N,f) given Sichel&apos;s (1986) model
token frequency of a word
token frequency of i-th word type in sample of size M
token frequency of i-th word in the k-th text slice
index for word types 1, , V
index for text slices 1, ,K
number of text slices
token frequency of a word in sample of size M
sample size in tokens when contrasting two sample sizes (M &lt; N)
population probability mass of frequency class f
Good-Turing sample estimate of Mp(f) in sample of size M
sample estimate of Mp (f)
heuristic estimate of Mp(f) (mean of K(M,f) and MGT(M,f ))
indicator variable for type i appearing first in chunk k
indicator variable for type i appearing first in chunk k and i being
underdispersed in k
number of word tokens in the sample
number of underdispersed tokens in chunk k
Hubert-Labbe coefficient of vocabulary partition
sample probability (f/N)
probability of wi
Good-Turing adjusted probability for sample of size M
proportion of new underdispersed tokens at k
proportion of new underdispersed types at k
growth rate of the vocabulary (E[V(N, 1)]/N)
number of different word types among N tokens
number of types with frequency f in a sample of N tokens
number of types in the first tokens
number of underdispersed types in chunk k
</bodyText>
<sectionHeader confidence="0.996097" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.614142">
I am indebted to Ken Church, Anne Cutler,
James McQueen, Rob Schreuder, Richard
Sproat, Fiona Tweedie, and five anonymous
reviewers for valuable comments and
discussion.
</bodyText>
<sectionHeader confidence="0.855383" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999176153846154">
Baayen, R. Harald. 1989. A Corpus-based
Approach to Morphological Productivity.
Statistical Analysis and Psycholinguistic
Interpretation. Ph.D. thesis, Free
University, Amsterdam.
Baayen, R. Harald. 1992. Quantitative
aspects of morphological productivity. In
G. E. Booij and J. van Marle, editors,
Yearbook of Morphology 1991. Kluwer
Academic Publishers, Dordrecht, pages
109-149.
Baayen, R. Harald. 1996. The randomness
assumption in word frequency statistics.
</reference>
<page confidence="0.977948">
479
</page>
<note confidence="0.350814">
Computational Linguistics Volume 22, Number 4
</note>
<reference confidence="0.999811466666667">
In G. Perissinotto, editor, Research in
Humanities Computing 5. Oxford
University Press, Oxford.
Baayen, R. Harald and Antoinette Renouf.
1996. Chronicling the Times: Productive
lexical innovations in an English
newspaper. Language, 72:69-96.
Baayen, R. Harald and Richard Sproat. 1996.
Estimating lexical priors for
low-frequency morphologically
ambiguous forms. Computational
Linguistics, 22(2):155-166.
Bod, Rens. 1995. Enriching Linguistics with
Statistics: Performance Models of Natural
Language. University of Amsterdam:
Institute for logic, language and
computation, Amsterdam.
Brunet, Etienne. 1978. Le vocabulaire de Jean
Giraudoux, volume 1 of TLQ. Slatkine,
Geneve.
Chitashvili, Revas J and R. Harald Baayen.
1993. Word frequency distributions. In
G. Altman and L. Hfebleek, editors,
Quantitative Text Analysis.
Wissenschaftlicher Verlag Trier, Trier,
pages 54-135.
Church, Kenneth and William Gale. 1991. A
comparison of the enhanced Good-Turing
and deleted estimation methods for
estimating probabilities of English
bigrams. Computer Speech and Language,
5:19-54.
Church, Kenneth and William Gale. 1995.
Poisson mixtures. Journal of Natural
Language Engineering, 1(2):163-190.
Cleveland, W. S. 1979. Robust locally
weighted regression and smoothing
scatterplots. Journal of the American
Statistical Association, 74:829-836.
Good, I. J. 1953. The population frequencies
of species and the estimation of
population parameters. Biometrika,
40:237-264.
Good, I. J and G. H. Toulmin. 1956. The
number of new species and the increase
in population coverage, when a sample is
increased. Biometrika, 43:45-63.
Herdan, Gustay. 1960. Type-Token
Mathematics. Mouton, The Hague.
Holmes, David. I. 1994. Authorship
attribution. Computers and the Humanities,
28(2):87-106.
Hubert, Pierre and Dominique Labbe. 1988.
A model of vocabulary partition. Literary
and Linguistic Computing, 3:223-225.
Indefrey, Peter and R. Harald Baayen. 1994,
Estimating word frequencies from
dispersion data. Statistica Neerlandica,
48:259-270.
Johnson, Norman L. and Samuel Kotz. 1977.
Urn Models and Their Application. An
Approach to Modern Discrete Probability
Theory. John Wiley &amp; Sons, New York.
Kalinin, V. M. 1965. Functionals related to
the Poisson distribution and statistical
structure of a text. In J. V. Finnik, editor,
Articles on Mathematical Statistics and the
Theory of Probability, pages 202-220,
Providence, Rhode Island. Steklov
Institute of Mathematics 79, American
Mathematical Society.
Khmaladze, Estate V and Revas J.
Chitashvili. 1989. Statistical analysis of
large number of rare events and related
problems. Transactions of the Tbilisi
Mathematical Institute, 91:196-245.
Muller, Charles. 1977. Principes et methodes de
statistique lexicale. Hachette, Paris.
Muller, Charles. 1979. Langue francaise et
linguistique quantitative. Slatkine, Geneve.
Sichel, H. S. 1986. Word frequency
distributions and type-token
characteristics. Mathematical Scientist,
11:45-72.
Tukey, John W. 1977. Exploratory Data
Analysis. Addison-Wesley, Reading, Mass.
Uit den Boogaart, Pieter C., editor. 1975.
Woordfrequenties in Gesproken en Geschreven
Nederlands. Oosthoek, Scheltema &amp;
Holkema, Utrecht.
</reference>
<page confidence="0.998228">
480
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9941555">The Effects of Lexical Specialization on the Growth Curve of the Vocabulary</title>
<author confidence="0.951084">R Harald Baayen Max Planck Institute for</author>
<abstract confidence="0.984516393939394">Psycholinguistics The number of different words expected on the basis of the urn model to appear in, for example, the first half of a text, is known to overestimate the observed number of different words. This paper examines the source of this overestimation bias. It is shown that this bias does not arise due to sentence-bound syntactic constraints, but that it is a direct consequence of topic cohesion in discourse. The nonrandom, clustered appearance of lexically specialized words, often the key words of the text, explains the main trends in the overestimation bias both quantitatively and qualitatively. The effects of nonrandomness are so strong that they introduce an overestimation bias in distributions of units derived from words, such as syllables and digrams. Nonrandom word usage also affects the accuracy of the Good-Turing frequency estimates which, for the lowest frequencies, reveal a strong underestimation bias. A heuristic adjusted frequency estimate is proposed that, at least for novel-sized texts, is considerably more accurate. When reading through a text, word token by word token, the number of different word types encountered increases, quickly at first, and ever more slowly as one progresses the text. The number of different word types encountered after reading the vocabulary size a function of expressions for based on the urn model are available. A classic problem in word frequency studies is, however, that these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. the theoretical or expected vocabulary size (N)] is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of productivity of affixes, Baayen and Sproat (to appear) apply this Good-Turing estimate to obtain enhanced estimates of lexical priors for unseen words, and the Good-Turing estimates also play an important role for estimating population probabilities (Church and Gale 1991). If a simple random variable such as the vocabulary size reveals consistent and significant deviation from its expectation, the accuracy of the Good-Turing estimates is also called question. The aim of this paper is to understand why this deviation between the-</abstract>
<note confidence="0.552220333333333">Wundtlaan 1, 6525 XD Nijmegen, The Netherlands. E-mail: baayen@mpi.n1 © 1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 4</note>
<abstract confidence="0.974977044444444">ory and observation arises in word frequency distributions, and in this light evaluate applications of the Good-Turing results. The remainder of this paper is structured as follows. In Section 2, I introduce some basic notation and the expressions for the growth curve of the vocabulary with which we will be concerned throughout, including a model proposed by Hubert and Labbe (1988), which, by introducing a smoothing parameter, leads to much-improved fits. Unfortunately, this model is based on a series of unrealistic simplifications, and cannot serve as an explanation for the divergence between the observed and expected vocabulary size. In Section 3, therefore, I consider a number of possible sources for the misfit in greater detail: nonrandomness at the sentence level due to syntactic structure, nonrandomness due to the discourse structure of the text as a whole, and noru-andomness due to thematic cohesion in restricted sequences of sentences (paragraphs). Section 4 traces the implications of the results obtained for distributions of units derived from words, such as syllables and digrams, and examines the accuracy of the Good-Turing frequency estimates. A list of symbols is provided at the end of the paper. 2. The Growth Curve of the Vocabulary the size of a text in word tokens, and let V denote the total number of different types observed among the tokens. Roughly half of the word types occur once, the so-called legomena, occur with higher Let denote the number of once-occurring types among and, similarly, the number of types occurring after sampling The number of different types E[V(M)] for M &lt; on the = 2, 3, ... can be estimated by E[V(M)] = V — M f (1 — . (1) A proof for (1) is presented in the appendix. 1 illustrates the problems that arise when (1) is applied to three texts, Wonderland, Lewis Carroll (upper panels), Dick Herman Melville (middle and Havelaar Multatuli (the pseudonym of Eduard Douwes Dekker, All panels show the sample size the horizontal axis. Thus the horizontal axis can be viewed as displaying the &amp;quot;text time&amp;quot; measured in word tokens. The vertical axis of the left-hand panels shows the number of observed word types (dotted line) and the number of types predicted by the model (solid line) obtained using (1). These panels reveal that the expected vocabulary size overestimates the observed vocabulary size for almost all of the 40 equidistant measurement points. To the eye, the overestimation seems fairly small. Nevertheless, in absolute terms the expectation may be several hundreds of types too high, and may run up to 5% of the total vocabulary size. The type definition I have used throughout is based on the orthographic word form: counted as two different types, two tokens of the same type. No lemmatization has been attempted, first, because the probabilistic aspects of the problem considered here are not affected by whether or not lemmatization is carried out, and second, because it is of interest to ascertain how much information can be extracted from texts with minimal preprocessing. 2 These texts were obtained by anonymous ftp from the Project Gutenberg at obi.std.com. The header of electronic version of Dick mention of E.F. Tray at the University of Colorado,</abstract>
<address confidence="0.4107725">Boulder, who prepared the text on the basis of the Hendricks House Edition. 456</address>
<title confidence="0.978121">Baayen Lexical Specialization</title>
<author confidence="0.599726">Lewis Carroll Alice in Wonderland</author>
<affiliation confidence="0.3112"></affiliation>
<address confidence="0.477731">5000 10000 15000 20000 25000 5000 10000 15000 20000 25000</address>
<author confidence="0.9465695">Herman Melville Moby Dick</author>
<phone confidence="0.658183">200000 50000 200000</phone>
<title confidence="0.651405">Multatuli</title>
<author confidence="0.995234">Max Havelaar</author>
<address confidence="0.861338">20000 40000 60000 80000 100000</address>
<phone confidence="0.686932">20000 40000 60000 80000 100000</phone>
<abstract confidence="0.994103609375">Figure 1 growth curve of the vocabulary. Observed vocabulary size lines) and vocabulary size lines) for three novels (left-hand panels) and the overstimation errors — V(N) lines) and their sentence-randomized versions (&amp;quot;+&amp;quot;-lines, see Section 3.1) (right-hand panels). right-hand panels of Figure 1 show the overestimation error functions V(N) to the left-hand panels using dotted lines. For the first 20 meapoints, the instances for which E[V(N)] diverges significantly from shown in Clearly, the divergence is significant for almost all of the first 20 points. This suggests informally that the discrepancy between significant over a wide range of sample sizes. 2.1 The Model Proposed by Hubert and Labbe problem of the systematic estimation error of been pointed out by Muller (1979) and Brunet (1978), who hypothesize that lexical specialization is at issue. In any text, there are words the use of which is mainly or even exclusively restricted to a given subsection of that text. Such locally concentrated clusters of words are at odds with the randomness assumption underlying the derivation of (1), and may be the cause of the divergence illustrated in Figure 1. Following this line of reasoning, Hubert and Labbe (1988) propose a model according to which (1) should be modified Since the for an estimate of the variance of in the Z-scores used here requires knowledge of E[V(2N)], the significance of the divergence for the second 20 measurement points is not available. For technical details, see Chitashvili and Baayen (1993). 457 Computational Linguistics Volume 22, Number 4 as follows (see the appendix for further details): = (1 – p)V – (1 – p) and Labbe&apos;s model contains one free parameter, the of vocabupartition of the proportion of specialized words in the vocabulary. text sizes for which the observed and expected vocabulary sizes are be estimated by minimizing the mean squared error (MSE) – or the chi-square statistic E_ E[17040])2 k=1 ignoring that the variance of with M; see Chitashvili Baayen [19931). For in Wonderland, of (4) for = leads to = and according to this rough estimate of goodness-of-fit the revised model the data very well indeed = p&gt; 0.5). For Dick, the chisquared statistic suggests a significant difference between the observed and expected sizes = 172.93,p &lt; 0.001), even though the value of the parameter (0.12) leads to a fit that is much improved with respect to the unadjusted growth 730.47). Closer inspection of the error pattern of the adjusted estimate ( reveals the source of the misfit: for the first 12 measurement points, the observed vocabulary size is consistently overestimated. From the 14th observation onwards, the Hubert-Labbe model consistently underestimates the real vocabulary size. Apparthe development of the vocabulary in Dick be modeled globally, but local fluctuations introducing additional points of inflection into the growth curve are outside its scope—a more detailed study of the development of lexical specialization in the narrative is required if the appearance of these points of inflection are to be understood. In spite of this deficiency, the Hubert-Labbe curve appears to be an optimal smoother, and this suggests that the value obtained for the coefficient of vocabulary a fairly reliable estimate of the extent to which a text is characterized by lexical specialization. In this light, the evaluation by Holmes (1994), who suggests that be a useful discriminant for authorship attribution studies, is understandable. Unfortunately, the assumptions underlying (2) are overly simplistic, and seriously call question the reliability of as a of lexical specialization, and the same holds for the explanatory value of this model for the inaccuracy of E[V(N)]. 2.2 Problems with the Hubert and Labbe Model One highly questionable simplification underlying the derivation of (2) spelled out in the appendix is that specialized words are assumed to occur in a single text slice only. Figure 2, which plots the number of times in 40 successive, sized text slices that jointly constitute the full text of Dick. dotted line reveals the main developmental pattern (time-series smoothing using running Even though Ahab is one of the main characters in Dick, even though his name certainly belongs to the specialized vocabulary of the novel, Ahab is 458 Baayen Lexical Specialization 30 text slice Figure 2 word usage illustrated for in Moby Dick. horizontal axis plots the 40 sized text slices, the vertical axis the frequency of these text slices. The dotted line represents a time-series smoother using running medians (Tukey 1977). not mentioned by name in one text slice only, as the Hubert-Labbe model would have. What we find is that he is not mentioned at all in the first five text slices. Following this we observe a series of text slices in which he appears frequently. These are in turn succeeded by slices in which Ahab is hardly mentioned, but he reappears in the last of the book, and as the book draws to its dramatic close, the frequency of increases to its maximum. This is an illustration of what Indefrey and Baayen (1994) to as cohesion: word specialized use, but it occurs in a series of subtexts within the novel as a whole, contributing to its overall cohesion. slices where frequently mentioned, the cohesion be strengthened. For instance, to be a specialized word in text slice 23, but he is mentioned only in passing in text slice 25. His appearance in the two text slices strengthens the intertextual cohesion of the whole novel, but it is only the intra-textual cohesion of slice 23 that is raised. The presence of inter-textual cohesion in addition to intra-textual cohesion and the concomitant phenomenon of global lexical suggest that in order to understand the discrepancy between its expectation, a more fine-grained approach is required. A second question concerns how lexical specialization affects the empirical growth of the vocabulary. Inspection of plots such as those presented in Figure 1 for Alin Wonderland that the effects of lexical specialization appear in the central sections of the text, as it is there that the largest differences between the expected and the observed vocabulary are to be observed—differences that are highly penalized by the MSE and chi-squared techniques used to estimate the proportion of specialized words in the vocabulary. Unfortunately, the central sections are not necessarily the ones characterized by the highest degree of lexical specialization. To see this, consider Figure 3, which plots the difference between the expected number of new types using 459 scores for the influx of new types in in Wonderland. = ... ,40 text slices displayed on the horizontal axis, the progressive difference scores shown on the vertical axis. The dashed line represents a nonparametric scatterplot smoother (Cleveland 1979), the dotted line a least squares regression line (the negative slope is significant, F(1,38) -= 11.07,p &lt; .002). and the observed number of new types for the successive text slices of in Wonfor each text slice k ,40, we calculate the error scores k = . . 40: {E[V(Mk)] - E[V(Mk_i)]1 - {V(Mk) - 1/(Mk-i)}• Note that in addition to positive difference scores, which should be present given that &gt; most, or, as in in Wonderland, all values of also have negative difference scores. Text slices containing more types than expected under conditions are necessarily present given the existence of text slices which - 0: the total number of types accumulated over the 40 text slices has sum up to 3 shows that the expected numbers of new word types are overestimated for the initial part of the novel, that the theoretical estimates are fairly reliable for the middle section of the novel, while the final chapters show a slightly greater increase in the number of new types than expected under chance conditions. specialization affects the influx of new types, its effects appear not in the central sections of the novel as suggested by Figure 1, but rather in the beginning and perhaps at the end. This finding seriously questions the appropriateness of using the growth curve of the vocabulary for deriving a measure of lexical specialization. A third question arises with respect to how one&apos;s measure of lexical concentration affected by the number of text slices Hubert and Labbe&apos;s model, the optimal of the is independent of the number of text slices not-too- (K&gt; Since the expected growth curve and the observed growth curve completely fixed and independent of is fully determined by the fre-</abstract>
<note confidence="0.975097">Volume 22, Number 4 Computational Linguistics 0 cJ 0 10 30 20 40 • 0 C■1 2 0 460</note>
<title confidence="0.755352">Baayen Lexical Specialization</title>
<abstract confidence="0.989768028169014">quency spectrum of the complete text, the latter is determined by the text itself—the of only the number of points at which the divergence between the two curves is measured. Increasing the number of measurement points increases degrees of freedom along with the deviance, and the optimal value of the parameter remains virtually unchanged. But is this a desirable property for a measure of lexical specialization? Even without taking the effects of inter-textual cohesion into account, and concentrating solely on local specialization and intra-textual cohesion, formulating lexical specialization in terms of concentration at a particular point in the text is unrealistic: it is absurd to assume that all tokens of a specialized word appear in one chunk without any other intervening words. A more realistic definition of (local) lexical specialization is the concentration of the tokens of a given word within a particular text slice. In such an approach, however, the size of the text slice is of crucial importance. A word appearing only in the first half of a book enjoys some specialized use, but to a far lesser extent than a word with the same frequency that occurs in the first half of the first chapter only In other words, an approach to lexical specialization in terms of concentration of use is incomplete without a specification of the unit of concentration itself. 3. Sources of Nonrandomness To avoid these problems, I will now sketch a somewhat more fine-grained approach to why its expectation diverge, adopting Hubert and Labbe&apos;s central insight that lexical specialization can be modeled in terms of local concentration. Consider again the potential sources for violation of the randomness assumption the derivation of least three possibilities suggest themselves: syntactic constraints on word usage within sentences, global discourse organization, and local repetition. I will consider these possibilities in turn. 3.1 Syntactic Constraints Syntactic constraints at the level of the sentence introduce many restrictions on the occurrence of words. For instance, in normal written English, following the determiner appearance of a second instance of the same determiner (as in this sentence), is extremely unlikely. According to the urn model, however, such a sequence is likely to once every 278 words (the relative frequency of English is approximately 0.06), say once every two pages. This is not what we normally find. Clearly, syntax imposes severe constraints on the occurrence of words. Does this imply that the urn model is wrong? For individual sentences, the answer is undoubtedly yes. But for more global textual properties such as vocabulary size, a motivated answer is less easy to give. According to Herdan (1960, 40), reacting to Halle&apos;s criticism of the urn model as a realistic model for language, there is no problem, since statistics is concerned with not Whatever the force of this argument may be, Figure 1 demonstrates clearly that the urn model lacks precision for our data. In order to ascertain the potential relevance of syntactic constraints referred to by Halle, we may proceed as follows: If sentence-level syntax underlies the misfit between the observed and the expected vocabulary size, then this misfit should remain visible for randomized versions of the text in which the sentences have been left unchanged, but in which the order of the sentences has been permuted. If the misfit disappears, M. Halle, &amp;quot;In defence of the number two,&amp;quot; in Presented to J. Whatmough, Hague, 1957, quoted in Herdan, 1960, page 40. 461 Computational Linguistics Volume 22, Number 4 we know that constraints the domain of which are restricted to the sentence can be ruled out. results of this randomization test applied to in Wonderland, Moby Dick, Havelaar shown in the right-hand panels of Figure 1 by means of &amp;quot;+&amp;quot; symbols. What we find is that following sentence randomization, all traces of a significant divergence between the observed and expected vocabulary size disappear. differences between substantially reduced and may remain negative, as in in Wonderland, slightly positive, as for Dick, may fluctuate around zero in an unpredictable way, as in Havelaar. we are left with variation that is probably to be attributed to the particularities of the individual randomization orders, we may conclude that at the global level of the text as an (unordered) aggregate of sentences, the randomness assumption remains reasonable. The nonrandomness at the level of sentence structure does not influence the expected vocabulary size. As a global text characteristic, it is probably insensitive to the strictly local constraints imposed by syntax. Apparently, it is the sequential order in which sentences actually appear that crucially determines the bias of our theoretical estimates. There are at least two domains where this sequential order might be relevant: the global domain of the discourse structure of the text as a whole, and the more local domain of relatively small sequences of sentences sharing a particular topic. To explore these two potential explanatory domains in detail, we need a method for linking topical discourse structure and local topic continuity with word usage. Lexical specialization, informally defined as topic-linked concentrated word usage, and formalized in terms of underdispersion, provides us with the required tool. 3.2 Lexical Specialization that the word unevenly distributed in Dick. high frequency (510), one would expect it to occur in all 40 text slices, but it does not. In fact, are 11 text slices where Ahab is not mentioned at all. Technically speaking, there are many such words, and if these underdispersed words cluster together, the resulting deviations from randomness may be substantial enough to become visible as a divergence between the observed and theoretical growth curves of the vocabulary. In order to explore this intuition, we need a reliable way to ascertain whether a is underdispersed. Let the dispersion a word co, be the number of different slices in which co, appears. Analytical expressions for E[c/,1 and are available (Johnson and Kotz 1977, 113-114), so that in principle Z-scores can be calculated. These Z-scores can then be used to ascertain which words are significantly underdispersed in that they occur in significantly too few text slices given the urn model (cf. Baayen, 1996). Unfortunately, dispersions deviate substantially from normality, so that Z-scores remain somewhat impressionistic. I have therefore used a randomization test to ascertain which words are significantly underdispersed. The randomization test proceeded as follows: The sequence of words of a text was randomized 1,000 times. For each permutation, the dispersion of each word type in that particular permutation was obtained. For each word, we calculated the proportion of permutations for which the dispersion was lower than or equal to the empirical For 1,000 permutations revealed full dispersion = which that the probability that the low empirical dispersion of (d = due chance is (much) less than The content words singled out as being signifi- 5 I am indebted to an anonymous referee for pointing out to me that Z-scores are imprecise. I am 462 Baayen Lexical Specialization cantly underdispersed at the 1% level (the significance level I will use throughout this for determining underdispersion) reveal a strong tendency to be words. for Dick, ten most frequent underdispersed content words are captain, said, white, Stubb, whales, men, sperm, five most frequent function words are ye, such, her, The number of chunks in which an underdispersed word appears, and the frequencies with which such a word appears in the various chunks, cannot be predicted on the basis of the urn model. (Instead of the binomial or Poisson models, the negative binomial has been found to be a good model for such words, see, e.g., Church and Gale [1995]). Before studying how these words appear in texts and how they affect the growth curve of the vocabulary, it is useful to further refine our definition of underdispersion. again the distribution of the word Figure 2. In text slice 25, only once. Although this single occurrence contributes to the inter-textual cohesion of the novel as a whole, it can hardly be said to be a key word within text slice 25. In order to eliminate such spurious instances of key words, it is useful to set a frequency threshold. The threshold used here is that the frequency of the word in a given text slice should be at least equal to the mean frequency of the word calculated the text slices in which the word appears. More formally, let the frequency of i-th word type in the k-th text slice, and define the indicator variable follows: = 1 iff k wi underdispersed d, — (6) 0 otherwise. number of underdispersed types in text slice VU(k), the corresponding of underdispersed tokens, now be defined as = = Specialization and Discourse Structure We are now in a position to investigate where underdispersed words appear and how they influence the observed growth curve of the vocabulary. First consider Fig- 4, which summarizes a number of diagnostic functions for in Wonderland. panels plot and the numbers of underdispersed types and tokens appearing in the successive text chunks. Over sampling time, we observe a slight increase in both the numbers of tokens and the numbers of types. Both trends are significant according to least squares regressions, represented by dotlines (F(1,38) = 6.591,p &lt; .02 for = 16.58,p &lt; .001 for time-series smoother using running medians (Tukey 1977), represented by solid lines, similarly indebted to Fiona Tweedie, who suggested the use of the randomization test. Comparison of the results based on Z-scores (see Baayen, to appear) and the results based on the randomization test, however, reveal only minor differences that leave the main patterns in the data unaffected. 6 The present method of finding underdispersed words appears to be fairly robust with respect to the of text slices different numbers of text chunks, virtually the same high-frequency words appear to be underdispersed. The number of text chunks exploited in this paper, 40, has been chosen to allow patterns in &amp;quot;sampling time&amp;quot; to become visible without leading to overly small text slices for the smaller texts.</abstract>
<note confidence="0.8037586">463 Computational Linguistics Volume 22, Number 4 Alice in Wonderland 0 -SC CV &gt;0 • .... .. t .... ........ ... • • ••</note>
<phone confidence="0.604211">0 10 20 30 40 10 20 30 40</phone>
<email confidence="0.25062">Series:VUSeries:NU</email>
<phone confidence="0.728109">5 10 15</phone>
<email confidence="0.351155">Lag</email>
<phone confidence="0.562733">30 40</phone>
<abstract confidence="0.885544011494253">u_ cc! o 10 Lag 4cg 10 20 30 40 9 0 10 20 2 a 0 10 40 30 20 functions for in Wonderland. VU(k) of underdispersed and tokens in text slice auto-correlation function; Pr(U, type) and Pr(U, token): of underdispersed types and tokens; difference scores for the overall vocabulary and the underdispersed words. a slightly oscillating pattern. At least for a time lag of 1, this finds some support in the autocorrelation functions, shown in the second line of panels of Figure 4. key words are not uniformly distributed in in Wonderland. only does use key words in one text slice appear to influence the intensity with which key words are used in the immediately neighboring text slices, but as the novel proceeds key words appear with increasing frequency. How does this nonrandom organization of key words in the discourse as a whole answer this question, it is convenient to investigate the nature of the new types that arrive with the successive text slices. Let V(Mk-i) the number of new types observed in text slice let the number of new underdispersed types for text slice proportion of new types in text slice the total number of new types, Pr(U, type, is given by type, = AV(k) • plot of Pr(U, types, shown on the third row of Figure 4 (left-hand panel). According to a least squares regression (dotted line), there is a significant increase in 464 Baayen Lexical Specialization proportion of underdispersed new types as (F(1, 38) = 5.804, &lt; The right-hand side counterpart shows a similar trend for the word tokens that is also by a least squares regression (F(1, 38) = 5.681, &lt; Here, the proportion of new underdispersed tokens on the total number of new tokens is defined as with and k-1 f f n otherwise, F.k iff,m and 1 0 otherwise. The increase in the proportions of new underdispersed types and tokens shows that the pattern observed for the absolute numbers of types and tokens observed in the top panels of Figure 4 persists with respect to the new types and tokens. We can now test to what extent the underdispersed types are responsible for the of its expectation by comparing the progressive difference in (5) with the progressive difference scores for the subset of the words as =-- E[VU(k)] – E[VU(k – – The two progressive difference score functions are shown in the bottom left panel of 4, and the – DU(k) plotted in the bottom right-hand panel. The residuals do not reveal any significant trend (F(1, 38) &lt; 1), which suggests that the underdispersed vocabulary is indeed responsible for the main trend in the progressive scores the vocabulary and hence for the divergence between the next section, I will argue that intra-textual cohesion is in large part for the general downward curvature of what follows, I will first an attempt to understand the differences in the error scores shown in Figure 1 as a function of differences in the use of key words at the discourse level. in Wonderland, words are relatively rare in the initial text slices. As a result, these text slices reveal fewer types than expected under chance conditions. smaller than increasing shown in the upright panel of Figure 1, the divergence between its expectation first increases—the initial text slices contain the lowest numbers of underdispersed types and tokens—and then decreases as more and more underdispersed words appear. the semi-circular shape of the error scores – V(N) in Figure 1 is direct consequence of the topical structure at discourse level of in Wonderland. error scores – V(N) Dick Havelaar in Figure 1 reveal a different developmental profile. In these novels, the maximal divergence appears early on in the text, after which the divergence decreases until, just the end, even slightly larger than its expectation. Is it possible understand this qualitatively pattern in terms of the discourse structure these novels? First, consider Dick. series of diagnostic plots is shown in 5. The numbers of underdispersed types and tokens variation, but unlike in in Wonderland, is only a nonsignificant trend</abstract>
<note confidence="0.696040125">465 Computational Linguistics Volume 22, Number 4 Moby Dick 10 20 30 40 40</note>
<phone confidence="0.569787">10 20 30</phone>
<note confidence="0.870765117647059">I [Ahab] 0 _Nc 9 0 -Y 0 0 10 20 30 40 03 0 CD 0 Z•ci c‘i 0 _0</note>
<abstract confidence="0.894715">C o •—• 0 ---- 0 _Y 0 • . • • • • • • • • • • • 10 20 30 40 • • • . • • • • • • • •• 0 10 20 30 40 • • . • • .</abstract>
<phone confidence="0.6210655">10 20 30 40 0 10 20 30 40</phone>
<abstract confidence="0.978177176470588">f • • . . • k Figure 5 functions for Dick. VU(k) of underdispersed types and in text slice type) and Pr(U, token): proportions of underdispersed types and difference scores for the overall vocabulary and the words; f[Ahab](k): frequency of text slice = 2.11, .15 for = 1.98,p &gt; .15 for underdispersion to occur more often as the novel progresses. The absence of a trend is supported by the proportions of underdispersed types and tokens, shown in the second row of &lt; for both types and tokens). In the last text slices, underdispersed words are even underrepresented. The bottom panels show that the progressive difference the underdispersed words capture the main trend in the progressive scores of the total vocabulary well: The residuals — DU(k) not reveal a significant trend (F(1, 38) = 1.08, the use of underdispersed words in Dick to some extent with the frequency of the word respect to both types and tokens = 4.61,p &lt; = .11 for = 10.77,p &lt; = for panels on the third row of Figure 5 show the frequencies of a function of the frequency of A nonparametric time series smoother (solid line) supports the least squares regression line (dotted line). In other the key figure of Dick a somewhat more intensive use of the key words of the novel. nonuniform distribution of some light on the details of the shape the difference function — V(N) in Figure 1. The initial sections do mention Ahab, it is here that its highest values, and here too we find largest discrepancies between text slice 20, Ahab has been firmly established as a principal character in the novel, and the main key words have 466</abstract>
<title confidence="0.990741">Baayen Lexical Specialization</title>
<author confidence="0.998891">Max Havelaar</author>
<affiliation confidence="0.466102">apos;2 D • • • • Lr) • * • • .&amp;quot;</affiliation>
<phone confidence="0.600832">0 10 20 30 40 10 20 30 40</phone>
<note confidence="0.8841635">VU Series: CO • 0 &lt; 0 5 10 15 Lag LL 0 0 •:/* 0 0 5 10 20liii Lag I &amp;quot; 15 CD 0 D 0 0 o 0 a 7 , 0 . . • • • • .. • Nheft</note>
<phone confidence="0.653492">10 20 30 40 0 10 20 30 40</phone>
<abstract confidence="0.993166057142857">o 10 20 30 40 0 10 20 30 40 Figure 6 functions for Havelaar. VU(k) of underdispersed types tokens in text slice auto-correlation function; Pr(U, type) and Pr(U, token): of underdispersed types and tokens; difference scores for the overall vocabulary and the underdispersed words. appeared. The overestimation of the vocabulary is substantially reduced. As the novel to its dramatic end, the frequency of to its maximum. The plots on the first row of Figure 5 suggest that underdispersed types and tokens are also more intensively in the last text slices. proportions plots on the second row show a final dip, suggesting that at the very end of the novel, a more than average number of normally dispersed new types appears. Considered together, this may explain why at the very end of the novel the expected vocabulary slightly underestimates the observed vocabulary size, as shown in Figure 1. consider the diagnostic plots for Havelaar, in Figure 6. The time smoother (solid line) for the absolute numbers of underdispersed types tokens an oscillating use of key words without any increase in the use of key words over time (the dotted lines represent the least squares regression lines, of which are significant: &lt; 1 both cases). This oscillatory structure receives some support from the autocorrelation functions shown in the second row of panels. Especially in the token analysis, there is some evidence for positive autocorrelation at lag 1, and for a negative polarity at time lags 8 and 9. No trend emerges from proportions of new underdispersed types and tokens (third row, &lt; 1 both A comparison of the progressive difference scores row) shows that the underdispersed words are again largely responsible for the large of small significant trend remains in the residuals = 1.848, 467 Computational Linguistics Volume 22, Number 4 1 revealed that E[V(N)] — largest around text slices 3 to 7, but becomes negative for roughly the last third of the novel. This pattern may be due the oscillating use of key words in Havelaar. there is a fair number of key words in the first few text chunks, the intensity of key words drops quickly, only to rise again around chunk 20. Thus, key words are slightly underrepresented in the first part of the novel, allowing the largest divergence between the expected and observed vocabulary size to emerge there. 3.4 The Paragraph as the Domain of Topic Continuity The preceding analyses all revealed violations of the randomness assumption underlying the urn model that originate in the topical structure of the narrative as a whole. I have argued that a detailed analysis of the distribution of key word tokens and types may shed some light on why the theoretical vocabulary size sometimes overestimates and sometimes underestimates the observed vocabulary size. We are left with the question of to what extent repeated use of words within relatively short seof sentences, henceforth for ease of reference the accuracy carried out two additional analyses, one using five issues of the Dutch newspaper Trouw, and one using the random samples of the Dutch newspaper De Telegraaf available in the Uit den Boogaart (1975) corpus. For both texts, no overall topical discourse structure is at issue, so that we can obtain a better view of the effects of intra-textual cohesion by itself. For each newspaper, the available texts were brought together in one large corpus, preserving chronological order. Each corpus was divided into 40 equally large text slices. The upper left panel of Figure 7 shows that in the consecutive issues of Trouw (March 1994) the expected vocabulary size differs significantly from the observed vocabulary size for all of the first 20 measurement points, the domain for which significance can be ascertained (see footnote 3). The upper right panel reveals that for the chronologically ordered series of samples from De Telegraaf in the Uit den Boogaart corpus (268 randomly sampled text fragments with on average 75 word tokens) only text chunks reveal a significant difference between bottom panels of Figure 7 show the corresponding plots of the progressive difference scores the complete vocabulary underdispersed words &amp;quot;+&amp;quot;). squares regression lines (dotted) for by nonparametric scattersmoothers (solid lines), reveal a significant negative slope (F(1, 38) = 6.89, &lt; Trouw, F(1,38) = 10.99, &lt; for De Telegraaf). The residuals — DU(k) reveal any significant trends &lt; for both newspapers). Note that for De Telenot capture the downward curvature of well as it should large may be due to the relatively small number of words that emerge as significantly underdispersed for this corpus. Figure 7 shows that intra-textual cohesion within paragraphs is sufficient to give to substantial deviation between texts with no overall discourse organization. Within successive issues of a newspaper, in which a given topic is often discussed on several pages within the same newspaper, and in which a topic may reappear in subsequent issues, strands of inter-textual cohesion may still contribute significantly to the large divergence between the observed and expected vocabulary size. It is only by randomly sampling short text fragments, as for the data from the Uit den Boogaart corpus, which contains samples evenly spread out over a period of one year, that a substantial reduction in overestimation is obtained. Note, however, that for the corpus data we again find that the expectation of consistently too high. Within paragraphs, words tend to be reused more often than expected under change conditions. This reuse pre-empts the use of other word tokens, among which 468 Baayen Lexical Specialization Figure 7 Diagnostic plots for two Dutch newspapers. The difference between the expected and observed vocabulary size for the Trouw data (five issues from March 1994) and the random samples of De Telegraaf in the Uit den Boogaart coipus (upper panels; significant differences are highlighted for the first 20 measurement points). The bottom panels show the progressive error scores for the total vocabulary for the subset of underdispersed dotted line is a least squares regression, the solid line a nonparametric scatterplot smoother. types that have not been observed among the preceding tokens, and leads to a decrease in type richness. Since intra-textual cohesion is also present in the texts of novels, we may conclude that the overestimation bias in novels is determined by a combination of intra-textual and inter-textual cohesion. 4. Implications We have seen that intra-textual and inter-textual cohesion lead to a significant difference between the expected and observed vocabulary size for a wide range of sample sizes. This section addresses two additional questions. First, to what extent does the nonrandomness of word occurrences affect distributions of units selected or derived from words? Second, how does cohesive word usage affect the Good-Turing frequency estimates? Units First consider the effect of nonrandomrtess on the frequency distributions of morphological categories. The upper panels of Figure 8 plot the difference between the expected and observed vocabulary size for the morphological category of words with</abstract>
<note confidence="0.7573818">10 10 20 30 20 40 30 40 Trouw (full issues) 0 50000 100000 150000 200000 250000 Trouw (full issues) De Telegraaf (samples) 5000 10000 15000 20000 De Telegraaf (samples) 0 c‘i 0 0 „_.... 0 0</note>
<abstract confidence="0.741232333333333">z z 0 \ • &apos; \ • \ • \</abstract>
<note confidence="0.88732947826087">469 Computational Linguistics Volume 22, Number 4 -heid in &apos;Max Havelaar&apos; -heid in Trouw 10 \ C•1 \ 0 10 20 30 40 5 10 15 20 Syllables in Trouw Digraphs in &apos;Alice in Wonderland&apos; 10 Z 0 0 ^ 01 Z 0 0 10 20 30 40 10 20 30 40</note>
<abstract confidence="0.991371986111111">i •s. Figure 8 Diagnostic plots for affixes, syllables, and digraphs. The difference between the expected and vocabulary size for the morphological category of words with the Dutch suffix in Havelaar left) and in Trouw (upper right), for syllables in Trouw (lower and for digraphs in in Wonderland. differences are shown in bold for the first half of the tokens. suffix like English, is used to coin abstract nouns from (e.g., from The plots are based on samples of all and only those words occurring in Havelaar left) and (upper right) that belong to the morphological category of all other words, and preserving their order of appearance in the original texts. The sample in Havelaar of 640 tokens representing 260 types, of which 146 hapax legomena. From Trouw, 1145 tokens representing 394 types were extracted, among which 246 hapax legomena. Havelaar, number of words in as and &apos;freedom&apos;, are underdispersed key words. Not surprisingly, this affects the growth of For small values of observe a significant divergence between and (N). the newspaper Trouw, where do not play a central role in an overall discourse, no significant divergence emerges. Nevertheless, we again observe a consistent trend for the expected vocabulary size to overestimate the actual vocabulary size. Figure 8 also plots the development of the vocabulary of syllables in Trouw (botleft), and the development of the vocabulary of digraphs in in Wonderland (bottom right). The &amp;quot;texts&amp;quot; of syllables and digraphs preserve the linear order of the texts from which they were derived. For both digraphs (80,870 tokens representing 398 types, of which 30 hapax legomena) and syllables (470,520 tokens, 6,748 types, 470 Baayen Lexical Specialization and 1,909 hapax legomena), Figure 8 reveals significant deviation in the first half of both texts. This suggests that the nonrandomness observed for words carries over to word-based units such as digraphs and syllables. Accuracy of Estimates Samples of words generally contain—often small—subsets of all the different types available in the population. The probability mass of the unseen types is generally large enough to significantly bias population probabilities estimated from sample relative frequencies. Good (1953) introduced an adjusted frequency estimate (which he credits to Turing) to correct this bias. Instead of estimating the probability of a word with its sample relative frequency Good suggests the use of the adjusted estimate 1)] (17) N E[V(N,f )] • closely related statistic is the probability sampling a new, unseen type after tokens have been sampled: = E[V(N,1)]. These estimates are in wide use (see, e.g., Church and Gale [1991] for application to bigrams, Bod [1995] for application to syntax, and Baayen [1992] and Baayen and Sproat [1996] for application to morphology). Hence, it is useful to consider in some detail how their accuracy is affected by inter-textual and intra-textual cohesion. To this end, I carried out a short series of experiments of the following kind. Assume that the Trouw data used in the previous section constitute a population = word tokens from which we sample the first N/2 = 132,680 words. For Trouw data, this is a matter of stipulation; but for texts such as Dick Wonderland, argument can be made that the novel is the true population rather than a sample from a population. For the present purposes, the crucial point is that we now have defined a population for which we know exactly what the population probabilities—the relative frequencies in the complete texts—are. First consider how accurately we can estimate the vocabulary size of the popufrom the sample. The expression for in (1) that we have used thus far does not allow us to extrapolate to larger sample sizes. However, analytical that allow both interpolation (in the sense of estimating the basis the frequency spectrum for sample sizes M &lt; extrapolation (in the sense of M &gt; available (for a review, see Chitashvili and Baayen Here, make use of a smoother developed by Sichel (1986). The three of this smoother are estimated by requiring that = and by minimizing the chi-square statistic for a given span of frequency ranks. The upper left panel of Figure 9 shows that it was possible to select the parameters of Sichel&apos;s model such that the observed frequencies of the first 20 frequency = ,20) do not differ significantly from their model-dependent</abstract>
<note confidence="0.868628586206897">471 Computational Linguistics Volume 22, Number 4 Frequency spectrum at N = 132680 Interpolation from N = 132680 o 0 Z 0 — Lc) 0 0 Lo .4; • • .4.• • 15 20 20 40 60 80 100 120 N (*1000) Interpolation and extrapolation from N = 132680 Estimation errors conditional on N = 132680 6 a CV■-• 1- 0 ■, . ,,.... - •-&apos; W0 (&amp;quot; Probability mass errors (Good-Turing) 0 S 6 0</note>
<abstract confidence="0.91891952">o 0 Figure 9 Interpolation and extrapolation from sample (the first half of the Trouw data) to population complete Trouw data). and observed frequency spectrum; and observed numbers of types; probability of the types with frequency the sample; estimate of sample estimate of The upper right panel shows that interpolation on the basis of Sichel&apos;s model (dashed line) is virtually indistinguishable from interpolation using (1) (dotted line). The observed vocabulary sizes are represented by large dots. As expected, both (1) and the parametric smoother reveal the characteristic overestimation pattern. The center panels of Figure 9 show that the overestimation characteristic for interpolation is reversed when extrapolating to larger samples. For extrapolation, underestimation is typical. The dotted line in the left-hand panel represents the observed vocabulary size of the complete Trouw text, the solid line shows the result from inand extrapolation from = The right-hand panel highlights the difference scores. For = the error is large: the actual vocabulary size. established that extrapolating, the question is how well the Good-Turing estimates perform. To determine this, I will the probability mass of the frequency classes = ... 40. Let = pit (M) (19)</abstract>
<note confidence="0.767680631578947">The fit = 9.93,p &gt; .9) was obtained for the parameter values a = 0.291,-y = —0.7, and = 50 100 150 200 250 10 20 30 40 0 50 100 150 200 250 N ( 000) Probability mass errors (unadjusted) 0 • 0 c9 0 10 ........ 20 30 40 472</note>
<title confidence="0.584084">Baayen Lexical Specialization</title>
<abstract confidence="0.995948814814815">the joint Good-Turing probability mass of all types with frequency the sample M = 132,680 tokens, and let the joint probability mass of exactly the same types, but now in the population = tokens): .f(i, N) X) frequency of the i-th type in a sample of X tokens. The bottom left of Figure 9 shows that for the first frequency ranks Good-Turing estimate (f, M) the probability mass of the frequency class in the population. For the higher-frequency ranks, the estimates are fairly reliable. The bottom right panel of Figure 9 plots the corresponding errors for the unadjusted sample probability estimate = which overestimates the population values. Surprisingly, the unadjusted estimates overestimate the population values to roughly the same extent that the adjusted estimates lead to underestimation. A heuristic estimate, V(M,f)+ 1)Es +1)] + Mh(f,M) (22) mean of M) to approximate the population relative frequencies (f) well, as shown in Table 1 for the Trouw data as as for in Wonderland, Moby Dick, Havelaar. &gt; as shown in Figure 10, the heuristic estimate remains a reasonable compromise. have seen that both the Good-Turing estimates (esfor &lt; lead to underestimation of population values. Interestingly, P(M) overestimates the probability mass of unseen types. For the Trouw data, at M = 132,680 we count 11,363 hapax legomena, hence P(M) = 0.0856. However, the probability mass of the types that do not appear among the first 132,680 tokens, M(0), is much smaller: Table 1 shows that P(M) similarly leads to overestimation for in Wonder- Moby Dick, Havelaar. judge from Table 1, the Good-Turing estimate (1, M) an approximate lower bound and the unadjusted estimate M) a strict upper bound for M(0). is easy to see why an upper bound for coherent text by focusing on its interpretation. Given the urn model, the probability that the first token sampled a type that will not be represented by any other token equals By symmetry, this probability is identical to the probability that the very last token sampled will represent an unseen type. This probability approximates the probability after have been sampled, the next token sampled will be a new type. However, this interpretation hinges on the random selection of word tokens, and this paper presents ample evidence that once a word has been used it is much more likely to be used again than the urn model predicts. Hence, the probability that after the next token represents an unseen type is less than to intra-textual and inter-textual cohesion, the types that have already been observed have a slightly higher probability of appearing than expected under chance conditions, and consequently the unseen types have a lower probability. Summing up, the Good-Turing frequency estimates are severely effected by the cohesive use of words in normal text. In the absence of probabilistic models that take cohesive word usage into account, estimates of (relative) frequencies remain heuristic 473 Computational Linguistics Volume 22, Number 4 Table 1 of probability mass estimates for frequencies = , 5 using the smoother (N , f )] Sichel (1986). The probability mass of unseen types, (0), is also tabulated. , M): estimate; M, M): estimate; h M): population mass. For Havelaar, sample comprising the first third of the novel was used, for the other texts, a sample consisting of the first half of the tokens was selected.</abstract>
<note confidence="0.529352333333333">V (N , f ) (N, f )] , M) Ms(f Mh(f M) M(f) Alice in Wonderland 0 0.0630</note>
<phone confidence="0.638205">1 885 885.00 0.0411 0.0668 0.0540 0.0560 2 287 272.27 0.0328 0.0434 0.0381 0.0372 3 147 137.27 0.0277 0.0333 0.0305 0.0293 4 97 85.52 0.0255 0.0293 0.0274 0.0289 5 68 59.55 0.0230 0.0257 0.0243 0.0228</phone>
<author confidence="0.891814">Moby Dick</author>
<phone confidence="0.6980875">0 0.0350 1 5,914 5,914.04 0.0366 0.0553 0.0460 0.0472 2 2,035 1,958.22 0.0272 0.0381 0.0326 0.0331 3 990 932.22 0.0218 0.0278 0.0248 0.0251 4 601 549.44 0.0187 0.0225 0.0206 0.0210 5 416 366.22 0.0168 0.0195 0.0181 0.0179</phone>
<author confidence="0.993491">Max Havelaar</author>
<phone confidence="0.6507955">0 0.0921 1 3,513 3,513.01 0.0494 0.1058 0.0776 0.0692 2 908 821.04 0.0362 0.0547 0.0455 0.0411 3 346 362.65 0.0240 0.0313 0.0276 0.0246</phone>
<address confidence="0.693985">4 214 208.95 0.0213 0.0258 0.0235 0.0216</address>
<phone confidence="0.70024">5 157 137.84 0.0203 0.0236 0.0220 0.0189</phone>
<email confidence="0.558626">Trouw</email>
<phone confidence="0.677397166666667">0 0.0609 1 11,363 11,363.05 0.0431 0.0856 0.0644 0.0639 2 2,941 2,856.65 0.0297 0.0443 0.0370 0.0359 3 1,338 1,276.07 0.0233 0.0303 0.0268 0.0256 4 826 737.69 0.0206 0.0249 0.0227 0.0219 5 532 487.51 0.0172 0.0200 0.0186 0.0190</phone>
<abstract confidence="0.953202601809955">nature. the frequencies of types occurring at least once in the sample, the average of the sample and Good-Turing adjusted frequencies is a useful heuristic. For estimates of the probability of unseen types, the sample and Good-Turing estimates provide approximate upper and lower bounds. Words do not occur randomly in texts. This simple fact is difficult to take into account in statistical models of word frequency distributions. Hence, it is often ignored, in the hope that violations of the randomness assumption will not seriously affect the accuracy of quantitative measures and estimates. goal of this paper has been to explore in detail the consequences of intra- 474 Baayen Lexical Specialization 30 40 Figure 10 Frequency class probability mass estimates for the first 40 frequency ranks in a sample of = 132,680 of the Trouw data. The dots denote the probability mass the full text = of the words with frequency the sample. The Good-Turing estimates , are represented by &amp;quot;e,&amp;quot; the sample estimates Ms &amp;quot;s,&amp;quot; and the heuristic , &amp;quot;+&amp;quot;. textual and inter-textual cohesion on the accuracy of theoretical estimates of vocabulary size, the growth rate of the vocabulary, and Good-Turing adjusted frequency estimates, in the belief that knowledge of how nonrandomness might affect these measures ultimately leads to a better understanding of the conditions under which these measures may, or may not, be reliable. Analyses of three novels, five consecutive issues of the Dutch newspaper Trouw, and the chronologically ordered samples of the Dutch newspaper De Telegraaf in the Uit den Boogaart corpus, all revealed systematic overestimation for the expected vocabulary size. Further analyses of subsets of derived words, syllables, and digrams showed that the overestimation bias reappears in units derived from words when these words occur in normal, cohesive text. The overestimation bias disappears when the order of the sentences is randomized. This indicates that the bias should not be attributed to syntactic and semantic constraints on word usage operating within the sentence. Instead, the bias arises due to intra-textual and inter-textual cohesion. In sequences of sentences, words are more likely to be reused than expected under chance conditions. Coherent discourse requires topic continuity cohesion gives rise to a substantial of the overestimation bias, a bias that leads to significant deviations even when small text fragments of some 75 words are selected randomly from a newspaper. addition to intra-textual cohesion, there are words that contribute to the cohe- 475 Computational Linguistics Volume 22, Number 4 sion of the discourse as a whole. Detailed analyses of how these key words appear over sampling time in the novels reveal marked differences in their distributions. These differences in turn shed light on the details of the differences in the patterns of errors characterize the texts. The progressive difference scores of the key words, the deviation scores for the expected and observed numbers of new types appearing in the successive text slices, reveal a pattern that is highly similar to the same scores for the vocabulary as a whole, both qualitatively and quantitatively. This supports the hypothesis that the key words are primarily responsible for the deviation of the expected vocabulary size from its expectation. Nonrandomness in word usage not only introduces a bias with respect to the expected vocabulary size—overestimation when interpolating and underestimation when extrapolating, it also affects the accuracy of the Good-Turing estimates. To correct for an overestimation bias, Good (1953) introduced adjusted estimates, building on the assumption that word usage is to all practical purposes random. These adjusted estimates, however, appear to overshoot their mark for continuous text in that they underestimate the population relative frequencies to roughly the same extent that the unadjusted probabilities lead to overestimation, especially for the lowest frequencies. Again, the effect of inter-textual and intra-textual cohesion manifests itself. Once used, words tend to be used again, and this leads to a somewhat higher relative population frequency than expected. The other side of the same coin is that Good&apos;s estimate for probability mass of unseen types, (N), an upper bound. The words that have already been used have a raised probability of being used again. Hence, the probability for unseen types to appear is lowered. There are two major ways to deal with the effects of nonrandomness in word usage on the accuracy of statistical estimates. First, by randomly sampling individual sentences instead of sequences of sentences, the effects of intra-textual and inter-textual cohesion will be largely eliminated. With the increasingly large corpora that are becoming available at present, enhanced sampling methods should pose no serious problem. For literary studies, however, the discourse structure of a text is part and parcel of the object of study itself. Here, the use of the heuristically adjusted estimates proposed in Section 4.2 may prove to be useful. Finally, the investigation of the distribution of key words may turn out to be a useful tool for investigating the structure of literary texts, a tool that may lead to an improved understanding of the role of lexical specialization in shaping the quantitative developmental structure of the vocabulary. Appendix Equation (1) can be derived as follows; see Good 1953; Good and Toulmin 1956; Kalinin Let the frequency of co, in a sample of M tokens (M &lt; define xi= f 1 if (i, M) = m (23) 0 otherwise. the probability of co, by expected total number of word types with m in a sample of M tokens, E[V(M, given by E [E = E E[X] 476 Baayen Lexical Specialization = 1) + 0 • Pr(Xi = 0) prino. we assume that the frequencies independently and identically bi- (M, The expected overall number of different types in the sample, irrespective of their frequency, follows immediately: = E[ m&gt;i _ _ large M and small probabilities can be approximated by Poisson probabilities, leading to the simplified expressions , (26) e = on a given frequency spectrum = 2, ...}, the vocabulary size for sample size M &lt; V(N) i=1 V(N) _ V(N) (27) f=1 the last step, all sharing the same frequency been grouped Note that when the themselves constitute a sample from a larger population, E[V(M)] is in fact an estimate. The derivation of (27) uses an urn model in which words are sampled with replacement. A model in which words are sampled without replacement is more precise. For instance, for a randomly reordered text, the likelihood that a hapax-legomenon in the full text that appears in the first M tokens will reappear among the remaining tokens is greater than zero in a model that assumes constant probabilities, conto fact. For large M, however, the binomial probabilities (sampling with replacement) are a good approximation of the hypergeometric probabilities (sampling without replacement). 477 Computational Linguistics Volume 22, Number 4 Finally note that (27) suggests that, under randomness, and conditional on the appearing in the sample of alternatively be viewed as distributed random variable with parameters (i,N) (i,N) &lt; 1977): = V - M (28) V(N,f) (1 - . The modification of (28) proposed by Hubert and Labbe (1988) requires the assumption that all the tokens of a word type with specialized use occur in a single slice. Let the total number of words in the set types with specialized use be also assume that the text slices in which these specialized words appear are randomly distributed over the text. Let E occurs in otherwise, (29) and let = if w, w occurs in Pi otherwise. number of in is x, + &gt; Y,. E its&apos;&apos;, M) will appear in the same part of the text. The probability that they will appear in is M Hence N • (30) EHL[V(M)] Yj Pr(X, = 1) + + E (1p)V = 1) + (1p)V - (1 f + the implicit assumption that the proportion of the types with specialized, irrespective of the value N) (31) 478 Baayen Lexical Specialization List of Symbols di,k D(k) DU(k) V(k) VU(k) E[X] EHL[V (M)] ES[V(N,f)] f(i,m) M(f) mGT(f,m) ms(f,m) mh(f,m) ni,k nUi,k NU(k) Pf pi (M) type, P(N) V(N) V (N,f) V(Mk) VU(k) of word type variable for underdispersion of type chunk difference score for text slice difference score of underdispersed words at chunk of new types at of new underdispersed types at expectation of X of the Hubert-Labbe model of Sichel&apos;s (1986) model token frequency of a word token frequency of i-th word type in sample of size M token frequency of i-th word in the k-th text slice for word types 1, , for text slices 1, number of text slices token frequency of a word in sample of size M size in tokens when contrasting two sample sizes (M &lt; probability mass of frequency class sample estimate of sample of size M estimate of (f) estimate of of variable for type first in chunk variable for type first in chunk in number of word tokens in the sample of underdispersed tokens in chunk Hubert-Labbe coefficient of vocabulary partition probability of Good-Turing adjusted probability for sample of size M of new underdispersed tokens at of new underdispersed types at rate of the vocabulary of different word types among of types with frequency a sample of number of types in the first of underdispersed types in chunk</abstract>
<note confidence="0.651458">Acknowledgments indebted to Ken Church, Anne Cutler,</note>
<author confidence="0.461595">James McQueen</author>
<author confidence="0.461595">Rob Schreuder</author>
<author confidence="0.461595">Richard</author>
<abstract confidence="0.707543666666667">Sproat, Fiona Tweedie, and five anonymous reviewers for valuable comments and discussion.</abstract>
<note confidence="0.949003647058824">References R. Harald. 1989. Approach to Morphological Productivity. Statistical Analysis and Psycholinguistic thesis, Free University, Amsterdam. Baayen, R. Harald. 1992. Quantitative aspects of morphological productivity. In G. E. Booij and J. van Marle, editors, of Morphology 1991. Academic Publishers, Dordrecht, pages 109-149. Baayen, R. Harald. 1996. The randomness assumption in word frequency statistics. 479 Computational Linguistics Volume 22, Number 4 G. Perissinotto, editor, in</note>
<affiliation confidence="0.511867">Computing Oxford University Press, Oxford.</affiliation>
<address confidence="0.3475975">Baayen, R. Harald and Antoinette Renouf. 1996. Chronicling the Times: Productive</address>
<abstract confidence="0.7611954">lexical innovations in an English Baayen, R. Harald and Richard Sproat. 1996. Estimating lexical priors for low-frequency morphologically forms.</abstract>
<note confidence="0.872194066666667">Rens. 1995. Linguistics with Statistics: Performance Models of Natural of Amsterdam: Institute for logic, language and computation, Amsterdam. Etienne. 1978. vocabulaire de Jean 1 of Geneve. Chitashvili, Revas J and R. Harald Baayen. 1993. Word frequency distributions. In G. Altman and L. Hfebleek, editors, Quantitative Text Analysis. Wissenschaftlicher Verlag Trier, Trier, pages 54-135. Church, Kenneth and William Gale. 1991. A</note>
<abstract confidence="0.868701666666667">comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English</abstract>
<note confidence="0.602112">Speech and Language, 5:19-54. Church, Kenneth and William Gale. 1995. mixtures. of Natural Engineering,</note>
<address confidence="0.970691">Cleveland, W. S. 1979. Robust locally</address>
<abstract confidence="0.913057764705882">weighted regression and smoothing of the American Association, Good, I. J. 1953. The population frequencies of species and the estimation of parameters. 40:237-264. Good, I. J and G. H. Toulmin. 1956. The number of new species and the increase in population coverage, when a sample is Gustay. 1960. The Hague. Holmes, David. I. 1994. Authorship and the Humanities, 28(2):87-106. Hubert, Pierre and Dominique Labbe. 1988. model of vocabulary partition.</abstract>
<note confidence="0.718966095238095">Linguistic Computing, Indefrey, Peter and R. Harald Baayen. 1994, Estimating word frequencies from data. Neerlandica, 48:259-270. Johnson, Norman L. and Samuel Kotz. 1977. Urn Models and Their Application. An Approach to Modern Discrete Probability Wiley &amp; Sons, New York. Kalinin, V. M. 1965. Functionals related to the Poisson distribution and statistical structure of a text. In J. V. Finnik, editor, Articles on Mathematical Statistics and the of Probability, 202-220, Providence, Rhode Island. Steklov Institute of Mathematics 79, American Mathematical Society. Khmaladze, Estate V and Revas J. Chitashvili. 1989. Statistical analysis of large number of rare events and related of the Tbilisi</note>
<affiliation confidence="0.823334">Institute,</affiliation>
<address confidence="0.868447">Charles. 1977. et methodes de</address>
<abstract confidence="0.768290333333333">lexicale. Paris. Charles. 1979. francaise et quantitative. Geneve.</abstract>
<note confidence="0.805206875">Sichel, H. S. 1986. Word frequency distributions and type-token Scientist, 11:45-72. John W. 1977. Data Reading, Mass. Uit den Boogaart, Pieter C., editor. 1975. Woordfrequenties in Gesproken en Geschreven</note>
<affiliation confidence="0.782242">Scheltema &amp;</affiliation>
<address confidence="0.896285">Holkema, Utrecht. 480</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
</authors>
<title>A Corpus-based Approach to Morphological Productivity. Statistical Analysis and Psycholinguistic Interpretation.</title>
<date>1989</date>
<tech>Ph.D. thesis,</tech>
<institution>Free University,</institution>
<location>Amsterdam.</location>
<contexts>
<context position="2236" citStr="Baayen (1989" startWordPosition="345" endWordPosition="346">at these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. Although the theoretical or expected vocabulary size E[V (N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of productivity of affixes, Baayen and Sproat (to appear) apply this Good-Turing estimate to obtain enhanced estimates of lexical priors for unseen words, and the Good-Turing estimates also play an important role for estimating population probabilities (Church and Gale 1991). If a simple random variable such as the vocabulary size reveals consistent and significant deviation from its expectation, the accuracy of the Good-Turing estima</context>
</contexts>
<marker>Baayen, 1989</marker>
<rawString>Baayen, R. Harald. 1989. A Corpus-based Approach to Morphological Productivity. Statistical Analysis and Psycholinguistic Interpretation. Ph.D. thesis, Free University, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
</authors>
<title>Quantitative aspects of morphological productivity.</title>
<date>1992</date>
<booktitle>Yearbook of Morphology</booktitle>
<pages>109--149</pages>
<editor>In G. E. Booij and J. van Marle, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht,</location>
<marker>Baayen, 1992</marker>
<rawString>Baayen, R. Harald. 1992. Quantitative aspects of morphological productivity. In G. E. Booij and J. van Marle, editors, Yearbook of Morphology 1991. Kluwer Academic Publishers, Dordrecht, pages 109-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
</authors>
<title>The randomness assumption in word frequency statistics.</title>
<date>1996</date>
<contexts>
<context position="23457" citStr="Baayen, 1996" startWordPosition="3789" endWordPosition="3790">a divergence between the observed and theoretical growth curves of the vocabulary. In order to explore this intuition, we need a reliable way to ascertain whether a word is underdispersed. Let the dispersion d, of a word co, be the number of different text slices in which co, appears. Analytical expressions for E[c/,1 and VAR[di] are available (Johnson and Kotz 1977, 113-114), so that in principle Z-scores can be calculated. These Z-scores can then be used to ascertain which words are significantly underdispersed in that they occur in significantly too few text slices given the urn model (cf. Baayen, 1996). Unfortunately, dispersions deviate substantially from normality, so that Z-scores remain somewhat impressionistic. I have therefore used a randomization test to ascertain which words are significantly underdispersed. The randomization test proceeded as follows: The sequence of words of a text was randomized 1,000 times. For each permutation, the dispersion of each word type in that particular permutation was obtained. For each word, we calculated the proportion of permutations for which the dispersion was lower than or equal to the empirical dispersion. For Ahab, all 1,000 permutations revea</context>
</contexts>
<marker>Baayen, 1996</marker>
<rawString>Baayen, R. Harald. 1996. The randomness assumption in word frequency statistics.</rawString>
</citation>
<citation valid="false">
<booktitle>Research in Humanities Computing 5.</booktitle>
<editor>In G. Perissinotto, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<marker></marker>
<rawString>In G. Perissinotto, editor, Research in Humanities Computing 5. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
<author>Antoinette Renouf</author>
</authors>
<title>Chronicling the Times: Productive lexical innovations in an English newspaper.</title>
<date>1996</date>
<journal>Language,</journal>
<pages>72--69</pages>
<contexts>
<context position="2272" citStr="Baayen and Renouf (1996)" startWordPosition="349" endWordPosition="352">essions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. Although the theoretical or expected vocabulary size E[V (N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of productivity of affixes, Baayen and Sproat (to appear) apply this Good-Turing estimate to obtain enhanced estimates of lexical priors for unseen words, and the Good-Turing estimates also play an important role for estimating population probabilities (Church and Gale 1991). If a simple random variable such as the vocabulary size reveals consistent and significant deviation from its expectation, the accuracy of the Good-Turing estimates is also called into question. Th</context>
</contexts>
<marker>Baayen, Renouf, 1996</marker>
<rawString>Baayen, R. Harald and Antoinette Renouf. 1996. Chronicling the Times: Productive lexical innovations in an English newspaper. Language, 72:69-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
<author>Richard Sproat</author>
</authors>
<title>Estimating lexical priors for low-frequency morphologically ambiguous forms.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<marker>Baayen, Sproat, 1996</marker>
<rawString>Baayen, R. Harald and Richard Sproat. 1996. Estimating lexical priors for low-frequency morphologically ambiguous forms. Computational Linguistics, 22(2):155-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Enriching Linguistics with Statistics: Performance Models of Natural Language. University of Amsterdam: Institute for logic, language and computation,</title>
<date>1995</date>
<location>Amsterdam.</location>
<marker>Bod, 1995</marker>
<rawString>Bod, Rens. 1995. Enriching Linguistics with Statistics: Performance Models of Natural Language. University of Amsterdam: Institute for logic, language and computation, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Etienne Brunet</author>
</authors>
<title>Le vocabulaire de Jean Giraudoux,</title>
<date>1978</date>
<journal>of TLQ. Slatkine, Geneve.</journal>
<volume>1</volume>
<contexts>
<context position="1827" citStr="Brunet 1978" startWordPosition="280" endWordPosition="281">xt, word token by word token, the number of different word types encountered increases, quickly at first, and ever more slowly as one progresses through the text. The number of different word types encountered after reading N tokens, the vocabulary size V(N), is a function of N. Analytical expressions for V(N) based on the urn model are available. A classic problem in word frequency studies is, however, that these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. Although the theoretical or expected vocabulary size E[V (N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of productivity of affixes, B</context>
<context position="8005" citStr="Brunet (1978)" startWordPosition="1272" endWordPosition="1273">f Figure 1 show the overestimation error functions E[V(N)] — V(N) corresponding to the left-hand panels using dotted lines. For the first 20 measurement points, the instances for which E[V(N)] diverges significantly from V(N) are shown in bold.3 Clearly, the divergence is significant for almost all of the first 20 measurement points. This suggests informally that the discrepancy between E[V(N)] and V(N) is significant over a wide range of sample sizes. 2.1 The Model Proposed by Hubert and Labbe The problem of the systematic estimation error of E[V(N)] has been pointed out by Muller (1979) and Brunet (1978), who hypothesize that lexical specialization is at issue. In any text, there are words the use of which is mainly or even exclusively restricted to a given subsection of that text. Such locally concentrated clusters of words are at odds with the randomness assumption underlying the derivation of (1), and may be the cause of the divergence illustrated in Figure 1. Following this line of reasoning, Hubert and Labbe (1988) propose a model according to which (1) should be modified 3 Since the expression for an estimate of the variance of V(N) figuring in the Z-scores used here requires knowledge </context>
</contexts>
<marker>Brunet, 1978</marker>
<rawString>Brunet, Etienne. 1978. Le vocabulaire de Jean Giraudoux, volume 1 of TLQ. Slatkine, Geneve.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Revas J Chitashvili</author>
<author>R Harald Baayen</author>
</authors>
<title>Word frequency distributions.</title>
<date>1993</date>
<pages>54--135</pages>
<editor>In G. Altman and L. Hfebleek, editors,</editor>
<location>Trier,</location>
<contexts>
<context position="1916" citStr="Chitashvili and Baayen 1993" startWordPosition="290" endWordPosition="293">red increases, quickly at first, and ever more slowly as one progresses through the text. The number of different word types encountered after reading N tokens, the vocabulary size V(N), is a function of N. Analytical expressions for V(N) based on the urn model are available. A classic problem in word frequency studies is, however, that these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. Although the theoretical or expected vocabulary size E[V (N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of productivity of affixes, Baayen and Sproat (to appear) apply this Good-Turing estimate to obtain enhanced estimates</context>
<context position="8764" citStr="Chitashvili and Baayen (1993)" startWordPosition="1394" endWordPosition="1397">vely restricted to a given subsection of that text. Such locally concentrated clusters of words are at odds with the randomness assumption underlying the derivation of (1), and may be the cause of the divergence illustrated in Figure 1. Following this line of reasoning, Hubert and Labbe (1988) propose a model according to which (1) should be modified 3 Since the expression for an estimate of the variance of V(N) figuring in the Z-scores used here requires knowledge of E[V(2N)], the significance of the divergence for the second 20 measurement points is not available. For technical details, see Chitashvili and Baayen (1993). 457 Computational Linguistics Volume 22, Number 4 as follows (see the appendix for further details): EHL[V(M)] = p-NV + (1 – p)V – (1 – p) E v(N,f) -N (2) Hubert and Labbe&apos;s model contains one free parameter, the coefficient of vocabulary partition p, an estimate of the proportion of specialized words in the vocabulary. Given K different text sizes for which the observed and expected vocabulary sizes are known, p can be estimated by minimizing the mean squared error (MSE) Elk( i(V(Mk) – E[V(Mk)[)2 or the chi-square statistic E (v(mk) _ E[17040])2 E{v(mk)] k=1 (conveniently ignoring that the </context>
</contexts>
<marker>Chitashvili, Baayen, 1993</marker>
<rawString>Chitashvili, Revas J and R. Harald Baayen. 1993. Word frequency distributions. In G. Altman and L. Hfebleek, editors, Quantitative Text Analysis. Wissenschaftlicher Verlag Trier, Trier, pages 54-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--19</pages>
<contexts>
<context position="2673" citStr="Church and Gale 1991" startWordPosition="409" endWordPosition="412">cabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of productivity of affixes, Baayen and Sproat (to appear) apply this Good-Turing estimate to obtain enhanced estimates of lexical priors for unseen words, and the Good-Turing estimates also play an important role for estimating population probabilities (Church and Gale 1991). If a simple random variable such as the vocabulary size reveals consistent and significant deviation from its expectation, the accuracy of the Good-Turing estimates is also called into question. The aim of this paper is to understand why this deviation between the* Wundtlaan 1, 6525 XD Nijmegen, The Netherlands. E-mail: baayen@mpi.n1 © 1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 4 ory and observation arises in word frequency distributions, and in this light evaluate applications of the Good-Turing results. The remainder of this paper is structur</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Church, Kenneth and William Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>Poisson mixtures.</title>
<date>1995</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>1--2</pages>
<marker>Church, Gale, 1995</marker>
<rawString>Church, Kenneth and William Gale. 1995. Poisson mixtures. Journal of Natural Language Engineering, 1(2):163-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W S Cleveland</author>
</authors>
<title>Robust locally weighted regression and smoothing scatterplots.</title>
<date>1979</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>74--829</pages>
<contexts>
<context position="14719" citStr="Cleveland 1979" startWordPosition="2361" endWordPosition="2362">chniques used to estimate the proportion of specialized words in the vocabulary. Unfortunately, the central sections are not necessarily the ones characterized by the highest degree of lexical specialization. To see this, consider Figure 3, which plots the difference between the expected number of new types using 459 Figure 3 Error scores for the influx of new types in Alice in Wonderland. The k = 1,2, ... ,40 text slices are displayed on the horizontal axis, the progressive difference scores D(k) are shown on the vertical axis. The dashed line represents a nonparametric scatterplot smoother (Cleveland 1979), the dotted line a least squares regression line (the negative slope is significant, F(1,38) -= 11.07,p &lt; .002). (1) and the observed number of new types for the successive text slices of Alice in Wonderland. More precisely, for each text slice k, k -=- 1, ,40, we calculate the progressive difference error scores D(k), k = 1. . . 40: D(k) = {E[V(Mk)] - E[V(Mk_i)]1 - {V(Mk) - 1/(Mk-i)}• (5) Note that in addition to positive difference scores, which should be present given that E[V(Mk)] &gt; V(Mk) for most, or, as in Alice in Wonderland, for all values of k, we also have negative difference scores</context>
</contexts>
<marker>Cleveland, 1979</marker>
<rawString>Cleveland, W. S. 1979. Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association, 74:829-836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<pages>40--237</pages>
<contexts>
<context position="1777" citStr="Good 1953" startWordPosition="272" endWordPosition="273">rate. 1. Introduction When reading through a text, word token by word token, the number of different word types encountered increases, quickly at first, and ever more slowly as one progresses through the text. The number of different word types encountered after reading N tokens, the vocabulary size V(N), is a function of N. Analytical expressions for V(N) based on the urn model are available. A classic problem in word frequency studies is, however, that these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. Although the theoretical or expected vocabulary size E[V (N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop mea</context>
<context position="46424" citStr="Good (1953)" startWordPosition="7752" endWordPosition="7753">yllables (470,520 tokens, 6,748 types, 470 Baayen Lexical Specialization and 1,909 hapax legomena), Figure 8 reveals significant deviation in the first half of both texts. This suggests that the nonrandomness observed for words carries over to word-based units such as digraphs and syllables. 4.2 Accuracy of Good-Turing Estimates Samples of words generally contain—often small—subsets of all the different types available in the population. The probability mass of the unseen types is generally large enough to significantly bias population probabilities estimated from sample relative frequencies. Good (1953) introduced an adjusted frequency estimate (which he credits to Turing) to correct this bias. Instead of estimating the probability of a word with frequency f by its sample relative frequency P1= (16) Good suggests the use of the adjusted estimate 1 (f + 1)E[V(N,f + 1)] (17) 1(m )1; N E[V(N,f )] • A closely related statistic is the probability P(N) of sampling a new, unseen type after N word tokens have been sampled: P(N) = E[V(N,1)] . (18) These estimates are in wide use (see, e.g., Church and Gale [1991] for application to bigrams, Bod [1995] for application to syntax, and Baayen [1992] and </context>
<context position="59816" citStr="Good (1953)" startWordPosition="9993" endWordPosition="9994">f new types appearing in the successive text slices, reveal a pattern that is highly similar to the same scores for the vocabulary as a whole, both qualitatively and quantitatively. This supports the hypothesis that the key words are primarily responsible for the deviation of the expected vocabulary size from its expectation. Nonrandomness in word usage not only introduces a bias with respect to the expected vocabulary size—overestimation when interpolating and underestimation when extrapolating, it also affects the accuracy of the Good-Turing estimates. To correct for an overestimation bias, Good (1953) introduced adjusted estimates, building on the assumption that word usage is to all practical purposes random. These adjusted estimates, however, appear to overshoot their mark for continuous text in that they underestimate the population relative frequencies to roughly the same extent that the unadjusted probabilities lead to overestimation, especially for the lowest frequencies. Again, the effect of inter-textual and intra-textual cohesion manifests itself. Once used, words tend to be used again, and this leads to a somewhat higher relative population frequency than expected. The other side</context>
<context position="61680" citStr="Good 1953" startWordPosition="10285" endWordPosition="10286">d pose no serious problem. For literary studies, however, the discourse structure of a text is part and parcel of the object of study itself. Here, the use of the heuristically adjusted estimates proposed in Section 4.2 may prove to be useful. Finally, the investigation of the distribution of key words may turn out to be a useful tool for investigating the structure of literary texts, a tool that may lead to an improved understanding of the role of lexical specialization in shaping the quantitative developmental structure of the vocabulary. Appendix Equation (1) can be derived as follows; see Good 1953; Good and Toulmin 1956; Kalinin 1965: Let f(i,M) denote the frequency of co, in a sample of M tokens (M &lt; N), and define xi = f 1 if f (i, M) = m (23) 0 otherwise. Denoting the probability of co, by pi, the expected total number of word types with frequency m in a sample of M tokens, E[V(M, m)], is given by E[V(M, m)] = E [E Xi] = E E[X] 476 Baayen Lexical Specialization E Pr(Xi = 1) + 0 • Pr(Xi = 0) Emm prino. )M-m (24) where we assume that the frequencies f(i,M) are independently and identically binomially (M, p1) distributed. The expected overall number of different types in the sample, ir</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, I. J. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40:237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
<author>G H Toulmin</author>
</authors>
<title>The number of new species and the increase in population coverage, when a sample is increased.</title>
<date>1956</date>
<journal>Biometrika,</journal>
<pages>43--45</pages>
<contexts>
<context position="1800" citStr="Good and Toulmin 1956" startWordPosition="274" endWordPosition="277">troduction When reading through a text, word token by word token, the number of different word types encountered increases, quickly at first, and ever more slowly as one progresses through the text. The number of different word types encountered after reading N tokens, the vocabulary size V(N), is a function of N. Analytical expressions for V(N) based on the urn model are available. A classic problem in word frequency studies is, however, that these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. Although the theoretical or expected vocabulary size E[V (N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of</context>
<context position="61703" citStr="Good and Toulmin 1956" startWordPosition="10287" endWordPosition="10290">erious problem. For literary studies, however, the discourse structure of a text is part and parcel of the object of study itself. Here, the use of the heuristically adjusted estimates proposed in Section 4.2 may prove to be useful. Finally, the investigation of the distribution of key words may turn out to be a useful tool for investigating the structure of literary texts, a tool that may lead to an improved understanding of the role of lexical specialization in shaping the quantitative developmental structure of the vocabulary. Appendix Equation (1) can be derived as follows; see Good 1953; Good and Toulmin 1956; Kalinin 1965: Let f(i,M) denote the frequency of co, in a sample of M tokens (M &lt; N), and define xi = f 1 if f (i, M) = m (23) 0 otherwise. Denoting the probability of co, by pi, the expected total number of word types with frequency m in a sample of M tokens, E[V(M, m)], is given by E[V(M, m)] = E [E Xi] = E E[X] 476 Baayen Lexical Specialization E Pr(Xi = 1) + 0 • Pr(Xi = 0) Emm prino. )M-m (24) where we assume that the frequencies f(i,M) are independently and identically binomially (M, p1) distributed. The expected overall number of different types in the sample, irrespective of their fre</context>
</contexts>
<marker>Good, Toulmin, 1956</marker>
<rawString>Good, I. J and G. H. Toulmin. 1956. The number of new species and the increase in population coverage, when a sample is increased. Biometrika, 43:45-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gustay Herdan</author>
</authors>
<title>Type-Token Mathematics.</title>
<date>1960</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="19603" citStr="Herdan (1960" startWordPosition="3167" endWordPosition="3168">f a second instance of the same determiner (as in this sentence), is extremely unlikely. According to the urn model, however, such a sequence is likely to occur once every 278 words (the relative frequency of the in English is approximately 0.06), say once every two pages. This is not what we normally find. Clearly, syntax imposes severe constraints on the occurrence of words. Does this imply that the urn model is wrong? For individual sentences, the answer is undoubtedly yes. But for more global textual properties such as vocabulary size, a motivated answer is less easy to give. According to Herdan (1960, 40), reacting to Halle&apos;s criticism of the urn model as a realistic model for language, there is no problem, since statistics is concerned with form, not content.4 Whatever the force of this argument may be, Figure 1 demonstrates clearly that the urn model lacks precision for our data. In order to ascertain the potential relevance of syntactic constraints referred to by Halle, we may proceed as follows: If sentence-level syntax underlies the misfit between the observed and the expected vocabulary size, then this misfit should remain visible for randomized versions of the text in which the sen</context>
</contexts>
<marker>Herdan, 1960</marker>
<rawString>Herdan, Gustay. 1960. Type-Token Mathematics. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I</author>
</authors>
<date>1994</date>
<booktitle>Authorship attribution. Computers and the Humanities,</booktitle>
<pages>28--2</pages>
<marker>I, 1994</marker>
<rawString>Holmes, David. I. 1994. Authorship attribution. Computers and the Humanities, 28(2):87-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Hubert</author>
<author>Dominique Labbe</author>
</authors>
<title>A model of vocabulary partition.</title>
<date>1988</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>3--223</pages>
<contexts>
<context position="3490" citStr="Hubert and Labbe (1988)" startWordPosition="534" endWordPosition="537">n. The aim of this paper is to understand why this deviation between the* Wundtlaan 1, 6525 XD Nijmegen, The Netherlands. E-mail: baayen@mpi.n1 © 1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 4 ory and observation arises in word frequency distributions, and in this light evaluate applications of the Good-Turing results. The remainder of this paper is structured as follows. In Section 2, I introduce some basic notation and the expressions for the growth curve of the vocabulary with which we will be concerned throughout, including a model proposed by Hubert and Labbe (1988), which, by introducing a smoothing parameter, leads to much-improved fits. Unfortunately, this model is based on a series of unrealistic simplifications, and cannot serve as an explanation for the divergence between the observed and expected vocabulary size. In Section 3, therefore, I consider a number of possible sources for the misfit in greater detail: nonrandomness at the sentence level due to syntactic structure, nonrandomness due to the discourse structure of the text as a whole, and noru-andomness due to thematic cohesion in restricted sequences of sentences (paragraphs). Section 4 tra</context>
<context position="8429" citStr="Hubert and Labbe (1988)" startWordPosition="1340" endWordPosition="1343">ificant over a wide range of sample sizes. 2.1 The Model Proposed by Hubert and Labbe The problem of the systematic estimation error of E[V(N)] has been pointed out by Muller (1979) and Brunet (1978), who hypothesize that lexical specialization is at issue. In any text, there are words the use of which is mainly or even exclusively restricted to a given subsection of that text. Such locally concentrated clusters of words are at odds with the randomness assumption underlying the derivation of (1), and may be the cause of the divergence illustrated in Figure 1. Following this line of reasoning, Hubert and Labbe (1988) propose a model according to which (1) should be modified 3 Since the expression for an estimate of the variance of V(N) figuring in the Z-scores used here requires knowledge of E[V(2N)], the significance of the divergence for the second 20 measurement points is not available. For technical details, see Chitashvili and Baayen (1993). 457 Computational Linguistics Volume 22, Number 4 as follows (see the appendix for further details): EHL[V(M)] = p-NV + (1 – p)V – (1 – p) E v(N,f) -N (2) Hubert and Labbe&apos;s model contains one free parameter, the coefficient of vocabulary partition p, an estimate</context>
<context position="64001" citStr="Hubert and Labbe (1988)" startWordPosition="10697" endWordPosition="10700">ontrary to fact. For large N and M, however, the binomial probabilities (sampling with replacement) are a good approximation of the hypergeometric probabilities (sampling without replacement). 477 Computational Linguistics Volume 22, Number 4 Finally note that (27) suggests that, under randomness, and conditional on the words appearing in the sample of N tokens, f(i,M) can alternatively be viewed as a binomially distributed random variable with parameters M/N and f (i,N) for f (i,N) &lt; M,N (Muller 1977): E[V(M)] = V - E V(N,f)e-114f V - M (28) V(N,f) (1 - . The modification of (28) proposed by Hubert and Labbe (1988) requires the assumption that all the tokens of a word type with specialized use occur in a single text slice. Let the total number of words in the set S of types with specialized use be pV, and also assume that the text slices in which these specialized words appear are randomly distributed over the text. Let if wi E S and wi occurs in Pi (29) otherwise, and let y = 01 if w, S and w occurs in Pi otherwise. The overall number of types in Pi is E, x, + &gt; Y,. If w, E S, its&apos;&apos;, tokens (f, &lt; M) will all appear in the same part of the text. The probability that they will appear in Pi is M Hence N •</context>
</contexts>
<marker>Hubert, Labbe, 1988</marker>
<rawString>Hubert, Pierre and Dominique Labbe. 1988. A model of vocabulary partition. Literary and Linguistic Computing, 3:223-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Indefrey</author>
<author>R Harald Baayen</author>
</authors>
<title>Estimating word frequencies from dispersion data.</title>
<date>1994</date>
<journal>Statistica Neerlandica,</journal>
<pages>48--259</pages>
<contexts>
<context position="12789" citStr="Indefrey and Baayen (1994)" startWordPosition="2055" endWordPosition="2058">b in these text slices. The dotted line represents a time-series smoother using running medians (Tukey 1977). not mentioned by name in one text slice only, as the Hubert-Labbe model would have. What we find is that he is not mentioned at all in the first five text slices. Following this we observe a series of text slices in which he appears frequently. These are in turn succeeded by slices in which Ahab is hardly mentioned, but he reappears in the last part of the book, and as the book draws to its dramatic close, the frequency of Ahab increases to its maximum. This is an illustration of what Indefrey and Baayen (1994) refer to as inter-textual cohesion: the word Ahab enjoys specialized use, but it occurs in a series of subtexts within the novel as a whole, contributing to its overall cohesion. Within text slices where Ahab is frequently mentioned, the intra-textual cohesion may similarly be strengthened. For instance, Ahab appears to be a specialized word in text slice 23, but he is mentioned only in passing in text slice 25. His appearance in the two text slices strengthens the intertextual cohesion of the whole novel, but it is only the intra-textual cohesion of slice 23 that is raised. The presence of i</context>
</contexts>
<marker>Indefrey, Baayen, 1994</marker>
<rawString>Indefrey, Peter and R. Harald Baayen. 1994, Estimating word frequencies from dispersion data. Statistica Neerlandica, 48:259-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman L Johnson</author>
<author>Samuel Kotz</author>
</authors>
<title>Urn Models and Their Application. An Approach to Modern Discrete Probability Theory.</title>
<date>1977</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="23212" citStr="Johnson and Kotz 1977" startWordPosition="3747" endWordPosition="3750"> where Ahab is not mentioned at all. Technically speaking, Ahab is underdispersed. If there are many such words, and if these underdispersed words cluster together, the resulting deviations from randomness may be substantial enough to become visible as a divergence between the observed and theoretical growth curves of the vocabulary. In order to explore this intuition, we need a reliable way to ascertain whether a word is underdispersed. Let the dispersion d, of a word co, be the number of different text slices in which co, appears. Analytical expressions for E[c/,1 and VAR[di] are available (Johnson and Kotz 1977, 113-114), so that in principle Z-scores can be calculated. These Z-scores can then be used to ascertain which words are significantly underdispersed in that they occur in significantly too few text slices given the urn model (cf. Baayen, 1996). Unfortunately, dispersions deviate substantially from normality, so that Z-scores remain somewhat impressionistic. I have therefore used a randomization test to ascertain which words are significantly underdispersed. The randomization test proceeded as follows: The sequence of words of a text was randomized 1,000 times. For each permutation, the dispe</context>
</contexts>
<marker>Johnson, Kotz, 1977</marker>
<rawString>Johnson, Norman L. and Samuel Kotz. 1977. Urn Models and Their Application. An Approach to Modern Discrete Probability Theory. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V M Kalinin</author>
</authors>
<title>Functionals related to the Poisson distribution and statistical structure of a text.</title>
<date>1965</date>
<booktitle>Articles on Mathematical Statistics and the Theory of Probability,</booktitle>
<pages>202--220</pages>
<editor>In J. V. Finnik, editor,</editor>
<publisher>American Mathematical Society.</publisher>
<location>Providence, Rhode</location>
<contexts>
<context position="61717" citStr="Kalinin 1965" startWordPosition="10291" endWordPosition="10292">erary studies, however, the discourse structure of a text is part and parcel of the object of study itself. Here, the use of the heuristically adjusted estimates proposed in Section 4.2 may prove to be useful. Finally, the investigation of the distribution of key words may turn out to be a useful tool for investigating the structure of literary texts, a tool that may lead to an improved understanding of the role of lexical specialization in shaping the quantitative developmental structure of the vocabulary. Appendix Equation (1) can be derived as follows; see Good 1953; Good and Toulmin 1956; Kalinin 1965: Let f(i,M) denote the frequency of co, in a sample of M tokens (M &lt; N), and define xi = f 1 if f (i, M) = m (23) 0 otherwise. Denoting the probability of co, by pi, the expected total number of word types with frequency m in a sample of M tokens, E[V(M, m)], is given by E[V(M, m)] = E [E Xi] = E E[X] 476 Baayen Lexical Specialization E Pr(Xi = 1) + 0 • Pr(Xi = 0) Emm prino. )M-m (24) where we assume that the frequencies f(i,M) are independently and identically binomially (M, p1) distributed. The expected overall number of different types in the sample, irrespective of their frequency, follow</context>
</contexts>
<marker>Kalinin, 1965</marker>
<rawString>Kalinin, V. M. 1965. Functionals related to the Poisson distribution and statistical structure of a text. In J. V. Finnik, editor, Articles on Mathematical Statistics and the Theory of Probability, pages 202-220, Providence, Rhode Island. Steklov Institute of Mathematics 79, American Mathematical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Estate V Khmaladze</author>
<author>Revas J Chitashvili</author>
</authors>
<title>Statistical analysis of large number of rare events and related problems.</title>
<date>1989</date>
<journal>Transactions of the Tbilisi Mathematical Institute,</journal>
<pages>91--196</pages>
<contexts>
<context position="1886" citStr="Khmaladze and Chitashvili 1989" startWordPosition="286" endWordPosition="289">of different word types encountered increases, quickly at first, and ever more slowly as one progresses through the text. The number of different word types encountered after reading N tokens, the vocabulary size V(N), is a function of N. Analytical expressions for V(N) based on the urn model are available. A classic problem in word frequency studies is, however, that these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. Although the theoretical or expected vocabulary size E[V (N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of productivity of affixes, Baayen and Sproat (to appear) apply this Good-Turing estimat</context>
</contexts>
<marker>Khmaladze, Chitashvili, 1989</marker>
<rawString>Khmaladze, Estate V and Revas J. Chitashvili. 1989. Statistical analysis of large number of rare events and related problems. Transactions of the Tbilisi Mathematical Institute, 91:196-245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Muller</author>
</authors>
<title>Principes et methodes de statistique lexicale.</title>
<date>1977</date>
<location>Hachette, Paris.</location>
<contexts>
<context position="63885" citStr="Muller 1977" startWordPosition="10675" endWordPosition="10676">r among the remaining N — M tokens is greater than zero in a model that assumes constant probabilities, contrary to fact. For large N and M, however, the binomial probabilities (sampling with replacement) are a good approximation of the hypergeometric probabilities (sampling without replacement). 477 Computational Linguistics Volume 22, Number 4 Finally note that (27) suggests that, under randomness, and conditional on the words appearing in the sample of N tokens, f(i,M) can alternatively be viewed as a binomially distributed random variable with parameters M/N and f (i,N) for f (i,N) &lt; M,N (Muller 1977): E[V(M)] = V - E V(N,f)e-114f V - M (28) V(N,f) (1 - . The modification of (28) proposed by Hubert and Labbe (1988) requires the assumption that all the tokens of a word type with specialized use occur in a single text slice. Let the total number of words in the set S of types with specialized use be pV, and also assume that the text slices in which these specialized words appear are randomly distributed over the text. Let if wi E S and wi occurs in Pi (29) otherwise, and let y = 01 if w, S and w occurs in Pi otherwise. The overall number of types in Pi is E, x, + &gt; Y,. If w, E S, its&apos;&apos;, toke</context>
</contexts>
<marker>Muller, 1977</marker>
<rawString>Muller, Charles. 1977. Principes et methodes de statistique lexicale. Hachette, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Muller</author>
</authors>
<title>Langue francaise et linguistique quantitative.</title>
<date>1979</date>
<journal>Slatkine, Geneve.</journal>
<contexts>
<context position="1813" citStr="Muller 1979" startWordPosition="278" endWordPosition="279"> through a text, word token by word token, the number of different word types encountered increases, quickly at first, and ever more slowly as one progresses through the text. The number of different word types encountered after reading N tokens, the vocabulary size V(N), is a function of N. Analytical expressions for V(N) based on the urn model are available. A classic problem in word frequency studies is, however, that these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. Although the theoretical or expected vocabulary size E[V (N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of productivity</context>
<context position="7987" citStr="Muller (1979)" startWordPosition="1269" endWordPosition="1270">ight-hand panels of Figure 1 show the overestimation error functions E[V(N)] — V(N) corresponding to the left-hand panels using dotted lines. For the first 20 measurement points, the instances for which E[V(N)] diverges significantly from V(N) are shown in bold.3 Clearly, the divergence is significant for almost all of the first 20 measurement points. This suggests informally that the discrepancy between E[V(N)] and V(N) is significant over a wide range of sample sizes. 2.1 The Model Proposed by Hubert and Labbe The problem of the systematic estimation error of E[V(N)] has been pointed out by Muller (1979) and Brunet (1978), who hypothesize that lexical specialization is at issue. In any text, there are words the use of which is mainly or even exclusively restricted to a given subsection of that text. Such locally concentrated clusters of words are at odds with the randomness assumption underlying the derivation of (1), and may be the cause of the divergence illustrated in Figure 1. Following this line of reasoning, Hubert and Labbe (1988) propose a model according to which (1) should be modified 3 Since the expression for an estimate of the variance of V(N) figuring in the Z-scores used here r</context>
</contexts>
<marker>Muller, 1979</marker>
<rawString>Muller, Charles. 1979. Langue francaise et linguistique quantitative. Slatkine, Geneve.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Sichel</author>
</authors>
<title>Word frequency distributions and type-token characteristics.</title>
<date>1986</date>
<booktitle>Mathematical Scientist,</booktitle>
<pages>11--45</pages>
<contexts>
<context position="1854" citStr="Sichel 1986" startWordPosition="284" endWordPosition="285">, the number of different word types encountered increases, quickly at first, and ever more slowly as one progresses through the text. The number of different word types encountered after reading N tokens, the vocabulary size V(N), is a function of N. Analytical expressions for V(N) based on the urn model are available. A classic problem in word frequency studies is, however, that these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric (Good 1953; Good and Toulmin 1956; Muller 1979; Brunet 1978) or parametric (Sichel 1986; Khmaladze and Chitashvili 1989; Chitashvili and Baayen 1993) in nature. Although the theoretical or expected vocabulary size E[V (N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics. For instance, Baayen (1989, 1992) and Baayen and Renouf (1996) exploit the Good-Turing estimate for the probability of sampling unseen types (Good 1953) to develop measures for the degree of productivity of affixes, Baayen and Sproat (to appear</context>
<context position="48423" citStr="Sichel (1986)" startWordPosition="8098" endWordPosition="8099">e frequencies in the complete texts—are. First consider how accurately we can estimate the vocabulary size of the population from the sample. The expression for E[V(N)] given in (1) that we have used thus far does not allow us to extrapolate to larger sample sizes. However, analytical expressions that allow both interpolation (in the sense of estimating V(N) on the basis of the frequency spectrum for sample sizes M &lt; N) and extrapolation (in the sense of estimating V(M) for M &gt; N) are available (for a review, see Chitashvili and Baayen [19931). Here, I will make use of a smoother developed by Sichel (1986). The three parameters of this smoother are estimated by requiring that E[V(N)] = V(N), that E[V(N, 1)] = V(N, 1), and by minimizing the chi-square statistic for a given span of frequency ranks. The upper left panel of Figure 9 shows that it was possible to select the parameters of Sichel&apos;s model such that the observed frequencies of the first 20 frequency ranks (V(N,f),f = 1, ,20) do not differ significantly from their model-dependent 471 Computational Linguistics Volume 22, Number 4 Frequency spectrum at N = 132680 Interpolation from N = 132680 o 0 Z 0 — Lc) 0 0 Lo r•-• .4; • • .4.• 4, • 5 1</context>
<context position="54668" citStr="Sichel (1986)" startWordPosition="9156" endWordPosition="9157"> types that have already been observed have a slightly higher probability of appearing than expected under chance conditions, and consequently the unseen types have a lower probability. Summing up, the Good-Turing frequency estimates are severely effected by the cohesive use of words in normal text. In the absence of probabilistic models that take cohesive word usage into account, estimates of (relative) frequencies remain heuristic 473 Computational Linguistics Volume 22, Number 4 Table 1 Comparison of probability mass estimates for frequencies f = 1, , 5 using the smoother Es[V (N , f )] of Sichel (1986). The probability mass of unseen types, Mp (0), is also tabulated. Notation: MGT (f , M): Good-Turing estimate; M, (f, M): sample estimate; M h (f, M): heuristic estimate; Mp(f): population mass. For Max Havelaar, a sample comprising the first third of the novel was used, for the other texts, a sample consisting of the first half of the tokens was selected. V (N , f ) Es [V (N, f )] MGT , M) Ms(f , M) Mh(f M) M(f) Alice in Wonderland 0 0.0630 1 885 885.00 0.0411 0.0668 0.0540 0.0560 2 287 272.27 0.0328 0.0434 0.0381 0.0372 3 147 137.27 0.0277 0.0333 0.0305 0.0293 4 97 85.52 0.0255 0.0293 0.027</context>
</contexts>
<marker>Sichel, 1986</marker>
<rawString>Sichel, H. S. 1986. Word frequency distributions and type-token characteristics. Mathematical Scientist, 11:45-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John W Tukey</author>
</authors>
<title>Exploratory Data Analysis.</title>
<date>1977</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Mass.</location>
<contexts>
<context position="12271" citStr="Tukey 1977" startWordPosition="1961" endWordPosition="1962">the full text of Moby Dick. The dotted line reveals the main developmental pattern (time-series smoothing using running medians). Even though Ahab is one of the main characters in Moby Dick, and even though his name certainly belongs to the specialized vocabulary of the novel, Ahab is 458 Baayen Lexical Specialization lb 20 30 text slice Figure 2 Nonrandom word usage illustrated for Ahab in Moby Dick. The horizontal axis plots the 40 equally sized text slices, the vertical axis the frequency of Ahab in these text slices. The dotted line represents a time-series smoother using running medians (Tukey 1977). not mentioned by name in one text slice only, as the Hubert-Labbe model would have. What we find is that he is not mentioned at all in the first five text slices. Following this we observe a series of text slices in which he appears frequently. These are in turn succeeded by slices in which Ahab is hardly mentioned, but he reappears in the last part of the book, and as the book draws to its dramatic close, the frequency of Ahab increases to its maximum. This is an illustration of what Indefrey and Baayen (1994) refer to as inter-textual cohesion: the word Ahab enjoys specialized use, but it </context>
<context position="27031" citStr="Tukey 1977" startWordPosition="4390" endWordPosition="4391">erved growth curve of the vocabulary. First consider Figure 4, which summarizes a number of diagnostic functions for Alice in Wonderland. The upper panels plot VU(k) (left) and NU(k) (right), the numbers of underdispersed types and tokens appearing in the successive text chunks. Over sampling time, we observe a slight increase in both the numbers of tokens and the numbers of types. Both trends are significant according to least squares regressions, represented by dotted lines (F(1,38) = 6.591,p &lt; .02 for VU(k); F(1,38) = 16.58,p &lt; .001 for NU(k)). A time-series smoother using running medians (Tukey 1977), represented by solid lines, similarly indebted to Fiona Tweedie, who suggested the use of the randomization test. Comparison of the results based on Z-scores (see Baayen, to appear) and the results based on the randomization test, however, reveal only minor differences that leave the main patterns in the data unaffected. 6 The present method of finding underdispersed words appears to be fairly robust with respect to the number of text slices K. For different numbers of text chunks, virtually the same high-frequency words appear to be underdispersed. The number of text chunks exploited in thi</context>
</contexts>
<marker>Tukey, 1977</marker>
<rawString>Tukey, John W. 1977. Exploratory Data Analysis. Addison-Wesley, Reading, Mass.</rawString>
</citation>
<citation valid="true">
<date>1975</date>
<booktitle>Woordfrequenties in Gesproken en Geschreven Nederlands. Oosthoek, Scheltema &amp; Holkema,</booktitle>
<editor>Uit den Boogaart, Pieter C., editor.</editor>
<location>Utrecht.</location>
<contexts>
<context position="39065" citStr="(1975)" startWordPosition="6557" endWordPosition="6557">analysis of the distribution of key word tokens and types may shed some light on why the theoretical vocabulary size sometimes overestimates and sometimes underestimates the observed vocabulary size. We are left with the question of to what extent repeated use of words within relatively short sequences of sentences, henceforth for ease of reference paragraphs, affects the accuracy of E[V(N)]. I therefore carried out two additional analyses, one using five issues of the Dutch newspaper Trouw, and one using the random samples of the Dutch newspaper De Telegraaf available in the Uit den Boogaart (1975) corpus. For both texts, no overall topical discourse structure is at issue, so that we can obtain a better view of the effects of intra-textual cohesion by itself. For each newspaper, the available texts were brought together in one large corpus, preserving chronological order. Each corpus was divided into 40 equally large text slices. The upper left panel of Figure 7 shows that in the consecutive issues of Trouw (March 1994) the expected vocabulary size differs significantly from the observed vocabulary size for all of the first 20 measurement points, the domain for which significance can be</context>
</contexts>
<marker>1975</marker>
<rawString>Uit den Boogaart, Pieter C., editor. 1975. Woordfrequenties in Gesproken en Geschreven Nederlands. Oosthoek, Scheltema &amp; Holkema, Utrecht.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>