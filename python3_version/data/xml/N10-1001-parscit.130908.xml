<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000047">
<title confidence="0.987668">
Recognition and Understanding of Meetings
</title>
<author confidence="0.992618">
Steve Renals
</author>
<affiliation confidence="0.985961">
Centre for Speech Technology Research, University of Edinburgh
</affiliation>
<address confidence="0.866388">
Informatics Forum, 10 Crichton Street, Edinburgh EH8 9AB, UK
</address>
<email confidence="0.991112">
s.renals@ed.ac.uk homepages.inf.ed.ac.uk/srenals/
</email>
<sectionHeader confidence="0.995432" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999712421052631">
This paper is about interpreting human com-
munication in meetings using audio, video and
other signals. Automatic meeting recognition
and understanding is extremely challenging,
since communication in a meeting is sponta-
neous and conversational, and involves mul-
tiple speakers and multiple modalities. This
leads to a number of significant research prob-
lems in signal processing, in speech recog-
nition, and in discourse interpretation, tak-
ing account of both individual and group be-
haviours. Addressing these problems requires
an interdisciplinary effort. In this paper, I
discuss the capture and annotation of multi-
modal meeting recordings—resulting in the
AMI meeting corpus—and how we have built
on this to develop techniques and applications
for the recognition and interpretation of meet-
ings.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946923076923">
On the face of it, meetings do not seem to form a
compelling research area. Although many people
spend a substantial fraction of their time in meetings
(e.g. the 1998 3M online survey at http://www.
3m.com/meetingnetwork/), for most people
they are not the most enjoyable aspect of their work.
However, for all the time that is spent in meet-
ings, technological support for the meeting process
is scant. Meeting records usually take the form of
brief minutes, personal notes, and more recent use
of collaborative web 2.0 software. Such records are
labour intensive to produce—because they are man-
ually created—and usually fail to capture much of
</bodyText>
<page confidence="0.845773">
1
</page>
<bodyText confidence="0.999987705882353">
the content of a meeting, for example the factors
that led to a particular decision and the different sub-
jective attitudes displayed by the meeting attendees.
For all the time invested in meetings, very little of
the wealth of information that is exchanged is ex-
plicitly preserved.
To preserve the information recorded in meet-
ings, it is necessary to capture it. Obviously this
involves recording the speech of the meeting partic-
ipants. However, human communication is a mul-
timodal activity with information being exchanged
via gestures, handwritten diagrams, and numerous
social signals. The creation of a rich meeting record
involves the capture of data across several modal-
ities. It is a key engineering challenge to capture
such multimodal signals in a reliable, unobtrusive
and flexible way, but the greater challenges arise
from unlocking the multimodal recordings. If such
recordings are not transcribed and indexed (at the
least), then access merely corresponds to replay.
And it is rare that people will have the time, or the
inclination, to replay a meeting.
There is a long and interesting thread of research
which is concerned to better understand the dynam-
ics of meetings and the way that groups function
(Bales, 1951; McGrath, 1991; Stasser and Taylor,
1991). The types of analyses and studies carried
out by these authors is still someway beyond what
we can do automatically. The first significant work
on automatic processing of meetings, coupled with
an exploration of how people might interact with
an archive of recorded meetings, was performed in
the mid 1990s (Kazman et al., 1996). This work
was limited by the fact that it was not possible at
</bodyText>
<note confidence="0.6367415">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1–9,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99996296969697">
that time to transcribe meeting speech automatically.
Other early work in the area concentrated on the
multimodal capture and broadcast of meetings (Roy
and Luz, 1999; Cutler et al., 2002; Yong et al.,
2001).
Three groups further developed approaches to
automatically index the content of meetings. A
team at Fuji Xerox PARC used video retrieval tech-
niques such as keyframing to automatically gener-
ate manga-style summaries of meetings (Uchihashi
et al., 1999), Waibel and colleagues at CMU used
speech recognition and video tracking for meet-
ings (Waibel et al., 2001), and Morgan and col-
leagues at ICSI focused on audio-only capture and
speech recognition (Morgan et al., 2003). Since
2003 research in the recognition and understand-
ing of meetings has developed substantially, stim-
ulated by evaluation campaigns such as the NIST
Rich Transcription (RT)1 and CLEAR2 evaluations,
as well as some large multidisciplinary projects such
as AMI/AMIDA3, CHIL4 and CALO5.
This paper is about the work we have carried out
in meeting capture, recognition and interpretation
within the AMI and AMIDA projects since 2004.
One of the principal outputs of these projects was
a multimodal corpus of meeting recordings, anno-
tated at a number of different levels. In section 2 we
discuss collection of meeting data, and the construc-
tion of the AMI corpus. The remainder of the pa-
per discusses the automatic recognition (section 3)
and interpretation (section 4) of multimodal meeting
recordings, application prototypes (section 5) and is-
sues relating to evaluation (section 6).
</bodyText>
<sectionHeader confidence="0.932398" genericHeader="method">
2 The AMI corpus
</sectionHeader>
<bodyText confidence="0.99928925">
Ideally it would not be necessary to undertake a large
scale data collection and annotation exercise, every
time we address a new domain. However unsuper-
vised adaptation techniques are still rather imma-
ture, and prior to the collection of the AMI corpus,
there had not been a controlled collection and multi-
level annotation of multiparty interactions, recorded
across multiple modalities.
</bodyText>
<footnote confidence="0.9994966">
1www.itl.nist.gov/iad/mig/tests/rt/
2clear-evaluation.org/
3www.amiproject.org/
4chil.server.de/
5caloproject.sri.com/
</footnote>
<figureCaption confidence="0.99001575">
Figure 1: AMI instrumented meeting room: four co-
located participants, one joined by video conference. In
this case two microphone arrays and seven cameras were
used.
</figureCaption>
<bodyText confidence="0.99989625">
One of our key motivations is the development
of automatic approaches to recognise and interpret
group interactions, using information spread across
multiple modalities, but collected as unobtrusively
as possible. This led to the design and construction
of the AMI Instrumented Meeting Rooms (figure 1)
at the University of Edinburgh, Idiap Research In-
stitute, and TNO Human Factors. These rooms con-
tained a set of standardised recording equipment in-
cluding six or seven cameras (four of which would
be used for close-up views in meeting of up to four
people), an 8-element microphone array, a close-
talking microphone for each participant (used to
guarantee a clean audio signal for each speaker),
as well capture of digital pens, whiteboards, shared
laptop spaces, data projector and videoconferencing
if used. A considerable amount of hardware was
necessary for ensuring frame-level synchronisation.
More recently we have used a lighter weight setup,
that uses a high resolution spherical digital video
camera system, and a single microphone array (7–
20 elements, depending on meeting size) synchro-
nised using software. We have also constructed a
prototype system using a low-cost, flexible array of
digital MEMS microphones (Zwyssig et al., 2010).
We used these instrumented meeting rooms to
record the AMI Meeting Corpus (Carletta, 2007).
This corpus contains over 100 hours of meeting
recordings, with the different recording streams syn-
chronised to a common timeline. The corpus con-
tains a number of manually created and automatic
annotations, synchronised to the same timeline. This
</bodyText>
<page confidence="0.988278">
2
</page>
<bodyText confidence="0.988806813559322">
includes a high-quality manual word-level transcrip-
tion of the complete corpus, as well as reference au-
tomatic speech recognition output, using the speech
recognition system discussed in section 3 (using 5-
fold cross-validation). In addition to word-level
transcriptions, the corpus includes manual annota-
tions that describe the behaviour of meeting partici-
pants at a number of levels. These include dialogue
acts, topic segmentation, extractive and abstractive
summaries, named entities, limited forms of head
and hand gestures, gaze direction, movement around
the room, and head pose information. Some of these
annotations, in particular video annotation, are ex-
pensive to perform: about 10 hours of meetings have
been completely annotated at all these levels; over
70% of the corpus has been fully annotated with
the linguistic annotations. NXT—the NITE XML
Toolkit6—an XML-based open source software in-
frastructure for multimodal annotation was used to
carry out and manage the annotations.
About 70% of the AMI corpus consists of meet-
ings based on a design scenario, in which four par-
ticipants play roles in a design team. The scenario
involves four team meetings, between which the par-
ticipants had tasks to accomplish. The participant
roles were stimulated in real-time by email and web
content. Although the use of a scenario reduces the
overall realism of the meetings, we adopted this ap-
proach for several reasons, most importantly: (1)
there were some preferred design outcomes, mak-
ing it possible to define some objective group out-
come measures; (2) the knowledge and motivation
of the participants was controlled, thus removing the
serious confounding factors that would arise from
the long history and context found in real organ-
isations; and (3) allowing the meeting scenario to
be replicated, thus enabling system-level evaluations
(as discussed in section 6). We recorded and anno-
tated thirty replicates of the scenario: this provides
an unparalleled resource for system evaluation, but
also reduces the variability of the corpus (for ex-
ample in terms of the language used). The remain-
ing 30% of the corpus contains meetings that would
have occurred anyway; these are meetings with a
lot less control than the scenario meetings, but with
greater linguistic variability.
6sourceforge.net/projects/nite/
All the meetings in the AMI corpus are spoken
in English, but over half the participants are non-
native speakers. This adds realism in a European
context, as well as providing an additional speech
recognition challenge. The corpus is publicly avail-
able7, and is released under a licence that is based on
the Creative Commons Attribution NonCommercial
ShareAlike 2.5 Licence. This includes all the signals
and manual annotations, plus a number of automatic
annotations (e.g. speech recognition) made available
to lower the startup cost of performing research on
the corpus.
</bodyText>
<sectionHeader confidence="0.993818" genericHeader="method">
3 Multimodal recognition
</sectionHeader>
<bodyText confidence="0.999960935483871">
The predominant motivation behind the collection
and annotation of the AMI corpus was to enable the
development of multimodal recognisers to address
issues such as speech recognition, speaker diarisar-
tion (Wooters and Huijbregts, 2007), gesture recog-
nition (Al-Hames et al., 2007) and focus of attention
(Ba and Odobez, 2008). Although speech recog-
nition is based on the (multichannel) audio signal,
the other problems can be successfully addressed by
combining modalities. (There is certainly informa-
tion in other modalities that has the potential to make
speech recognition more accurate, but so far we have
not been able to use it consistently and robustly.)
Speech recognition: The automatic transcription
of what is spoken in a meeting is an essential pre-
requisite to interpreting a meeting. Morgan et al
(2003) described the speech recognition of meetings
as an “ASR-complete” problem. Developing an ac-
curate system for meeting recognition involves the
automatic segmentation of the recording into utter-
ances from a single talker, robustness to reverbera-
tion and competing acoustic sources, handling over-
lapping talkers, exploitation of multiple microphone
recordings, as well as the core acoustic and language
modelling problems that arise when attempting to
recognise spontaneous, conversational speech.
Our initial systems for meeting recognition used
audio recorded with close-talking microphones, in
order to develop the core acoustic modelling tech-
niques. More recently our focus has been on recog-
nising speech obtained using tabletop microphone
</bodyText>
<footnote confidence="0.38483">
7corpus.amiproject.org/
</footnote>
<page confidence="0.99111">
3
</page>
<bodyText confidence="0.999992">
arrays, which are less intrusive but have a lower
signal-to-noise ratio. Multiple microphone sys-
tems are based on microphone array beamforming
in which the individual microphone signals are fil-
tered and summed to enhance signals coming from
a particular direction, while suppressing signals
from competing locations (W¨olfel and McDonough,
2009).
The core acoustic and language modelling com-
ponents for meeting speech recognition correspond
quite closely to the state-of-the-art systems used in
other domains. Acoustic modelling techniques in-
clude vocal tract length normalisation, speaker adap-
tation based on maximum likelihood linear trans-
forms, and further training using a discriminative
minimum Bayes risk criterion such as minimum
phone error rate (Gales and Young, 2007; Renals
and Hain, 2010). In addition we have employed a
number of novel acoustic parameterisations includ-
ing approaches based on local posterior probability
estimation (Grezl et al., 2007) and pitch adaptive
features (Garau and Renals, 2008), the automatic
construction of domain-specific language models
using documents obtained from the web by search-
ing with n-grams obtained from meeting transcripts
(Wan and Hain, 2006; Bulyko et al., 2007), and au-
tomatic approaches to acoustic segmentation opti-
mised for meetings (Wrigley et al., 2005; Dines et
al., 2006).
A feature of the systems developed for meeting
recognition is the use of multiple recognition passes,
cross-adaptation and model combination (Hain et
al., 2007). In particular successive passes make use
of more detailed—and more diverse—acoustic and
language models. Different acoustic models trained
on different feature representations (e.g. standard
PLP features and posterior probability-based fea-
tures) are cross-adapted, and different feature repre-
sentations are also combined using linear transforms
such as heteroscedastic linear discriminant analysis
(Kumar and Andreou, 1998).
These systems have been evaluated in successive
NIST RT evaluations: the core microphone array
based system has a word error rate of about 40%;
after adaptation and feature combination steps, this
error rate can be reduced to about 30%. The equiv-
alent close-talking microphone system has baseline
word error rate of about 35%, reduced to less than
25% after further recognition passes (Hain et al.,
2007). The core system runs about five times slower
than real-time, and the full system is about fourteen
times slower than real-time, on current commodity
hardware. We have developed a low-latency real-
time system (with an error rate of about 41% for mi-
crophone array input) (Garner et al., 2009), based on
an open source runtime system8.
</bodyText>
<sectionHeader confidence="0.996727" genericHeader="method">
4 Meeting interpretation
</sectionHeader>
<bodyText confidence="0.999992914285714">
One of the interdisciplinary joys of working on
meetings is that researchers with different ap-
proaches are able to build collaborations through
working on common problems and common data.
The automatic interpretation of meetings is a very
good example: meetings form an exciting challenge
for work in things such as topic identification, sum-
marisation, dialogue act recognition and the recog-
nition of subjective content. Although text-based
approaches (using the output of a speech recogni-
tion system) form strong baselines, it is often the
case that systems can be improved through the in-
corporation of information characteristic of spoken
communication, such as prosody and speaker turn
patterns, as well video information such as head or
hand movements.
Segmentation: We have explored multistream
statistical models to automatically segment meeting
recordings. Meetings can be usefully segmented at
many different levels, for example into speech and
non-speech (an essential pre-processing for speech
recognition), into utterances spoken by a single
talker, into dialogue acts, into topics, and into “meet-
ing phases”. The latter was the subject of our first in-
vestigations in using multimodal multistream mod-
els to segment meetings.
Meetings are group events, characterised by both
individual actions and group actions. To obtain
structure at the group level, we and colleagues in
the M4 and AMI projects investigated segmenting
a meeting into a sequence of group actions such as
monologue, discussion and presentation (McCowan
et al., 2005). We used a number of feature streams
for this segmentation and labelling task including
speaker turn dynamics, prosody, lexical information,
</bodyText>
<footnote confidence="0.492682">
8juicer.amiproject.org/
</footnote>
<page confidence="0.985906">
4
</page>
<bodyText confidence="0.999914075268818">
and participant head and hand movements (Diel-
mann and Renals, 2007). Our initial experiments
used an HMM to model the feature streams with a
single hidden state space, and resulted in an “action
error rate” of over 40% (action error rate is analo-
gous to word error rate, but defined over meeting
actions, presumed not to overlap). The HMM was
then substituted by a richer DBN multistream model
in which each feature stream was processed inde-
pendently at a lower level of the model. These par-
tial results were then combined at a higher level,
thus providing hierarchical integration of the multi-
modal feature streams. This multistream approach
enabled a later integration of feature streams and
increased flexibility in modelling the interdepen-
dences between the different streams, enabling some
accommodation for asynchrony and multiple time
scales. Thus use of the richer DBN multistream
model resulted in a significant lowering of the ac-
tion error rate to around 13%.
We extended this approach to look at a much finer
grained segmentation: dialogue acts. A dialogue act
can be viewed as a segment of speech labelled so as
to roughly categorise the speaker’s intention. In the
AMI corpus each dialogue act in a meeting is given
one of 15 labels, which may be categorised as infor-
mation exchange, making or eliciting suggestions or
offers, commenting on the discussion, social acts,
backchannels, or “other”. The segmentation prob-
lem is non-trivial, since a single stretch of speech
(with no pauses) from a speaker may comprise sev-
eral dialogue acts—and conversely a single dialogue
act may contain pauses. To address the tasks of auto-
matically segmenting the speech into dialogue acts,
and assigning a label to each segment, we employed
a switching dynamic Bayesian network architecture,
which modelled a set of features related to lexical
content and prosody and incorporates a weighted in-
terpolated factored language model (Dielmann and
Renals, 2008). The switching DBN coordinated the
recognition process by integrating all the available
resources. This approach was able to leverage addi-
tional corpora of conversational data by using them
as training data for a factored language model which
was used in conjunction with additional task spe-
cific language models. We followed this joint gener-
ative model, with a discriminative approach, based
on conditional random fields, which performed a re-
classification of the segmented dialogue acts.
Our experiments on dialogue act recognition used
both automatic and manual transcriptions of the
AMI corpus. The degradation when moving from
manual transcriptions to the output of a speech
recogniser was less than 10% absolute for both di-
alogue act classification and segmentation. Our ex-
periments indicated that it is possible to perform au-
tomatic segmentation into dialogue acts with a rel-
atively low error rate. However the operations of
tagging and recognition into fifteen imbalanced DA
categories have a relatively high error rate, even after
discriminative reclassification, indicating that this
remains a challenging task.
Summarisation: The automatic generation of
summaries provides a natural way to succinctly de-
scribe the content of a meeting, and can be an effi-
cient way for users to obtain information. We have
focussed on extractive techniques to construct sum-
maries, in which the most relevant parts of a meeting
are located, and concatenated together to provide a
‘cut-and-paste’ summary, which may be textual or
multimodal.
Our approach to extractive summarisation is
based on automatically extracting relevant dialogue
acts (or alternatively “spurts”, segments spoken by
a single speaker and delimited by silence) from a
meeting (Murray et al., 2006). This requires (as a
minimum) the automatic speech transcription and,
if spurts are not used, dialogue act segmentation.
Lexical information is clearly extremely important
for summarisation, but we have also found speaker
features (relating to activity, dominance and over-
lap), structural features (the length and position of
dialogue acts), prosody, and discourse cues (phrases
which signal likely relevance) to be important for
the development of accurate methods for extractive
summarisation of meetings. Furthermore we have
explored reduced dimension representations of text,
based on latent semantic analysis, which we found
added precision to the summarisation. Using an
evaluation measure referred to as weighted preci-
sion, we discovered that it is possible to reliably
extract the most relevant dialogue acts, even in the
presence of speech recognition errors.
</bodyText>
<page confidence="0.994886">
5
</page>
<sectionHeader confidence="0.981214" genericHeader="method">
5 Application prototypes
</sectionHeader>
<bodyText confidence="0.999375347826087">
We have incorporated these meeting recognition and
interpretation components in a number of applica-
tions. Our basic approach to navigating meeting
archives centres on the notion of meeting browsers,
in which media files, transcripts and segmentations
are synchronised to a common time line. Figure 2
(a) gives an example of such a browser, which also
enables a user to pan and zoom within the captured
spherical video stream.
We have explored (and, as discussed below, eval-
uated) a number of ways of including automatically
generated summaries in meeting browsers. The
browser illustrated in figure 2 (b) enables navigation
by the summarised transcript or via the topic seg-
mentation. In this case the degree of summarisation
is controlled by a slider, which removes those speech
segments that do no contribute to the summary. We
have also explored real-time (with a few utterances
latency) approaches to summarisation, to facilitate
meeting “catchup” scenarios, including the genera-
tion of audio only summaries, with about 60% of
the speech removed (Tucker et al., 2010). Visual-
isations of summaries include a comic book layout
(Castronovo et al., 2008), illustrated in figure 3. This
is related to “VideoManga” (Uchihashi et al., 1999),
but driven by transcribed speech rather than visually
identified keyframes.
The availability of real-time meeting speech
recognition, with phrase-level latency (Garner et al.,
2009), enables a new class of applications. Within
AMIDA we developed a software architecture re-
ferred to as “The Hub” to support real-time ap-
plications. The Hub is essentially a real-time an-
notation server, mediating between annotation pro-
ducers, such as speech recognition, and annotation
consumers, such as a real-time catchup browser.
Of course many applications will be both produc-
ers and consumers: for instance topic segmenta-
tion consumes transcripts and speaker turn informa-
tion and produces time aligned topic segments. A
good example of an application made possible by
real-time recognition components and the Hub is the
AMIDA Content Linking Device (Popescu-Belis et
al., 2008). Content linking is essentially a continual
real-time search in which a repository is searched
using a query constructed from the current conver-
</bodyText>
<figure confidence="0.9993995">
(a) Basic web-based browser
(b) Summary browser
</figure>
<figureCaption confidence="0.9995188">
Figure 2: Two examples of meeting browsers, both in-
clude time synchronisation with a searchable ASR tran-
script and speaker activities. (a) is a basic web-based
browser; (b) also employs extractive summarisation and
topic segmentation components.
</figureCaption>
<bodyText confidence="0.999884533333333">
sational context. In this case the context is obtained
from a speech recognition transcript of the past 30
seconds of the conversation, and a query is con-
structed using tf·idf or a similar measure, combined
with predefined keywords or topic weightings. The
repository to be searched may be the web, or a por-
tion of the web, or it may be an organisational doc-
ument repository, including transcribed, structured
and indexed recordings of previous meetings. Figure
4 shows a basic interface to content linking. We have
constructed live content-linking systems, driven by
microphone array based real-time speech recogni-
tion, with the aim of presenting—without explicit
query—potentially relevant documents to meeting
participants.
</bodyText>
<page confidence="0.997709">
6
</page>
<figureCaption confidence="0.995798">
Figure 3: Comic book display of automatically generated
</figureCaption>
<bodyText confidence="0.4615195">
yeah materials
meeting summary.
</bodyText>
<sectionHeader confidence="0.980454" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.884093783783784">
The multiple streams of data and multiple layers of
annotations that make up the AMI corpus enable it to
yea
be used for evaluations of specific recognition com-
ponents. The corpus has been used to evaluate many
different things including voice activity detection,
speaker diarisation and speech recognition (in the
NIST RT evaluations), and head pose recognition
(in the CLEAR evaluation). In the spoken language
processing domain, the AMI corpus has been used
to evaluate meeting summarisation, topic segmen-
kay
tation, dialogue act recognition and cross-language
retrieval.
In addition to intrinsic component-level evalu-
ations, it is valuable to evaluate complete sys-
tems, and components in a system context. In the
AMI/AMIDA projects, we investigated a number of
extrinsic evaluation frameworks for browsing and
accessing meeting archives. The Browser Evalua-
tion Test (BET) (Wellner et al., 2005) provides a
framework for the comparison of arbitrary meet-
ing browser setups, which may differ in terms of
which content extraction or abstraction components
materals
are employed. In the BET test subjects have to an-
it is
swer true/false questions about a number of “obser-
vations of interest” relating to a recorded meeting,
you we could
using the browser under test with a specified time
whats beter yeah i men
pla plastic s
limit(typically half the meeting length).
rubber
o you could go
We developed of a variant of the BET to specifi-
</bodyText>
<figureCaption confidence="0.995715">
Figure 4: Demonstration screenshot of the AMI auto-
</figureCaption>
<bodyText confidence="0.944072266666666">
n
matic content linking device. The subpanels show (clock-
wise from top left) the ASR transcript, relevant docu-
ments from the meeting document base, relevant web hits
and a a tag cloud.
well i figure if we
cally evaluate different summarisation approaches.
In the Decision Audit evaluation (Murray et al.,
2009) the user’s task is to ascertain the factors across
yeah
a number of meetings that lead to a particular deci-
sion being made. A set of browsers were constructed
differing in the summarisation approach employed
(manual vs. ASR transcripts; extractive vs. abstrac-
tive vs. human vs. keyword-based summarisation),
</bodyText>
<subsectionHeader confidence="0.705636">
Buttons
</subsectionHeader>
<bodyText confidence="0.968529954545454">
and the test subjects used them to perform the deci-
sion audit. Like the BET this evaluation is labour-
bcause you can build
intensive, but the results can be analysed using a
butto and some
st
battery of objective and subjective measures. Con-
want to watc two
yeah which channels
ts ou
clusions from carrying out this evaluation indicated
want to in it
that the task itself was quite challenging for users
(even with human transcripts and summaries, most
users could not find many factors involved in the de-
cision), that automatic extractive summaries outper-
formed reasonably competitive baseline approaches,
and that although subjects reported ASR transcripts
to be unsatisfactory (due to the error rate) browsing
using the ASR transcript still resulted in users’ be-
ing generally able to find the relevant parts of the
meeting archive.
</bodyText>
<figure confidence="0.978483888888889">
okay like
this
Materials
like yeah a
sponge-ball
yeah
yeah they like
spongy
material
</figure>
<page confidence="0.998179">
7
</page>
<sectionHeader confidence="0.995962" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999912891304348">
In this paper I have given an overview of our inves-
tigations into automatic meeting recognition and in-
terpretation. Multiparty communication is a chal-
lenging problem at many levels, from signal pro-
cessing to discourse modelling. A major part of
our attempt to address this problem, in an interdisci-
plinary way, was the collection, annotation, and dis-
tribution of the AMI meeting corpus. The AMI cor-
pus has been at the basis of nearly all the work that
we have carried out in the area, from speech recog-
nition to summarisation. Multiparty speech recog-
nition remains a difficult task, with a typical error
rate of over 20%, however the accuracy is enough to
enable various components to build on top of it. A
major achievement has been the development of pro-
totype applications that can use phrase-level latency
real-time speech recognition.
Many of the automatic approaches to meeting
recognition and characterisation are characterised by
extensive combination at the feature stream, model
and system level. In our experience, such ap-
proaches offer consistent improvements in accuracy
for these complex, multimodal tasks.
Meetings serve a social function, and much of
this has been ignored in our work, so far. We have
focussed principally on understanding meetings in
terms of their lexical content, augmented by vari-
ous multimodal streams. However in many inter-
actions, the social signals are at least as important
as the propositional content of the words (Pentland,
2008); it is a major challenge to develop meeting in-
terpretation components that can infer and take ad-
vantage of such social cues. We have made initial
attempts to do this, by attempting to include aspects
such as social role (Huang and Renals, 2008).
The AMI corpus involved a substantial effort from
many individuals, and provides an invaluable re-
source. However, we do not wish to do this again,
even if we are dealing with a domain that is sig-
nificantly different, such as larger groups, or family
“meetings”. However, our recognisers rely strongly
on annotated in-domain data. It is a major chal-
lenge to develop algorithms that are unsupervised
and adaptive to free us from the need to collect and
annotate large amount of data each time we are in-
terested in a new domain.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974363636364">
This paper has arisen from a collaboration involving
several laboratories. I have benefitted, in particular,
from long-term collaborations with Herv´e Bourlard,
Jean Carletta, Thomas Hain, and Mike Lincoln, and
from a number of fantastic PhD students. This work
was supported by the European IST/ICT Programme
Projects IST-2001-34485 (M4), FP6-506811 (AMI),
FP6-033812 (AMIDA), and FP7-231287 (SSPNet).
This paper only reflects the author’s views and fund-
ing agencies are not liable for any use that may be
made of the information contained herein.
</bodyText>
<sectionHeader confidence="0.998947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999572783783784">
M. Al-Hames, C. Lenz, S. Reiter, J. Schenk, F. Wallhoff,
and G. Rigoll. 2007. Robust multi-modal group action
recognition in meetings from disturbed videos with the
asynchronous hidden Markov model. In Proc IEEE
ICIP.
S. O. Ba and J. M. Odobez. 2008. Multi-party focus of
attention recognition in meetings from head pose and
multimodal contextual cues. In Proc. IEEE ICASSP.
R. F. Bales. 1951. Interaction Process Analysis. Addi-
son Wesley, Cambridge MA, USA.
I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and
O. Cetin. 2007. Web resources for language modeling
in conversational speech recognition. ACM Transac-
tions on Speech and Language Processing, 5(1):1–25.
J. Carletta. 2007. Unleashing the killer corpus: expe-
riences in creating the multi-everything AMI Meet-
ing Corpus. Language Resources and Evaluation,
41:181–190.
S. Castronovo, J. Frey, and P. Poller. 2008. A generic
layout-tool for summaries of meetings in a constraint-
based approach. In Machine Learning for Multimodal
Interaction (Proc. MLMI ’08). Springer.
R. Cutler, Y. Rui, A. Gupta, J. Cadiz, I. Tashev, L. He,
A. Colburn, Z. Zhang, Z. Liu, and S. Silverberg. 2002.
Distributed meetings: a meeting capture and broad-
casting system. In Proc. ACM Multimedia, pages 503–
512.
A. Dielmann and S. Renals. 2007. Automatic meet-
ing segmentation using dynamic Bayesian networks.
IEEE Transactions on Multimedia, 9(1):25–36.
A. Dielmann and S. Renals. 2008. Recognition of di-
alogue acts in multiparty meetings using a switching
DBN. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 16(7):1303–1314.
J. Dines, J. Vepa, and T. Hain. 2006. The segmenta-
tion of multi-channel meeting recordings for automatic
speech recognition. In Proc. Interspeech.
</reference>
<page confidence="0.990274">
8
</page>
<reference confidence="0.999000121495327">
M. J. F. Gales and S. J. Young. 2007. The application of
hidden Markov models in speech recognition. Foun-
dations and Trends in Signal Processing, 1(3):195–
304.
G. Garau and S. Renals. 2008. Combining spectral rep-
resentations for large vocabulary continuous speech
recognition. IEEE Transactions on Audio, Speech and
Language Processing, 16(3):508–518.
P. Garner, J. Dines, T. Hain, A. El Hannani, M. Karafiat,
D. Korchagin, M. Lincoln, V. Wan, and L. Zhang.
2009. Real-time ASR from meetings. In Proc. In-
terspeech.
F. Grezl, M. Karafiat, S. Kontar, and J. Cernocky. 2007.
Probabilistic and bottle-neck features for lvcsr of
meetings. In Acoustics, Speech and Signal Process-
ing, 2007. ICASSP 2007. IEEE International Confer-
ence on, volume 4, pages IV–757–IV–760.
T. Hain, L. Burget, J. Dines, G. Garau, M. Karafiat,
M. Lincoln, J. Vepa, and V. Wan. 2007. The ami
system for the transcription of speech in meetings. In
Proc. IEEE ICASSP–07.
S. Huang and S. Renals. 2008. Unsupervised language
model adaptation based on topic and role information
in multiparty meetings. In Proc. Interspeech ’08.
R. Kazman, R. Al-Halimi, W. Hunt, and M. Mantei.
1996. Four paradigms for indexing video conferences.
Multimedia, IEEE, 3(1):63–73.
N. Kumar and A. G. Andreou. 1998. Heteroscedastic
discriminant analysis and reduced rank HMMs for im-
proved recognition. Speech Communication, 26:283–
297.
I. McCowan, D. Gatica-Perez, S. Bengio, G. Lathoud,
M. Barnard, and D. Zhang. 2005. Automatic analysis
of multimodal group actions in meetings. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
27(3):305–317.
J. E. McGrath. 1991. Time, interaction, and performance
(TIP): A theory of groups. Small Group Research,
22(2):147.
N. Morgan, D. Baron, S. Bhagat, H. Carvey, R. Dhillon,
J. Edwards, D. Gelbart, A. Janin, A. Krupski, B. Pe-
skin, T. Pfau, E. Shriberg, A. Stolcke, and C. Woot-
ers. 2003. Meetings about meetings: research at ICSI
on speech in multiparty conversations. In Proc. IEEE
ICASSP.
G. Murray, S. Renals, J. Moore, and J. Carletta. 2006. In-
corporating speaker and discourse features into speech
summarization. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
367–374.
G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals,
and J. Kilgour. 2009. Extrinsic summarization eval-
uation: A decision audit task. ACM Transactions on
Speech and Language Processing, 6(2):1–29.
A.S. Pentland. 2008. Honest signals: how they shape
our world. The MIT Press.
A. Popescu-Belis, E. Boertjes, J. Kilgour, P. Poller,
S. Castronovo, T. Wilson, A. Jaimes, and J. Car-
letta. 2008. The amida automatic content linking de-
vice: Just-in-time document retrieval in meetings. In
Machine Learning for Multimodal Interaction (Proc.
MLMI ’08).
S. Renals and T. Hain. 2010. Speech recognition. In
A. Clark, C. Fox, and S. Lappin, editors, Handbook
of Computational Linguistics and Natural Language
Processing. Wiley Blackwell.
D. M. Roy and S. Luz. 1999. Audio meeting history
tool: Interactive graphical user-support for virtual au-
dio meetings. In Proc. ESCA Workshop on Accessing
Information in Spoken Audio, pages 107–110.
G. Stasser and LA Taylor. 1991. Speaking turns in face-
to-face discussions. Journal of Personality and Social
Psychology, 60(5):675–684.
S. Tucker, O. Bergman, A. Ramamoorthy, and S. Whit-
taker. 2010. Catchup: a useful application of time-
travel in meetings. In Proc. ACM CSCW, pages 99–
102.
S. Uchihashi, J. Foote, A. Girgensohn, and J. Boreczky.
1999. Video manga: generating semantically mean-
ingful video summaries. In Proc. ACM Multimedia,
pages 383–392.
A. Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf,
T. Schultz, H. Soltau, H. Yu, and K. Zechner. 2001.
Advances in automatic meeting record creation and ac-
cess. In Proc IEEE ICASSP.
V. Wan and T. Hain. 2006. Strategies for language model
web-data collection. In Proc IEEE ICASSP.
P. Wellner, M. Flynn, S. Tucker, and S. Whittaker. 2005.
A meeting browser evaluation test. In Proc. ACM CHI,
pages 2021–2024.
M. W¨olfel and J. McDonough. 2009. Distant Speech
Recognition. Wiley.
C. Wooters and M. Huijbregts. 2007. The ICSI RT07s
speaker diarization system. In Multimodal Technolo-
gies for Perception of Humans. International Evalu-
ation Workshops CLEAR 2007 and RT 2007, volume
4625 of LNCS, pages 509–519. Springer.
S. Wrigley, G. Brown, V. Wan, and S. Renals. 2005.
Speech and crosstalk detection in multichannel audio.
IEEE Transactions on Speech and Audio Processing,
13(1):84–91.
R. Yong, A. Gupta, and J. Cadiz. 2001. Viewing meet-
ings captured by an omni-directional camera. ACM
Transactions on Computing Human Interaction.
E. Zwyssig, M. Lincoln, and S. Renals. 2010. A digital
microphone array for distant speech recognition. In
Proc. IEEE ICASSP–10.
</reference>
<page confidence="0.997085">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.512598">
<title confidence="0.999444">Recognition and Understanding of Meetings</title>
<author confidence="0.98387">Steve Renals</author>
<affiliation confidence="0.994229">Centre for Speech Technology Research, University of</affiliation>
<address confidence="0.911107">Informatics Forum, 10 Crichton Street, Edinburgh EH8 9AB, UK</address>
<email confidence="0.657863">s.renals@ed.ac.ukhomepages.inf.ed.ac.uk/srenals/</email>
<abstract confidence="0.99043595">This paper is about interpreting human communication in meetings using audio, video and other signals. Automatic meeting recognition and understanding is extremely challenging, since communication in a meeting is spontaneous and conversational, and involves multiple speakers and multiple modalities. This leads to a number of significant research problems in signal processing, in speech recognition, and in discourse interpretation, taking account of both individual and group behaviours. Addressing these problems requires an interdisciplinary effort. In this paper, I discuss the capture and annotation of multimodal meeting recordings—resulting in the AMI meeting corpus—and how we have built on this to develop techniques and applications for the recognition and interpretation of meetings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Al-Hames</author>
<author>C Lenz</author>
<author>S Reiter</author>
<author>J Schenk</author>
<author>F Wallhoff</author>
<author>G Rigoll</author>
</authors>
<title>Robust multi-modal group action recognition in meetings from disturbed videos with the asynchronous hidden Markov model.</title>
<date>2007</date>
<booktitle>In Proc IEEE ICIP.</booktitle>
<contexts>
<context position="10600" citStr="Al-Hames et al., 2007" startWordPosition="1625" endWordPosition="1628">nd is released under a licence that is based on the Creative Commons Attribution NonCommercial ShareAlike 2.5 Licence. This includes all the signals and manual annotations, plus a number of automatic annotations (e.g. speech recognition) made available to lower the startup cost of performing research on the corpus. 3 Multimodal recognition The predominant motivation behind the collection and annotation of the AMI corpus was to enable the development of multimodal recognisers to address issues such as speech recognition, speaker diarisartion (Wooters and Huijbregts, 2007), gesture recognition (Al-Hames et al., 2007) and focus of attention (Ba and Odobez, 2008). Although speech recognition is based on the (multichannel) audio signal, the other problems can be successfully addressed by combining modalities. (There is certainly information in other modalities that has the potential to make speech recognition more accurate, but so far we have not been able to use it consistently and robustly.) Speech recognition: The automatic transcription of what is spoken in a meeting is an essential prerequisite to interpreting a meeting. Morgan et al (2003) described the speech recognition of meetings as an “ASR-complet</context>
</contexts>
<marker>Al-Hames, Lenz, Reiter, Schenk, Wallhoff, Rigoll, 2007</marker>
<rawString>M. Al-Hames, C. Lenz, S. Reiter, J. Schenk, F. Wallhoff, and G. Rigoll. 2007. Robust multi-modal group action recognition in meetings from disturbed videos with the asynchronous hidden Markov model. In Proc IEEE ICIP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S O Ba</author>
<author>J M Odobez</author>
</authors>
<title>Multi-party focus of attention recognition in meetings from head pose and multimodal contextual cues.</title>
<date>2008</date>
<booktitle>In Proc. IEEE ICASSP.</booktitle>
<contexts>
<context position="10645" citStr="Ba and Odobez, 2008" startWordPosition="1633" endWordPosition="1636"> the Creative Commons Attribution NonCommercial ShareAlike 2.5 Licence. This includes all the signals and manual annotations, plus a number of automatic annotations (e.g. speech recognition) made available to lower the startup cost of performing research on the corpus. 3 Multimodal recognition The predominant motivation behind the collection and annotation of the AMI corpus was to enable the development of multimodal recognisers to address issues such as speech recognition, speaker diarisartion (Wooters and Huijbregts, 2007), gesture recognition (Al-Hames et al., 2007) and focus of attention (Ba and Odobez, 2008). Although speech recognition is based on the (multichannel) audio signal, the other problems can be successfully addressed by combining modalities. (There is certainly information in other modalities that has the potential to make speech recognition more accurate, but so far we have not been able to use it consistently and robustly.) Speech recognition: The automatic transcription of what is spoken in a meeting is an essential prerequisite to interpreting a meeting. Morgan et al (2003) described the speech recognition of meetings as an “ASR-complete” problem. Developing an accurate system for</context>
</contexts>
<marker>Ba, Odobez, 2008</marker>
<rawString>S. O. Ba and J. M. Odobez. 2008. Multi-party focus of attention recognition in meetings from head pose and multimodal contextual cues. In Proc. IEEE ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Bales</author>
</authors>
<title>Interaction Process Analysis.</title>
<date>1951</date>
<publisher>Addison Wesley,</publisher>
<location>Cambridge MA, USA.</location>
<contexts>
<context position="2927" citStr="Bales, 1951" startWordPosition="450" endWordPosition="451"> record involves the capture of data across several modalities. It is a key engineering challenge to capture such multimodal signals in a reliable, unobtrusive and flexible way, but the greater challenges arise from unlocking the multimodal recordings. If such recordings are not transcribed and indexed (at the least), then access merely corresponds to replay. And it is rare that people will have the time, or the inclination, to replay a meeting. There is a long and interesting thread of research which is concerned to better understand the dynamics of meetings and the way that groups function (Bales, 1951; McGrath, 1991; Stasser and Taylor, 1991). The types of analyses and studies carried out by these authors is still someway beyond what we can do automatically. The first significant work on automatic processing of meetings, coupled with an exploration of how people might interact with an archive of recorded meetings, was performed in the mid 1990s (Kazman et al., 1996). This work was limited by the fact that it was not possible at Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1–9, Los Angeles, California, June 2010. c�2010 Association </context>
</contexts>
<marker>Bales, 1951</marker>
<rawString>R. F. Bales. 1951. Interaction Process Analysis. Addison Wesley, Cambridge MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Bulyko</author>
<author>M Ostendorf</author>
<author>M Siu</author>
<author>T Ng</author>
<author>A Stolcke</author>
<author>O Cetin</author>
</authors>
<title>Web resources for language modeling in conversational speech recognition.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="13125" citStr="Bulyko et al., 2007" startWordPosition="1995" endWordPosition="1998">based on maximum likelihood linear transforms, and further training using a discriminative minimum Bayes risk criterion such as minimum phone error rate (Gales and Young, 2007; Renals and Hain, 2010). In addition we have employed a number of novel acoustic parameterisations including approaches based on local posterior probability estimation (Grezl et al., 2007) and pitch adaptive features (Garau and Renals, 2008), the automatic construction of domain-specific language models using documents obtained from the web by searching with n-grams obtained from meeting transcripts (Wan and Hain, 2006; Bulyko et al., 2007), and automatic approaches to acoustic segmentation optimised for meetings (Wrigley et al., 2005; Dines et al., 2006). A feature of the systems developed for meeting recognition is the use of multiple recognition passes, cross-adaptation and model combination (Hain et al., 2007). In particular successive passes make use of more detailed—and more diverse—acoustic and language models. Different acoustic models trained on different feature representations (e.g. standard PLP features and posterior probability-based features) are cross-adapted, and different feature representations are also combine</context>
</contexts>
<marker>Bulyko, Ostendorf, Siu, Ng, Stolcke, Cetin, 2007</marker>
<rawString>I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and O. Cetin. 2007. Web resources for language modeling in conversational speech recognition. ACM Transactions on Speech and Language Processing, 5(1):1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation,</title>
<date>2007</date>
<pages>41--181</pages>
<contexts>
<context position="7152" citStr="Carletta, 2007" startWordPosition="1097" endWordPosition="1098">ns, whiteboards, shared laptop spaces, data projector and videoconferencing if used. A considerable amount of hardware was necessary for ensuring frame-level synchronisation. More recently we have used a lighter weight setup, that uses a high resolution spherical digital video camera system, and a single microphone array (7– 20 elements, depending on meeting size) synchronised using software. We have also constructed a prototype system using a low-cost, flexible array of digital MEMS microphones (Zwyssig et al., 2010). We used these instrumented meeting rooms to record the AMI Meeting Corpus (Carletta, 2007). This corpus contains over 100 hours of meeting recordings, with the different recording streams synchronised to a common timeline. The corpus contains a number of manually created and automatic annotations, synchronised to the same timeline. This 2 includes a high-quality manual word-level transcription of the complete corpus, as well as reference automatic speech recognition output, using the speech recognition system discussed in section 3 (using 5- fold cross-validation). In addition to word-level transcriptions, the corpus includes manual annotations that describe the behaviour of meetin</context>
</contexts>
<marker>Carletta, 2007</marker>
<rawString>J. Carletta. 2007. Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation, 41:181–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Castronovo</author>
<author>J Frey</author>
<author>P Poller</author>
</authors>
<title>A generic layout-tool for summaries of meetings in a constraintbased approach.</title>
<date>2008</date>
<booktitle>In Machine Learning for Multimodal Interaction (Proc. MLMI ’08).</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="22070" citStr="Castronovo et al., 2008" startWordPosition="3368" endWordPosition="3371">ated summaries in meeting browsers. The browser illustrated in figure 2 (b) enables navigation by the summarised transcript or via the topic segmentation. In this case the degree of summarisation is controlled by a slider, which removes those speech segments that do no contribute to the summary. We have also explored real-time (with a few utterances latency) approaches to summarisation, to facilitate meeting “catchup” scenarios, including the generation of audio only summaries, with about 60% of the speech removed (Tucker et al., 2010). Visualisations of summaries include a comic book layout (Castronovo et al., 2008), illustrated in figure 3. This is related to “VideoManga” (Uchihashi et al., 1999), but driven by transcribed speech rather than visually identified keyframes. The availability of real-time meeting speech recognition, with phrase-level latency (Garner et al., 2009), enables a new class of applications. Within AMIDA we developed a software architecture referred to as “The Hub” to support real-time applications. The Hub is essentially a real-time annotation server, mediating between annotation producers, such as speech recognition, and annotation consumers, such as a real-time catchup browser. </context>
</contexts>
<marker>Castronovo, Frey, Poller, 2008</marker>
<rawString>S. Castronovo, J. Frey, and P. Poller. 2008. A generic layout-tool for summaries of meetings in a constraintbased approach. In Machine Learning for Multimodal Interaction (Proc. MLMI ’08). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cutler</author>
<author>Y Rui</author>
<author>A Gupta</author>
<author>J Cadiz</author>
<author>I Tashev</author>
<author>L He</author>
<author>A Colburn</author>
<author>Z Zhang</author>
<author>Z Liu</author>
<author>S Silverberg</author>
</authors>
<title>Distributed meetings: a meeting capture and broadcasting system.</title>
<date>2002</date>
<booktitle>In Proc. ACM Multimedia,</booktitle>
<pages>503--512</pages>
<contexts>
<context position="3744" citStr="Cutler et al., 2002" startWordPosition="578" endWordPosition="581">ic processing of meetings, coupled with an exploration of how people might interact with an archive of recorded meetings, was performed in the mid 1990s (Kazman et al., 1996). This work was limited by the fact that it was not possible at Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that time to transcribe meeting speech automatically. Other early work in the area concentrated on the multimodal capture and broadcast of meetings (Roy and Luz, 1999; Cutler et al., 2002; Yong et al., 2001). Three groups further developed approaches to automatically index the content of meetings. A team at Fuji Xerox PARC used video retrieval techniques such as keyframing to automatically generate manga-style summaries of meetings (Uchihashi et al., 1999), Waibel and colleagues at CMU used speech recognition and video tracking for meetings (Waibel et al., 2001), and Morgan and colleagues at ICSI focused on audio-only capture and speech recognition (Morgan et al., 2003). Since 2003 research in the recognition and understanding of meetings has developed substantially, stimulate</context>
</contexts>
<marker>Cutler, Rui, Gupta, Cadiz, Tashev, He, Colburn, Zhang, Liu, Silverberg, 2002</marker>
<rawString>R. Cutler, Y. Rui, A. Gupta, J. Cadiz, I. Tashev, L. He, A. Colburn, Z. Zhang, Z. Liu, and S. Silverberg. 2002. Distributed meetings: a meeting capture and broadcasting system. In Proc. ACM Multimedia, pages 503– 512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dielmann</author>
<author>S Renals</author>
</authors>
<title>Automatic meeting segmentation using dynamic Bayesian networks.</title>
<date>2007</date>
<journal>IEEE Transactions on Multimedia,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="16373" citStr="Dielmann and Renals, 2007" startWordPosition="2482" endWordPosition="2486">r first investigations in using multimodal multistream models to segment meetings. Meetings are group events, characterised by both individual actions and group actions. To obtain structure at the group level, we and colleagues in the M4 and AMI projects investigated segmenting a meeting into a sequence of group actions such as monologue, discussion and presentation (McCowan et al., 2005). We used a number of feature streams for this segmentation and labelling task including speaker turn dynamics, prosody, lexical information, 8juicer.amiproject.org/ 4 and participant head and hand movements (Dielmann and Renals, 2007). Our initial experiments used an HMM to model the feature streams with a single hidden state space, and resulted in an “action error rate” of over 40% (action error rate is analogous to word error rate, but defined over meeting actions, presumed not to overlap). The HMM was then substituted by a richer DBN multistream model in which each feature stream was processed independently at a lower level of the model. These partial results were then combined at a higher level, thus providing hierarchical integration of the multimodal feature streams. This multistream approach enabled a later integrat</context>
</contexts>
<marker>Dielmann, Renals, 2007</marker>
<rawString>A. Dielmann and S. Renals. 2007. Automatic meeting segmentation using dynamic Bayesian networks. IEEE Transactions on Multimedia, 9(1):25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dielmann</author>
<author>S Renals</author>
</authors>
<title>Recognition of dialogue acts in multiparty meetings using a switching DBN.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>16</volume>
<issue>7</issue>
<contexts>
<context position="18259" citStr="Dielmann and Renals, 2008" startWordPosition="2788" endWordPosition="2791">s or offers, commenting on the discussion, social acts, backchannels, or “other”. The segmentation problem is non-trivial, since a single stretch of speech (with no pauses) from a speaker may comprise several dialogue acts—and conversely a single dialogue act may contain pauses. To address the tasks of automatically segmenting the speech into dialogue acts, and assigning a label to each segment, we employed a switching dynamic Bayesian network architecture, which modelled a set of features related to lexical content and prosody and incorporates a weighted interpolated factored language model (Dielmann and Renals, 2008). The switching DBN coordinated the recognition process by integrating all the available resources. This approach was able to leverage additional corpora of conversational data by using them as training data for a factored language model which was used in conjunction with additional task specific language models. We followed this joint generative model, with a discriminative approach, based on conditional random fields, which performed a reclassification of the segmented dialogue acts. Our experiments on dialogue act recognition used both automatic and manual transcriptions of the AMI corpus. </context>
</contexts>
<marker>Dielmann, Renals, 2008</marker>
<rawString>A. Dielmann and S. Renals. 2008. Recognition of dialogue acts in multiparty meetings using a switching DBN. IEEE Transactions on Audio, Speech and Language Processing, 16(7):1303–1314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dines</author>
<author>J Vepa</author>
<author>T Hain</author>
</authors>
<title>The segmentation of multi-channel meeting recordings for automatic speech recognition.</title>
<date>2006</date>
<booktitle>In Proc. Interspeech.</booktitle>
<contexts>
<context position="13242" citStr="Dines et al., 2006" startWordPosition="2014" endWordPosition="2017">n such as minimum phone error rate (Gales and Young, 2007; Renals and Hain, 2010). In addition we have employed a number of novel acoustic parameterisations including approaches based on local posterior probability estimation (Grezl et al., 2007) and pitch adaptive features (Garau and Renals, 2008), the automatic construction of domain-specific language models using documents obtained from the web by searching with n-grams obtained from meeting transcripts (Wan and Hain, 2006; Bulyko et al., 2007), and automatic approaches to acoustic segmentation optimised for meetings (Wrigley et al., 2005; Dines et al., 2006). A feature of the systems developed for meeting recognition is the use of multiple recognition passes, cross-adaptation and model combination (Hain et al., 2007). In particular successive passes make use of more detailed—and more diverse—acoustic and language models. Different acoustic models trained on different feature representations (e.g. standard PLP features and posterior probability-based features) are cross-adapted, and different feature representations are also combined using linear transforms such as heteroscedastic linear discriminant analysis (Kumar and Andreou, 1998). These syste</context>
</contexts>
<marker>Dines, Vepa, Hain, 2006</marker>
<rawString>J. Dines, J. Vepa, and T. Hain. 2006. The segmentation of multi-channel meeting recordings for automatic speech recognition. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J F Gales</author>
<author>S J Young</author>
</authors>
<title>The application of hidden Markov models in speech recognition.</title>
<date>2007</date>
<booktitle>Foundations and Trends in Signal Processing,</booktitle>
<volume>1</volume>
<issue>3</issue>
<pages>304</pages>
<contexts>
<context position="12680" citStr="Gales and Young, 2007" startWordPosition="1929" endWordPosition="1932"> individual microphone signals are filtered and summed to enhance signals coming from a particular direction, while suppressing signals from competing locations (W¨olfel and McDonough, 2009). The core acoustic and language modelling components for meeting speech recognition correspond quite closely to the state-of-the-art systems used in other domains. Acoustic modelling techniques include vocal tract length normalisation, speaker adaptation based on maximum likelihood linear transforms, and further training using a discriminative minimum Bayes risk criterion such as minimum phone error rate (Gales and Young, 2007; Renals and Hain, 2010). In addition we have employed a number of novel acoustic parameterisations including approaches based on local posterior probability estimation (Grezl et al., 2007) and pitch adaptive features (Garau and Renals, 2008), the automatic construction of domain-specific language models using documents obtained from the web by searching with n-grams obtained from meeting transcripts (Wan and Hain, 2006; Bulyko et al., 2007), and automatic approaches to acoustic segmentation optimised for meetings (Wrigley et al., 2005; Dines et al., 2006). A feature of the systems developed f</context>
</contexts>
<marker>Gales, Young, 2007</marker>
<rawString>M. J. F. Gales and S. J. Young. 2007. The application of hidden Markov models in speech recognition. Foundations and Trends in Signal Processing, 1(3):195– 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Garau</author>
<author>S Renals</author>
</authors>
<title>Combining spectral representations for large vocabulary continuous speech recognition.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="12922" citStr="Garau and Renals, 2008" startWordPosition="1965" endWordPosition="1968">nts for meeting speech recognition correspond quite closely to the state-of-the-art systems used in other domains. Acoustic modelling techniques include vocal tract length normalisation, speaker adaptation based on maximum likelihood linear transforms, and further training using a discriminative minimum Bayes risk criterion such as minimum phone error rate (Gales and Young, 2007; Renals and Hain, 2010). In addition we have employed a number of novel acoustic parameterisations including approaches based on local posterior probability estimation (Grezl et al., 2007) and pitch adaptive features (Garau and Renals, 2008), the automatic construction of domain-specific language models using documents obtained from the web by searching with n-grams obtained from meeting transcripts (Wan and Hain, 2006; Bulyko et al., 2007), and automatic approaches to acoustic segmentation optimised for meetings (Wrigley et al., 2005; Dines et al., 2006). A feature of the systems developed for meeting recognition is the use of multiple recognition passes, cross-adaptation and model combination (Hain et al., 2007). In particular successive passes make use of more detailed—and more diverse—acoustic and language models. Different a</context>
</contexts>
<marker>Garau, Renals, 2008</marker>
<rawString>G. Garau and S. Renals. 2008. Combining spectral representations for large vocabulary continuous speech recognition. IEEE Transactions on Audio, Speech and Language Processing, 16(3):508–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Garner</author>
<author>J Dines</author>
<author>T Hain</author>
<author>A El Hannani</author>
<author>M Karafiat</author>
<author>D Korchagin</author>
<author>M Lincoln</author>
<author>V Wan</author>
<author>L Zhang</author>
</authors>
<title>Real-time ASR from meetings.</title>
<date>2009</date>
<booktitle>In Proc. Interspeech.</booktitle>
<marker>Garner, Dines, Hain, El Hannani, Karafiat, Korchagin, Lincoln, Wan, Zhang, 2009</marker>
<rawString>P. Garner, J. Dines, T. Hain, A. El Hannani, M. Karafiat, D. Korchagin, M. Lincoln, V. Wan, and L. Zhang. 2009. Real-time ASR from meetings. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Grezl</author>
<author>M Karafiat</author>
<author>S Kontar</author>
<author>J Cernocky</author>
</authors>
<title>Probabilistic and bottle-neck features for lvcsr of meetings.</title>
<date>2007</date>
<booktitle>In Acoustics, Speech and Signal Processing,</booktitle>
<volume>4</volume>
<pages>757--760</pages>
<contexts>
<context position="12869" citStr="Grezl et al., 2007" startWordPosition="1957" endWordPosition="1960"> The core acoustic and language modelling components for meeting speech recognition correspond quite closely to the state-of-the-art systems used in other domains. Acoustic modelling techniques include vocal tract length normalisation, speaker adaptation based on maximum likelihood linear transforms, and further training using a discriminative minimum Bayes risk criterion such as minimum phone error rate (Gales and Young, 2007; Renals and Hain, 2010). In addition we have employed a number of novel acoustic parameterisations including approaches based on local posterior probability estimation (Grezl et al., 2007) and pitch adaptive features (Garau and Renals, 2008), the automatic construction of domain-specific language models using documents obtained from the web by searching with n-grams obtained from meeting transcripts (Wan and Hain, 2006; Bulyko et al., 2007), and automatic approaches to acoustic segmentation optimised for meetings (Wrigley et al., 2005; Dines et al., 2006). A feature of the systems developed for meeting recognition is the use of multiple recognition passes, cross-adaptation and model combination (Hain et al., 2007). In particular successive passes make use of more detailed—and m</context>
</contexts>
<marker>Grezl, Karafiat, Kontar, Cernocky, 2007</marker>
<rawString>F. Grezl, M. Karafiat, S. Kontar, and J. Cernocky. 2007. Probabilistic and bottle-neck features for lvcsr of meetings. In Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, volume 4, pages IV–757–IV–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hain</author>
<author>L Burget</author>
<author>J Dines</author>
<author>G Garau</author>
<author>M Karafiat</author>
<author>M Lincoln</author>
<author>J Vepa</author>
<author>V Wan</author>
</authors>
<title>The ami system for the transcription of speech in meetings.</title>
<date>2007</date>
<booktitle>In Proc. IEEE ICASSP–07.</booktitle>
<contexts>
<context position="13404" citStr="Hain et al., 2007" startWordPosition="2038" endWordPosition="2041">ing approaches based on local posterior probability estimation (Grezl et al., 2007) and pitch adaptive features (Garau and Renals, 2008), the automatic construction of domain-specific language models using documents obtained from the web by searching with n-grams obtained from meeting transcripts (Wan and Hain, 2006; Bulyko et al., 2007), and automatic approaches to acoustic segmentation optimised for meetings (Wrigley et al., 2005; Dines et al., 2006). A feature of the systems developed for meeting recognition is the use of multiple recognition passes, cross-adaptation and model combination (Hain et al., 2007). In particular successive passes make use of more detailed—and more diverse—acoustic and language models. Different acoustic models trained on different feature representations (e.g. standard PLP features and posterior probability-based features) are cross-adapted, and different feature representations are also combined using linear transforms such as heteroscedastic linear discriminant analysis (Kumar and Andreou, 1998). These systems have been evaluated in successive NIST RT evaluations: the core microphone array based system has a word error rate of about 40%; after adaptation and feature </context>
</contexts>
<marker>Hain, Burget, Dines, Garau, Karafiat, Lincoln, Vepa, Wan, 2007</marker>
<rawString>T. Hain, L. Burget, J. Dines, G. Garau, M. Karafiat, M. Lincoln, J. Vepa, and V. Wan. 2007. The ami system for the transcription of speech in meetings. In Proc. IEEE ICASSP–07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Huang</author>
<author>S Renals</author>
</authors>
<title>Unsupervised language model adaptation based on topic and role information in multiparty meetings.</title>
<date>2008</date>
<booktitle>In Proc. Interspeech ’08.</booktitle>
<contexts>
<context position="29054" citStr="Huang and Renals, 2008" startWordPosition="4473" endWordPosition="4476">mplex, multimodal tasks. Meetings serve a social function, and much of this has been ignored in our work, so far. We have focussed principally on understanding meetings in terms of their lexical content, augmented by various multimodal streams. However in many interactions, the social signals are at least as important as the propositional content of the words (Pentland, 2008); it is a major challenge to develop meeting interpretation components that can infer and take advantage of such social cues. We have made initial attempts to do this, by attempting to include aspects such as social role (Huang and Renals, 2008). The AMI corpus involved a substantial effort from many individuals, and provides an invaluable resource. However, we do not wish to do this again, even if we are dealing with a domain that is significantly different, such as larger groups, or family “meetings”. However, our recognisers rely strongly on annotated in-domain data. It is a major challenge to develop algorithms that are unsupervised and adaptive to free us from the need to collect and annotate large amount of data each time we are interested in a new domain. Acknowledgments This paper has arisen from a collaboration involving sev</context>
</contexts>
<marker>Huang, Renals, 2008</marker>
<rawString>S. Huang and S. Renals. 2008. Unsupervised language model adaptation based on topic and role information in multiparty meetings. In Proc. Interspeech ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kazman</author>
<author>R Al-Halimi</author>
<author>W Hunt</author>
<author>M Mantei</author>
</authors>
<title>Four paradigms for indexing video conferences.</title>
<date>1996</date>
<pages>3--1</pages>
<publisher>Multimedia, IEEE,</publisher>
<contexts>
<context position="3299" citStr="Kazman et al., 1996" startWordPosition="508" endWordPosition="511">nd it is rare that people will have the time, or the inclination, to replay a meeting. There is a long and interesting thread of research which is concerned to better understand the dynamics of meetings and the way that groups function (Bales, 1951; McGrath, 1991; Stasser and Taylor, 1991). The types of analyses and studies carried out by these authors is still someway beyond what we can do automatically. The first significant work on automatic processing of meetings, coupled with an exploration of how people might interact with an archive of recorded meetings, was performed in the mid 1990s (Kazman et al., 1996). This work was limited by the fact that it was not possible at Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that time to transcribe meeting speech automatically. Other early work in the area concentrated on the multimodal capture and broadcast of meetings (Roy and Luz, 1999; Cutler et al., 2002; Yong et al., 2001). Three groups further developed approaches to automatically index the content of meetings. A team at Fuji Xerox PARC used video retrie</context>
</contexts>
<marker>Kazman, Al-Halimi, Hunt, Mantei, 1996</marker>
<rawString>R. Kazman, R. Al-Halimi, W. Hunt, and M. Mantei. 1996. Four paradigms for indexing video conferences. Multimedia, IEEE, 3(1):63–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kumar</author>
<author>A G Andreou</author>
</authors>
<title>Heteroscedastic discriminant analysis and reduced rank HMMs for improved recognition.</title>
<date>1998</date>
<journal>Speech Communication,</journal>
<volume>26</volume>
<pages>297</pages>
<contexts>
<context position="13829" citStr="Kumar and Andreou, 1998" startWordPosition="2092" endWordPosition="2095">ley et al., 2005; Dines et al., 2006). A feature of the systems developed for meeting recognition is the use of multiple recognition passes, cross-adaptation and model combination (Hain et al., 2007). In particular successive passes make use of more detailed—and more diverse—acoustic and language models. Different acoustic models trained on different feature representations (e.g. standard PLP features and posterior probability-based features) are cross-adapted, and different feature representations are also combined using linear transforms such as heteroscedastic linear discriminant analysis (Kumar and Andreou, 1998). These systems have been evaluated in successive NIST RT evaluations: the core microphone array based system has a word error rate of about 40%; after adaptation and feature combination steps, this error rate can be reduced to about 30%. The equivalent close-talking microphone system has baseline word error rate of about 35%, reduced to less than 25% after further recognition passes (Hain et al., 2007). The core system runs about five times slower than real-time, and the full system is about fourteen times slower than real-time, on current commodity hardware. We have developed a low-latency r</context>
</contexts>
<marker>Kumar, Andreou, 1998</marker>
<rawString>N. Kumar and A. G. Andreou. 1998. Heteroscedastic discriminant analysis and reduced rank HMMs for improved recognition. Speech Communication, 26:283– 297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I McCowan</author>
<author>D Gatica-Perez</author>
<author>S Bengio</author>
<author>G Lathoud</author>
<author>M Barnard</author>
<author>D Zhang</author>
</authors>
<title>Automatic analysis of multimodal group actions in meetings.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="16138" citStr="McCowan et al., 2005" startWordPosition="2450" endWordPosition="2453">, for example into speech and non-speech (an essential pre-processing for speech recognition), into utterances spoken by a single talker, into dialogue acts, into topics, and into “meeting phases”. The latter was the subject of our first investigations in using multimodal multistream models to segment meetings. Meetings are group events, characterised by both individual actions and group actions. To obtain structure at the group level, we and colleagues in the M4 and AMI projects investigated segmenting a meeting into a sequence of group actions such as monologue, discussion and presentation (McCowan et al., 2005). We used a number of feature streams for this segmentation and labelling task including speaker turn dynamics, prosody, lexical information, 8juicer.amiproject.org/ 4 and participant head and hand movements (Dielmann and Renals, 2007). Our initial experiments used an HMM to model the feature streams with a single hidden state space, and resulted in an “action error rate” of over 40% (action error rate is analogous to word error rate, but defined over meeting actions, presumed not to overlap). The HMM was then substituted by a richer DBN multistream model in which each feature stream was proce</context>
</contexts>
<marker>McCowan, Gatica-Perez, Bengio, Lathoud, Barnard, Zhang, 2005</marker>
<rawString>I. McCowan, D. Gatica-Perez, S. Bengio, G. Lathoud, M. Barnard, and D. Zhang. 2005. Automatic analysis of multimodal group actions in meetings. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(3):305–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E McGrath</author>
</authors>
<title>Time, interaction, and performance (TIP): A theory of groups.</title>
<date>1991</date>
<journal>Small Group Research,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="2942" citStr="McGrath, 1991" startWordPosition="452" endWordPosition="453">ves the capture of data across several modalities. It is a key engineering challenge to capture such multimodal signals in a reliable, unobtrusive and flexible way, but the greater challenges arise from unlocking the multimodal recordings. If such recordings are not transcribed and indexed (at the least), then access merely corresponds to replay. And it is rare that people will have the time, or the inclination, to replay a meeting. There is a long and interesting thread of research which is concerned to better understand the dynamics of meetings and the way that groups function (Bales, 1951; McGrath, 1991; Stasser and Taylor, 1991). The types of analyses and studies carried out by these authors is still someway beyond what we can do automatically. The first significant work on automatic processing of meetings, coupled with an exploration of how people might interact with an archive of recorded meetings, was performed in the mid 1990s (Kazman et al., 1996). This work was limited by the fact that it was not possible at Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computation</context>
</contexts>
<marker>McGrath, 1991</marker>
<rawString>J. E. McGrath. 1991. Time, interaction, and performance (TIP): A theory of groups. Small Group Research, 22(2):147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Morgan</author>
<author>D Baron</author>
<author>S Bhagat</author>
<author>H Carvey</author>
<author>R Dhillon</author>
<author>J Edwards</author>
<author>D Gelbart</author>
<author>A Janin</author>
<author>A Krupski</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>Meetings about meetings: research at ICSI on speech in multiparty conversations.</title>
<date>2003</date>
<booktitle>In Proc. IEEE ICASSP.</booktitle>
<contexts>
<context position="4235" citStr="Morgan et al., 2003" startWordPosition="656" endWordPosition="659">r early work in the area concentrated on the multimodal capture and broadcast of meetings (Roy and Luz, 1999; Cutler et al., 2002; Yong et al., 2001). Three groups further developed approaches to automatically index the content of meetings. A team at Fuji Xerox PARC used video retrieval techniques such as keyframing to automatically generate manga-style summaries of meetings (Uchihashi et al., 1999), Waibel and colleagues at CMU used speech recognition and video tracking for meetings (Waibel et al., 2001), and Morgan and colleagues at ICSI focused on audio-only capture and speech recognition (Morgan et al., 2003). Since 2003 research in the recognition and understanding of meetings has developed substantially, stimulated by evaluation campaigns such as the NIST Rich Transcription (RT)1 and CLEAR2 evaluations, as well as some large multidisciplinary projects such as AMI/AMIDA3, CHIL4 and CALO5. This paper is about the work we have carried out in meeting capture, recognition and interpretation within the AMI and AMIDA projects since 2004. One of the principal outputs of these projects was a multimodal corpus of meeting recordings, annotated at a number of different levels. In section 2 we discuss collec</context>
<context position="11136" citStr="Morgan et al (2003)" startWordPosition="1711" endWordPosition="1714">sartion (Wooters and Huijbregts, 2007), gesture recognition (Al-Hames et al., 2007) and focus of attention (Ba and Odobez, 2008). Although speech recognition is based on the (multichannel) audio signal, the other problems can be successfully addressed by combining modalities. (There is certainly information in other modalities that has the potential to make speech recognition more accurate, but so far we have not been able to use it consistently and robustly.) Speech recognition: The automatic transcription of what is spoken in a meeting is an essential prerequisite to interpreting a meeting. Morgan et al (2003) described the speech recognition of meetings as an “ASR-complete” problem. Developing an accurate system for meeting recognition involves the automatic segmentation of the recording into utterances from a single talker, robustness to reverberation and competing acoustic sources, handling overlapping talkers, exploitation of multiple microphone recordings, as well as the core acoustic and language modelling problems that arise when attempting to recognise spontaneous, conversational speech. Our initial systems for meeting recognition used audio recorded with close-talking microphones, in order</context>
</contexts>
<marker>Morgan, Baron, Bhagat, Carvey, Dhillon, Edwards, Gelbart, Janin, Krupski, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>N. Morgan, D. Baron, S. Bhagat, H. Carvey, R. Dhillon, J. Edwards, D. Gelbart, A. Janin, A. Krupski, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. Meetings about meetings: research at ICSI on speech in multiparty conversations. In Proc. IEEE ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Moore</author>
<author>J Carletta</author>
</authors>
<title>Incorporating speaker and discourse features into speech summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>367--374</pages>
<contexts>
<context position="20020" citStr="Murray et al., 2006" startWordPosition="3056" endWordPosition="3059"> automatic generation of summaries provides a natural way to succinctly describe the content of a meeting, and can be an efficient way for users to obtain information. We have focussed on extractive techniques to construct summaries, in which the most relevant parts of a meeting are located, and concatenated together to provide a ‘cut-and-paste’ summary, which may be textual or multimodal. Our approach to extractive summarisation is based on automatically extracting relevant dialogue acts (or alternatively “spurts”, segments spoken by a single speaker and delimited by silence) from a meeting (Murray et al., 2006). This requires (as a minimum) the automatic speech transcription and, if spurts are not used, dialogue act segmentation. Lexical information is clearly extremely important for summarisation, but we have also found speaker features (relating to activity, dominance and overlap), structural features (the length and position of dialogue acts), prosody, and discourse cues (phrases which signal likely relevance) to be important for the development of accurate methods for extractive summarisation of meetings. Furthermore we have explored reduced dimension representations of text, based on latent sem</context>
</contexts>
<marker>Murray, Renals, Moore, Carletta, 2006</marker>
<rawString>G. Murray, S. Renals, J. Moore, and J. Carletta. 2006. Incorporating speaker and discourse features into speech summarization. In Proceedings of the Human Language Technology Conference of the NAACL, pages 367–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>T Kleinbauer</author>
<author>P Poller</author>
<author>T Becker</author>
<author>S Renals</author>
<author>J Kilgour</author>
</authors>
<title>Extrinsic summarization evaluation: A decision audit task.</title>
<date>2009</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="26083" citStr="Murray et al., 2009" startWordPosition="3989" endWordPosition="3992">ns of interest” relating to a recorded meeting, you we could using the browser under test with a specified time whats beter yeah i men pla plastic s limit(typically half the meeting length). rubber o you could go We developed of a variant of the BET to specifiFigure 4: Demonstration screenshot of the AMI auton matic content linking device. The subpanels show (clockwise from top left) the ASR transcript, relevant documents from the meeting document base, relevant web hits and a a tag cloud. well i figure if we cally evaluate different summarisation approaches. In the Decision Audit evaluation (Murray et al., 2009) the user’s task is to ascertain the factors across yeah a number of meetings that lead to a particular decision being made. A set of browsers were constructed differing in the summarisation approach employed (manual vs. ASR transcripts; extractive vs. abstractive vs. human vs. keyword-based summarisation), Buttons and the test subjects used them to perform the decision audit. Like the BET this evaluation is labourbcause you can build intensive, but the results can be analysed using a butto and some st battery of objective and subjective measures. Conwant to watc two yeah which channels ts ou </context>
</contexts>
<marker>Murray, Kleinbauer, Poller, Becker, Renals, Kilgour, 2009</marker>
<rawString>G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals, and J. Kilgour. 2009. Extrinsic summarization evaluation: A decision audit task. ACM Transactions on Speech and Language Processing, 6(2):1–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Pentland</author>
</authors>
<title>Honest signals: how they shape our world.</title>
<date>2008</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="28809" citStr="Pentland, 2008" startWordPosition="4432" endWordPosition="4433">ic approaches to meeting recognition and characterisation are characterised by extensive combination at the feature stream, model and system level. In our experience, such approaches offer consistent improvements in accuracy for these complex, multimodal tasks. Meetings serve a social function, and much of this has been ignored in our work, so far. We have focussed principally on understanding meetings in terms of their lexical content, augmented by various multimodal streams. However in many interactions, the social signals are at least as important as the propositional content of the words (Pentland, 2008); it is a major challenge to develop meeting interpretation components that can infer and take advantage of such social cues. We have made initial attempts to do this, by attempting to include aspects such as social role (Huang and Renals, 2008). The AMI corpus involved a substantial effort from many individuals, and provides an invaluable resource. However, we do not wish to do this again, even if we are dealing with a domain that is significantly different, such as larger groups, or family “meetings”. However, our recognisers rely strongly on annotated in-domain data. It is a major challenge</context>
</contexts>
<marker>Pentland, 2008</marker>
<rawString>A.S. Pentland. 2008. Honest signals: how they shape our world. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu-Belis</author>
<author>E Boertjes</author>
<author>J Kilgour</author>
<author>P Poller</author>
<author>S Castronovo</author>
<author>T Wilson</author>
<author>A Jaimes</author>
<author>J Carletta</author>
</authors>
<title>The amida automatic content linking device: Just-in-time document retrieval in meetings.</title>
<date>2008</date>
<booktitle>In Machine Learning for Multimodal Interaction (Proc. MLMI ’08).</booktitle>
<contexts>
<context position="23019" citStr="Popescu-Belis et al., 2008" startWordPosition="3510" endWordPosition="3513">eveloped a software architecture referred to as “The Hub” to support real-time applications. The Hub is essentially a real-time annotation server, mediating between annotation producers, such as speech recognition, and annotation consumers, such as a real-time catchup browser. Of course many applications will be both producers and consumers: for instance topic segmentation consumes transcripts and speaker turn information and produces time aligned topic segments. A good example of an application made possible by real-time recognition components and the Hub is the AMIDA Content Linking Device (Popescu-Belis et al., 2008). Content linking is essentially a continual real-time search in which a repository is searched using a query constructed from the current conver(a) Basic web-based browser (b) Summary browser Figure 2: Two examples of meeting browsers, both include time synchronisation with a searchable ASR transcript and speaker activities. (a) is a basic web-based browser; (b) also employs extractive summarisation and topic segmentation components. sational context. In this case the context is obtained from a speech recognition transcript of the past 30 seconds of the conversation, and a query is constructe</context>
</contexts>
<marker>Popescu-Belis, Boertjes, Kilgour, Poller, Castronovo, Wilson, Jaimes, Carletta, 2008</marker>
<rawString>A. Popescu-Belis, E. Boertjes, J. Kilgour, P. Poller, S. Castronovo, T. Wilson, A. Jaimes, and J. Carletta. 2008. The amida automatic content linking device: Just-in-time document retrieval in meetings. In Machine Learning for Multimodal Interaction (Proc. MLMI ’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Renals</author>
<author>T Hain</author>
</authors>
<title>Speech recognition.</title>
<date>2010</date>
<booktitle>Handbook of Computational Linguistics and Natural Language Processing.</booktitle>
<editor>In A. Clark, C. Fox, and S. Lappin, editors,</editor>
<publisher>Wiley Blackwell.</publisher>
<contexts>
<context position="12704" citStr="Renals and Hain, 2010" startWordPosition="1933" endWordPosition="1936">signals are filtered and summed to enhance signals coming from a particular direction, while suppressing signals from competing locations (W¨olfel and McDonough, 2009). The core acoustic and language modelling components for meeting speech recognition correspond quite closely to the state-of-the-art systems used in other domains. Acoustic modelling techniques include vocal tract length normalisation, speaker adaptation based on maximum likelihood linear transforms, and further training using a discriminative minimum Bayes risk criterion such as minimum phone error rate (Gales and Young, 2007; Renals and Hain, 2010). In addition we have employed a number of novel acoustic parameterisations including approaches based on local posterior probability estimation (Grezl et al., 2007) and pitch adaptive features (Garau and Renals, 2008), the automatic construction of domain-specific language models using documents obtained from the web by searching with n-grams obtained from meeting transcripts (Wan and Hain, 2006; Bulyko et al., 2007), and automatic approaches to acoustic segmentation optimised for meetings (Wrigley et al., 2005; Dines et al., 2006). A feature of the systems developed for meeting recognition i</context>
</contexts>
<marker>Renals, Hain, 2010</marker>
<rawString>S. Renals and T. Hain. 2010. Speech recognition. In A. Clark, C. Fox, and S. Lappin, editors, Handbook of Computational Linguistics and Natural Language Processing. Wiley Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Roy</author>
<author>S Luz</author>
</authors>
<title>Audio meeting history tool: Interactive graphical user-support for virtual audio meetings.</title>
<date>1999</date>
<booktitle>In Proc. ESCA Workshop on Accessing Information in Spoken Audio,</booktitle>
<pages>107--110</pages>
<contexts>
<context position="3723" citStr="Roy and Luz, 1999" startWordPosition="574" endWordPosition="577">ant work on automatic processing of meetings, coupled with an exploration of how people might interact with an archive of recorded meetings, was performed in the mid 1990s (Kazman et al., 1996). This work was limited by the fact that it was not possible at Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that time to transcribe meeting speech automatically. Other early work in the area concentrated on the multimodal capture and broadcast of meetings (Roy and Luz, 1999; Cutler et al., 2002; Yong et al., 2001). Three groups further developed approaches to automatically index the content of meetings. A team at Fuji Xerox PARC used video retrieval techniques such as keyframing to automatically generate manga-style summaries of meetings (Uchihashi et al., 1999), Waibel and colleagues at CMU used speech recognition and video tracking for meetings (Waibel et al., 2001), and Morgan and colleagues at ICSI focused on audio-only capture and speech recognition (Morgan et al., 2003). Since 2003 research in the recognition and understanding of meetings has developed sub</context>
</contexts>
<marker>Roy, Luz, 1999</marker>
<rawString>D. M. Roy and S. Luz. 1999. Audio meeting history tool: Interactive graphical user-support for virtual audio meetings. In Proc. ESCA Workshop on Accessing Information in Spoken Audio, pages 107–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Stasser</author>
<author>LA Taylor</author>
</authors>
<title>Speaking turns in faceto-face discussions.</title>
<date>1991</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>60</volume>
<issue>5</issue>
<contexts>
<context position="2969" citStr="Stasser and Taylor, 1991" startWordPosition="454" endWordPosition="457"> of data across several modalities. It is a key engineering challenge to capture such multimodal signals in a reliable, unobtrusive and flexible way, but the greater challenges arise from unlocking the multimodal recordings. If such recordings are not transcribed and indexed (at the least), then access merely corresponds to replay. And it is rare that people will have the time, or the inclination, to replay a meeting. There is a long and interesting thread of research which is concerned to better understand the dynamics of meetings and the way that groups function (Bales, 1951; McGrath, 1991; Stasser and Taylor, 1991). The types of analyses and studies carried out by these authors is still someway beyond what we can do automatically. The first significant work on automatic processing of meetings, coupled with an exploration of how people might interact with an archive of recorded meetings, was performed in the mid 1990s (Kazman et al., 1996). This work was limited by the fact that it was not possible at Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that time to</context>
</contexts>
<marker>Stasser, Taylor, 1991</marker>
<rawString>G. Stasser and LA Taylor. 1991. Speaking turns in faceto-face discussions. Journal of Personality and Social Psychology, 60(5):675–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tucker</author>
<author>O Bergman</author>
<author>A Ramamoorthy</author>
<author>S Whittaker</author>
</authors>
<title>Catchup: a useful application of timetravel in meetings.</title>
<date>2010</date>
<booktitle>In Proc. ACM CSCW,</booktitle>
<pages>99--102</pages>
<contexts>
<context position="21987" citStr="Tucker et al., 2010" startWordPosition="3355" endWordPosition="3358">s discussed below, evaluated) a number of ways of including automatically generated summaries in meeting browsers. The browser illustrated in figure 2 (b) enables navigation by the summarised transcript or via the topic segmentation. In this case the degree of summarisation is controlled by a slider, which removes those speech segments that do no contribute to the summary. We have also explored real-time (with a few utterances latency) approaches to summarisation, to facilitate meeting “catchup” scenarios, including the generation of audio only summaries, with about 60% of the speech removed (Tucker et al., 2010). Visualisations of summaries include a comic book layout (Castronovo et al., 2008), illustrated in figure 3. This is related to “VideoManga” (Uchihashi et al., 1999), but driven by transcribed speech rather than visually identified keyframes. The availability of real-time meeting speech recognition, with phrase-level latency (Garner et al., 2009), enables a new class of applications. Within AMIDA we developed a software architecture referred to as “The Hub” to support real-time applications. The Hub is essentially a real-time annotation server, mediating between annotation producers, such as </context>
</contexts>
<marker>Tucker, Bergman, Ramamoorthy, Whittaker, 2010</marker>
<rawString>S. Tucker, O. Bergman, A. Ramamoorthy, and S. Whittaker. 2010. Catchup: a useful application of timetravel in meetings. In Proc. ACM CSCW, pages 99– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Uchihashi</author>
<author>J Foote</author>
<author>A Girgensohn</author>
<author>J Boreczky</author>
</authors>
<title>Video manga: generating semantically meaningful video summaries.</title>
<date>1999</date>
<booktitle>In Proc. ACM Multimedia,</booktitle>
<pages>383--392</pages>
<contexts>
<context position="4017" citStr="Uchihashi et al., 1999" startWordPosition="620" endWordPosition="623">e 2010 Annual Conference of the North American Chapter of the ACL, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that time to transcribe meeting speech automatically. Other early work in the area concentrated on the multimodal capture and broadcast of meetings (Roy and Luz, 1999; Cutler et al., 2002; Yong et al., 2001). Three groups further developed approaches to automatically index the content of meetings. A team at Fuji Xerox PARC used video retrieval techniques such as keyframing to automatically generate manga-style summaries of meetings (Uchihashi et al., 1999), Waibel and colleagues at CMU used speech recognition and video tracking for meetings (Waibel et al., 2001), and Morgan and colleagues at ICSI focused on audio-only capture and speech recognition (Morgan et al., 2003). Since 2003 research in the recognition and understanding of meetings has developed substantially, stimulated by evaluation campaigns such as the NIST Rich Transcription (RT)1 and CLEAR2 evaluations, as well as some large multidisciplinary projects such as AMI/AMIDA3, CHIL4 and CALO5. This paper is about the work we have carried out in meeting capture, recognition and interpreta</context>
<context position="22153" citStr="Uchihashi et al., 1999" startWordPosition="3381" endWordPosition="3384">navigation by the summarised transcript or via the topic segmentation. In this case the degree of summarisation is controlled by a slider, which removes those speech segments that do no contribute to the summary. We have also explored real-time (with a few utterances latency) approaches to summarisation, to facilitate meeting “catchup” scenarios, including the generation of audio only summaries, with about 60% of the speech removed (Tucker et al., 2010). Visualisations of summaries include a comic book layout (Castronovo et al., 2008), illustrated in figure 3. This is related to “VideoManga” (Uchihashi et al., 1999), but driven by transcribed speech rather than visually identified keyframes. The availability of real-time meeting speech recognition, with phrase-level latency (Garner et al., 2009), enables a new class of applications. Within AMIDA we developed a software architecture referred to as “The Hub” to support real-time applications. The Hub is essentially a real-time annotation server, mediating between annotation producers, such as speech recognition, and annotation consumers, such as a real-time catchup browser. Of course many applications will be both producers and consumers: for instance topi</context>
</contexts>
<marker>Uchihashi, Foote, Girgensohn, Boreczky, 1999</marker>
<rawString>S. Uchihashi, J. Foote, A. Girgensohn, and J. Boreczky. 1999. Video manga: generating semantically meaningful video summaries. In Proc. ACM Multimedia, pages 383–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waibel</author>
<author>M Bett</author>
<author>F Metze</author>
<author>K Ries</author>
<author>T Schaaf</author>
<author>T Schultz</author>
<author>H Soltau</author>
<author>H Yu</author>
<author>K Zechner</author>
</authors>
<title>Advances in automatic meeting record creation and access.</title>
<date>2001</date>
<booktitle>In Proc IEEE ICASSP.</booktitle>
<contexts>
<context position="4125" citStr="Waibel et al., 2001" startWordPosition="638" endWordPosition="641">0. c�2010 Association for Computational Linguistics that time to transcribe meeting speech automatically. Other early work in the area concentrated on the multimodal capture and broadcast of meetings (Roy and Luz, 1999; Cutler et al., 2002; Yong et al., 2001). Three groups further developed approaches to automatically index the content of meetings. A team at Fuji Xerox PARC used video retrieval techniques such as keyframing to automatically generate manga-style summaries of meetings (Uchihashi et al., 1999), Waibel and colleagues at CMU used speech recognition and video tracking for meetings (Waibel et al., 2001), and Morgan and colleagues at ICSI focused on audio-only capture and speech recognition (Morgan et al., 2003). Since 2003 research in the recognition and understanding of meetings has developed substantially, stimulated by evaluation campaigns such as the NIST Rich Transcription (RT)1 and CLEAR2 evaluations, as well as some large multidisciplinary projects such as AMI/AMIDA3, CHIL4 and CALO5. This paper is about the work we have carried out in meeting capture, recognition and interpretation within the AMI and AMIDA projects since 2004. One of the principal outputs of these projects was a mult</context>
</contexts>
<marker>Waibel, Bett, Metze, Ries, Schaaf, Schultz, Soltau, Yu, Zechner, 2001</marker>
<rawString>A. Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf, T. Schultz, H. Soltau, H. Yu, and K. Zechner. 2001. Advances in automatic meeting record creation and access. In Proc IEEE ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Wan</author>
<author>T Hain</author>
</authors>
<title>Strategies for language model web-data collection.</title>
<date>2006</date>
<booktitle>In Proc IEEE ICASSP.</booktitle>
<contexts>
<context position="13103" citStr="Wan and Hain, 2006" startWordPosition="1991" endWordPosition="1994"> speaker adaptation based on maximum likelihood linear transforms, and further training using a discriminative minimum Bayes risk criterion such as minimum phone error rate (Gales and Young, 2007; Renals and Hain, 2010). In addition we have employed a number of novel acoustic parameterisations including approaches based on local posterior probability estimation (Grezl et al., 2007) and pitch adaptive features (Garau and Renals, 2008), the automatic construction of domain-specific language models using documents obtained from the web by searching with n-grams obtained from meeting transcripts (Wan and Hain, 2006; Bulyko et al., 2007), and automatic approaches to acoustic segmentation optimised for meetings (Wrigley et al., 2005; Dines et al., 2006). A feature of the systems developed for meeting recognition is the use of multiple recognition passes, cross-adaptation and model combination (Hain et al., 2007). In particular successive passes make use of more detailed—and more diverse—acoustic and language models. Different acoustic models trained on different feature representations (e.g. standard PLP features and posterior probability-based features) are cross-adapted, and different feature representa</context>
</contexts>
<marker>Wan, Hain, 2006</marker>
<rawString>V. Wan and T. Hain. 2006. Strategies for language model web-data collection. In Proc IEEE ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wellner</author>
<author>M Flynn</author>
<author>S Tucker</author>
<author>S Whittaker</author>
</authors>
<title>A meeting browser evaluation test.</title>
<date>2005</date>
<booktitle>In Proc. ACM CHI,</booktitle>
<pages>2021--2024</pages>
<contexts>
<context position="25186" citStr="Wellner et al., 2005" startWordPosition="3837" endWordPosition="3840">sation and speech recognition (in the NIST RT evaluations), and head pose recognition (in the CLEAR evaluation). In the spoken language processing domain, the AMI corpus has been used to evaluate meeting summarisation, topic segmenkay tation, dialogue act recognition and cross-language retrieval. In addition to intrinsic component-level evaluations, it is valuable to evaluate complete systems, and components in a system context. In the AMI/AMIDA projects, we investigated a number of extrinsic evaluation frameworks for browsing and accessing meeting archives. The Browser Evaluation Test (BET) (Wellner et al., 2005) provides a framework for the comparison of arbitrary meeting browser setups, which may differ in terms of which content extraction or abstraction components materals are employed. In the BET test subjects have to anit is swer true/false questions about a number of “observations of interest” relating to a recorded meeting, you we could using the browser under test with a specified time whats beter yeah i men pla plastic s limit(typically half the meeting length). rubber o you could go We developed of a variant of the BET to specifiFigure 4: Demonstration screenshot of the AMI auton matic conte</context>
</contexts>
<marker>Wellner, Flynn, Tucker, Whittaker, 2005</marker>
<rawString>P. Wellner, M. Flynn, S. Tucker, and S. Whittaker. 2005. A meeting browser evaluation test. In Proc. ACM CHI, pages 2021–2024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W¨olfel</author>
<author>J McDonough</author>
</authors>
<title>Distant Speech Recognition.</title>
<date>2009</date>
<publisher>Wiley.</publisher>
<marker>W¨olfel, McDonough, 2009</marker>
<rawString>M. W¨olfel and J. McDonough. 2009. Distant Speech Recognition. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wooters</author>
<author>M Huijbregts</author>
</authors>
<title>The ICSI RT07s speaker diarization system.</title>
<date>2007</date>
<booktitle>In Multimodal Technologies for Perception of Humans. International Evaluation Workshops CLEAR 2007 and RT</booktitle>
<volume>4625</volume>
<pages>509--519</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10555" citStr="Wooters and Huijbregts, 2007" startWordPosition="1618" endWordPosition="1621">tion challenge. The corpus is publicly available7, and is released under a licence that is based on the Creative Commons Attribution NonCommercial ShareAlike 2.5 Licence. This includes all the signals and manual annotations, plus a number of automatic annotations (e.g. speech recognition) made available to lower the startup cost of performing research on the corpus. 3 Multimodal recognition The predominant motivation behind the collection and annotation of the AMI corpus was to enable the development of multimodal recognisers to address issues such as speech recognition, speaker diarisartion (Wooters and Huijbregts, 2007), gesture recognition (Al-Hames et al., 2007) and focus of attention (Ba and Odobez, 2008). Although speech recognition is based on the (multichannel) audio signal, the other problems can be successfully addressed by combining modalities. (There is certainly information in other modalities that has the potential to make speech recognition more accurate, but so far we have not been able to use it consistently and robustly.) Speech recognition: The automatic transcription of what is spoken in a meeting is an essential prerequisite to interpreting a meeting. Morgan et al (2003) described the spee</context>
</contexts>
<marker>Wooters, Huijbregts, 2007</marker>
<rawString>C. Wooters and M. Huijbregts. 2007. The ICSI RT07s speaker diarization system. In Multimodal Technologies for Perception of Humans. International Evaluation Workshops CLEAR 2007 and RT 2007, volume 4625 of LNCS, pages 509–519. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wrigley</author>
<author>G Brown</author>
<author>V Wan</author>
<author>S Renals</author>
</authors>
<title>Speech and crosstalk detection in multichannel audio.</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="13221" citStr="Wrigley et al., 2005" startWordPosition="2010" endWordPosition="2013">um Bayes risk criterion such as minimum phone error rate (Gales and Young, 2007; Renals and Hain, 2010). In addition we have employed a number of novel acoustic parameterisations including approaches based on local posterior probability estimation (Grezl et al., 2007) and pitch adaptive features (Garau and Renals, 2008), the automatic construction of domain-specific language models using documents obtained from the web by searching with n-grams obtained from meeting transcripts (Wan and Hain, 2006; Bulyko et al., 2007), and automatic approaches to acoustic segmentation optimised for meetings (Wrigley et al., 2005; Dines et al., 2006). A feature of the systems developed for meeting recognition is the use of multiple recognition passes, cross-adaptation and model combination (Hain et al., 2007). In particular successive passes make use of more detailed—and more diverse—acoustic and language models. Different acoustic models trained on different feature representations (e.g. standard PLP features and posterior probability-based features) are cross-adapted, and different feature representations are also combined using linear transforms such as heteroscedastic linear discriminant analysis (Kumar and Andreo</context>
</contexts>
<marker>Wrigley, Brown, Wan, Renals, 2005</marker>
<rawString>S. Wrigley, G. Brown, V. Wan, and S. Renals. 2005. Speech and crosstalk detection in multichannel audio. IEEE Transactions on Speech and Audio Processing, 13(1):84–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yong</author>
<author>A Gupta</author>
<author>J Cadiz</author>
</authors>
<title>Viewing meetings captured by an omni-directional camera.</title>
<date>2001</date>
<journal>ACM Transactions on Computing Human Interaction.</journal>
<contexts>
<context position="3764" citStr="Yong et al., 2001" startWordPosition="582" endWordPosition="585">ings, coupled with an exploration of how people might interact with an archive of recorded meetings, was performed in the mid 1990s (Kazman et al., 1996). This work was limited by the fact that it was not possible at Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that time to transcribe meeting speech automatically. Other early work in the area concentrated on the multimodal capture and broadcast of meetings (Roy and Luz, 1999; Cutler et al., 2002; Yong et al., 2001). Three groups further developed approaches to automatically index the content of meetings. A team at Fuji Xerox PARC used video retrieval techniques such as keyframing to automatically generate manga-style summaries of meetings (Uchihashi et al., 1999), Waibel and colleagues at CMU used speech recognition and video tracking for meetings (Waibel et al., 2001), and Morgan and colleagues at ICSI focused on audio-only capture and speech recognition (Morgan et al., 2003). Since 2003 research in the recognition and understanding of meetings has developed substantially, stimulated by evaluation camp</context>
</contexts>
<marker>Yong, Gupta, Cadiz, 2001</marker>
<rawString>R. Yong, A. Gupta, and J. Cadiz. 2001. Viewing meetings captured by an omni-directional camera. ACM Transactions on Computing Human Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Zwyssig</author>
<author>M Lincoln</author>
<author>S Renals</author>
</authors>
<title>A digital microphone array for distant speech recognition.</title>
<date>2010</date>
<booktitle>In Proc. IEEE ICASSP–10.</booktitle>
<contexts>
<context position="7060" citStr="Zwyssig et al., 2010" startWordPosition="1081" endWordPosition="1084">rticipant (used to guarantee a clean audio signal for each speaker), as well capture of digital pens, whiteboards, shared laptop spaces, data projector and videoconferencing if used. A considerable amount of hardware was necessary for ensuring frame-level synchronisation. More recently we have used a lighter weight setup, that uses a high resolution spherical digital video camera system, and a single microphone array (7– 20 elements, depending on meeting size) synchronised using software. We have also constructed a prototype system using a low-cost, flexible array of digital MEMS microphones (Zwyssig et al., 2010). We used these instrumented meeting rooms to record the AMI Meeting Corpus (Carletta, 2007). This corpus contains over 100 hours of meeting recordings, with the different recording streams synchronised to a common timeline. The corpus contains a number of manually created and automatic annotations, synchronised to the same timeline. This 2 includes a high-quality manual word-level transcription of the complete corpus, as well as reference automatic speech recognition output, using the speech recognition system discussed in section 3 (using 5- fold cross-validation). In addition to word-level </context>
</contexts>
<marker>Zwyssig, Lincoln, Renals, 2010</marker>
<rawString>E. Zwyssig, M. Lincoln, and S. Renals. 2010. A digital microphone array for distant speech recognition. In Proc. IEEE ICASSP–10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>