<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.99748">
Unsupervised Topic Identification by Integrating Linguistic and
Visual Information Based on Hidden Markov Models
</title>
<author confidence="0.988189">
Tomohide Shibata
</author>
<affiliation confidence="0.995098">
Graduate School of Information Science
and Technology, University of Tokyo
</affiliation>
<address confidence="0.9307275">
7-3-1 Hongo, Bunkyo-ku,
Tokyo, 113-8656, Japan
</address>
<email confidence="0.998361">
shibata@kc.t.u-tokyo.ac.jp
</email>
<author confidence="0.985278">
Sadao Kurohashi
</author>
<affiliation confidence="0.995625">
Graduate School of Informatics,
Kyoto University
</affiliation>
<address confidence="0.814255">
Yoshida-honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
</address>
<email confidence="0.995351">
kuro@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895090909091">
This paper presents an unsupervised topic
identification method integrating linguis-
tic and visual information based on Hid-
den Markov Models (HMMs). We employ
HMMs for topic identification, wherein a
state corresponds to a topic and various
features including linguistic, visual and
audio information are observed. Our ex-
periments on two kinds of cooking TV
programs show the effectiveness of our
proposed method.
</bodyText>
<sectionHeader confidence="0.999401" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999102225806452">
Recent years have seen the rapid increase of mul-
timedia contents with the continuing advance of
information technology. To make the best use
of multimedia contents, it is necessary to seg-
ment them into meaningful segments and annotate
them. Because manual annotation is extremely ex-
pensive and time consuming, automatic annotation
technique is required.
In the field of video analysis, there have been
a number of studies on shot analysis for video
retrieval or summarization (highlight extraction)
using Hidden Markov Models (HMMs) (e.g.,
(Chang et al., 2002; Nguyen et al., 2005; Q.Phung
et al., 2005)). These studies first segmented videos
into shots, within which the camera motion is con-
tinuous, and extracted features such as color his-
tograms and motion vectors. Then, they classi-
fied the shots based on HMMs into several classes
(for baseball sports video, for example, pitch view,
running overview or audience view). In these
studies, to achieve high accuracy, they relied on
handmade domain-specific knowledge or trained
HMMs with manually labeled data. Therefore,
they cannot be easily extended to new domains
on a large scale. In addition, although linguistic
information, such as narration, speech of charac-
ters, and commentary, is intuitively useful for shot
analysis, it is not utilized by many of the previous
studies. Although some studies attempted to uti-
lize linguistic information (Jasinschi et al., 2001;
Babaguchi and Nitta, 2003), it was just keywords.
In the field of Natural Language Processing,
Barzilay and Lee have recently proposed a prob-
abilistic content model for representing topics and
topic shifts (Barzilay and Lee, 2004). This content
model is based on HMMs wherein a state corre-
sponds to a topic and generates sentences relevant
to that topic according to a state-specific language
model, which are learned from raw texts via anal-
ysis of word distribution patterns.
In this paper, we describe an unsupervised topic
identification method integrating linguistic and vi-
sual information using HMMs. Among several
types of videos, in which instruction videos (how-
to videos) about sports, cooking, D.I.Y., and oth-
ers are the most valuable, we focus on cooking
TV programs. In an example shown in Figure 1,
preparation, sauteing, and dishing up are automat-
ically labeled in sequence. Identified topics lead to
video segmentation and can be utilized for video
summarization.
Inspired by Barzilay’s work, we employ HMMs
for topic identification, wherein a state corre-
sponds to a topic, like preparation and frying, and
various features, which include visual and audio
information as well as linguistic information (in-
structor’s utterances), are observed. This study
considers a clause as an unit of analysis and the
following eight topics as a set of states: prepara-
tion, sauteing, frying, baking, simmering, boiling,
dishing up, steaming.
In Barzilay’s model, although domain-specific
</bodyText>
<page confidence="0.979353">
755
</page>
<note confidence="0.6218795">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 755–762,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figure confidence="0.996496230769231">
t
hidden
states
observed
data
identified
topic:
case frame
utterance
dishing up
preparation
sauteing
image
Cut an avocado.
preparation sauteing dishing up
cut:1 saute:1 add:3 put:2
‥‥
cue phrase
“then”
We’ll saute. Add spices.
‥‥
‥‥
silence
Put cheese between
slices of bread.
‥‥
</figure>
<figureCaption confidence="0.99998">
Figure 1: Topic identification with Hidden Markov Models.
</figureCaption>
<bodyText confidence="0.999836529411765">
word distribution can be learned from raw texts,
their model cannot utilize discourse features, such
as cue phrases and lexical chains. We incorpo-
rate domain-independent discourse features such
as cue phrases, noun/verb chaining, which indicate
topic change/persistence, into the domain-specific
word distribution.
Our main claim is that we utilize visual and au-
dio information to achieve robust topic identifi-
cation. As for visual information, we can utilize
background color distribution of the image. For
example, frying and boiling are usually performed
on a gas range and preparation and dishing up are
usually performed on a cutting board. This infor-
mation can be an aid to topic identification. As for
audio information, silence can be utilized as a clue
to a topic shift.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999442685714286">
In Natural Language Processing, text segmenta-
tion tasks have been actively studied for infor-
mation retrieval and summarization. Hearst pro-
posed a technique called TextTiling for subdivid-
ing texts into sub-topics (Hearst.M, 1997). This
method is based on lexical co-occurrence. Galley
et al. presented a domain-independent topic seg-
mentation algorithm for multi-party speech (Gal-
ley et al., 2003). This segmentation algorithm
uses automatically induced decision rules to com-
bine linguistic features (lexical cohesion and cue
phrases) and speech features (silences, overlaps
and speaker change). These studies aim just at
segmenting a given text, not at identifying topics
of segmented texts.
Marcu performed rhetorical parsing in the
framework of Rhetorical Structure Theory (RST)
based on a discourse-annotated corpus (Marcu,
2000). Although this model is suitable for ana-
lyzing local modification in a text, it is difficult for
this model to capture the structure of topic transi-
tion in the whole text.
In contrast, Barzilay and Lee modeled a con-
tent structure of texts within specific domains,
such as earthquake and finance (Barzilay and Lee,
2004). They used HMMs wherein each state cor-
responds to a distinct topic (e.g., in earthquake
domain, earthquake magnitude or previous earth-
quake occurrences) and generates sentences rel-
evant to that topic according to a state-specific
language model. Their method first create clus-
ters via complete-link clustering, measuring sen-
tence similarity by the cosine metric using word
bigrams as features. They calculate initial proba-
bilities: state sz specific language model pszW|w)
</bodyText>
<page confidence="0.996553">
756
</page>
<figure confidence="0.997553823529412">
小松菜を切ります。 (Cut a Chinese cabbage.)
[individual action]
cut:1
根元を切り落とし、一度 洗います。 (Cut off its root and wash it.)
[individual action] [individual action]
cut off:1 wash:1
代わりに大根もおいしいです。 (A Japanese radish would taste delicious.)
[substitution]
縦に3等分に切ります。 (Divide it into three equal parts.)
[individual action]
divide:3
あと少しですからここだけ頑張って下さい。 (Just a little more and go for it!)
[small talk] [small talk]
‥‥
では炒めていきます。 (Now, we&apos;ll saute.)
[action declaration]
saute:1
</figure>
<figureCaption confidence="0.995189">
Figure 2: An example of closed captions. (The phrase sandwiched by a square bracket means an utterance
</figureCaption>
<bodyText confidence="0.985187666666667">
type and the word surrounded by a rectangle means an extracted utterance referring to an action. The
bold word means a case frame assigned to the verb.)
and state-transition probability p(sj|sz) from state
sz to state sj. Then, they continue to estimate
HMM parameters with the Viterbi algorithm un-
til the clustering stabilizes. They applied the con-
structed content model to two tasks: information
ordering and summarization. We differ from this
study in that we utilize multimodal features and
domain-independent discourse features to achieve
robust topic identification.
In the field of video analysis, there have been
a number of studies on shot analysis with HMMs.
Chang et al. described a method for classifying
shots into several classes for highlight extraction
in baseball games (Chang et al., 2002). Nguyen
et al. proposed a robust statistical framework to
extract highlights from a baseball video (Nguyen
et al., 2005). They applied multi-stream HMMs
to control the weight among different features,
such as principal component features capturing
color information and frame-difference features
for moving objects. Phung et al. proposed a prob-
abilistic framework to exploit hierarchy structure
for topic transition detection in educational videos
(Q.Phung et al., 2005).
Some studies attempted to utilize linguistic
information in shot analysis (Jasinschi et al.,
2001; Babaguchi and Nitta, 2003). For exam-
ple, Babaguchi and Nitta segmented closed cap-
tion text into meaningful units and linked them to
video streams in sports video. However, linguistic
information they utilized was just keywords.
</bodyText>
<sectionHeader confidence="0.999825" genericHeader="method">
3 Features for Topic Identification
</sectionHeader>
<bodyText confidence="0.999985409090909">
First, we’ll describe the features that we use for
topic identification, which are listed in Table 1.
They consist of three modalities: linguistic, visual
and audio modality.
We utilize as linguistic information the instruc-
tor’s utterances in video, which can be divided into
various types such as actions, tips, and even small
talk. Among them, actions, such as cut, peel and
grease a pan, are dominant and supposed to be use-
ful for topic identification and others can be noise.
In the case of analyzing utterances in video, it
is natural to utilize visual information as well as
linguistic information for robust analysis. We uti-
lize background image as visual information. For
example, frying and boiling are usually performed
on a gas range and preparation and dishing up are
usually performed on a cutting board.
Furthermore, we utilize cue phrases and silence
as a clue to a topic shift, and noun/verb chaining
as a clue to a topic persistence.
We describe these features in detail in the fol-
lowing sections.
</bodyText>
<subsectionHeader confidence="0.997574">
3.1 Linguistic Features
</subsectionHeader>
<bodyText confidence="0.9686055">
Closed captions of Japanese cooking TV programs
are used as a source for extracting linguistic fea-
</bodyText>
<page confidence="0.997928">
757
</page>
<tableCaption confidence="0.947433">
Table 1: Features for topic identification.
</tableCaption>
<table confidence="0.984506">
Modality Feature Domain dependent Domain independent
linguistic case frame utterance generalization
cue phrases topic change
noun chaining topic persistence
verb chaining topic persistence
visual background image bottom of image
audio silence topic change
</table>
<tableCaption confidence="0.825936">
Table 2: Utterance-type classification. (An underlined phrase means a pattern for recognizing utterance
type.)
</tableCaption>
<figure confidence="0.93985825">
[action declaration]
ex. (Then, we ’ll cook a steak)
(OK, we’ll fry.)
[individual action]
ex. (Cut off a step of this eggplant.)
(Pour water into a pan.)
[food state]
ex. (There is no water in the carrot.)
[note]
ex. (Don’t cut this core off.)
[substitution]
ex. (You may use a leek.)
[food/tool presentation]
ex. Today, we use this handy mixer.)
[small talk]
ex. (Hello.)
</figure>
<figureCaption confidence="0.82107175">
tures. An example of closed captions is shown in
Figure 2. We first process them with the Japanese
morphological analyzer, JUMAN (Kurohashi et
al., 1994), and make syntactic/case analysis and
anaphora resolution with the Japanese analyzer,
KNP (Kurohashi and Nagao, 1994). Then, we
perform the following process to extract linguis-
tic features.
</figureCaption>
<sectionHeader confidence="0.917614" genericHeader="method">
3.1.1 Extracting Utterances Referring to
Actions
</sectionHeader>
<bodyText confidence="0.9829563">
Considering a clause as a basic unit, utterances
referring to an action are extracted in the form
of case frame, which is assigned by case analy-
sis. This procedure is performed for generaliza-
tion and word sense disambiguation. For exam-
ple, “ (add salt)” and “
(add sugar into a pan)” are assigned to case
frame ireru:1 (add) and “ (carve with
a knife)” is assigned to case frame ireru:2 (carve).
We describe this procedure in detail below.
</bodyText>
<subsectionHeader confidence="0.615457">
Utterance-type recognition
</subsectionHeader>
<bodyText confidence="0.9995765">
To extract utterances referring to actions, we
classify utterances into several types listed in Ta-
ble 21. Note that actions are supposed to have two
levels: [action declaration] means a declaration of
beginning a series of actions and [individual ac-
tion] means an action that is the finest one.
</bodyText>
<footnote confidence="0.805214">
1In this paper, [ ] means an utterance type.
</footnote>
<bodyText confidence="0.999913166666667">
Input sentences are first segmented into
clauses and their utterance type is recognized.
Among several utterance types, [individual ac-
tion], [food/tool presentation], [substitution],
[note], and [small talk] can be recognized by
clause-end patterns. We prepare approximately
500 patterns for recognizing the utterance type. As
for [individual action] and [food state], consider-
ing the portability of our system, we use general
rules regarding intransitive verbs or adjective + “
(become)” as [food state], and others as [in-
dividual action].
</bodyText>
<sectionHeader confidence="0.50846" genericHeader="method">
Action extraction
</sectionHeader>
<bodyText confidence="0.9972184">
We extract utterances whose utterance type is
recognized as action ([action declaration] or [indi-
vidual action]). For example, “ (peel)” and “
(cut)” are extracted from the following sen-
tence.
</bodyText>
<listItem confidence="0.506635">
(1) [individual action]
</listItem>
<bodyText confidence="0.998713125">
[individual action] (We
peel this carrot and cut it in half.)
We make two exceptions to reduce noises. One
is that clauses are not extracted from the sen-
tence in which sentence-end clause’s utterance-
type is not recognized as an action. In the fol-
lowing example, “ (simmer)” and “ (cut)”
are not extracted because the utterance type of
</bodyText>
<page confidence="0.998001">
758
</page>
<tableCaption confidence="0.941618">
Table 3: An example of the automatically con-
structed case frame.
</tableCaption>
<table confidence="0.999243857142857">
Verb Case Examples
marker
kiru:1 ga &lt;agent&gt;
(cut) wo pork, carrot, vegetable, · · ·
ni rectangle, diamonds, · · ·
kiru:2 ga &lt;agent&gt;
(drain) wo damp · · ·
no eggplant, bean curd, · · ·
ireru:1 ga &lt;agent&gt;
(add) wo salt, oil, vegetable, · · ·
ni pan, bowl, · · ·
ireru:2 ga &lt;agent&gt;
(carve) wo knife · · ·
ni fish ···
</table>
<bodyText confidence="0.9216614">
the sentence-end clause is recognized as [substi-
tution].
(2) [individual action] [indi-
vidual action] [substitution] (It
doesn’t matter if you cut it after simmering.)
</bodyText>
<subsectionHeader confidence="0.729695">
3.1.2 Cue phrases
</subsectionHeader>
<bodyText confidence="0.996517333333333">
As Grosz and Sidner (Grosz and Sidner, 1986)
pointed out, cue phrases such as now and well
serve to indicate a topic change. We use approx-
imately 20 domain-independent cue phrases, such
as “ (then)”, “ (next)” and “
(then)”.
</bodyText>
<subsectionHeader confidence="0.49181">
3.1.3 Noun Chaining
</subsectionHeader>
<bodyText confidence="0.9987845">
In text segmentation algorithms such as Text-
Tiling (Hearst.M, 1997), lexical chains are widely
utilized for detecting a topic shift. We utilize such
a feature as a clue to topic persistence.
When two continuous actions are performed to
the same ingredient, their topics are often identi-
cal. For example, because “ (grate)” and “
(raise)” are performed to the same ingredi-
ent “ (turnip)” , the topics (in this instance,
preparation) in the two utterances are identical.
</bodyText>
<listItem confidence="0.405324">
(5) a.
(We’ll grate a turnip.)
b.
</listItem>
<bodyText confidence="0.955058666666667">
The other is that conditional/causal clauses are
not extracted because they sometimes refer to the
previous/next topic.
</bodyText>
<listItem confidence="0.9644085">
(3) (After we
finish cutting it, we’ll fry.)
</listItem>
<bodyText confidence="0.984342774193549">
(We cut in this cherry tomato,
because we’ll fry it in oil.)
Note that relations between clauses are recognized
by clause-end patterns.
Verb sense disambiguation by assigning to a
case frame
In general, a verb has multiple mean-
ings/usages. For example, “ ” has multiple
usages, “ (add salt)” and “
(carve with a knife)” , which appear in
different topics. We do not extract a surface form
of verb but a case frame, which is assigned by
case analysis. Case frames are automatically
constructed from Web cooking texts (12 million
sentences) by clustering similar verb usages
(Kawahara and Kurohashi, 2002). An example of
the automatically constructed case frame is shown
in Table 3. For example, “ (add salt)”
is assigned to ireru:1 (add) and “
(carve with a knife)” is assigned to case frame
ireru:2 (carve).
(Raise this turnip on this basket.)
However, in the case of spoken language, be-
cause there exist many omissions, it is often the
case that noun chaining cannot be detected with
surface word matching. Therefore, we detect
noun chaining by using the anaphora resolution
result2 of verbs (ex.(6)) and nouns (ex.(7)). The
verb, noun anaphora resolution is conducted by
the method proposed by (Kawahara and Kuro-
hashi, 2004), (Sasano et al., 2004), respectively.
</bodyText>
<listItem confidence="0.945417">
(6) a. (Cut a cabbage.)
b. [ ] (Wash it
once.)
(7) a.
</listItem>
<bodyText confidence="0.596291">
(Slice a carrot into 4-cm pieces.)
b. [ ]
(Peel its skin.)
</bodyText>
<subsectionHeader confidence="0.619338">
3.1.4 Verb Chaining
</subsectionHeader>
<bodyText confidence="0.9990106">
When a verb of a clause is identical with that
of the previous clause, they are likely to have the
same topic. We utilize the fact that the adjoining
two clauses contain an identical verbs or not as an
observed feature.
</bodyText>
<listItem confidence="0.9967395">
(8) a. (Add some
red peppers.)
</listItem>
<footnote confidence="0.5081595">
2[ ] indicates an element complemented with anaphora
resolution.
</footnote>
<page confidence="0.993993">
759
</page>
<figure confidence="0.823599">
b. (Add chicken
wings.)
</figure>
<subsectionHeader confidence="0.992494">
3.2 Image Features
</subsectionHeader>
<bodyText confidence="0.9999965">
It is difficult for the current image processing tech-
nique to extract what object appears or what ac-
tion is performing in video unless a detailed ob-
ject/action model for a specific domain is con-
structed by hand. Therefore, referring to (Hamada
et al., 2000), we focus our attention on color dis-
tribution at the bottom of the image, which is com-
paratively easy to exploit. As shown in Figure 1,
we utilize the mass point of RGB in the bottom of
the image at each clause.
</bodyText>
<subsectionHeader confidence="0.996938">
3.3 Audio Features
</subsectionHeader>
<bodyText confidence="0.999931083333333">
A cooking video contains various types of audio
information, such as instructor’s speech, cutting
sounds and frizzling sound. If cutting sound or
frizzling sound could be distinguished from other
sounds, they could be an aid to topic identification,
but it is difficult to recognize them.
As Galley et al. (Galley et al., 2003) pointed
out, a longer silence often appears when topic
changes, and we can utilize it as a clue to topic
change. In this study, silence is automatically ex-
tracted by finding duration below a certain ampli-
tude level which lasts more than one second.
</bodyText>
<sectionHeader confidence="0.988913" genericHeader="method">
4 Topic Identification based on HMMs
</sectionHeader>
<bodyText confidence="0.999991555555556">
We employ HMMs for topic identification, where
a hidden state corresponds to a topic and vari-
ous features described in Section 3 are observed.
In our model, considering the case frame as a
basic unit, the case frame and background im-
age are observed from the state, and discourse
features indicating to topic shift/persistence (cue
phrases, noun/verb chaining and silence) are ob-
served when the state transits.
</bodyText>
<subsectionHeader confidence="0.995086">
4.1 Parameters
</subsectionHeader>
<bodyText confidence="0.964604">
HMM parameters are as follows:
</bodyText>
<listItem confidence="0.997405625">
• initial state distribution πi : the probability
that state si is a start state.
• state transition probability aij : the probabil-
ity that state si transits to state sj.
• observation probability bij(ot) : the proba-
bility that symbol ot is emitted when state si
transits to state sj. This probability is given
by the following equation:
</listItem>
<equation confidence="0.894839">
bij(ot) = bj(cfk) · bj(R, G, B)
· bij(discourse features) (1)
</equation>
<bodyText confidence="0.998545555555556">
- case frame bj(cfk): the probability that
case frame cfk is emitted by state sj.
- background image bj(R, G, B): the prob-
ability that background image bj(R, G, B) is
emitted by state sj. The emission probability
is modeled by a single Gaussian distribution
with mean (Rj,Gj,Bj) and variance σj.
- discourse features : the probability that
discourse features are emitted when state si
transits to state sj. This probability is defined
as multiplication of the observation probabil-
ity of each feature (cue phrase, noun chain-
ing, verb chaining, silence). The observation
probability of each feature does not depend
on state si and sj, but on whether si and sj
are the same or different. For example, in the
case of cue phrase (c), the probability is given
by the following equation:
</bodyText>
<equation confidence="0.996451">
� psame(c)(i = j)
bij(c) = (2)
pdiff (c) (i 7� j)
</equation>
<subsectionHeader confidence="0.99514">
4.2 Parameters Estimation
</subsectionHeader>
<bodyText confidence="0.9999895">
We apply the Baum-Welch algorithm for esti-
mating these parameters. To achieve high accu-
racy with the Baum-Welch algorithm, which is
an unsupervised learning method, some labeled
data have been required or proper initial param-
eters have been set depending on domain-specific
knowledge. These requirements, however, make
it difficult to extend to other domains. We auto-
matically extract “pseudo-labeled” data focusing
on the following linguistic expressions: if a clause
has the utterance-type [action declaration] and an
original form of its verb corresponds to a topic, its
topic is set to that topic. Remind that [action dec-
laration] is a kind of declaration of starting a series
of actions. For example, in Figure 1, the topic of
the clause “We’ll saute.” is set to sauteing because
its utterance-type is recognized as [action decla-
ration] and the original form of its verb is topic
sauteing.
By using a small amounts of “pseudo-labeled”
data as well as unlabeled data, we train the
HMM parameters. Once the HMM parameters are
trained, the topic identification is performed using
the standard Viterbi algorithm.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="evaluation">
5 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.997777">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999399666666667">
To demonstrate the effectiveness of our proposed
method, we made experiments on two kinds of
cooking TV programs: NHK “Today’s Cooking”
</bodyText>
<page confidence="0.997823">
760
</page>
<tableCaption confidence="0.999298">
Table 5: Experimental result of topic identification.
</tableCaption>
<table confidence="0.999454727272727">
Features Accuracy
case frame background image discourse features silence “Today’s Cooking” “Kewpie 3-Min Cooking”
√ √ √ √ 61.7% 66.4%
√ √ √ 56.8% 72.9%
√ √ 69.9% 77.1%
√ √ 70.5% 82.9%
70.5% 82.9%
First, saute and Chop a garlic Let’s start cooked
body. noisely. vegitable.
sauteing
preparation sauteing
</table>
<tableCaption confidence="0.8299455">
Table 4: Characteristics of the two cooking pro-
grams we used for our experiments.
</tableCaption>
<table confidence="0.997762">
Program Today’s Cooking Kewpie 3-Min Cooking linguistic
linguistic
+ visual
Videos 200 70
Duration 25min 10min
# of utterances 249.4 183.4
per video
</table>
<bodyText confidence="0.998691">
and NTV “Kewpie 3-Min Cooking”. Table 4
presents the characteristics of the two programs.
Note that time stamps of closed captions syn-
chronize themselves with the video stream. Ex-
tracted “pseudo-labeled” data by the expression
mentioned in Section 4.2 are 525 clauses out of
13564 (3.87%) in “Today’s Cooking”, and 107
clauses out of 1865 (5.74%) in “Kewpie 3-Min
Cooking”.
</bodyText>
<subsectionHeader confidence="0.979403">
5.2 Experiments and Discussion
</subsectionHeader>
<bodyText confidence="0.99640264">
We conducted the experiment of the topic iden-
tification. We first trained HMM parameters for
each program, and then applied the trained model
to five videos each, in which, we manually as-
signed appropriate topics to clauses. Table 5
gives the evaluation results. The unit of evalua-
tion was a clause. The accuracy was improved
by integrating linguistic and visual information
compared to using linguistic / visual informa-
tion alone. (Note that “visual information” uses
pseudo-labeled data.) In addition, the accuracy
was improved by using various discourse features.
The reason why silence did not contribute to ac-
curacy improvement is supposed to be that closed
captions and video streams were not synchronized
precisely due to time lagging of closed captions.
To deal with this problem, an automatic closed
caption alignment technique (Huang et al., 2003)
will be applied or automatic speech recognition
will be used as texts instead of closed captions
with the advance of speech recognition technol-
ogy.
Figure 3 illustrates an improved example by
adding visual information. In the case of using
only linguistic information, this topic was rec-
</bodyText>
<figureCaption confidence="0.618829">
Figure 3: An improved example by adding visual
information.
</figureCaption>
<bodyText confidence="0.999491964285714">
ognized as sauteing, but this topic was actually
preparation, which referred to the next topic. By
using the visual information that background color
was white, this topic was correctly recognized as
preparation.
We conducted another experiment to demon-
strate the validity of several linguistic processes,
such as utterance-type recognition and word sense
disambiguation with case frames, for extracting
linguistic information from closed captions de-
scribed in Section 3.1.1. We compared our method
to three methods: a method that does not per-
form word sense disambiguation with case frames
(w/o cf), a method that does not perform utterance-
type recognition for extracting actions (uses all
utterance-type texts) (w/o utype), a method, in
which a sentence is emitted according to a state-
specific language model (bigram) as Barzilay and
Lee adopted (bigram). Figure 6 gives the exper-
imental result, which demonstrates our method is
appropriate.
One cause of errors in topic identification is that
some case frames are incorrectly constructed. For
example, kiru:1 (cut) contains “ (cut
a vegetable)” and “ (drain oil)”. This
leads to incorrect parameter training. Other cause
is that some verbs are assigned to an inaccurate
case frame by the failure of case analysis.
</bodyText>
<sectionHeader confidence="0.999707" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.994088">
This paper has described an unsupervised topic
identification method integrating linguistic and vi-
sual information based on Hidden Markov Mod-
</bodyText>
<page confidence="0.997702">
761
</page>
<tableCaption confidence="0.996963">
Table 6: Results of the experiment that compares our method to three methods.
</tableCaption>
<figure confidence="0.993756733333333">
Method Accuracy
“Today’s Cooking”
61.7%
57.1%
61.7%
54.7%
“Kewpie 3-Min Cooking”
66.4%
60.0%
62.1%
59.3%
proposed method
w/o cf
w/o utype
bigram
</figure>
<bodyText confidence="0.9982619">
els. Our experiments on the two kinds of cooking
TV programs showed the effectiveness of integra-
tion of linguistic and visual information and in-
corporation of domain-independent discourse fea-
tures to domain-dependent features (case frame
and background image).
We are planning to perform object recognition
using the automatically-constructed object model
and utilize the object recognition results as a fea-
ture for HMM-based topic identification.
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984574025974">
Noboru Babaguchi and Naoko Nitta. 2003. Intermodal
collaboration: A strategy for semantic content anal-
ysis for broadcasted sports video. In Proceedings of
IEEE International Conference on Image Process-
ing(ICIP2003), pages 13–16.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the NAACL/HLT, pages 113–120.
Peng Chang, Mei Han, and Yihong Gong. 2002.
Extract highlights from baseball game video with
hidden markov models. In Proceedings of the
International Conference on Image Processing
2002(ICIP2002), pages 609–612.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 562–569, 7.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistic, 12:175–204.
Reiko Hamada, Ichiro Ide, Shuichi Sakai, and Hide-
hiko Tanaka. 2000. Associating cooking video with
related textbook. In Proceedings of ACM Multime-
dia 2000 workshops, pages 237–241.
Hearst.M. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64, March.
Chih-Wei Huang, Winston Hsu, and Shin-Fu Chang.
2003. Automatic closed caption alignment based
on speech recognition transcripts. Technical report,
Columbia ADVENT.
Radu Jasinschi, Nevenka Dimitrova, Thomas McGee,
Lalitha Agnihotri, John Zimmerman, and Dongge.
2001. Integrated multimedia processing for topic
segmentation and classification. In Proceedings of
IEEE International Conference on Image Process-
ing(ICIP2003), pages 366–369.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of case frame dictionary for robust japanese
case analysis. In Proceedings of 19th COLING
(COLING02), pages 425–431.
Daisuke Kawahara and Sadao Kurohashi. 2004. Zero
pronoun resolution based on automatically con-
structed case frames and structural preference of an-
tecedents. In Proceedings of The 1st International
Joint Conference on Natural Language Processing,
pages 334–341.
Sadao Kurohashi and Makoto Nagao. 1994. A syntac-
tic analysis method of long japanese sentences based
on the detection of conjunctive structures. Compu-
tational Linguistics, 20(4).
Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improve-
ments of Japanese morphological analyzer JUMAN.
In Proceedings of the International Workshop on
Sharable Natural Language, pages 22–28.
Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395–448.
Huu Bach Nguyen, Koichi Shinoda, and Sadaoki Fu-
rui. 2005. Robust highlight extraction using multi-
stream hidden markov models for baseball video. In
Proceedings of the International Conference on Im-
age Processing 2005(ICIP2005), pages 173–176.
Dinh Q.Phung, Thi V.T Duong, Hung H.Bui, and
S.Venkatesh. 2005. Topic transition detection using
hierarchical hidden markov and semi-markov mod-
els. In Proceedings of ACM International Confer-
ence on Multimedia(ACM-MM05), pages 6–11.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proceedings of the 20th International
Conference on Computational Linguistics, number
1201–1207, 8.
</reference>
<page confidence="0.996657">
762
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.545990">
<title confidence="0.9504755">Unsupervised Topic Identification by Integrating Linguistic and Visual Information Based on Hidden Markov Models</title>
<author confidence="0.898862">Tomohide Shibata</author>
<affiliation confidence="0.9978745">Graduate School of Information Science and Technology, University of Tokyo</affiliation>
<address confidence="0.9984495">7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan</address>
<email confidence="0.983407">shibata@kc.t.u-tokyo.ac.jp</email>
<author confidence="0.748043">Sadao Kurohashi</author>
<affiliation confidence="0.9998425">Graduate School of Informatics, Kyoto University</affiliation>
<address confidence="0.944464">Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan</address>
<email confidence="0.986078">kuro@i.kyoto-u.ac.jp</email>
<abstract confidence="0.997478166666667">This paper presents an unsupervised topic identification method integrating linguistic and visual information based on Hidden Markov Models (HMMs). We employ HMMs for topic identification, wherein a state corresponds to a topic and various features including linguistic, visual and audio information are observed. Our experiments on two kinds of cooking TV programs show the effectiveness of our proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Noboru Babaguchi</author>
<author>Naoko Nitta</author>
</authors>
<title>Intermodal collaboration: A strategy for semantic content analysis for broadcasted sports video.</title>
<date>2003</date>
<booktitle>In Proceedings of IEEE International Conference on Image Processing(ICIP2003),</booktitle>
<pages>13--16</pages>
<contexts>
<context position="2304" citStr="Babaguchi and Nitta, 2003" startWordPosition="334" endWordPosition="337">al classes (for baseball sports video, for example, pitch view, running overview or audience view). In these studies, to achieve high accuracy, they relied on handmade domain-specific knowledge or trained HMMs with manually labeled data. Therefore, they cannot be easily extended to new domains on a large scale. In addition, although linguistic information, such as narration, speech of characters, and commentary, is intuitively useful for shot analysis, it is not utilized by many of the previous studies. Although some studies attempted to utilize linguistic information (Jasinschi et al., 2001; Babaguchi and Nitta, 2003), it was just keywords. In the field of Natural Language Processing, Barzilay and Lee have recently proposed a probabilistic content model for representing topics and topic shifts (Barzilay and Lee, 2004). This content model is based on HMMs wherein a state corresponds to a topic and generates sentences relevant to that topic according to a state-specific language model, which are learned from raw texts via analysis of word distribution patterns. In this paper, we describe an unsupervised topic identification method integrating linguistic and visual information using HMMs. Among several types </context>
<context position="8655" citStr="Babaguchi and Nitta, 2003" startWordPosition="1295" endWordPosition="1298">l games (Chang et al., 2002). Nguyen et al. proposed a robust statistical framework to extract highlights from a baseball video (Nguyen et al., 2005). They applied multi-stream HMMs to control the weight among different features, such as principal component features capturing color information and frame-difference features for moving objects. Phung et al. proposed a probabilistic framework to exploit hierarchy structure for topic transition detection in educational videos (Q.Phung et al., 2005). Some studies attempted to utilize linguistic information in shot analysis (Jasinschi et al., 2001; Babaguchi and Nitta, 2003). For example, Babaguchi and Nitta segmented closed caption text into meaningful units and linked them to video streams in sports video. However, linguistic information they utilized was just keywords. 3 Features for Topic Identification First, we’ll describe the features that we use for topic identification, which are listed in Table 1. They consist of three modalities: linguistic, visual and audio modality. We utilize as linguistic information the instructor’s utterances in video, which can be divided into various types such as actions, tips, and even small talk. Among them, actions, such as</context>
</contexts>
<marker>Babaguchi, Nitta, 2003</marker>
<rawString>Noboru Babaguchi and Naoko Nitta. 2003. Intermodal collaboration: A strategy for semantic content analysis for broadcasted sports video. In Proceedings of IEEE International Conference on Image Processing(ICIP2003), pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the NAACL/HLT,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="2508" citStr="Barzilay and Lee, 2004" startWordPosition="366" endWordPosition="369">Ms with manually labeled data. Therefore, they cannot be easily extended to new domains on a large scale. In addition, although linguistic information, such as narration, speech of characters, and commentary, is intuitively useful for shot analysis, it is not utilized by many of the previous studies. Although some studies attempted to utilize linguistic information (Jasinschi et al., 2001; Babaguchi and Nitta, 2003), it was just keywords. In the field of Natural Language Processing, Barzilay and Lee have recently proposed a probabilistic content model for representing topics and topic shifts (Barzilay and Lee, 2004). This content model is based on HMMs wherein a state corresponds to a topic and generates sentences relevant to that topic according to a state-specific language model, which are learned from raw texts via analysis of word distribution patterns. In this paper, we describe an unsupervised topic identification method integrating linguistic and visual information using HMMs. Among several types of videos, in which instruction videos (howto videos) about sports, cooking, D.I.Y., and others are the most valuable, we focus on cooking TV programs. In an example shown in Figure 1, preparation, sautei</context>
<context position="6205" citStr="Barzilay and Lee, 2004" startWordPosition="932" endWordPosition="935">ses) and speech features (silences, overlaps and speaker change). These studies aim just at segmenting a given text, not at identifying topics of segmented texts. Marcu performed rhetorical parsing in the framework of Rhetorical Structure Theory (RST) based on a discourse-annotated corpus (Marcu, 2000). Although this model is suitable for analyzing local modification in a text, it is difficult for this model to capture the structure of topic transition in the whole text. In contrast, Barzilay and Lee modeled a content structure of texts within specific domains, such as earthquake and finance (Barzilay and Lee, 2004). They used HMMs wherein each state corresponds to a distinct topic (e.g., in earthquake domain, earthquake magnitude or previous earthquake occurrences) and generates sentences relevant to that topic according to a state-specific language model. Their method first create clusters via complete-link clustering, measuring sentence similarity by the cosine metric using word bigrams as features. They calculate initial probabilities: state sz specific language model pszW|w) 756 小松菜を切ります。 (Cut a Chinese cabbage.) [individual action] cut:1 根元を切り落とし、一度 洗います。 (Cut off its root and wash it.) [individual</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of the NAACL/HLT, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Chang</author>
<author>Mei Han</author>
<author>Yihong Gong</author>
</authors>
<title>Extract highlights from baseball game video with hidden markov models.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Image Processing 2002(ICIP2002),</booktitle>
<pages>609--612</pages>
<contexts>
<context position="1412" citStr="Chang et al., 2002" startWordPosition="195" endWordPosition="198">w the effectiveness of our proposed method. 1 Introduction Recent years have seen the rapid increase of multimedia contents with the continuing advance of information technology. To make the best use of multimedia contents, it is necessary to segment them into meaningful segments and annotate them. Because manual annotation is extremely expensive and time consuming, automatic annotation technique is required. In the field of video analysis, there have been a number of studies on shot analysis for video retrieval or summarization (highlight extraction) using Hidden Markov Models (HMMs) (e.g., (Chang et al., 2002; Nguyen et al., 2005; Q.Phung et al., 2005)). These studies first segmented videos into shots, within which the camera motion is continuous, and extracted features such as color histograms and motion vectors. Then, they classified the shots based on HMMs into several classes (for baseball sports video, for example, pitch view, running overview or audience view). In these studies, to achieve high accuracy, they relied on handmade domain-specific knowledge or trained HMMs with manually labeled data. Therefore, they cannot be easily extended to new domains on a large scale. In addition, although</context>
<context position="8057" citStr="Chang et al., 2002" startWordPosition="1210" endWordPosition="1213">sj|sz) from state sz to state sj. Then, they continue to estimate HMM parameters with the Viterbi algorithm until the clustering stabilizes. They applied the constructed content model to two tasks: information ordering and summarization. We differ from this study in that we utilize multimodal features and domain-independent discourse features to achieve robust topic identification. In the field of video analysis, there have been a number of studies on shot analysis with HMMs. Chang et al. described a method for classifying shots into several classes for highlight extraction in baseball games (Chang et al., 2002). Nguyen et al. proposed a robust statistical framework to extract highlights from a baseball video (Nguyen et al., 2005). They applied multi-stream HMMs to control the weight among different features, such as principal component features capturing color information and frame-difference features for moving objects. Phung et al. proposed a probabilistic framework to exploit hierarchy structure for topic transition detection in educational videos (Q.Phung et al., 2005). Some studies attempted to utilize linguistic information in shot analysis (Jasinschi et al., 2001; Babaguchi and Nitta, 2003). </context>
</contexts>
<marker>Chang, Han, Gong, 2002</marker>
<rawString>Peng Chang, Mei Han, and Yihong Gong. 2002. Extract highlights from baseball game video with hidden markov models. In Proceedings of the International Conference on Image Processing 2002(ICIP2002), pages 609–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Eric FoslerLussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>562--569</pages>
<contexts>
<context position="5449" citStr="Galley et al., 2003" startWordPosition="815" endWordPosition="819">ge and preparation and dishing up are usually performed on a cutting board. This information can be an aid to topic identification. As for audio information, silence can be utilized as a clue to a topic shift. 2 Related Work In Natural Language Processing, text segmentation tasks have been actively studied for information retrieval and summarization. Hearst proposed a technique called TextTiling for subdividing texts into sub-topics (Hearst.M, 1997). This method is based on lexical co-occurrence. Galley et al. presented a domain-independent topic segmentation algorithm for multi-party speech (Galley et al., 2003). This segmentation algorithm uses automatically induced decision rules to combine linguistic features (lexical cohesion and cue phrases) and speech features (silences, overlaps and speaker change). These studies aim just at segmenting a given text, not at identifying topics of segmented texts. Marcu performed rhetorical parsing in the framework of Rhetorical Structure Theory (RST) based on a discourse-annotated corpus (Marcu, 2000). Although this model is suitable for analyzing local modification in a text, it is difficult for this model to capture the structure of topic transition in the who</context>
<context position="17187" citStr="Galley et al., 2003" startWordPosition="2708" endWordPosition="2711">ructed by hand. Therefore, referring to (Hamada et al., 2000), we focus our attention on color distribution at the bottom of the image, which is comparatively easy to exploit. As shown in Figure 1, we utilize the mass point of RGB in the bottom of the image at each clause. 3.3 Audio Features A cooking video contains various types of audio information, such as instructor’s speech, cutting sounds and frizzling sound. If cutting sound or frizzling sound could be distinguished from other sounds, they could be an aid to topic identification, but it is difficult to recognize them. As Galley et al. (Galley et al., 2003) pointed out, a longer silence often appears when topic changes, and we can utilize it as a clue to topic change. In this study, silence is automatically extracted by finding duration below a certain amplitude level which lasts more than one second. 4 Topic Identification based on HMMs We employ HMMs for topic identification, where a hidden state corresponds to a topic and various features described in Section 3 are observed. In our model, considering the case frame as a basic unit, the case frame and background image are observed from the state, and discourse features indicating to topic shif</context>
</contexts>
<marker>Galley, McKeown, FoslerLussier, Jing, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 562–569, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistic,</journal>
<pages>12--175</pages>
<contexts>
<context position="13741" citStr="Grosz and Sidner, 1986" startWordPosition="2119" endWordPosition="2122">ce type of 758 Table 3: An example of the automatically constructed case frame. Verb Case Examples marker kiru:1 ga &lt;agent&gt; (cut) wo pork, carrot, vegetable, · · · ni rectangle, diamonds, · · · kiru:2 ga &lt;agent&gt; (drain) wo damp · · · no eggplant, bean curd, · · · ireru:1 ga &lt;agent&gt; (add) wo salt, oil, vegetable, · · · ni pan, bowl, · · · ireru:2 ga &lt;agent&gt; (carve) wo knife · · · ni fish ··· the sentence-end clause is recognized as [substitution]. (2) [individual action] [individual action] [substitution] (It doesn’t matter if you cut it after simmering.) 3.1.2 Cue phrases As Grosz and Sidner (Grosz and Sidner, 1986) pointed out, cue phrases such as now and well serve to indicate a topic change. We use approximately 20 domain-independent cue phrases, such as “ (then)”, “ (next)” and “ (then)”. 3.1.3 Noun Chaining In text segmentation algorithms such as TextTiling (Hearst.M, 1997), lexical chains are widely utilized for detecting a topic shift. We utilize such a feature as a clue to topic persistence. When two continuous actions are performed to the same ingredient, their topics are often identical. For example, because “ (grate)” and “ (raise)” are performed to the same ingredient “ (turnip)” , the topics</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistic, 12:175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reiko Hamada</author>
<author>Ichiro Ide</author>
<author>Shuichi Sakai</author>
<author>Hidehiko Tanaka</author>
</authors>
<title>Associating cooking video with related textbook.</title>
<date>2000</date>
<booktitle>In Proceedings of ACM Multimedia</booktitle>
<pages>237--241</pages>
<contexts>
<context position="16628" citStr="Hamada et al., 2000" startWordPosition="2610" endWordPosition="2613">When a verb of a clause is identical with that of the previous clause, they are likely to have the same topic. We utilize the fact that the adjoining two clauses contain an identical verbs or not as an observed feature. (8) a. (Add some red peppers.) 2[ ] indicates an element complemented with anaphora resolution. 759 b. (Add chicken wings.) 3.2 Image Features It is difficult for the current image processing technique to extract what object appears or what action is performing in video unless a detailed object/action model for a specific domain is constructed by hand. Therefore, referring to (Hamada et al., 2000), we focus our attention on color distribution at the bottom of the image, which is comparatively easy to exploit. As shown in Figure 1, we utilize the mass point of RGB in the bottom of the image at each clause. 3.3 Audio Features A cooking video contains various types of audio information, such as instructor’s speech, cutting sounds and frizzling sound. If cutting sound or frizzling sound could be distinguished from other sounds, they could be an aid to topic identification, but it is difficult to recognize them. As Galley et al. (Galley et al., 2003) pointed out, a longer silence often appe</context>
</contexts>
<marker>Hamada, Ide, Sakai, Tanaka, 2000</marker>
<rawString>Reiko Hamada, Ichiro Ide, Shuichi Sakai, and Hidehiko Tanaka. 2000. Associating cooking video with related textbook. In Proceedings of ACM Multimedia 2000 workshops, pages 237–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<marker>Hearst, 1997</marker>
<rawString>Hearst.M. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Wei Huang</author>
<author>Winston Hsu</author>
<author>Shin-Fu Chang</author>
</authors>
<title>Automatic closed caption alignment based on speech recognition transcripts. Technical report,</title>
<date>2003</date>
<location>Columbia ADVENT.</location>
<contexts>
<context position="22329" citStr="Huang et al., 2003" startWordPosition="3548" endWordPosition="3551"> the evaluation results. The unit of evaluation was a clause. The accuracy was improved by integrating linguistic and visual information compared to using linguistic / visual information alone. (Note that “visual information” uses pseudo-labeled data.) In addition, the accuracy was improved by using various discourse features. The reason why silence did not contribute to accuracy improvement is supposed to be that closed captions and video streams were not synchronized precisely due to time lagging of closed captions. To deal with this problem, an automatic closed caption alignment technique (Huang et al., 2003) will be applied or automatic speech recognition will be used as texts instead of closed captions with the advance of speech recognition technology. Figure 3 illustrates an improved example by adding visual information. In the case of using only linguistic information, this topic was recFigure 3: An improved example by adding visual information. ognized as sauteing, but this topic was actually preparation, which referred to the next topic. By using the visual information that background color was white, this topic was correctly recognized as preparation. We conducted another experiment to demo</context>
</contexts>
<marker>Huang, Hsu, Chang, 2003</marker>
<rawString>Chih-Wei Huang, Winston Hsu, and Shin-Fu Chang. 2003. Automatic closed caption alignment based on speech recognition transcripts. Technical report, Columbia ADVENT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Jasinschi</author>
<author>Nevenka Dimitrova</author>
<author>Thomas McGee</author>
<author>Lalitha Agnihotri</author>
<author>John Zimmerman</author>
<author>Dongge</author>
</authors>
<title>Integrated multimedia processing for topic segmentation and classification.</title>
<date>2001</date>
<booktitle>In Proceedings of IEEE International Conference on Image Processing(ICIP2003),</booktitle>
<pages>366--369</pages>
<contexts>
<context position="2276" citStr="Jasinschi et al., 2001" startWordPosition="330" endWordPosition="333">based on HMMs into several classes (for baseball sports video, for example, pitch view, running overview or audience view). In these studies, to achieve high accuracy, they relied on handmade domain-specific knowledge or trained HMMs with manually labeled data. Therefore, they cannot be easily extended to new domains on a large scale. In addition, although linguistic information, such as narration, speech of characters, and commentary, is intuitively useful for shot analysis, it is not utilized by many of the previous studies. Although some studies attempted to utilize linguistic information (Jasinschi et al., 2001; Babaguchi and Nitta, 2003), it was just keywords. In the field of Natural Language Processing, Barzilay and Lee have recently proposed a probabilistic content model for representing topics and topic shifts (Barzilay and Lee, 2004). This content model is based on HMMs wherein a state corresponds to a topic and generates sentences relevant to that topic according to a state-specific language model, which are learned from raw texts via analysis of word distribution patterns. In this paper, we describe an unsupervised topic identification method integrating linguistic and visual information usin</context>
<context position="8627" citStr="Jasinschi et al., 2001" startWordPosition="1291" endWordPosition="1294">ht extraction in baseball games (Chang et al., 2002). Nguyen et al. proposed a robust statistical framework to extract highlights from a baseball video (Nguyen et al., 2005). They applied multi-stream HMMs to control the weight among different features, such as principal component features capturing color information and frame-difference features for moving objects. Phung et al. proposed a probabilistic framework to exploit hierarchy structure for topic transition detection in educational videos (Q.Phung et al., 2005). Some studies attempted to utilize linguistic information in shot analysis (Jasinschi et al., 2001; Babaguchi and Nitta, 2003). For example, Babaguchi and Nitta segmented closed caption text into meaningful units and linked them to video streams in sports video. However, linguistic information they utilized was just keywords. 3 Features for Topic Identification First, we’ll describe the features that we use for topic identification, which are listed in Table 1. They consist of three modalities: linguistic, visual and audio modality. We utilize as linguistic information the instructor’s utterances in video, which can be divided into various types such as actions, tips, and even small talk. </context>
</contexts>
<marker>Jasinschi, Dimitrova, McGee, Agnihotri, Zimmerman, Dongge, 2001</marker>
<rawString>Radu Jasinschi, Nevenka Dimitrova, Thomas McGee, Lalitha Agnihotri, John Zimmerman, and Dongge. 2001. Integrated multimedia processing for topic segmentation and classification. In Proceedings of IEEE International Conference on Image Processing(ICIP2003), pages 366–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Fertilization of case frame dictionary for robust japanese case analysis.</title>
<date>2002</date>
<booktitle>In Proceedings of 19th COLING (COLING02),</booktitle>
<pages>425--431</pages>
<contexts>
<context position="15213" citStr="Kawahara and Kurohashi, 2002" startWordPosition="2364" endWordPosition="2367">h cutting it, we’ll fry.) (We cut in this cherry tomato, because we’ll fry it in oil.) Note that relations between clauses are recognized by clause-end patterns. Verb sense disambiguation by assigning to a case frame In general, a verb has multiple meanings/usages. For example, “ ” has multiple usages, “ (add salt)” and “ (carve with a knife)” , which appear in different topics. We do not extract a surface form of verb but a case frame, which is assigned by case analysis. Case frames are automatically constructed from Web cooking texts (12 million sentences) by clustering similar verb usages (Kawahara and Kurohashi, 2002). An example of the automatically constructed case frame is shown in Table 3. For example, “ (add salt)” is assigned to ireru:1 (add) and “ (carve with a knife)” is assigned to case frame ireru:2 (carve). (Raise this turnip on this basket.) However, in the case of spoken language, because there exist many omissions, it is often the case that noun chaining cannot be detected with surface word matching. Therefore, we detect noun chaining by using the anaphora resolution result2 of verbs (ex.(6)) and nouns (ex.(7)). The verb, noun anaphora resolution is conducted by the method proposed by (Kawaha</context>
</contexts>
<marker>Kawahara, Kurohashi, 2002</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of case frame dictionary for robust japanese case analysis. In Proceedings of 19th COLING (COLING02), pages 425–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Zero pronoun resolution based on automatically constructed case frames and structural preference of antecedents.</title>
<date>2004</date>
<booktitle>In Proceedings of The 1st International Joint Conference on Natural Language Processing,</booktitle>
<pages>334--341</pages>
<contexts>
<context position="15836" citStr="Kawahara and Kurohashi, 2004" startWordPosition="2467" endWordPosition="2471"> 2002). An example of the automatically constructed case frame is shown in Table 3. For example, “ (add salt)” is assigned to ireru:1 (add) and “ (carve with a knife)” is assigned to case frame ireru:2 (carve). (Raise this turnip on this basket.) However, in the case of spoken language, because there exist many omissions, it is often the case that noun chaining cannot be detected with surface word matching. Therefore, we detect noun chaining by using the anaphora resolution result2 of verbs (ex.(6)) and nouns (ex.(7)). The verb, noun anaphora resolution is conducted by the method proposed by (Kawahara and Kurohashi, 2004), (Sasano et al., 2004), respectively. (6) a. (Cut a cabbage.) b. [ ] (Wash it once.) (7) a. (Slice a carrot into 4-cm pieces.) b. [ ] (Peel its skin.) 3.1.4 Verb Chaining When a verb of a clause is identical with that of the previous clause, they are likely to have the same topic. We utilize the fact that the adjoining two clauses contain an identical verbs or not as an observed feature. (8) a. (Add some red peppers.) 2[ ] indicates an element complemented with anaphora resolution. 759 b. (Add chicken wings.) 3.2 Image Features It is difficult for the current image processing technique to ext</context>
</contexts>
<marker>Kawahara, Kurohashi, 2004</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2004. Zero pronoun resolution based on automatically constructed case frames and structural preference of antecedents. In Proceedings of The 1st International Joint Conference on Natural Language Processing, pages 334–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>A syntactic analysis method of long japanese sentences based on the detection of conjunctive structures.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="11089" citStr="Kurohashi and Nagao, 1994" startWordPosition="1679" endWordPosition="1682">n] ex. (Then, we ’ll cook a steak) (OK, we’ll fry.) [individual action] ex. (Cut off a step of this eggplant.) (Pour water into a pan.) [food state] ex. (There is no water in the carrot.) [note] ex. (Don’t cut this core off.) [substitution] ex. (You may use a leek.) [food/tool presentation] ex. Today, we use this handy mixer.) [small talk] ex. (Hello.) tures. An example of closed captions is shown in Figure 2. We first process them with the Japanese morphological analyzer, JUMAN (Kurohashi et al., 1994), and make syntactic/case analysis and anaphora resolution with the Japanese analyzer, KNP (Kurohashi and Nagao, 1994). Then, we perform the following process to extract linguistic features. 3.1.1 Extracting Utterances Referring to Actions Considering a clause as a basic unit, utterances referring to an action are extracted in the form of case frame, which is assigned by case analysis. This procedure is performed for generalization and word sense disambiguation. For example, “ (add salt)” and “ (add sugar into a pan)” are assigned to case frame ireru:1 (add) and “ (carve with a knife)” is assigned to case frame ireru:2 (carve). We describe this procedure in detail below. Utterance-type recognition To extract </context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. A syntactic analysis method of long japanese sentences based on the detection of conjunctive structures. Computational Linguistics, 20(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Toshihisa Nakamura</author>
<author>Yuji Matsumoto</author>
<author>Makoto Nagao</author>
</authors>
<title>Improvements of Japanese morphological analyzer JUMAN.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Workshop on Sharable Natural Language,</booktitle>
<pages>22--28</pages>
<contexts>
<context position="10971" citStr="Kurohashi et al., 1994" startWordPosition="1663" endWordPosition="1666">ance-type classification. (An underlined phrase means a pattern for recognizing utterance type.) [action declaration] ex. (Then, we ’ll cook a steak) (OK, we’ll fry.) [individual action] ex. (Cut off a step of this eggplant.) (Pour water into a pan.) [food state] ex. (There is no water in the carrot.) [note] ex. (Don’t cut this core off.) [substitution] ex. (You may use a leek.) [food/tool presentation] ex. Today, we use this handy mixer.) [small talk] ex. (Hello.) tures. An example of closed captions is shown in Figure 2. We first process them with the Japanese morphological analyzer, JUMAN (Kurohashi et al., 1994), and make syntactic/case analysis and anaphora resolution with the Japanese analyzer, KNP (Kurohashi and Nagao, 1994). Then, we perform the following process to extract linguistic features. 3.1.1 Extracting Utterances Referring to Actions Considering a clause as a basic unit, utterances referring to an action are extracted in the form of case frame, which is assigned by case analysis. This procedure is performed for generalization and word sense disambiguation. For example, “ (add salt)” and “ (add sugar into a pan)” are assigned to case frame ireru:1 (add) and “ (carve with a knife)” is assi</context>
</contexts>
<marker>Kurohashi, Nakamura, Matsumoto, Nagao, 1994</marker>
<rawString>Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. In Proceedings of the International Workshop on Sharable Natural Language, pages 22–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The rhetorical parsing of unrestricted texts: A surface-based approach.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="5885" citStr="Marcu, 2000" startWordPosition="879" endWordPosition="880">.M, 1997). This method is based on lexical co-occurrence. Galley et al. presented a domain-independent topic segmentation algorithm for multi-party speech (Galley et al., 2003). This segmentation algorithm uses automatically induced decision rules to combine linguistic features (lexical cohesion and cue phrases) and speech features (silences, overlaps and speaker change). These studies aim just at segmenting a given text, not at identifying topics of segmented texts. Marcu performed rhetorical parsing in the framework of Rhetorical Structure Theory (RST) based on a discourse-annotated corpus (Marcu, 2000). Although this model is suitable for analyzing local modification in a text, it is difficult for this model to capture the structure of topic transition in the whole text. In contrast, Barzilay and Lee modeled a content structure of texts within specific domains, such as earthquake and finance (Barzilay and Lee, 2004). They used HMMs wherein each state corresponds to a distinct topic (e.g., in earthquake domain, earthquake magnitude or previous earthquake occurrences) and generates sentences relevant to that topic according to a state-specific language model. Their method first create cluster</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The rhetorical parsing of unrestricted texts: A surface-based approach. Computational Linguistics, 26(3):395–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huu Bach Nguyen</author>
<author>Koichi Shinoda</author>
<author>Sadaoki Furui</author>
</authors>
<title>Robust highlight extraction using multistream hidden markov models for baseball video.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Image Processing 2005(ICIP2005),</booktitle>
<pages>173--176</pages>
<contexts>
<context position="1433" citStr="Nguyen et al., 2005" startWordPosition="199" endWordPosition="202">of our proposed method. 1 Introduction Recent years have seen the rapid increase of multimedia contents with the continuing advance of information technology. To make the best use of multimedia contents, it is necessary to segment them into meaningful segments and annotate them. Because manual annotation is extremely expensive and time consuming, automatic annotation technique is required. In the field of video analysis, there have been a number of studies on shot analysis for video retrieval or summarization (highlight extraction) using Hidden Markov Models (HMMs) (e.g., (Chang et al., 2002; Nguyen et al., 2005; Q.Phung et al., 2005)). These studies first segmented videos into shots, within which the camera motion is continuous, and extracted features such as color histograms and motion vectors. Then, they classified the shots based on HMMs into several classes (for baseball sports video, for example, pitch view, running overview or audience view). In these studies, to achieve high accuracy, they relied on handmade domain-specific knowledge or trained HMMs with manually labeled data. Therefore, they cannot be easily extended to new domains on a large scale. In addition, although linguistic informati</context>
<context position="8178" citStr="Nguyen et al., 2005" startWordPosition="1229" endWordPosition="1232">ustering stabilizes. They applied the constructed content model to two tasks: information ordering and summarization. We differ from this study in that we utilize multimodal features and domain-independent discourse features to achieve robust topic identification. In the field of video analysis, there have been a number of studies on shot analysis with HMMs. Chang et al. described a method for classifying shots into several classes for highlight extraction in baseball games (Chang et al., 2002). Nguyen et al. proposed a robust statistical framework to extract highlights from a baseball video (Nguyen et al., 2005). They applied multi-stream HMMs to control the weight among different features, such as principal component features capturing color information and frame-difference features for moving objects. Phung et al. proposed a probabilistic framework to exploit hierarchy structure for topic transition detection in educational videos (Q.Phung et al., 2005). Some studies attempted to utilize linguistic information in shot analysis (Jasinschi et al., 2001; Babaguchi and Nitta, 2003). For example, Babaguchi and Nitta segmented closed caption text into meaningful units and linked them to video streams in </context>
</contexts>
<marker>Nguyen, Shinoda, Furui, 2005</marker>
<rawString>Huu Bach Nguyen, Koichi Shinoda, and Sadaoki Furui. 2005. Robust highlight extraction using multistream hidden markov models for baseball video. In Proceedings of the International Conference on Image Processing 2005(ICIP2005), pages 173–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dinh Q Phung</author>
<author>Thi V T Duong</author>
<author>Hung H Bui</author>
<author>S Venkatesh</author>
</authors>
<title>Topic transition detection using hierarchical hidden markov and semi-markov models.</title>
<date>2005</date>
<booktitle>In Proceedings of ACM International Conference on Multimedia(ACM-MM05),</booktitle>
<pages>6--11</pages>
<contexts>
<context position="1456" citStr="Phung et al., 2005" startWordPosition="203" endWordPosition="206"> 1 Introduction Recent years have seen the rapid increase of multimedia contents with the continuing advance of information technology. To make the best use of multimedia contents, it is necessary to segment them into meaningful segments and annotate them. Because manual annotation is extremely expensive and time consuming, automatic annotation technique is required. In the field of video analysis, there have been a number of studies on shot analysis for video retrieval or summarization (highlight extraction) using Hidden Markov Models (HMMs) (e.g., (Chang et al., 2002; Nguyen et al., 2005; Q.Phung et al., 2005)). These studies first segmented videos into shots, within which the camera motion is continuous, and extracted features such as color histograms and motion vectors. Then, they classified the shots based on HMMs into several classes (for baseball sports video, for example, pitch view, running overview or audience view). In these studies, to achieve high accuracy, they relied on handmade domain-specific knowledge or trained HMMs with manually labeled data. Therefore, they cannot be easily extended to new domains on a large scale. In addition, although linguistic information, such as narration, </context>
<context position="8528" citStr="Phung et al., 2005" startWordPosition="1277" endWordPosition="1280">ith HMMs. Chang et al. described a method for classifying shots into several classes for highlight extraction in baseball games (Chang et al., 2002). Nguyen et al. proposed a robust statistical framework to extract highlights from a baseball video (Nguyen et al., 2005). They applied multi-stream HMMs to control the weight among different features, such as principal component features capturing color information and frame-difference features for moving objects. Phung et al. proposed a probabilistic framework to exploit hierarchy structure for topic transition detection in educational videos (Q.Phung et al., 2005). Some studies attempted to utilize linguistic information in shot analysis (Jasinschi et al., 2001; Babaguchi and Nitta, 2003). For example, Babaguchi and Nitta segmented closed caption text into meaningful units and linked them to video streams in sports video. However, linguistic information they utilized was just keywords. 3 Features for Topic Identification First, we’ll describe the features that we use for topic identification, which are listed in Table 1. They consist of three modalities: linguistic, visual and audio modality. We utilize as linguistic information the instructor’s uttera</context>
</contexts>
<marker>Phung, Duong, Bui, Venkatesh, 2005</marker>
<rawString>Dinh Q.Phung, Thi V.T Duong, Hung H.Bui, and S.Venkatesh. 2005. Topic transition detection using hierarchical hidden markov and semi-markov models. In Proceedings of ACM International Conference on Multimedia(ACM-MM05), pages 6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Automatic construction of nominal case frames and its application to indirect anaphora resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>1201--1207</pages>
<contexts>
<context position="15859" citStr="Sasano et al., 2004" startWordPosition="2472" endWordPosition="2475">tically constructed case frame is shown in Table 3. For example, “ (add salt)” is assigned to ireru:1 (add) and “ (carve with a knife)” is assigned to case frame ireru:2 (carve). (Raise this turnip on this basket.) However, in the case of spoken language, because there exist many omissions, it is often the case that noun chaining cannot be detected with surface word matching. Therefore, we detect noun chaining by using the anaphora resolution result2 of verbs (ex.(6)) and nouns (ex.(7)). The verb, noun anaphora resolution is conducted by the method proposed by (Kawahara and Kurohashi, 2004), (Sasano et al., 2004), respectively. (6) a. (Cut a cabbage.) b. [ ] (Wash it once.) (7) a. (Slice a carrot into 4-cm pieces.) b. [ ] (Peel its skin.) 3.1.4 Verb Chaining When a verb of a clause is identical with that of the previous clause, they are likely to have the same topic. We utilize the fact that the adjoining two clauses contain an identical verbs or not as an observed feature. (8) a. (Add some red peppers.) 2[ ] indicates an element complemented with anaphora resolution. 759 b. (Add chicken wings.) 3.2 Image Features It is difficult for the current image processing technique to extract what object appear</context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2004</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2004. Automatic construction of nominal case frames and its application to indirect anaphora resolution. In Proceedings of the 20th International Conference on Computational Linguistics, number 1201–1207, 8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>