<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000698">
<title confidence="0.940214">
Streaming for large scale NLP: Language Modeling
</title>
<author confidence="0.995399">
Amit Goyal, Hal Daum´e III, and Suresh Venkatasubramanian
</author>
<affiliation confidence="0.998925">
University of Utah, School of Computing
</affiliation>
<email confidence="0.997555">
{amitg,hal,suresh}@cs.utah.edu
</email>
<sectionHeader confidence="0.994774" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999107176470588">
In this paper, we explore a streaming al-
gorithm paradigm to handle large amounts
of data for NLP problems. We present an
efficient low-memory method for construct-
ing high-order approximate n-gram frequency
counts. The method is based on a determinis-
tic streaming algorithm which efficiently com-
putes approximate frequency counts over a
stream of data while employing a small mem-
ory footprint. We show that this method eas-
ily scales to billion-word monolingual corpora
using a conventional (8 GB RAM) desktop
machine. Statistical machine translation ex-
perimental results corroborate that the result-
ing high-n approximate small language model
is as effective as models obtained from other
count pruning methods.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995835625">
In many NLP problems, we are faced with the chal-
lenge of dealing with large amounts of data. Many
problems boil down to computing relative frequen-
cies of certain items on this data. Items can be
words, patterns, associations, n-grams, and others.
Language modeling (Chen and Goodman, 1996),
noun-clustering (Ravichandran et al., 2005), con-
structing syntactic rules for SMT (Galley et al.,
2004), and finding analogies (Turney, 2008) are
examples of some of the problems where we need
to compute relative frequencies. We use language
modeling as a canonical example of a large-scale
task that requires relative frequency estimation.
Computing relative frequencies seems like an
easy problem. However, as corpus sizes grow,
it becomes a highly computational expensive task.
</bodyText>
<table confidence="0.99843">
Cutoff Size BLEU NIST MET
Exact 367.6m 28.73 7.691 56.32
2 229.8m 28.23 7.613 56.03
3 143.6m 28.17 7.571 56.53
5 59.4m 28.33 7.636 56.03
10 18.3m 27.91 7.546 55.64
100 1.1m 28.03 7.607 55.91
200 0.5m 27.62 7.550 55.67
</table>
<tableCaption confidence="0.685905">
Table 1: Effect of count-based pruning on SMT per-
formance using EAN corpus. Results are according to
BLEU, NIST and METEOR (MET) metrics. Bold #s are
not statistically significant worse than exact model.
</tableCaption>
<bodyText confidence="0.999389347826087">
Brants et al. (2007) used 1500 machines for a
day to compute the relative frequencies of n-grams
(summed over all orders from 1 to 5) from 1.8TB
of web data. Their resulting model contained 300
million unique n-grams.
It is not realistic using conventional computing re-
sources to use all the 300 million n-grams for ap-
plications like speech recognition, spelling correc-
tion, information extraction, and statistical machine
translation (SMT). Hence, one of the easiest way to
reduce the size of this model is to use count-based
pruning which discards all n-grams whose count is
less than a pre-defined threshold. Although count-
based pruning is quite simple, yet it is effective for
machine translation. As we do not have a copy of
the web, we will use a portion of gigaword i.e. EAN
(see Section 4.1) to show the effect of count-based
pruning on performance of SMT (see Section 5.1).
Table 1 shows that using a cutoff of 100 produces a
model of size 1.1 million n-grams with a Bleu score
of 28.03. If we compare this with an exact model
of size 367.6 million n-grams, we see an increase of
0.8 points in Bleu (95% statistical significance level
</bodyText>
<page confidence="0.902745">
512
</page>
<note confidence="0.9170085">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 512–520,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<table confidence="0.987905125">
ǫ Size BLEU NIST MET
Exact 367.6m 28.73 7.691 56.32
1e-10 218.4m 28.64 7.669 56.33
5e-10 171.0m 28.48 7.666 56.38
1e-9 148.0m 28.56 7.646 56.51
5e-9 91.9m 28.27 7.623 56.16
1e-8 69.4m 28.15 7.609 56.19
5e-7 28.5m 28.08 7.595 55.91
</table>
<tableCaption confidence="0.995279">
Table 2: Effect of entropy-based pruning on SMT perfor-
mance using EAN corpus. Results are as in Table 1
</tableCaption>
<bodyText confidence="0.999937490566038">
is ≈ 0.53 Bleu). However, we need 300 times big-
ger model to get such an increase. Unfortunately, it
is not possible to integrate such a big model inside a
decoder using normal computation resources.
A better way of reducing the size of n-grams is to
use entropy pruning (Stolcke, 1998). Table 2 shows
the results with entropy pruning with different set-
tings of ǫ. We see that for three settings of ǫ equal to
1e-10, 5e-10 and 1e-9, we get Bleu scores compara-
ble to the exact model. However, the size of all these
models is not at all small. The size of smallest model
is 25% of the exact model. Even with this size it is
still not feasible to integrate such a big model inside
a decoder. If we take a model of size comparable to
count cutoff of 100, i.e., with ǫ = 5e-7, we see both
count-based pruning as well as entropy pruning per-
forms the same.
There also have been prior work on maintain-
ing approximate counts for higher-order language
models (LMs) ((Talbot and Osborne, 2007a; Tal-
bot and Osborne, 2007b; Talbot and Brants, 2008))
operates under the model that the goal is to store a
compressed representation of a disk-resident table of
counts and use this compressed representation to an-
swer count queries approximately.
There are two difficulties with scaling all the
above approaches as the order of the LM increases.
Firstly, the computation time to build the database of
counts increases rapidly. Secondly, the initial disk
storage required to maintain these counts, prior to
building the compressed representation is enormous.
The method we propose solves both of these prob-
lems. We do this by making use of the streaming al-
gorithm paradigm (Muthukrishnan, 2005). Working
under the assumption that multiple-GB models are
infeasible, our goal is to instead of estimating a large
model and then compressing it, we directly estimate
a small model. We use a deterministic streaming al-
gorithm (Manku and Motwani, 2002) that computes
approximate frequency counts of frequently occur-
ring n-grams. This scheme is considerably more ac-
curate in getting the actual counts as compared to
other schemes (Demaine et al., 2002; Karp et al.,
2003) that find the set of frequent items without car-
ing about the accuracy of counts.
We use these counts directly as features in an
SMT system, and propose a direct way to integrate
these features into an SMT decoder. Experiments
show that directly storing approximate counts of fre-
quent 5-grams compared to using count or entropy-
based pruning counts gives equivalent SMT perfor-
mance, while dramatically reducing the memory us-
age and getting rid of pre-computing a large model.
</bodyText>
<sectionHeader confidence="0.997023" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999187">
2.1 n-gram Language Models
</subsectionHeader>
<bodyText confidence="0.9999705">
Language modeling is based on assigning probabil-
ities to sentences. It can either compute the proba-
bility of an entire sentence or predict the probability
of the next word in a sequence. Let wm1 denote a se-
quence of words (w1, ... , wm). The probability of
estimating word wm depends on previous n-1 words
where n denotes the size of n-gram. This assump-
tion that probability of predicting a current word de-
pends on the previous words is called a Markov as-
sumption, typically estimated by relative frequency:
</bodyText>
<equation confidence="0.9988875">
m−1 (1)
C(wm−n+1)
</equation>
<bodyText confidence="0.999091">
Eq 1 estimates the n-gram probability by taking the
ratio of observed frequency of a particular sequence
and the observed frequency of the prefix. This is
precisely the relative frequency estimate we seek.
</bodyText>
<subsectionHeader confidence="0.999798">
2.2 Large-scale Language modeling
</subsectionHeader>
<bodyText confidence="0.990748888888889">
Using higher order LMs to improve the accuracy
of SMT is not new. (Brants et al., 2007; Emami
et al., 2007) built 5-gram LMs over web using dis-
tributed cluster of machines and queried them via
network requests. Since the use of cluster of ma-
chines is not always practical, (Talbot and Osborne,
2007b; Talbot and Osborne, 2007a) showed a ran-
domized data structure called Bloom filter, that can
be used to construct space efficient language models
</bodyText>
<equation confidence="0.99662075">
m−1
C(
P(wm  |wm−1
m−n+1) = wm−n+1wm)
</equation>
<page confidence="0.985863">
513
</page>
<bodyText confidence="0.999892833333333">
for SMT. (Talbot and Brants, 2008) presented ran-
domized language model based on perfect hashing
combined with entropy pruning to achieve further
memory reductions. A problem mentioned in (Tal-
bot and Brants, 2008) is that the algorithm that com-
putes the compressed representation might need to
retain the entire database in memory; in their paper,
they design strategies to work around this problem.
(Federico and Bertoldi, 2006) also used single ma-
chine and fewer bits to store the LM probability by
using efficient prefix trees.
(Uszkoreit and Brants, 2008) used partially class-
based LMs together with word-based LMs to im-
prove SMT performance despite the large size of
the word-based models used. (Schwenk and Koehn,
2008; Zhang et al., 2006) used higher language mod-
els at time of re-ranking rather than integrating di-
rectly into the decoder to avoid the overhead of
keeping LMs in the main memory since disk lookups
are simply too slow. Now using higher order LMs at
time of re-ranking looks like a good option. How-
ever, the target n-best hypothesis list is not diverse
enough. Hence if possible it is always better to inte-
grate LMs directly into the decoder.
</bodyText>
<subsectionHeader confidence="0.998918">
2.3 Streaming
</subsectionHeader>
<bodyText confidence="0.999981333333333">
Consider an algorithm that reads the input from a
read-only stream from left to right, with no ability
to go back to the input that it has already processed.
This algorithm has working storage that it can use to
store parts of the input or other intermediate compu-
tations. However, (and this is a critical constraint),
this working storage space is significantly smaller
than the input stream length. For typical algorithms,
the storage size is of the order of logk N, where N
is the input size and k is some constant.
Stream algorithms were first developed in the
early 80s, but gained in popularity in the late 90s
as researchers first realized the challenges of dealing
with massive data sets. A good survey of the model
and core challenges can be found in (Muthukrish-
nan, 2005). There has been considerable work on the
problem of identifying high-frequency items (items
with frequency above a threshold), and a detailed re-
view of these methods is beyond the scope of this ar-
ticle. A new survey by (Cormode and Hadjielefthe-
riou, 2008) comprehensively reviews the literature.
</bodyText>
<sectionHeader confidence="0.9360535" genericHeader="method">
3 Space-Efficient Approximate Frequency
Estimation
</sectionHeader>
<bodyText confidence="0.99939392">
Prior work on approximate frequency estimation for
language models provide a “no-false-negative” guar-
antee, ensuring that counts for n-grams in the model
are returned exactly, while working to make sure the
false-positive rate remains small (Talbot and Os-
borne, 2007a). The notion of approximation we use
is different: in our approach, it is the actual count
values that will be approximated. We also exploit
the fact that low-frequency n-grams, while consti-
tuting the vast majority of the set of unique n-grams,
are usually smoothed away and are less likely to in-
fluence the language model significantly. Discard-
ing low-frequency n-grams is particularly important
in a stream setting, because it can be shown in gen-
eral that any algorithm that generates approximate
frequency counts for all n-grams requires space lin-
ear in the input stream (Alon et al., 1999).
We employ an algorithm for approximate fre-
quency counting proposed by (Manku and Motwani,
2002) in the context of database management. Fix
parameters s E (0, 1), and E E (0, 1), E ≪ s. Our
goal is to approximately find all n-grams with fre-
quency at least sN. For an input stream of n-grams
of length N, the algorithm outputs a set of items
(and frequencies) and guarantees the following:
</bodyText>
<listItem confidence="0.998186444444444">
• All items with frequencies exceeding sN are
output (no false negatives).
• No item with frequency less than (s − E)N is
output (few false positives).
• All reported frequencies are less than the true
frequencies by at most EN (close-to-exact fre-
quencies).
• The space used by the algorithm is
O(1ǫ log EN).
</listItem>
<bodyText confidence="0.999939375">
A simple example illustrates these properties. Let
us fix s = 0.01, E = 0.001. Then the algorithm guar-
antees that all n-grams with frequency at least 1%
will be returned, no element with frequency less than
0.9% will be returned, and all frequencies will be no
more than 0.1% away from the true frequencies. The
space used by the algorithm is O(log N), which can
be compared to the much larger (close to N) space
</bodyText>
<page confidence="0.987292">
514
</page>
<bodyText confidence="0.999948333333333">
needed to store the initial frequency counts. In addi-
tion, the algorithm runs in linear time by definition,
requiring only one pass over the input. Note that
there might be ǫ1 elements with frequency at least
ǫN, and so the algorithm uses optimal space (up to
a logarithmic factor).
</bodyText>
<subsectionHeader confidence="0.998912">
3.1 The Algorithm
</subsectionHeader>
<bodyText confidence="0.999971413793104">
We present a high-level overview of the algorithm;
for more details, the reader is referred to (Manku
and Motwani, 2002). The algorithm proceeds by
conceptually dividing the stream into epochs, each
containing 1/ǫ elements. Note that there are ǫN
epochs. Each such epoch has an ID, starting from
1. The algorithm maintains a list of tuples1 of the
form (e, f, A), where e is an n-gram, f is its re-
ported frequency, and A is the maximum error in the
frequency estimation. While the algorithm reads n-
grams associated with the current epoch, it does one
of two things: if the new element e is contained in
the list of tuples, it merely increments the frequency
count f. If not, it creates a new tuple of the form
(e, 1, T −1), where T is the ID of the current epoch.
After each epoch, the algorithm “cleans house” by
eliminating tuples whose maximum true frequency
is small. Formally, if the epoch that just ended
has ID T, then the algorithm deletes all tuples sat-
isfying condition f + A G T. Since T G ǫN,
this ensures that no low-frequency tuples are re-
tained. When all elements in the stream have been
processed, the algorithm returns all tuples (e, f, A)
where f &gt; (s−ǫ)N. In practice, however we do not
care about s and return all tuples. At a high level,
the reason the algorithm works is that if an element
has high frequency, it shows up more than once each
epoch, and so its frequency gets updated enough to
stave off elimination.
</bodyText>
<sectionHeader confidence="0.998289" genericHeader="method">
4 Intrinsic Evaluation
</sectionHeader>
<bodyText confidence="0.9994944">
We conduct a set of experiments with approxi-
mate n-gram counts (stream counts) produced by
the stream algorithm. We define various metrics on
which we evaluate the quality of stream counts com-
pared with exact n-gram counts (true counts). To
</bodyText>
<footnote confidence="0.954923">
1We use hash tables to store tuples; however smarter data
structures like suffix trees could also be used.
</footnote>
<table confidence="0.999633166666667">
Corpus Gzip-MB M-wrds Perplexity
EP 63 38 1122.69
afe 417 171 1829.57
apw 1213 540 1872.96
nyt 2104 914 1785.84
xie 320 132 1885.33
</table>
<tableCaption confidence="0.993335">
Table 3: Corpus Statistics and perplexity of LMs made
with each of these corpuses on development set
</tableCaption>
<bodyText confidence="0.9932895">
evaluate the quality of stream counts on these met-
rics, we carry out three experiments.
</bodyText>
<subsectionHeader confidence="0.968734">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999987125">
The freely available English side of Europarl (EP)
and Gigaword corpus (Graff, 2003) is used for
computing n-gram counts. We only use EP along
with two sections of the Gigaword corpus: Agence
France Press English Service(afe) and The New
York Times Newswire Service (nyt). The unigram
language models built using these corpuses yield
better perplexity scores on the development set (see
Section 5.1) compared to The Xinhua News Agency
English Service (xie) and Associated Press World-
stream English Service (apw) as shown in Table 3.
The LMs are build using the SRILM language mod-
elling toolkit (Stolcke, 2002) with modified Kneser-
Ney discounting and interpolation. The evaluation
of stream counts is done on EP+afe+nyt (EAN) cor-
pus, consisting of 1.1 billion words.
</bodyText>
<subsectionHeader confidence="0.996395">
4.2 Description of the metrics
</subsectionHeader>
<bodyText confidence="0.9999956">
To evaluate the quality of counts produced by our
stream algorithm four different metrics are used.
The accuracy metric measures the quality of top N
stream counts by taking the fraction of top N stream
counts that are contained in the top N true counts.
</bodyText>
<sectionHeader confidence="0.694917" genericHeader="method">
Accuracy =
</sectionHeader>
<subsectionHeader confidence="0.506134">
True Counts
</subsectionHeader>
<bodyText confidence="0.999677333333333">
Spearman’s rank correlation coefficient or Spear-
man’s rho(ρ) computes the difference between the
ranks of each observation (i.e. n-gram) on two vari-
ables (that are top N stream and true counts). This
measure captures how different the stream count or-
dering is from the true count ordering.
</bodyText>
<equation confidence="0.99428625">
1 − 6Ed2
i
ρ
N(N2 − 1)
</equation>
<bodyText confidence="0.412774">
Stream Counts n True Counts
</bodyText>
<page confidence="0.967809">
515
</page>
<bodyText confidence="0.980147166666667">
di is the difference between the ranks of correspond-
ing elements Xi and Yi; N is the number of elements
found in both sets; Xi and Yi in our case denote the
stream and true counts.
Mean square error (MSE) quantifies the amount
by which a predicted value differs from the true
value. In our case, it estimates how different the
stream counts are from the true counts.
(truei − predictedi)2
true and predicted denotes values of true and stream
counts; N denotes the number of stream counts con-
tained in true counts.
</bodyText>
<subsectionHeader confidence="0.998696">
4.3 Varying c experiments
</subsectionHeader>
<bodyText confidence="0.998334709677419">
In our first experiment, we use accuracy, p and MSE
metrics for evaluation. Here, we compute 5-gram
stream counts with different settings of c on the EAN
corpus. c controls the number of stream counts pro-
duced by the algorithm. The results in Table 4 sup-
port the theory that decreasing the value of c im-
proves the quality of stream counts. Also, as ex-
pected, the algorithm produces more stream counts
with smaller values of c. The evaluation of stream
counts obtained with c = 50e-8 and 20e-8 reveal that
the stream counts learned with this large value are
more susceptible to errors.
If we look closely at the counts for c = 50e-8, we
see that we get at least 30% of the stream counts
from 245k true counts. This number is not signifi-
cantly worse than the 36% of stream counts obtained
from 4,018k true counts for the smallest value of
c = 5e-8. However, if we look at the other two met-
rics, the ranking correlation p of stream counts com-
pared with true counts on c = 50e-8 and 20e-8 is low
compared to other c values. For the MSE, the error
with stream counts on these cm values is again high
compared to other values. As we decrease the value
of c we continually get better results: decreasing c
pushes the stream counts towards the true counts.
However, using a smaller c increases the memory
usage. Looking at the evaluation, it is therefore ad-
visable to use 5-gram stream counts produced with
at most c &lt; 10e-7 for the EAN corpus.
Since it is not possible to compute true 7-grams
counts on EAN with available computing resources,
</bodyText>
<table confidence="0.999301833333333">
c 5-gram Acc p MSE
produced
50e-8 245k 0.294 -3.6097 0.4954
20e-8 726k 0.326 -2.6517 0.1155
10e-8 1655k 0.352 -1.9960 0.0368
5e-8 4018k 0.359 -1.7835 0.0114
</table>
<tableCaption confidence="0.9989025">
Table 4: Evaluating quality of 5-gram stream counts for
different settings of c on EAN corpus
</tableCaption>
<table confidence="0.999421833333333">
c 7-gram Acc p MSE
produced
50e-8 44k 0.509 0.3230 0.0341
20e-8 128k 0.596 0.5459 0.0063
10e-8 246k 0.689 0.7413 0.0018
5e-8 567k 0.810 0.8599 0.0004
</table>
<tableCaption confidence="0.997476">
Table 5: Evaluating quality of 7-gram stream counts for
different settings of c on EP corpus
</tableCaption>
<bodyText confidence="0.999938636363636">
we carry out a similar experiment for 7-grams on EP
to verify the results for higher order n-grams 2. The
results in Table 5 tell a story similar to our results for
7-grams. The size of EP corpus is much smaller than
EAN and so we see even better results on each of the
metrics with decreasing the value of c. The overall
trend remains the same; here too, setting c &lt; 10e-
8 is the most effective strategy. The fact that these
results are consistent across two datasets of different
sizes and different n-gram sizes suggests that they
will carry over to other tasks.
</bodyText>
<subsectionHeader confidence="0.996761">
4.4 Varying top K experiments
</subsectionHeader>
<bodyText confidence="0.999915266666667">
In the second experiment, we evaluate the quality
of the top K (sorted by frequency) 5-gram stream
counts. Here again, we use accuracy, p and MSE for
evaluation. We fix the value of c to 5e-8 and com-
pute 5-gram stream counts on the EAN corpus. We
vary the value of K between 100k and 4,018k (i.e
all the n-gram counts produced by the stream algo-
rithm). The experimental results in Table 6 support
the theory that stream count algorithm computes the
exact count of most of the high frequency n-grams.
Looking closer, we see that if we evaluate the algo-
rithm on just the top 100k 5-grams (roughly 5% of
all 5-grams produced), we see almost perfect results.
Further, if we take the top 1, 000k 5-grams (approx-
imately 25% of all 5-grams) we again see excellent
</bodyText>
<footnote confidence="0.9492725">
2Similar evaluation scores are observed for 9-gram stream
counts with different values of ǫ on EP corpus.
</footnote>
<equation confidence="0.9843374">
1
MSE =
N
N
i=1
</equation>
<page confidence="0.997229">
516
</page>
<table confidence="0.9988215">
Top K Accuracy p MSE
100k 0.994 0.9994 0.01266
500k 0.934 0.9795 0.0105
1000k 0.723 0.8847 0.0143
2000k 0.504 0.2868 0.0137
4018k 0.359 -1.7835 0.0114
</table>
<tableCaption confidence="0.843663">
Table 6: Evaluating top K sorted 5-gram stream counts
for c=5e-8 on EAN corpus
</tableCaption>
<table confidence="0.999601333333333">
Top K Accuracy p MSE
10k 0.996 0.9997 0.0015
20k 0.989 0.9986 0.0016
50k 0.950 0.9876 0.0016
100k 0.876 0.9493 0.0017
246k 0.689 0.7413 0.0018
</table>
<tableCaption confidence="0.972465">
Table 7: Evaluating top K sorted 7-gram stream counts
for c=10e-8 on EP corpus
</tableCaption>
<bodyText confidence="0.999691793103448">
performance on all metrics. The accuracy of the re-
sults decrease slightly, but the p and MSE metrics
are not decreased that much in comparison. Perfor-
mance starts to degrade as we get to 2, 000k (over
50% of all 5-grams), a result that is not too surpris-
ing. However, even here we note that the MSE is
low, suggesting that the frequencies of stream counts
(found in top K true counts) are very close to the
true counts. Thus, we conclude that the quality of
the 5-gram stream counts produced for this value of
c is quite high (in relation to the true counts).
As before, we corroborate our results with higher
order n-grams. We evaluate the quality of top K 7-
gram stream counts on EP.3 Since EP is a smaller
corpus, we evaluate the stream counts produced by
setting c to 10e-8. Here we vary the value of K be-
tween 10k and 246k (the total number produced by
the stream algorithm). Results are shown in Table
7. As we saw earlier with 5-grams, the top 10k (i.e.
approximately 5% of all 7-grams) are of very high
quality. Results, and this remains true even when
we increase K to 100k. There is a drop in the accu-
racy and a slight drop in p, while the MSE remains
the same. Taking all counts again shows a signifi-
cant decrease in both accuracy and p scores, but this
does not affect MSE scores significantly. Hence, the
7-gram stream counts i.e. 246k counts produced by
c = 10e-8 are quite accurate when compared to the
top 246k true counts.
</bodyText>
<subsectionHeader confidence="0.8537935">
4.5 Analysis of tradeoff between coverage and
space
</subsectionHeader>
<bodyText confidence="0.999618">
In our third experiment, we investigate whether a
large LM can help MT performance. We evaluate
the coverage of stream counts built on the EAN cor-
pus on the test data for SMT experiments (see Sec-
</bodyText>
<footnote confidence="0.8749715">
3Similar evaluation scores are observed for different top K
sorted 9-gram stream counts with a=10e-8 on EP corpus.
</footnote>
<bodyText confidence="0.957551">
tion 5.1) with different values of cm. We compute
the recall of each model against 3071 sentences of
test data where recall is the fraction of number of
n-grams of a dataset found in stream counts.
Number of n-grams found in stream counts
</bodyText>
<equation confidence="0.5868115">
Recall =
Number of n-grams in dataset
</equation>
<bodyText confidence="0.999931580645161">
We build unigram, bigram, trigram, 5-gram and
7-gram with four different values of c. Table 8 con-
tains the gzip size of the count file and the recall
of various different stream count n-grams. As ex-
pected, the recall with respect to true counts is max-
imum for unigrams, bigrams, trigrams and 5-grams.
However the amount of space required to store all
true counts in comparison to stream counts is ex-
tremely high: we need 4.8GB of compressed space
to store all the true counts for 5-grams.
For unigram models, we see that the recall scores
are good for all values of c. If we compare the
approximate stream counts produced by largest c
(which is worst) to all true counts, we see that the
stream counts compressed size is 50 times smaller
than the true counts size, and is only three points
worse in recall. Similar trends hold for bigrams,
although the loss in recall is higher. As with uni-
grams, the loss in recall is more than made up for by
the memory savings (a factor of nearly 150). For
trigrams, we see a 14 point loss in recall for the
smallest c, but a memory savings of 400 times. For
5-grams, the best recall value is .020 (1.2k out of
60k 5-gram stream counts are found in the test set).
However, compared with the true counts we only
loss a recall of 0.05 (4.3k out of 60k) points but
memory savings of 150 times. In extrinsic evalua-
tions, we will show that integrating 5-gram stream
counts with an SMT system performs slightly worse
than the true counts, while dramatically reducing the
memory usage.
</bodyText>
<page confidence="0.990466">
517
</page>
<table confidence="0.999750625">
N-gram unigram bigram trigram 5-gram 7-gram
ǫ Gzip Recall Gzip Recall Gzip Recall Gzip Recall Gzip Recall
MB MB MB MB MB
50e-8 .352 .785 2.3 .459 3.3 .167 1.9 .006 .864 5.6e-5
20e-8 .568 .788 4.5 .494 7.6 .207 5.3 .011 2.7 1.3e-4
10e-8 .824 .791 7.6 .518 15 .237 13 .015 9.7 4.1e-4
5e-8 1.3 .794 13 .536 30 .267 31 .020 43 5.9e-4
all 17 .816 228 .596 1200 .406 4800 .072 NA
</table>
<tableCaption confidence="0.999892">
Table 8: Gzipped space required to store n-gram counts on disk and their coverage on a test set with different ǫm
</tableCaption>
<bodyText confidence="0.999951">
For 7-gram we can not compute the true n-gram
counts due to limitations of available computational
resources. The memory requirements with smallest
value of ǫ are similar to those of 5-gram, but the re-
call values are quite small. For 7-grams, the best re-
call value is 5.9e-4 which means that stream counts
contains only 32 out of 54k 7-grams contained in
test set. The small recall value for 7-grams suggests
that these counts may not be that useful in SMT.
We further substantiate our findings in our extrinsic
evaluations. There we show that integrating 7-gram
stream counts with an SMT system does not affect
its overall performance significantly.
</bodyText>
<sectionHeader confidence="0.972356" genericHeader="method">
5 Extrinsic Evaluation
</sectionHeader>
<subsectionHeader confidence="0.969208">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9986353125">
All the experiments conducted here make use of
publicly available resources. Europarl (EP) corpus
French-English section is used as parallel data. The
publicly available Moses4 decoder is used for train-
ing and decoding (Koehn and Hoang, 2007). The
news corpus released for ACL SMT workshop in
2007 consisting of 1057 sentences5 is used as the de-
velopment set. Minimum error rate training (MERT)
is used on this set to obtain feature weights to opti-
mize translation quality. The final SMT system per-
formance is evaluated on a uncased test set of 3071
sentences using the BLEU (Papineni et al., 2002),
NIST (Doddington, 2002) and METEOR (Banerjee
and Lavie, 2005) scores. The test set is the union of
the 2007 news devtest and 2007 news test data from
ACL SMT workshop 2007.6
</bodyText>
<footnote confidence="0.9963252">
4http://www.statmt.org/moses/
5http://www.statmt.org/wmt07/
6We found that testing on Parliamentary test data was com-
pletely insensitive to large n-gram LMs, even when these LMs
are exact. This suggests that for SMT performance, more data
</footnote>
<subsectionHeader confidence="0.972838">
5.2 Integrating stream counts feature into
decoder
</subsectionHeader>
<bodyText confidence="0.999874111111111">
Our method only computes high-frequency n-gram
counts; it does not estimate conditional probabili-
ties. We can either turn these counts into conditional
probabilities (by using SRILM) or use the counts di-
rectly. We observed no significant difference in per-
formance between these two approaches. However,
using the counts directly consumes significantly less
memory at run-time and is therefore preferable. Due
to space constraints, SRILM results are omitted.
The only remaining open question is: how should
we turn the counts into a feature that can be used in
an SMTsystem? We considered several alternatives;
the most successful was a simple weighted count
of n-gram matches of varying size, appropriately
backed-off. Specifically, consider an n-gram model.
For every sequence of words wi, ... , wi+N_1, we
obtain a feature score computed recursively accord-
ing to Eq (2).
</bodyText>
<equation confidence="0.99808">
f(wi) = log „C(Zi)« (2)
„C(wi, ... , wi+k) «
f(wi, . . . , wi+k) = log Z
1
+ 2 f(wi+1, ... ,wi+k)
</equation>
<bodyText confidence="0.989494555555555">
Here, 21 is the backoff factor and Z is the largest
count in the count set (the presence of Z is simply to
ensure that these values remain manageable). In or-
der to efficiently compute these features, we store
the counts in a suffix-tree. The computation pro-
ceeds by first considering wi+N_1 alone and then
“expanding” to consider the bigram, then trigram
and so on. The advantage to this order of computa-
tion is that the recursive calls can cease whenever a
</bodyText>
<footnote confidence="0.400849">
is better only if it comes from the right domain.
</footnote>
<page confidence="0.974018">
518
</page>
<table confidence="0.999588642857143">
n-gram(c) BLEU NIST MET Mem
GB
3 EP(exact) 25.57 7.300 54.48 2.7
5 EP(exact) 25.79 7.286 54.44 2.9
3 EAN(exact) 27.04 7.428 55.07 4.6
5 EAN(exact) 28.73 7.691 56.32 20.5
4(10e-8) 27.36 7.506 56.19 2.7
4(5e-8) 27.40 7.507 55.90 2.8
5(10e-8) 27.97 7.605 55.52 2.8
5(5e-8) 27.98 7.611 56.07 2.8
7(10e-8) 27.97 7.590 55.88 2.9
7(5e-8) 27.88 7.577 56.01 2.9
9(10e-8) 28.18 7.611 55.95 2.9
9(5e-8) 27.98 7.608 56.08 2.9
</table>
<tableCaption confidence="0.996843333333333">
Table 9: Evaluating SMT with different LMs on EAN.
Results are according to BLEU, NIST and MET metrics.
Bold #s are not statistically significant worse than exact.
</tableCaption>
<bodyText confidence="0.8911505">
zero count is reached. (Extending Moses to include
this required only about 100 lines of code.)
</bodyText>
<sectionHeader confidence="0.577008" genericHeader="evaluation">
5.3 Results
</sectionHeader>
<bodyText confidence="0.999929022222223">
Table 9 summarizes SMT results. We have 4 base-
line LMs that are conventional LMs smoothed using
modified Kneser-Ney smoothing. The first two tri-
gram and 5-gram LMs are built on EP corpus and
the other two are built on EAN corpus. Table 9
show that there is not much significant difference
in SMT results of 5-gram and trigram LM on EP.
As expected, the trigram built on the large corpus
EAN gets an improvement of 1.5 Bleu Score. How-
ever, unlike the EP corpus, building a 5-gram LM
on EAN (huge corpus) gets an improvement of 3.2
Bleu Score. (The 95% statistical significance bound-
ary is about ± 0.53 Bleu on the test data, 0.077 Nist
and 0.16 Meteor according to bootstrap resampling)
We see similar gains in Nist and Meteor metrics as
shown in Table 9.
We use stream counts computed with two values
of c, 5e-8 and 10e-8 on EAN corpus. We use all
the stream counts produced by the algorithm. 4, 5, 7
and 9 order n-gram stream counts are computed with
these settings of c. These counts are used along with
a trigram LM built on EP to improve SMT perfor-
mance. The memory usage (Mem) shown in Table
9 is the full memory size required to run on the test
data (including phrase tables).
Adding 4-gram and 5-gram stream counts as fea-
ture helps the most. The performance gain by using
5-gram stream counts is slightly worse than com-
pared to true 5-gram LM on EAN. However, using
5-gram stream counts directly is more memory ef-
ficient. Also, the gains for stream counts are ex-
actly the same as we saw for same sized count-
based and entropy-based pruning counts in Table 1
and 2 respectively. Moreover, unlike the pruning
methods, our algorithm directly computes a small
model, as opposed to compressing a pre-computed
large model.
Adding 7-gram and 9-gram does not help signifi-
cantly, a fact anticipated by the low recall of 7-gram-
based counts that we saw in Section 4.5. The results
with two different settings of c are largely the same.
This validates our intrinsic evaluation results in Sec-
tion 4.3 that stream counts learned using c ≤ 10e-8
are of good quality, and that the quality of the stream
counts is high.
</bodyText>
<sectionHeader confidence="0.999016" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99994324">
We have proposed an efficient, low-memory method
to construct high-order approximate n-gram LMs.
Our method easily scales to billion-word monolin-
gual corpora on conventional (8GB) desktop ma-
chines. We have demonstrated that approximate n-
gram features could be used as a direct replacement
for conventional higher order LMs in SMT with
significant reductions in memory usage. In future,
we will be looking into building streaming skip n-
grams, and other variants (like cluster n-grams).
In NLP community, it has been shown that having
more data results in better performance (Ravichan-
dran et al., 2005; Brants et al., 2007; Turney, 2008).
At web scale, we have terabytes of data and that can
capture broader knowledge. Streaming algorithm
paradigm provides a memory and space-efficient
platform to deal with terabytes of data. We hope
that other NLP applications (where we need to com-
pute relative frequencies) like noun-clustering, con-
structing syntactic rules for SMT, finding analogies,
and others can also benefit from streaming methods.
We also believe that stream counts can be applied to
other problems involving higher order LMs such as
speech recognition, information extraction, spelling
correction and text generation.
</bodyText>
<page confidence="0.997356">
519
</page>
<sectionHeader confidence="0.983195" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999770852941176">
Noga Alon, Yossi Matias, and Mario Szegedy. 1999. The
space complexity of approximating the frequency mo-
ments. J. Comput. Syst. Sci., 58(1).
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
S. Chen and J. Goodman. 1996. An Empirical Study
of Smoothing Techniques for Language Modeling. In
Proceedings of 34th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 310–318,
Santa Cruz, CA, June.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
E.D. Demaine, A. Lopez-Ortiz, and J.I. Munro. 2002.
Frequency estimation of internet packet streams with
limited space.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of the 2007 IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), volume 4, pages 37–40.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings on the Workshop on
Statistical Machine Translation at ACL06.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings ofHLT/NAACL-04.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Richard M. Karp, Christos H. Papadimitriou, and Scott
Shenker. 2003. A simple algorithm for finding fre-
quent elements in streams and bags.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868–876.
G. S. Manku and R. Motwani. 2002. Approximate fre-
quency counts over data streams. In Proceedings of
the 28th International Conference on Very Large Data
Bases.
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Foundations and Trends in Theoretical
Computer Science, 1(2).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In ACL ’05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics.
Holger Schwenk and Philipp Koehn. 2008. Large and
diverse language models for statistical machine trans-
lation. In Proceedings of The Third International Joint
Conference on Natural Language Processing (IJCNP).
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In In Proc. DARPA Broadcast
News Transcription and Understanding Workshop.
A. Stolcke. 2002. SRILM – An Extensible Language
Modeling Toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, CO, September.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings ofACL-08: HLT.
David Talbot and Miles Osborne. 2007a. Randomised
language modelling for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
David Talbot and Miles Osborne. 2007b. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning (EM
NLP-CoNLL).
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-08: HLT.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing.
</reference>
<page confidence="0.996961">
520
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986443">
<title confidence="0.999946">Streaming for large scale NLP: Language Modeling</title>
<author confidence="0.995803">Amit Goyal</author>
<author confidence="0.995803">Hal Daum´e</author>
<author confidence="0.995803">Suresh Venkatasubramanian</author>
<affiliation confidence="0.993691">University of Utah, School of Computing</affiliation>
<abstract confidence="0.9997375">In this paper, we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems. We present an efficient low-memory method for constructhigh-order approximate frequency counts. The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint. We show that this method easily scales to billion-word monolingual corpora a conventional RAM) desktop machine. Statistical machine translation experimental results corroborate that the resultmodel is as effective as models obtained from other count pruning methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Noga Alon</author>
<author>Yossi Matias</author>
<author>Mario Szegedy</author>
</authors>
<title>The space complexity of approximating the frequency moments.</title>
<date>1999</date>
<journal>J. Comput. Syst. Sci.,</journal>
<volume>58</volume>
<issue>1</issue>
<contexts>
<context position="10860" citStr="Alon et al., 1999" startWordPosition="1797" endWordPosition="1800">albot and Osborne, 2007a). The notion of approximation we use is different: in our approach, it is the actual count values that will be approximated. We also exploit the fact that low-frequency n-grams, while constituting the vast majority of the set of unique n-grams, are usually smoothed away and are less likely to influence the language model significantly. Discarding low-frequency n-grams is particularly important in a stream setting, because it can be shown in general that any algorithm that generates approximate frequency counts for all n-grams requires space linear in the input stream (Alon et al., 1999). We employ an algorithm for approximate frequency counting proposed by (Manku and Motwani, 2002) in the context of database management. Fix parameters s E (0, 1), and E E (0, 1), E ≪ s. Our goal is to approximately find all n-grams with frequency at least sN. For an input stream of n-grams of length N, the algorithm outputs a set of items (and frequencies) and guarantees the following: • All items with frequencies exceeding sN are output (no false negatives). • No item with frequency less than (s − E)N is output (few false positives). • All reported frequencies are less than the true frequenc</context>
</contexts>
<marker>Alon, Matias, Szegedy, 1999</marker>
<rawString>Noga Alon, Yossi Matias, and Mario Szegedy. 1999. The space complexity of approximating the frequency moments. J. Comput. Syst. Sci., 58(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="25795" citStr="Banerjee and Lavie, 2005" startWordPosition="4468" endWordPosition="4471">ake use of publicly available resources. Europarl (EP) corpus French-English section is used as parallel data. The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007). The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences5 is used as the development set. Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality. The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.6 4http://www.statmt.org/moses/ 5http://www.statmt.org/wmt07/ 6We found that testing on Parliamentary test data was completely insensitive to large n-gram LMs, even when these LMs are exact. This suggests that for SMT performance, more data 5.2 Integrating stream counts feature into decoder Our method only computes high-frequency n-gram counts; it does not estimate conditional probabilities. We can either turn these counts into conditional probabilities (by using SRILM) or use the cou</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="2126" citStr="Brants et al. (2007)" startWordPosition="326" endWordPosition="329">quires relative frequency estimation. Computing relative frequencies seems like an easy problem. However, as corpus sizes grow, it becomes a highly computational expensive task. Cutoff Size BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 2 229.8m 28.23 7.613 56.03 3 143.6m 28.17 7.571 56.53 5 59.4m 28.33 7.636 56.03 10 18.3m 27.91 7.546 55.64 100 1.1m 28.03 7.607 55.91 200 0.5m 27.62 7.550 55.67 Table 1: Effect of count-based pruning on SMT performance using EAN corpus. Results are according to BLEU, NIST and METEOR (MET) metrics. Bold #s are not statistically significant worse than exact model. Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams (summed over all orders from 1 to 5) from 1.8TB of web data. Their resulting model contained 300 million unique n-grams. It is not realistic using conventional computing resources to use all the 300 million n-grams for applications like speech recognition, spelling correction, information extraction, and statistical machine translation (SMT). Hence, one of the easiest way to reduce the size of this model is to use count-based pruning which discards all n-grams whose count is less than a pre-defined threshold. Although</context>
<context position="7291" citStr="Brants et al., 2007" startWordPosition="1205" endWordPosition="1208">wm). The probability of estimating word wm depends on previous n-1 words where n denotes the size of n-gram. This assumption that probability of predicting a current word depends on the previous words is called a Markov assumption, typically estimated by relative frequency: m−1 (1) C(wm−n+1) Eq 1 estimates the n-gram probability by taking the ratio of observed frequency of a particular sequence and the observed frequency of the prefix. This is precisely the relative frequency estimate we seek. 2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new. (Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests. Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models m−1 C( P(wm |wm−1 m−n+1) = wm−n+1wm) 513 for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and </context>
<context position="31109" citStr="Brants et al., 2007" startWordPosition="5379" endWordPosition="5382">e have proposed an efficient, low-memory method to construct high-order approximate n-gram LMs. Our method easily scales to billion-word monolingual corpora on conventional (8GB) desktop machines. We have demonstrated that approximate ngram features could be used as a direct replacement for conventional higher order LMs in SMT with significant reductions in memory usage. In future, we will be looking into building streaming skip ngrams, and other variants (like cluster n-grams). In NLP community, it has been shown that having more data results in better performance (Ravichandran et al., 2005; Brants et al., 2007; Turney, 2008). At web scale, we have terabytes of data and that can capture broader knowledge. Streaming algorithm paradigm provides a memory and space-efficient platform to deal with terabytes of data. We hope that other NLP applications (where we need to compute relative frequencies) like noun-clustering, constructing syntactic rules for SMT, finding analogies, and others can also benefit from streaming methods. We also believe that stream counts can be applied to other problems involving higher order LMs such as speech recognition, information extraction, spelling correction and text gene</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="1201" citStr="Chen and Goodman, 1996" startWordPosition="178" endWordPosition="181">. We show that this method easily scales to billion-word monolingual corpora using a conventional (8 GB RAM) desktop machine. Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods. 1 Introduction In many NLP problems, we are faced with the challenge of dealing with large amounts of data. Many problems boil down to computing relative frequencies of certain items on this data. Items can be words, patterns, associations, n-grams, and others. Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al., 2005), constructing syntactic rules for SMT (Galley et al., 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. We use language modeling as a canonical example of a large-scale task that requires relative frequency estimation. Computing relative frequencies seems like an easy problem. However, as corpus sizes grow, it becomes a highly computational expensive task. Cutoff Size BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 2 229.8m 28.23 7.613 56.03 3 143.6m 28.17 7.571 56.53 5 59.4</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. Chen and J. Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In Proceedings of 34th Annual Meeting of the Association for Computational Linguistics, pages 310–318, Santa Cruz, CA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>Marios Hadjieleftheriou</author>
</authors>
<title>Finding frequent items in data streams.</title>
<date>2008</date>
<booktitle>In VLDB.</booktitle>
<contexts>
<context position="9907" citStr="Cormode and Hadjieleftheriou, 2008" startWordPosition="1649" endWordPosition="1653">or typical algorithms, the storage size is of the order of logk N, where N is the input size and k is some constant. Stream algorithms were first developed in the early 80s, but gained in popularity in the late 90s as researchers first realized the challenges of dealing with massive data sets. A good survey of the model and core challenges can be found in (Muthukrishnan, 2005). There has been considerable work on the problem of identifying high-frequency items (items with frequency above a threshold), and a detailed review of these methods is beyond the scope of this article. A new survey by (Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature. 3 Space-Efficient Approximate Frequency Estimation Prior work on approximate frequency estimation for language models provide a “no-false-negative” guarantee, ensuring that counts for n-grams in the model are returned exactly, while working to make sure the false-positive rate remains small (Talbot and Osborne, 2007a). The notion of approximation we use is different: in our approach, it is the actual count values that will be approximated. We also exploit the fact that low-frequency n-grams, while constituting the vast majority of the set of unique n-gr</context>
</contexts>
<marker>Cormode, Hadjieleftheriou, 2008</marker>
<rawString>Graham Cormode and Marios Hadjieleftheriou. 2008. Finding frequent items in data streams. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E D Demaine</author>
<author>A Lopez-Ortiz</author>
<author>J I Munro</author>
</authors>
<title>Frequency estimation of internet packet streams with limited space.</title>
<date>2002</date>
<contexts>
<context position="5904" citStr="Demaine et al., 2002" startWordPosition="970" endWordPosition="973">pressed representation is enormous. The method we propose solves both of these problems. We do this by making use of the streaming algorithm paradigm (Muthukrishnan, 2005). Working under the assumption that multiple-GB models are infeasible, our goal is to instead of estimating a large model and then compressing it, we directly estimate a small model. We use a deterministic streaming algorithm (Manku and Motwani, 2002) that computes approximate frequency counts of frequently occurring n-grams. This scheme is considerably more accurate in getting the actual counts as compared to other schemes (Demaine et al., 2002; Karp et al., 2003) that find the set of frequent items without caring about the accuracy of counts. We use these counts directly as features in an SMT system, and propose a direct way to integrate these features into an SMT decoder. Experiments show that directly storing approximate counts of frequent 5-grams compared to using count or entropybased pruning counts gives equivalent SMT performance, while dramatically reducing the memory usage and getting rid of pre-computing a large model. 2 Background 2.1 n-gram Language Models Language modeling is based on assigning probabilities to sentence</context>
</contexts>
<marker>Demaine, Lopez-Ortiz, Munro, 2002</marker>
<rawString>E.D. Demaine, A. Lopez-Ortiz, and J.I. Munro. 2002. Frequency estimation of internet packet streams with limited space.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research.</booktitle>
<contexts>
<context position="25757" citStr="Doddington, 2002" startWordPosition="4464" endWordPosition="4465">e experiments conducted here make use of publicly available resources. Europarl (EP) corpus French-English section is used as parallel data. The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007). The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences5 is used as the development set. Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality. The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.6 4http://www.statmt.org/moses/ 5http://www.statmt.org/wmt07/ 6We found that testing on Parliamentary test data was completely insensitive to large n-gram LMs, even when these LMs are exact. This suggests that for SMT performance, more data 5.2 Integrating stream counts feature into decoder Our method only computes high-frequency n-gram counts; it does not estimate conditional probabilities. We can either turn these counts into conditional probabi</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the second international conference on Human Language Technology Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Kishore Papineni</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Large-scale distributed language modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<volume>4</volume>
<pages>37--40</pages>
<contexts>
<context position="7312" citStr="Emami et al., 2007" startWordPosition="1209" endWordPosition="1212">of estimating word wm depends on previous n-1 words where n denotes the size of n-gram. This assumption that probability of predicting a current word depends on the previous words is called a Markov assumption, typically estimated by relative frequency: m−1 (1) C(wm−n+1) Eq 1 estimates the n-gram probability by taking the ratio of observed frequency of a particular sequence and the observed frequency of the prefix. This is precisely the relative frequency estimate we seek. 2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new. (Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests. Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models m−1 C( P(wm |wm−1 m−n+1) = wm−n+1wm) 513 for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that</context>
</contexts>
<marker>Emami, Papineni, Sorensen, 2007</marker>
<rawString>Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen. 2007. Large-scale distributed language modeling. In Proceedings of the 2007 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 4, pages 37–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
</authors>
<title>How many bits are needed to store probabilities for phrasebased translation?</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation at ACL06.</booktitle>
<contexts>
<context position="8120" citStr="Federico and Bertoldi, 2006" startWordPosition="1340" endWordPosition="1343">borne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models m−1 C( P(wm |wm−1 m−n+1) = wm−n+1wm) 513 for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. (Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow. Now using higher order LMs at time of re-ranking looks like a good option. However, the</context>
</contexts>
<marker>Federico, Bertoldi, 2006</marker>
<rawString>Marcello Federico and Nicola Bertoldi. 2006. How many bits are needed to store probabilities for phrasebased translation? In Proceedings on the Workshop on Statistical Machine Translation at ACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT/NAACL-04.</booktitle>
<contexts>
<context position="1306" citStr="Galley et al., 2004" startWordPosition="193" endWordPosition="196"> desktop machine. Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods. 1 Introduction In many NLP problems, we are faced with the challenge of dealing with large amounts of data. Many problems boil down to computing relative frequencies of certain items on this data. Items can be words, patterns, associations, n-grams, and others. Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al., 2005), constructing syntactic rules for SMT (Galley et al., 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. We use language modeling as a canonical example of a large-scale task that requires relative frequency estimation. Computing relative frequencies seems like an easy problem. However, as corpus sizes grow, it becomes a highly computational expensive task. Cutoff Size BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 2 229.8m 28.23 7.613 56.03 3 143.6m 28.17 7.571 56.53 5 59.4m 28.33 7.636 56.03 10 18.3m 27.91 7.546 55.64 100 1.1m 28.03 7.607 55.91 200 0.5m 27.62 7.550 55.67 Tabl</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings ofHLT/NAACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia, PA,</location>
<contexts>
<context position="14514" citStr="Graff, 2003" startWordPosition="2452" endWordPosition="2453">quality of stream counts compared with exact n-gram counts (true counts). To 1We use hash tables to store tuples; however smarter data structures like suffix trees could also be used. Corpus Gzip-MB M-wrds Perplexity EP 63 38 1122.69 afe 417 171 1829.57 apw 1213 540 1872.96 nyt 2104 914 1785.84 xie 320 132 1885.33 Table 3: Corpus Statistics and perplexity of LMs made with each of these corpuses on development set evaluate the quality of stream counts on these metrics, we carry out three experiments. 4.1 Experimental Setup The freely available English side of Europarl (EP) and Gigaword corpus (Graff, 2003) is used for computing n-gram counts. We only use EP along with two sections of the Gigaword corpus: Agence France Press English Service(afe) and The New York Times Newswire Service (nyt). The unigram language models built using these corpuses yield better perplexity scores on the development set (see Section 5.1) compared to The Xinhua News Agency English Service (xie) and Associated Press Worldstream English Service (apw) as shown in Table 3. The LMs are build using the SRILM language modelling toolkit (Stolcke, 2002) with modified KneserNey discounting and interpolation. The evaluation of s</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>D. Graff. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia, PA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard M Karp</author>
<author>Christos H Papadimitriou</author>
<author>Scott Shenker</author>
</authors>
<title>A simple algorithm for finding frequent elements in streams and bags.</title>
<date>2003</date>
<contexts>
<context position="5924" citStr="Karp et al., 2003" startWordPosition="974" endWordPosition="977"> is enormous. The method we propose solves both of these problems. We do this by making use of the streaming algorithm paradigm (Muthukrishnan, 2005). Working under the assumption that multiple-GB models are infeasible, our goal is to instead of estimating a large model and then compressing it, we directly estimate a small model. We use a deterministic streaming algorithm (Manku and Motwani, 2002) that computes approximate frequency counts of frequently occurring n-grams. This scheme is considerably more accurate in getting the actual counts as compared to other schemes (Demaine et al., 2002; Karp et al., 2003) that find the set of frequent items without caring about the accuracy of counts. We use these counts directly as features in an SMT system, and propose a direct way to integrate these features into an SMT decoder. Experiments show that directly storing approximate counts of frequent 5-grams compared to using count or entropybased pruning counts gives equivalent SMT performance, while dramatically reducing the memory usage and getting rid of pre-computing a large model. 2 Background 2.1 n-gram Language Models Language modeling is based on assigning probabilities to sentences. It can either com</context>
</contexts>
<marker>Karp, Papadimitriou, Shenker, 2003</marker>
<rawString>Richard M. Karp, Christos H. Papadimitriou, and Scott Shenker. 2003. A simple algorithm for finding frequent elements in streams and bags.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>868--876</pages>
<contexts>
<context position="25376" citStr="Koehn and Hoang, 2007" startWordPosition="4396" endWordPosition="4399">ly 32 out of 54k 7-grams contained in test set. The small recall value for 7-grams suggests that these counts may not be that useful in SMT. We further substantiate our findings in our extrinsic evaluations. There we show that integrating 7-gram stream counts with an SMT system does not affect its overall performance significantly. 5 Extrinsic Evaluation 5.1 Experimental Setup All the experiments conducted here make use of publicly available resources. Europarl (EP) corpus French-English section is used as parallel data. The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007). The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences5 is used as the development set. Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality. The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.6 4http://www.statmt.org/moses/ 5http://www.statmt.org/wmt07/ 6We foun</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Manku</author>
<author>R Motwani</author>
</authors>
<title>Approximate frequency counts over data streams.</title>
<date>2002</date>
<booktitle>In Proceedings of the 28th International Conference on Very Large Data Bases.</booktitle>
<contexts>
<context position="5706" citStr="Manku and Motwani, 2002" startWordPosition="939" endWordPosition="942">rder of the LM increases. Firstly, the computation time to build the database of counts increases rapidly. Secondly, the initial disk storage required to maintain these counts, prior to building the compressed representation is enormous. The method we propose solves both of these problems. We do this by making use of the streaming algorithm paradigm (Muthukrishnan, 2005). Working under the assumption that multiple-GB models are infeasible, our goal is to instead of estimating a large model and then compressing it, we directly estimate a small model. We use a deterministic streaming algorithm (Manku and Motwani, 2002) that computes approximate frequency counts of frequently occurring n-grams. This scheme is considerably more accurate in getting the actual counts as compared to other schemes (Demaine et al., 2002; Karp et al., 2003) that find the set of frequent items without caring about the accuracy of counts. We use these counts directly as features in an SMT system, and propose a direct way to integrate these features into an SMT decoder. Experiments show that directly storing approximate counts of frequent 5-grams compared to using count or entropybased pruning counts gives equivalent SMT performance, </context>
<context position="10957" citStr="Manku and Motwani, 2002" startWordPosition="1812" endWordPosition="1815">, it is the actual count values that will be approximated. We also exploit the fact that low-frequency n-grams, while constituting the vast majority of the set of unique n-grams, are usually smoothed away and are less likely to influence the language model significantly. Discarding low-frequency n-grams is particularly important in a stream setting, because it can be shown in general that any algorithm that generates approximate frequency counts for all n-grams requires space linear in the input stream (Alon et al., 1999). We employ an algorithm for approximate frequency counting proposed by (Manku and Motwani, 2002) in the context of database management. Fix parameters s E (0, 1), and E E (0, 1), E ≪ s. Our goal is to approximately find all n-grams with frequency at least sN. For an input stream of n-grams of length N, the algorithm outputs a set of items (and frequencies) and guarantees the following: • All items with frequencies exceeding sN are output (no false negatives). • No item with frequency less than (s − E)N is output (few false positives). • All reported frequencies are less than the true frequencies by at most EN (close-to-exact frequencies). • The space used by the algorithm is O(1ǫ log EN)</context>
<context position="12397" citStr="Manku and Motwani, 2002" startWordPosition="2072" endWordPosition="2075">ll be returned, and all frequencies will be no more than 0.1% away from the true frequencies. The space used by the algorithm is O(log N), which can be compared to the much larger (close to N) space 514 needed to store the initial frequency counts. In addition, the algorithm runs in linear time by definition, requiring only one pass over the input. Note that there might be ǫ1 elements with frequency at least ǫN, and so the algorithm uses optimal space (up to a logarithmic factor). 3.1 The Algorithm We present a high-level overview of the algorithm; for more details, the reader is referred to (Manku and Motwani, 2002). The algorithm proceeds by conceptually dividing the stream into epochs, each containing 1/ǫ elements. Note that there are ǫN epochs. Each such epoch has an ID, starting from 1. The algorithm maintains a list of tuples1 of the form (e, f, A), where e is an n-gram, f is its reported frequency, and A is the maximum error in the frequency estimation. While the algorithm reads ngrams associated with the current epoch, it does one of two things: if the new element e is contained in the list of tuples, it merely increments the frequency count f. If not, it creates a new tuple of the form (e, 1, T −</context>
</contexts>
<marker>Manku, Motwani, 2002</marker>
<rawString>G. S. Manku and R. Motwani. 2002. Approximate frequency counts over data streams. In Proceedings of the 28th International Conference on Very Large Data Bases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muthukrishnan</author>
</authors>
<title>Data streams: Algorithms and applications. Foundations and Trends in</title>
<date>2005</date>
<journal>Theoretical Computer Science,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="5455" citStr="Muthukrishnan, 2005" startWordPosition="901" endWordPosition="902"> model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. There are two difficulties with scaling all the above approaches as the order of the LM increases. Firstly, the computation time to build the database of counts increases rapidly. Secondly, the initial disk storage required to maintain these counts, prior to building the compressed representation is enormous. The method we propose solves both of these problems. We do this by making use of the streaming algorithm paradigm (Muthukrishnan, 2005). Working under the assumption that multiple-GB models are infeasible, our goal is to instead of estimating a large model and then compressing it, we directly estimate a small model. We use a deterministic streaming algorithm (Manku and Motwani, 2002) that computes approximate frequency counts of frequently occurring n-grams. This scheme is considerably more accurate in getting the actual counts as compared to other schemes (Demaine et al., 2002; Karp et al., 2003) that find the set of frequent items without caring about the accuracy of counts. We use these counts directly as features in an SM</context>
<context position="9651" citStr="Muthukrishnan, 2005" startWordPosition="1608" endWordPosition="1610">s algorithm has working storage that it can use to store parts of the input or other intermediate computations. However, (and this is a critical constraint), this working storage space is significantly smaller than the input stream length. For typical algorithms, the storage size is of the order of logk N, where N is the input size and k is some constant. Stream algorithms were first developed in the early 80s, but gained in popularity in the late 90s as researchers first realized the challenges of dealing with massive data sets. A good survey of the model and core challenges can be found in (Muthukrishnan, 2005). There has been considerable work on the problem of identifying high-frequency items (items with frequency above a threshold), and a detailed review of these methods is beyond the scope of this article. A new survey by (Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature. 3 Space-Efficient Approximate Frequency Estimation Prior work on approximate frequency estimation for language models provide a “no-false-negative” guarantee, ensuring that counts for n-grams in the model are returned exactly, while working to make sure the false-positive rate remains small (Talbot and</context>
</contexts>
<marker>Muthukrishnan, 2005</marker>
<rawString>S. Muthukrishnan. 2005. Data streams: Algorithms and applications. Foundations and Trends in Theoretical Computer Science, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<contexts>
<context position="25732" citStr="Papineni et al., 2002" startWordPosition="4459" endWordPosition="4462"> 5.1 Experimental Setup All the experiments conducted here make use of publicly available resources. Europarl (EP) corpus French-English section is used as parallel data. The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007). The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences5 is used as the development set. Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality. The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.6 4http://www.statmt.org/moses/ 5http://www.statmt.org/wmt07/ 6We found that testing on Parliamentary test data was completely insensitive to large n-gram LMs, even when these LMs are exact. This suggests that for SMT performance, more data 5.2 Integrating stream counts feature into decoder Our method only computes high-frequency n-gram counts; it does not estimate conditional probabilities. We can either turn these counts</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1246" citStr="Ravichandran et al., 2005" startWordPosition="183" endWordPosition="186">o billion-word monolingual corpora using a conventional (8 GB RAM) desktop machine. Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods. 1 Introduction In many NLP problems, we are faced with the challenge of dealing with large amounts of data. Many problems boil down to computing relative frequencies of certain items on this data. Items can be words, patterns, associations, n-grams, and others. Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al., 2005), constructing syntactic rules for SMT (Galley et al., 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. We use language modeling as a canonical example of a large-scale task that requires relative frequency estimation. Computing relative frequencies seems like an easy problem. However, as corpus sizes grow, it becomes a highly computational expensive task. Cutoff Size BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 2 229.8m 28.23 7.613 56.03 3 143.6m 28.17 7.571 56.53 5 59.4m 28.33 7.636 56.03 10 18.3m 27.91 7.546 55.6</context>
<context position="31088" citStr="Ravichandran et al., 2005" startWordPosition="5374" endWordPosition="5378">nts is high. 6 Conclusion We have proposed an efficient, low-memory method to construct high-order approximate n-gram LMs. Our method easily scales to billion-word monolingual corpora on conventional (8GB) desktop machines. We have demonstrated that approximate ngram features could be used as a direct replacement for conventional higher order LMs in SMT with significant reductions in memory usage. In future, we will be looking into building streaming skip ngrams, and other variants (like cluster n-grams). In NLP community, it has been shown that having more data results in better performance (Ravichandran et al., 2005; Brants et al., 2007; Turney, 2008). At web scale, we have terabytes of data and that can capture broader knowledge. Streaming algorithm paradigm provides a memory and space-efficient platform to deal with terabytes of data. We hope that other NLP applications (where we need to compute relative frequencies) like noun-clustering, constructing syntactic rules for SMT, finding analogies, and others can also benefit from streaming methods. We also believe that stream counts can be applied to other problems involving higher order LMs such as speech recognition, information extraction, spelling cor</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Philipp Koehn</author>
</authors>
<title>Large and diverse language models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of The Third International Joint Conference on Natural Language Processing (IJCNP).</booktitle>
<contexts>
<context position="8415" citStr="Schwenk and Koehn, 2008" startWordPosition="1389" endWordPosition="1392">ined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. (Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow. Now using higher order LMs at time of re-ranking looks like a good option. However, the target n-best hypothesis list is not diverse enough. Hence if possible it is always better to integrate LMs directly into the decoder. 2.3 Streaming Consider an algorithm that reads the input from a read-only stream from left to right, with no ability to go back to the input that it has alread</context>
</contexts>
<marker>Schwenk, Koehn, 2008</marker>
<rawString>Holger Schwenk and Philipp Koehn. 2008. Large and diverse language models for statistical machine translation. In Proceedings of The Third International Joint Conference on Natural Language Processing (IJCNP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models. In</title>
<date>1998</date>
<booktitle>In Proc. DARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<contexts>
<context position="4066" citStr="Stolcke, 1998" startWordPosition="660" endWordPosition="661"> BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 1e-10 218.4m 28.64 7.669 56.33 5e-10 171.0m 28.48 7.666 56.38 1e-9 148.0m 28.56 7.646 56.51 5e-9 91.9m 28.27 7.623 56.16 1e-8 69.4m 28.15 7.609 56.19 5e-7 28.5m 28.08 7.595 55.91 Table 2: Effect of entropy-based pruning on SMT performance using EAN corpus. Results are as in Table 1 is ≈ 0.53 Bleu). However, we need 300 times bigger model to get such an increase. Unfortunately, it is not possible to integrate such a big model inside a decoder using normal computation resources. A better way of reducing the size of n-grams is to use entropy pruning (Stolcke, 1998). Table 2 shows the results with entropy pruning with different settings of ǫ. We see that for three settings of ǫ equal to 1e-10, 5e-10 and 1e-9, we get Bleu scores comparable to the exact model. However, the size of all these models is not at all small. The size of smallest model is 25% of the exact model. Even with this size it is still not feasible to integrate such a big model inside a decoder. If we take a model of size comparable to count cutoff of 100, i.e., with ǫ = 5e-7, we see both count-based pruning as well as entropy pruning performs the same. There also have been prior work on m</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In In Proc. DARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO,</location>
<contexts>
<context position="15039" citStr="Stolcke, 2002" startWordPosition="2537" endWordPosition="2538">etup The freely available English side of Europarl (EP) and Gigaword corpus (Graff, 2003) is used for computing n-gram counts. We only use EP along with two sections of the Gigaword corpus: Agence France Press English Service(afe) and The New York Times Newswire Service (nyt). The unigram language models built using these corpuses yield better perplexity scores on the development set (see Section 5.1) compared to The Xinhua News Agency English Service (xie) and Associated Press Worldstream English Service (apw) as shown in Table 3. The LMs are build using the SRILM language modelling toolkit (Stolcke, 2002) with modified KneserNey discounting and interpolation. The evaluation of stream counts is done on EP+afe+nyt (EAN) corpus, consisting of 1.1 billion words. 4.2 Description of the metrics To evaluate the quality of counts produced by our stream algorithm four different metrics are used. The accuracy metric measures the quality of top N stream counts by taking the fraction of top N stream counts that are contained in the top N true counts. Accuracy = True Counts Spearman’s rank correlation coefficient or Spearman’s rho(ρ) computes the difference between the ranks of each observation (i.e. n-gra</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing, pages 901–904, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Thorsten Brants</author>
</authors>
<title>Randomized language models via perfect hash functions.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08:</booktitle>
<publisher>HLT.</publisher>
<contexts>
<context position="4815" citStr="Talbot and Brants, 2008" startWordPosition="798" endWordPosition="801">e-10, 5e-10 and 1e-9, we get Bleu scores comparable to the exact model. However, the size of all these models is not at all small. The size of smallest model is 25% of the exact model. Even with this size it is still not feasible to integrate such a big model inside a decoder. If we take a model of size comparable to count cutoff of 100, i.e., with ǫ = 5e-7, we see both count-based pruning as well as entropy pruning performs the same. There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a; Talbot and Osborne, 2007b; Talbot and Brants, 2008)) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. There are two difficulties with scaling all the above approaches as the order of the LM increases. Firstly, the computation time to build the database of counts increases rapidly. Secondly, the initial disk storage required to maintain these counts, prior to building the compressed representation is enormous. The method we propose solves both of these problems. We do this by making use of the streaming </context>
<context position="7726" citStr="Talbot and Brants, 2008" startWordPosition="1279" endWordPosition="1282">e prefix. This is precisely the relative frequency estimate we seek. 2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new. (Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests. Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models m−1 C( P(wm |wm−1 m−n+1) = wm−n+1wm) 513 for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT p</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>David Talbot and Thorsten Brants. 2008. Randomized language models via perfect hash functions. In Proceedings ofACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="4761" citStr="Talbot and Osborne, 2007" startWordPosition="789" endWordPosition="792">s of ǫ. We see that for three settings of ǫ equal to 1e-10, 5e-10 and 1e-9, we get Bleu scores comparable to the exact model. However, the size of all these models is not at all small. The size of smallest model is 25% of the exact model. Even with this size it is still not feasible to integrate such a big model inside a decoder. If we take a model of size comparable to count cutoff of 100, i.e., with ǫ = 5e-7, we see both count-based pruning as well as entropy pruning performs the same. There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a; Talbot and Osborne, 2007b; Talbot and Brants, 2008)) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. There are two difficulties with scaling all the above approaches as the order of the LM increases. Firstly, the computation time to build the database of counts increases rapidly. Secondly, the initial disk storage required to maintain these counts, prior to building the compressed representation is enormous. The method we propose solves both of thes</context>
<context position="7503" citStr="Talbot and Osborne, 2007" startWordPosition="1242" endWordPosition="1245">led a Markov assumption, typically estimated by relative frequency: m−1 (1) C(wm−n+1) Eq 1 estimates the n-gram probability by taking the ratio of observed frequency of a particular sequence and the observed frequency of the prefix. This is precisely the relative frequency estimate we seek. 2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new. (Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests. Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models m−1 C( P(wm |wm−1 m−n+1) = wm−n+1wm) 513 for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico an</context>
<context position="10265" citStr="Talbot and Osborne, 2007" startWordPosition="1698" endWordPosition="1702">nan, 2005). There has been considerable work on the problem of identifying high-frequency items (items with frequency above a threshold), and a detailed review of these methods is beyond the scope of this article. A new survey by (Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature. 3 Space-Efficient Approximate Frequency Estimation Prior work on approximate frequency estimation for language models provide a “no-false-negative” guarantee, ensuring that counts for n-grams in the model are returned exactly, while working to make sure the false-positive rate remains small (Talbot and Osborne, 2007a). The notion of approximation we use is different: in our approach, it is the actual count values that will be approximated. We also exploit the fact that low-frequency n-grams, while constituting the vast majority of the set of unique n-grams, are usually smoothed away and are less likely to influence the language model significantly. Discarding low-frequency n-grams is particularly important in a stream setting, because it can be shown in general that any algorithm that generates approximate frequency counts for all n-grams requires space linear in the input stream (Alon et al., 1999). We </context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007a. Randomised language modelling for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Smoothed Bloom filter language models: Tera-scale LMs on the cheap.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EM NLP-CoNLL).</booktitle>
<contexts>
<context position="4761" citStr="Talbot and Osborne, 2007" startWordPosition="789" endWordPosition="792">s of ǫ. We see that for three settings of ǫ equal to 1e-10, 5e-10 and 1e-9, we get Bleu scores comparable to the exact model. However, the size of all these models is not at all small. The size of smallest model is 25% of the exact model. Even with this size it is still not feasible to integrate such a big model inside a decoder. If we take a model of size comparable to count cutoff of 100, i.e., with ǫ = 5e-7, we see both count-based pruning as well as entropy pruning performs the same. There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a; Talbot and Osborne, 2007b; Talbot and Brants, 2008)) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. There are two difficulties with scaling all the above approaches as the order of the LM increases. Firstly, the computation time to build the database of counts increases rapidly. Secondly, the initial disk storage required to maintain these counts, prior to building the compressed representation is enormous. The method we propose solves both of thes</context>
<context position="7503" citStr="Talbot and Osborne, 2007" startWordPosition="1242" endWordPosition="1245">led a Markov assumption, typically estimated by relative frequency: m−1 (1) C(wm−n+1) Eq 1 estimates the n-gram probability by taking the ratio of observed frequency of a particular sequence and the observed frequency of the prefix. This is precisely the relative frequency estimate we seek. 2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new. (Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests. Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models m−1 C( P(wm |wm−1 m−n+1) = wm−n+1wm) 513 for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico an</context>
<context position="10265" citStr="Talbot and Osborne, 2007" startWordPosition="1698" endWordPosition="1702">nan, 2005). There has been considerable work on the problem of identifying high-frequency items (items with frequency above a threshold), and a detailed review of these methods is beyond the scope of this article. A new survey by (Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature. 3 Space-Efficient Approximate Frequency Estimation Prior work on approximate frequency estimation for language models provide a “no-false-negative” guarantee, ensuring that counts for n-grams in the model are returned exactly, while working to make sure the false-positive rate remains small (Talbot and Osborne, 2007a). The notion of approximation we use is different: in our approach, it is the actual count values that will be approximated. We also exploit the fact that low-frequency n-grams, while constituting the vast majority of the set of unique n-grams, are usually smoothed away and are less likely to influence the language model significantly. Discarding low-frequency n-grams is particularly important in a stream setting, because it can be shown in general that any algorithm that generates approximate frequency counts for all n-grams requires space linear in the input stream (Alon et al., 1999). We </context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007b. Smoothed Bloom filter language models: Tera-scale LMs on the cheap. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EM NLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1344" citStr="Turney, 2008" startWordPosition="200" endWordPosition="201">tion experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods. 1 Introduction In many NLP problems, we are faced with the challenge of dealing with large amounts of data. Many problems boil down to computing relative frequencies of certain items on this data. Items can be words, patterns, associations, n-grams, and others. Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al., 2005), constructing syntactic rules for SMT (Galley et al., 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. We use language modeling as a canonical example of a large-scale task that requires relative frequency estimation. Computing relative frequencies seems like an easy problem. However, as corpus sizes grow, it becomes a highly computational expensive task. Cutoff Size BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 2 229.8m 28.23 7.613 56.03 3 143.6m 28.17 7.571 56.53 5 59.4m 28.33 7.636 56.03 10 18.3m 27.91 7.546 55.64 100 1.1m 28.03 7.607 55.91 200 0.5m 27.62 7.550 55.67 Table 1: Effect of count-based pruning on </context>
<context position="31124" citStr="Turney, 2008" startWordPosition="5383" endWordPosition="5384">ficient, low-memory method to construct high-order approximate n-gram LMs. Our method easily scales to billion-word monolingual corpora on conventional (8GB) desktop machines. We have demonstrated that approximate ngram features could be used as a direct replacement for conventional higher order LMs in SMT with significant reductions in memory usage. In future, we will be looking into building streaming skip ngrams, and other variants (like cluster n-grams). In NLP community, it has been shown that having more data results in better performance (Ravichandran et al., 2005; Brants et al., 2007; Turney, 2008). At web scale, we have terabytes of data and that can capture broader knowledge. Streaming algorithm paradigm provides a memory and space-efficient platform to deal with terabytes of data. We hope that other NLP applications (where we need to compute relative frequencies) like noun-clustering, constructing syntactic rules for SMT, finding analogies, and others can also benefit from streaming methods. We also believe that stream counts can be applied to other problems involving higher order LMs such as speech recognition, information extraction, spelling correction and text generation. 519 Ref</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="8250" citStr="Uszkoreit and Brants, 2008" startWordPosition="1362" endWordPosition="1365">e efficient language models m−1 C( P(wm |wm−1 m−n+1) = wm−n+1wm) 513 for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. (Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow. Now using higher order LMs at time of re-ranking looks like a good option. However, the target n-best hypothesis list is not diverse enough. Hence if possible it is always better to integrate LMs directly into the dec</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Distributed language modeling for n-best list re-ranking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8436" citStr="Zhang et al., 2006" startWordPosition="1393" endWordPosition="1396"> to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. (Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow. Now using higher order LMs at time of re-ranking looks like a good option. However, the target n-best hypothesis list is not diverse enough. Hence if possible it is always better to integrate LMs directly into the decoder. 2.3 Streaming Consider an algorithm that reads the input from a read-only stream from left to right, with no ability to go back to the input that it has already processed. This alg</context>
</contexts>
<marker>Zhang, Hildebrand, Vogel, 2006</marker>
<rawString>Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel. 2006. Distributed language modeling for n-best list re-ranking. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>