<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009250">
<title confidence="0.997306">
Topic Models for Dynamic Translation Model Adaptation
</title>
<author confidence="0.989533">
Vladimir Eidelman
</author>
<affiliation confidence="0.969740333333333">
Computer Science
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.890632">
College Park, MD
</address>
<email confidence="0.998788">
vlad@umiacs.umd.edu
</email>
<author confidence="0.923926">
Jordan Boyd-Graber
</author>
<affiliation confidence="0.883315333333333">
iSchool
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.887615">
College Park, MD
</address>
<email confidence="0.998961">
jbg@umiacs.umd.edu
</email>
<author confidence="0.731576">
Philip Resnik
</author>
<affiliation confidence="0.707884333333333">
Linguistics
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.877458">
College Park, MD
</address>
<email confidence="0.999171">
resnik@umd.edu
</email>
<sectionHeader confidence="0.993908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995050625">
We propose an approach that biases machine
translation systems toward relevant transla-
tions based on topic-specific contexts, where
topics are induced in an unsupervised way
using topic models; this can be thought of
as inducing subcorpora for adaptation with-
out any human annotation. We use these topic
distributions to compute topic-dependent lex-
ical weighting probabilities and directly in-
corporate them into our translation model as
features. Conditioning lexical probabilities
on the topic biases translations toward topic-
relevant output, resulting in significant im-
provements of up to 1 BLEU and 3 TER on
Chinese to English translation over a strong
baseline.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977591836735">
The performance of a statistical machine translation
(SMT) system on a translation task depends largely
on the suitability of the available parallel training
data. Domains (e.g., newswire vs. blogs) may vary
widely in their lexical choices and stylistic prefer-
ences, and what may be preferable in a general set-
ting, or in one domain, is not necessarily preferable
in another domain. Indeed, sometimes the domain
can change the meaning of a phrase entirely.
In a food related context, the Chinese sentence
“粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They
have a lot of vermicelli”; however, in an informal In-
ternet conversation, this sentence would mean “They
have a lot of fans”. Without the broader context, it
is impossible to determine the correct translation in
otherwise identical sentences.
This problem has led to a substantial amount of
recent work in trying to bias, or adapt, the transla-
tion model (TM) toward particular domains of inter-
est (Axelrod et al., 2011; Foster et al., 2010; Snover
et al., 2008).1 The intuition behind TM adapta-
tion is to increase the likelihood of selecting rele-
vant phrases for translation. Matsoukas et al. (2009)
introduced assigning a pair of binary features to
each training sentence, indicating sentences’ genre
and collection as a way to capture domains. They
then learn a mapping from these features to sen-
tence weights, use the sentence weights to bias the
model probability estimates and subsequently learn
the model weights. As sentence weights were found
to be most beneficial for lexical weighting, Chiang
et al. (2011) extends the same notion of condition-
ing on provenance (i.e., the origin of the text) by re-
moving the separate mapping step, directly optimiz-
ing the weight of the genre and collection features
by computing a separate word translation table for
each feature, estimated from only those sentences
that comprise that genre or collection.
The common thread throughout prior work is the
concept of a domain. A domain is typically a hard
constraint that is externally imposed and hand la-
beled, such as genre or corpus collection. For ex-
ample, a sentence either comes from newswire, or
weblog, but not both. However, this poses sev-
eral problems. First, since a sentence contributes its
counts only to the translation table for the source it
came from, many word pairs will be unobserved for
a given table. This sparsity requires smoothing. Sec-
ond, we may not know the (sub)corpora our training
</bodyText>
<footnote confidence="0.954965">
1Language model adaptation is also prevalent but is not the
focus of this work.
</footnote>
<page confidence="0.952637">
115
</page>
<note confidence="0.693219">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 115–119,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999929459459459">
data come from; and even if we do, “subcorpus” may
not be the most useful notion of domain for better
translations.
We take a finer-grained, flexible, unsupervised ap-
proach for lexical weighting by domain. We induce
unsupervised domains from large corpora, and we
incorporate soft, probabilistic domain membership
into a translation model. Unsupervised modeling of
the training data produces naturally occurring sub-
corpora, generalizing beyond corpus and genre. De-
pending on the model used to select subcorpora, we
can bias our translation toward any arbitrary distinc-
tion. This reduces the problem to identifying what
automatically defined subsets of the training corpus
may be beneficial for translation.
In this work, we consider the underlying latent
topics of the documents (Blei et al., 2003). Topic
modeling has received some use in SMT, for in-
stance Bilingual LSA adaptation (Tam et al., 2007),
and the BiTAM model (Zhao and Xing, 2006),
which uses a bilingual topic model for learning
alignment. In our case, by building a topic distri-
bution for the source side of the training data, we
abstract the notion of domain to include automati-
cally derived subcorpora with probabilistic member-
ship. This topic model infers the topic distribution
of a test set and biases sentence translations to ap-
propriate topics. We accomplish this by introduc-
ing topic dependent lexical probabilities directly as
features in the translation model, and interpolating
them log-linearly with our other features, thus allow-
ing us to discriminatively optimize their weights on
an arbitrary objective function. Incorporating these
features into our hierarchical phrase-based transla-
tion system significantly improved translation per-
formance, by up to 1 BLEU and 3 TER over a strong
Chinese to English baseline.
</bodyText>
<sectionHeader confidence="0.940329" genericHeader="method">
2 Model Description
</sectionHeader>
<bodyText confidence="0.980511">
Lexical Weighting Lexical weighting features es-
timate the quality of a phrase pair by combining
the lexical translation probabilities of the words in
the phrase2 (Koehn et al., 2003). Lexical condi-
tional probabilities p(e|f) are obtained with maxi-
mum likelihood estimates from relative frequencies
2For hierarchical systems, these correspond to translation
rules.
c(f, e)/Ee c(f, e). Phrase pair probabilities p(e|f)
are computed from these as described in Koehn et
al. (2003).
Chiang et al. (2011) showed that is it benefi-
cial to condition the lexical weighting features on
provenance by assigning each sentence pair a set
of features, fs(e|f), one for each domain s, which
compute a new word translation table ps(e|f) esti-
mated from only those sentences which belong to s:
cs(f, e)/Ee cs(f, e), where cs(·) is the number of
occurrences of the word pair in s.
Topic Modeling for MT We extend provenance
to cover a set of automatically generated topics zn.
Given a parallel training corpus T composed of doc-
uments di, we build a source side topic model over
T, which provides a topic distribution p(zn|di) for
zn = 11, ... , K} over each document, using Latent
Dirichlet Allocation (LDA) (Blei et al., 2003). Then,
we assign p(zn|di) to be the topic distribution for
every sentence xj E di, thus enforcing topic sharing
across sentence pairs in the same document instead
of treating them as unrelated. Computing the topic
distribution over a document and assigning it to the
sentences serves to tie the sentences together in the
document context.
To obtain the lexical probability conditioned on
topic distribution, we first compute the expected
count ez..(e, f) of a word pair under topic zn:
</bodyText>
<equation confidence="0.99297">
�ez..(e,f) = p(zn|di) � cj(e, f) (1)
diET 2jEdi
</equation>
<bodyText confidence="0.9998895">
where cj(·) denotes the number of occurrences of
the word pair in sentence xj, and then compute:
</bodyText>
<equation confidence="0.990471">
pz..(e|f) = ez.. (e, f ) (2)
Ee ez..(e, f)
</equation>
<bodyText confidence="0.9962732">
Thus, we will introduce 2·K new word trans-
lation tables, one for each pz..(e|f) and pz..(f|e),
and as many new corresponding features fz..(e|f),
fz..(f|e). The actual feature values we compute will
depend on the topic distribution of the document we
are translating. For a test document V , we infer
topic assignments on V , p(zn|V ), keeping the topics
found from T fixed. The feature value then becomes
fz..(e|f) = − log {pz..(e|f) · p(zn|V )1, a combi-
nation of the topic dependent lexical weight and the
</bodyText>
<page confidence="0.995593">
116
</page>
<bodyText confidence="0.9997858">
topic distribution of the sentence from which we are
extracting the phrase. To optimize the weights of
these features we combine them in our linear model
with the other features when computing the model
score for each phrase pair3:
</bodyText>
<equation confidence="0.83653">
X Aphp(e, f) X Az..fz..(e|f) (3)
p +
z�
</equation>
<bodyText confidence="0.999311457142857">
Combining the topic conditioned word translation
table pz1z(e|f) computed from the training corpus
with the topic distribution p(zn|V ) of the test sen-
tence being translated provides a probability on how
relevant that translation table is to the sentence. This
allows us to bias the translation toward the topic of
the sentence. For example, if topic k is dominant in
T, pk(e|f) may be quite large, but if p(k|V ) is very
small, then we should steer away from this phrase
pair and select a competing phrase pair which may
have a lower probability in T, but which is more rel-
evant to the test sentence at hand.
In many cases, document delineations may not be
readily available for the training corpus. Further-
more, a document may be too broad, covering too
many disparate topics, to effectively bias the weights
on a phrase level. For this case, we also propose a
local LDA model (LTM), which treats each sentence
as a separate document.
While Chiang et al. (2011) has to explicitly
smooth the resulting ps(e|f), since many word pairs
will be unseen for a given domain s, we are already
performing an implicit form of smoothing (when
computing the expected counts), since each docu-
ment has a distribution over all topics, and therefore
we have some probability of observing each word
pair in every topic.
Feature Representation After obtaining the topic
conditional features, there are two ways to present
them to the model. They could answer the question
F1: What is the probability under topic 1, topic 2,
etc., or F2: What is the probability under the most
probable topic, second most, etc.
A model using F1 learns whether a specific topic
is useful for translation, i.e., feature f1 would be
</bodyText>
<equation confidence="0.406702">
f1 •= pz=1(e|f) · p(z = 1|V ). With F2, we
</equation>
<footnote confidence="0.930227">
3The unadapted lexical weight p(elf) is included in the
model features.
</footnote>
<bodyText confidence="0.735844">
are learning how useful knowledge of the topic dis-
</bodyText>
<equation confidence="0.794772">
tribution is, i.e., f1 p(arg max,,(p(z,|V ))(e|f) ·
p(arg maxz,(p(zn|V ))|V ).
</equation>
<bodyText confidence="0.99909235">
Using F1, if we restrict our topics to have a one-
to-one mapping with genre/collection4 we see that
our method fully recovers Chiang (2011).
F1 is appropriate for cross-domain adaptation
when we have advance knowledge that the distribu-
tion of the tuning data will match the test data, as in
Chiang (2011), where they tune and test on web. In
general, we may not know what our data will be, so
this will overfit the tuning set.
F2, however, is intuitively what we want, since
we do not want to bias our system toward a spe-
cific distribution, but rather learn to utilize informa-
tion from any topic distribution if it helps us cre-
ate topic relevant translations. F2 is useful for dy-
namic adaptation, where the adapted feature weight
changes based on the source sentence.
Thus, F2 is the approach we use in our work,
which allows us to tune our system weights toward
having topic information be useful, not toward a spe-
cific distribution.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.976178347826087">
Setup To evaluate our approach, we performed ex-
periments on Chinese to English MT in two set-
tings. First, we use the FBIS corpus as our training
bitext. Since FBIS has document delineations, we
compare local topic modeling (LTM) with model-
ing at the document level (GTM). The second setting
uses the non-UN and non-HK Hansards portions of
the NIST training corpora with LTM only. Table 1
summarizes the data statistics. For both settings,
the data were lowercased, tokenized and aligned us-
ing GIZA++ (Och and Ney, 2003) to obtain bidi-
rectional alignments, which were symmetrized us-
ing the grow-diag-final-and method (Koehn
et al., 2003). The Chinese data were segmented us-
ing the Stanford segmenter. We trained a trigram
LM on the English side of the corpus with an addi-
tional 150M words randomly selected from the non-
NYT and non-LAT portions of the Gigaword v4 cor-
pus using modified Kneser-Ney smoothing (Chen
and Goodman, 1996). We used cdec (Dyer et al.,
4By having as many topics as genres/collections and setting
p(z.|di) to 1 for every sentence in the collection and 0 to ev-
erything else.
</bodyText>
<figure confidence="0.989895">
 |{z }
unadapted features
 |{z }
adapted features
</figure>
<page confidence="0.864738">
117
</page>
<table confidence="0.86501425">
Corpus Sentences Tokens
En Zh
FBIS 269K 10.3M 7.9M
NIST 1.6M 44.4M 40.4M
</table>
<tableCaption confidence="0.999326">
Table 1: Corpus statistics
</tableCaption>
<bodyText confidence="0.995409742857143">
2010) as our decoder, and tuned the parameters of
the system to optimize BLEU (Papineni et al., 2002)
on the NIST MT06 tuning corpus using the Mar-
gin Infused Relaxed Algorithm (MIRA) (Crammer
et al., 2006; Eidelman, 2012). Topic modeling was
performed with Mallet (Mccallum, 2002), a stan-
dard implementation of LDA, using a Chinese sto-
plist and setting the per-document Dirichlet parame-
ter α = 0.01. This setting of was chosen to encour-
age sparse topic assignments, which make induced
subdomains consistent within a document.
Results Results for both settings are shown in Ta-
ble 2. GTM models the latent topics at the document
level, while LTM models each sentence as a separate
document. To evaluate the effect topic granularity
would have on translation, we varied the number of
latent topics in each model to be 5, 10, and 20. On
FBIS, we can see that both models achieve moderate
but consistent gains over the baseline on both BLEU
and TER. The best model, LTM-10, achieves a gain
of about 0.5 and 0.6 BLEU and 2 TER. Although the
performance on BLEU for both the 20 topic models
LTM-20 and GTM-20 is suboptimal, the TER im-
provement is better. Interestingly, the difference in
translation quality between capturing document co-
herence in GTM and modeling purely on the sen-
tence level is not substantial.5 In fact, the opposite
is true, with the LTM models achieving better per-
formance.6
On the NIST corpus, LTM-10 again achieves the
best gain of approximately 1 BLEU and up to 3 TER.
LTM performs on par with or better than GTM, and
provides significant gains even in the NIST data set-
ting, showing that this method can be effectively ap-
plied directly on the sentence level to large training
</bodyText>
<footnote confidence="0.98225">
5An avenue of future work would condition the sentence
topic distribution on a document distribution over topics (Teh
et al., 2006).
6As an empirical validation of our earlier intuition regarding
feature representation, presenting the features in the form of Fl
caused the performance to remain virtually unchanged from the
baseline model.
</footnote>
<table confidence="0.999877875">
Model MT03 MT05
TBLEU ,TER TBLEU ,TER
BL 28.72 65.96 27.71 67.58
GTM-5 28.95ns 65.45 27.98ns 67.38ns
GTM-10 29.22 64.47 28.19 66.15
GTM-20 29.19 63.41 28.00ns 64.89
LTM-5 29.23 64.57 28.19 66.30
LTM-10 29.29 63.98 28.18 65.56
LTM-20 29.09ns 63.57 27.90ns 65.17
Model MT03 MT05
TBLEU ,TER TBLEU ,TER
BL 34.31 61.14 30.63 65.10
MERT 34.60 60.66 30.53 64.56
LTM-5 35.21 59.48 31.47 62.34
LTM-10 35.32 59.16 31.56 62.01
LTM-20 33.90ns 60.89ns 30.12ns 63.87
</table>
<tableCaption confidence="0.995654">
Table 2: Performance using FBIS training corpus (top)
and NIST corpus (bottom). Improvements are significant
at the p &lt;0.05 level, except where indicated (11).
</tableCaption>
<bodyText confidence="0.999925">
corpora which have no document markings. De-
pending on the diversity of training corpus, a vary-
ing number of underlying topics may be appropriate.
However, in both settings, 10 topics performed best.
</bodyText>
<sectionHeader confidence="0.999081" genericHeader="discussions">
4 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.99994075">
Applying SMT to new domains requires techniques
to inform our algorithms how best to adapt. This pa-
per extended the usual notion of domains to finer-
grained topic distributions induced in an unsuper-
vised fashion. We show that incorporating lexi-
cal weighting features conditioned on soft domain
membership directly into our model is an effective
strategy for dynamically biasing SMT towards rele-
vant translations, as evidenced by significant perfor-
mance gains. This method presents several advan-
tages over existing approaches. We can construct
a topic model once on the training data, and use
it infer topics on any test set to adapt the transla-
tion model. We can also incorporate large quanti-
ties of additional data (whether parallel or not) in
the source language to infer better topics without re-
lying on collection or genre annotations. Multilin-
gual topic models (Boyd-Graber and Resnik, 2010)
would provide a technique to use data from multiple
languages to ensure consistent topics.
</bodyText>
<page confidence="0.997899">
118
</page>
<sectionHeader confidence="0.99832" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999746454545455">
Vladimir Eidelman is supported by a National De-
fense Science and Engineering Graduate Fellow-
ship. This work was also supported in part by
NSF grant #1018625, ARL Cooperative Agree-
ment W911NF-09-2-0072, and by the BOLT and
GALE programs of the Defense Advanced Research
Projects Agency, Contracts HR0011-12-C-0015 and
HR0011-06-2-001, respectively. Any opinions, find-
ings, conclusions, or recommendations expressed
are the authors’ and do not necessarily reflect those
of the sponsors.
</bodyText>
<sectionHeader confidence="0.998932" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99947052">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of Emperical Methods in Natural
Language Processing.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet Allocation.
Journal of Machine Learning Research, 3:2003.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su-
pervised latent Dirichlet allocation. In Proceedings of
Emperical Methods in Natural Language Processing.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics, pages
310–318.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In Pro-
ceedings of the Human Language Technology Confer-
ence.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of Emperical Methods in Natural Language Process-
ing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, Stroudsburg, PA, USA.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of Em-
perical Methods in Natural Language Processing.
A. K. Mccallum. 2002. MALLET: A Machine Learning
for Language Toolkit.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. In
Computational Linguistics, volume 29(21), pages 19–
51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 311–
318.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of Emperical
Methods in Natural Language Processing.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, 21(4):187–207.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the Association for Computational Lin-
guistics.
</reference>
<page confidence="0.99907">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.132155">
<title confidence="0.998992">Topic Models for Dynamic Translation Model Adaptation</title>
<author confidence="0.95019">Vladimir</author>
<affiliation confidence="0.945398">Computer and University of</affiliation>
<address confidence="0.595388">College Park,</address>
<email confidence="0.999448">vlad@umiacs.umd.edu</email>
<author confidence="0.98847">Jordan</author>
<affiliation confidence="0.8314465">and University of</affiliation>
<address confidence="0.716526">College Park,</address>
<email confidence="0.999851">jbg@umiacs.umd.edu</email>
<author confidence="0.988873">Philip</author>
<affiliation confidence="0.9741485">and University of</affiliation>
<address confidence="0.641022">College Park,</address>
<email confidence="0.999865">resnik@umd.edu</email>
<abstract confidence="0.996377117647059">We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts, where topics are induced in an unsupervised way using topic models; this can be thought of as inducing subcorpora for adaptation without any human annotation. We use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features. Conditioning lexical probabilities on the topic biases translations toward topicrelevant output, resulting in significant imof up to 1 3 Chinese to English translation over a strong baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2008" citStr="Axelrod et al., 2011" startWordPosition="301" endWordPosition="304">ily preferable in another domain. Indeed, sometimes the domain can change the meaning of a phrase entirely. In a food related context, the Chinese sentence “粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) e</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>John Lafferty</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--2003</pages>
<contexts>
<context position="4568" citStr="Blei et al., 2003" startWordPosition="709" endWordPosition="712">ical weighting by domain. We induce unsupervised domains from large corpora, and we incorporate soft, probabilistic domain membership into a translation model. Unsupervised modeling of the training data produces naturally occurring subcorpora, generalizing beyond corpus and genre. Depending on the model used to select subcorpora, we can bias our translation toward any arbitrary distinction. This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation. In this work, we consider the underlying latent topics of the documents (Blei et al., 2003). Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment. In our case, by building a topic distribution for the source side of the training data, we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership. This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics. We accomplish this by introducing topic dependent lexical probabilities directly</context>
<context position="6791" citStr="Blei et al., 2003" startWordPosition="1068" endWordPosition="1071">assigning each sentence pair a set of features, fs(e|f), one for each domain s, which compute a new word translation table ps(e|f) estimated from only those sentences which belong to s: cs(f, e)/Ee cs(f, e), where cs(·) is the number of occurrences of the word pair in s. Topic Modeling for MT We extend provenance to cover a set of automatically generated topics zn. Given a parallel training corpus T composed of documents di, we build a source side topic model over T, which provides a topic distribution p(zn|di) for zn = 11, ... , K} over each document, using Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Then, we assign p(zn|di) to be the topic distribution for every sentence xj E di, thus enforcing topic sharing across sentence pairs in the same document instead of treating them as unrelated. Computing the topic distribution over a document and assigning it to the sentences serves to tie the sentences together in the document context. To obtain the lexical probability conditioned on topic distribution, we first compute the expected count ez..(e, f) of a word pair under topic zn: �ez..(e,f) = p(zn|di) � cj(e, f) (1) diET 2jEdi where cj(·) denotes the number of occurrences of the word pair in</context>
</contexts>
<marker>Blei, Ng, Jordan, Lafferty, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John Lafferty. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="16153" citStr="Boyd-Graber and Resnik, 2010" startWordPosition="2657" endWordPosition="2660">conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations, as evidenced by significant performance gains. This method presents several advantages over existing approaches. We can construct a topic model once on the training data, and use it infer topics on any test set to adapt the translation model. We can also incorporate large quantities of additional data (whether parallel or not) in the source language to infer better topics without relying on collection or genre annotations. Multilingual topic models (Boyd-Graber and Resnik, 2010) would provide a technique to use data from multiple languages to ensure consistent topics. 118 Acknowledgments Vladimir Eidelman is supported by a National Defense Science and Engineering Graduate Fellowship. This work was also supported in part by NSF grant #1018625, ARL Cooperative Agreement W911NF-09-2-0072, and by the BOLT and GALE programs of the Defense Advanced Research Projects Agency, Contracts HR0011-12-C-0015 and HR0011-06-2-001, respectively. Any opinions, findings, conclusions, or recommendations expressed are the authors’ and do not necessarily reflect those of the sponsors. Ref</context>
</contexts>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="12065" citStr="Chen and Goodman, 1996" startWordPosition="1978" endWordPosition="1981"> and non-HK Hansards portions of the NIST training corpora with LTM only. Table 1 summarizes the data statistics. For both settings, the data were lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). The Chinese data were segmented using the Stanford segmenter. We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4By having as many topics as genres/collections and setting p(z.|di) to 1 for every sentence in the collection and 0 to everything else. |{z } unadapted features |{z } adapted features 117 Corpus Sentences Tokens En Zh FBIS 269K 10.3M 7.9M NIST 1.6M 44.4M 40.4M Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), </context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Steve DeNeefe</author>
<author>Michael Pust</author>
</authors>
<title>Two easy improvements to lexical weighting.</title>
<date>2011</date>
<booktitle>In Proceedings of the Human Language Technology Conference.</booktitle>
<contexts>
<context position="2606" citStr="Chiang et al. (2011)" startWordPosition="397" endWordPosition="400"> (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notion of conditioning on provenance (i.e., the origin of the text) by removing the separate mapping step, directly optimizing the weight of the genre and collection features by computing a separate word translation table for each feature, estimated from only those sentences that comprise that genre or collection. The common thread throughout prior work is the concept of a domain. A domain is typically a hard constraint that is externally imposed and hand labeled, such as genre or corpus collection. For example, a sentence either comes from newswire, or weblog, but not both. </context>
<context position="6082" citStr="Chiang et al. (2011)" startWordPosition="943" endWordPosition="946">mproved translation performance, by up to 1 BLEU and 3 TER over a strong Chinese to English baseline. 2 Model Description Lexical Weighting Lexical weighting features estimate the quality of a phrase pair by combining the lexical translation probabilities of the words in the phrase2 (Koehn et al., 2003). Lexical conditional probabilities p(e|f) are obtained with maximum likelihood estimates from relative frequencies 2For hierarchical systems, these correspond to translation rules. c(f, e)/Ee c(f, e). Phrase pair probabilities p(e|f) are computed from these as described in Koehn et al. (2003). Chiang et al. (2011) showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features, fs(e|f), one for each domain s, which compute a new word translation table ps(e|f) estimated from only those sentences which belong to s: cs(f, e)/Ee cs(f, e), where cs(·) is the number of occurrences of the word pair in s. Topic Modeling for MT We extend provenance to cover a set of automatically generated topics zn. Given a parallel training corpus T composed of documents di, we build a source side topic model over T, which provides a topic distribution p</context>
<context position="9212" citStr="Chiang et al. (2011)" startWordPosition="1488" endWordPosition="1491">f topic k is dominant in T, pk(e|f) may be quite large, but if p(k|V ) is very small, then we should steer away from this phrase pair and select a competing phrase pair which may have a lower probability in T, but which is more relevant to the test sentence at hand. In many cases, document delineations may not be readily available for the training corpus. Furthermore, a document may be too broad, covering too many disparate topics, to effectively bias the weights on a phrase level. For this case, we also propose a local LDA model (LTM), which treats each sentence as a separate document. While Chiang et al. (2011) has to explicitly smooth the resulting ps(e|f), since many word pairs will be unseen for a given domain s, we are already performing an implicit form of smoothing (when computing the expected counts), since each document has a distribution over all topics, and therefore we have some probability of observing each word pair in every topic. Feature Representation After obtaining the topic conditional features, there are two ways to present them to the model. They could answer the question F1: What is the probability under topic 1, topic 2, etc., or F2: What is the probability under the most prob</context>
</contexts>
<marker>Chiang, DeNeefe, Pust, 2011</marker>
<rawString>David Chiang, Steve DeNeefe, and Michael Pust. 2011. Two easy improvements to lexical weighting. In Proceedings of the Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="12587" citStr="Crammer et al., 2006" startWordPosition="2070" endWordPosition="2073">AT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4By having as many topics as genres/collections and setting p(z.|di) to 1 for every sentence in the collection and 0 to everything else. |{z } unadapted features |{z } adapted features 117 Corpus Sentences Tokens En Zh FBIS 269K 10.3M 7.9M NIST 1.6M 44.4M 40.4M Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a Chinese stoplist and setting the per-document Dirichlet parameter α = 0.01. This setting of was chosen to encourage sparse topic assignments, which make induced subdomains consistent within a document. Results Results for both settings are shown in Table 2. GTM models the latent topics at the document level, while LTM models each sentence as a separate document. To evaluate the effect topic granularity would have on translation, we varied the number of latent topics in each m</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL System Demonstrations.</booktitle>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. In Proceedings of ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
</authors>
<title>Optimization strategies for online large-margin learning in machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="12604" citStr="Eidelman, 2012" startWordPosition="2074" endWordPosition="2075">aword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4By having as many topics as genres/collections and setting p(z.|di) to 1 for every sentence in the collection and 0 to everything else. |{z } unadapted features |{z } adapted features 117 Corpus Sentences Tokens En Zh FBIS 269K 10.3M 7.9M NIST 1.6M 44.4M 40.4M Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a Chinese stoplist and setting the per-document Dirichlet parameter α = 0.01. This setting of was chosen to encourage sparse topic assignments, which make induced subdomains consistent within a document. Results Results for both settings are shown in Table 2. GTM models the latent topics at the document level, while LTM models each sentence as a separate document. To evaluate the effect topic granularity would have on translation, we varied the number of latent topics in each model to be 5, 10,</context>
</contexts>
<marker>Eidelman, 2012</marker>
<rawString>Vladimir Eidelman. 2012. Optimization strategies for online large-margin learning in machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2029" citStr="Foster et al., 2010" startWordPosition="305" endWordPosition="308">her domain. Indeed, sometimes the domain can change the meaning of a phrase entirely. In a food related context, the Chinese sentence “粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notio</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5766" citStr="Koehn et al., 2003" startWordPosition="897" endWordPosition="900">robabilities directly as features in the translation model, and interpolating them log-linearly with our other features, thus allowing us to discriminatively optimize their weights on an arbitrary objective function. Incorporating these features into our hierarchical phrase-based translation system significantly improved translation performance, by up to 1 BLEU and 3 TER over a strong Chinese to English baseline. 2 Model Description Lexical Weighting Lexical weighting features estimate the quality of a phrase pair by combining the lexical translation probabilities of the words in the phrase2 (Koehn et al., 2003). Lexical conditional probabilities p(e|f) are obtained with maximum likelihood estimates from relative frequencies 2For hierarchical systems, these correspond to translation rules. c(f, e)/Ee c(f, e). Phrase pair probabilities p(e|f) are computed from these as described in Koehn et al. (2003). Chiang et al. (2011) showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features, fs(e|f), one for each domain s, which compute a new word translation table ps(e|f) estimated from only those sentences which belong to s: cs(f, e</context>
<context position="11772" citStr="Koehn et al., 2003" startWordPosition="1927" endWordPosition="1930"> our approach, we performed experiments on Chinese to English MT in two settings. First, we use the FBIS corpus as our training bitext. Since FBIS has document delineations, we compare local topic modeling (LTM) with modeling at the document level (GTM). The second setting uses the non-UN and non-HK Hansards portions of the NIST training corpora with LTM only. Table 1 summarizes the data statistics. For both settings, the data were lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). The Chinese data were segmented using the Stanford segmenter. We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4By having as many topics as genres/collections and setting p(z.|di) to 1 for every sentence in the collection and 0 to everything else. |{z } unadapted features |{z } adapted features 117 Corpus Sentences Tokens En Zh FBIS 269K 10.3M 7.9M NIST 1.6M 44.4M 40.4M Table 1: Corpus </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2189" citStr="Matsoukas et al. (2009)" startWordPosition="331" endWordPosition="334">du¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notion of conditioning on provenance (i.e., the origin of the text) by removing the separate mapping step, directly optimizing the weight of the genre and collection</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Mccallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<contexts>
<context position="12663" citStr="Mccallum, 2002" startWordPosition="2082" endWordPosition="2083">nd Goodman, 1996). We used cdec (Dyer et al., 4By having as many topics as genres/collections and setting p(z.|di) to 1 for every sentence in the collection and 0 to everything else. |{z } unadapted features |{z } adapted features 117 Corpus Sentences Tokens En Zh FBIS 269K 10.3M 7.9M NIST 1.6M 44.4M 40.4M Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a Chinese stoplist and setting the per-document Dirichlet parameter α = 0.01. This setting of was chosen to encourage sparse topic assignments, which make induced subdomains consistent within a document. Results Results for both settings are shown in Table 2. GTM models the latent topics at the document level, while LTM models each sentence as a separate document. To evaluate the effect topic granularity would have on translation, we varied the number of latent topics in each model to be 5, 10, and 20. On FBIS, we can see that both models achieve moder</context>
</contexts>
<marker>Mccallum, 2002</marker>
<rawString>A. K. Mccallum. 2002. MALLET: A Machine Learning for Language Toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>29</volume>
<issue>21</issue>
<pages>19--51</pages>
<contexts>
<context position="11655" citStr="Och and Ney, 2003" startWordPosition="1910" endWordPosition="1913">ights toward having topic information be useful, not toward a specific distribution. 3 Experiments Setup To evaluate our approach, we performed experiments on Chinese to English MT in two settings. First, we use the FBIS corpus as our training bitext. Since FBIS has document delineations, we compare local topic modeling (LTM) with modeling at the document level (GTM). The second setting uses the non-UN and non-HK Hansards portions of the NIST training corpora with LTM only. Table 1 summarizes the data statistics. For both settings, the data were lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). The Chinese data were segmented using the Stanford segmenter. We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4By having as many topics as genres/collections and setting p(z.|di) to 1 for every sentence in the collection and 0 to everything else. |{z } unadapted features</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. In Computational Linguistics, volume 29(21), pages 19– 51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="12484" citStr="Papineni et al., 2002" startWordPosition="2052" endWordPosition="2055"> the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4By having as many topics as genres/collections and setting p(z.|di) to 1 for every sentence in the collection and 0 to everything else. |{z } unadapted features |{z } adapted features 117 Corpus Sentences Tokens En Zh FBIS 269K 10.3M 7.9M NIST 1.6M 44.4M 40.4M Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a Chinese stoplist and setting the per-document Dirichlet parameter α = 0.01. This setting of was chosen to encourage sparse topic assignments, which make induced subdomains consistent within a document. Results Results for both settings are shown in Table 2. GTM models the latent topics at the document level, while LTM models each sentence as a separate document. To evaluate </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics, pages 311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Language and translation model adaptation using comparable corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2051" citStr="Snover et al., 2008" startWordPosition="309" endWordPosition="312">ometimes the domain can change the meaning of a phrase entirely. In a food related context, the Chinese sentence “粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notion of conditioning on p</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2008. Language and translation model adaptation using comparable corpora. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Bilingual LSA-based adaptation for statistical machine translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="4671" citStr="Tam et al., 2007" startWordPosition="727" endWordPosition="730">obabilistic domain membership into a translation model. Unsupervised modeling of the training data produces naturally occurring subcorpora, generalizing beyond corpus and genre. Depending on the model used to select subcorpora, we can bias our translation toward any arbitrary distinction. This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation. In this work, we consider the underlying latent topics of the documents (Blei et al., 2003). Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment. In our case, by building a topic distribution for the source side of the training data, we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership. This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics. We accomplish this by introducing topic dependent lexical probabilities directly as features in the translation model, and interpolating them log-linearly with our other features, thu</context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007. Bilingual LSA-based adaptation for statistical machine translation. Machine Translation, 21(4):187–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="14209" citStr="Teh et al., 2006" startWordPosition="2349" endWordPosition="2352">capturing document coherence in GTM and modeling purely on the sentence level is not substantial.5 In fact, the opposite is true, with the LTM models achieving better performance.6 On the NIST corpus, LTM-10 again achieves the best gain of approximately 1 BLEU and up to 3 TER. LTM performs on par with or better than GTM, and provides significant gains even in the NIST data setting, showing that this method can be effectively applied directly on the sentence level to large training 5An avenue of future work would condition the sentence topic distribution on a document distribution over topics (Teh et al., 2006). 6As an empirical validation of our earlier intuition regarding feature representation, presenting the features in the form of Fl caused the performance to remain virtually unchanged from the baseline model. Model MT03 MT05 TBLEU ,TER TBLEU ,TER BL 28.72 65.96 27.71 67.58 GTM-5 28.95ns 65.45 27.98ns 67.38ns GTM-10 29.22 64.47 28.19 66.15 GTM-20 29.19 63.41 28.00ns 64.89 LTM-5 29.23 64.57 28.19 66.30 LTM-10 29.29 63.98 28.18 65.56 LTM-20 29.09ns 63.57 27.90ns 65.17 Model MT03 MT05 TBLEU ,TER TBLEU ,TER BL 34.31 61.14 30.63 65.10 MERT 34.60 60.66 30.53 64.56 LTM-5 35.21 59.48 31.47 62.34 LTM-10</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>BiTAM: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4714" citStr="Zhao and Xing, 2006" startWordPosition="735" endWordPosition="738">nslation model. Unsupervised modeling of the training data produces naturally occurring subcorpora, generalizing beyond corpus and genre. Depending on the model used to select subcorpora, we can bias our translation toward any arbitrary distinction. This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation. In this work, we consider the underlying latent topics of the documents (Blei et al., 2003). Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment. In our case, by building a topic distribution for the source side of the training data, we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership. This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics. We accomplish this by introducing topic dependent lexical probabilities directly as features in the translation model, and interpolating them log-linearly with our other features, thus allowing us to discriminatively optimize </context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual topic admixture models for word alignment. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>