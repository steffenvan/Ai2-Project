<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997533">
Joint Optimization for Machine Translation System Combination
</title>
<author confidence="0.993468">
Xiaodong He
</author>
<affiliation confidence="0.9071825">
Microsoft Research
One Microsoft Way, Redmond, WA
</affiliation>
<email confidence="0.998174">
xiaohe@microsoft.com
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999328933333333">
System combination has emerged as a
powerful method for machine translation
(MT). This paper pursues a joint optimization
strategy for combining outputs from multiple
MT systems, where word alignment, ordering,
and lexical selection decisions are made
jointly according to a set of feature functions
combined in a single log-linear model. The
decoding algorithm is described in detail and a
set of new features that support this joint
decoding approach is proposed. The approach
is evaluated in comparison to state-of-the-art
confusion-network-based system combination
methods using equivalent features and shown
to outperform them significantly.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999697541666667">
System combination for machine translation
(MT) has emerged as a powerful method of
combining the strengths of multiple MT systems
and achieving results which surpass those of
each individual system (e.g. Bangalore, et. al.,
2001, Matusov, et. al., 2006, Rosti, et. al.,
2007a). Most state-of-the-art system combination
methods are based on constructing a confusion
network (CN) from several input translation
hypotheses, and choosing the best output from
the CN based on several scoring functions (e.g.
Rosti et. al., 2007a, He et. al., 2008, Matusov et
al. 2008). Confusion networks allow word-level
system combination, which was shown to
outperform sentence re-ranking methods and
phrase-level combination (Rosti, et. al. 2007a).
We will review confusion-network-based
system combination with the help of the
examples in Figures 1 and 2. Figure 1 shows
translation hypotheses from three Chinese-to-
English MT systems. The general idea is to
combine hypotheses in a representation such as
the ones in Figure 2, where for each word
position there is a set of possible words, shown
</bodyText>
<author confidence="0.534035">
Kristina Toutanova
</author>
<affiliation confidence="0.6021435">
Microsoft Research
One Microsoft Way, Redmond, WA
</affiliation>
<email confidence="0.983337">
kristout@microsoft.com
</email>
<bodyText confidence="0.99998715">
in columns.1The final output is determined by
choosing one word from each column, which can
be a real word or the empty word ε. For example,
the CN in Figure 2a) can generate eight distinct
sequences of words, including e.g. “she bought
the Jeep” and “she bought the SUV Jeep”. The
choice is performed to maximize a scoring
function using a set of features and a log-linear
model (Matusov, et. al 2006, Rosti, et al. 2007a).
We can view a confusion network as an
ordered sequence of columns (correspondence
sets). Each word from each input hypothesis
belongs to exactly one correspondence set. Each
correspondence set contains at most one word
from each input hypothesis and contributes
exactly one of its words (including the possible
ε) to the final output. Final words are output in
the order of correspondence sets. In order to
construct such a representation, we need to solve
the following two sub-problems: arrange words
from all input hypotheses into correspondence
sets (alignment problem) and order
correspondence sets (ordering problem). After
constructing the confusion network we need to
solve a third sub-problem: decide which words to
output from each correspondence set (lexical
choice problem).
In current state-of-the-art approaches, the
construction of the confusion network is
performed as follows: first, a backbone
hypothesis is selected. The backbone hypothesis
determines the order of words in the final system
output, and guides word-level alignments for
construction of columns of possible words at
each position. Let us assume that for our
example in Figure 1, the second hypothesis is
selected as a backbone. All other hypotheses are
aligned to the backbone such that these
alignments are one-to-one; empty words are
inserted where necessary to make one-to-one
</bodyText>
<footnote confidence="0.841075">
1 This representation is alternative to directed acyclic
graph representations of confusion networks.
</footnote>
<page confidence="0.894804">
1202
</page>
<note confidence="0.996623">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1202–1211,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99974522972973">
alignment possible. Words in all hypotheses are
sorted by the position of the backbone word they
align to and the confusion network is determined.
It is clear that the quality of selection of the
backbone and alignments has a large impact on
the performance, because the word order is
determined by the backbone, and the set of
possible words at each position is determined by
alignment. Since the space of possible
alignments is extremely large, approximate and
heuristic techniques have been employed to
derive them. In pair-wise alignment, each
hypothesis is aligned to the backbone in turn,
with separate processing to combine the multiple
alignments. Several models have been used for
pair-wise alignment, starting with TER and
proceeding with more sophisticated techniques
such as HMM models, ITG, and IHMM (Rosti
et. al 2007a, Matusov et al 2008, Krakos et al.
2008, He et al. 2008). A major problem with
such methods is that each hypothesis is aligned
to the backbone independently, leading to sub-
optimal behavior. For example, suppose that we
use a state-of-the-art word alignment model for
pairs of hypotheses, such as the IHMM. Figure 1
shows likely alignment links between every pair
of hypotheses. If Hypothesis 1 is aligned to
Hypothesis 2 (the backbone), Jeep is likely to
align to SUV because they express similar
Chinese content. Hypothesis 3 is separately
aligned to the backbone and since the alignment
is constrained to be one-to-one, SUV is aligned to
SUV and Jeep to an empty word which is
inserted after SUV. The network in Figure 2a) is
the result of this process. An undesirable
property of this CN is that the two instances of
Jeep are placed in separate columns and cannot
vote to reinforce each other.
Incremental alignment methods have been
proposed to relax the independence assumption
of pair-wise alignment (Rosti et al. 2008, Li et al.
2009). Such methods align hypotheses to a
partially constructed CN in some order. For
example, if in such method, Hypothesis 3 is first
aligned to the backbone, followed by Hypothesis
1, we are likely to arrive at the CN in Figure 2b)
in which the two instances of Jeep are aligned.
However, if Hypothesis 1 is aligned to the
backbone first, we would still get the CN in
Figure 2a). Notice that the desirable output “She
bought the Jeep SUV” cannot be generated from
either of the confusion networks because a re-
reordering of columns would be required.
A common characteristic of CN-based
approaches is that the order of words (backbone)
and the alignment of words (correspondence
sets) are decided as greedy steps independently
of the lexical choice for the final output. The
backbone and alignment are optimized according
to auxiliary scoring functions and heuristics
which may or may not be optimal with respect to
producing CNs leading to good translations. In
some recent approaches, these assumptions are
relaxed to allow each input hypothesis as a
backbone. Each backbone produces a separate
CN and the decision of which CN to choose is
taken at a later decoding stage, but this still
restricts the possible orders and alignments
greatly (Rosti et al. 2008, Matusov et al. 2008).
In this paper, we present a joint optimization
method for system combination. In this method,
the alignment, ordering and lexical selection sub-
problems are solved jointly in a single decoding
framework based on a log-linear model.
</bodyText>
<figureCaption confidence="0.994604">
Figure 1. Three MT system hypotheses with pair-
wise alignments.
</figureCaption>
<bodyText confidence="0.981186857142857">
she bought the Jeep E
she buys the SUV E
she bought the SUV Jeep
a) Confusion network with pair-wise alignment.
she bought the E Jeep
she buys the SUV E
she bought the SUV Jeep
</bodyText>
<figure confidence="0.861484">
b) Confusion network with incremental alignment.
</figure>
<figureCaption confidence="0.791069">
Figure 2. Correspondence sets of confusion networks
under pair-wise and incremental alignment, using the
second hypothesis as a backbone.
</figureCaption>
<sectionHeader confidence="0.999612" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999961625">
There has been a large body of work on MT
system combination. Among confusion-network-
based algorithms, most relevant to our work are
state-of-the-art methods for constructing word
alignments (correspondence sets) and methods
for improving the selection of a backbone
hypothesis. We have already reviewed such work
in the introduction and will note relation to
</bodyText>
<figure confidence="0.942448">
she bought the SUV Jeep
ε
the
ε SUV ε
she
buys
she
the
Jeep
bought
</figure>
<page confidence="0.980654">
1203
</page>
<bodyText confidence="0.999929197183099">
specific models throughout the paper as we
discuss specifics of our scoring functions.
In confusion network algorithms which use
pair-wise (or incremental) word-level alignment
algorithms for correspondence set construction,
problems of converting many-to-many
alignments and handling multiple insertions and
deletions need to be addressed. Prior work has
used a number of heuristics to deal with these
problems (Matusov, et. al., 2006, He et al 08).
Some work has made such decisions in a more
principled fashion by computing model-based
scores (Matusov et al. 2008), but still special-
purpose algorithms and heuristics are needed and
a single alignment is fixed.
In our approach, no heuristics are used to
convert alignments and no concept of a backbone
is used. Instead, the globally highest scoring
combination of alignment, order, and lexical
choice is selected (subject to search error).
Other than confusion-network-based
algorithms, work most closely related to ours is
the method of MT system combination proposed
in (Jayaraman and Lavie 2005), which we will
refer to as J&amp;L. Like our method, this approach
performs word-level system combination and is
not limited to following the word order of a
single backbone hypothesis; it also allows more
flexibility in the selection of correspondence sets
during decoding, compared to a confusion-
network-based approach. Even though their
algorithm and ours are broadly similar, there are
several important differences.
Firstly, the J&amp;L approach is based on pair-
wise alignments between words in different
hypotheses, which are hard and do not have
associated probabilities. Every word in every
hypothesis is aligned to at most one word from
each of the remaining hypotheses. Thus there is
no uncertainty about which words should belong
to the correspondence set of an aligned word w,
once that word is selected to extend a partial
hypothesis during search. If words do not have
corresponding matching words in some
hypotheses, heuristic matching to currently
unused words is attempted.
In contrast, our algorithm is based on the
definition of a joint scoring model, which takes
into account alignment uncertainty and combines
information from word-level alignment models,
ordering and lexical selection models, to address
the three sub-problems of word-level system
combination. In addition to the language model
and word-voting features used by the J&amp;L
model, we incorporate features which measure
alignment confidence via word-level alignment
models and features which evaluate re-ordering
via distortion models with respect to original
hypotheses. While the J&amp;L search algorithm
incorporates a number of special-purpose
heuristics to address phenomena of unused words
lagging behind the last used words, the goal in
our work is to minimize heuristics and perform
search to jointly optimize the assignment of
hidden variables (ordered correspondence sets)
and observed output variables (words in final
translations).
Finally, the J&amp;L method has not been
evaluated in comparison to confusion-network-
based methods to study the impact of performing
joint decoding for the three sub-problems.
</bodyText>
<sectionHeader confidence="0.99709" genericHeader="method">
3 Notation
</sectionHeader>
<bodyText confidence="0.9999735">
Before elaborating the models and decoding
algorithms, we first clarify the notation that will
be used in the paper.
We denote by H = {h1, ..., hN}the set of
hypotheses from multiple MT systems, where hi
is the hypothesis from the i-th system and hi is a
word sequence Wij, ...,Wi,L(i) with length L(i).
For simplicity, we assume that each system
contributes only its 1-best hypothesis for
combination. Accordingly, the i-th hypothesis hi
will be associated with a weight W (i) which is
the weight of the i-th system. In the scenario that
N-best lists are available from individual systems
for combination, the weight of each hypothesis
can be computed based on its rank in the N-best
list (Rosti et. al. 2007a).
Like in CN-based system combination, we
construct a set of ordered correspondence sets
(CS) from input hypotheses, and select one word
from each CS to form the final output. A CS is
defined as a set of (possibly empty) words, one
from each hypothesis, that implicitly align to
each other and that contributes exactly one of its
words to the final output. A valid complete set of
CS includes each non-empty word from each
hypothesis in exactly one CS. As opposed to CN-
based algorithms, our ordered correspondence
sets are constructed during a joint decoding
process which performs lexical selection at the
same time.
To facilitate the presentation of our features,
we define notation for ordered CS. A sequence
of correspondence sets is denoted by
C= CS1,..., CS,,,. Each correspondence set is
specified by listing the positions of each of the
words in the CS in their respective input
</bodyText>
<page confidence="0.976302">
1204
</page>
<bodyText confidence="0.9700473">
hypotheses. Each input hypothesis is assumed
to have one special empty word ε at position 0.
A CS is denoted by 𝐶𝑆 𝑙1, ...,𝑙𝑁 = 𝑤1,𝑙1, ... , 𝑤𝑁,𝑙𝑁 , where 𝑤𝑖,𝑙𝑖 is the li-th word in
the i-th hypothesis and the word position vector
𝑣 = 𝑙1, ... ,𝑙𝑁 𝑇 specifies the position of each
word in its original hypothesis. Correspondingly,
word 𝑤𝑖,𝑙𝑖 has the same weight 𝑊 (𝑖) as its
original hypothesis𝑕𝑖. As an example, the last
two correspondence sets specified by the CN in
Figure 2a) would be specified as 𝐶𝑆4 =
</bodyText>
<equation confidence="0.4080915">
𝐶𝑆 4,4,4 = {𝐽𝑒𝑒𝑝, 𝑆𝑈𝑉, 𝑆𝑈𝑉} and 𝐶𝑆5 =
𝐶𝑆 0,0,5 = {𝘀, 𝘀,𝐽𝑒𝑒𝑝}.
</equation>
<bodyText confidence="0.999963571428571">
As opposed to the CS defined in a
conventional CN, words that have the same
surface form but come from different hypotheses
are not collapsed to be one single candidate since
they have different original word positions. We
need to trace each of them separately during the
decoding process.
</bodyText>
<sectionHeader confidence="0.983574" genericHeader="method">
4 A Joint Optimization Framework For
</sectionHeader>
<subsectionHeader confidence="0.721117">
System Combination
</subsectionHeader>
<bodyText confidence="0.9998575">
The joint decoding framework chooses optimal
output according to the following log-linear
model:
where we denote by C the set of all possible
valid arrangements of CS, O the set of all
possible orders of CS, W the set of all possible
word sequences, consisting of words from the
input hypotheses. {𝑓𝑖(𝑤, 𝑂, 𝐶, 𝑯)} are the
features and {𝛼𝑖} are the feature weights in the
log-linear model, respectively.
</bodyText>
<subsectionHeader confidence="0.925514">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.934974">
A set of features are used in this framework.
Each of them models one or more of the
alignment, ordering, and lexical selection sub-
problems. Features are defined as follows.
Word posterior model:
The word posterior feature is the same as the
one proposed by Rosti et. al. (2007a). i.e.,
</bodyText>
<equation confidence="0.837713666666667">
𝑀
𝑓𝑤𝑝 𝑤, 𝑂, 𝐶, 𝑯 = 𝑙𝑜𝑔 𝑃 𝑤𝑚 𝐶𝑆𝑚
𝑚=1
</equation>
<bodyText confidence="0.998684">
where the posterior of a single word in a CS is
</bodyText>
<equation confidence="0.9870326">
computed based on a weighted voting score:
𝑃 𝑤𝑖,𝑙𝑖 𝐶𝑆 = 𝑃 𝑤𝑖,𝑙𝑖 𝐶𝑆 𝑙1, ...,𝑙𝑁
𝑁
= 𝑊(𝑘) 𝛿(𝑤𝑘,𝑙𝑘 = 𝑤𝑖,𝑙𝑖)
𝑘=1
</equation>
<bodyText confidence="0.99788225">
and M is the number of CS generated. Note
that M may be larger than the length of the
output word sequence w since some CS may
generate empty words.
</bodyText>
<subsectionHeader confidence="0.597112">
Bi-gram voting model:
</subsectionHeader>
<bodyText confidence="0.99880925">
The second feature we used is a bi-gram
voting feature proposed by Zhao and He (2009),
i.e., for each bi-gram 𝑤𝑖, 𝑤𝑖+1 , a weighted
position-independent voting score is computed:
</bodyText>
<equation confidence="0.991547666666667">
𝑁
𝑃 𝑤𝑖,𝑤𝑖+1 𝑯 = 𝑊(𝑘) 𝛿( 𝑤𝑖,𝑤𝑖+1 ∈ 𝑕𝑖)
𝑘=1
</equation>
<bodyText confidence="0.658458">
And the global bi-gram voting feature is
defined as:
</bodyText>
<equation confidence="0.86349275">
|𝑤|−1
𝑓𝑏𝑔𝑣 𝑤, 𝑂, 𝐶, 𝑯 = 𝑙𝑜𝑔 𝑃 𝑤𝑖, 𝑤𝑖+1 𝑯
𝑖=1
Distortion model:
</equation>
<bodyText confidence="0.999586470588235">
Unlike in the conventional CN-based system
combination, flexible orders of CS are allowed in
this joint decoding framework. In order to model
the distortion of different orderings, a distortion
model between two CS is defined as follows:
First we define the distortion cost between two
words at a single hypothesis. Similarly to the
distortion penalty in the conventional phrase-
based decoder (Koehn 2004b), the distortion cost
of jumping from a word at position i to another
word at position j, d(i,j), is proportional to the
distance between i and j, e.g., |i-j|. Then, the
distortion cost of jumping from one CS, which
has a position vector recording the original
position of each word in that CS, to another CS
is a weighted sum of single-hypothesis-based
distortion costs:
</bodyText>
<equation confidence="0.984304">
𝑁
𝑑(𝐶𝑆𝑚, 𝐶𝑆𝑚+1) = 𝑊 (𝑘) ∙ |𝑙𝑚,𝑘 − 𝑙𝑚 +1,𝑘 |
𝑘=1
</equation>
<bodyText confidence="0.999739">
where 𝑙𝑚,𝑘 and 𝑙𝑚+1,𝑘 are the k-th element of
the word position vector of CSm and CSm+1,
respectively. For the purpose of computing the
distortion feature, the position of an empty
word is taken to be the same as the position of
</bodyText>
<equation confidence="0.9702412">
𝑒𝑥𝑝 𝛼𝑖 ⋅ 𝑓𝑖(𝑤, 𝑂, 𝐶, 𝑯)
𝑖=1
𝐹
𝑤∗ = argmax
𝑤 ∈𝑾,𝑂 ∈𝑶,𝐶∈𝑪
</equation>
<page confidence="0.81011">
1205
</page>
<bodyText confidence="0.9984315">
the last visited non-empty word from the same
hypothesis.
The overall ordering feature can then be
computed based on 𝑑(𝐶𝑆𝑚, 𝐶𝑆𝑚 +1):
</bodyText>
<equation confidence="0.980938">
𝑀−1
𝑓𝑑𝑖𝑠 𝑤, 𝑂, 𝐶, 𝑯 = − 𝑑(𝐶𝑆𝑚, 𝐶𝑆𝑚 +1)
𝑚=1
</equation>
<bodyText confidence="0.999874">
It is worth noting that this is not the only
feature modeling the re-ordering behavior.
Under the joint decoding framework, other
features such as the language model and bi-gram
voting affect the ordering as well.
</bodyText>
<subsubsectionHeader confidence="0.352363">
Alignment model:
</subsubsectionHeader>
<bodyText confidence="0.99962725">
Each CS consists of a set of words, one from
each hypothesis, that are implicitly aligned to
each other. Therefore, a valid complete set of CS
defines the word alignment among different
hypotheses. In this paper, we derive an alignment
score of a CS based on alignment scores of word
pairs in that CS. To compute scores for word
pairs, we perform pair-wise hypothesis alignment
using the indirect HMM (He et al. 2008) for
every pair of input hypotheses. Note that this
involves a total of N by (N-1)/2 bi-directional
hypothesis alignments. The alignment score for a
pair of words 𝑤𝑗,𝑙𝑗 and 𝑤𝑘,𝑙𝑘 is defined as the
average of posterior probabilities of alignment
links in both directions and is thus direction
independent:
</bodyText>
<equation confidence="0.866231">
𝑝 𝑤𝑗,𝑙𝑗 ,𝑤𝑘,𝑙𝑘 =
1
2 𝑝(𝑎𝑙𝑗 = 𝑙𝑘|𝑕𝑗 , 𝑕𝑘) + 𝑝(𝑎𝑙𝑘 = 𝑙𝑗 |𝑕𝑘,
</equation>
<bodyText confidence="0.9398985">
𝑕𝑗 )
If one of the two words is ε, the posterior of
aligning word ε to state j is computed as
suggested by Liang et al. (2006), i.e.,
</bodyText>
<equation confidence="0.985946666666667">
𝐿(𝑘)
𝑝 𝑎0 = 𝑙𝑗 𝑕𝑘, 𝑕𝑗 = 1 − 𝑝 𝑎𝑖 = 𝑙𝑗 𝑕𝑘, 𝑕𝑗
𝑖=1
</equation>
<bodyText confidence="0.990874625">
And 𝑝 (𝑎𝑙𝑗 = 0|𝑕𝑗 , 𝑕𝑘) can be computed by the
HMM directly.
If both words are ε, then a pre-defined 𝑝𝘀𝘀 is
assigned, i.e., 𝑝 𝑎0 = 0 𝑕𝑘, 𝑕𝑗 = 𝑝𝘀𝘀, where 𝑝𝘀𝘀
can be optimized on a held-out validation set.
For a CS of words, if we set the j-th word as
an anchor word, the probability that all other
words align to that word is:
</bodyText>
<equation confidence="0.924937">
𝑁
𝑝(𝑗|𝐶𝑆) = 𝑝 𝑤𝑗,𝑙𝑗 ,𝑤𝑘,𝑙𝑘
𝑘=1
𝑘≠𝑗
</equation>
<bodyText confidence="0.996675">
The alignment score of the whole CS is a
weighted sum of the logarithm of the above
alignment probabilities, i.e.,
</bodyText>
<equation confidence="0.989466666666667">
𝑁
𝑆𝑎𝑙𝑛 (𝐶𝑆) = 𝑊(𝑗) 𝑙𝑜𝑔 𝑃 (𝑗 |𝐶𝑆)
𝑗=1
</equation>
<bodyText confidence="0.910315">
and the global alignment score is computed as:
</bodyText>
<equation confidence="0.97739375">
𝑀
𝑓𝑎𝑙𝑛 𝑤, 𝑂, 𝐶, 𝑯 = 𝑆𝑎𝑙𝑛 (𝐶𝑆𝑚)
𝑚=1
Entropy model:
</equation>
<bodyText confidence="0.9995975">
In general, it is preferable to align the same
word from different hypotheses into a common
CS. Therefore, we use entropy to model the
purity of a CS. The entropy of a CS is defined as:
</bodyText>
<equation confidence="0.98505475">
𝐸𝑛𝑡 𝐶𝑆 = 𝐸𝑛𝑡(𝐶𝑆 𝑙1, ... ,𝑙𝑁 ) =
𝑁′
𝑃 𝑤𝑖,𝑙𝑖 𝐶𝑆 𝑙𝑜𝑔𝑃 𝑤𝑖,𝑙𝑖 𝐶𝑆
𝑖=1
</equation>
<bodyText confidence="0.998648666666667">
where the sum is taken over all distinct words in
the CS. Then the global entropy score is
computed as:
</bodyText>
<equation confidence="0.918346666666667">
𝑀
𝑓𝑒𝑛𝑡 𝑤, 𝑂, 𝐶, 𝑯 = 𝐸𝑛𝑡( 𝐶𝑆𝑚)
𝑚=1
</equation>
<bodyText confidence="0.999903111111111">
Other features used in our log-linear model
include the count of real words |w|, a n-gram
language model, and the count M of CS sets.
These features address one or more of the
three sub-problems of MT system combination.
By performing joint decoding with all these
features working together, we hope to derive
better decisions on alignment, ordering and
lexical selection.
</bodyText>
<sectionHeader confidence="0.986618" genericHeader="method">
5 Joint Decoding
</sectionHeader>
<subsectionHeader confidence="0.988881">
5.1 Core algorithm
</subsectionHeader>
<bodyText confidence="0.9679495">
Decoding is based on a beam search algorithm
similar to that of the phrase-based MT decoder
(Koehn 2004b). The input is a set of translation
hypotheses to be combined, and the final output
</bodyText>
<page confidence="0.977304">
1206
</page>
<bodyText confidence="0.949489833333333">
sentence is generated left to right. Figure 3
illustrates the decoding process, using the
example input hypotheses from Figure 1. Each
decoding state represents a partial sequence of
correspondence sets covering some of the words
in the input hypotheses and a sequence of words
selected from the CS to form a partial output
hypothesis. The initial decoding state has an
empty sequence of CS and an empty output
sequence. A state corresponds to a complete
output candidate if its CS covers all input words.
d) decoding states
</bodyText>
<figureCaption confidence="0.997055">
Figure 3. Illustration of the decoding process.
</figureCaption>
<bodyText confidence="0.999931181818182">
In practice, because the features over
hypotheses can be decomposed, we do not need
to encode all of this information in a decoding
state. It suffices to store a few attributes. They
include positions of words from each input
hypothesis that have been visited, the last two
non-empty words generated (if a tri-gram LM is
used), and an &amp;quot;end position vector (EPV)&amp;quot;
recording positions of words in the last CS,
which were just visited. In the figure, the visited
words are shown with filled circles and the EPV
is shown with a dotted pattern in the filled
circles. Words specified by the EPV are
implicitly aligned. In the state in Figure 3 a) the
first three words of each hypothesis have been
visited, the third word of each hypothesis is the
last word visited (in the EPV), and the last two
words produced are “bought the”. The states also
record the decoding score accumulated so far and
an estimated future score to cover words that
have not been visited yet (not shown).
The expansion from one decoding state to a
set of new decoding states is illustrated in Figure
3. The expansion is done in three steps with the
help of intermediate states. Starting from a
decoding state as shown in Figure 3a), first a set
of “seed states” as shown in Figure 3b) are
generated. Each seed state represents a choice of
one of unvisited words, called a “seed word”
which is selected and marked as visited. For
example, the word Jeep from the first hypothesis
and the word SUV from the second hypothesis
are selected in the two seed states shown in
Figure 3b), respectively. These seed states
further expand into a set of &amp;quot;CS states&amp;quot; as shown
in Figure 3c). I.e., a CS is formed by picking one
word from each of the other hypotheses which is
unvisited and has a valid alignment link to the
seed word. Figure 3c) shows two CS states
expanded from the first seed state of Figure 3b),
using Jeep from the first hypothesis as a seed
word. In one of them the empty word from the
second hypothesis is chosen, and in the other, the
word SUV is chosen. Both are allowed by the
alignments illustrated in Figure 1. Finally, each
CS state generates one or more complete
decoding states, in which a word is chosen from
the current CS and the EPV vector is advanced to
reflect the last newly visited words. Figure 3d)
shows two such states, descending from the
corresponding CS states in 3c). After one more
expansion the state in 3d) on the left can generate
the translation “She bought the Jeep SUV”,
which cannot be produced by either confusion
network in Figure 2.
</bodyText>
<subsectionHeader confidence="0.997342">
5.2 Pruning
</subsectionHeader>
<bodyText confidence="0.911138333333333">
The full search space of joint decoding is a
product of the alignment, ordering, and lexical
selection spaces. Its size is exponential in the
length of the sentence and the number of
hypotheses involved in combination. Therefore,
pruning techniques are necessary to reduce the
search space.
First we will prune down the alignment space.
Instead of allowing any alignment link between
c) correspondence set states
lm: ... the Jeep
lm: ... the Jeep
</bodyText>
<equation confidence="0.476416571428571">
lm: ... bought the
a) a decoding state
lm: ... bought the
b) seed states
lm: ... bought the
lm: ... bought the
lm: ... bought the
</equation>
<page confidence="0.95701">
1207
</page>
<bodyText confidence="0.99992684">
arbitrary words of two hypotheses, only links
that have alignment score higher than a threshold
are allowed, plus links in the union of the Viterbi
alignments in both directions. In order to prevent
the garbage collection problem where many
words align to a rare word at the other side
(Moore, 2004), we further impose the limit that if
one word is aligned to more than T words, these
links are sorted by their alignment score and only
the top T links are kept. Meanwhile, alignments
between a real word and c are always allowed.
We then prune down the ordering space by
limiting the expansion of new states. Only states
that are adjacent to their preceding states are
created. Two states are called adjacent if their
EPVs are adjacent, i.e., given the EPV of the
preceding state m as [1m,1, •••, 1m,N]T and the
EPV of the next state m+1 as
[1m+1,1, ••• , 1m+1,N]T , if at least at one
dimension k, 1m+1,k = 1m,k+1, then these two
states are adjacent. When checking the
adjacency of two states, the position of an
empty word is taken to be the same as the
position of the last visited non-empty word
from the same hypothesis.
The number of possible CS states expanded
from a decoding state is exponential in the
number of hypotheses. In decoding, these CS
states are sorted by their alignment scores and
only the top K CS states are kept.
The search space can be further pruned down
by the widely used technique of path
recombination and by best-first pruning.
Path recombination is a risk-free pruning
method. Two paths can be recombined if they
agree on a) words from each hypothesis that have
been visited so far, b) the last two real words
generated, and c) their EPVs. In such case, we
only need to keep the path with the higher score.
Best-first pruning can help to reduce the
search space even further. In the decoding
process we compare paths that have generated
the same number of words (both real and empty
words) and only keep a certain number of most
promising paths. Pruning is based on an
estimated overall score of each path, which is the
sum of the decoding score accumulated so far
and an estimated future score to cover the words
that have not been visited. Next we discuss the
future score computation.
</bodyText>
<subsectionHeader confidence="0.997771">
5.3 Computing the future score
</subsectionHeader>
<bodyText confidence="0.999992909090909">
In order to estimate the future cost of an
unfinished path, we treat the unvisited words of
one input hypothesis as a backbone, and apply a
greedy search for alignment based on it; i.e., for
each word of this backbone, the most likely
words (based on the alignment link scores) from
other hypotheses, one word from each
hypothesis, are collected to form a CS. These CS
are ordered according to the word order of the
backbone and form a CN. Then, a light decoding
process with a search beam of size one is applied
to decode this CN and find the approximate
future path, with future feature scores computed
during the decoding process. If there are leftover
words not included in this CN, they are treated in
the way described in section 5.4. Additionally,
caching techniques are applied to speed up the
computation of future scores further.
Given the method discussed above, we can
estimate a future score based on each input
hypothesis, and the final future score is estimated
as the best of these hypothesis-dependent scores.
</bodyText>
<subsectionHeader confidence="0.999383">
5.4 Dealing with leftover input words
</subsectionHeader>
<bodyText confidence="0.999985363636364">
At a certain point a path will reach the end, i.e.,
no more states can be generated from it
according to the state expansion requirement.
Then it is marked as a finished path. However,
sometimes the state may contain a few input
words that have not been visited. An example of
this situation is the second state in Figure 3d).
The word SUV in the third input hypothesis is
left unvisited and it cannot be selected next
because there is no adjacent state that can be
generated. For such cases, we need to compute
an extra score of covering these leftover words.
Our approach is to create a state that produces
the same output translation, but also covers all
remaining words. For each leftover word, we
create a pseudo CS that contains just that word
plus c’s from all other hypotheses, and let it
output c. Moreover, that CS is inserted at a place
such that no extra distortion cost is incurred.
Figure 4 shows an example using the second
state in Figure 3d). The last two words from the
first two MT hypotheses “the Jeep” and “the
SUV” align to the third and fifth words of the
third hypothesis “the Jeep”; the word w3,4 from
the third hypothesis is left unvisited. The original
path has two CS and one left-over word w3,4. It is
expanded to have three CS, with a pseudo CS
inserted between the two CS.
It is worth noting that the new inserted pseudo
CS will not affect the word count feature and
contextually dependent feature scores such as the
LM and bi-gram voting, since it only generates
an empty word. Moreover, it will not affect the
</bodyText>
<page confidence="0.980883">
1208
</page>
<bodyText confidence="0.997929444444444">
distortion score either. For example, as shown in
Figure 4, the distortion cost of jumping from
word w2,3 to E2 and then to w2,4 is the same as
the cost of jumping from w2,3 to w2,4 given the
way we assign position to empty word and the
fact that the distortion cost is proportional to the
difference between word positions.
Scores of other features for this pseudo CS
such as word posterior (of E), alignment score,
CS entropy, and CS count are all local scores and
can be computed easily. Unlike future scores
which are approximate, the score computed in
this process is exact. Adding this extra score to
the existing score accumulated in the final state
gives the complete score of this finished path.
When all paths are finished, the one with the best
complete score is returned as the final output
sentence.
</bodyText>
<figureCaption confidence="0.8543755">
Figure 4. Expanding a leftover word to a pseudo
correspondence set.
</figureCaption>
<sectionHeader confidence="0.995977" genericHeader="conclusions">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999576">
6.1 Experimental conditions
</subsectionHeader>
<bodyText confidence="0.997706">
For the joint decoding method, the threshold for
alignment-score-based pruning is set to 0.25 and
the maximum number of words that can align to
the same word is limited to 3. We call this the
standard setting. The joint decoding approach is
evaluated on the Chinese-to-English (C2E) test
set of the 2008 NIST Open MT Evaluation
(NIST 2008). Results are reported in case
insensitive BLEU score in percentages
(Papineni et. al., 2002).
The NIST MT08 C2E test set contains 691
and 666 sentences of data from two genres,
newswire and web-data, respectively. Each test
sentence has four references provided by human
translators. Individual systems in our
experiments belong to the official submissions of
the MT08 C2E constraint-training track. Each
submission provides 1-best translation of the
whole test set. In order to train feature weights,
the original test set is divided into two parts,
called the dev and test set, respectively. The dev
set consists of the first half of both newswire and
web-data, and the test set consists of the second
half of data of both genres.
There are 20 individual systems available. We
ranked them by their BLEU score results on the
dev set and picked the top five systems,
excluding systems ranked 5th and 6th since they
are subsets of the first entry (NIST 2008).
Performance of these systems on the dev and test
sets is shown in Table 1.
The baselines include a pair-wise hypothesis
alignment approach using the indirect HMM
(IHMM) proposed by He et al. (2008), and an
incremental hypothesis alignment approach using
the incremental HMM (IncHMM) proposed by
Li et al. (2009). The lexical translation model
used to compute the semantic similarity is
estimated from two million parallel sentence-
pairs selected from the training corpus of MT08.
The backbone for the IHMM-based approach is
selected based on Minimum Bayes Risk (MBR)
using a BLEU-based loss function. The various
parameters of the IHMM and the IncHMM are
tuned on the dev set. The same IHMM is used to
compute the alignment feature score for the joint
decoding approach.
The final combination output can be obtained
by decoding the CN with a set of features. The
features used for the baseline systems are the
same as the features used by the joint decoding
approach. Some of these features are constant
across decoding hypotheses and can be ignored.
The non-constant features are word posterior, bi-
gram voting, language model score, and word
count. They are computed in the same way as for
the joint decoding approach.
System weights and feature weights are
trained together using Powell&apos;s search for the
IHMM-based approach. Then the same system
weights are applied to both IncHMM and Joint
Decoding -based approaches, and the feature
weights of them are trained using the max-BLEU
training method proposed by Och (2003) and
refined by Moore and Quirk (2008).
</bodyText>
<tableCaption confidence="0.9889295">
Table 1: Performance of individual systems on
the dev and test set
</tableCaption>
<table confidence="0.998538333333333">
System ID dev test
System A 32.88 31.81
System B 32.82 32.03
System C 32.16 31.87
System D 31.40 31.32
System E 27.44 27.67
</table>
<subsectionHeader confidence="0.997361">
6.2 Comparison against baselines
</subsectionHeader>
<bodyText confidence="0.998876">
Table 2 lists the BLEU scores achieved by the
two baselines and the joint decoding approach.
Both baselines surpass the best individual system
</bodyText>
<equation confidence="0.894535">
w1,3 w1,4 w1,3 E1 w1,4
w2,3 w2,4 =&gt; w2,3 E2 w2,4
w3,3 w3,4 w3,5 w3,3 w3,4 w3,5
</equation>
<page confidence="0.992524">
1209
</page>
<note confidence="0.902303333333333">
significantly. However, the gain of incremental to the standard setting that allows backbone-free
HMM over IHMM is smaller than that reported word ordering, the constrained settings did not
in Li et al. (2009). One possible reason of such lead to significant performance degradation. This
</note>
<bodyText confidence="0.632320714285714">
discrepancy could be that fewer hypotheses are indicates that most of the gain due to the joint
used for combination in this experiment decoding approach comes from the joint
compared to that of Li et al. (2009), so the optimization of alignment and word selection. It
performance difference between them is is possible, though, that if we lift the CS
narrowed accordingly. Despite that, the proposed adjacency constraint during search, we might
joint decoding method outperforms both IHMM derive more benefit from flexible word ordering.
and IncHMM baselines significantly. ng constraints
</bodyText>
<tableCaption confidence="0.999689666666667">
Table 2: Comparison between the joint decoding
approach and the two baselines
Table 4: Effect of orderi
</tableCaption>
<figure confidence="0.8410001">
Setting test
method dev test
IHMM 36.91 35.85 y hyp. 37.12
IncHMM 37.32 36.38
Joint Decoding 37.94*
7 Discussion
37.20
99%, measured based on the paired bootstrap re
-
sampling method (Koehn 2004a)
</figure>
<tableCaption confidence="0.995333">
Table 3: Compari
</tableCaption>
<bodyText confidence="0.984049046511628">
son between different settings
of alignment pruning
Setting Test
standard sett
ings 37.20
union of Viterbi 36.88
orderi
ng of CS in the decoding process. In the
first case, we restrict the order of CS to follow
the word order of a backbone, which is one of
the input hypotheses selected by MBR-BLEU. In
the second case, the order of CS is constrained to
follow the word order of at least one of the input
hypotheses. As shown in Table 4, in comparison
is evaluated against
baselines on
the NIST MT08 C2E task. The joint decoding
approach is shown to outperform baselines
significantly.
Because of the complexity of search, a
challenge for our approach is combining a large
number of input hypotheses. When N-best
hypotheses from the same system are added, it is
possible to pre-compute and fix the one-to-one
word alignment among the same-system
hypotheses; such pre-computation is reasonable
given our observation that the disagreement
among hypotheses from different systems is
larger than that among hypotheses from the same
system. This will reduce the alignment search
space to be the same as that for 1-best case. We
plan to study this setting in future work.
To further improve the performance of our
approach we see the biggest opportunity in
developing better estimates of future scores an
state-of-the-art
d
incorporating additional features. Beside
potential performance improvement, they may
help on more effective pruning and speed up the
overall decoding process as well.
* The gains of Joint Decoding over IHMM and
IncHMM are both with a statistical significance level &gt;
</bodyText>
<subsectionHeader confidence="0.996642">
6.3 Comparison of alignment pruning
</subsectionHeader>
<bodyText confidence="0.999977428571429">
The effect of alignment pruning is also studied.
We tested with limiting the allowable links to
just those that in the union of bi-directional
Viterbi alignments.
The results are presented in Table 3.
Compared to the standard setting, allowing only
links in the union of the bi-directional Viterbi
alignments causes slight performance
degradation. On the other hand, it still
outperforms the IHMM baseline by a fair margin.
This is because the joint decoding approach is
effectively resolving the ambiguous 1-to-many
alignments and deciding proper places to insert
empty words during decoding.
</bodyText>
<subsectionHeader confidence="0.99844">
6.4 Comparison of ordering constraints
</subsectionHeader>
<bodyText confidence="0.999654368421053">
In order to investigate the effect of allowing
flexible word ordering, we conducted
experiments using different constraints on the
standard settings 37.20
monotone w.r.t. backbone 37.22
monotone w.r.t. an
This paper proposed a joint optimization
approach for word-level combination of
translation hypotheses from multiple machine
translation systems. Unlike conventional
confusion-network-based methods, alignments
between words from different hypotheses are not
pre-determined and flexible word orderings are
allowed. Decisions on word alignment between
hypotheses, word ordering, and the lexical choice
of the final output are made jointly according to
a set of features in the decoding process. A new
set of features to model alignment and re-
ordering behavior is also proposed. The method
</bodyText>
<page confidence="0.966867">
1210
</page>
<sectionHeader confidence="0.995701" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984598630137">
Srinivas Bangalore, German Bordel, and Giuseppe
Riccardi. 2001. Computing consensus translation
from multiple machine translation systems. In
Proceedings of IEEE ASRU.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore 2008. Indirect HMM
based Hypothesis Alignment for Combining
Outputs from Machine Translation Systems. In
Proceedings of EMNLP.
Shyamsundar Jayaraman and Alon Lavie. 2005.
Multi-engine machine translation guided by explicit
word matching. In Proceedings of EAMT.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer 2008. Machine Translation
System Combination using ITG-based Alignments.
In Proceedings of ACL.
Philipp Koehn, 2004a, Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of EMNLP.
Philipp Koehn. 2004b. Pharaoh: A Beam Search
Decoder For Phrase Based Statistical Machine
Translation Models. In Proceedings of AMTA.
Chi-Ho Li, Xiaodong He, Yupeng Liu and Ning Xi,
2009. Incremental HMM Alignment for MT
System Combination. In Proceedings of ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006.
Personal Communication
Evgeny Matusov, Nicola Ueffing and Hermann Ney.
2006. Computing Consensus Translation from
Multiple Machine Translation Systems using
Enhanced Hypothesis Alignment. In Proceedings of
EACL.
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Déchelotte, Marcello
Federico, Muntsin Kolss, Young-Suk Lee, José B.
Mariño, Matthias Paulik, Salim Roukos, Holger
Schwenk, and Hermann Ney. 2008. System
combination for machine translation of spoken and
written language. IEEE transactions on audio
speech and language processing 16(7).
Robert C. Moore and Chris Quirk. 2008. Random
Restarts in Minimum Error Rate Training for
Statistical Machine Translation, In Proceedings of
COLING
Robert C. Moore. 2004. Improving IBM Word
Alignment Model 1, In Proceedings of ACL.
NIST 2008. The NIST Open Machine Translation
Evaluation.www.nist.gov/speech/tests/mt/2008/doc/
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward and
Wei- Jing Zhu 2002. BLEU: a Method for
Automatic Evaluation of Machine Translation. In
Proceedings of ACL.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007a. Combining outputs from multiple
machine translation systems. In Proceedings of
NAACL-HLT.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz 2007b. Improved Word-level System
Combination for Machine Translation. In
Proceedings of ACL.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz 2008. Incremental
Hypothesis Alignment for Building Confusion
Networks with Application to Machine Translation
System Combination. In Proceedings of the 3rd
ACL Workshop on SMT.
Yong Zhao and Xiaodong He, 2009. Using N-gram
based Features for Machine Translation System
Combination. In Proceedings of NAACL-HLT
</reference>
<page confidence="0.991216">
1211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937613">
<title confidence="0.999379">Joint Optimization for Machine Translation System Combination</title>
<author confidence="0.974325">Xiaodong He</author>
<affiliation confidence="0.999588">Microsoft Research</affiliation>
<address confidence="0.979736">One Microsoft Way, Redmond, WA</address>
<email confidence="0.999873">xiaohe@microsoft.com</email>
<abstract confidence="0.99891125">System combination has emerged as a powerful method for machine translation (MT). This paper pursues a joint optimization strategy for combining outputs from multiple MT systems, where word alignment, ordering, and lexical selection decisions are made jointly according to a set of feature functions combined in a single log-linear model. The decoding algorithm is described in detail and a set of new features that support this joint decoding approach is proposed. The approach is evaluated in comparison to state-of-the-art confusion-network-based system combination methods using equivalent features and shown to outperform them significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>German Bordel</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems.</title>
<date>2001</date>
<booktitle>In Proceedings of IEEE ASRU.</booktitle>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>Srinivas Bangalore, German Bordel, and Giuseppe Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In Proceedings of IEEE ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect HMM based Hypothesis Alignment for Combining Outputs from Machine Translation Systems.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4928" citStr="He et al. 2008" startWordPosition="748" endWordPosition="751">d order is determined by the backbone, and the set of possible words at each position is determined by alignment. Since the space of possible alignments is extremely large, approximate and heuristic techniques have been employed to derive them. In pair-wise alignment, each hypothesis is aligned to the backbone in turn, with separate processing to combine the multiple alignments. Several models have been used for pair-wise alignment, starting with TER and proceeding with more sophisticated techniques such as HMM models, ITG, and IHMM (Rosti et. al 2007a, Matusov et al 2008, Krakos et al. 2008, He et al. 2008). A major problem with such methods is that each hypothesis is aligned to the backbone independently, leading to suboptimal behavior. For example, suppose that we use a state-of-the-art word alignment model for pairs of hypotheses, such as the IHMM. Figure 1 shows likely alignment links between every pair of hypotheses. If Hypothesis 1 is aligned to Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content. Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-one, SUV is aligned to SUV and Jeep to </context>
<context position="17311" citStr="He et al. 2008" startWordPosition="2828" endWordPosition="2831">ot the only feature modeling the re-ordering behavior. Under the joint decoding framework, other features such as the language model and bi-gram voting affect the ordering as well. Alignment model: Each CS consists of a set of words, one from each hypothesis, that are implicitly aligned to each other. Therefore, a valid complete set of CS defines the word alignment among different hypotheses. In this paper, we derive an alignment score of a CS based on alignment scores of word pairs in that CS. To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al. 2008) for every pair of input hypotheses. Note that this involves a total of N by (N-1)/2 bi-directional hypothesis alignments. The alignment score for a pair of words 𝑤𝑗,𝑙𝑗 and 𝑤𝑘,𝑙𝑘 is defined as the average of posterior probabilities of alignment links in both directions and is thus direction independent: 𝑝 𝑤𝑗,𝑙𝑗 ,𝑤𝑘,𝑙𝑘 = 1 2 𝑝(𝑎𝑙𝑗 = 𝑙𝑘|𝑗 , 𝑘) + 𝑝(𝑎𝑙𝑘 = 𝑙𝑗 |𝑘, 𝑗 ) If one of the two words is ε, the posterior of aligning word ε to state j is computed as suggested by Liang et al. (2006), i.e., 𝐿(𝑘) 𝑝 𝑎0 = 𝑙𝑗 𝑘, 𝑗 = 1 − 𝑝 𝑎𝑖 = 𝑙𝑗 𝑘, 𝑗 𝑖=1 And 𝑝 (𝑎𝑙𝑗 = 0|𝑗 , 𝑘) can be computed by the HMM di</context>
<context position="30453" citStr="He et al. (2008)" startWordPosition="5184" endWordPosition="5187">two parts, called the dev and test set, respectively. The dev set consists of the first half of both newswire and web-data, and the test set consists of the second half of data of both genres. There are 20 individual systems available. We ranked them by their BLEU score results on the dev set and picked the top five systems, excluding systems ranked 5th and 6th since they are subsets of the first entry (NIST 2008). Performance of these systems on the dev and test sets is shown in Table 1. The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al. (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al. (2009). The lexical translation model used to compute the semantic similarity is estimated from two million parallel sentencepairs selected from the training corpus of MT08. The backbone for the IHMM-based approach is selected based on Minimum Bayes Risk (MBR) using a BLEU-based loss function. The various parameters of the IHMM and the IncHMM are tuned on the dev set. The same IHMM is used to compute the alignment feature score for the joint decoding approach. The final combination outpu</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore 2008. Indirect HMM based Hypothesis Alignment for Combining Outputs from Machine Translation Systems. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shyamsundar Jayaraman</author>
<author>Alon Lavie</author>
</authors>
<title>Multi-engine machine translation guided by explicit word matching.</title>
<date>2005</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="9351" citStr="Jayaraman and Lavie 2005" startWordPosition="1467" endWordPosition="1470">al., 2006, He et al 08). Some work has made such decisions in a more principled fashion by computing model-based scores (Matusov et al. 2008), but still specialpurpose algorithms and heuristics are needed and a single alignment is fixed. In our approach, no heuristics are used to convert alignments and no concept of a backbone is used. Instead, the globally highest scoring combination of alignment, order, and lexical choice is selected (subject to search error). Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&amp;L. Like our method, this approach performs word-level system combination and is not limited to following the word order of a single backbone hypothesis; it also allows more flexibility in the selection of correspondence sets during decoding, compared to a confusionnetwork-based approach. Even though their algorithm and ours are broadly similar, there are several important differences. Firstly, the J&amp;L approach is based on pairwise alignments between words in different hypotheses, which are hard and do not have associated probabilities. Every word in every hypothes</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-engine machine translation guided by explicit word matching. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Markus Dreyer</author>
</authors>
<title>Machine Translation System Combination using ITG-based Alignments.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer 2008. Machine Translation System Combination using ITG-based Alignments. In Proceedings of ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>2004a, Statistical Significance Tests for Machine Translation Evaluation.</title>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Koehn, </marker>
<rawString>Philipp Koehn, 2004a, Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A Beam Search Decoder For Phrase Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="15775" citStr="Koehn 2004" startWordPosition="2551" endWordPosition="2552">d position-independent voting score is computed: 𝑁 𝑃 𝑤𝑖,𝑤𝑖+1 𝑯 = 𝑊(𝑘) 𝛿( 𝑤𝑖,𝑤𝑖+1 ∈ 𝑖) 𝑘=1 And the global bi-gram voting feature is defined as: |𝑤|−1 𝑓𝑏𝑔𝑣 𝑤, 𝑂, 𝐶, 𝑯 = 𝑙𝑜𝑔 𝑃 𝑤𝑖, 𝑤𝑖+1 𝑯 𝑖=1 Distortion model: Unlike in the conventional CN-based system combination, flexible orders of CS are allowed in this joint decoding framework. In order to model the distortion of different orderings, a distortion model between two CS is defined as follows: First we define the distortion cost between two words at a single hypothesis. Similarly to the distortion penalty in the conventional phrasebased decoder (Koehn 2004b), the distortion cost of jumping from a word at position i to another word at position j, d(i,j), is proportional to the distance between i and j, e.g., |i-j|. Then, the distortion cost of jumping from one CS, which has a position vector recording the original position of each word in that CS, to another CS is a weighted sum of single-hypothesis-based distortion costs: 𝑁 𝑑(𝐶𝑆𝑚, 𝐶𝑆𝑚+1) = 𝑊 (𝑘) ∙ |𝑙𝑚,𝑘 − 𝑙𝑚 +1,𝑘 | 𝑘=1 where 𝑙𝑚,𝑘 and 𝑙𝑚+1,𝑘 are the k-th element of the word position vector of CSm and CSm+1, respectively. For the purpose of computing the distortion feature, the position of an emp</context>
<context position="19368" citStr="Koehn 2004" startWordPosition="3237" endWordPosition="3238"> words in the CS. Then the global entropy score is computed as: 𝑀 𝑓𝑒𝑛𝑡 𝑤, 𝑂, 𝐶, 𝑯 = 𝐸𝑛𝑡( 𝐶𝑆𝑚) 𝑚=1 Other features used in our log-linear model include the count of real words |w|, a n-gram language model, and the count M of CS sets. These features address one or more of the three sub-problems of MT system combination. By performing joint decoding with all these features working together, we hope to derive better decisions on alignment, ordering and lexical selection. 5 Joint Decoding 5.1 Core algorithm Decoding is based on a beam search algorithm similar to that of the phrase-based MT decoder (Koehn 2004b). The input is a set of translation hypotheses to be combined, and the final output 1206 sentence is generated left to right. Figure 3 illustrates the decoding process, using the example input hypotheses from Figure 1. Each decoding state represents a partial sequence of correspondence sets covering some of the words in the input hypotheses and a sequence of words selected from the CS to form a partial output hypothesis. The initial decoding state has an empty sequence of CS and an empty output sequence. A state corresponds to a complete output candidate if its CS covers all input words. d) </context>
<context position="33446" citStr="Koehn 2004" startWordPosition="5674" endWordPosition="5675">e difference between them is is possible, though, that if we lift the CS narrowed accordingly. Despite that, the proposed adjacency constraint during search, we might joint decoding method outperforms both IHMM derive more benefit from flexible word ordering. and IncHMM baselines significantly. ng constraints Table 2: Comparison between the joint decoding approach and the two baselines Table 4: Effect of orderi Setting test method dev test IHMM 36.91 35.85 y hyp. 37.12 IncHMM 37.32 36.38 Joint Decoding 37.94* 7 Discussion 37.20 99%, measured based on the paired bootstrap re - sampling method (Koehn 2004a) Table 3: Compari son between different settings of alignment pruning Setting Test standard sett ings 37.20 union of Viterbi 36.88 orderi ng of CS in the decoding process. In the first case, we restrict the order of CS to follow the word order of a backbone, which is one of the input hypotheses selected by MBR-BLEU. In the second case, the order of CS is constrained to follow the word order of at least one of the input hypotheses. As shown in Table 4, in comparison is evaluated against baselines on the NIST MT08 C2E task. The joint decoding approach is shown to outperform baselines significa</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004b. Pharaoh: A Beam Search Decoder For Phrase Based Statistical Machine Translation Models. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
</authors>
<title>Xiaodong He, Yupeng Liu and Ning Xi,</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Li, 2009</marker>
<rawString>Chi-Ho Li, Xiaodong He, Yupeng Liu and Ning Xi, 2009. Incremental HMM Alignment for MT System Combination. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<date>2006</date>
<journal>Personal Communication</journal>
<contexts>
<context position="17801" citStr="Liang et al. (2006)" startWordPosition="2921" endWordPosition="2924">s in that CS. To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al. 2008) for every pair of input hypotheses. Note that this involves a total of N by (N-1)/2 bi-directional hypothesis alignments. The alignment score for a pair of words 𝑤𝑗,𝑙𝑗 and 𝑤𝑘,𝑙𝑘 is defined as the average of posterior probabilities of alignment links in both directions and is thus direction independent: 𝑝 𝑤𝑗,𝑙𝑗 ,𝑤𝑘,𝑙𝑘 = 1 2 𝑝(𝑎𝑙𝑗 = 𝑙𝑘|𝑗 , 𝑘) + 𝑝(𝑎𝑙𝑘 = 𝑙𝑗 |𝑘, 𝑗 ) If one of the two words is ε, the posterior of aligning word ε to state j is computed as suggested by Liang et al. (2006), i.e., 𝐿(𝑘) 𝑝 𝑎0 = 𝑙𝑗 𝑘, 𝑗 = 1 − 𝑝 𝑎𝑖 = 𝑙𝑗 𝑘, 𝑗 𝑖=1 And 𝑝 (𝑎𝑙𝑗 = 0|𝑗 , 𝑘) can be computed by the HMM directly. If both words are ε, then a pre-defined 𝑝𝘀𝘀 is assigned, i.e., 𝑝 𝑎0 = 0 𝑘, 𝑗 = 𝑝𝘀𝘀, where 𝑝𝘀𝘀 can be optimized on a held-out validation set. For a CS of words, if we set the j-th word as an anchor word, the probability that all other words align to that word is: 𝑁 𝑝(𝑗|𝐶𝑆) = 𝑝 𝑤𝑗,𝑙𝑗 ,𝑤𝑘,𝑙𝑘 𝑘=1 𝑘≠𝑗 The alignment score of the whole CS is a weighted sum of the logarithm of the above alignment probabilities, i.e., 𝑁 𝑆𝑎𝑙𝑛 (𝐶𝑆) = 𝑊(𝑗) 𝑙𝑜𝑔 𝑃 (𝑗 |𝐶𝑆) 𝑗=1 and the global alignment score</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Personal Communication</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing Consensus Translation from Multiple Machine Translation Systems using Enhanced Hypothesis Alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing and Hermann Ney. 2006. Computing Consensus Translation from Multiple Machine Translation Systems using Enhanced Hypothesis Alignment. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Gregor Leusch</author>
<author>Rafael E Banchs</author>
<author>Nicola Bertoldi</author>
<author>Daniel Déchelotte</author>
<author>Marcello Federico</author>
<author>Muntsin Kolss</author>
<author>Young-Suk Lee</author>
<author>José B Mariño</author>
<author>Matthias Paulik</author>
<author>Salim Roukos</author>
<author>Holger Schwenk</author>
<author>Hermann Ney</author>
</authors>
<title>System combination for machine translation of spoken and written language. IEEE transactions on audio speech and language processing 16(7).</title>
<date>2008</date>
<contexts>
<context position="1380" citStr="Matusov et al. 2008" startWordPosition="195" endWordPosition="198"> shown to outperform them significantly. 1 Introduction System combination for machine translation (MT) has emerged as a powerful method of combining the strengths of multiple MT systems and achieving results which surpass those of each individual system (e.g. Bangalore, et. al., 2001, Matusov, et. al., 2006, Rosti, et. al., 2007a). Most state-of-the-art system combination methods are based on constructing a confusion network (CN) from several input translation hypotheses, and choosing the best output from the CN based on several scoring functions (e.g. Rosti et. al., 2007a, He et. al., 2008, Matusov et al. 2008). Confusion networks allow word-level system combination, which was shown to outperform sentence re-ranking methods and phrase-level combination (Rosti, et. al. 2007a). We will review confusion-network-based system combination with the help of the examples in Figures 1 and 2. Figure 1 shows translation hypotheses from three Chinese-toEnglish MT systems. The general idea is to combine hypotheses in a representation such as the ones in Figure 2, where for each word position there is a set of possible words, shown Kristina Toutanova Microsoft Research One Microsoft Way, Redmond, WA kristout@micro</context>
<context position="4891" citStr="Matusov et al 2008" startWordPosition="740" endWordPosition="743">pact on the performance, because the word order is determined by the backbone, and the set of possible words at each position is determined by alignment. Since the space of possible alignments is extremely large, approximate and heuristic techniques have been employed to derive them. In pair-wise alignment, each hypothesis is aligned to the backbone in turn, with separate processing to combine the multiple alignments. Several models have been used for pair-wise alignment, starting with TER and proceeding with more sophisticated techniques such as HMM models, ITG, and IHMM (Rosti et. al 2007a, Matusov et al 2008, Krakos et al. 2008, He et al. 2008). A major problem with such methods is that each hypothesis is aligned to the backbone independently, leading to suboptimal behavior. For example, suppose that we use a state-of-the-art word alignment model for pairs of hypotheses, such as the IHMM. Figure 1 shows likely alignment links between every pair of hypotheses. If Hypothesis 1 is aligned to Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content. Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-on</context>
<context position="7193" citStr="Matusov et al. 2008" startWordPosition="1128" endWordPosition="1131">(correspondence sets) are decided as greedy steps independently of the lexical choice for the final output. The backbone and alignment are optimized according to auxiliary scoring functions and heuristics which may or may not be optimal with respect to producing CNs leading to good translations. In some recent approaches, these assumptions are relaxed to allow each input hypothesis as a backbone. Each backbone produces a separate CN and the decision of which CN to choose is taken at a later decoding stage, but this still restricts the possible orders and alignments greatly (Rosti et al. 2008, Matusov et al. 2008). In this paper, we present a joint optimization method for system combination. In this method, the alignment, ordering and lexical selection subproblems are solved jointly in a single decoding framework based on a log-linear model. Figure 1. Three MT system hypotheses with pairwise alignments. she bought the Jeep E she buys the SUV E she bought the SUV Jeep a) Confusion network with pair-wise alignment. she bought the E Jeep she buys the SUV E she bought the SUV Jeep b) Confusion network with incremental alignment. Figure 2. Correspondence sets of confusion networks under pair-wise and increm</context>
<context position="8867" citStr="Matusov et al. 2008" startWordPosition="1393" endWordPosition="1396">Jeep ε the ε SUV ε she buys she the Jeep bought 1203 specific models throughout the paper as we discuss specifics of our scoring functions. In confusion network algorithms which use pair-wise (or incremental) word-level alignment algorithms for correspondence set construction, problems of converting many-to-many alignments and handling multiple insertions and deletions need to be addressed. Prior work has used a number of heuristics to deal with these problems (Matusov, et. al., 2006, He et al 08). Some work has made such decisions in a more principled fashion by computing model-based scores (Matusov et al. 2008), but still specialpurpose algorithms and heuristics are needed and a single alignment is fixed. In our approach, no heuristics are used to convert alignments and no concept of a backbone is used. Instead, the globally highest scoring combination of alignment, order, and lexical choice is selected (subject to search error). Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&amp;L. Like our method, this approach performs word-level system combination and is not li</context>
</contexts>
<marker>Matusov, Leusch, Banchs, Bertoldi, Déchelotte, Federico, Kolss, Lee, Mariño, Paulik, Roukos, Schwenk, Ney, 2008</marker>
<rawString>Evgeny Matusov, Gregor Leusch, Rafael E. Banchs, Nicola Bertoldi, Daniel Déchelotte, Marcello Federico, Muntsin Kolss, Young-Suk Lee, José B. Mariño, Matthias Paulik, Salim Roukos, Holger Schwenk, and Hermann Ney. 2008. System combination for machine translation of spoken and written language. IEEE transactions on audio speech and language processing 16(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Random Restarts in Minimum Error Rate Training for Statistical Machine Translation,</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="31815" citStr="Moore and Quirk (2008)" startWordPosition="5406" endWordPosition="5409"> the joint decoding approach. Some of these features are constant across decoding hypotheses and can be ignored. The non-constant features are word posterior, bigram voting, language model score, and word count. They are computed in the same way as for the joint decoding approach. System weights and feature weights are trained together using Powell&apos;s search for the IHMM-based approach. Then the same system weights are applied to both IncHMM and Joint Decoding -based approaches, and the feature weights of them are trained using the max-BLEU training method proposed by Och (2003) and refined by Moore and Quirk (2008). Table 1: Performance of individual systems on the dev and test set System ID dev test System A 32.88 31.81 System B 32.82 32.03 System C 32.16 31.87 System D 31.40 31.32 System E 27.44 27.67 6.2 Comparison against baselines Table 2 lists the BLEU scores achieved by the two baselines and the joint decoding approach. Both baselines surpass the best individual system w1,3 w1,4 w1,3 E1 w1,4 w2,3 w2,4 =&gt; w2,3 E2 w2,4 w3,3 w3,4 w3,5 w3,3 w3,4 w3,5 1209 significantly. However, the gain of incremental to the standard setting that allows backbone-free HMM over IHMM is smaller than that reported word </context>
</contexts>
<marker>Moore, Quirk, 2008</marker>
<rawString>Robert C. Moore and Chris Quirk. 2008. Random Restarts in Minimum Error Rate Training for Statistical Machine Translation, In Proceedings of COLING</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving IBM Word Alignment Model 1,</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="23468" citStr="Moore, 2004" startWordPosition="3958" endWordPosition="3959">uce the search space. First we will prune down the alignment space. Instead of allowing any alignment link between c) correspondence set states lm: ... the Jeep lm: ... the Jeep lm: ... bought the a) a decoding state lm: ... bought the b) seed states lm: ... bought the lm: ... bought the lm: ... bought the 1207 arbitrary words of two hypotheses, only links that have alignment score higher than a threshold are allowed, plus links in the union of the Viterbi alignments in both directions. In order to prevent the garbage collection problem where many words align to a rare word at the other side (Moore, 2004), we further impose the limit that if one word is aligned to more than T words, these links are sorted by their alignment score and only the top T links are kept. Meanwhile, alignments between a real word and c are always allowed. We then prune down the ordering space by limiting the expansion of new states. Only states that are adjacent to their preceding states are created. Two states are called adjacent if their EPVs are adjacent, i.e., given the EPV of the preceding state m as [1m,1, •••, 1m,N]T and the EPV of the next state m+1 as [1m+1,1, ••• , 1m+1,N]T , if at least at one dimension k, </context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving IBM Word Alignment Model 1, In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<date>2008</date>
<booktitle>The NIST Open Machine Translation Evaluation.www.nist.gov/speech/tests/mt/2008/doc/</booktitle>
<contexts>
<context position="29296" citStr="NIST 2008" startWordPosition="4993" endWordPosition="4994">state gives the complete score of this finished path. When all paths are finished, the one with the best complete score is returned as the final output sentence. Figure 4. Expanding a leftover word to a pseudo correspondence set. 6 Evaluation 6.1 Experimental conditions For the joint decoding method, the threshold for alignment-score-based pruning is set to 0.25 and the maximum number of words that can align to the same word is limited to 3. We call this the standard setting. The joint decoding approach is evaluated on the Chinese-to-English (C2E) test set of the 2008 NIST Open MT Evaluation (NIST 2008). Results are reported in case insensitive BLEU score in percentages (Papineni et. al., 2002). The NIST MT08 C2E test set contains 691 and 666 sentences of data from two genres, newswire and web-data, respectively. Each test sentence has four references provided by human translators. Individual systems in our experiments belong to the official submissions of the MT08 C2E constraint-training track. Each submission provides 1-best translation of the whole test set. In order to train feature weights, the original test set is divided into two parts, called the dev and test set, respectively. The d</context>
</contexts>
<marker>NIST, 2008</marker>
<rawString>NIST 2008. The NIST Open Machine Translation Evaluation.www.nist.gov/speech/tests/mt/2008/doc/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="31777" citStr="Och (2003)" startWordPosition="5401" endWordPosition="5402">me as the features used by the joint decoding approach. Some of these features are constant across decoding hypotheses and can be ignored. The non-constant features are word posterior, bigram voting, language model score, and word count. They are computed in the same way as for the joint decoding approach. System weights and feature weights are trained together using Powell&apos;s search for the IHMM-based approach. Then the same system weights are applied to both IncHMM and Joint Decoding -based approaches, and the feature weights of them are trained using the max-BLEU training method proposed by Och (2003) and refined by Moore and Quirk (2008). Table 1: Performance of individual systems on the dev and test set System ID dev test System A 32.88 31.81 System B 32.82 32.03 System C 32.16 31.87 System D 31.40 31.32 System E 27.44 27.67 6.2 Comparison against baselines Table 2 lists the BLEU scores achieved by the two baselines and the joint decoding approach. Both baselines surpass the best individual system w1,3 w1,4 w1,3 E1 w1,4 w2,3 w2,4 =&gt; w2,3 E2 w2,4 w3,3 w3,4 w3,5 w3,3 w3,4 w3,5 1209 significantly. However, the gain of incremental to the standard setting that allows backbone-free HMM over IH</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei- Jing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and Wei- Jing Zhu 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2410" citStr="Rosti, et al. 2007" startWordPosition="360" endWordPosition="363">ation such as the ones in Figure 2, where for each word position there is a set of possible words, shown Kristina Toutanova Microsoft Research One Microsoft Way, Redmond, WA kristout@microsoft.com in columns.1The final output is determined by choosing one word from each column, which can be a real word or the empty word ε. For example, the CN in Figure 2a) can generate eight distinct sequences of words, including e.g. “she bought the Jeep” and “she bought the SUV Jeep”. The choice is performed to maximize a scoring function using a set of features and a log-linear model (Matusov, et. al 2006, Rosti, et al. 2007a). We can view a confusion network as an ordered sequence of columns (correspondence sets). Each word from each input hypothesis belongs to exactly one correspondence set. Each correspondence set contains at most one word from each input hypothesis and contributes exactly one of its words (including the possible ε) to the final output. Final words are output in the order of correspondence sets. In order to construct such a representation, we need to solve the following two sub-problems: arrange words from all input hypotheses into correspondence sets (alignment problem) and order corresponden</context>
</contexts>
<marker>Rosti, Xiang, Matsoukas, Schwartz, Ayan, Dorr, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, Richard Schwartz, Necip Fazil Ayan, and Bonnie J. Dorr. 2007a. Combining outputs from multiple machine translation systems. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Antti-Veikko I Rosti</author>
</authors>
<title>Spyros Matsoukas, and Richard Schwartz 2007b. Improved Word-level System Combination for Machine Translation.</title>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Rosti, </marker>
<rawString>Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard Schwartz 2007b. Improved Word-level System Combination for Machine Translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Incremental Hypothesis Alignment for Building Confusion Networks with Application to Machine Translation System Combination.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd ACL Workshop on SMT.</booktitle>
<contexts>
<context position="5896" citStr="Rosti et al. 2008" startWordPosition="909" endWordPosition="912"> Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content. Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-one, SUV is aligned to SUV and Jeep to an empty word which is inserted after SUV. The network in Figure 2a) is the result of this process. An undesirable property of this CN is that the two instances of Jeep are placed in separate columns and cannot vote to reinforce each other. Incremental alignment methods have been proposed to relax the independence assumption of pair-wise alignment (Rosti et al. 2008, Li et al. 2009). Such methods align hypotheses to a partially constructed CN in some order. For example, if in such method, Hypothesis 3 is first aligned to the backbone, followed by Hypothesis 1, we are likely to arrive at the CN in Figure 2b) in which the two instances of Jeep are aligned. However, if Hypothesis 1 is aligned to the backbone first, we would still get the CN in Figure 2a). Notice that the desirable output “She bought the Jeep SUV” cannot be generated from either of the confusion networks because a rereordering of columns would be required. A common characteristic of CN-based</context>
<context position="7171" citStr="Rosti et al. 2008" startWordPosition="1124" endWordPosition="1127">alignment of words (correspondence sets) are decided as greedy steps independently of the lexical choice for the final output. The backbone and alignment are optimized according to auxiliary scoring functions and heuristics which may or may not be optimal with respect to producing CNs leading to good translations. In some recent approaches, these assumptions are relaxed to allow each input hypothesis as a backbone. Each backbone produces a separate CN and the decision of which CN to choose is taken at a later decoding stage, but this still restricts the possible orders and alignments greatly (Rosti et al. 2008, Matusov et al. 2008). In this paper, we present a joint optimization method for system combination. In this method, the alignment, ordering and lexical selection subproblems are solved jointly in a single decoding framework based on a log-linear model. Figure 1. Three MT system hypotheses with pairwise alignments. she bought the Jeep E she buys the SUV E she bought the SUV Jeep a) Confusion network with pair-wise alignment. she bought the E Jeep she buys the SUV E she bought the SUV Jeep b) Confusion network with incremental alignment. Figure 2. Correspondence sets of confusion networks unde</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2008</marker>
<rawString>Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz 2008. Incremental Hypothesis Alignment for Building Confusion Networks with Application to Machine Translation System Combination. In Proceedings of the 3rd ACL Workshop on SMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong Zhao</author>
<author>Xiaodong He</author>
</authors>
<title>Using N-gram based Features for Machine Translation System Combination.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="15120" citStr="Zhao and He (2009)" startWordPosition="2437" endWordPosition="2440"> selection subproblems. Features are defined as follows. Word posterior model: The word posterior feature is the same as the one proposed by Rosti et. al. (2007a). i.e., 𝑀 𝑓𝑤𝑝 𝑤, 𝑂, 𝐶, 𝑯 = 𝑙𝑜𝑔 𝑃 𝑤𝑚 𝐶𝑆𝑚 𝑚=1 where the posterior of a single word in a CS is computed based on a weighted voting score: 𝑃 𝑤𝑖,𝑙𝑖 𝐶𝑆 = 𝑃 𝑤𝑖,𝑙𝑖 𝐶𝑆 𝑙1, ...,𝑙𝑁 𝑁 = 𝑊(𝑘) 𝛿(𝑤𝑘,𝑙𝑘 = 𝑤𝑖,𝑙𝑖) 𝑘=1 and M is the number of CS generated. Note that M may be larger than the length of the output word sequence w since some CS may generate empty words. Bi-gram voting model: The second feature we used is a bi-gram voting feature proposed by Zhao and He (2009), i.e., for each bi-gram 𝑤𝑖, 𝑤𝑖+1 , a weighted position-independent voting score is computed: 𝑁 𝑃 𝑤𝑖,𝑤𝑖+1 𝑯 = 𝑊(𝑘) 𝛿( 𝑤𝑖,𝑤𝑖+1 ∈ 𝑖) 𝑘=1 And the global bi-gram voting feature is defined as: |𝑤|−1 𝑓𝑏𝑔𝑣 𝑤, 𝑂, 𝐶, 𝑯 = 𝑙𝑜𝑔 𝑃 𝑤𝑖, 𝑤𝑖+1 𝑯 𝑖=1 Distortion model: Unlike in the conventional CN-based system combination, flexible orders of CS are allowed in this joint decoding framework. In order to model the distortion of different orderings, a distortion model between two CS is defined as follows: First we define the distortion cost between two words at a single hypothesis. Similarly to the distortion pena</context>
</contexts>
<marker>Zhao, He, 2009</marker>
<rawString>Yong Zhao and Xiaodong He, 2009. Using N-gram based Features for Machine Translation System Combination. In Proceedings of NAACL-HLT</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>