<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.998684">
Using Smaller Constituents Rather Than Sentences
in Active Learning for Japanese Dependency Parsing
</title>
<author confidence="0.830362">
Manabu Sassano
</author>
<affiliation confidence="0.756778">
Yahoo Japan Corporation
</affiliation>
<address confidence="0.712552333333333">
Midtown Tower,
9-7-1 Akasaka, Minato-ku,
Tokyo 107-6211, Japan
</address>
<email confidence="0.912156">
msassano@yahoo-corp.jp
</email>
<sectionHeader confidence="0.992464" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998639375">
We investigate active learning methods for
Japanese dependency parsing. We propose
active learning methods of using partial
dependency relations in a given sentence
for parsing and evaluate their effective-
ness empirically. Furthermore, we utilize
syntactic constraints of Japanese to ob-
tain more labeled examples from precious
labeled ones that annotators give. Ex-
perimental results show that our proposed
methods improve considerably the learn-
ing curve of Japanese dependency parsing.
In order to achieve an accuracy of over
88.3%, one of our methods requires only
34.4% of labeled examples as compared to
passive learning.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999809952380952">
Reducing annotation cost is very important be-
cause supervised learning approaches, which have
been successful in natural language processing, re-
quire typically a large number of labeled exam-
ples. Preparing many labeled examples is time
consuming and labor intensive.
One of most promising approaches to this is-
sue is active learning. Recently much attention has
been paid to it in the field of natural language pro-
cessing. Various tasks have been targeted in the
research on active learning. They include word
sense disambiguation, e.g., (Zhu and Hovy, 2007),
POS tagging (Ringger et al., 2007), named entity
recognition (Laws and Sch¨utze, 2008), word seg-
mentation, e.g., (Sassano, 2002), and parsing, e.g.,
(Tang et al., 2002; Hwa, 2004).
It is the main purpose of this study to propose
methods of improving active learning for parsing
by using a smaller constituent than a sentence as
a unit that is selected at each iteration of active
learning. Typically in active learning for parsing a
</bodyText>
<note confidence="0.9621054">
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan
</note>
<email confidence="0.982078">
kuro@i.kyoto-u.ac.jp
</email>
<bodyText confidence="0.999717095238095">
sentence has been considered to be a basic unit for
selection. Small constituents such as chunks have
not been used in sample selection for parsing. We
use Japanese dependency parsing as a target task
in this study since a simple and efficient algorithm
of parsing is proposed and, to our knowledge, ac-
tive learning for Japanese dependency parsing has
never been studied.
The remainder of the paper is organized as fol-
lows. Section 2 describes the basic framework of
active learning which is employed in this research.
Section 3 describes the syntactic characteristics of
Japanese and the parsing algorithm that we use.
Section 4 briefly reviews previous work on active
learning for parsing and discusses several research
challenges. In Section 5 we describe our proposed
methods and others of active learning for Japanese
dependency parsing. Section 6 describes experi-
mental evaluation and discussion. Finally, in Sec-
tion 7 we conclude this paper and point out some
future directions.
</bodyText>
<sectionHeader confidence="0.998112" genericHeader="introduction">
2 Active Learning
</sectionHeader>
<subsectionHeader confidence="0.997787">
2.1 Pool-based Active Learning
</subsectionHeader>
<bodyText confidence="0.999684">
Our base framework of active learning is based on
the algorithm of (Lewis and Gale, 1994), which is
called pool-based active learning. Following their
sequential sampling algorithm, we show in Fig-
ure 1 the basic flow of pool-based active learning.
Various methods for selecting informative exam-
ples can be combined with this framework.
</bodyText>
<subsectionHeader confidence="0.974761">
2.2 Selection Algorithm for Large Margin
Classifiers
</subsectionHeader>
<bodyText confidence="0.998856333333333">
One of the most accurate approaches to classifica-
tion tasks is an approach with large margin classi-
fiers. Suppose that we are given data points {xil
such that the associated label yi will be either −1
or 1, and we have a hyperplane of some large mar-
gin classifier defined by {x : f(x) = 01 where the
</bodyText>
<page confidence="0.963605">
356
</page>
<note confidence="0.9368455">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 356–365,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.996519090909091">
1. Build an initial classifier from an initial la-
beled training set.
2. While resources for labeling examples are
available
(a) Apply the current classifier to each un-
labeled example
(b) Find the m examples which are most in-
formative for the classifier
(c) Have annotators label the m examples
(d) Train a new classifier on all labeled ex-
amples
</listItem>
<figureCaption confidence="0.995752">
Figure 1: Flow of the pool-based active learning
</figureCaption>
<bodyText confidence="0.97277875">
Lisa-ga kare-ni ano pen-wo age-ta.
Lisa-subj to him that pen-acc give-past.
ID 0 1 2 3 4
Head 4 4 3 4 -
</bodyText>
<figureCaption confidence="0.890947">
Figure 2: Sample sentence. An English translation
is “Lisa gave that pen to him.”
</figureCaption>
<bodyText confidence="0.97394075">
classification function is G(x) = sign{f(x)}. In
pool-based active learning with large margin clas-
sifiers, selection of examples can be done as fol-
lows:
</bodyText>
<listItem confidence="0.9996585">
1. Compute f(xi) over all unlabeled examples
xi in the pool.
2. Sort xi with |f(xi) |in ascending order.
3. Select top m examples.
</listItem>
<bodyText confidence="0.8876242">
This type of selection methods with SVMs is dis-
cussed in (Tong and Koller, 2000; Schohn and
Cohn, 2000). They obtain excellent results on text
classification. These selection methods are simple
but very effective.
</bodyText>
<sectionHeader confidence="0.962658" genericHeader="method">
3 Japanese Parsing
</sectionHeader>
<subsectionHeader confidence="0.995981">
3.1 Syntactic Units
</subsectionHeader>
<bodyText confidence="0.995724333333333">
A basic syntactic unit used in Japanese parsing is
a bunsetsu, the concept of which was initially in-
troduced by Hashimoto (1934). We assume that
in Japanese we have a sequence of bunsetsus be-
fore parsing a sentence. A bunsetsu contains one
or more content words and zero or more function
words.
A sample sentence in Japanese is shown in Fig-
ure 2. This sentence consists of five bunsetsus:
Lisa-ga, kare-ni, ano, pen-wo, and age-ta where
ga, ni, and wo are postpositions and ta is a verb
ending for past tense.
</bodyText>
<subsectionHeader confidence="0.999038">
3.2 Constraints of Japanese Dependency
Analysis
</subsectionHeader>
<bodyText confidence="0.999256">
Japanese is a head final language and in written
Japanese we usually hypothesize the following:
</bodyText>
<listItem confidence="0.9997378">
• Each bunsetsu has only one head except the
rightmost one.
• Dependency links between bunsetsus go
from left to right.
• Dependencies do not cross one another.
</listItem>
<bodyText confidence="0.9997856">
We can see that these constraints are satisfied in
the sample sentence in Figure 2. In this paper we
also assume that the above constraints hold true
when we discuss algorithms of Japanese parsing
and active learning for it.
</bodyText>
<subsectionHeader confidence="0.998223">
3.3 Algorithm of Japanese Dependency
Parsing
</subsectionHeader>
<bodyText confidence="0.999853736842105">
We use Sassano’s algorithm (Sassano, 2004) for
Japanese dependency parsing. The reason for this
is that it is very accurate and efficient1. Further-
more, it is easy to implement. His algorithm is
one of the simplest form of shift-reduce parsers
and runs in linear-time.2 Since Japanese is a head
final language and its dependencies are projective
as described in Section 3.2, that simplification can
be made.
The basic flow of Sassano’s algorithm is shown
in Figure 3, which is slightly simplified from the
original by Sassano (2004). When we use this al-
gorithm with a machine learning-based classifier,
function Dep() in Figure 3 uses the classifier to
decide whether two bunsetsus have a dependency
relation. In order to prepare training examples for
the trainable classifier used with his algorithm, we
first have to convert a treebank to suitable labeled
instances by using the algorithm in Figure 4. Note
</bodyText>
<footnote confidence="0.992801833333333">
1Iwatate et al. (2008) compare their proposed algorithm
with various ones that include Sassano’s, cascaded chunk-
ing (Kudo and Matsumoto, 2002), and one in (McDonald et
al., 2005). Kudo and Matsumoto (2002) compare cascaded
chunking with the CYK method (Kudo and Matsumoto,
2000). After considering these results, we have concluded
so far that Sassano’s is a reasonable choice for our purpose.
2Roughly speaking, Sassano’s is considered to be a sim-
plified version, which is modified for head final languages, of
Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required
to handle multiclass prediction, while binary classifiers can
work with Sassano’s for Japanese.
</footnote>
<page confidence="0.998056">
357
</page>
<bodyText confidence="0.902004">
Input: wi: bunsetsus in a given sentence.
N: the number of bunsetsus.
Output: hj: the head IDs of bunsetsus wj.
Functions: Push(i, s): pushes i on the stack s.
Pop(s): pops a value off the stack s.
Dep(j, i, w): returns true when wj should
modify wi. Otherwise returns false.
procedure Analyze(w, N, h)
var s: a stack for IDs of modifier bunsetsus
begin
</bodyText>
<equation confidence="0.921406466666667">
{−1 indicates no modifier candidate}
Push(−1, s);
Push(0, s);
for i ← 1 to N − 1 do begin
j ← Pop(s);
while (j =6 −1
and ((i = N − 1) or Dep(j, i, w)) ) do
begin
hj ← i;
j ← Pop(s)
end
Push(j, s);
Push(i, s)
end
end
</equation>
<figureCaption confidence="0.9797355">
Figure 3: Algorithm of Japanese dependency pars-
ing
</figureCaption>
<bodyText confidence="0.51922">
that the algorithm in Figure 4 does not generate
every pair of bunsetsus.3
</bodyText>
<sectionHeader confidence="0.976088" genericHeader="method">
4 Active Learning for Parsing
</sectionHeader>
<bodyText confidence="0.981760458333333">
Most of the methods of active learning for parsing
in previous work use selection of sentences that
seem to contribute to the improvement of accuracy
(Tang et al., 2002; Hwa, 2004; Baldridge and Os-
borne, 2004). Although Hwa suggests that sample
selection for parsing would be improved by select-
ing finer grained constituents rather than sentences
(Hwa, 2004), such methods have not been investi-
gated so far.
Typical methods of selecting sentences are
3We show a sample set of generated examples for training
the classifier of the parser in Figure 3. By using the algorithm
in Figure 4, we can obtain labeled examples from the sample
sentences in Figure 2: {0, 1, “O”}, {1, 2, “O”}, {2, 3, “D”},
and {1, 3, “O”}. Please see Section 5.2 for the notation
used here. For example, an actual labeled instance generated
from {2, 3, “D”} will be like ”label=D, features={modifier-
content-word=ano, ..., head-content-word=pen, ...}.”
Input: hi: the head IDs of bunsetsus wi.
Function: Dep(j, i, w, h): returns true if hj = i.
Otherwise returns false. Also prints a
feature vector with a label according to hj.
procedure Generate(w, N, h)
begin
</bodyText>
<equation confidence="0.99847475">
Push(−1, s);
Push(0, s);
for i ← 1 to N − 1 do begin
j ← Pop(s);
</equation>
<bodyText confidence="0.677990666666667">
while (j =6 −1
and ((i = N − 1) or Dep(j, i, w, h)) ) do
begin
</bodyText>
<equation confidence="0.918796833333333">
j ← Pop(s)
end
Push(j, s);
Push(i, s)
end
end
</equation>
<figureCaption confidence="0.974879">
Figure 4: Algorithm of generating training exam-
ples
</figureCaption>
<bodyText confidence="0.999958272727273">
based on some entropy-based measure of a given
sentence (e.g., (Tang et al., 2002)). We cannot
use this kind of measures when we want to select
other smaller constituents than sentences. Other
bigger problem is an algorithm of parsing itself.
If we sample smaller units rather than sentences,
we have partially annotated sentences and have to
use a parsing algorithm that can be trained from
incompletely annotated sentences. Therefore, it is
difficult to use some of probabilistic models for
parsing. 4
</bodyText>
<sectionHeader confidence="0.899294" genericHeader="method">
5 Active Learning for Japanese
</sectionHeader>
<subsectionHeader confidence="0.757855">
Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999952">
In this section we describe sample selection meth-
ods which we investigated.
</bodyText>
<subsectionHeader confidence="0.999043">
5.1 Sentence-wise Sample Selection
</subsectionHeader>
<bodyText confidence="0.9630506">
Passive Selection (Passive) This method is to
select sequentially sentences that appear in the
training corpus. Since it gets harder for the read-
ers to reproduce the same experimental setting, we
4We did not employ query-by-committee (QBC) (Seung
et al., 1992), which is another important general framework
of active learning, since the selection strategy with large mar-
gin classifiers (Section 2.2) is much simpler and seems more
practical for active learning in Japanese dependency parsing
with smaller constituents.
</bodyText>
<page confidence="0.99279">
358
</page>
<bodyText confidence="0.69226575">
avoid to use random sampling in this paper.
Minimum Margin Selection (Min) This
method is to select sentences that contain bun-
setsu pairs which have smaller margin values
of outputs of the classifier used in parsing. The
procedure of selection of MIN are summarized as
follows. Assume that we have sentences sz in the
pool of unlabeled sentences.
</bodyText>
<listItem confidence="0.999789285714286">
1. Parse sz in the pool with the current model.
2. Sort sz with min |f(xk) |where xk are bun-
setsu pairs in the sentence sz. Note that xk
are not all possible bunsetsu pairs in sz and
they are limited to bunsetsu pairs checked in
the process of parsing sz.
3. Select top m sentences.
</listItem>
<bodyText confidence="0.955580444444445">
Averaged Margin Selection (Avg) This method
is to select sentences that have smaller values of
averaged margin values of outputs of the classi-
fier in a give sentences over the number of deci-
sions which are carried out in parsing. The differ-
ence between AVG and MIN is that for AVG we
use ∑ |f(xk)|/l where l is the number of calling
Dep() in Figure 3 for the sentence sz instead of
min |f(xk) |for MIN.
</bodyText>
<subsectionHeader confidence="0.999712">
5.2 Chunk-wise Sample Selection
</subsectionHeader>
<bodyText confidence="0.99727061971831">
In chunk-wise sample selection, we select bun-
setsu pairs rather than sentences. Bunsetsu pairs
are selected from different sentences in a pool.
This means that structures of sentences in the pool
are partially annotated.
Note that we do not use every bunsetsu pair in
a sentence. When we use Sassano’s algorithm, we
have to generate training examples for the classi-
fier by using the algorithm in Figure 4. In other
words, we should not sample bunsetsu pairs inde-
pendently from a given sentence.
Therefore, we select bunsetsu pairs that have
smaller margin values of outputs given by the clas-
sifier during the parsing process. All the sentences
in the pool are processed by the current parser. We
cannot simply split the sentences in the pool into
labeled and unlabeled ones because we do not se-
lect every bunsetsu pair in a given sentence.
Naive Selection (Naive) This method is to select
bunsetsu pairs that have smaller margin values of
outputs of the classifier. Then it is assumed that
annotators would label either “D” for the two bun-
setsu having a dependency relation or “O”, which
represents the two does not.
Modified Simple Selection (ModSimple) Al-
though NAIVE seems to work well, it did not (dis-
cussed later). MODSIMPLE is to select bunsetsu
pairs that have smaller margin values of outputs
of the classifier, which is the same as in NAIVE.
The difference between MODSIMPLE and NAIVE
is the way annotators label examples. Assume that
we have an annotator and the learner selects some
bunsetsu pair of the j-th bunsetsu and the i-th bun-
setsu such that j &lt; i. The annotator is then asked
what the head of the j-th bunsetsu is. We define
here the head bunsetsu is the k-th one.
We differently generate labeled examples from
the information annotators give according to the
relation among bunsetsus j, i, and k.
Below we use the notation {s, t, “D”} to de-
note that the s-th bunsetsu modifies the t-th one.
The use of “O” instead of “D” indicates that the
s-th does not modify the t-th. That is generating
{s, t, “D”} means outputting an example with the
label “D”.
Case 1 if j &lt; i &lt; k, then generate {j, i, “O”} and
{j, k, “D”}.
Case 2 if j &lt; i = k, then generate {j, k, “D”}.
Case 3 if j &lt; k &lt; i, then generate {j, k, “D”}.
Note that we do not generate {j, i, “O”} in
this case because in Sassano’s algorithm we
do not need such labeled examples if j de-
pends on k such that k &lt; i.
Syntactically Extended Selection (Syn) This
selection method is one based on MODSIMPLE
and extended to generate more labeled examples
for the classifier. You may notice that more labeled
examples for the classifier can be generated from
a single label which the annotator gives. Syntac-
tic constraints of the Japanese language allow us
to extend labeled examples.
For example, suppose that we have four bunset-
sus A, B, C, and D in this order. If A depends
on C, i.e., the head of A is C, then it is automati-
cally derived that B also should depend on C be-
cause the Japanese language has the no-crossing
constraint for dependencies (Section 3.2). By uti-
lizing this property we can obtain more labeled ex-
amples from a single labeled one annotators give.
In the example above, we obtain {A, B, “O”} and
{B, C, “D”} from {A, C, “D”}.
</bodyText>
<page confidence="0.995811">
359
</page>
<bodyText confidence="0.99531225">
Although we can employ various extensions to
MODSIMPLE, we use a rather simple extension in
this research.
the same features here. They are divided into three
groups: modifier bunsetsu features, head bunsetsu
features, and gap features. A summary of the fea-
tures is described in Table 1.
Case 1 if (j &lt; i &lt; k), then generate
</bodyText>
<listItem confidence="0.923098888888889">
• {j, i, “O”},
• {k − 1,k,“D”} if k − 1 &gt; j,
• and {j, k, “D”}.
Case 2 if (j &lt; i = k), then generate
• {k − 1,k,“D”} if k − 1 &gt; j,
• and {j, k, “D”}.
Case 3 if (j &lt; k &lt; i), then generate
• {k − 1,k,“D”} if k − 1 &gt; j,
• and {j, k, “D”}.
</listItem>
<bodyText confidence="0.991889">
In SYN as well as MODSIMPLE, we generate
examples with ”O” only for bunsetsu pairs that oc-
cur to the left of the correct head (i.e., case 1).
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="method">
6 Experimental Evaluation and
Discussion
</sectionHeader>
<subsectionHeader confidence="0.998349">
6.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9999925">
In our experiments we used the Kyoto University
Corpus Version 2 (Kurohashi and Nagao, 1998).
Initial seed sentences and a pool of unlabeled sen-
tences for training are taken from the articles on
January 1st through 8th (7,958 sentences) and the
test data is a set of sentences in the articles on Jan-
uary 9th (1,246 sentences). The articles on Jan-
uary 10th were used for development. The split of
these articles for training/test/development is the
same as in (Uchimoto et al., 1999).
</bodyText>
<subsectionHeader confidence="0.998959">
6.2 Averaged Perceptron
</subsectionHeader>
<bodyText confidence="0.999970666666667">
We used the averaged perceptron (AP) (Freund
and Schapire, 1999) with polynomial kernels. We
set the degree of the kernels to 3 since cubic ker-
nels with SVM have proved effective for Japanese
dependency parsing (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2002). We found the best
value of the epoch T of the averaged perceptron
by using the development set. We fixed T = 12
through all experiments for simplicity.
</bodyText>
<subsectionHeader confidence="0.872449">
6.3 Features
</subsectionHeader>
<bodyText confidence="0.9997055">
There are features that have been commonly used
for Japanese dependency parsing among related
papers, e.g., (Kudo and Matsumoto, 2002; Sas-
sano, 2004; Iwatate et al., 2008). We also used
</bodyText>
<subsectionHeader confidence="0.964529">
6.4 Implementation
</subsectionHeader>
<bodyText confidence="0.99998175">
We implemented a parser and a tool for the av-
eraged perceptron in C++ and used them for ex-
periments. We wrote the main program of active
learning and some additional scripts in Perl and sh.
</bodyText>
<subsectionHeader confidence="0.999261">
6.5 Settings of Active Learning
</subsectionHeader>
<bodyText confidence="0.999988111111111">
For initial seed sentences, first 500 sentences are
taken from the articles on January 1st. In ex-
periments about sentence wise selection, 500 sen-
tences are selected at each iteration of active learn-
ing and labeled5 and added into the training data.
In experiments about chunk wise selection 4000
pairs of bunsetsus, which are roughly equal to the
averaged number of bunsetsus in 500 sentences,
are selected at each iteration of active learning.
</bodyText>
<subsectionHeader confidence="0.992796">
6.6 Dependency Accuracy
</subsectionHeader>
<bodyText confidence="0.9998084">
We use dependency accuracy as a performance
measure of a parser. The dependency accuracy is
the percentage of correct dependencies. This mea-
sure is commonly used for the Kyoto University
Corpus.
</bodyText>
<subsectionHeader confidence="0.686642">
6.7 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999624333333333">
Learning Curves First we compare methods for
sentence wise selection. Figure 5 shows that MIN
is the best among them, while AVG is not good
and similar to PASSIVE. It is observed that active
learning with large margin classifiers also works
well for Sassano’s algorithm of Japanese depen-
dency parsing.
Next we compare chunk-wise selection with
sentence-wise one. The comparison is shown in
Figure 6. Note that we must carefully consider
how to count labeled examples. In sentence wise
selection we obviously count the number of sen-
tences. However, it is impossible to count such
number when we label bunsetsus pairs.
Therefore, we use the number of bunsetsus that
have an annotated head. Although we know this
may not be a completely fair comparison, we be-
lieve our choice in this experiment is reasonable
</bodyText>
<footnote confidence="0.989022666666667">
5In our experiments human annotators do not give labels.
Instead, labels are given virtually from correct ones that the
Kyoto University Corpus has.
</footnote>
<page confidence="0.998062">
360
</page>
<bodyText confidence="0.9934355625">
Bunsetsu features for modifiers
and heads
rightmost content word, rightmost function word, punctuation,
parentheses, location (BOS or EOS)
Gap features
distance (1, 2–5, or 6 &lt;), particles, parentheses, punctuation
Table 1: Features for deciding a dependency relation between two bunsetsus. Morphological features
for each word (morpheme) are major part-of-speech (POS), minor POS, conjugation type, conjugation
form, and surface form.
for assessing the effect of reduction by chunk-wise
selection.
In Figure 6 NAIVE has a better learning curve
compared to MIN at the early stage of learning.
However, the curve of NAIVE declines at the later
stage and gets worse than PASSIVE and MIN.
Why does this phenomenon occur? It is because
each bunsetsu pair is not independent and pairs in
the same sentence are related to each other. They
satisfy the constraints discussed in Section 3.2.
Furthermore, the algorithm we use, i.e., Sassano’s,
assumes these constraints and has the specific or-
der for processing bunsetsu pairs as we see in Fig-
ure 3. Let us consider the meaning of {j, i, “O”} if
the head of the j-th bunsetsu is the k-th one such
that j &lt; k &lt; i. In the context of the algorithm in
Figure 3, {j, i, “O”} actually means that the j-th
bunsetsu modifies th l-th one such that i &lt; l. That
is “O” does not simply mean that two bunsetsus
does not have a dependency relation. Therefore,
we should not generate {j, i, “O”} in the case of
j &lt; k &lt; i. Such labeled instances are not needed
and the algorithm in Figure 4 does not generate
them even if a fully annotated sentence is given.
Based on the analysis above, we modified NAIVE
and defined MODSIMPLE, where unnecessary la-
beled examples are not generated.
Now let us compare NAIVE with MODSIMPLE
(Figure 7). MODSIMPLE is almost always better
than PASSIVE and does not cause a significant de-
terioration of accuracy unlike NAIVE.6
Comparison of MODSIMPLE and SYN is shown
in Figure 8. Both exhibit a similar curve. Figure 9
shows the same comparison in terms of required
queries to human annotators. It shows that SYN is
better than MODSIMPLE especially at the earlier
stage of active learning.
Reduction of Annotations Next we examined
the number of labeled bunsetsus to be required in
</bodyText>
<footnote confidence="0.588323">
6We have to carefully see the curves of NAIVE and MOD-
SIMPLE. In Figure 7 at the early stage NAIVE is slightly
better than MODSIMPLE, while in Figure 9 NAIVE does not
outperform MODSIMPLE. This is due to the difference of the
way of accessing annotation efforts.
</footnote>
<figure confidence="0.997511">
0 1000 2000 3000 4000 5000 6000 7000 8000
Number of Labeled Sentences
0 10000 20000 30000 40000 50000
Number of bunsetsus which have a head
</figure>
<figureCaption confidence="0.9932605">
Figure 6: Learning curves of MIN (sentence-wise)
and NAIVE (chunk-wise).
</figureCaption>
<figure confidence="0.999796166666667">
Accuracy
0.885
0.875
0.865
0.855
0.89
0.88
0.87
0.86
Passive
Min
Average
</figure>
<figureCaption confidence="0.7883465">
Figure 5: Learning curves of methods for sentence
wise selection
</figureCaption>
<figure confidence="0.991719292682927">
0.89
0.885
0.88
0.875
0.87
0.865
0.86
0.855
Accuracy
Passive
Min
Naive
361
Syn
Passive Min Avg Naive Mod
Simple
0.885
0.875
0.865
0.855
0.89
0.88
0.87
0.86
Passive
ModSimple
Naive
0 10000 20000 30000 40000 50000
Number of bunsetsus which have a head
Selection strategy
Accuracy
40000
35000
30000
25000
20000
15000
10000
5000
0
# of bunsetsus that have a head
</figure>
<figureCaption confidence="0.9984198">
Figure 7: Learning curves of NAIVE, MODSIM-
PLE and PASSIVE in terms of the number of bun-
setsus that have a head.
Figure 10: Number of labeled bunsetsus to be re-
quired to achieve an accuracy of over 88.3%.
</figureCaption>
<figure confidence="0.9891935">
0 10000 20000 30000 40000 50000
Number of bunsetsus which have a head
</figure>
<figureCaption confidence="0.986833333333333">
Figure 8: Learning curves of MODSIMPLE and
SYN in terms of the number of bunsetsus which
have a head.
</figureCaption>
<figure confidence="0.994894">
0 1000 2000 3000 4000 5000 6000 7000 8000
Number of Labeled Sentences
</figure>
<figureCaption confidence="0.9984685">
Figure 11: Changes of number of support vectors
in sentence-wise active learning
</figureCaption>
<figure confidence="0.998465130434783">
Accuracy
0.885
0.875
0.865
0.855
0.89
0.88
0.87
0.86
Passive
ModSimple
Syntax
Number of Support Vectors
25000
20000
15000
10000
5000
0
Passive
Min
0 10000 20000 30000 40000 50000 60000
Number of queris to human annotators
</figure>
<figureCaption confidence="0.980272666666667">
Figure 9: Comparison of MODSIMPLE and SYN
in terms of the number of queries to human anno-
tators
</figureCaption>
<figure confidence="0.974444">
0 10000 20000 30000 40000 50000 60000
Number of Queries
</figure>
<figureCaption confidence="0.997125">
Figure 12: Changes of number of support vectors
in chunk-wise active learning (MODSIMPLE)
</figureCaption>
<figure confidence="0.99942045">
Accuracy
0.885
0.875
0.865
0.855
0.89
0.88
0.87
0.86
ModSimple
Syntax
Naive
Number of Support Vectors
25000
20000
15000
10000
5000
0
ModSimple
</figure>
<page confidence="0.995012">
362
</page>
<bodyText confidence="0.999623837209302">
order to achieve a certain level of accuracy. Fig-
ure 10 shows that the number of labeled bunsetsus
to achieve an accuracy of over 88.3% depending
on the active learning methods discussed in this
research.
PASSIVE needs 37766 labeled bunsetsus which
have a head to achieve an accuracy of 88.48%,
while SYN needs 13021 labeled bunsetsus to
achieve an accuracy of 88.56%. SYN requires only
34.4% of the labeled bunsetsu pairs that PASSIVE
requires.
Stopping Criteria It is known that increment
rate of the number of support vectors in SVM in-
dicates saturation of accuracy improvement dur-
ing iterations of active learning (Schohn and Cohn,
2000). It is interesting to examine whether the
observation for SVM is also useful for support
vectors7 of the averaged perceptron. We plotted
changes of the number of support vectors in the
cases of both PASSIVE and MIN in Figure 11 and
changes of the number of support vectors in the
case of MOnSIMPLE in Figure 12. We observed
that the increment rate of support vectors mildly
gets smaller. However, it is not so clear as in the
case of text classification in (Schohn and Cohn,
2000).
Issues on Accessing the Total Cost of Annota-
tion In this paper, we assume that each annota-
tion cost for dependency relations is constant. It
is however not true in an actual annotation work.8
In addition, we have to note that it may be easier
to annotate a whole sentence than some bunsetsu
pairs in a sentence9. In a real annotation task, it
will be better to show a whole sentence to anno-
tators even when annotating some part of the sen-
tence.
Nevertheless, it is noteworthy that our research
shows the minimum number of annotations in
preparing training examples for Japanese depen-
dency parsing. The methods we have proposed
must be helpful when checking repeatedly anno-
tations that are important and might be wrong or
difficult to label while building an annotated cor-
</bodyText>
<footnote confidence="0.9817505">
7Following (Freund and Schapire, 1999), we use the term
“support vectors” for AP as well as SVM. “Support vectors”
of AP means vectors which are selected in the training phase
and contribute to the prediction.
8Thus it is very important to construct models for estimat-
ing the actual annotation cost as Haertel et al. (2008) do.
9Hwa (2004) discusses similar aspects of researches on
active learning.
</footnote>
<bodyText confidence="0.997829275">
pus. They also will be useful for domain adapta-
tion of a dependency parser.10
Applicability to Other Languages and Other
Parsing Algorithms We discuss here whether
or not the proposed methods and the experiments
are useful for other languages and other parsing
algorithms. First we take languages similar to
Japanese in terms of syntax, i.e., Korean and Mon-
golian. These two languages are basically head-
final languages and have similar constraints in
Section 3.2. Although no one has reported appli-
cation of (Sassano, 2004) to the languages so far,
we believe that similar parsing algorithms will be
applicable to them and the discussion in this study
would be useful.
On the other hand, the algorithm of (Sassano,
2004) cannot be applied to head-initial languages
such as English. If target languages are assumed
to be projective, the algorithm of (Nivre, 2003)
can be used. It is highly likely that we will invent
the effective use of finer-grained constituents, e.g.,
head-modifier pairs, rather than sentences in active
learning for Nivre’s algorithm with large margin
classifiers since Sassano’s seems to be a simplified
version of Nivre’s and they have several properties
in common. However, syntactic constraints in Eu-
ropean languages like English may be less helpful
than those in Japanese because their dependency
links do not have a single direction.
Even though the use of syntactic constraints is
limited, smaller constituents will still be useful for
other parsing algorithms that use some determin-
istic methods with machine learning-based classi-
fiers. There are many algorithms that have such
a framework, which include (Yamada and Mat-
sumoto, 2003) for English and (Kudo and Mat-
sumoto, 2002; Iwatate et al., 2008) for Japanese.
Therefore, effective use of smaller constituents in
active learning would not be limited to the specific
algorithm.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999850833333333">
We have investigated that active learning methods
for Japanese dependency parsing. It is observed
that active learning of parsing with the averaged
perceptron, which is one of the large margin clas-
sifiers, works also well for Japanese dependency
analysis.
</bodyText>
<footnote confidence="0.5849845">
10Ohtake (2006) examines heuristic methods of selecting
sentences.
</footnote>
<page confidence="0.998178">
363
</page>
<bodyText confidence="0.999861869565217">
In addition, as far as we know, we are the first
to propose the active learning methods of using
partial dependency relations in a given sentence
for parsing and we have evaluated the effective-
ness of our methods. Furthermore, we have tried
to obtain more labeled examples from precious la-
beled ones that annotators give by utilizing syntac-
tic constraints of the Japanese language. It is note-
worthy that linguistic constraints have been shown
useful for reducing annotations in active learning
for NLP.
Experimental results show that our proposed
methods have improved considerably the learning
curve of Japanese dependency parsing.
We are currently building a new annotated cor-
pus with an annotation tool. We have a plan to in-
corporate our proposed methods to the annotation
tool. We will use it to accelerate building of the
large annotated corpus to improved our Japanese
parser.
It would be interesting to explore the use of par-
tially labeled constituents in a sentence in another
language, e.g., English, for active learning.
</bodyText>
<sectionHeader confidence="0.996544" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999863666666667">
We would like to thank the anonymous review-
ers and Tomohide Shibata for their valuable com-
ments.
</bodyText>
<sectionHeader confidence="0.994694" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987632271428572">
Jason Baldridge and Miles Osborne. 2004. Active
learning and the total cost of annotation. In Proc.
of EMNLP 2004, pages 9–16.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277–296.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
costs of sampling methods in active learning for an-
notation. In Proc. of ACL-08: HLT, short papers
(Companion Volume), pages 65–68.
Shinkichi Hashimoto. 1934. Essentials of Japanese
Grammar (Kokugoho Yousetsu) (in Japanese).
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253–276.
Masakazu Iwatate, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese dependency parsing us-
ing a tournament model. In Proc. of COLING 2008,
pages 361–368.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proc. ofEMNLP/VLC 2000, pages 18–
25.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of CoNLL-2002, pages 63–69.
Sadao Kurohashi and Makoto Nagao. 1998. Building a
Japanese parsed corpus while improving the parsing
system. In Proc. ofLREC-1998, pages 719–724.
Florian Laws and Hinrich Sch¨utze. 2008. Stopping cri-
teria for active learning of named entity recognition.
In Proc. of COLING 2008, pages 465–472.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
Proc. of the Seventeenth Annual International ACM-
SIGIR Conference on Research and Development in
Information Retrieval, pages 3–12.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. of ACL-2005, pages
523–530.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proc. of IWPT-03,
pages 149–160.
Kiyonori Ohtake. 2006. Analysis of selective strate-
gies to build a dependency-analyzed corpus. In
Proc. of COLING/ACL 2006 Main Conf. Poster Ses-
sions, pages 635–642.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learn-
ing for part-of-speech tagging: Accelerating corpus
annotation. In Proc. of the Linguistic Annotation
Workshop, pages 101–108.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for Japanese
word segmentation. In Proc. of ACL-2002, pages
505–512.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proc. of COLING 2004, pages
8–14.
Greg Schohn and David Cohn. 2000. Less is more:
Active learning with support vector machines. In
Proc. ofICML-2000, pages 839–846.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proc. of COLT ’92, pages
287–294.
Min Tang, Xaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proc. ofACL-2002, pages 120–127.
</reference>
<page confidence="0.98757">
364
</page>
<reference confidence="0.998673533333333">
Simon Tong and Daphne Koller. 2000. Support vec-
tor machine active learning with applications to text
classification. In Proc. of ICML-2000, pages 999–
1006.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese dependency structure analy-
sis based on maximum entropy models. In Proc. of
EACL-99, pages 196–203.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. ofIWPT 2003, pages 195–206.
Jingbo Zhu and Eduard Hovy. 2007. Active learning
for word sense disambiguation with methods for ad-
dressing the class imbalance problem. In Proc. of
EMNLP-CoNLL 2007, pages 783–790.
</reference>
<page confidence="0.999092">
365
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.596677">
<title confidence="0.9992685">Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</title>
<author confidence="0.943473">Manabu Sassano</author>
<affiliation confidence="0.805456">Yahoo Japan Corporation Midtown Tower,</affiliation>
<address confidence="0.954629">9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan</address>
<email confidence="0.979088">msassano@yahoo-corp.jp</email>
<abstract confidence="0.998828">We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Miles Osborne</author>
</authors>
<title>Active learning and the total cost of annotation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>9--16</pages>
<contexts>
<context position="8636" citStr="Baldridge and Osborne, 2004" startWordPosition="1406" endWordPosition="1410">s: a stack for IDs of modifier bunsetsus begin {−1 indicates no modifier candidate} Push(−1, s); Push(0, s); for i ← 1 to N − 1 do begin j ← Pop(s); while (j =6 −1 and ((i = N − 1) or Dep(j, i, w)) ) do begin hj ← i; j ← Pop(s) end Push(j, s); Push(i, s) end end Figure 3: Algorithm of Japanese dependency parsing that the algorithm in Figure 4 does not generate every pair of bunsetsus.3 4 Active Learning for Parsing Most of the methods of active learning for parsing in previous work use selection of sentences that seem to contribute to the improvement of accuracy (Tang et al., 2002; Hwa, 2004; Baldridge and Osborne, 2004). Although Hwa suggests that sample selection for parsing would be improved by selecting finer grained constituents rather than sentences (Hwa, 2004), such methods have not been investigated so far. Typical methods of selecting sentences are 3We show a sample set of generated examples for training the classifier of the parser in Figure 3. By using the algorithm in Figure 4, we can obtain labeled examples from the sample sentences in Figure 2: {0, 1, “O”}, {1, 2, “O”}, {2, 3, “D”}, and {1, 3, “O”}. Please see Section 5.2 for the notation used here. For example, an actual labeled instance genera</context>
</contexts>
<marker>Baldridge, Osborne, 2004</marker>
<rawString>Jason Baldridge and Miles Osborne. 2004. Active learning and the total cost of annotation. In Proc. of EMNLP 2004, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="16587" citStr="Freund and Schapire, 1999" startWordPosition="2834" endWordPosition="2837">. 6 Experimental Evaluation and Discussion 6.1 Corpus In our experiments we used the Kyoto University Corpus Version 2 (Kurohashi and Nagao, 1998). Initial seed sentences and a pool of unlabeled sentences for training are taken from the articles on January 1st through 8th (7,958 sentences) and the test data is a set of sentences in the articles on January 9th (1,246 sentences). The articles on January 10th were used for development. The split of these articles for training/test/development is the same as in (Uchimoto et al., 1999). 6.2 Averaged Perceptron We used the averaged perceptron (AP) (Freund and Schapire, 1999) with polynomial kernels. We set the degree of the kernels to 3 since cubic kernels with SVM have proved effective for Japanese dependency parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). We found the best value of the epoch T of the averaged perceptron by using the development set. We fixed T = 12 through all experiments for simplicity. 6.3 Features There are features that have been commonly used for Japanese dependency parsing among related papers, e.g., (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008). We also used 6.4 Implementation We implemented a parser and a</context>
<context position="25310" citStr="Freund and Schapire, 1999" startWordPosition="4317" endWordPosition="4320">n work.8 In addition, we have to note that it may be easier to annotate a whole sentence than some bunsetsu pairs in a sentence9. In a real annotation task, it will be better to show a whole sentence to annotators even when annotating some part of the sentence. Nevertheless, it is noteworthy that our research shows the minimum number of annotations in preparing training examples for Japanese dependency parsing. The methods we have proposed must be helpful when checking repeatedly annotations that are important and might be wrong or difficult to label while building an annotated cor7Following (Freund and Schapire, 1999), we use the term “support vectors” for AP as well as SVM. “Support vectors” of AP means vectors which are selected in the training phase and contribute to the prediction. 8Thus it is very important to construct models for estimating the actual annotation cost as Haertel et al. (2008) do. 9Hwa (2004) discusses similar aspects of researches on active learning. pus. They also will be useful for domain adaptation of a dependency parser.10 Applicability to Other Languages and Other Parsing Algorithms We discuss here whether or not the proposed methods and the experiments are useful for other langu</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbie Haertel</author>
<author>Eric Ringger</author>
<author>Kevin Seppi</author>
<author>James Carroll</author>
<author>Peter McClanahan</author>
</authors>
<title>Assessing the costs of sampling methods in active learning for annotation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT, short papers (Companion Volume),</booktitle>
<pages>65--68</pages>
<contexts>
<context position="25595" citStr="Haertel et al. (2008)" startWordPosition="4367" endWordPosition="4370">hy that our research shows the minimum number of annotations in preparing training examples for Japanese dependency parsing. The methods we have proposed must be helpful when checking repeatedly annotations that are important and might be wrong or difficult to label while building an annotated cor7Following (Freund and Schapire, 1999), we use the term “support vectors” for AP as well as SVM. “Support vectors” of AP means vectors which are selected in the training phase and contribute to the prediction. 8Thus it is very important to construct models for estimating the actual annotation cost as Haertel et al. (2008) do. 9Hwa (2004) discusses similar aspects of researches on active learning. pus. They also will be useful for domain adaptation of a dependency parser.10 Applicability to Other Languages and Other Parsing Algorithms We discuss here whether or not the proposed methods and the experiments are useful for other languages and other parsing algorithms. First we take languages similar to Japanese in terms of syntax, i.e., Korean and Mongolian. These two languages are basically headfinal languages and have similar constraints in Section 3.2. Although no one has reported application of (Sassano, 2004)</context>
</contexts>
<marker>Haertel, Ringger, Seppi, Carroll, McClanahan, 2008</marker>
<rawString>Robbie Haertel, Eric Ringger, Kevin Seppi, James Carroll, and Peter McClanahan. 2008. Assessing the costs of sampling methods in active learning for annotation. In Proc. of ACL-08: HLT, short papers (Companion Volume), pages 65–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinkichi Hashimoto</author>
</authors>
<date>1934</date>
<journal>Essentials of Japanese Grammar (Kokugoho Yousetsu) (in Japanese).</journal>
<contexts>
<context position="5163" citStr="Hashimoto (1934)" startWordPosition="818" endWordPosition="819">pool-based active learning with large margin classifiers, selection of examples can be done as follows: 1. Compute f(xi) over all unlabeled examples xi in the pool. 2. Sort xi with |f(xi) |in ascending order. 3. Select top m examples. This type of selection methods with SVMs is discussed in (Tong and Koller, 2000; Schohn and Cohn, 2000). They obtain excellent results on text classification. These selection methods are simple but very effective. 3 Japanese Parsing 3.1 Syntactic Units A basic syntactic unit used in Japanese parsing is a bunsetsu, the concept of which was initially introduced by Hashimoto (1934). We assume that in Japanese we have a sequence of bunsetsus before parsing a sentence. A bunsetsu contains one or more content words and zero or more function words. A sample sentence in Japanese is shown in Figure 2. This sentence consists of five bunsetsus: Lisa-ga, kare-ni, ano, pen-wo, and age-ta where ga, ni, and wo are postpositions and ta is a verb ending for past tense. 3.2 Constraints of Japanese Dependency Analysis Japanese is a head final language and in written Japanese we usually hypothesize the following: • Each bunsetsu has only one head except the rightmost one. • Dependency l</context>
</contexts>
<marker>Hashimoto, 1934</marker>
<rawString>Shinkichi Hashimoto. 1934. Essentials of Japanese Grammar (Kokugoho Yousetsu) (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="1613" citStr="Hwa, 2004" startWordPosition="237" endWordPosition="238"> processing, require typically a large number of labeled examples. Preparing many labeled examples is time consuming and labor intensive. One of most promising approaches to this issue is active learning. Recently much attention has been paid to it in the field of natural language processing. Various tasks have been targeted in the research on active learning. They include word sense disambiguation, e.g., (Zhu and Hovy, 2007), POS tagging (Ringger et al., 2007), named entity recognition (Laws and Sch¨utze, 2008), word segmentation, e.g., (Sassano, 2002), and parsing, e.g., (Tang et al., 2002; Hwa, 2004). It is the main purpose of this study to propose methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. Typically in active learning for parsing a Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan kuro@i.kyoto-u.ac.jp sentence has been considered to be a basic unit for selection. Small constituents such as chunks have not been used in sample selection for parsing. We use Japanese dependency parsing as a target task in this s</context>
<context position="8606" citStr="Hwa, 2004" startWordPosition="1404" endWordPosition="1405"> N, h) var s: a stack for IDs of modifier bunsetsus begin {−1 indicates no modifier candidate} Push(−1, s); Push(0, s); for i ← 1 to N − 1 do begin j ← Pop(s); while (j =6 −1 and ((i = N − 1) or Dep(j, i, w)) ) do begin hj ← i; j ← Pop(s) end Push(j, s); Push(i, s) end end Figure 3: Algorithm of Japanese dependency parsing that the algorithm in Figure 4 does not generate every pair of bunsetsus.3 4 Active Learning for Parsing Most of the methods of active learning for parsing in previous work use selection of sentences that seem to contribute to the improvement of accuracy (Tang et al., 2002; Hwa, 2004; Baldridge and Osborne, 2004). Although Hwa suggests that sample selection for parsing would be improved by selecting finer grained constituents rather than sentences (Hwa, 2004), such methods have not been investigated so far. Typical methods of selecting sentences are 3We show a sample set of generated examples for training the classifier of the parser in Figure 3. By using the algorithm in Figure 4, we can obtain labeled examples from the sample sentences in Figure 2: {0, 1, “O”}, {1, 2, “O”}, {2, 3, “D”}, and {1, 3, “O”}. Please see Section 5.2 for the notation used here. For example, an </context>
<context position="25611" citStr="Hwa (2004)" startWordPosition="4372" endWordPosition="4373">the minimum number of annotations in preparing training examples for Japanese dependency parsing. The methods we have proposed must be helpful when checking repeatedly annotations that are important and might be wrong or difficult to label while building an annotated cor7Following (Freund and Schapire, 1999), we use the term “support vectors” for AP as well as SVM. “Support vectors” of AP means vectors which are selected in the training phase and contribute to the prediction. 8Thus it is very important to construct models for estimating the actual annotation cost as Haertel et al. (2008) do. 9Hwa (2004) discusses similar aspects of researches on active learning. pus. They also will be useful for domain adaptation of a dependency parser.10 Applicability to Other Languages and Other Parsing Algorithms We discuss here whether or not the proposed methods and the experiments are useful for other languages and other parsing algorithms. First we take languages similar to Japanese in terms of syntax, i.e., Korean and Mongolian. These two languages are basically headfinal languages and have similar constraints in Section 3.2. Although no one has reported application of (Sassano, 2004) to the language</context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>Rebecca Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masakazu Iwatate</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency parsing using a tournament model.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>361--368</pages>
<contexts>
<context position="7051" citStr="Iwatate et al. (2008)" startWordPosition="1131" endWordPosition="1134">and its dependencies are projective as described in Section 3.2, that simplification can be made. The basic flow of Sassano’s algorithm is shown in Figure 3, which is slightly simplified from the original by Sassano (2004). When we use this algorithm with a machine learning-based classifier, function Dep() in Figure 3 uses the classifier to decide whether two bunsetsus have a dependency relation. In order to prepare training examples for the trainable classifier used with his algorithm, we first have to convert a treebank to suitable labeled instances by using the algorithm in Figure 4. Note 1Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary clas</context>
<context position="17124" citStr="Iwatate et al., 2008" startWordPosition="2924" endWordPosition="2927">Averaged Perceptron We used the averaged perceptron (AP) (Freund and Schapire, 1999) with polynomial kernels. We set the degree of the kernels to 3 since cubic kernels with SVM have proved effective for Japanese dependency parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). We found the best value of the epoch T of the averaged perceptron by using the development set. We fixed T = 12 through all experiments for simplicity. 6.3 Features There are features that have been commonly used for Japanese dependency parsing among related papers, e.g., (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008). We also used 6.4 Implementation We implemented a parser and a tool for the averaged perceptron in C++ and used them for experiments. We wrote the main program of active learning and some additional scripts in Perl and sh. 6.5 Settings of Active Learning For initial seed sentences, first 500 sentences are taken from the articles on January 1st. In experiments about sentence wise selection, 500 sentences are selected at each iteration of active learning and labeled5 and added into the training data. In experiments about chunk wise selection 4000 pairs of bunsetsus, which are roughly equal to t</context>
<context position="27399" citStr="Iwatate et al., 2008" startWordPosition="4653" endWordPosition="4656"> to be a simplified version of Nivre’s and they have several properties in common. However, syntactic constraints in European languages like English may be less helpful than those in Japanese because their dependency links do not have a single direction. Even though the use of syntactic constraints is limited, smaller constituents will still be useful for other parsing algorithms that use some deterministic methods with machine learning-based classifiers. There are many algorithms that have such a framework, which include (Yamada and Matsumoto, 2003) for English and (Kudo and Matsumoto, 2002; Iwatate et al., 2008) for Japanese. Therefore, effective use of smaller constituents in active learning would not be limited to the specific algorithm. 7 Conclusion We have investigated that active learning methods for Japanese dependency parsing. It is observed that active learning of parsing with the averaged perceptron, which is one of the large margin classifiers, works also well for Japanese dependency analysis. 10Ohtake (2006) examines heuristic methods of selecting sentences. 363 In addition, as far as we know, we are the first to propose the active learning methods of using partial dependency relations in </context>
</contexts>
<marker>Iwatate, Asahara, Matsumoto, 2008</marker>
<rawString>Masakazu Iwatate, Masayuki Asahara, and Yuji Matsumoto. 2008. Japanese dependency parsing using a tournament model. In Proc. of COLING 2008, pages 361–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on support vector machines.</title>
<date>2000</date>
<booktitle>In Proc. ofEMNLP/VLC</booktitle>
<pages>18--25</pages>
<contexts>
<context position="7307" citStr="Kudo and Matsumoto, 2000" startWordPosition="1170" endWordPosition="1173">ith a machine learning-based classifier, function Dep() in Figure 3 uses the classifier to decide whether two bunsetsus have a dependency relation. In order to prepare training examples for the trainable classifier used with his algorithm, we first have to convert a treebank to suitable labeled instances by using the algorithm in Figure 4. Note 1Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary classifiers can work with Sassano’s for Japanese. 357 Input: wi: bunsetsus in a given sentence. N: the number of bunsetsus. Output: hj: the head IDs of bunsetsus wj. Functions: Push(i, s): pushes i on the stack s. Pop(s): pops a value off the stack s. Dep(j, i</context>
<context position="16759" citStr="Kudo and Matsumoto, 2000" startWordPosition="2863" endWordPosition="2866"> a pool of unlabeled sentences for training are taken from the articles on January 1st through 8th (7,958 sentences) and the test data is a set of sentences in the articles on January 9th (1,246 sentences). The articles on January 10th were used for development. The split of these articles for training/test/development is the same as in (Uchimoto et al., 1999). 6.2 Averaged Perceptron We used the averaged perceptron (AP) (Freund and Schapire, 1999) with polynomial kernels. We set the degree of the kernels to 3 since cubic kernels with SVM have proved effective for Japanese dependency parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). We found the best value of the epoch T of the averaged perceptron by using the development set. We fixed T = 12 through all experiments for simplicity. 6.3 Features There are features that have been commonly used for Japanese dependency parsing among related papers, e.g., (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008). We also used 6.4 Implementation We implemented a parser and a tool for the averaged perceptron in C++ and used them for experiments. We wrote the main program of active learning and some additional scripts in Perl and sh. 6.5 Setting</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Japanese dependency structure analysis based on support vector machines. In Proc. ofEMNLP/VLC 2000, pages 18– 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL-2002,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="7171" citStr="Kudo and Matsumoto, 2002" startWordPosition="1148" endWordPosition="1151">Sassano’s algorithm is shown in Figure 3, which is slightly simplified from the original by Sassano (2004). When we use this algorithm with a machine learning-based classifier, function Dep() in Figure 3 uses the classifier to decide whether two bunsetsus have a dependency relation. In order to prepare training examples for the trainable classifier used with his algorithm, we first have to convert a treebank to suitable labeled instances by using the algorithm in Figure 4. Note 1Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary classifiers can work with Sassano’s for Japanese. 357 Input: wi: bunsetsus in a given sentence. N: the number of bunsetsus. </context>
<context position="16786" citStr="Kudo and Matsumoto, 2002" startWordPosition="2867" endWordPosition="2870">nces for training are taken from the articles on January 1st through 8th (7,958 sentences) and the test data is a set of sentences in the articles on January 9th (1,246 sentences). The articles on January 10th were used for development. The split of these articles for training/test/development is the same as in (Uchimoto et al., 1999). 6.2 Averaged Perceptron We used the averaged perceptron (AP) (Freund and Schapire, 1999) with polynomial kernels. We set the degree of the kernels to 3 since cubic kernels with SVM have proved effective for Japanese dependency parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). We found the best value of the epoch T of the averaged perceptron by using the development set. We fixed T = 12 through all experiments for simplicity. 6.3 Features There are features that have been commonly used for Japanese dependency parsing among related papers, e.g., (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008). We also used 6.4 Implementation We implemented a parser and a tool for the averaged perceptron in C++ and used them for experiments. We wrote the main program of active learning and some additional scripts in Perl and sh. 6.5 Settings of Active Learning For in</context>
<context position="27376" citStr="Kudo and Matsumoto, 2002" startWordPosition="4648" endWordPosition="4652">iers since Sassano’s seems to be a simplified version of Nivre’s and they have several properties in common. However, syntactic constraints in European languages like English may be less helpful than those in Japanese because their dependency links do not have a single direction. Even though the use of syntactic constraints is limited, smaller constituents will still be useful for other parsing algorithms that use some deterministic methods with machine learning-based classifiers. There are many algorithms that have such a framework, which include (Yamada and Matsumoto, 2003) for English and (Kudo and Matsumoto, 2002; Iwatate et al., 2008) for Japanese. Therefore, effective use of smaller constituents in active learning would not be limited to the specific algorithm. 7 Conclusion We have investigated that active learning methods for Japanese dependency parsing. It is observed that active learning of parsing with the averaged perceptron, which is one of the large margin classifiers, works also well for Japanese dependency analysis. 10Ohtake (2006) examines heuristic methods of selecting sentences. 363 In addition, as far as we know, we are the first to propose the active learning methods of using partial d</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proc. of CoNLL-2002, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Building a Japanese parsed corpus while improving the parsing system.</title>
<date>1998</date>
<booktitle>In Proc. ofLREC-1998,</booktitle>
<pages>719--724</pages>
<contexts>
<context position="16107" citStr="Kurohashi and Nagao, 1998" startWordPosition="2753" endWordPosition="2756">d gap features. A summary of the features is described in Table 1. Case 1 if (j &lt; i &lt; k), then generate • {j, i, “O”}, • {k − 1,k,“D”} if k − 1 &gt; j, • and {j, k, “D”}. Case 2 if (j &lt; i = k), then generate • {k − 1,k,“D”} if k − 1 &gt; j, • and {j, k, “D”}. Case 3 if (j &lt; k &lt; i), then generate • {k − 1,k,“D”} if k − 1 &gt; j, • and {j, k, “D”}. In SYN as well as MODSIMPLE, we generate examples with ”O” only for bunsetsu pairs that occur to the left of the correct head (i.e., case 1). 6 Experimental Evaluation and Discussion 6.1 Corpus In our experiments we used the Kyoto University Corpus Version 2 (Kurohashi and Nagao, 1998). Initial seed sentences and a pool of unlabeled sentences for training are taken from the articles on January 1st through 8th (7,958 sentences) and the test data is a set of sentences in the articles on January 9th (1,246 sentences). The articles on January 10th were used for development. The split of these articles for training/test/development is the same as in (Uchimoto et al., 1999). 6.2 Averaged Perceptron We used the averaged perceptron (AP) (Freund and Schapire, 1999) with polynomial kernels. We set the degree of the kernels to 3 since cubic kernels with SVM have proved effective for J</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1998. Building a Japanese parsed corpus while improving the parsing system. In Proc. ofLREC-1998, pages 719–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Laws</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Stopping criteria for active learning of named entity recognition.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>465--472</pages>
<marker>Laws, Sch¨utze, 2008</marker>
<rawString>Florian Laws and Hinrich Sch¨utze. 2008. Stopping criteria for active learning of named entity recognition. In Proc. of COLING 2008, pages 465–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proc. of the Seventeenth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>3--12</pages>
<contexts>
<context position="3127" citStr="Lewis and Gale, 1994" startWordPosition="474" endWordPosition="477"> this research. Section 3 describes the syntactic characteristics of Japanese and the parsing algorithm that we use. Section 4 briefly reviews previous work on active learning for parsing and discusses several research challenges. In Section 5 we describe our proposed methods and others of active learning for Japanese dependency parsing. Section 6 describes experimental evaluation and discussion. Finally, in Section 7 we conclude this paper and point out some future directions. 2 Active Learning 2.1 Pool-based Active Learning Our base framework of active learning is based on the algorithm of (Lewis and Gale, 1994), which is called pool-based active learning. Following their sequential sampling algorithm, we show in Figure 1 the basic flow of pool-based active learning. Various methods for selecting informative examples can be combined with this framework. 2.2 Selection Algorithm for Large Margin Classifiers One of the most accurate approaches to classification tasks is an approach with large margin classifiers. Suppose that we are given data points {xil such that the associated label yi will be either −1 or 1, and we have a hyperplane of some large margin classifier defined by {x : f(x) = 01 where the </context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proc. of the Seventeenth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval, pages 3–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL-2005,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="7207" citStr="McDonald et al., 2005" startWordPosition="1155" endWordPosition="1158">3, which is slightly simplified from the original by Sassano (2004). When we use this algorithm with a machine learning-based classifier, function Dep() in Figure 3 uses the classifier to decide whether two bunsetsus have a dependency relation. In order to prepare training examples for the trainable classifier used with his algorithm, we first have to convert a treebank to suitable labeled instances by using the algorithm in Figure 4. Note 1Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary classifiers can work with Sassano’s for Japanese. 357 Input: wi: bunsetsus in a given sentence. N: the number of bunsetsus. Output: hj: the head IDs of bunsetsu</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proc. of ACL-2005, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT-03,</booktitle>
<pages>149--160</pages>
<contexts>
<context position="7561" citStr="Nivre, 2003" startWordPosition="1212" endWordPosition="1213">treebank to suitable labeled instances by using the algorithm in Figure 4. Note 1Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary classifiers can work with Sassano’s for Japanese. 357 Input: wi: bunsetsus in a given sentence. N: the number of bunsetsus. Output: hj: the head IDs of bunsetsus wj. Functions: Push(i, s): pushes i on the stack s. Pop(s): pops a value off the stack s. Dep(j, i, w): returns true when wj should modify wi. Otherwise returns false. procedure Analyze(w, N, h) var s: a stack for IDs of modifier bunsetsus begin {−1 indicates no modifier candidate} Push(−1, s); Push(0, s); for i ← 1 to N − 1 do begin j ← Pop(s); whil</context>
<context position="26534" citStr="Nivre, 2003" startWordPosition="4520" endWordPosition="4521">er parsing algorithms. First we take languages similar to Japanese in terms of syntax, i.e., Korean and Mongolian. These two languages are basically headfinal languages and have similar constraints in Section 3.2. Although no one has reported application of (Sassano, 2004) to the languages so far, we believe that similar parsing algorithms will be applicable to them and the discussion in this study would be useful. On the other hand, the algorithm of (Sassano, 2004) cannot be applied to head-initial languages such as English. If target languages are assumed to be projective, the algorithm of (Nivre, 2003) can be used. It is highly likely that we will invent the effective use of finer-grained constituents, e.g., head-modifier pairs, rather than sentences in active learning for Nivre’s algorithm with large margin classifiers since Sassano’s seems to be a simplified version of Nivre’s and they have several properties in common. However, syntactic constraints in European languages like English may be less helpful than those in Japanese because their dependency links do not have a single direction. Even though the use of syntactic constraints is limited, smaller constituents will still be useful fo</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proc. of IWPT-03, pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyonori Ohtake</author>
</authors>
<title>Analysis of selective strategies to build a dependency-analyzed corpus.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL 2006 Main Conf. Poster Sessions,</booktitle>
<pages>635--642</pages>
<contexts>
<context position="27814" citStr="Ohtake (2006)" startWordPosition="4717" endWordPosition="4718"> machine learning-based classifiers. There are many algorithms that have such a framework, which include (Yamada and Matsumoto, 2003) for English and (Kudo and Matsumoto, 2002; Iwatate et al., 2008) for Japanese. Therefore, effective use of smaller constituents in active learning would not be limited to the specific algorithm. 7 Conclusion We have investigated that active learning methods for Japanese dependency parsing. It is observed that active learning of parsing with the averaged perceptron, which is one of the large margin classifiers, works also well for Japanese dependency analysis. 10Ohtake (2006) examines heuristic methods of selecting sentences. 363 In addition, as far as we know, we are the first to propose the active learning methods of using partial dependency relations in a given sentence for parsing and we have evaluated the effectiveness of our methods. Furthermore, we have tried to obtain more labeled examples from precious labeled ones that annotators give by utilizing syntactic constraints of the Japanese language. It is noteworthy that linguistic constraints have been shown useful for reducing annotations in active learning for NLP. Experimental results show that our propos</context>
</contexts>
<marker>Ohtake, 2006</marker>
<rawString>Kiyonori Ohtake. 2006. Analysis of selective strategies to build a dependency-analyzed corpus. In Proc. of COLING/ACL 2006 Main Conf. Poster Sessions, pages 635–642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Ringger</author>
<author>Peter McClanahan</author>
<author>Robbie Haertel</author>
<author>George Busby</author>
<author>Marc Carmen</author>
<author>James Carroll</author>
<author>Kevin Seppi</author>
<author>Deryle Lonsdale</author>
</authors>
<title>Active learning for part-of-speech tagging: Accelerating corpus annotation.</title>
<date>2007</date>
<booktitle>In Proc. of the Linguistic Annotation Workshop,</booktitle>
<pages>101--108</pages>
<contexts>
<context position="1468" citStr="Ringger et al., 2007" startWordPosition="213" endWordPosition="216">e learning. 1 Introduction Reducing annotation cost is very important because supervised learning approaches, which have been successful in natural language processing, require typically a large number of labeled examples. Preparing many labeled examples is time consuming and labor intensive. One of most promising approaches to this issue is active learning. Recently much attention has been paid to it in the field of natural language processing. Various tasks have been targeted in the research on active learning. They include word sense disambiguation, e.g., (Zhu and Hovy, 2007), POS tagging (Ringger et al., 2007), named entity recognition (Laws and Sch¨utze, 2008), word segmentation, e.g., (Sassano, 2002), and parsing, e.g., (Tang et al., 2002; Hwa, 2004). It is the main purpose of this study to propose methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. Typically in active learning for parsing a Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan kuro@i.kyoto-u.ac.jp sentence has been considered to be a basic unit for selection. Sm</context>
</contexts>
<marker>Ringger, McClanahan, Haertel, Busby, Carmen, Carroll, Seppi, Lonsdale, 2007</marker>
<rawString>Eric Ringger, Peter McClanahan, Robbie Haertel, George Busby, Marc Carmen, James Carroll, Kevin Seppi, and Deryle Lonsdale. 2007. Active learning for part-of-speech tagging: Accelerating corpus annotation. In Proc. of the Linguistic Annotation Workshop, pages 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
</authors>
<title>An empirical study of active learning with support vector machines for Japanese word segmentation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL-2002,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="1562" citStr="Sassano, 2002" startWordPosition="228" endWordPosition="229">roaches, which have been successful in natural language processing, require typically a large number of labeled examples. Preparing many labeled examples is time consuming and labor intensive. One of most promising approaches to this issue is active learning. Recently much attention has been paid to it in the field of natural language processing. Various tasks have been targeted in the research on active learning. They include word sense disambiguation, e.g., (Zhu and Hovy, 2007), POS tagging (Ringger et al., 2007), named entity recognition (Laws and Sch¨utze, 2008), word segmentation, e.g., (Sassano, 2002), and parsing, e.g., (Tang et al., 2002; Hwa, 2004). It is the main purpose of this study to propose methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. Typically in active learning for parsing a Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan kuro@i.kyoto-u.ac.jp sentence has been considered to be a basic unit for selection. Small constituents such as chunks have not been used in sample selection for parsing. We use Jap</context>
</contexts>
<marker>Sassano, 2002</marker>
<rawString>Manabu Sassano. 2002. An empirical study of active learning with support vector machines for Japanese word segmentation. In Proc. of ACL-2002, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
</authors>
<title>Linear-time dependency analysis for Japanese.</title>
<date>2004</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>8--14</pages>
<contexts>
<context position="6162" citStr="Sassano, 2004" startWordPosition="987" endWordPosition="988"> Constraints of Japanese Dependency Analysis Japanese is a head final language and in written Japanese we usually hypothesize the following: • Each bunsetsu has only one head except the rightmost one. • Dependency links between bunsetsus go from left to right. • Dependencies do not cross one another. We can see that these constraints are satisfied in the sample sentence in Figure 2. In this paper we also assume that the above constraints hold true when we discuss algorithms of Japanese parsing and active learning for it. 3.3 Algorithm of Japanese Dependency Parsing We use Sassano’s algorithm (Sassano, 2004) for Japanese dependency parsing. The reason for this is that it is very accurate and efficient1. Furthermore, it is easy to implement. His algorithm is one of the simplest form of shift-reduce parsers and runs in linear-time.2 Since Japanese is a head final language and its dependencies are projective as described in Section 3.2, that simplification can be made. The basic flow of Sassano’s algorithm is shown in Figure 3, which is slightly simplified from the original by Sassano (2004). When we use this algorithm with a machine learning-based classifier, function Dep() in Figure 3 uses the cla</context>
<context position="17101" citStr="Sassano, 2004" startWordPosition="2921" endWordPosition="2923">l., 1999). 6.2 Averaged Perceptron We used the averaged perceptron (AP) (Freund and Schapire, 1999) with polynomial kernels. We set the degree of the kernels to 3 since cubic kernels with SVM have proved effective for Japanese dependency parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). We found the best value of the epoch T of the averaged perceptron by using the development set. We fixed T = 12 through all experiments for simplicity. 6.3 Features There are features that have been commonly used for Japanese dependency parsing among related papers, e.g., (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008). We also used 6.4 Implementation We implemented a parser and a tool for the averaged perceptron in C++ and used them for experiments. We wrote the main program of active learning and some additional scripts in Perl and sh. 6.5 Settings of Active Learning For initial seed sentences, first 500 sentences are taken from the articles on January 1st. In experiments about sentence wise selection, 500 sentences are selected at each iteration of active learning and labeled5 and added into the training data. In experiments about chunk wise selection 4000 pairs of bunsetsus, which</context>
<context position="26195" citStr="Sassano, 2004" startWordPosition="4464" endWordPosition="4465"> et al. (2008) do. 9Hwa (2004) discusses similar aspects of researches on active learning. pus. They also will be useful for domain adaptation of a dependency parser.10 Applicability to Other Languages and Other Parsing Algorithms We discuss here whether or not the proposed methods and the experiments are useful for other languages and other parsing algorithms. First we take languages similar to Japanese in terms of syntax, i.e., Korean and Mongolian. These two languages are basically headfinal languages and have similar constraints in Section 3.2. Although no one has reported application of (Sassano, 2004) to the languages so far, we believe that similar parsing algorithms will be applicable to them and the discussion in this study would be useful. On the other hand, the algorithm of (Sassano, 2004) cannot be applied to head-initial languages such as English. If target languages are assumed to be projective, the algorithm of (Nivre, 2003) can be used. It is highly likely that we will invent the effective use of finer-grained constituents, e.g., head-modifier pairs, rather than sentences in active learning for Nivre’s algorithm with large margin classifiers since Sassano’s seems to be a simplifi</context>
</contexts>
<marker>Sassano, 2004</marker>
<rawString>Manabu Sassano. 2004. Linear-time dependency analysis for Japanese. In Proc. of COLING 2004, pages 8–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Schohn</author>
<author>David Cohn</author>
</authors>
<title>Less is more: Active learning with support vector machines.</title>
<date>2000</date>
<booktitle>In Proc. ofICML-2000,</booktitle>
<pages>839--846</pages>
<contexts>
<context position="4885" citStr="Schohn and Cohn, 2000" startWordPosition="773" endWordPosition="776">Figure 1: Flow of the pool-based active learning Lisa-ga kare-ni ano pen-wo age-ta. Lisa-subj to him that pen-acc give-past. ID 0 1 2 3 4 Head 4 4 3 4 - Figure 2: Sample sentence. An English translation is “Lisa gave that pen to him.” classification function is G(x) = sign{f(x)}. In pool-based active learning with large margin classifiers, selection of examples can be done as follows: 1. Compute f(xi) over all unlabeled examples xi in the pool. 2. Sort xi with |f(xi) |in ascending order. 3. Select top m examples. This type of selection methods with SVMs is discussed in (Tong and Koller, 2000; Schohn and Cohn, 2000). They obtain excellent results on text classification. These selection methods are simple but very effective. 3 Japanese Parsing 3.1 Syntactic Units A basic syntactic unit used in Japanese parsing is a bunsetsu, the concept of which was initially introduced by Hashimoto (1934). We assume that in Japanese we have a sequence of bunsetsus before parsing a sentence. A bunsetsu contains one or more content words and zero or more function words. A sample sentence in Japanese is shown in Figure 2. This sentence consists of five bunsetsus: Lisa-ga, kare-ni, ano, pen-wo, and age-ta where ga, ni, and w</context>
<context position="24018" citStr="Schohn and Cohn, 2000" startWordPosition="4092" endWordPosition="4095">certain level of accuracy. Figure 10 shows that the number of labeled bunsetsus to achieve an accuracy of over 88.3% depending on the active learning methods discussed in this research. PASSIVE needs 37766 labeled bunsetsus which have a head to achieve an accuracy of 88.48%, while SYN needs 13021 labeled bunsetsus to achieve an accuracy of 88.56%. SYN requires only 34.4% of the labeled bunsetsu pairs that PASSIVE requires. Stopping Criteria It is known that increment rate of the number of support vectors in SVM indicates saturation of accuracy improvement during iterations of active learning (Schohn and Cohn, 2000). It is interesting to examine whether the observation for SVM is also useful for support vectors7 of the averaged perceptron. We plotted changes of the number of support vectors in the cases of both PASSIVE and MIN in Figure 11 and changes of the number of support vectors in the case of MOnSIMPLE in Figure 12. We observed that the increment rate of support vectors mildly gets smaller. However, it is not so clear as in the case of text classification in (Schohn and Cohn, 2000). Issues on Accessing the Total Cost of Annotation In this paper, we assume that each annotation cost for dependency re</context>
</contexts>
<marker>Schohn, Cohn, 2000</marker>
<rawString>Greg Schohn and David Cohn. 2000. Less is more: Active learning with support vector machines. In Proc. ofICML-2000, pages 839–846.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proc. of COLT ’92,</booktitle>
<pages>287--294</pages>
<contexts>
<context position="10712" citStr="Seung et al., 1992" startWordPosition="1759" endWordPosition="1762">ally annotated sentences and have to use a parsing algorithm that can be trained from incompletely annotated sentences. Therefore, it is difficult to use some of probabilistic models for parsing. 4 5 Active Learning for Japanese Dependency Parsing In this section we describe sample selection methods which we investigated. 5.1 Sentence-wise Sample Selection Passive Selection (Passive) This method is to select sequentially sentences that appear in the training corpus. Since it gets harder for the readers to reproduce the same experimental setting, we 4We did not employ query-by-committee (QBC) (Seung et al., 1992), which is another important general framework of active learning, since the selection strategy with large margin classifiers (Section 2.2) is much simpler and seems more practical for active learning in Japanese dependency parsing with smaller constituents. 358 avoid to use random sampling in this paper. Minimum Margin Selection (Min) This method is to select sentences that contain bunsetsu pairs which have smaller margin values of outputs of the classifier used in parsing. The procedure of selection of MIN are summarized as follows. Assume that we have sentences sz in the pool of unlabeled s</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H. S. Seung, M. Opper, and H. Sompolinsky. 1992. Query by committee. In Proc. of COLT ’92, pages 287–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Tang</author>
<author>Xaoqiang Luo</author>
<author>Salim Roukos</author>
</authors>
<title>Active learning for statistical natural language parsing.</title>
<date>2002</date>
<booktitle>In Proc. ofACL-2002,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="1601" citStr="Tang et al., 2002" startWordPosition="233" endWordPosition="236">in natural language processing, require typically a large number of labeled examples. Preparing many labeled examples is time consuming and labor intensive. One of most promising approaches to this issue is active learning. Recently much attention has been paid to it in the field of natural language processing. Various tasks have been targeted in the research on active learning. They include word sense disambiguation, e.g., (Zhu and Hovy, 2007), POS tagging (Ringger et al., 2007), named entity recognition (Laws and Sch¨utze, 2008), word segmentation, e.g., (Sassano, 2002), and parsing, e.g., (Tang et al., 2002; Hwa, 2004). It is the main purpose of this study to propose methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. Typically in active learning for parsing a Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan kuro@i.kyoto-u.ac.jp sentence has been considered to be a basic unit for selection. Small constituents such as chunks have not been used in sample selection for parsing. We use Japanese dependency parsing as a target ta</context>
<context position="8595" citStr="Tang et al., 2002" startWordPosition="1400" endWordPosition="1403">rocedure Analyze(w, N, h) var s: a stack for IDs of modifier bunsetsus begin {−1 indicates no modifier candidate} Push(−1, s); Push(0, s); for i ← 1 to N − 1 do begin j ← Pop(s); while (j =6 −1 and ((i = N − 1) or Dep(j, i, w)) ) do begin hj ← i; j ← Pop(s) end Push(j, s); Push(i, s) end end Figure 3: Algorithm of Japanese dependency parsing that the algorithm in Figure 4 does not generate every pair of bunsetsus.3 4 Active Learning for Parsing Most of the methods of active learning for parsing in previous work use selection of sentences that seem to contribute to the improvement of accuracy (Tang et al., 2002; Hwa, 2004; Baldridge and Osborne, 2004). Although Hwa suggests that sample selection for parsing would be improved by selecting finer grained constituents rather than sentences (Hwa, 2004), such methods have not been investigated so far. Typical methods of selecting sentences are 3We show a sample set of generated examples for training the classifier of the parser in Figure 3. By using the algorithm in Figure 4, we can obtain labeled examples from the sample sentences in Figure 2: {0, 1, “O”}, {1, 2, “O”}, {2, 3, “D”}, and {1, 3, “O”}. Please see Section 5.2 for the notation used here. For e</context>
<context position="9869" citStr="Tang et al., 2002" startWordPosition="1628" endWordPosition="1631"> “D”} will be like ”label=D, features={modifiercontent-word=ano, ..., head-content-word=pen, ...}.” Input: hi: the head IDs of bunsetsus wi. Function: Dep(j, i, w, h): returns true if hj = i. Otherwise returns false. Also prints a feature vector with a label according to hj. procedure Generate(w, N, h) begin Push(−1, s); Push(0, s); for i ← 1 to N − 1 do begin j ← Pop(s); while (j =6 −1 and ((i = N − 1) or Dep(j, i, w, h)) ) do begin j ← Pop(s) end Push(j, s); Push(i, s) end end Figure 4: Algorithm of generating training examples based on some entropy-based measure of a given sentence (e.g., (Tang et al., 2002)). We cannot use this kind of measures when we want to select other smaller constituents than sentences. Other bigger problem is an algorithm of parsing itself. If we sample smaller units rather than sentences, we have partially annotated sentences and have to use a parsing algorithm that can be trained from incompletely annotated sentences. Therefore, it is difficult to use some of probabilistic models for parsing. 4 5 Active Learning for Japanese Dependency Parsing In this section we describe sample selection methods which we investigated. 5.1 Sentence-wise Sample Selection Passive Selection</context>
</contexts>
<marker>Tang, Luo, Roukos, 2002</marker>
<rawString>Min Tang, Xaoqiang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. In Proc. ofACL-2002, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2000</date>
<booktitle>In Proc. of ICML-2000,</booktitle>
<pages>999--1006</pages>
<contexts>
<context position="4861" citStr="Tong and Koller, 2000" startWordPosition="769" endWordPosition="772">n all labeled examples Figure 1: Flow of the pool-based active learning Lisa-ga kare-ni ano pen-wo age-ta. Lisa-subj to him that pen-acc give-past. ID 0 1 2 3 4 Head 4 4 3 4 - Figure 2: Sample sentence. An English translation is “Lisa gave that pen to him.” classification function is G(x) = sign{f(x)}. In pool-based active learning with large margin classifiers, selection of examples can be done as follows: 1. Compute f(xi) over all unlabeled examples xi in the pool. 2. Sort xi with |f(xi) |in ascending order. 3. Select top m examples. This type of selection methods with SVMs is discussed in (Tong and Koller, 2000; Schohn and Cohn, 2000). They obtain excellent results on text classification. These selection methods are simple but very effective. 3 Japanese Parsing 3.1 Syntactic Units A basic syntactic unit used in Japanese parsing is a bunsetsu, the concept of which was initially introduced by Hashimoto (1934). We assume that in Japanese we have a sequence of bunsetsus before parsing a sentence. A bunsetsu contains one or more content words and zero or more function words. A sample sentence in Japanese is shown in Figure 2. This sentence consists of five bunsetsus: Lisa-ga, kare-ni, ano, pen-wo, and ag</context>
</contexts>
<marker>Tong, Koller, 2000</marker>
<rawString>Simon Tong and Daphne Koller. 2000. Support vector machine active learning with applications to text classification. In Proc. of ICML-2000, pages 999– 1006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Japanese dependency structure analysis based on maximum entropy models.</title>
<date>1999</date>
<booktitle>In Proc. of EACL-99,</booktitle>
<pages>196--203</pages>
<contexts>
<context position="16497" citStr="Uchimoto et al., 1999" startWordPosition="2821" endWordPosition="2824"> ”O” only for bunsetsu pairs that occur to the left of the correct head (i.e., case 1). 6 Experimental Evaluation and Discussion 6.1 Corpus In our experiments we used the Kyoto University Corpus Version 2 (Kurohashi and Nagao, 1998). Initial seed sentences and a pool of unlabeled sentences for training are taken from the articles on January 1st through 8th (7,958 sentences) and the test data is a set of sentences in the articles on January 9th (1,246 sentences). The articles on January 10th were used for development. The split of these articles for training/test/development is the same as in (Uchimoto et al., 1999). 6.2 Averaged Perceptron We used the averaged perceptron (AP) (Freund and Schapire, 1999) with polynomial kernels. We set the degree of the kernels to 3 since cubic kernels with SVM have proved effective for Japanese dependency parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). We found the best value of the epoch T of the averaged perceptron by using the development set. We fixed T = 12 through all experiments for simplicity. 6.3 Features There are features that have been commonly used for Japanese dependency parsing among related papers, e.g., (Kudo and Matsumoto, 2002; Sassano, </context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese dependency structure analysis based on maximum entropy models. In Proc. of EACL-99, pages 196–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. ofIWPT</booktitle>
<pages>195--206</pages>
<contexts>
<context position="27334" citStr="Yamada and Matsumoto, 2003" startWordPosition="4640" endWordPosition="4644">r Nivre’s algorithm with large margin classifiers since Sassano’s seems to be a simplified version of Nivre’s and they have several properties in common. However, syntactic constraints in European languages like English may be less helpful than those in Japanese because their dependency links do not have a single direction. Even though the use of syntactic constraints is limited, smaller constituents will still be useful for other parsing algorithms that use some deterministic methods with machine learning-based classifiers. There are many algorithms that have such a framework, which include (Yamada and Matsumoto, 2003) for English and (Kudo and Matsumoto, 2002; Iwatate et al., 2008) for Japanese. Therefore, effective use of smaller constituents in active learning would not be limited to the specific algorithm. 7 Conclusion We have investigated that active learning methods for Japanese dependency parsing. It is observed that active learning of parsing with the averaged perceptron, which is one of the large margin classifiers, works also well for Japanese dependency analysis. 10Ohtake (2006) examines heuristic methods of selecting sentences. 363 In addition, as far as we know, we are the first to propose the </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. ofIWPT 2003, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Eduard Hovy</author>
</authors>
<title>Active learning for word sense disambiguation with methods for addressing the class imbalance problem.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL</booktitle>
<pages>783--790</pages>
<contexts>
<context position="1432" citStr="Zhu and Hovy, 2007" startWordPosition="207" endWordPosition="210">led examples as compared to passive learning. 1 Introduction Reducing annotation cost is very important because supervised learning approaches, which have been successful in natural language processing, require typically a large number of labeled examples. Preparing many labeled examples is time consuming and labor intensive. One of most promising approaches to this issue is active learning. Recently much attention has been paid to it in the field of natural language processing. Various tasks have been targeted in the research on active learning. They include word sense disambiguation, e.g., (Zhu and Hovy, 2007), POS tagging (Ringger et al., 2007), named entity recognition (Laws and Sch¨utze, 2008), word segmentation, e.g., (Sassano, 2002), and parsing, e.g., (Tang et al., 2002; Hwa, 2004). It is the main purpose of this study to propose methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. Typically in active learning for parsing a Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan kuro@i.kyoto-u.ac.jp sentence has been considered </context>
</contexts>
<marker>Zhu, Hovy, 2007</marker>
<rawString>Jingbo Zhu and Eduard Hovy. 2007. Active learning for word sense disambiguation with methods for addressing the class imbalance problem. In Proc. of EMNLP-CoNLL 2007, pages 783–790.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>