<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016362">
<title confidence="0.996296">
Towards Tracking Semantic Change by Visual Analytics
</title>
<author confidence="0.987782">
Christian Rohrdantz1 Annette Hautli2 Thomas Mayer2
Miriam Butt2 Daniel A. Keim1 Frans Plank2
</author>
<affiliation confidence="0.9990435">
Department of Computer Science1 Department of Linguistics2
University of Konstanz
</affiliation>
<sectionHeader confidence="0.988094" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878944444444">
This paper presents a new approach to detect-
ing and tracking changes in word meaning by
visually modeling and representing diachronic
development in word contexts. Previous stud-
ies have shown that computational models
are capable of clustering and disambiguat-
ing senses, a more recent trend investigates
whether changes in word meaning can be
tracked by automatic methods. The aim of our
study is to offer a new instrument for inves-
tigating the diachronic development of word
senses in a way that allows for a better under-
standing of the nature of semantic change in
general. For this purpose we combine tech-
niques from the field of Visual Analytics with
unsupervised methods from Natural Language
Processing, allowing for an interactive visual
exploration of semantic change.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998994808510638">
The problem of determining and inferring the sense
of a word on the basis of its context has been the
subject of quite a bit of research. Earlier investiga-
tions have mainly focused on the disambiguation of
word senses from information contained in the con-
text, e.g. Sch¨utze (1998) or on the induction of word
senses (Yarowsky, 1995). Only recently, the field
has added a diachronic dimension to its investiga-
tions and has moved towards the computational de-
tection of sense development over time (Sagi et al.,
2009; Cook and Stevenson, 2010), thereby comple-
menting theoretical investigations in historical lin-
guistics with information gained from large corpora.
These approaches have concentrated on measuring
general changes in the meaning of a word (e.g., nar-
rowing or pejoration), whereas in this paper we deal
with cases where words acquire a new sense by ex-
tending their contexts to other domains.
For the scope of this investigation we restrict our-
selves to cases of semantic change in English even
though the methodology is generally language in-
dependent. Our choice is on the one hand moti-
vated by the extensive knowledge available on se-
mantic change in English. On the other hand, our
choice was driven by the availability of large cor-
pora for English. In particular, we used the New
York Times Annotated Corpus.1 Given the variety
and the amount of text available, we are able to track
changes from 1987 until 2007 in 1.8 million news-
paper articles.
In order to be able to explore our approach in a
fruitful manner, we decided to concentrate on words
which have acquired a new dimension of use due
to the introduction of computing and the internet,
e.g., to browse, to surf, bookmark. In particular,
the Netscape Navigator was introduced in 1994 and
our data show that this does indeed correlate with a
change in use of these words.
Our approach combines methods from the fields
of Information Visualization and Visual Analyt-
ics (Thomas and Cook, 2005; Keim et al., 2010)
with unsupervised techniques from Natural Lan-
guage Processing (NLP). This combination provides
a novel instrument which allows for tracking the di-
achronic development of word meaning by visual-
izing the contexts in which the words occur. Our
overall aim is not to replace linguistic analysis in
</bodyText>
<footnote confidence="0.992912">
1http://http://www.ldc.upenn.edu/
</footnote>
<page confidence="0.975093">
305
</page>
<note confidence="0.5905345">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 305–310,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999745891304348">
this field with an automatic method, but to guide re- 3 An interactive visualization approach to
search by generating new hypotheses about the de- semantic change
velopment of semantic change. In order to test our approach, we opted for a large
2 Related work corpus with a high temporal resolution. The New
The computational modeling of word senses is based York Times Annotated Corpus with 1.8 million
on the assumption that the meaning of a word can newspaper articles from 1987 to 2007 has a rather
be inferred from the words in its immediate con- small time depth of 20 years but provides a time
text (“context words”). Research in this area mainly stamp for the exact publication date. Therefore,
focuses on two related tasks: Word Sense Disam- changes can be tracked on a daily basis.
biguation (WSD) and Word Sense Induction (WSI). The data processing involved context extraction,
The goal of WSD is to classify occurrences of pol- vector space creation, and sense modeling. As
ysemous words according to manually predefined Sch¨utze (1998) showed, looking at a context win-
senses. One popular method for performing such dow of 25 words before and after a key word pro-
a classification is Latent Semantic Analysis (LSA) vides enough information in order to disambiguate
(Deerwester et al., 1990), with other methods also word senses. Each extracted context is comple-
suitable for the task (see Navigli (2009) for an ex- mented with the time stamp from the corpus. To
tensive survey). reduce the dimensionality, all context words were
The aim of WSI is to learn word senses from lemmatized and stop words were filtered out.
text corpora without having a predefined number of For the set of all contexts of a key word, a global
senses. This goal is more difficult to achieve, as it LDA model was trained using the MALLET toolkit2
is not clear beforehand how many senses should be (McCallum, 2002). Each context is assigned to its
extracted and how a sense could be described in an most probable topic/sense, complemented by a spe-
abstract way. Recently, however, Brody and Lapata cific point on the time scale according to its time
(2009) have shown that Latent Dirichlet Allocation stamp from the corpus. Contexts for which the high-
(LDA) (Blei et al., 2003) can be successfully applied est probability was less than 40% were omitted be-
to perform word sense induction from small word cause they could not be assigned to a certain sense
contexts. unambiguously. The distribution of senses over time
The original idea of LSA and LDA is to learn “top- was then visualized.
ics” from documents, whereas in our scenario word 3.1 Visualization
contexts rather than documents are used, i.e., a small Different visualizations provide multidimensional
number of words before and after the word under views on the data and yield a better understanding
investigation (bag of words). Sagi et al. (2009) of the developments. While plotting every word oc-
have demonstrated that broadening and narrowing currence individually offers the opportunity to detect
of word senses can be tracked over time by applying and inspect outliers, aggregated views on the data
LSA to small word contexts in diachronic corpora. are able to provide insights on overall developments.
In addition, we will use LDA, which has proven even Figure 1 provides a view where the percentages of
more reliable in the course of our investigations. word contexts belonging to different senses are plot-
In general, the aim of our paper is to go beyond ted over time. For the verbs to browse and to surf
the approach of Sagi et al. (2009) and analyze se- seven senses are learned with LDA. Each sense cor-
mantic change in more detail. Ideally, a starting responds to one row and is described by the top five
point of change is found and the development over terms identified by LDA. The higher the gray area
time can be tracked, paired with a quantitative com- at a certain x-axis point, the more of the contexts of
parison of prevailing senses. We therefore suggest the corresponding year belong to the specific sense.
to visualize word contexts in order to gain a better Each shade of gray represents 10% of the overall
understanding of diachronic developments and also data, i.e., three shades of gray mean that between
generate hypotheses for further investigations.
</bodyText>
<figure confidence="0.678377432432432">
306
2http://mallet.cs.umass.edu/
to browse to surf
time, library,
student, music,
people
shop, street,
book, store, art
book, read,
bookstore, find,
year
deer, plant,
tree, garden,
animal
software, microsoft,
internet, netscape,
windows
web, internet,
site, mail ,
computer
store, shop,
buy, day,
customer
f
g
a
c
d
e
b
h
i
j
k
m
n
l
</figure>
<construct confidence="0.700970315789474">
sport, wind,
water, ski, offer
wave, surfer,
board, year,
sport
channel,
television,
show, watch, tv
web, internet,
site, computer,
company
film, boy,
movie, show,
ride
year, day, time,
school, friend
beach, wave,
surfer, long,
coast
</construct>
<figureCaption confidence="0.998743">
Figure 1: Temporal development of different senses concerning the verbs to browse (left) and to surf (right)
</figureCaption>
<bodyText confidence="0.99988025">
20% and 30% of the contexts can be attributed to
that sense. For each year one value has been gener-
ated and values between two years are linearly inter-
polated.
Figure 2 shows the development of contexts over
time, with each context plotted individually. The
more recent the context, the darker the color.3 Each
axis represents one sense of to browse, in each sub-
figure different combinations of senses are plotted.
A random jitter has been introduced to avoid over-
laps. Contexts in the middle (not the lower left cor-
ner, but the middle of the graph, e.g., see e vs. f)
belong to both senses with at least 40% probabil-
ity. Senses that share many ambiguous contexts are
usually similar. By mousing over a colored dot, its
context is shown, allowing for an in depth analysis.
</bodyText>
<subsectionHeader confidence="0.99992">
3.2 Case studies
</subsectionHeader>
<bodyText confidence="0.9986502">
In order to be able to judge the effectiveness of our
new approach, we chose key words that are likely
candidates for a change in use in the time from 1987
to 2007. That is, we concentrated on terms relat-
ing to the relatively recent introduction of the inter-
net. The advantage of these terms is that the cause
of change can be located precisely in time.
Figure 1 shows the temporal sense development
of the verbs to browse and to surf, together with
the descriptive terms for each sense. Sense e for to
</bodyText>
<footnote confidence="0.790237">
3The pdf version of this paper contains a bipolar color map.
</footnote>
<bodyText confidence="0.999854655172414">
browse and sense k for to surf pattern quite similarly.
Inspecting their contexts reveals that both senses ap-
pear with the invention of web browsers, peaking
shortly after the introduction of Netscape Navigator
(1994). For to browse, another broader sense (sense
f) concerning browsing in both the internet and dig-
ital media collections shows a continuous increase
over time, dominating in 2007.
The first occurrences assigned to sense f in 1987
are “browse data bases”, “word-by-word brows-
ing” in databases and “browsing files in the cen-
ter’s library”, referring to physical files, namely pho-
tographs. We speculate that the sense of browsing
physical media might haven given rise to the sense
which refers to browsing electronic media, which in
turn becomes the dominating sense with the advent
of the web.
Figure 2 shows pairwise comparisons of word
senses with respect to the contexts they share, i.e.,
contexts that cannot unambiguously be assigned to
one or the other. Each context is represented by
one dot colored according to its time stamp. It can
be seen that senses d (animals that browse) and e
(browsing the web) share no contexts at all. Senses
d (animals that browse) and f (browsing files) share
only few contexts. In turn, senses e and f share a
fair number of contexts, which is to be expected, as
they are closely related. Single contexts, each rep-
resented by a colored dot, can be inspected via a
</bodyText>
<page confidence="0.997342">
307
</page>
<figureCaption confidence="0.986484">
Figure 2: Pairwise comparisons of different senses for the verb “to browse”. In each subfigure different combinations
of LDA dimensions are mapped on the axes.
</figureCaption>
<table confidence="0.802664466666667">
LSA dimensions
1 web 0.40, internet 0.38, software 0.36, microsoft 0.28, win-
dows 0.18
2 microsoft 0.24, software 0.23, windows 0.13, internet 0.13,
netscape 0.12
3 microsoft 0.27, store 0.22, shop 0.20, windows 0.19, software
0.16
4 shop 0.32, netscape 0.23, web 0.23, store 0.19, software 0.19
5 book 0.48, netscape 0.26, software 0.17, world 0.13, commu-
nication 0.12
6 internet 0.58, shop 0.25, service 0.16, computer 0.13, people
0.11
7 make 0.39, shop 0.34, site 0.16, windows 0.13, art 0.08
... ...
15 find 0.30, people 0.22, year 0.19, deer 0.16, day 0.15
</table>
<tableCaption confidence="0.70584125">
Table 1: Descriptive terms for the top LSA dimensions for
the contexts of to browse. For each dimension the top 5
positively associated terms were extracted, together with
their value in the corresponding dimension.
</tableCaption>
<bodyText confidence="0.999644666666667">
mouse roll over. This allows for an in-depth look at
specific data points and a better understanding how
the data points relate to a sense.
</bodyText>
<subsectionHeader confidence="0.99785">
3.3 LSA vs. LDA
</subsectionHeader>
<bodyText confidence="0.999947333333333">
In comparison, Table 1 shows the LSA dimensions
learned from the contexts of the verb to browse. The
top five associated terms for each dimension have
been extracted as descriptor. The dimensions are
heavily dominated by senses strongly represented
in the corpus (e.g., browsing the web). Infrequent
senses (e.g., animals that browse) only occur in very
low-ranked dimensions and are mixed with other
senses (see the bold term deer in dimension 15).
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="introduction">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999851428571429">
We compared the findings provided by our visual-
ization with word sense information coming from
various resources, namely the 2007 Collins dictio-
nary (COLL), the English WordNet4 (WN) (Fell-
baum, 1998) and the Longman Dictionary (LONG)
from 1987. Senses that evolved later than 1987
should not appear in LONG, but should appear in
later dictionaries.
However, we are well aware that dictionaries are
by no means good gold standards as lexicogra-
phers themselves vary greatly when assigning word
senses. Nevertheless, this comparison can provide a
first indication as to whether the results of our tool
is in line with other methods of identifying senses.
In the case of to browse, COLL and WordNet
suggest the senses “shopping around; not necessar-
ily buying”, “feed as in a meadow or pasture” and
“browse a computer directory, surf the internet or the
world wide web.” These senses are also identified in
our visualizations, which even additionally differen-
tiate between the senses of “browsing the web” and
“browsing a computer directory.” A WordNet sense
that cannot be detected in the data is the meaning “to
eat lightly and try different dishes.”
Table 2 shows the results of comparing dictionary
word senses (DIC) with the results from our visual-
ization (VIS). What can be seen is that our method
is able to track semantic change diachronically and
</bodyText>
<footnote confidence="0.976286">
4http://wordnetweb.princeton.edu
</footnote>
<page confidence="0.990329">
308
</page>
<table confidence="0.993404333333333">
to browse to surf messenger bug bookmark
# of word senses # of word senses # of word senses # of word senses # of word senses
DIC VIS DIC VIS DIC VIS DIC VIS DIC VIS
1987 (LONG) 2 3 1 1 1 2 6 3 1 1
1998 (WN) 5 4 3 3 1 3 5 3 1 2
2007 (COLL) 3 4 3 2 1 3 5 3 2 2
</table>
<tableCaption confidence="0.999056">
Table 2: A comparison of different word senses as given in dictionaries with the visualization results across time
</tableCaption>
<bodyText confidence="0.999974555555555">
in the majority of cases, the number of our senses
correspond to the information coming from the dic-
tionaries. In some cases we are even more accurate
in discriminating them. In the case of “messenger”,
the visualizations suggest another sense related to
“instant messaging” that arises with the advent of
the AOL instant messenger in 1997. This leads us to
the conclusion that our method is appropriate from a
historical linguistic point of view.
</bodyText>
<sectionHeader confidence="0.971265" genericHeader="discussions">
5 Discussion and conclusions
</sectionHeader>
<bodyText confidence="0.999989877192983">
When dealing with a complex phenomenon such as
semantic change, one has to be aware of the limita-
tions of an automatic approach in order to be able
to draw the right conclusions from its results. The
first results of the case studies presented in this pa-
per show that LDA is useful for distinguishing dif-
ferent word senses on the basis of word contexts and
performs better than LSA for this task. Further, it
has been demonstrated by exemplary cases that the
emergence of a new word sense can be detected by
our new methodology
One of the main reasons for an interactive visu-
alization approach is the possibility of being able to
detect conspicuous patterns at-a-glance, yet at the
same time being able to delve into the details of the
data by zooming in on the occurrences of particu-
lar words in their contexts. This makes it possible
to compensate for one of the major disadvantages
of generative and vector space models, namely their
functioning as “black boxes” whose results cannot
be tracked easily.
The biggest problem in dealing with a corpus-
based method of detecting meaning change is the
availability of suitable corpora. First, computing se-
mantic information on the basis of contexts requires
a large amount of data in order to be able to infer re-
liable results. Second, the words in the context from
which the meanings will be distinguished should be
both semantically and orthographically stable over
time so that comparisons between different stages in
the development of the language can be made. Un-
fortunately, both requirements are not always met.
On the one hand words do change their meaning,
after all this is what the present study is all about.
However, we assume that the meanings in a certain
context window are stable enough to infer reliable
results provided it is possible that the forms of the
same words in different periods can be linked. This
of course limits the applicability of the approach to
smaller time ranges due to changes in the phonetic
form of words. Moreover, in particular for older pe-
riods of the language, different variants for the same
word, either due to sound changes or different (or
rather no) spelling conventions, abound. For now,
we circumvent this problem by testing our tool on
corpora where the drawbacks of historical texts are
less severe but at the same time interesting develop-
ments can be detected to prove our approach correct.
For future research, we want to test our methodol-
ogy on a broader range of terms, texts and languages
and develop novel interactive visualizations to aid
investigations in two ways. As a first aim, the user
should be allowed to check the validity and quality
of the visualizations by experimenting with param-
eter settings and inspecting their outcome. Second,
the user is supposed to gain a better understanding of
semantic change by interactively exploring a corpus.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99954575">
This work has partly been funded by the Research
Initiative “Computational Analysis of Linguistic
Development” at the University of Konstanz and by
the German Research Society (DFG) under the grant
GK-1042, Explorative Analysis and Visualization of
Large Information Spaces, Konstanz. The authors
would like to thank Zdravko Monov for his program-
ming support.
</bodyText>
<page confidence="0.998893">
309
</page>
<sectionHeader confidence="0.995884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901468085106">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ’09, pages 103–
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally Identifying Changes in the Semantic Orientation
of Words. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10), pages 28–34, Valletta, Malta.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391–
407.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Daniel A. Keim, Joern Kohlhammer, Geoffrey Ellis, and
Florian Mansmann, editors. 2010. Mastering The In-
formation Age - Solving Problems with Visual Analyt-
ics. Goslar: Eurographics.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):1–69.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic Density Analysis: Comparing Word Mean-
ing across Time and Phonetic Space. In Proceedings
of the EACL 2009 Workshop on GEMS: GEometical
Models of Natural Language Semantics, pages 104–
111, Athens, Greece.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–123.
James J. Thomas and Kristin A. Cook. 2005. Illuminat-
ing the Path The Research and Development Agenda
for Visual Analytics. National Visualization and Ana-
lytics Center.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics (ACL ‘95), pages 189–196,
Cambridge, Massachusetts.
</reference>
<page confidence="0.998612">
310
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.678186">
<title confidence="0.849864">Towards Tracking Semantic Change by Visual Analytics Annette Thomas</title>
<author confidence="0.995509">Daniel A Frans</author>
<affiliation confidence="0.9849475">of Computer Department of University of Konstanz</affiliation>
<abstract confidence="0.999358105263158">This paper presents a new approach to detecting and tracking changes in word meaning by visually modeling and representing diachronic development in word contexts. Previous studies have shown that computational models are capable of clustering and disambiguating senses, a more recent trend investigates whether changes in word meaning can be tracked by automatic methods. The aim of our study is to offer a new instrument for investigating the diachronic development of word senses in a way that allows for a better understanding of the nature of semantic change in general. For this purpose we combine techniques from the field of Visual Analytics with unsupervised methods from Natural Language Processing, allowing for an interactive visual exploration of semantic change.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5804" citStr="Blei et al., 2003" startWordPosition="943" endWordPosition="946">ithout having a predefined number of For the set of all contexts of a key word, a global senses. This goal is more difficult to achieve, as it LDA model was trained using the MALLET toolkit2 is not clear beforehand how many senses should be (McCallum, 2002). Each context is assigned to its extracted and how a sense could be described in an most probable topic/sense, complemented by a speabstract way. Recently, however, Brody and Lapata cific point on the time scale according to its time (2009) have shown that Latent Dirichlet Allocation stamp from the corpus. Contexts for which the high(LDA) (Blei et al., 2003) can be successfully applied est probability was less than 40% were omitted beto perform word sense induction from small word cause they could not be assigned to a certain sense contexts. unambiguously. The distribution of senses over time The original idea of LSA and LDA is to learn “top- was then visualized. ics” from documents, whereas in our scenario word 3.1 Visualization contexts rather than documents are used, i.e., a small Different visualizations provide multidimensional number of words before and after the word under views on the data and yield a better understanding investigation (b</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09,</booktitle>
<pages>103--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09, pages 103– 111, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatically Identifying Changes in the Semantic Orientation of Words.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<pages>28--34</pages>
<location>Valletta,</location>
<contexts>
<context position="1570" citStr="Cook and Stevenson, 2010" startWordPosition="245" endWordPosition="248">, allowing for an interactive visual exploration of semantic change. 1 Introduction The problem of determining and inferring the sense of a word on the basis of its context has been the subject of quite a bit of research. Earlier investigations have mainly focused on the disambiguation of word senses from information contained in the context, e.g. Sch¨utze (1998) or on the induction of word senses (Yarowsky, 1995). Only recently, the field has added a diachronic dimension to its investigations and has moved towards the computational detection of sense development over time (Sagi et al., 2009; Cook and Stevenson, 2010), thereby complementing theoretical investigations in historical linguistics with information gained from large corpora. These approaches have concentrated on measuring general changes in the meaning of a word (e.g., narrowing or pejoration), whereas in this paper we deal with cases where words acquire a new sense by extending their contexts to other domains. For the scope of this investigation we restrict ourselves to cases of semantic change in English even though the methodology is generally language independent. Our choice is on the one hand motivated by the extensive knowledge available o</context>
</contexts>
<marker>Cook, Stevenson, 2010</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2010. Automatically Identifying Changes in the Semantic Orientation of Words. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), pages 28–34, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<pages>407</pages>
<contexts>
<context position="4845" citStr="Deerwester et al., 1990" startWordPosition="777" endWordPosition="780">lication date. Therefore, focuses on two related tasks: Word Sense Disam- changes can be tracked on a daily basis. biguation (WSD) and Word Sense Induction (WSI). The data processing involved context extraction, The goal of WSD is to classify occurrences of pol- vector space creation, and sense modeling. As ysemous words according to manually predefined Sch¨utze (1998) showed, looking at a context winsenses. One popular method for performing such dow of 25 words before and after a key word proa classification is Latent Semantic Analysis (LSA) vides enough information in order to disambiguate (Deerwester et al., 1990), with other methods also word senses. Each extracted context is complesuitable for the task (see Navigli (2009) for an ex- mented with the time stamp from the corpus. To tensive survey). reduce the dimensionality, all context words were The aim of WSI is to learn word senses from lemmatized and stop words were filtered out. text corpora without having a predefined number of For the set of all contexts of a key word, a global senses. This goal is more difficult to achieve, as it LDA model was trained using the MALLET toolkit2 is not clear beforehand how many senses should be (McCallum, 2002). </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41:391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="13075" citStr="Fellbaum, 1998" startWordPosition="2168" endWordPosition="2170"> learned from the contexts of the verb to browse. The top five associated terms for each dimension have been extracted as descriptor. The dimensions are heavily dominated by senses strongly represented in the corpus (e.g., browsing the web). Infrequent senses (e.g., animals that browse) only occur in very low-ranked dimensions and are mixed with other senses (see the bold term deer in dimension 15). 4 Evaluation We compared the findings provided by our visualization with word sense information coming from various resources, namely the 2007 Collins dictionary (COLL), the English WordNet4 (WN) (Fellbaum, 1998) and the Longman Dictionary (LONG) from 1987. Senses that evolved later than 1987 should not appear in LONG, but should appear in later dictionaries. However, we are well aware that dictionaries are by no means good gold standards as lexicographers themselves vary greatly when assigning word senses. Nevertheless, this comparison can provide a first indication as to whether the results of our tool is in line with other methods of identifying senses. In the case of to browse, COLL and WordNet suggest the senses “shopping around; not necessarily buying”, “feed as in a meadow or pasture” and “brow</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel A Keim</author>
</authors>
<title>Mastering The Information Age - Solving Problems with Visual Analytics.</title>
<date>2010</date>
<editor>Joern Kohlhammer, Geoffrey Ellis, and Florian Mansmann, editors.</editor>
<publisher>Eurographics.</publisher>
<location>Goslar:</location>
<marker>Keim, 2010</marker>
<rawString>Daniel A. Keim, Joern Kohlhammer, Geoffrey Ellis, and Florian Mansmann, editors. 2010. Mastering The Information Age - Solving Problems with Visual Analytics. Goslar: Eurographics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="5443" citStr="McCallum, 2002" startWordPosition="884" endWordPosition="885">ter et al., 1990), with other methods also word senses. Each extracted context is complesuitable for the task (see Navigli (2009) for an ex- mented with the time stamp from the corpus. To tensive survey). reduce the dimensionality, all context words were The aim of WSI is to learn word senses from lemmatized and stop words were filtered out. text corpora without having a predefined number of For the set of all contexts of a key word, a global senses. This goal is more difficult to achieve, as it LDA model was trained using the MALLET toolkit2 is not clear beforehand how many senses should be (McCallum, 2002). Each context is assigned to its extracted and how a sense could be described in an most probable topic/sense, complemented by a speabstract way. Recently, however, Brody and Lapata cific point on the time scale according to its time (2009) have shown that Latent Dirichlet Allocation stamp from the corpus. Contexts for which the high(LDA) (Blei et al., 2003) can be successfully applied est probability was less than 40% were omitted beto perform word sense induction from small word cause they could not be assigned to a certain sense contexts. unambiguously. The distribution of senses over time</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="4957" citStr="Navigli (2009)" startWordPosition="797" endWordPosition="798">on (WSD) and Word Sense Induction (WSI). The data processing involved context extraction, The goal of WSD is to classify occurrences of pol- vector space creation, and sense modeling. As ysemous words according to manually predefined Sch¨utze (1998) showed, looking at a context winsenses. One popular method for performing such dow of 25 words before and after a key word proa classification is Latent Semantic Analysis (LSA) vides enough information in order to disambiguate (Deerwester et al., 1990), with other methods also word senses. Each extracted context is complesuitable for the task (see Navigli (2009) for an ex- mented with the time stamp from the corpus. To tensive survey). reduce the dimensionality, all context words were The aim of WSI is to learn word senses from lemmatized and stop words were filtered out. text corpora without having a predefined number of For the set of all contexts of a key word, a global senses. This goal is more difficult to achieve, as it LDA model was trained using the MALLET toolkit2 is not clear beforehand how many senses should be (McCallum, 2002). Each context is assigned to its extracted and how a sense could be described in an most probable topic/sense, co</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR), 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Sagi</author>
<author>Stefan Kaufmann</author>
<author>Brady Clark</author>
</authors>
<title>Semantic Density Analysis: Comparing Word Meaning across Time and Phonetic Space.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics,</booktitle>
<pages>104--111</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1543" citStr="Sagi et al., 2009" startWordPosition="241" endWordPosition="244">Language Processing, allowing for an interactive visual exploration of semantic change. 1 Introduction The problem of determining and inferring the sense of a word on the basis of its context has been the subject of quite a bit of research. Earlier investigations have mainly focused on the disambiguation of word senses from information contained in the context, e.g. Sch¨utze (1998) or on the induction of word senses (Yarowsky, 1995). Only recently, the field has added a diachronic dimension to its investigations and has moved towards the computational detection of sense development over time (Sagi et al., 2009; Cook and Stevenson, 2010), thereby complementing theoretical investigations in historical linguistics with information gained from large corpora. These approaches have concentrated on measuring general changes in the meaning of a word (e.g., narrowing or pejoration), whereas in this paper we deal with cases where words acquire a new sense by extending their contexts to other domains. For the scope of this investigation we restrict ourselves to cases of semantic change in English even though the methodology is generally language independent. Our choice is on the one hand motivated by the exte</context>
<context position="6436" citStr="Sagi et al. (2009)" startWordPosition="1045" endWordPosition="1048">sfully applied est probability was less than 40% were omitted beto perform word sense induction from small word cause they could not be assigned to a certain sense contexts. unambiguously. The distribution of senses over time The original idea of LSA and LDA is to learn “top- was then visualized. ics” from documents, whereas in our scenario word 3.1 Visualization contexts rather than documents are used, i.e., a small Different visualizations provide multidimensional number of words before and after the word under views on the data and yield a better understanding investigation (bag of words). Sagi et al. (2009) of the developments. While plotting every word ochave demonstrated that broadening and narrowing currence individually offers the opportunity to detect of word senses can be tracked over time by applying and inspect outliers, aggregated views on the data LSA to small word contexts in diachronic corpora. are able to provide insights on overall developments. In addition, we will use LDA, which has proven even Figure 1 provides a view where the percentages of more reliable in the course of our investigations. word contexts belonging to different senses are plotIn general, the aim of our paper is</context>
</contexts>
<marker>Sagi, Kaufmann, Clark, 2009</marker>
<rawString>Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009. Semantic Density Analysis: Comparing Word Meaning across Time and Phonetic Space. In Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 104– 111, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James J Thomas</author>
<author>Kristin A Cook</author>
</authors>
<title>Illuminating the Path The Research and Development Agenda for Visual Analytics. National Visualization and Analytics Center.</title>
<date>2005</date>
<contexts>
<context position="2990" citStr="Thomas and Cook, 2005" startWordPosition="485" endWordPosition="488"> and the amount of text available, we are able to track changes from 1987 until 2007 in 1.8 million newspaper articles. In order to be able to explore our approach in a fruitful manner, we decided to concentrate on words which have acquired a new dimension of use due to the introduction of computing and the internet, e.g., to browse, to surf, bookmark. In particular, the Netscape Navigator was introduced in 1994 and our data show that this does indeed correlate with a change in use of these words. Our approach combines methods from the fields of Information Visualization and Visual Analytics (Thomas and Cook, 2005; Keim et al., 2010) with unsupervised techniques from Natural Language Processing (NLP). This combination provides a novel instrument which allows for tracking the diachronic development of word meaning by visualizing the contexts in which the words occur. Our overall aim is not to replace linguistic analysis in 1http://http://www.ldc.upenn.edu/ 305 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 305–310, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics this field with an automatic method, but to gu</context>
</contexts>
<marker>Thomas, Cook, 2005</marker>
<rawString>James J. Thomas and Kristin A. Cook. 2005. Illuminating the Path The Research and Development Agenda for Visual Analytics. National Visualization and Analytics Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics (ACL ‘95),</booktitle>
<pages>189--196</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1362" citStr="Yarowsky, 1995" startWordPosition="213" endWordPosition="214">better understanding of the nature of semantic change in general. For this purpose we combine techniques from the field of Visual Analytics with unsupervised methods from Natural Language Processing, allowing for an interactive visual exploration of semantic change. 1 Introduction The problem of determining and inferring the sense of a word on the basis of its context has been the subject of quite a bit of research. Earlier investigations have mainly focused on the disambiguation of word senses from information contained in the context, e.g. Sch¨utze (1998) or on the induction of word senses (Yarowsky, 1995). Only recently, the field has added a diachronic dimension to its investigations and has moved towards the computational detection of sense development over time (Sagi et al., 2009; Cook and Stevenson, 2010), thereby complementing theoretical investigations in historical linguistics with information gained from large corpora. These approaches have concentrated on measuring general changes in the meaning of a word (e.g., narrowing or pejoration), whereas in this paper we deal with cases where words acquire a new sense by extending their contexts to other domains. For the scope of this investig</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics (ACL ‘95), pages 189–196, Cambridge, Massachusetts.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>