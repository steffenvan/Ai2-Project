<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.976301">
Generating Referring Expressions in Open Domains
</title>
<author confidence="0.944444">
Advaith Siddharthan Ann Copestake
</author>
<affiliation confidence="0.9768405">
Computer Science Department Computer Laboratory
Columbia University University of Cambridge
</affiliation>
<email confidence="0.984832">
as372@cs.columbia.edu aac10@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.997196" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999810857142857">
We present an algorithm for generating referring
expressions in open domains. Existing algorithms
work at the semantic level and assume the avail-
ability of a classification for attributes, which is
only feasible for restricted domains. Our alterna-
tive works at the realisation level, relies on Word-
Net synonym and antonym sets, and gives equiva-
lent results on the examples cited in the literature
and improved results for examples that prior ap-
proaches cannot handle. We believe that ours is
also the first algorithm that allows for the incremen-
tal incorporation of relations. We present a novel
corpus-evaluation using referring expressions from
the Penn Wall Street Journal Treebank.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998802186046512">
Referring expression generation has historically
been treated as a part of the wider issue of gener-
ating text from an underlying semantic representa-
tion. The task has therefore traditionally been ap-
proached at the semantic level. Entities in the real
world are logically represented; for example (ignor-
ing quantifiers), a big brown dog might be repre-
sented as big1(x) n brown1(x) n dog1(x), where
the predicates big1, brown1 and dog1 represent dif-
ferent attributes of the variable (entity) x. The task
of referring expression generation has traditionally
been framed as the identification of the shortest log-
ical description for the referent entity that differen-
tiates it from all other entities in the discourse do-
main. For example, if there were a small brown dog
(small1(x) n brown1(x) n dog1(x)) in context, the
minimal description for the big brown dog would be
big1(x) n dog1(x)1.
This semantic framework makes it difficult to ap-
ply existing referring expression generation algo-
rithms to the many regeneration tasks that are im-
portant today; for example, summarisation, open-
ended question answering and text simplification.
Unlike in traditional generation, the starting point in
&apos;The predicate dog1 is selected because it has a distin-
guished status, referred to as type in Reiter and Dale (1992).
One such predicate has to to be present in the description.
these tasks is unrestricted text, rather than a seman-
tic representation of a small domain. It is difficult
to extract the required semantics from unrestricted
text (this task would require sense disambiguation,
among other issues) and even harder to construct
a classification for the extracted predicates in the
manner that existing approaches require (cf., §2).
In this paper, we present an algorithm for generat-
ing referring expressions in open domains. We dis-
cuss the literature and detail the problems in apply-
ing existing approaches to reference generation to
open domains in §2. We then present our approach
in §3, contrasting it with existing approaches. We
extend our approach to handle relations in §3.3 and
present a novel corpus-based evaluation on the Penn
WSJ Treebank in §4.
</bodyText>
<sectionHeader confidence="0.921964" genericHeader="introduction">
2 Overview of Prior Approaches
</sectionHeader>
<bodyText confidence="0.901654884615385">
The incremental algorithm (Reiter and Dale, 1992)
is the most widely discussed attribute selection
algorithm. It takes as input the intended refer-
ent and a contrast set of distractors (other enti-
ties that could be confused with the intended refer-
ent). Entities are represented as attribute value ma-
trices (AVMs). The algorithm also takes as input
a *preferred-attributes* list that contains, in order
of preference, the attributes that human writers use
to reference objects. For example, the preference
might be {colour, size, shape...}. The algorithm
then repeatedly selects attributes from *preferred-
attributes* that rule out at least one entity in the
contrast set until all distractors have been ruled out.
It is instructive to look at how the incremental al-
gorithm works. Consider an example where a large
brown dog needs to be referred to. The contrast set
contains a large black dog. These are represented
by the AVMs shown below.
� �
type dog
� size large �
colour black
Assuming that the *preferred-attributes* list is
[size, colour,...], the algorithm would first com-
pare the values of the size attribute (both large),
</bodyText>
<footnote confidence="0.794840666666667">
type dog
size large
colour brown
</footnote>
<page confidence="0.658871">
�
�
1
</page>
<bodyText confidence="0.997797866666667">
disregard that attribute as not being discriminating,
compare the values of the colour attribute and re-
turn the brown dog.
Subsequent work on referring expression genera-
tion has expanded the logical framework to allow
reference by negation (the dog that is not black)
and references to multiple entities (the brown or
black dogs) (van Deemter, 2002), explored different
search algorithms for finding the minimal descrip-
tion (e.g., Horacek (2003)) and offered different
representation frameworks like graph theory (Krah-
mer et al., 2003) as alternatives to AVMs. However,
all these approaches are based on very similar for-
malisations of the problem, and all make the follow-
ing assumptions:
</bodyText>
<listItem confidence="0.9924125">
1. A semantic representation exists.
2. A classification scheme for attributes exists.
3. The linguistic realisations are unambiguous.
4. Attributes cannot be reference modifying.
</listItem>
<bodyText confidence="0.999969177419355">
All these assumptions are violated when we move
from generation in a very restricted domain to re-
generation in an open domain. In regeneration
tasks such as summarisation, open-ended question
answering and text simplification, AVMs for enti-
ties are typically constructed from noun phrases,
with the head noun as the type and pre-modifiers
as attributes. Converting words into semantic la-
bels would involve sense disambiguation, adding
to the cost and complexity of the analysis module.
Also, attribute classification is a hard problem and
there is no existing classification scheme that can be
used for open domains like newswire; for example,
WordNet (Miller et al., 1993) organises adjectives
as concepts that are related by the non-hierarchical
relations of synonymy and antonymy (unlike nouns
that are related through hierarchical links such as
hyponymy, hypernymy and metonymy). In addi-
tion, selecting attributes at the semantic level is
risky because their linguistic realisation might be
ambiguous and many common adjectives are pol-
ysemous (cf., example 1 in §3.1). Reference modi-
fication, which has not been considered in the refer-
ring expression generation literature, raises further
issues; for example, referring to an alleged mur-
derer as the murderer is potentially libellous.
In addition to the above, there is the issue of over-
lap between values of attributes. The case of sub-
sumption (for example, that the colour red sub-
sumes crimson and the type dog subsumes chi-
huahua) has received formal treatment in the liter-
ature; Dale and Reiter (1995) provide a find-best-
value function that evaluates tree-like hierarchies
of values. As mentioned earlier, such hierarchi-
cal knowledge bases do not exist for open domains.
Further, a treatment of subsumption is insufficient,
and degrees of intersection between attribute values
also require consideration. van Deemter (2000) dis-
cusses the generation of vague descriptions when
entities have gradable attributes like size; for ex-
ample, in a domain with four mice sized 2, 5, 7
and 10cm, it is possible to refer to the large mouse
(the mouse sized 10cm) or the two small mice (the
mice sized 2 and 5cm). However, when applying re-
ferring expression generation to regeneration tasks
where the representation of entities is derived from
text rather than a knowledge base, we have to con-
sider the case where the grading of attributes is not
explicit. For example, we might need to compare
the attribute dark with black, light or white.
In contrast to previous approaches, our algorithm
works at the level of words, not semantic labels, and
measures the relatedness of adjectives (lexicalised
attributes) using the lexical knowledge base Word-
Net rather than a semantic classification. Our ap-
proach also addresses the issue of comparing inter-
sective attributes that are not explicitly graded, by
making novel use of the synonymy and antonymy
links in WordNet. Further, it treats discriminating
power as only one criteria for selecting attributes
and allows for the easy incorporation of other con-
siderations such as reference modification (§5).
</bodyText>
<sectionHeader confidence="0.996731" genericHeader="method">
3 The Lexicalised Approach
</sectionHeader>
<subsectionHeader confidence="0.999712">
3.1 Quantifying Discriminating Power
</subsectionHeader>
<bodyText confidence="0.941337">
We define the following three quotients.
</bodyText>
<subsectionHeader confidence="0.593414">
Similarity Quotient (5Q)
</subsectionHeader>
<bodyText confidence="0.994391962962964">
We define similarity as transitive synonymy. The
idea is that if X is a synonym of Y and Y is a syn-
onym of Z, then X is likely to be similar to Z. The
degree of similarity between two adjectives depends
on how many steps must be made through WordNet
synonymy lists to get from one to the other.
Suppose we need to find a referring expression
for e0. For each adjective aj describing e0, we cal-
culate a similarity quotient 5Qj by initialising it to
0, forming a set of WordNet synonyms 51 of aj,
forming a synonymy set 52 containing all the Word-
Net synonyms of all the adjectives in 51 and form-
ing 53 from 52 similarly. Now for each adjective
describing any distractor, we increment 5Qj by 4 if
it is present in 51, by 2 if it is present in 52, and by 1
if it is present in 53. 5Qj now measures how similar
aj is to other adjectives describing distractors.
Contrastive Quotient (CQ)
Similarly, we define contrastive in terms of
antonymy relationships. We form the set C1 of
strict WordNet antonyms of aj. The set C2 con-
sists of strict WordNet antonyms of members of 51
and WordNet synonyms of members of C1. C3 is
similarly constructed from 52 and C2. We now ini-
tialise CQj to zero and for each adjective describing
each distractor, we add w = {4,2, 1} to CQj, de-
pending on whether it is a member of C1, C2 or C3.
CQj now measures how contrasting aj is to other
adjectives describing distractors.
Discriminating Quotient (DQ)
An attribute that has a high value of 5Q has bad
discriminating power. An attribute that has a high
value of CQ has good discriminating power. We
can now define the Discriminating Quotient (DQ)
as DQ = CQ − 5Q. We now have an order (de-
creasing DQs) in which to incorporate attributes.
This constitutes our *preferred* list. We illustrate
the benefits of our approach with two examples.
Example 1: The Importance of Lexicalisation
Previous referring expression generation algorithms
ignore the issue of realising the logical description
for the referent. The semantic labels are chosen
such that they have a direct correspondence with
their linguistic realisation and the realisation is thus
considered trivial. Ambiguity and syntactically
optional arguments are ignored. To illustrate one
problem this causes, consider the two entities
below: e1 e2
� type president � � type president
age old � age young
� tenure current � tenure past
If we followed the strict typing system used
by previous algorithms, with *preferred*={age,
tenure}, to refer to e1 we would compare the
age attributes and rule out e2 and generate the
old president. This expression is ambiguous since
old can also mean previous. Models that select
attributes at the semantic level will run into trouble
when their linguistic realisations are ambiguous.
In contrast, our algorithm, given flattened attribute
lists:
successfully picks the current president as current
has a higher DQ (2) than old (0):
attribute distractor CQ SQ DQ
old e2{young, past} 4 4 0
current e2{young, past} 2 0 2
In this example, old is a WordNet antonym of young
and a WordNet synonym of past. Current is a
WordNet synonym of present, which is a WordNet
antonym of past. Note that WordNet synonym and
antonym links capture the implicit gradation in the
lexicalised values of the age and tenure attributes.
Example 2: Naive Incrementality
To illustrate another problem with the original in-
cremental algorithm, consider three dogs: e1(a big
black dog), e2(a small black dog) and e3(a tiny
white dog).
Consider using the original incremental algo-
rithm to refer to e1 with *preferred*={colour,
size}. The colour attribute black rules out e3.
We then we have to select the size attribute big as
well to rule out e2, thus generating the sub-optimal
expression the big black dog. Here, the use of a
predetermined *preferred* list fails to capture what
is obvious from the context: that e1 stands out not
because it is black, but because it is big.
In our approach, for each of e1’s attributes, we
calculate DQ with respect to e2 and e3:
attribute distractor CQ SQ DQ
big e2{small, black} 4 0 4
big e3{tiny, white} 2 0 2
black e2{small, black} 1 4 -3
black e3{tiny, white} 2 1 1
Overall, big has a higher discriminating power
(6) than black (-2) and rules out both e2 and e3.
We therefore generate the big dog. Our incremen-
tal approach thus manages to select the attribute that
stands out in context. This is because we construct
the *preferred* list after observing the context. We
discuss this issue further in the next section. Note
again that WordNet antonym and synonym links
capture the gradation in the lexicalised size and
colour attributes. However, this only works where
the gradation is along one axis; in particular, this
approach will not work for colours in general, and
cannot be used to deduce the relative similarity be-
tween yellow and orange as compared to, say, yel-
low and blue.
</bodyText>
<subsectionHeader confidence="0.999955">
3.2 Justifying our Algorithm
</subsectionHeader>
<bodyText confidence="0.999643">
The psycholinguistic justification for the incremen-
tal algorithm (IA) hinges on two premises:
</bodyText>
<listItem confidence="0.990936">
1. Humans build referring expressions incrementally.
2. There is a preferred order in which humans select
attributes (e.g., colour&gt;shape&gt;size...).
</listItem>
<bodyText confidence="0.999937818181818">
Our algorithm is also incremental. However, it
departs significantly from premise 2. We assume
that speakers pick out attributes that are distinctive
in context (cf., example 2, previous section). Aver-
aged over contexts, some attributes have more dis-
criminating power than others (largely because of
the way we visualise entities) and premise 2 is an
approximation to our approach.
We now quantify the extra effort we are making
to identify attributes that “stand out” in a given con-
text. Let N be the maximum number of entities in
</bodyText>
<equation confidence="0.605223">
e1 e2
</equation>
<bodyText confidence="0.937744166666667">
r head president II head president 1
L attrib old, current J L attrib young, past J
the contrast set and n be the maximum number of
attributes per entity. The table below compares the
computational complexity of an optimal algorithm
(such as Reiter (1990)), our algorithm and the IA.
</bodyText>
<subsectionHeader confidence="0.753941">
Incremental Algo Our Algorithm Optimal Algo
</subsectionHeader>
<equation confidence="0.661704">
O(nN) O(n2N) O(n2N)
</equation>
<bodyText confidence="0.999893">
Both the IA and our algorithm are linear in the
number of entities N. This is because neither al-
gorithm allows backtracking; an attribute, once se-
lected, cannot be discarded. In contrast, an opti-
mal search requires O(2N) comparisons. As our
algorithm compares each attribute of the discourse
referent to every attribute of every distractor, it is
quadratic in n. The IA compares each attribute of
the discourse referent to only one attribute per dis-
tractor and is linear in n. Note, however, that values
for n of over 4 are rare.
</bodyText>
<subsectionHeader confidence="0.95396">
3.3 Relations
</subsectionHeader>
<bodyText confidence="0.999868541666667">
Semantically, attributes describe an entity (e.g., the
small grey dog) and relations relate an entity to
other entities (e.g., the dog in the bin). Relations
are troublesome because in relating an entity e,, to
el, we need to recursively generate a referring ex-
pression for el. The IA does not consider relations
and the referring expression is constructed out of at-
tributes alone. The Dale and Haddock (1991) algo-
rithm allows for relational descriptions but involves
exponential global search, or a greedy search ap-
proximation. To incorporate relational descriptions
in the incremental framework would require a clas-
sification system which somehow takes into account
the relations themselves and the secondary entities
el etc. This again suggests that the existing algo-
rithms force the incrementality at the wrong stage
in the generation process. Our approach computes
the order in which attributes are incorporated after
observing the context, by quantifying their utility
through the quotient DQ. This makes it easy for
us to extend our algorithm to handle relations, be-
cause we can compute DQ for relations in much
the same way as we did for attributes.We illustrate
this for prepositions.
</bodyText>
<subsectionHeader confidence="0.983351">
3.4 Calculating DQ for Relations
</subsectionHeader>
<bodyText confidence="0.988888729166666">
Suppose the referent entity eref contains a relation
[prep,, e,,] that we need to calculate the three quo-
tients for (cf., figure 1 for representation of rela-
tions in AVMs). We consider each entity ez in the
contrast set for eref in turn. If ez does not have a
prep,, relation then the relation is useful and we in-
crement CQ by 4. If ez has a prep,, relation then
two cases arise. If the object of ez’s prep,, rela-
tion is e,, then we increment 5Q by 4. If it is not
e,,, the relation is useful and we increment CQ by
4. This is an efficient non-recursive way of com-
puting the quotients CQ and 5Q for relations. We
now discuss how to calculate DQ. For attributes,
we defined DQ = CQ − 5Q. However, as the lin-
guistic realisation of a relation is a phrase and not
a word, we would like to normalise the discriminat-
ing power of a relation with the length of its lin-
guistic realisation. Calculating the length involves
recursively generating referring expressions for the
object of the preposition, an expensive task that we
want to avoid unless we are actually using that rela-
tion in the final referring expression. We therefore
initially approximate the length as follows. The re-
alisation of a relation [prep,, e,,] consists of prep,,,
a determiner and the referring expression for e,,. If
none of eref’s distractors have a prep,, relation then
we only require the head noun of e,, in the refer-
ring expression and length = 3. In this case, the
relation is sufficient to identify both entities; for ex-
ample, even if there were multiple bins in figure 1,
as long as only one dog is in a bin, the reference
the dog in the bin succeeds in uniquely referencing
both the dog and the bin. If n distractors of eref
contain a prep,, relation with a non-e,, object that is
distractor for e,,, we set length = 3 + n. This is an
estimate for the word length of the realised relation
that assumes one extra attribute for distinguishing
e,, from each distractor. Normalisation by estimated
length is vital; if e,, requires a long description, the
relations’s DQ should be small so that shorter pos-
sibilities are considered first in the incremental pro-
cess. The formula for DQ for relations is therefore
DQ = (CQ − 5Q)/length.
This approach can also be extended to allow for
relations such as comparatives which have syntac-
tically optional arguments (e.g., the earlier flight vs
the flight earlier than UA941) which are not allowed
for by approaches which ignore realisation.
</bodyText>
<subsectionHeader confidence="0.959386">
3.5 The Lexicalised Context-Sensitive IA
</subsectionHeader>
<bodyText confidence="0.999716846153846">
Our lexicalised context-sensitive incremental algo-
rithm (below) generates a referring expression for
Entity. As it recurses, it keeps track of entities it has
used up in order to avoid entering loops like the dog
in the bin containing the dog in the bin.... To gener-
ate a referring expression for an entity, the algorithm
calculates the DQs for all its attributes and approxi-
mates the DQs for all its relations (2). It then forms
the *preferred* list (3) and constructs the referring
expression by adding elements of *preferred* till
the contrast set is empty (4). This is straightfor-
ward for attributes (5). For relations (6), it needs to
recursively generate the prepositional phrase first.
It checks that it hasn’t entered a loop (6a), gener-
ates a new contrast set for the object of the relation
(6(a)i), recursively generates a referring expression
for the object of the preposition (6(a)ii), recalculates
DQ (6(a)iii) and either incorporates the relation in
the referring expression or shifts the relation down
the *preferred* list (6(a)iv). This step ensures that
an initial mis-estimation in the word length of a re-
lation doesn’t force its inclusion at the expense of
shorter possibilities. If after incorporating all at-
tributes and relations, the contrast set is still non-
empty, the algorithm returns the best expression it
can find (7).
</bodyText>
<figure confidence="0.877609777777778">
set generate-ref-exp(Entity, ContrastSet, UsedEntities)
1. IF ContrastSet= [] THEN RETURN {Entity.head}
2. Calculate CQ, SQ and DQ for each attribute and
relation of Entity (as in Sec 3.1 and 3.4)
3. Let *preferred* be the list of attributes/ relations
sorted in decreasing order of DQs. FOR each ele-
ment (Mod) of *preferred* DO steps 4, 5 and 6
4. IF ContrastSet = [] THEN RETURN RefExp U
{Entity.head}
5. IF Mod is an Attribute THEN
(a) LET RefExp = {Mod} U RefExp
(b) Remove from ContrastSet, any entities Mod
rules out
6. IF Mod is a Relation [prepi ei] THEN
(a) IF ei E UsedEntities THEN
i. Set DQ = −oo
ii. Move Mod to the end of *preferred*
ELSE
</figure>
<figureCaption confidence="0.448221333333333">
i. LET ContrastSet2 be the set of non-ei en-
tities that are the objects of prepi rela-
tions in members of ContrastSet
</figureCaption>
<figure confidence="0.631574363636364">
ii. LET RE = generate-referring-exp(ei,
ContrastSet2, {ei}UUsedEntities)
iii. recalculate DQ using length = 2 +
length(RE)
iv. IF position in *preferred* is lowered
THEN re-sort *preferred*
ELSE
(a) SET Re xp = Re. . U
{ [prepildeterminerlRE] }
(o) Remove from ContrastSet, any
entities that Mod rules out
</figure>
<sectionHeader confidence="0.866957" genericHeader="method">
7. RETURN RefExp U {Entity.head}
</sectionHeader>
<subsectionHeader confidence="0.59475">
An Example Trace:
</subsectionHeader>
<bodyText confidence="0.860599">
We now trace the algorithm above as it generates a
referring expression for d1 in figure 1.
call generate-ref-exp(d1,[d2],[])
</bodyText>
<listItem confidence="0.83982">
• step 1: ContrastSet is not empty
• step 2: DQsmall = −4, DQgrey = −4
DQ[in b1] = 4/3, DQ[near d2] = 4/4
• step 3: *preferred* = [[in b1], [near d2], small,
</listItem>
<figure confidence="0.9700618">
grey]
head dog
attrib [small,
grey]
outside b1
near d1
head bin
attrib [large, steel]
containing d1
near d2
</figure>
<figureCaption confidence="0.999851">
Figure 1: AVMs for two dogs and a bin
</figureCaption>
<listItem confidence="0.870065">
• Iteration 1 — mod = [in b1]
– step 6(a)i: ContrastSet2 = []
– step 6(a)ii: call generate-ref-exp(b1,[],[d1])
* step 1: ContrastSet = []
return {bin}
– step 6(a)iii: D Q[in b1] = 4/3
– step 6(a)iva: RefExp = {[in, the, {bin}]}
– step 6(a)ivo: ContrastSet = []
• Iteration 2 — mod = [near d2]
</listItem>
<bodyText confidence="0.965978923076923">
– step 4: ContrastSet = []
return {[in the {bin}], dog}
The algorithm presented above is designed to re-
turn the shortest referring expression that uniquely
identifies an entity. If the scene in figure 1 were clut-
tered with bins, the algorithm would still refer to d1
as the dog in the bin as there is only one dog that is
in a bin. The user gets no help in locating the bin.
If helping the user locate entities is important to the
discourse plan, we need to change step 6(a)(ELSE)i
so that the contrast set includes all bins in context,
not just bins that are objects of in relations of dis-
tractors of d1.
</bodyText>
<subsectionHeader confidence="0.983438">
3.6 Compound Nominals
</subsectionHeader>
<bodyText confidence="0.99987525">
Our analysis so far has assumed that attributes are
adjectives. However, many nominals introduced
through relations can also be introduced in com-
pound nominals, for example:
</bodyText>
<listItem confidence="0.993222666666667">
1. a church in Paris H a Paris church
2. a novel by Archer H an Archer novel
3. a company from London H a London company
</listItem>
<figure confidence="0.988473190476191">
d1
d2
b1
head dog
attrib [small,
grey]
in b1
near d2
�
� � �
d1
1
d2
�
� � �
1
�
�
b1 �
�
� �
</figure>
<bodyText confidence="0.936310923076923">
This is an important issue for regeneration appli-
cations, where the AVMs for entities are constructed
from text rather than a semantic knowledge base
(which could be constructed such that such cases
are stored in relational form, though possibly with
an underspecified relation). We need to augment our
algorithm so that it can compare AVMs like:
� head church � �head �
church
in [ head Paris ] and attrib [Paris]
Formally, the algorithm for calculating SQ and
CQ for a nominal attribute a,,,om of entity eo is:
FOR each distractor ei of eo DO
</bodyText>
<listItem confidence="0.968809625">
1. IF anom is similar to any nominal attribute of ei
THEN SQ = SQ + 4
2. IF anom is similar to the head noun of the object
of any relation of ei THEN
(a) SQ = SQ + 4
(b) flatten that relation for ei, i.e., add the at-
tributes of the object of the relation to the at-
tribute list for ei
</listItem>
<bodyText confidence="0.999243727272727">
In step 2, we compare a nominal attribute a,,,om
of eo to the head noun of the object of a relation
of ei. If they are similar, it is likely that any at-
tributes of that object might help distinguish eo from
ei. We then add those attributes to the attribute list
of ei. Now, if SQ is non-zero, the nominal at-
tribute a,,,om has bad discriminating power and we
set DQ = −SQ. If SQ = 0, then a,,,om has good
discriminating power and we set DQ = 4.
We also extend the algorithm for calculating DQ
for a relation [prep] ej] of eo as follows:
</bodyText>
<listItem confidence="0.326695166666667">
1. IF any distractor ei has a nominal attribute anom
THEN
(a) IF anom is similar to the head of ej THEN
i. Add all attributes of eo to the attribute list
and calculate their DQs
2. calculate DQ for the relation as in section 3.4
</listItem>
<bodyText confidence="0.97219725">
We can demonstrate how this approach works us-
ing entities extracted from the following sentence
(from the Wall Street Journal):
Also contributing to the firmness in copper, the
analyst noted, was a report by Chicago pur-
chasing agents, which precedes the full pur-
chasing agents report that is due out today and
gives an indication of what the full report might
hold.
Consider generating a referring expression for eo
when the distractor is el:
head report
</bodyText>
<equation confidence="0.867057714285714">
� �
head agents
by �attrib [Chicago, �
purchasing]
� head �
report
e1 = attributes [full, purchasing, agents]
</equation>
<bodyText confidence="0.998435666666667">
The distractor the full purchasing agents report
contains the nominal attribute agents. To compare
report by Chicago purchasing agents with full pur-
chasing agents report, our algorithm flattens the for-
mer to Chicago purchasing agents report. Our algo-
rithm now gives:
</bodyText>
<equation confidence="0.9380115">
DQagents = −4, DQpurchasing = −4,
DQChicago = 4, DQby Chicagopurchasing agents = 4/4
</equation>
<bodyText confidence="0.9994371">
We thus generate the referring expression the
Chicago report. This approach takes advantage of
the flexibility of the relationships that can hold be-
tween nouns in a compound: although examples can
be devised where removing a nominal causes un-
grammaticality, it works well enough empirically.
To generate a referring expression for e1 (full
purchasing agents report) when the distractor is
eo(report by Chicago purchasing agents), our algo-
rithm again flattens eo to obtain:
</bodyText>
<equation confidence="0.8342175">
DQagents = −4, DQpurchasing = −4
DQfull = 4
</equation>
<bodyText confidence="0.999881666666667">
The generated referring expression is the full report.
This is identical to the referring expression used in
the original text.
</bodyText>
<sectionHeader confidence="0.997222" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999925333333333">
As our algorithm works in open domains, we were
able to perform a corpus-based evaluation using the
Penn WSJ Treebank (Marcus et al., 1993). Our eval-
uation aimed to reproduce existing referring expres-
sions (NPs with a definite determiner) in the Penn
Treebank by providing our algorithm as input:
</bodyText>
<sectionHeader confidence="0.7174185" genericHeader="method">
1. The first mention NP for that reference.
2. The contrast set of distractor NPs
</sectionHeader>
<bodyText confidence="0.998756083333333">
For each referring expression (NP with a definite
determiner) in the Penn Treebank, we automatically
identified its first mention and all its distractors in a
four sentence window, as described in §4.1. We then
used our program to generate a referring expres-
sion for the first mention NP, giving it a contrast-
set containing the distractor NPs. Our evaluation
compared this generated description with the orig-
inal WSJ reference that we had started out with.
Our algorithm was developed using toy examples
and counter-examples constructed by hand, and the
Penn Treebank was unseen data for this evaluation.
</bodyText>
<subsectionHeader confidence="0.99929">
4.1 Identifying Antecedents and Distractors
</subsectionHeader>
<bodyText confidence="0.999978333333333">
For every definite noun phrase NPo in the Penn
Treebank, we shortlisted all the noun phrases NPi
in a discourse window of four sentences (the two
</bodyText>
<equation confidence="0.943512666666667">
�
� � �
eo =
</equation>
<bodyText confidence="0.99369092">
preceding sentences, current sentence and the fol-
lowing sentence) that had a head noun identical to
or a WordNet synonym of the head noun of NPo.
We compared the set of attributes and relations
for each shortlisted NPi that preceded NPo in the
discourse window with that of NPo. If the attributes
and relations set of NPi was a superset of that of
NPo, we assumed that NPo referred to NPi and
added NPi to an antecedent set. We added all other
NPi to the contrast set of distractors.
Similarly, we excluded any noun phrase NPi that
appeared in the discourse after NPo whose attributes
and relations set was a subset of NPo’s and added
the remaining NPi to the contrast set. We then se-
lected the longest noun phrase in the antecedent set
to be the antecedent that we would try and generate
a referring expression from.
The table below gives some examples of distrac-
tors that our program found using WordNet syn-
onyms to compare head nouns:
Entity Distractors
first half-free Soviet vote fair elections in the GDR
military construction bill fiscal measure
steep fall in currency drop in market stock
permanent insurance death benefit coverage
</bodyText>
<sectionHeader confidence="0.800882" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.998414">
There were 146 instances of definite descriptions in
the WSJ where the following conditions (that ensure
that the referring expression generation task is non-
trivial) were satisfied:
</bodyText>
<reference confidence="0.4987838">
1. The definite NP (referring expression) contained at
least one attribute or relation.
2. An antecedent was found for the definite NP.
3. There was at least one distractor NP in the dis-
course window.
</reference>
<bodyText confidence="0.98526232">
In 81.5% of these cases, our program returned a
referring expression that was identical to the one
used in the WSJ. This is a surprisingly high accu-
racy, considering that there is a fair amount of vari-
ability in the way human writers use referring ex-
pressions. For comparison, the baseline of repro-
ducing the antecedent NP performed at 48%2.
Some errors were due to non-recognition of mul-
tiword expessions in the antecedent (for example,
our program generated care product from personal
care product). In many of the remaining error cases,
it was difficult to decide whether what our pro-
gram generated was acceptable or wrong. For ex-
ample, the WSJ contained the referring expression
the one-day limit, where the automatically detected
antecedent was the maximum one-day limit for the
2We are only evaluating content selection (the nouns and
pre- and post-modifiers) and ignore determiner choice.
S&amp;P 500 stock-index futures contract and the auto-
matically detected contrast set was:
{the five-point opening limit for the contract,
the 12-point limit, the 30-point limit, the in-
termediate limit of 20 points}
Our program generated the maximum limit, where
the WSJ writer preferred the one-day limit.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="method">
5 Further Issues
</sectionHeader>
<subsectionHeader confidence="0.997312">
5.1 Reference Modifying Attributes
</subsectionHeader>
<bodyText confidence="0.999958588235294">
The analysis thus far has assumed that all at-
tributes modify the referent rather than the refer-
ence to the referent. However, for example, if e1
is an alleged murderer, the attribute alleged mod-
ifies the reference murderer rather than the refer-
ent e1 and referring to e1 as the murderer would
be factually incorrect. Logically e1 could be rep-
resented as (alleged1(murderer1))(x), rather than
alleged1(x) n murderer1(x). This is no longer
first-order, and presents new difficulties for the tra-
ditional formalisation of the reference generation
problem. One (inelegant) solution would be to in-
troduce a new predicate allegedMurderer1(x).
A working approach in our framework would be
to add a large positive weight to the DQs of refer-
ence modifying attributes, thus forcing them to be
selected in the referring expression.
</bodyText>
<subsectionHeader confidence="0.997173">
5.2 Discourse Context and Salience
</subsectionHeader>
<bodyText confidence="0.999980333333333">
The incremental algorithm assumes the availability
of a contrast set and does not provide an algorithm
for constructing and updating it. The contrast set, in
general, needs to take context into account. Krah-
mer and Theune (2002) propose an extension to the
IA which treats the context set as a combination of a
discourse domain and a salience function. The black
dog would then refer to the most salient entity in the
discourse domain that is both black and a dog.
Incorporating salience into our algorithm is
straightforward. As described earlier, we compute
the quotients SQ and CQ for each attribute or re-
lation by adding an amount w E {4,2, 1} to the
relevant quotient based on a comparison with the at-
tributes and relations of each distractor. We can in-
corporate salience by weighting w with the salience
of the distractor whose attribute or relation we are
considering. This will result in attributes and rela-
tions with high discriminating power with regard to
more salient distractors getting selected first in the
incremental process.
</bodyText>
<subsectionHeader confidence="0.959996">
5.3 Discourse Plans
</subsectionHeader>
<bodyText confidence="0.999938652173913">
In many situations, attributes and relations serve dif-
ferent discourse functions. For example, attributes
might be used to help the hearer identify an entity
while relations might serve to help locate the en-
tity. This needs to be taken into account when gen-
erating a referring expression. If we were gener-
ating instructions for using a machine, we might
want to include both attributes and relations; so to
instruct the user to switch on the power, we might
say switch on the red button on the top-left corner.
This would help the user locate the switch (on the
top-left corner) and identify it (red). If we were
helping a chef find the salt in a kitchen, we might
want to use only relations because the chef knows
what salt looks like. The salt behind the corn flakes
on the shelf above the fridge is in this context prefer-
able to the white powder. If the discourse plan that
controls generation requires our algorithm to pref-
erentially select relations or attributes, it can add a
positive amount a to their DQs. Then, the resultant
formula is DQ = (CQ − SQ)/length + a, where
length = 1 for attributes and by default a = 0 for
both relations and attributes.
</bodyText>
<sectionHeader confidence="0.999054" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999988363636364">
We have described an algorithm for generating re-
ferring expressions that can be used in any domain.
Our algorithm selects attributes and relations that
are distinctive in context. It does not rely on the
availability of an adjective classification scheme and
uses WordNet antonym and synonym lists instead.
It is also, as far as we know, the first algorithm that
allows for the incremental incorporation of relations
and the first that handles nominals. In a novel eval-
uation, our algorithm successfully generates identi-
cal referring expressions to those in the Penn WSJ
Treebank in over 80% of cases.
In future work, we plan to use this algorithm as
part of a system for generation from a database of
user opinions on products which has been automat-
ically extracted from newsgroups and similar text.
This is midway between regeneration and the clas-
sical task of generating from a knowledge base be-
cause, while the database itself provides structure,
many of the field values are strings corresponding
to phrases used in the original text. Thus, our lexi-
calised approach is directly applicable to this task.
</bodyText>
<sectionHeader confidence="0.999294" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998216625">
Thanks are due to Kees van Deemter and three
anonymous ACL reviewers for useful feedback on
prior versions of this paper.
This document was generated partly in the con-
text of the Deep Thought project, funded under
the Thematic Programme User-friendly Information
Society of the 5th Framework Programme of the Eu-
ropean Community (Contract N IST-2001-37836)
</bodyText>
<sectionHeader confidence="0.995972" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999525226415094">
Robert Dale and Nicholas Haddock. 1991. Gen-
erating referring expressions involving relations.
In Proceedings of the 5th Conference of the Eu-
ropean Chapter of the Association for Compu-
tational Linguistics (EACL’91), pages 161–166,
Berlin, Germany.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gen-
eration of referring expressions. Cognitive Sci-
ence, 19:233–263.
Helmut Horacek. 2003. A best-first search algo-
rithm for generating referring expressions. In
Proceedings of the 11th Conference of the Eu-
ropean Chapter of the Association for Compu-
tational Linguistics (EACL’03), pages 103–106,
Budapest, Hungary.
Emiel Krahmer and Mari¨et Theune. 2002. Efficient
context-sensitive generation of referring expres-
sions. In Kees van Deemter and Rodger Kib-
ble, editors, Information Sharing: Givenness and
Newness in Language Processing, pages 223–
264. CSLI Publications, Stanford,California.
Emiel Krahmer, Sebastiaan van Erk, and Andr´e
Verleg. 2003. Graph-based generation of re-
ferring expressions. Computational Linguistics,
29(1):53–72.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large natural
language corpus of English: The Penn Treebank.
Computational Linguistics, 19:313–330.
George A. Miller, Richard Beckwith, Christiane D.
Fellbaum, Derek Gross, and Katherine Miller.
1993. Five Papers on WordNet. Technical report,
Princeton University, Princeton, N.J.
Ehud Reiter. 1990. The computational complex-
ity of avoiding conversational implicatures. In
Proceedings of the 28th Annual Meeting of Asso-
ciation for Computational Linguistics (ACL’90),
pages 97–104, Pittsburgh, Pennsylvania.
Ehud Reiter and Robert Dale. 1992. A fast al-
gorithm for the generation of referring expres-
sions. In Proceedings of the 14th International
Conference on Computational Linguistics (COL-
ING’92), pages 232–238, Nantes, France.
Kees van Deemter. 2000. Generating vague de-
scriptions. In Proceedings of the 1st Interna-
tional Conference on Natural Language Genera-
tion (INLG’00), pages 179–185, Mitzpe Ramon,
Israel.
Kees van Deemter. 2002. Generating referring ex-
pressions: Boolean extensions of the incremental
algorithm. Computational Linguistics, 28(1):37–
52.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573587">
<title confidence="0.999887">Generating Referring Expressions in Open Domains</title>
<author confidence="0.991815">Advaith Siddharthan Ann Copestake</author>
<affiliation confidence="0.9998925">Computer Science Department Computer Laboratory Columbia University University of Cambridge</affiliation>
<abstract confidence="0.971371266666667">We present an algorithm for generating referring expressions in open domains. Existing algorithms work at the semantic level and assume the availability of a classification for attributes, which is only feasible for restricted domains. Our alternative works at the realisation level, relies on Word- Net synonym and antonym sets, and gives equivalent results on the examples cited in the literature and improved results for examples that prior approaches cannot handle. We believe that ours is also the first algorithm that allows for the incremental incorporation of relations. We present a novel corpus-evaluation using referring expressions from the Penn Wall Street Journal Treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>The definite NP (referring expression) contained at least one attribute or relation. 2. An antecedent was found for the definite NP.</title>
<marker></marker>
<rawString>1. The definite NP (referring expression) contained at least one attribute or relation. 2. An antecedent was found for the definite NP.</rawString>
</citation>
<citation valid="false">
<title>There was at least one distractor NP in the discourse window.</title>
<marker></marker>
<rawString>3. There was at least one distractor NP in the discourse window.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Nicholas Haddock</author>
</authors>
<title>Generating referring expressions involving relations.</title>
<date>1991</date>
<booktitle>In Proceedings of the 5th Conference of the European Chapter of the Association for Computational Linguistics (EACL’91),</booktitle>
<pages>161--166</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="15340" citStr="Dale and Haddock (1991)" startWordPosition="2501" endWordPosition="2504">istractor, it is quadratic in n. The IA compares each attribute of the discourse referent to only one attribute per distractor and is linear in n. Note, however, that values for n of over 4 are rare. 3.3 Relations Semantically, attributes describe an entity (e.g., the small grey dog) and relations relate an entity to other entities (e.g., the dog in the bin). Relations are troublesome because in relating an entity e,, to el, we need to recursively generate a referring expression for el. The IA does not consider relations and the referring expression is constructed out of attributes alone. The Dale and Haddock (1991) algorithm allows for relational descriptions but involves exponential global search, or a greedy search approximation. To incorporate relational descriptions in the incremental framework would require a classification system which somehow takes into account the relations themselves and the secondary entities el etc. This again suggests that the existing algorithms force the incrementality at the wrong stage in the generation process. Our approach computes the order in which attributes are incorporated after observing the context, by quantifying their utility through the quotient DQ. This make</context>
</contexts>
<marker>Dale, Haddock, 1991</marker>
<rawString>Robert Dale and Nicholas Haddock. 1991. Generating referring expressions involving relations. In Proceedings of the 5th Conference of the European Chapter of the Association for Computational Linguistics (EACL’91), pages 161–166, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<pages>19--233</pages>
<contexts>
<context position="6706" citStr="Dale and Reiter (1995)" startWordPosition="1046" endWordPosition="1049">vel is risky because their linguistic realisation might be ambiguous and many common adjectives are polysemous (cf., example 1 in §3.1). Reference modification, which has not been considered in the referring expression generation literature, raises further issues; for example, referring to an alleged murderer as the murderer is potentially libellous. In addition to the above, there is the issue of overlap between values of attributes. The case of subsumption (for example, that the colour red subsumes crimson and the type dog subsumes chihuahua) has received formal treatment in the literature; Dale and Reiter (1995) provide a find-bestvalue function that evaluates tree-like hierarchies of values. As mentioned earlier, such hierarchical knowledge bases do not exist for open domains. Further, a treatment of subsumption is insufficient, and degrees of intersection between attribute values also require consideration. van Deemter (2000) discusses the generation of vague descriptions when entities have gradable attributes like size; for example, in a domain with four mice sized 2, 5, 7 and 10cm, it is possible to refer to the large mouse (the mouse sized 10cm) or the two small mice (the mice sized 2 and 5cm). </context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19:233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>A best-first search algorithm for generating referring expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’03),</booktitle>
<pages>103--106</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="4725" citStr="Horacek (2003)" startWordPosition="741" endWordPosition="742">referred-attributes* list is [size, colour,...], the algorithm would first compare the values of the size attribute (both large), type dog size large colour brown � � 1 disregard that attribute as not being discriminating, compare the values of the colour attribute and return the brown dog. Subsequent work on referring expression generation has expanded the logical framework to allow reference by negation (the dog that is not black) and references to multiple entities (the brown or black dogs) (van Deemter, 2002), explored different search algorithms for finding the minimal description (e.g., Horacek (2003)) and offered different representation frameworks like graph theory (Krahmer et al., 2003) as alternatives to AVMs. However, all these approaches are based on very similar formalisations of the problem, and all make the following assumptions: 1. A semantic representation exists. 2. A classification scheme for attributes exists. 3. The linguistic realisations are unambiguous. 4. Attributes cannot be reference modifying. All these assumptions are violated when we move from generation in a very restricted domain to regeneration in an open domain. In regeneration tasks such as summarisation, open-</context>
</contexts>
<marker>Horacek, 2003</marker>
<rawString>Helmut Horacek. 2003. A best-first search algorithm for generating referring expressions. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’03), pages 103–106, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
</authors>
<title>Efficient context-sensitive generation of referring expressions.</title>
<date>2002</date>
<booktitle>Information Sharing: Givenness and Newness in Language Processing,</booktitle>
<pages>223--264</pages>
<editor>In Kees van Deemter and Rodger Kibble, editors,</editor>
<publisher>CSLI Publications, Stanford,California.</publisher>
<marker>Krahmer, Theune, 2002</marker>
<rawString>Emiel Krahmer and Mari¨et Theune. 2002. Efficient context-sensitive generation of referring expressions. In Kees van Deemter and Rodger Kibble, editors, Information Sharing: Givenness and Newness in Language Processing, pages 223– 264. CSLI Publications, Stanford,California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andr´e Verleg</author>
</authors>
<title>Graph-based generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg. 2003. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Marcinkiewicz</author>
</authors>
<title>Building a large natural language corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="26176" citStr="Marcus et al., 1993" startWordPosition="4422" endWordPosition="4425">though examples can be devised where removing a nominal causes ungrammaticality, it works well enough empirically. To generate a referring expression for e1 (full purchasing agents report) when the distractor is eo(report by Chicago purchasing agents), our algorithm again flattens eo to obtain: DQagents = −4, DQpurchasing = −4 DQfull = 4 The generated referring expression is the full report. This is identical to the referring expression used in the original text. 4 Evaluation As our algorithm works in open domains, we were able to perform a corpus-based evaluation using the Penn WSJ Treebank (Marcus et al., 1993). Our evaluation aimed to reproduce existing referring expressions (NPs with a definite determiner) in the Penn Treebank by providing our algorithm as input: 1. The first mention NP for that reference. 2. The contrast set of distractor NPs For each referring expression (NP with a definite determiner) in the Penn Treebank, we automatically identified its first mention and all its distractors in a four sentence window, as described in §4.1. We then used our program to generate a referring expression for the first mention NP, giving it a contrastset containing the distractor NPs. Our evaluation c</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large natural language corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane D Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Five Papers on WordNet.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Princeton University,</institution>
<location>Princeton, N.J.</location>
<contexts>
<context position="5821" citStr="Miller et al., 1993" startWordPosition="906" endWordPosition="909">generation in a very restricted domain to regeneration in an open domain. In regeneration tasks such as summarisation, open-ended question answering and text simplification, AVMs for entities are typically constructed from noun phrases, with the head noun as the type and pre-modifiers as attributes. Converting words into semantic labels would involve sense disambiguation, adding to the cost and complexity of the analysis module. Also, attribute classification is a hard problem and there is no existing classification scheme that can be used for open domains like newswire; for example, WordNet (Miller et al., 1993) organises adjectives as concepts that are related by the non-hierarchical relations of synonymy and antonymy (unlike nouns that are related through hierarchical links such as hyponymy, hypernymy and metonymy). In addition, selecting attributes at the semantic level is risky because their linguistic realisation might be ambiguous and many common adjectives are polysemous (cf., example 1 in §3.1). Reference modification, which has not been considered in the referring expression generation literature, raises further issues; for example, referring to an alleged murderer as the murderer is potenti</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane D. Fellbaum, Derek Gross, and Katherine Miller. 1993. Five Papers on WordNet. Technical report, Princeton University, Princeton, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>The computational complexity of avoiding conversational implicatures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of Association for Computational Linguistics (ACL’90),</booktitle>
<pages>97--104</pages>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="14294" citStr="Reiter (1990)" startWordPosition="2327" endWordPosition="2328">evious section). Averaged over contexts, some attributes have more discriminating power than others (largely because of the way we visualise entities) and premise 2 is an approximation to our approach. We now quantify the extra effort we are making to identify attributes that “stand out” in a given context. Let N be the maximum number of entities in e1 e2 r head president II head president 1 L attrib old, current J L attrib young, past J the contrast set and n be the maximum number of attributes per entity. The table below compares the computational complexity of an optimal algorithm (such as Reiter (1990)), our algorithm and the IA. Incremental Algo Our Algorithm Optimal Algo O(nN) O(n2N) O(n2N) Both the IA and our algorithm are linear in the number of entities N. This is because neither algorithm allows backtracking; an attribute, once selected, cannot be discarded. In contrast, an optimal search requires O(2N) comparisons. As our algorithm compares each attribute of the discourse referent to every attribute of every distractor, it is quadratic in n. The IA compares each attribute of the discourse referent to only one attribute per distractor and is linear in n. Note, however, that values for</context>
</contexts>
<marker>Reiter, 1990</marker>
<rawString>Ehud Reiter. 1990. The computational complexity of avoiding conversational implicatures. In Proceedings of the 28th Annual Meeting of Association for Computational Linguistics (ACL’90), pages 97–104, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>A fast algorithm for the generation of referring expressions.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING’92),</booktitle>
<pages>232--238</pages>
<location>Nantes, France.</location>
<contexts>
<context position="2228" citStr="Reiter and Dale (1992)" startWordPosition="336" endWordPosition="339"> other entities in the discourse domain. For example, if there were a small brown dog (small1(x) n brown1(x) n dog1(x)) in context, the minimal description for the big brown dog would be big1(x) n dog1(x)1. This semantic framework makes it difficult to apply existing referring expression generation algorithms to the many regeneration tasks that are important today; for example, summarisation, openended question answering and text simplification. Unlike in traditional generation, the starting point in &apos;The predicate dog1 is selected because it has a distinguished status, referred to as type in Reiter and Dale (1992). One such predicate has to to be present in the description. these tasks is unrestricted text, rather than a semantic representation of a small domain. It is difficult to extract the required semantics from unrestricted text (this task would require sense disambiguation, among other issues) and even harder to construct a classification for the extracted predicates in the manner that existing approaches require (cf., §2). In this paper, we present an algorithm for generating referring expressions in open domains. We discuss the literature and detail the problems in applying existing approaches</context>
</contexts>
<marker>Reiter, Dale, 1992</marker>
<rawString>Ehud Reiter and Robert Dale. 1992. A fast algorithm for the generation of referring expressions. In Proceedings of the 14th International Conference on Computational Linguistics (COLING’92), pages 232–238, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>Generating vague descriptions.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Conference on Natural Language Generation (INLG’00),</booktitle>
<pages>179--185</pages>
<location>Mitzpe Ramon,</location>
<marker>van Deemter, 2000</marker>
<rawString>Kees van Deemter. 2000. Generating vague descriptions. In Proceedings of the 1st International Conference on Natural Language Generation (INLG’00), pages 179–185, Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>Generating referring expressions: Boolean extensions of the incremental algorithm.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>52</pages>
<marker>van Deemter, 2002</marker>
<rawString>Kees van Deemter. 2002. Generating referring expressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28(1):37– 52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>