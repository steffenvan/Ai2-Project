<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000129">
<title confidence="0.9994045">
A Two-Stage Approach to Retrieving Answers for How-To
Questions
</title>
<author confidence="0.99072">
Ling Yin
</author>
<affiliation confidence="0.994088">
CMIS, University of Brighton,
</affiliation>
<address confidence="0.987202">
Brighton, BN2 4GJ, United Kingdom
</address>
<email confidence="0.999666">
Y.Ling@brighton.ac.uk
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930137931034">
This paper addresses the problem of
automatically retrieving answers for
how-to questions, focusing on those that
inquire about the procedure for
achieving a specific goal. For such
questions, typical information retrieval
methods, based on key word matching,
are better suited to detecting the content
of the goal (e.g., ‘installing a Windows
XP server’) than the general nature of the
desired information (i.e., procedural, a
series of steps for achieving this goal).
We suggest dividing the process of
retrieving answers for such questions
into two stages, with each stage focusing
on modeling one aspect of a how-to
question. We compare the two-stage
approach with two alternative
approaches: a baseline approach that
only uses the content of the goal to
retrieve relevant documents and another
approach that explores the potential of
automatic query expansion. The result of
the experiment shows that the two-stage
approach significantly outperforms the
baseline but achieves similar result with
the systems using automatic query
expansion techniques. We analyze the
reason and also present some future work.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957979591837">
How-To questions constitute a large proportion
of questions on the Web. Many how-to questions
inquire about the procedure for achieving a
specific goal. For such questions, typical
information retrieval (IR) methods, based on key
word matching, are better suited to detecting the
content of the goal (e.g., installing a Windows
XP server) than the general nature of the desired
information (i.e., procedural, a series of steps for
achieving this goal). The reasons are given as
below.
First, documents that describe a procedure
often do not contain the word ‘procedure’ itself,
but we are able to abstract the concept
‘procedure’ from cues such as ‘first’, ‘next’ and
‘then’, all of which indicate sequential
relationships between actions. Secondly, We
expect that the word ‘procedure’ or the phrase
‘how to’ will occur in a much broader context
than the words in the goal. In other words, a
document that contains the words in the goal is
more likely to be relevant than a document that
contains the word ‘procedure’ or the phrase ‘how
to’. Without noticing this difference, treating the
two parts equally in the retrieving process will
get many noisy documents.
Many information requests seem to show such
a structure, with one part identifying a specific
topic and another part constraining the kind of
information required about this topic (Yin and
Power, 2005). The second part is often omitted
when selecting retrieval terms from the request to
construct an effective query for an IR system,
such as in Picard (1999).
The first point given above suggests that using
cues such as ‘first’ and ‘next’ to expand the
initial query may help in retrieving more relevant
documents. Expansion terms can be generated
automatically by query expansion techniques.
The typical process is: (1) use the initial query to
retrieve documents (referred to as the first round
of retrieval); (2) consider a few top ranked
documents as relevant and the rest irrelevant; (3)
compare the relevant set with the irrelevant set to
extract a list of most distinctive terms; (4) use the
extracted terms to retrieve documents (referred to
as the second round of retrieval).
However, query expansion may not constitute
a good solution, because its effectiveness largely
</bodyText>
<page confidence="0.998928">
63
</page>
<bodyText confidence="0.9998627">
depends on the quality of the few top ranked
documents retrieved in the first round when the
aforementioned two problems are not yet
tackled.
Our solution is to divide the process of
retrieving answers for such questions into two
stages: (1) use typical IR approaches for
retrieving documents that are relevant to the
specific goal; (2) use a text categorization
approach to re-rank the retrieved documents
according to the proportion of procedural text
they contain. By ‘procedural text’ we refer to
ordered lists of steps, which are very common in
some instructional genres such as online manuals.
In this report, we will briefly introduce the
text categorization approach (details are
presented in (Yin and Power, 2006) ) and will
explain in more concrete terms how it is
integrated into the two-stage architecture
proposed above. We will compare the
performance of our two-stage architecture with a
baseline system that uses only the content of the
goal to retrieve relevant documents (equivalent
to the first stage in the two-stage architecture).
We will also compare the two-stage approach
with systems that applies automatic query
expansion techniques.
This paper is organized as follows. Section 2
introduces some relevant work in IR and
question answering (QA). Section 3 talks about
the text categorization approach for ranking
procedural documents, covering issues such as
the features used, the training corpus, the design
of a classification model as well as some
experiments for evaluation. Section 4 talks about
integrating the text categorizer into the two-stage
architecture and presents some experiments on
retrieving relevant documents for how-to
questions. Section 5 provides a short summary
and presents some future work.
</bodyText>
<sectionHeader confidence="0.999891" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999895477611941">
The idea of applying text categorization
technology to help information retrieval is not
new. In particular, text categorization techniques
are widely adopted to filter a document source
according to specific information needs. For
example, Stricker et al. (2000) experiment on
several news resources to find news addressing
specific topics. They present a method for
automatically generating “discriminant terms”
(Stricker et al., 2000) for each topic that are then
used as features to train a neural network
classifier. Compared to these approaches, the
novelty of our study lies in the idea that an
information request consists of two different
parts that should be retrieved in different ways
and the whole retrieval process should adopt a
two-stage architecture.
A research area that is closely related to IR is
question answering (QA), the differences being
a) the input of a QA system is a question rather
than a few key words; b) a QA system aims to
extract answers to a question rather than
retrieving relevant documents only. Most QA
systems do adopt a two-stage architecture (if not
consider the initial question analysis stage), i.e.,
perform IR with a few content words extracted
from the query to locate documents likely to
contain an answer and then use information
extraction (IE) to find the text snippets that
match the question type (Hovy et al., 2001;
Elworthy, 2000). However, most question
answering systems target factoid questions – the
research of non-factoid questions started only a
few years ago but limited to several kinds, such
as definitional questions (Xu et al., 2003) and
questions asking for biographies (Tsur et al.,
2004).
Only a few studies have addressed procedural
questions. Murdok and Croft (2002) distinguish
between “task-oriented questions” (i.e., ask about
a process) and “fact-oriented questions” (i.e., ask
about a fact) and present a method to
automatically classify questions into these two
categories. Following this work, Kelly et al.
(2002) explore the difference between documents
that contain relevant information to the two
different types of questions. They conclude,
“lists and FAQs occur in more documents judged
relevant to task-oriented questions than those
judged relevant to fact-oriented questions” (Kelly
et al., 2002: 645) and suggest, “retrieval
techniques specific to each type of question
should be considered” (Kelly et al., 2002: 647).
Schwitter et al. (2004) present a method to
extract answers from technical documentations
for How-questions. To identify answers, they
match the logical form of a sentence against that
of the question and also explore the
typographical conventions in technical domains.
The work that most resembles ours is Takechi et
al. (2003), which uses word n-grams to classify
(as procedural or non-procedural) list passages
extracted using HTML tags. Our approach,
however, applies to whole documents, the aim
being to measure the degree of procedurality —
i.e., the proportion of procedural text they
contain.
</bodyText>
<page confidence="0.999617">
64
</page>
<sectionHeader confidence="0.992444" genericHeader="method">
3 Ranking Procedural Texts
</sectionHeader>
<bodyText confidence="0.999952285714286">
Three essential elements of a text categorization
approach are the features used to represent the
document, the training corpus and the machine
learning method, which will be described in
section 3.1, 3.2 and 3.3 respectively. Section 3.4
presents experiments on applying the learned
model to rank documents in a small test set.
</bodyText>
<subsectionHeader confidence="0.860619666666667">
3.1 Feature Selection and Document
Representation
Linguistic Features and Cue Phrases
</subsectionHeader>
<bodyText confidence="0.99992996">
We targeted six procedural elements: actions,
times, sequence, conditionals, preconditions, and
purposes. These elements can be recognized
using linguistic features or cue phrases. For
example, an action is often conveyed by an
imperative; a precondition can be expressed by
the cue phrase ‘only if’. We used all the
syntactic and morphological tags defined in
Connexor’s syntax analyzer1. There are some
redundant tags in this set. For example, both the
syntactic tag ‘@INFMARK&gt;’ and the
morphological tag ‘INFMARK&gt;’ refer to the
infinitive marker ‘to’ and therefore always occur
together at the same time. We calculated the
Pearson’s product-moment correlation
coefficient (r) (Weisstein, 1999) between any
two tags based on their occurrences in sentences
of the whole training set. We removed one in
each pair of strongly correlated tags and finally
got 34 syntactic tags and 34 morphological tags.
We also handcrafted a list of relevant cue
phrases (44), which were extracted from
documents by using the Flex tool2 for pattern
matching. Some sample cue phrases and the
matching patterns are shown in table 1.
</bodyText>
<subsectionHeader confidence="0.590069">
Procedural Cue Phrase Pattern
Element
</subsectionHeader>
<table confidence="0.9619944">
Precondition ‘only if’ [Oo]nly[[:space:]]if[[:space:]]
Purpose ‘so that’ [sS]o[[:space:]]that[[:space:]]
Condition ‘as long as’ ([Aa]s) [[:space:]]long[[:space:]]as[[:space:]]
Sequence ‘first’ [fF]irst [[:space:][:punct:]]
Time ‘now’ [nN]ow[[:space:][:punct:]]
</table>
<tableCaption confidence="0.97393">
Table 1. Sample cue phrases and matching
</tableCaption>
<bodyText confidence="0.945138833333333">
patterns.
Modeling Inter-Sentential Feature Co-
occurrence
Some cue phrases are ambiguous and therefore
cannot reliably suggest a procedural element.
For example, the cue phrase ‘first’ can be used to
</bodyText>
<footnote confidence="0.713044">
1 Refer to http://www.connexor.com/
2 Refer to http://www.gnu.org/software/flex/flex.html
</footnote>
<bodyText confidence="0.999745384615385">
represent a ranking order or a spatial relationship
as well as a sequential order. However, it is more
likely to represent a sequential order between
actions if there is also an imperative in the same
sentence. Indeed, sentences that contain both an
ordinal number and an imperative are very
frequent in procedural texts. We compared
between the procedural training set and the non-
procedural training set to extract distinctive
feature co-occurrence patterns, each of which has
only 2 features. The formulae used to rank
patterns with regard to their distinctiveness can
be found in (Yin and Power, 2006).
</bodyText>
<subsectionHeader confidence="0.977074">
Document Representation
</subsectionHeader>
<bodyText confidence="0.951449">
Each document was represented as a vector
dj = {x1j , x2 j , ... , xNj } , where xij represents the
number of sentences in the document that
contains a particular feature normalized by the
document length. We compare the effectiveness
of using individual features ( xij refers to either a
single linguistic feature or a cue phrases) and of
using feature co-occurrence patterns ( xij refers to
a feature co-occurrence pattern).
</bodyText>
<subsectionHeader confidence="0.998083">
3.2 Corpus Preparation
</subsectionHeader>
<bodyText confidence="0.999944615384615">
Pagewise 3 provides a list of subject-matter
domains, ranging from household issues to arts
and entertainment. We downloaded 1536
documents from this website (referred to
hereafter as the Pagewise collection). We then
used some simple heuristics to select documents
from this set to build the initial training corpus.
Specifically, to build the procedural set we chose
documents with titles containing key phrases
‘how to’ and ‘how can I’ (209 web documents);
to build the non-procedural set, we chose
documents which did not include these phrases
in their titles, and which also had no phrases like
‘procedure’ and ‘recipe’ within the body of the
text (208 web documents).
Samples drawn randomly from the procedural
set (25) and non-procedural set (28) were
submitted to two human judges, who assigned
procedurality scores from 1 (meaning no
procedural text at all) to 5 (meaning over 90%
procedural text). The Kendall tau-b agreement
(Kendall, 1979) between the two rankings was
0.821. Overall, the average scores for the
procedural and non-procedural samples were
3.15 and 1.38. We used these 53 sample
documents as part of the test set and the
</bodyText>
<sectionHeader confidence="0.519298" genericHeader="method">
3 Refer to http://www.essortment.com
</sectionHeader>
<page confidence="0.998738">
65
</page>
<bodyText confidence="0.99974925">
remaining documents as the initial training set
(184 procedural and 180 non-procedural).
This initial training corpus is far from ideal:
first, it is small in size; a more serious problem is
that many positive training examples do not
contain a major proportion of procedural text. In
our experiments, we used this initial training set
to bootstrap a larger training set.
</bodyText>
<subsectionHeader confidence="0.999304">
3.3 Learning Method
</subsectionHeader>
<bodyText confidence="0.982475564102564">
Although shown to be not so effective in some
previous studies (Yang, 1999; Yang and Liu,
1999), Naive Bayes classifier is one of the most
commonly-used classifiers for text
categorization. Here we introduce a model
adapted from the Naive Bayes classifier from the
weka-3-4 package (Witten and Frank, 2000).
The Naive Bayes classifier scores a document
dj according to whether it is a typical member
of its set — i.e., the probability of randomly
picking up a document like it from the
procedural class ( p(dj  |C= procedural) ). This
probability is estimated from the training corpus.
As mentioned before, the average procedural
score of the procedural training set is low.
Therefore, there is obviously a danger that a true
procedural document will be ranked lower than a
document that contains less procedural texts
when using this training set to train a Naive
Bayes classifier. Although our procedural
training set is not representative of the
procedural class, by comparing it with the non-
procedural training set, we are able to tell the
difference between procedural documents and
non-procedural documents. We adapted the
Naive Bayes classifier to focus on modeling the
difference between the two classes. For example,
if the procedural training set has a higher
average value on feature Xi than the non-
procedural training set, we inferred that a
document with a higher feature value on Xi
should be scored higher. To reflect this rule, we
scored a document dj by the probability of
picking a document with a lower feature value
from the procedural class (i.e.,
p(Xi &lt; xij  |C = procedural) ). Again this
probability is estimated from the training set.
The new model will be referred to hereafter as
the Adapted Naive Bayes classifier. The details
of this new model can be found in (Yin and
Power, 2006).
3.4 Experiments on Ranking Procedural
Texts
There are two sources from which we compiled
the training and testing corpora: the Pagewise
collection and the SPIRIT collection. The
SPIRIT collection contains a terabyte of HTML
that are crawled from the web starting from an
initial seed set of a few thousands universities
and other educational organizations (Clarke et
al., 1998).
Our test set contained 103 documents,
including the 53 documents that were sampled
previously and then separated from the initial
training corpus, another 30 documents randomly
chosen from the Pagewise collection and 20
documents chosen from the SPIRIT collection.
We asked two human subjects to score the
procedurality for these documents, following the
same instruction described in section 3.2. The
correlation coefficient (Kendall tau-b) between
the two rankings was 0.725, which is the upper
bound of the performance of the classifiers.
We first used the initial training corpus to
bootstrap a larger training set (378 procedural
documents and 608 non-procedural documents),
which was then used to select distinctive feature
co-occurrence patterns and to train different
classifiers. We compared the Adapted Naive
Bayes classifier with the Naive Bayes classifier
and three other classifiers, including Maximum
Entropy (ME) 4 , Alternating Decision Tree
(ADTree) (Freund and Mason, 1999) and Linear
Regression (Witten and Frank, 2000).
Figure 1. Ranking results using individual
features: 1 refers to Adapted Naive Bayes, 2
refers to Naive Bayes, 3 refers to ME, 4 refers to
ADTree and 5 refers to Linear Regression.
</bodyText>
<table confidence="0.9978326">
Ranking Method Agreement Agreement Average
with Subject 1 with Subject 2
Adapted Naive Bayes 0.270841 0.367515 0.319178
Naive Bayes 0.381921 0.464577 0.423249
ME 0.446283 0.510926 0.478605
</table>
<figure confidence="0.918353333333333">
4 Refer to
http://homepages.inf.ed.ac.uk/s0450736/maxent.html
Sk1
3.0
1 2 3 4 5
0. 6
0. 5
0. 4
0.3
0. 2
0. 1
0
</figure>
<page confidence="0.572443">
66
</page>
<table confidence="0.8050285">
ADTree 0.371988 0.463966 0.417977
Linear Regression 0.497395 0.551597 0.524496
</table>
<tableCaption confidence="0.964872">
Table 2. Ranking results using individual
features.
</tableCaption>
<figureCaption confidence="0.931260666666667">
Figure 2. Ranking results using feature co-
occurrence patterns: 1 refers to Adapted Naive
Bayes, 2 refers to Naive Bayes, 3 refers to ME, 4
</figureCaption>
<table confidence="0.988784777777778">
refers to ADTree and 5 refers to Linear
Regression.
Ranking Method Agreement Agreement Average
with Subject 1 with Subject 2
Adapted Naive Bayes 0.420423 0.513336 0.466880
Naive Bayes 0.420866 0.475514 0.44819
ME 0.414184 0.455482 0.434833
ADTree 0.358095 0.422987 0.390541
Linear Regression 0.190609 0.279472 0.235041
</table>
<tableCaption confidence="0.9536535">
Table 3. Ranking results using feature co-
occurrence patterns.
</tableCaption>
<figureCaption confidence="0.760859571428571">
Figure 1 and table 2 show the Kendall tau-b
coefficient between human subjects’ ranking
results and the trained classifiers’ ranking results
of the test set when using individual features
(112); Figure 2 and table 3 show the Kendall
tau-b coefficient when using feature co-
occurrence patterns (813).
</figureCaption>
<bodyText confidence="0.999933733333333">
As we can see from the above figures, when
using individual features, Linear Regression
achieved the best result, Adapted Naive Bayes
performed the worst, Naive Bayes, ME and
ADTree were in the middle; however, when
using feature co-occurrence patterns, the order
almost reversed, i.e., Adapted Naive Bayes
performed the best and Linear Regression the
worst. Detailed analysis of the result is beyond
the scope of this paper. The best model gained
by using feature co-ocurrence patterns (i.e.,
Adapted Naive Bayes classifier) and by using
individual features (i.e., Linear Regression
classification model) will be used for further
experiments on the two-stage architecture.
</bodyText>
<sectionHeader confidence="0.933347" genericHeader="method">
4 Retrieving Relevant Documents for
How-To Questions
</sectionHeader>
<bodyText confidence="0.99993425">
In this section we will describe the experiments
on retrieving relevant documents for how-to
questions by applying different approaches
mentioned in the introduction section.
</bodyText>
<subsectionHeader confidence="0.991423">
4.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.9984869375">
We randomly chose 60 how-to questions from
the query logs of the FA Q finder system (Burke
et al., 1997). Three judges went through these
questions and agreed on 10 procedural
questions5.
We searched Google and downloaded 40 top
ranked documents for each question, which were
then mixed with 1000 web documents from the
SPIRIT collection to compile a test set. The two-
stage architecture is as shown in figure 3. In the
first stage, we sent only the content of the goal to
a state-of-the-art IR model to retrieve 30
documents from the test set, which were
reranked in the second stage according to the
degree of procedurality by a trained document
classifier.
</bodyText>
<figureCaption confidence="0.998554">
Figure 3. A two-stage architecture.
</figureCaption>
<bodyText confidence="0.993601625">
We also tried to test how well query expansion
could help in retrieving procedural documents,
following a process as shown in figure 4. First,
key words in the content of goal were used to
query an IR model to retrieve an initial set of
relevant documents, those of which that do not
contain the phrase ‘how to’ were then removed.
The remaining top ten documents were used to
generate 40 searching terms, which were applied
in the second round to retrieve documents.
Finally the 30 top ranked documents were
returned as relevant documents.
5 We distinguish questions asking for a series of steps
(i.e., procedural questions) from those of which the
answer could be a list of useful hints, e.g., ‘how to
make money’.
</bodyText>
<figure confidence="0.999154166666667">
Sub1
Sub2
1 2 3 4 5
0. 6
0. 5
0. 4
0. 3
0. 2
0. 1
0
Test set
The content
of the goal
IR model
Stage One
30 top
ranked
web docs
Document
classif�er
30
reranked
web docs
Stage Two
</figure>
<page confidence="0.893006">
67
</page>
<figureCaption confidence="0.9613315">
Figure 4. An alternative architecture using query
expansion.
</figureCaption>
<subsectionHeader confidence="0.762467">
4.2 IR Model
</subsectionHeader>
<bodyText confidence="0.999852192307692">
For the above-mentioned IR model, we used the
BM25 and PL2 algorithms from the Terrier IR
platform6.
The BM25 algorithm is one variety of the
probabilistic schema presented in (Robertson et
al. 1993). It has gained much success in TREC
competitions and has been adopted by many
other TREC participants.
The PL2 algorithm, as most other IR models
implemented in the Terrier IR platform, is based
on the Divergence From Randomness (DFR)
framework. Amati and Rijsbergen (2004)
provide a detailed explanation of this framework
and a set of term-weighting formulae derived by
applying different models of randomness and
different ways to normalize the weight of a term
according to the document length and according
to a notion called information gain. They test
these different formulae in the experiments on
retrieving relevant documents for various sets of
TREC topics and show that they achieve
comparable result with the BM25 algorithm.
We also used the Bo1 algorithm from the
same package to select terms for query
expansion. Refer to (Plachouras et al., 2004) for
details about this algorithm.
</bodyText>
<subsectionHeader confidence="0.995071">
4.3 Result
</subsectionHeader>
<bodyText confidence="0.999991111111111">
We tested eight systems, which could be
organized into two sets. The first set uses BM25
algorithm as the basic IR model and the second
set uses PL2 as the basic IR model. Each set
includes four systems: a baseline system that
returns the result of the first stage in the two-
stage architecture, one system that uses query
expansion technique following the architecture
in figure 4 and two systems that apply the two-
</bodyText>
<footnote confidence="0.440293">
6 http://ir.dcs.gla.ac.uk/terrier/index.html
</footnote>
<bodyText confidence="0.9967765">
stage architecture (one uses the Adapted Naive
Bayes classifier and another one uses the Linear
Regression classification model).
The mean average precision (MAP) 7 of
different retrieval systems is shown in table 4
and figure 5.
</bodyText>
<figure confidence="0.99889025">
0. 6 Basel i ne
0. 5 Query Egpansi on
0. 4 Adapt ed Naive Bayes
0. 3 Li near Fegr essi on
0. 2
0. 1
0
1 2
</figure>
<figureCaption confidence="0.856375">
Figure 5. MAPs of different systems: 1 refers to
using BM25 as the IR model, 2 refers to using
PL2 as the IR model.
</figureCaption>
<table confidence="0.985215333333333">
Model MAP
BM25 (Baseline) 0.33692
Set1 BM25 + Query Expansion 0.50162
BM25 + Adapted Naive Bayes 0.45605
BM25 + Linear Regression 0.41597
PL2 (Baseline) 0.33265
Set2 PL2 + Query Expansion 0.45821
PL2 + Adapted Naive Bayes 0.44263
PL2 + Linear Regression 0.40218
</table>
<tableCaption confidence="0.999721">
Table 4. Results of different systems.
</tableCaption>
<bodyText confidence="0.974101875">
We can see that in both sets: (1) systems that
adopts the two-stage architecture performed
better than the baseline system but worse than
the system that applies query expansion
technique; (2) the system that uses Adapted
Naive Bayes classifier in the second stage gained
better result than the one that uses Linear
Regression classification model. We performed a
pairwise t-test to test the significance of the
difference between the results of the two systems
with an integrated Adapted Naive Bayes
classifier and of the two baseline systems. Each
data set contained 20 figures, with each figure
representing the average precision of the
retrieving result for one question. The difference
is significant (p=0.02). We also performed a
pairwise t-test to test the significance of the
difference between the two systems with an
integrated Adapted Naive Bayes classifier and of
7 The average precision of a single question is the
mean of the precision scores after each relevant
document is retrieved. The mean average precision is
the mean of the average precisions of a collection of
questions.
</bodyText>
<figure confidence="0.993955652173913">
7KH FRQWHQW
RI WKH JRDO
&apos;RFXPHQW
UDQNLQJ OLVW
,5 PRGHO
5HPRYH GRFXPHQWV
FRQWDLQLQJ QR
ಪKRZ WRಫ
7HVW VHW
Round One
10 WRS
UDQNHG ZHE
GRFV
([WUDFW WHUPV IRU
TXHU\ H[SDQVLRQ
$Q H[SDQGHG
VHW RI TXHU\
WHUPV
30 WRS
UDQNHG
ZHE GRFV
,5 PRGHO
Round Two
</figure>
<page confidence="0.995955">
68
</page>
<bodyText confidence="0.999808666666667">
the two systems using query expansion
techniques. The difference is not significant
(p=0.66).
</bodyText>
<subsectionHeader confidence="0.91094">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999993243243243">
Contrary to our expectation, the result of the
experiments showed that the two-stage approach
did not perform better than simply applying a
query expansion technique to generate an
expanded list of querying terms. An explanation
can be sought from the following two aspects
(each of which corresponds to one of the two
problems mentioned in the first section).
First, we expected that many documents that
contain procedures do not contain the word
‘procedure’ or the phrase ‘how to’. Therefore, a
system based on key word matching would not
be able to identify such documents. However,
we found that such words or phrases, although
not included in the body of the text, often occur
in the title of the document.
Another problem we pointed out before is that
the phrase ‘how to’ occurs in a much broader
context than keywords in the content of the goal,
therefore, it would bring a lot of irrelevant
documents when used together with the content
of goal for document retrieval. However, in our
experiment, we used the content of the goal to
retrieve document first and then removed those
containing no phrase ‘how to’ (refer to figure 4).
This is actually also a two-stage approach in
itself.
Despite the experiment result, a well-known
defect of query expansion is that it is only
effective if relevant documents are similar to
each other while the two-stage approach does
not have this limitation. For example, for
retrieving documents about ‘how to cook
herring’, query expansion is only able to retrieve
typical recipes while our two-stage approach is
able to detect an exotic method as long as it is
described as a sequence of steps.
</bodyText>
<sectionHeader confidence="0.997885" genericHeader="conclusions">
5 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999349">
In this paper, we suggested that a how-to
question could be seen as consisting of two
parts: the specific goal and the general nature of
the desired information (i.e., procedural). We
proposed a two-stage architecture to retrieve
documents that meet the requirement of both
parts. We compared the two-stage architecture
with other approaches: one only uses the content
of the goal to retrieve documents (baseline
system) and another one uses an expanded set of
query terms obtained by automatic query
expansion techniques. The result has shown that
the two-stage architecture performed better than
the base line system but did not show superiority
over query expansion techniques. We provide an
explanation in section 4.4.
As suggested in section 1, many information
requests are formulated as consisting of two
parts. As a future work, we will test the two-
stage architecture for retrieving answers for other
kind of questions.
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997602475">
Amati, Gianni and Cornelis J. van Rijsbergen. 2002.
Probabilistic models of information retrieval based
on measuring the divergence from randomness.
ACM Transactions on Information Systems, 20 (4):
357-389.
Burke, Robin D., Kristian J. Hammond, Vladimir
Kulyukin, Steven L. Lytinen, Noriko Tomuro, and
Scott Schoenberg. 1997. Question answering from
frequently-asked question files: experiences with
the FAQ finder system. AI Magazine, 18(2): 57-66.
Clarke, Charles, Gordan Cormack, M. Laszlo,
Thomas Lynam, and Egidio Terra. 1998. The
impact of corpus size on question answering
performance. In Proceedings of the 25th Annual
International ACM SIGIR Conference on Research
and Development in IR, Tampere, Finland.
Elworthy, David. 2000. Question answering using a
large NLP system. In Proceedings of the Ninth Text
Retrieval Conference (TREC-9), pages 355-360.
Freund, Yoav and Llew Mason. 1999. The alternating
decision tree learning algorithm. In Proceeding of
the Sixteenth International Conference on Machine
Learning, pages 124-133, Bled, Slovenia.
Hovy, Eduard, Laurie Gerber, Ulf Hermjakob,
Michael Junk, and Chin-Yew Lin. 2001. Question
Answering in Webclopedia. In Proceedings of the
Ninth Text Retrieval Conference (TREC-9), pages
655-664.
Kelly, Diane, Vanessa Murdock, Xiao-Jun Yuan, W.
Bruce Croft, and Nicholas J. Belkin. 2002. Features
of documents relevant to task- and fact-oriented
questions. In Proceedings of the Eleventh
International Conference on Information and
Knowledge Management (CIKM &apos;02), pages 645-
647, McLean, VA.
Kendall, Maurice. 1979. The Advanced Theory of
Statistics. Fourth Edition. Griffin, London.
Murdok, Vanessa and Bruce Croft. 2002. Task
orientation in question answering. In Proceedings
of SIGIR ’02, pages 355-356, Tampere, Finland.
</reference>
<page confidence="0.983696">
69
</page>
<reference confidence="0.999623923076923">
Picard, Justin. 1999. Finding content-bearing terms
using term similarities. In Proceedings of the Ninth
Conference of the European Chapter of the
Association for Computational Linguistics (EACL
1999), pages 241-244, Bergen, Norway.
Plachouras, Vassilis, Ben He, and Iadh Ounis. 2004.
University of Glasgow at TREC2004: Experiments
in web, robust and terabyte tracks with Terrier. In
Proceedings of the Thirteenth Text REtrieval
Conference (TREC 2004).
Robertson, Stephen, Steve Walker, Susan Jones,
Micheline Hancock-Beaulieu, and Mike Gatford.
1993. Okapi at TREC-2. In Proceedings of the
Second Text Retrieval Conference (TREC-2),
pages 21-24.
Schwitter, Rolf, Fabio Rinaldi, and Simon Clematide.
2004. The importance of how-questions in
technical domains. In Proceedings of the Question-
Answering workshop of TALN 04, Fez, Morocco.
Stricker, Mathieu, Frantz Vichot, Gérard Dreyfus,
and Francis Wolinski. 2000. Two steps feature
selection and neural network classification for the
TREC-8 routing. CoRR cs. CL/0007016.
Takechi, Mineki, Takenobu Tokunaga, Yuji
Matsumoto, and Hozumi Tanaka. 2003. Feature
selection in categorizing procedural expressions. In
Proceedings of the Sixth International Workshop
on Information Retrieval with Asian Languages
(IRAL2003), pages 49-56, Sapporo, Japan.
Tsur, Oren, Maarten de Rijke, and Khalil Sima&apos;an.
2004. BioGrapher: biography questions as a
restricted domain question answering task. In
Proceedings ACL 2004 Workshop on Question
Answering in Restricted Domains, Barcelona.
Weisstein, Eric. 1999. Correlation Coefficient.
MathWorld--A Wolfram Web Resource. Available
at: &lt;URL: http://mathworld.wolfram.com/
CorrelationCoefficient.html&gt; [Accessed 21 Oct
2005]
Witten, Ian and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools with Java
Implementations, Morgan Kaufmann, San Mateo,
CA.
Xu, Jinxi, Ana Licuanan, and Ralph Weischedel.
2003. TREC2003 QA at BBN: answering
definitional questions. In Proceedings of the
Twelfth Text Retrieval Conference (TREC 2003),
pages 98-106.
Yang, Yi-Ming. 1999. An evaluation of statistical
approaches to text categorization. Journal of
Information Retrieval 1(1/2): 67-88.
Yang, Yi-Ming and Xin Liu. 1999. A re-examination
of text categorization methods. In Proceedings of
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR&apos;99),
pages 42-49, Berkeley, CA.
Yin, Ling and Richard Power. 2005. Investigating the
structure of topic expressions: a corpus-based
approach. In Proceedings from the Corpus
Linguistics Conference Series, Vol.1, No.1,
University of Birmingham, Birmingham.
Yin, Ling and Richard Power. 2006. Adapting the
Naive Bayes classifier to rank procedural texts. In
Proceedings of the 28th European Conference on
IR Research (ECIR 2006).
</reference>
<page confidence="0.998453">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983806">
<title confidence="0.997226">A Two-Stage Approach to Retrieving Answers for How-To Questions</title>
<author confidence="0.99815">Ling Yin</author>
<affiliation confidence="0.999983">CMIS, University of Brighton,</affiliation>
<address confidence="0.999973">Brighton, BN2 4GJ, United Kingdom</address>
<email confidence="0.998561">Y.Ling@brighton.ac.uk</email>
<abstract confidence="0.999749433333333">This paper addresses the problem of automatically retrieving answers for how-to questions, focusing on those that about the achieving a specific goal. For such questions, typical information retrieval methods, based on key word matching, are better suited to detecting the content of the goal (e.g., ‘installing a Windows XP server’) than the general nature of the desired information (i.e., procedural, a series of steps for achieving this goal). We suggest dividing the process of retrieving answers for such questions into two stages, with each stage focusing on modeling one aspect of a how-to question. We compare the two-stage approach with two alternative approaches: a baseline approach that only uses the content of the goal to retrieve relevant documents and another approach that explores the potential of automatic query expansion. The result of the experiment shows that the two-stage approach significantly outperforms the baseline but achieves similar result with the systems using automatic query expansion techniques. We analyze the reason and also present some future work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gianni Amati</author>
<author>Cornelis J van Rijsbergen</author>
</authors>
<title>Probabilistic models of information retrieval based on measuring the divergence from randomness.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>357--389</pages>
<marker>Amati, van Rijsbergen, 2002</marker>
<rawString>Amati, Gianni and Cornelis J. van Rijsbergen. 2002. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems, 20 (4): 357-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin D Burke</author>
<author>Kristian J Hammond</author>
<author>Vladimir Kulyukin</author>
<author>Steven L Lytinen</author>
<author>Noriko Tomuro</author>
<author>Scott Schoenberg</author>
</authors>
<title>Question answering from frequently-asked question files: experiences with the FAQ finder system.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>2</issue>
<pages>57--66</pages>
<contexts>
<context position="18849" citStr="Burke et al., 1997" startWordPosition="2922" endWordPosition="2925">pe of this paper. The best model gained by using feature co-ocurrence patterns (i.e., Adapted Naive Bayes classifier) and by using individual features (i.e., Linear Regression classification model) will be used for further experiments on the two-stage architecture. 4 Retrieving Relevant Documents for How-To Questions In this section we will describe the experiments on retrieving relevant documents for how-to questions by applying different approaches mentioned in the introduction section. 4.1 Experiment Setup We randomly chose 60 how-to questions from the query logs of the FA Q finder system (Burke et al., 1997). Three judges went through these questions and agreed on 10 procedural questions5. We searched Google and downloaded 40 top ranked documents for each question, which were then mixed with 1000 web documents from the SPIRIT collection to compile a test set. The twostage architecture is as shown in figure 3. In the first stage, we sent only the content of the goal to a state-of-the-art IR model to retrieve 30 documents from the test set, which were reranked in the second stage according to the degree of procedurality by a trained document classifier. Figure 3. A two-stage architecture. We also t</context>
</contexts>
<marker>Burke, Hammond, Kulyukin, Lytinen, Tomuro, Schoenberg, 1997</marker>
<rawString>Burke, Robin D., Kristian J. Hammond, Vladimir Kulyukin, Steven L. Lytinen, Noriko Tomuro, and Scott Schoenberg. 1997. Question answering from frequently-asked question files: experiences with the FAQ finder system. AI Magazine, 18(2): 57-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Clarke</author>
<author>Gordan Cormack</author>
<author>M Laszlo</author>
<author>Thomas Lynam</author>
<author>Egidio Terra</author>
</authors>
<title>The impact of corpus size on question answering performance.</title>
<date>1998</date>
<booktitle>In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in IR,</booktitle>
<location>Tampere, Finland.</location>
<contexts>
<context position="15346" citStr="Clarke et al., 1998" startWordPosition="2391" endWordPosition="2394"> p(Xi &lt; xij |C = procedural) ). Again this probability is estimated from the training set. The new model will be referred to hereafter as the Adapted Naive Bayes classifier. The details of this new model can be found in (Yin and Power, 2006). 3.4 Experiments on Ranking Procedural Texts There are two sources from which we compiled the training and testing corpora: the Pagewise collection and the SPIRIT collection. The SPIRIT collection contains a terabyte of HTML that are crawled from the web starting from an initial seed set of a few thousands universities and other educational organizations (Clarke et al., 1998). Our test set contained 103 documents, including the 53 documents that were sampled previously and then separated from the initial training corpus, another 30 documents randomly chosen from the Pagewise collection and 20 documents chosen from the SPIRIT collection. We asked two human subjects to score the procedurality for these documents, following the same instruction described in section 3.2. The correlation coefficient (Kendall tau-b) between the two rankings was 0.725, which is the upper bound of the performance of the classifiers. We first used the initial training corpus to bootstrap a</context>
</contexts>
<marker>Clarke, Cormack, Laszlo, Lynam, Terra, 1998</marker>
<rawString>Clarke, Charles, Gordan Cormack, M. Laszlo, Thomas Lynam, and Egidio Terra. 1998. The impact of corpus size on question answering performance. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in IR, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elworthy</author>
</authors>
<title>Question answering using a large NLP system.</title>
<date>2000</date>
<booktitle>In Proceedings of the Ninth Text Retrieval Conference (TREC-9),</booktitle>
<pages>355--360</pages>
<contexts>
<context position="6690" citStr="Elworthy, 2000" startWordPosition="1050" endWordPosition="1051">search area that is closely related to IR is question answering (QA), the differences being a) the input of a QA system is a question rather than a few key words; b) a QA system aims to extract answers to a question rather than retrieving relevant documents only. Most QA systems do adopt a two-stage architecture (if not consider the initial question analysis stage), i.e., perform IR with a few content words extracted from the query to locate documents likely to contain an answer and then use information extraction (IE) to find the text snippets that match the question type (Hovy et al., 2001; Elworthy, 2000). However, most question answering systems target factoid questions – the research of non-factoid questions started only a few years ago but limited to several kinds, such as definitional questions (Xu et al., 2003) and questions asking for biographies (Tsur et al., 2004). Only a few studies have addressed procedural questions. Murdok and Croft (2002) distinguish between “task-oriented questions” (i.e., ask about a process) and “fact-oriented questions” (i.e., ask about a fact) and present a method to automatically classify questions into these two categories. Following this work, Kelly et al.</context>
</contexts>
<marker>Elworthy, 2000</marker>
<rawString>Elworthy, David. 2000. Question answering using a large NLP system. In Proceedings of the Ninth Text Retrieval Conference (TREC-9), pages 355-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Llew Mason</author>
</authors>
<title>The alternating decision tree learning algorithm.</title>
<date>1999</date>
<booktitle>In Proceeding of the Sixteenth International Conference on Machine Learning,</booktitle>
<pages>124--133</pages>
<location>Bled, Slovenia.</location>
<contexts>
<context position="16339" citStr="Freund and Mason, 1999" startWordPosition="2537" endWordPosition="2540">ion described in section 3.2. The correlation coefficient (Kendall tau-b) between the two rankings was 0.725, which is the upper bound of the performance of the classifiers. We first used the initial training corpus to bootstrap a larger training set (378 procedural documents and 608 non-procedural documents), which was then used to select distinctive feature co-occurrence patterns and to train different classifiers. We compared the Adapted Naive Bayes classifier with the Naive Bayes classifier and three other classifiers, including Maximum Entropy (ME) 4 , Alternating Decision Tree (ADTree) (Freund and Mason, 1999) and Linear Regression (Witten and Frank, 2000). Figure 1. Ranking results using individual features: 1 refers to Adapted Naive Bayes, 2 refers to Naive Bayes, 3 refers to ME, 4 refers to ADTree and 5 refers to Linear Regression. Ranking Method Agreement Agreement Average with Subject 1 with Subject 2 Adapted Naive Bayes 0.270841 0.367515 0.319178 Naive Bayes 0.381921 0.464577 0.423249 ME 0.446283 0.510926 0.478605 4 Refer to http://homepages.inf.ed.ac.uk/s0450736/maxent.html Sk1 3.0 1 2 3 4 5 0. 6 0. 5 0. 4 0.3 0. 2 0. 1 0 66 ADTree 0.371988 0.463966 0.417977 Linear Regression 0.497395 0.5515</context>
</contexts>
<marker>Freund, Mason, 1999</marker>
<rawString>Freund, Yoav and Llew Mason. 1999. The alternating decision tree learning algorithm. In Proceeding of the Sixteenth International Conference on Machine Learning, pages 124-133, Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Laurie Gerber</author>
<author>Ulf Hermjakob</author>
<author>Michael Junk</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Question Answering in Webclopedia.</title>
<date>2001</date>
<booktitle>In Proceedings of the Ninth Text Retrieval Conference (TREC-9),</booktitle>
<pages>655--664</pages>
<contexts>
<context position="6673" citStr="Hovy et al., 2001" startWordPosition="1046" endWordPosition="1049"> architecture. A research area that is closely related to IR is question answering (QA), the differences being a) the input of a QA system is a question rather than a few key words; b) a QA system aims to extract answers to a question rather than retrieving relevant documents only. Most QA systems do adopt a two-stage architecture (if not consider the initial question analysis stage), i.e., perform IR with a few content words extracted from the query to locate documents likely to contain an answer and then use information extraction (IE) to find the text snippets that match the question type (Hovy et al., 2001; Elworthy, 2000). However, most question answering systems target factoid questions – the research of non-factoid questions started only a few years ago but limited to several kinds, such as definitional questions (Xu et al., 2003) and questions asking for biographies (Tsur et al., 2004). Only a few studies have addressed procedural questions. Murdok and Croft (2002) distinguish between “task-oriented questions” (i.e., ask about a process) and “fact-oriented questions” (i.e., ask about a fact) and present a method to automatically classify questions into these two categories. Following this w</context>
</contexts>
<marker>Hovy, Gerber, Hermjakob, Junk, Lin, 2001</marker>
<rawString>Hovy, Eduard, Laurie Gerber, Ulf Hermjakob, Michael Junk, and Chin-Yew Lin. 2001. Question Answering in Webclopedia. In Proceedings of the Ninth Text Retrieval Conference (TREC-9), pages 655-664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Kelly</author>
<author>Vanessa Murdock</author>
<author>Xiao-Jun Yuan</author>
<author>W Bruce Croft</author>
<author>Nicholas J Belkin</author>
</authors>
<title>Features of documents relevant to task- and fact-oriented questions.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eleventh International Conference on Information and Knowledge Management (CIKM &apos;02),</booktitle>
<pages>645--647</pages>
<location>McLean, VA.</location>
<contexts>
<context position="7297" citStr="Kelly et al. (2002)" startWordPosition="1138" endWordPosition="1141">orthy, 2000). However, most question answering systems target factoid questions – the research of non-factoid questions started only a few years ago but limited to several kinds, such as definitional questions (Xu et al., 2003) and questions asking for biographies (Tsur et al., 2004). Only a few studies have addressed procedural questions. Murdok and Croft (2002) distinguish between “task-oriented questions” (i.e., ask about a process) and “fact-oriented questions” (i.e., ask about a fact) and present a method to automatically classify questions into these two categories. Following this work, Kelly et al. (2002) explore the difference between documents that contain relevant information to the two different types of questions. They conclude, “lists and FAQs occur in more documents judged relevant to task-oriented questions than those judged relevant to fact-oriented questions” (Kelly et al., 2002: 645) and suggest, “retrieval techniques specific to each type of question should be considered” (Kelly et al., 2002: 647). Schwitter et al. (2004) present a method to extract answers from technical documentations for How-questions. To identify answers, they match the logical form of a sentence against that o</context>
</contexts>
<marker>Kelly, Murdock, Yuan, Croft, Belkin, 2002</marker>
<rawString>Kelly, Diane, Vanessa Murdock, Xiao-Jun Yuan, W. Bruce Croft, and Nicholas J. Belkin. 2002. Features of documents relevant to task- and fact-oriented questions. In Proceedings of the Eleventh International Conference on Information and Knowledge Management (CIKM &apos;02), pages 645-647, McLean, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Kendall</author>
</authors>
<title>The Advanced Theory of Statistics. Fourth Edition.</title>
<date>1979</date>
<location>Griffin, London.</location>
<contexts>
<context position="12530" citStr="Kendall, 1979" startWordPosition="1930" endWordPosition="1931">ild the procedural set we chose documents with titles containing key phrases ‘how to’ and ‘how can I’ (209 web documents); to build the non-procedural set, we chose documents which did not include these phrases in their titles, and which also had no phrases like ‘procedure’ and ‘recipe’ within the body of the text (208 web documents). Samples drawn randomly from the procedural set (25) and non-procedural set (28) were submitted to two human judges, who assigned procedurality scores from 1 (meaning no procedural text at all) to 5 (meaning over 90% procedural text). The Kendall tau-b agreement (Kendall, 1979) between the two rankings was 0.821. Overall, the average scores for the procedural and non-procedural samples were 3.15 and 1.38. We used these 53 sample documents as part of the test set and the 3 Refer to http://www.essortment.com 65 remaining documents as the initial training set (184 procedural and 180 non-procedural). This initial training corpus is far from ideal: first, it is small in size; a more serious problem is that many positive training examples do not contain a major proportion of procedural text. In our experiments, we used this initial training set to bootstrap a larger train</context>
</contexts>
<marker>Kendall, 1979</marker>
<rawString>Kendall, Maurice. 1979. The Advanced Theory of Statistics. Fourth Edition. Griffin, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Murdok</author>
<author>Bruce Croft</author>
</authors>
<title>Task orientation in question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR ’02,</booktitle>
<pages>355--356</pages>
<location>Tampere, Finland.</location>
<contexts>
<context position="7043" citStr="Murdok and Croft (2002)" startWordPosition="1102" endWordPosition="1105">estion analysis stage), i.e., perform IR with a few content words extracted from the query to locate documents likely to contain an answer and then use information extraction (IE) to find the text snippets that match the question type (Hovy et al., 2001; Elworthy, 2000). However, most question answering systems target factoid questions – the research of non-factoid questions started only a few years ago but limited to several kinds, such as definitional questions (Xu et al., 2003) and questions asking for biographies (Tsur et al., 2004). Only a few studies have addressed procedural questions. Murdok and Croft (2002) distinguish between “task-oriented questions” (i.e., ask about a process) and “fact-oriented questions” (i.e., ask about a fact) and present a method to automatically classify questions into these two categories. Following this work, Kelly et al. (2002) explore the difference between documents that contain relevant information to the two different types of questions. They conclude, “lists and FAQs occur in more documents judged relevant to task-oriented questions than those judged relevant to fact-oriented questions” (Kelly et al., 2002: 645) and suggest, “retrieval techniques specific to eac</context>
</contexts>
<marker>Murdok, Croft, 2002</marker>
<rawString>Murdok, Vanessa and Bruce Croft. 2002. Task orientation in question answering. In Proceedings of SIGIR ’02, pages 355-356, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Picard</author>
</authors>
<title>Finding content-bearing terms using term similarities.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>241--244</pages>
<location>Bergen,</location>
<contexts>
<context position="2819" citStr="Picard (1999)" startWordPosition="438" endWordPosition="439">tains the words in the goal is more likely to be relevant than a document that contains the word ‘procedure’ or the phrase ‘how to’. Without noticing this difference, treating the two parts equally in the retrieving process will get many noisy documents. Many information requests seem to show such a structure, with one part identifying a specific topic and another part constraining the kind of information required about this topic (Yin and Power, 2005). The second part is often omitted when selecting retrieval terms from the request to construct an effective query for an IR system, such as in Picard (1999). The first point given above suggests that using cues such as ‘first’ and ‘next’ to expand the initial query may help in retrieving more relevant documents. Expansion terms can be generated automatically by query expansion techniques. The typical process is: (1) use the initial query to retrieve documents (referred to as the first round of retrieval); (2) consider a few top ranked documents as relevant and the rest irrelevant; (3) compare the relevant set with the irrelevant set to extract a list of most distinctive terms; (4) use the extracted terms to retrieve documents (referred to as the </context>
</contexts>
<marker>Picard, 1999</marker>
<rawString>Picard, Justin. 1999. Finding content-bearing terms using term similarities. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL 1999), pages 241-244, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassilis Plachouras</author>
<author>Ben He</author>
<author>Iadh Ounis</author>
</authors>
<title>University of Glasgow at TREC2004: Experiments in web, robust and terabyte tracks with Terrier.</title>
<date>2004</date>
<booktitle>In Proceedings of the Thirteenth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="21472" citStr="Plachouras et al., 2004" startWordPosition="3372" endWordPosition="3375">amework. Amati and Rijsbergen (2004) provide a detailed explanation of this framework and a set of term-weighting formulae derived by applying different models of randomness and different ways to normalize the weight of a term according to the document length and according to a notion called information gain. They test these different formulae in the experiments on retrieving relevant documents for various sets of TREC topics and show that they achieve comparable result with the BM25 algorithm. We also used the Bo1 algorithm from the same package to select terms for query expansion. Refer to (Plachouras et al., 2004) for details about this algorithm. 4.3 Result We tested eight systems, which could be organized into two sets. The first set uses BM25 algorithm as the basic IR model and the second set uses PL2 as the basic IR model. Each set includes four systems: a baseline system that returns the result of the first stage in the twostage architecture, one system that uses query expansion technique following the architecture in figure 4 and two systems that apply the two6 http://ir.dcs.gla.ac.uk/terrier/index.html stage architecture (one uses the Adapted Naive Bayes classifier and another one uses the Linea</context>
</contexts>
<marker>Plachouras, He, Ounis, 2004</marker>
<rawString>Plachouras, Vassilis, Ben He, and Iadh Ounis. 2004. University of Glasgow at TREC2004: Experiments in web, robust and terabyte tracks with Terrier. In Proceedings of the Thirteenth Text REtrieval Conference (TREC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
<author>Steve Walker</author>
<author>Susan Jones</author>
<author>Micheline Hancock-Beaulieu</author>
<author>Mike Gatford</author>
</authors>
<title>Okapi at TREC-2.</title>
<date>1993</date>
<booktitle>In Proceedings of the Second Text Retrieval Conference (TREC-2),</booktitle>
<pages>21--24</pages>
<contexts>
<context position="20610" citStr="Robertson et al. 1993" startWordPosition="3234" endWordPosition="3237">tinguish questions asking for a series of steps (i.e., procedural questions) from those of which the answer could be a list of useful hints, e.g., ‘how to make money’. Sub1 Sub2 1 2 3 4 5 0. 6 0. 5 0. 4 0. 3 0. 2 0. 1 0 Test set The content of the goal IR model Stage One 30 top ranked web docs Document classif�er 30 reranked web docs Stage Two 67 Figure 4. An alternative architecture using query expansion. 4.2 IR Model For the above-mentioned IR model, we used the BM25 and PL2 algorithms from the Terrier IR platform6. The BM25 algorithm is one variety of the probabilistic schema presented in (Robertson et al. 1993). It has gained much success in TREC competitions and has been adopted by many other TREC participants. The PL2 algorithm, as most other IR models implemented in the Terrier IR platform, is based on the Divergence From Randomness (DFR) framework. Amati and Rijsbergen (2004) provide a detailed explanation of this framework and a set of term-weighting formulae derived by applying different models of randomness and different ways to normalize the weight of a term according to the document length and according to a notion called information gain. They test these different formulae in the experimen</context>
</contexts>
<marker>Robertson, Walker, Jones, Hancock-Beaulieu, Gatford, 1993</marker>
<rawString>Robertson, Stephen, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1993. Okapi at TREC-2. In Proceedings of the Second Text Retrieval Conference (TREC-2), pages 21-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rolf Schwitter</author>
<author>Fabio Rinaldi</author>
<author>Simon Clematide</author>
</authors>
<title>The importance of how-questions in technical domains.</title>
<date>2004</date>
<booktitle>In Proceedings of the QuestionAnswering workshop of TALN 04,</booktitle>
<location>Fez, Morocco.</location>
<contexts>
<context position="7734" citStr="Schwitter et al. (2004)" startWordPosition="1202" endWordPosition="1205">ocess) and “fact-oriented questions” (i.e., ask about a fact) and present a method to automatically classify questions into these two categories. Following this work, Kelly et al. (2002) explore the difference between documents that contain relevant information to the two different types of questions. They conclude, “lists and FAQs occur in more documents judged relevant to task-oriented questions than those judged relevant to fact-oriented questions” (Kelly et al., 2002: 645) and suggest, “retrieval techniques specific to each type of question should be considered” (Kelly et al., 2002: 647). Schwitter et al. (2004) present a method to extract answers from technical documentations for How-questions. To identify answers, they match the logical form of a sentence against that of the question and also explore the typographical conventions in technical domains. The work that most resembles ours is Takechi et al. (2003), which uses word n-grams to classify (as procedural or non-procedural) list passages extracted using HTML tags. Our approach, however, applies to whole documents, the aim being to measure the degree of procedurality — i.e., the proportion of procedural text they contain. 64 3 Ranking Procedura</context>
</contexts>
<marker>Schwitter, Rinaldi, Clematide, 2004</marker>
<rawString>Schwitter, Rolf, Fabio Rinaldi, and Simon Clematide. 2004. The importance of how-questions in technical domains. In Proceedings of the QuestionAnswering workshop of TALN 04, Fez, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathieu Stricker</author>
<author>Frantz Vichot</author>
<author>Gérard Dreyfus</author>
<author>Francis Wolinski</author>
</authors>
<title>Two steps feature selection and neural network classification for the TREC-8 routing. CoRR cs.</title>
<date>2000</date>
<pages>0007016</pages>
<contexts>
<context position="5566" citStr="Stricker et al. (2000)" startWordPosition="863" endWordPosition="866">ed, the training corpus, the design of a classification model as well as some experiments for evaluation. Section 4 talks about integrating the text categorizer into the two-stage architecture and presents some experiments on retrieving relevant documents for how-to questions. Section 5 provides a short summary and presents some future work. 2 Related Work The idea of applying text categorization technology to help information retrieval is not new. In particular, text categorization techniques are widely adopted to filter a document source according to specific information needs. For example, Stricker et al. (2000) experiment on several news resources to find news addressing specific topics. They present a method for automatically generating “discriminant terms” (Stricker et al., 2000) for each topic that are then used as features to train a neural network classifier. Compared to these approaches, the novelty of our study lies in the idea that an information request consists of two different parts that should be retrieved in different ways and the whole retrieval process should adopt a two-stage architecture. A research area that is closely related to IR is question answering (QA), the differences being</context>
</contexts>
<marker>Stricker, Vichot, Dreyfus, Wolinski, 2000</marker>
<rawString>Stricker, Mathieu, Frantz Vichot, Gérard Dreyfus, and Francis Wolinski. 2000. Two steps feature selection and neural network classification for the TREC-8 routing. CoRR cs. CL/0007016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mineki Takechi</author>
<author>Takenobu Tokunaga</author>
<author>Yuji Matsumoto</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Feature selection in categorizing procedural expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of the Sixth International Workshop on Information Retrieval with Asian Languages (IRAL2003),</booktitle>
<pages>49--56</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="8039" citStr="Takechi et al. (2003)" startWordPosition="1249" endWordPosition="1252"> conclude, “lists and FAQs occur in more documents judged relevant to task-oriented questions than those judged relevant to fact-oriented questions” (Kelly et al., 2002: 645) and suggest, “retrieval techniques specific to each type of question should be considered” (Kelly et al., 2002: 647). Schwitter et al. (2004) present a method to extract answers from technical documentations for How-questions. To identify answers, they match the logical form of a sentence against that of the question and also explore the typographical conventions in technical domains. The work that most resembles ours is Takechi et al. (2003), which uses word n-grams to classify (as procedural or non-procedural) list passages extracted using HTML tags. Our approach, however, applies to whole documents, the aim being to measure the degree of procedurality — i.e., the proportion of procedural text they contain. 64 3 Ranking Procedural Texts Three essential elements of a text categorization approach are the features used to represent the document, the training corpus and the machine learning method, which will be described in section 3.1, 3.2 and 3.3 respectively. Section 3.4 presents experiments on applying the learned model to rank</context>
</contexts>
<marker>Takechi, Tokunaga, Matsumoto, Tanaka, 2003</marker>
<rawString>Takechi, Mineki, Takenobu Tokunaga, Yuji Matsumoto, and Hozumi Tanaka. 2003. Feature selection in categorizing procedural expressions. In Proceedings of the Sixth International Workshop on Information Retrieval with Asian Languages (IRAL2003), pages 49-56, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Maarten de Rijke</author>
<author>Khalil Sima&apos;an</author>
</authors>
<title>BioGrapher: biography questions as a restricted domain question answering task.</title>
<date>2004</date>
<booktitle>In Proceedings ACL 2004 Workshop on Question Answering in Restricted Domains,</booktitle>
<location>Barcelona.</location>
<marker>Tsur, de Rijke, Sima&apos;an, 2004</marker>
<rawString>Tsur, Oren, Maarten de Rijke, and Khalil Sima&apos;an. 2004. BioGrapher: biography questions as a restricted domain question answering task. In Proceedings ACL 2004 Workshop on Question Answering in Restricted Domains, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Weisstein</author>
</authors>
<title>Correlation Coefficient. MathWorld--A Wolfram Web Resource. Available at: http://mathworld.wolfram.com/</title>
<date>1999</date>
<journal>CorrelationCoefficient.html&gt; [Accessed</journal>
<volume>21</volume>
<contexts>
<context position="9453" citStr="Weisstein, 1999" startWordPosition="1462" endWordPosition="1463">conditions, and purposes. These elements can be recognized using linguistic features or cue phrases. For example, an action is often conveyed by an imperative; a precondition can be expressed by the cue phrase ‘only if’. We used all the syntactic and morphological tags defined in Connexor’s syntax analyzer1. There are some redundant tags in this set. For example, both the syntactic tag ‘@INFMARK&gt;’ and the morphological tag ‘INFMARK&gt;’ refer to the infinitive marker ‘to’ and therefore always occur together at the same time. We calculated the Pearson’s product-moment correlation coefficient (r) (Weisstein, 1999) between any two tags based on their occurrences in sentences of the whole training set. We removed one in each pair of strongly correlated tags and finally got 34 syntactic tags and 34 morphological tags. We also handcrafted a list of relevant cue phrases (44), which were extracted from documents by using the Flex tool2 for pattern matching. Some sample cue phrases and the matching patterns are shown in table 1. Procedural Cue Phrase Pattern Element Precondition ‘only if’ [Oo]nly[[:space:]]if[[:space:]] Purpose ‘so that’ [sS]o[[:space:]]that[[:space:]] Condition ‘as long as’ ([Aa]s) [[:space:</context>
</contexts>
<marker>Weisstein, 1999</marker>
<rawString>Weisstein, Eric. 1999. Correlation Coefficient. MathWorld--A Wolfram Web Resource. Available at: &lt;URL: http://mathworld.wolfram.com/ CorrelationCoefficient.html&gt; [Accessed 21 Oct 2005]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2000</date>
<booktitle>Data Mining: Practical Machine Learning Tools with Java Implementations,</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="13465" citStr="Witten and Frank, 2000" startWordPosition="2079" endWordPosition="2082">-procedural). This initial training corpus is far from ideal: first, it is small in size; a more serious problem is that many positive training examples do not contain a major proportion of procedural text. In our experiments, we used this initial training set to bootstrap a larger training set. 3.3 Learning Method Although shown to be not so effective in some previous studies (Yang, 1999; Yang and Liu, 1999), Naive Bayes classifier is one of the most commonly-used classifiers for text categorization. Here we introduce a model adapted from the Naive Bayes classifier from the weka-3-4 package (Witten and Frank, 2000). The Naive Bayes classifier scores a document dj according to whether it is a typical member of its set — i.e., the probability of randomly picking up a document like it from the procedural class ( p(dj |C= procedural) ). This probability is estimated from the training corpus. As mentioned before, the average procedural score of the procedural training set is low. Therefore, there is obviously a danger that a true procedural document will be ranked lower than a document that contains less procedural texts when using this training set to train a Naive Bayes classifier. Although our procedural </context>
<context position="16386" citStr="Witten and Frank, 2000" startWordPosition="2544" endWordPosition="2547">oefficient (Kendall tau-b) between the two rankings was 0.725, which is the upper bound of the performance of the classifiers. We first used the initial training corpus to bootstrap a larger training set (378 procedural documents and 608 non-procedural documents), which was then used to select distinctive feature co-occurrence patterns and to train different classifiers. We compared the Adapted Naive Bayes classifier with the Naive Bayes classifier and three other classifiers, including Maximum Entropy (ME) 4 , Alternating Decision Tree (ADTree) (Freund and Mason, 1999) and Linear Regression (Witten and Frank, 2000). Figure 1. Ranking results using individual features: 1 refers to Adapted Naive Bayes, 2 refers to Naive Bayes, 3 refers to ME, 4 refers to ADTree and 5 refers to Linear Regression. Ranking Method Agreement Agreement Average with Subject 1 with Subject 2 Adapted Naive Bayes 0.270841 0.367515 0.319178 Naive Bayes 0.381921 0.464577 0.423249 ME 0.446283 0.510926 0.478605 4 Refer to http://homepages.inf.ed.ac.uk/s0450736/maxent.html Sk1 3.0 1 2 3 4 5 0. 6 0. 5 0. 4 0.3 0. 2 0. 1 0 66 ADTree 0.371988 0.463966 0.417977 Linear Regression 0.497395 0.551597 0.524496 Table 2. Ranking results using indi</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>Witten, Ian and Eibe Frank. 2000. Data Mining: Practical Machine Learning Tools with Java Implementations, Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>Ana Licuanan</author>
<author>Ralph Weischedel</author>
</authors>
<title>TREC2003 QA at BBN: answering definitional questions.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twelfth Text Retrieval Conference (TREC</booktitle>
<pages>98--106</pages>
<contexts>
<context position="6905" citStr="Xu et al., 2003" startWordPosition="1081" endWordPosition="1084">n rather than retrieving relevant documents only. Most QA systems do adopt a two-stage architecture (if not consider the initial question analysis stage), i.e., perform IR with a few content words extracted from the query to locate documents likely to contain an answer and then use information extraction (IE) to find the text snippets that match the question type (Hovy et al., 2001; Elworthy, 2000). However, most question answering systems target factoid questions – the research of non-factoid questions started only a few years ago but limited to several kinds, such as definitional questions (Xu et al., 2003) and questions asking for biographies (Tsur et al., 2004). Only a few studies have addressed procedural questions. Murdok and Croft (2002) distinguish between “task-oriented questions” (i.e., ask about a process) and “fact-oriented questions” (i.e., ask about a fact) and present a method to automatically classify questions into these two categories. Following this work, Kelly et al. (2002) explore the difference between documents that contain relevant information to the two different types of questions. They conclude, “lists and FAQs occur in more documents judged relevant to task-oriented que</context>
</contexts>
<marker>Xu, Licuanan, Weischedel, 2003</marker>
<rawString>Xu, Jinxi, Ana Licuanan, and Ralph Weischedel. 2003. TREC2003 QA at BBN: answering definitional questions. In Proceedings of the Twelfth Text Retrieval Conference (TREC 2003), pages 98-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Ming Yang</author>
</authors>
<title>An evaluation of statistical approaches to text categorization.</title>
<date>1999</date>
<journal>Journal of Information Retrieval</journal>
<volume>1</volume>
<issue>1</issue>
<pages>67--88</pages>
<contexts>
<context position="13233" citStr="Yang, 1999" startWordPosition="2045" endWordPosition="2046">rocedural samples were 3.15 and 1.38. We used these 53 sample documents as part of the test set and the 3 Refer to http://www.essortment.com 65 remaining documents as the initial training set (184 procedural and 180 non-procedural). This initial training corpus is far from ideal: first, it is small in size; a more serious problem is that many positive training examples do not contain a major proportion of procedural text. In our experiments, we used this initial training set to bootstrap a larger training set. 3.3 Learning Method Although shown to be not so effective in some previous studies (Yang, 1999; Yang and Liu, 1999), Naive Bayes classifier is one of the most commonly-used classifiers for text categorization. Here we introduce a model adapted from the Naive Bayes classifier from the weka-3-4 package (Witten and Frank, 2000). The Naive Bayes classifier scores a document dj according to whether it is a typical member of its set — i.e., the probability of randomly picking up a document like it from the procedural class ( p(dj |C= procedural) ). This probability is estimated from the training corpus. As mentioned before, the average procedural score of the procedural training set is low. </context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yang, Yi-Ming. 1999. An evaluation of statistical approaches to text categorization. Journal of Information Retrieval 1(1/2): 67-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Ming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;99),</booktitle>
<pages>42--49</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="13254" citStr="Yang and Liu, 1999" startWordPosition="2047" endWordPosition="2050">mples were 3.15 and 1.38. We used these 53 sample documents as part of the test set and the 3 Refer to http://www.essortment.com 65 remaining documents as the initial training set (184 procedural and 180 non-procedural). This initial training corpus is far from ideal: first, it is small in size; a more serious problem is that many positive training examples do not contain a major proportion of procedural text. In our experiments, we used this initial training set to bootstrap a larger training set. 3.3 Learning Method Although shown to be not so effective in some previous studies (Yang, 1999; Yang and Liu, 1999), Naive Bayes classifier is one of the most commonly-used classifiers for text categorization. Here we introduce a model adapted from the Naive Bayes classifier from the weka-3-4 package (Witten and Frank, 2000). The Naive Bayes classifier scores a document dj according to whether it is a typical member of its set — i.e., the probability of randomly picking up a document like it from the procedural class ( p(dj |C= procedural) ). This probability is estimated from the training corpus. As mentioned before, the average procedural score of the procedural training set is low. Therefore, there is o</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yang, Yi-Ming and Xin Liu. 1999. A re-examination of text categorization methods. In Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;99), pages 42-49, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ling Yin</author>
<author>Richard Power</author>
</authors>
<title>Investigating the structure of topic expressions: a corpus-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings from the Corpus Linguistics Conference Series, Vol.1, No.1,</booktitle>
<institution>University of Birmingham,</institution>
<location>Birmingham.</location>
<contexts>
<context position="2662" citStr="Yin and Power, 2005" startWordPosition="409" endWordPosition="412">dly, We expect that the word ‘procedure’ or the phrase ‘how to’ will occur in a much broader context than the words in the goal. In other words, a document that contains the words in the goal is more likely to be relevant than a document that contains the word ‘procedure’ or the phrase ‘how to’. Without noticing this difference, treating the two parts equally in the retrieving process will get many noisy documents. Many information requests seem to show such a structure, with one part identifying a specific topic and another part constraining the kind of information required about this topic (Yin and Power, 2005). The second part is often omitted when selecting retrieval terms from the request to construct an effective query for an IR system, such as in Picard (1999). The first point given above suggests that using cues such as ‘first’ and ‘next’ to expand the initial query may help in retrieving more relevant documents. Expansion terms can be generated automatically by query expansion techniques. The typical process is: (1) use the initial query to retrieve documents (referred to as the first round of retrieval); (2) consider a few top ranked documents as relevant and the rest irrelevant; (3) compare</context>
</contexts>
<marker>Yin, Power, 2005</marker>
<rawString>Yin, Ling and Richard Power. 2005. Investigating the structure of topic expressions: a corpus-based approach. In Proceedings from the Corpus Linguistics Conference Series, Vol.1, No.1, University of Birmingham, Birmingham.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ling Yin</author>
<author>Richard Power</author>
</authors>
<title>Adapting the Naive Bayes classifier to rank procedural texts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th European Conference on IR Research (ECIR</booktitle>
<contexts>
<context position="4267" citStr="Yin and Power, 2006" startWordPosition="668" endWordPosition="671">ioned two problems are not yet tackled. Our solution is to divide the process of retrieving answers for such questions into two stages: (1) use typical IR approaches for retrieving documents that are relevant to the specific goal; (2) use a text categorization approach to re-rank the retrieved documents according to the proportion of procedural text they contain. By ‘procedural text’ we refer to ordered lists of steps, which are very common in some instructional genres such as online manuals. In this report, we will briefly introduce the text categorization approach (details are presented in (Yin and Power, 2006) ) and will explain in more concrete terms how it is integrated into the two-stage architecture proposed above. We will compare the performance of our two-stage architecture with a baseline system that uses only the content of the goal to retrieve relevant documents (equivalent to the first stage in the two-stage architecture). We will also compare the two-stage approach with systems that applies automatic query expansion techniques. This paper is organized as follows. Section 2 introduces some relevant work in IR and question answering (QA). Section 3 talks about the text categorization appro</context>
<context position="11103" citStr="Yin and Power, 2006" startWordPosition="1699" endWordPosition="1702">/flex/flex.html represent a ranking order or a spatial relationship as well as a sequential order. However, it is more likely to represent a sequential order between actions if there is also an imperative in the same sentence. Indeed, sentences that contain both an ordinal number and an imperative are very frequent in procedural texts. We compared between the procedural training set and the nonprocedural training set to extract distinctive feature co-occurrence patterns, each of which has only 2 features. The formulae used to rank patterns with regard to their distinctiveness can be found in (Yin and Power, 2006). Document Representation Each document was represented as a vector dj = {x1j , x2 j , ... , xNj } , where xij represents the number of sentences in the document that contains a particular feature normalized by the document length. We compare the effectiveness of using individual features ( xij refers to either a single linguistic feature or a cue phrases) and of using feature co-occurrence patterns ( xij refers to a feature co-occurrence pattern). 3.2 Corpus Preparation Pagewise 3 provides a list of subject-matter domains, ranging from household issues to arts and entertainment. We downloaded</context>
<context position="14967" citStr="Yin and Power, 2006" startWordPosition="2332" endWordPosition="2335">etween the two classes. For example, if the procedural training set has a higher average value on feature Xi than the nonprocedural training set, we inferred that a document with a higher feature value on Xi should be scored higher. To reflect this rule, we scored a document dj by the probability of picking a document with a lower feature value from the procedural class (i.e., p(Xi &lt; xij |C = procedural) ). Again this probability is estimated from the training set. The new model will be referred to hereafter as the Adapted Naive Bayes classifier. The details of this new model can be found in (Yin and Power, 2006). 3.4 Experiments on Ranking Procedural Texts There are two sources from which we compiled the training and testing corpora: the Pagewise collection and the SPIRIT collection. The SPIRIT collection contains a terabyte of HTML that are crawled from the web starting from an initial seed set of a few thousands universities and other educational organizations (Clarke et al., 1998). Our test set contained 103 documents, including the 53 documents that were sampled previously and then separated from the initial training corpus, another 30 documents randomly chosen from the Pagewise collection and 20</context>
</contexts>
<marker>Yin, Power, 2006</marker>
<rawString>Yin, Ling and Richard Power. 2006. Adapting the Naive Bayes classifier to rank procedural texts. In Proceedings of the 28th European Conference on IR Research (ECIR 2006).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>