<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000475">
<title confidence="0.9997655">
A Speech Understanding Framework
that Uses Multiple Language Models and Multiple Understanding Models
</title>
<author confidence="0.995078">
†Masaki Katsumaru, ‡Mikio Nakano, †Kazunori Komatani,
‡Kotaro Funakoshi, †Tetsuya Ogata, †Hiroshi G. Okuno
</author>
<affiliation confidence="0.995871">
†Graduate School of Informatics, Kyoto University
</affiliation>
<address confidence="0.6208415">
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
</address>
<email confidence="0.959183">
fkatumaru, komatanig@kuis.kyoto-u.ac.jp
fogata, okunog@kuis.kyoto-u.ac.jp
</email>
<affiliation confidence="0.987108">
‡Honda Research Institute Japan Co., Ltd.
</affiliation>
<address confidence="0.9734825">
8-1 Honcho, Wako, Saitama
351-0188, Japan
</address>
<email confidence="0.989861">
fnakano, funakoshig@jp.honda-ri.com
</email>
<sectionHeader confidence="0.995472" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999708">
The optimal combination of language model
(LM) and language understanding model
(LUM) varies depending on available training
data and utterances to be handled. Usually, a
lot of effort and time are needed to find the op-
timal combination. Instead, we have designed
and developed a new framework that uses
multiple LMs and LUMs to improve speech
understanding accuracy under various situa-
tions. As one implementation of the frame-
work, we have developed a method for select-
ing the most appropriate speech understand-
ing result from several candidates. We use
two LMs and three LUMs, and thus obtain six
combinations of them. We empirically show
that our method improves speech understand-
ing accuracy. The performance of the oracle
selection suggests further potential improve-
ments in our system.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979510638298">
The speech understanding component in a spoken
dialogue system consists of an automatic speech
recognition (ASR) component and a language un-
derstanding (LU) component. To develop a speech
understanding component, we need to prepare an
ASR language model (LM) and a language under-
standing model (LUM) for the dialogue domain
of the system. There are many types of LMs
such as finite-state grammars and N-grams, and
many types of LUMs such as finite-state transduc-
ers (FST), weighted finite-state transducers (WFST),
and keyphrase-extractors (extractor). Selecting a
suitable combination of LM and LUM is necessary
for robust speech understanding against various user
utterances.
Conventional studies of speech understanding
have investigated which LM and LUM give the best
performance by using fixed training and test data
such as the Air Travel Information System (ATIS)
corpus. However, in real system development, re-
sources such as training data for statistical models
and efforts to write finite-state grammars vary ac-
cording to the available human resources or budgets.
Domain-dependent training data are particularly dif-
ficult to obtain. Therefore, in conventional system
development, system developers determine the types
of LM and LUM by trial and error. Every LM and
LUM has some advantages and disadvantages, so it
is difficult for a single combination of LM and LUM
to gain high accuracy except in a situation involv-
ing a lot of training data and effort. Therefore, using
multiple speech understanding methods is a more ef-
fective approach.
In this paper, we propose a speech understand-
ing framework called “Multiple Language models
and Multiple Understanding models (MLMU)”, in
which multiple LMs and LUMs are used, to achieve
better performance under the various development
situations. It selects the best speech understanding
result from the multiple results generated by arbi-
trary combinations of LMs and LUMs.
So far there have been several attempts to im-
prove ASR and speech understanding using mul-
tiple speech recognizers and speech understanding
modules. ROVER (Fiscus, 1997) tried to improve
ASR accuracy by integrating the outputs of multi-
ple ASRs with different acoustic and language mod-
</bodyText>
<page confidence="0.989315">
133
</page>
<note confidence="0.4722065">
Proceedings of NAACL HLT 2009: Short Papers, pages 133–136,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.4397475">
LM: Language Model
LUM: Language Understanding Model
</figure>
<figureCaption confidence="0.999519">
Figure 1: Flow of speech understanding in MLMU
</figureCaption>
<bodyText confidence="0.999953666666667">
els. The work is different from our study in the fol-
lowing two points: it does not deal with speech un-
derstanding, and it assumes that each ASR is well-
developed and achieves high accuracy for a variety
of speech inputs. Eckert et al. (1996) used multiple
LMs to deal with both in-grammar utterances and
out-of-grammar utterances, but did not mention lan-
guage understanding. Hahn et al. (2008) used mul-
tiple LUMs, but just a single language model.
</bodyText>
<sectionHeader confidence="0.989717" genericHeader="method">
2 Speech Understanding Framework
MLMU
</sectionHeader>
<bodyText confidence="0.9999835">
MLMU is a framework by which system developers
can use multiple speech understanding methods by
preparing multiple LMs and multiple LUMs. Fig-
ure 1 illustrates the flow of speech understanding in
MLMU. System developers list available LMs and
LUMs for each system’s domain, and the system
understands utterances by using these models. The
framework selects one understanding result from
multiple results or calculates a confidence score of
the result by using the generated multiple under-
standing results.
MLMU can improve speech understanding for the
following reason. The performance of each speech
understanding (a combination of LM and LUM)
might not be very high when either training data for
the statistical model or available expertise and ef-
fort for writing grammar are insufficient. In such
cases, some utterances might not be covered by the
system’s finite-state grammar LM, and probability
estimation in the statistical models may not be very
good. Using multiple speech understanding mod-
els is expected to solve this problem because each
model has different specialities. For example, finite-
state grammar LMs and FST-based LUMs achieve
high accuracy in recognizing and understanding in-
grammar utterances, whereas out-of-grammar utter-
ances are covered by N-gram models and LUMs
based on WFST and keyphrase-extractors. There-
fore it is more possible that the understanding results
of MLMU will include the correct result than a case
when a single understanding model is used.
The understanding results of MLMU will be help-
ful in many ways. We used them to achieve better
understanding accuracy by selecting the most reli-
able one. This selection is based on features con-
cerning ASR results and language understanding re-
sults. It is also possible to delay the selection, hold-
ing multiple understanding result candidates that
will be disambiguated as the dialogue proceeds (Bo-
hus, 2004). Furthermore, confidence scores, which
enable an efficient dialogue management (Komatani
and Kawahara, 2000), can be calculated by ranking
these results or by voting on them, by using multi-
ple speech understanding results. The understanding
results can be used in the discourse understanding
module and the dialogue management module. They
can choose one of the understanding results depend-
ing on the dialogue situation.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="method">
3 Implementation
</sectionHeader>
<subsectionHeader confidence="0.999476">
3.1 Available Language Models and Language
Understanding Models
</subsectionHeader>
<bodyText confidence="0.9997982">
We implemented MLMU as a library of RIME-
TK, which is a toolkit for building multi-domain
spoken dialogue systems (Nakano et al., 2008).
With the current implementation, developers can use
the following LMs:
</bodyText>
<listItem confidence="0.991190714285714">
1. A LM based on finite-state grammar (FSG)
2. A domain-dependent statistical N-gram model
(N-gram)
and the following LUMs:
1. Finite-state transducer (FST)
2. Weighted FST (WFST)
3. Keyphrase-extractor (extractor).
</listItem>
<bodyText confidence="0.8385985">
System developers can use multiple finite-state-
grammar-based LMs or N-gram-based LMs, and
</bodyText>
<figure confidence="0.998338222222222">
utterance
ASR
component
LM 1
LM 2
LM n
LU
component
LUM 1
LUM 2
LUM n
speech
understanding
results
integration
component
confidence
result
</figure>
<page confidence="0.990232">
134
</page>
<bodyText confidence="0.999921833333333">
also multiple FSTs and WFSTs. They can specify
the combination for each domain by preparing LMs
and LUMs. They can specify grammar models when
sufficient human labor is available for writing gram-
mar, and specify statistical models when a corpus for
training models is available.
</bodyText>
<subsectionHeader confidence="0.85652">
3.2 Selecting Understanding Result based on
ASR and LU Features
</subsectionHeader>
<bodyText confidence="0.987580321428571">
We also implemented a mechanism for selecting one
of the understanding results as the best hypothesis.
The mechanism chooses the result with the highest
estimated probability of correctness. Probabilities
are estimated for each understanding result by using
logistic regression, which uses several ASR and LU
features.
We define Pi as the probability that speech under-
standing result i is correct, and we select one result
based on argmax Pi. We den ote each speech un-
i
derstanding result as i (i = 1,... ,6). We constructed
a logistic regression model for Pi. The regression
function can be written as:
The coefficients ai1, ... , aim, bi were fitted us-
ing training data. The independent variables
Fi1, Fi2, ..., Fim are listed in Table 1. In the table,
n indicates the number of understanding results, that
is, n = 6 in this paper’s experiment. Here, we denote
the features as Fi1, Fi2,..., Fim.
Features from Fi1 to Fi3 represent characteristics
of ASR results. The acoustic scores were normal-
ized by utterance durations in seconds. These fea-
tures are used for verifying its ASR result. Features
from Fi4 to Fig represent characteristics of LU re-
sults. Features from Fi4 to Fi6 are defined on the
basis of the concept-based confidence scores (Ko-
matani and Kawahara, 2000).
</bodyText>
<sectionHeader confidence="0.994884" genericHeader="method">
4 Preliminary Experiment
</sectionHeader>
<bodyText confidence="0.999306333333333">
We conducted a preliminary experiment to show the
potential of the framework by using the two LMs
and three LUMs noted in Section 3.1.
</bodyText>
<tableCaption confidence="0.995692">
Table 1: Features from speech understanding result i
</tableCaption>
<table confidence="0.490118">
Fi1: acoustic score of ASR
Fi2: difference between Fi1 and acoustic score
of ASR for utterance verification
Fi3: utterance duration [sec.]
Fi4: average confidence scores for concepts in i
Fi5: average of Fi4 (1 ∑Z Fi4)
Fi6: proportion of Fi4 (Fi4 / ∑i Fi5)
Fi7: average # concepts (1 ∑i #concept;)
Fib: max. # concepts (max (#concept;) )
Fi9: min. # concepts (min (#concept;) )
</table>
<subsectionHeader confidence="0.999714">
4.1 Preparing LMs and LUMs
</subsectionHeader>
<bodyText confidence="0.9999906">
The finite-state grammar rules were written in sen-
tence units manually. A domain-dependent statisti-
cal N-gram model was trained on 10,000 sentences
randomly generated from the grammar. The vocab-
ulary sizes of the grammar LM and the domain-
dependent statistical LM were both 278. We
also used a domain-independent statistical N-gram
model for obtaining acoustic scores for utterance
verification, which was trained on Web texts (Kawa-
hara et al., 2004). Its vocabulary size was 60,250.
The grammar used in the FST was the same as the
FSG used as one of the LMs, which was manually
written by a system developer. The WFST-based LU
was based on a method to estimate WFST parame-
ters with a small amount of data (Fukubayashi et al.,
2008). Its parameters were estimated by using 105
utterances of just one user. The keyphrase extrac-
tor extracts as many concepts as possible from an
ASR result on the basis of a grammar while ignor-
ing words that do not match the grammar.
</bodyText>
<subsectionHeader confidence="0.99828">
4.2 Target Data for Evaluation
</subsectionHeader>
<bodyText confidence="0.999976888888889">
We used 3,055 utterances in the rent-a-car reserva-
tion domain (Nakano et al., 2007). We used Julius
(ver. 4.0.2) as the speech recognizer and a 3000-
state phonetic tied-mixture (PTM) triphone model
as the acoustic model&apos;. ASR accuracy in mora ac-
curacy when using the FSG and the N-gram model
were 71.9% and 75.5% respectively. We used con-
cept error rates (CERs) to represent the speech un-
derstanding accuracy, which is calculated as fol-
</bodyText>
<footnote confidence="0.755424">
1http://julius.sourceforge.jp/
</footnote>
<equation confidence="0.9723765">
1
Pi =
1 + exp(−(ai1Fi1 + ... + aimFim + bi)).
(1)
</equation>
<page confidence="0.998905">
135
</page>
<tableCaption confidence="0.9602335">
Table 2: CERs [%] for each speech understanding
method
</tableCaption>
<table confidence="0.9485744">
speech understanding method CER
(LM + LUM)
(1) FSG + FST 26.9
(2) FSG + WFST 29.9
(3) FSG + extractor 27.1
(4) N-gram + FST 35.2
(5) N-gram + WFST 25.3
(6) N-gram + extractor 26.0
selection from (1) through (6) (our method) 22.7
oracle selection 13.5
</table>
<sectionHeader confidence="0.285324" genericHeader="method">
lows:
</sectionHeader>
<bodyText confidence="0.99847275">
We manually annotated whether an understanding
result of each utterance was correct or not, and
used them as training data to fit the coefficients
ail, ... , aim, bi.
</bodyText>
<subsectionHeader confidence="0.992661">
4.3 Evaluation in Concept Error Rates
</subsectionHeader>
<bodyText confidence="0.999991181818182">
We fitted the coefficients of regression functions and
selected understanding results with a 10-fold cross
validation. Table 2 lists the CERs based on combi-
nations of single LM and LUM and by our method.
Of all combinations of single LM and LUM, the best
accuracy was obtained with (5) (N-gram + WFST).
Our method improved by 2.6 points over (5). Al-
though we achieved a lower CER, we used a lot
of data to estimate logistic regression coefficients.
Such a large amount of data may not be available in a
real situation. We will conduct more experiments by
changing the amount of training data. Table 2 also
shows the accuracy of the oracle selection, which
selected the best speech understanding result man-
ually. The CER of the oracle selection was 13.5%,
a significant improvement compared to all combina-
tions of a LM and LUM. There is no combination of
a LM and LUM whose understanding results were
not selected at all in the oracle selection and our
method’s selection. These results show that using
multiple LMs and multiple LUMs can potentially
improve speech understanding accuracy.
</bodyText>
<sectionHeader confidence="0.998665" genericHeader="conclusions">
5 Ongoing work
</sectionHeader>
<bodyText confidence="0.999973">
We will conduct more experiments in other domains
or with other resources to evaluate the effectiveness
of our framework. We plan to investigate the case
in which a smaller amount of the training data is
used to estimate the coefficients of the logistic re-
gressions. Furthermore, finding a way to calculate
confidence scores of speech understanding results is
on our agenda.
</bodyText>
<sectionHeader confidence="0.999186" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923333333333">
Dan Bohus. 2004. Error awareness and recovery in
task-oriented spoken dialogue systems. Ph.D. thesis,
Carnegie Mellon University.
Wieland Eckert, Florian Gallwitz, and Heinrich Nie-
mann. 1996. Combining stochastic and linguistic lan-
guage models for recognition of spontaneous speech.
In Proc. ICASSP, pages 423–426.
Jonathan G. Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer Out-
put Voting Error Reduction (ROVER). In Proc. ASRU,
pages 347–354.
Yuichiro Fukubayashi, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tetsuya
Ogata, and Hiroshi G. Okuno. 2008. Rapid prototyp-
ing of robust language understanding modules for spo-
ken dialogue systems. In Proc. IJCNLP, pages 210–
216.
Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008.
System combination for spoken language understand-
ing. In Proc. Interspeech, pages 236–239.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Recent
progress of open-source LVCSR Engine Julius and
Japanese model repository. In Proc. ICSLP, pages
3069–3072.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. COLING, volume 1, pages 467–
473.
Mikio Nakano, Yuka Nagano, Kotaro Funakoshi, Toshi-
hiko Ito, Kenji Araki, Yuji Hasegawa, and Hiroshi Tsu-
jino. 2007. Analysis of user reactions to turn-taking
failures in spoken dialogue systems. In Proc. SIGdial,
pages 120–123.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for building con-
versational agents based on a multi-expert model. In
Proc. SIGdial, pages 88–91.
</reference>
<figure confidence="0.98239325">
CER = # error concepts
(2)
.
#concepts in utterances
</figure>
<page confidence="0.971121">
136
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.142962">
<title confidence="0.9729435">A Speech Understanding that Uses Multiple Language Models and Multiple Understanding Models</title>
<author confidence="0.7292745">Nakano Katsumaru</author>
<author confidence="0.7292745">G Okuno</author>
<affiliation confidence="0.754811">School of Informatics, Kyoto Yoshida-Hommachi, Sakyo,</affiliation>
<address confidence="0.793428">606-8501,</address>
<affiliation confidence="0.996581">Research Institute Japan Co.,</affiliation>
<address confidence="0.8556785">8-1 Honcho, Wako, 351-0188,</address>
<abstract confidence="0.99845565">The optimal combination of language model (LM) and language understanding model (LUM) varies depending on available training data and utterances to be handled. Usually, a lot of effort and time are needed to find the optimal combination. Instead, we have designed and developed a new framework that uses multiple LMs and LUMs to improve speech understanding accuracy under various situations. As one implementation of the framework, we have developed a method for selecting the most appropriate speech understanding result from several candidates. We use two LMs and three LUMs, and thus obtain six combinations of them. We empirically show that our method improves speech understanding accuracy. The performance of the oracle selection suggests further potential improvements in our system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
</authors>
<title>Error awareness and recovery in task-oriented spoken dialogue systems.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="6143" citStr="Bohus, 2004" startWordPosition="934" endWordPosition="936">and LUMs based on WFST and keyphrase-extractors. Therefore it is more possible that the understanding results of MLMU will include the correct result than a case when a single understanding model is used. The understanding results of MLMU will be helpful in many ways. We used them to achieve better understanding accuracy by selecting the most reliable one. This selection is based on features concerning ASR results and language understanding results. It is also possible to delay the selection, holding multiple understanding result candidates that will be disambiguated as the dialogue proceeds (Bohus, 2004). Furthermore, confidence scores, which enable an efficient dialogue management (Komatani and Kawahara, 2000), can be calculated by ranking these results or by voting on them, by using multiple speech understanding results. The understanding results can be used in the discourse understanding module and the dialogue management module. They can choose one of the understanding results depending on the dialogue situation. 3 Implementation 3.1 Available Language Models and Language Understanding Models We implemented MLMU as a library of RIMETK, which is a toolkit for building multi-domain spoken d</context>
</contexts>
<marker>Bohus, 2004</marker>
<rawString>Dan Bohus. 2004. Error awareness and recovery in task-oriented spoken dialogue systems. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wieland Eckert</author>
<author>Florian Gallwitz</author>
<author>Heinrich Niemann</author>
</authors>
<title>Combining stochastic and linguistic language models for recognition of spontaneous speech.</title>
<date>1996</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>423--426</pages>
<contexts>
<context position="4007" citStr="Eckert et al. (1996)" startWordPosition="599" endWordPosition="602">. ROVER (Fiscus, 1997) tried to improve ASR accuracy by integrating the outputs of multiple ASRs with different acoustic and language mod133 Proceedings of NAACL HLT 2009: Short Papers, pages 133–136, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics LM: Language Model LUM: Language Understanding Model Figure 1: Flow of speech understanding in MLMU els. The work is different from our study in the following two points: it does not deal with speech understanding, and it assumes that each ASR is welldeveloped and achieves high accuracy for a variety of speech inputs. Eckert et al. (1996) used multiple LMs to deal with both in-grammar utterances and out-of-grammar utterances, but did not mention language understanding. Hahn et al. (2008) used multiple LUMs, but just a single language model. 2 Speech Understanding Framework MLMU MLMU is a framework by which system developers can use multiple speech understanding methods by preparing multiple LMs and multiple LUMs. Figure 1 illustrates the flow of speech understanding in MLMU. System developers list available LMs and LUMs for each system’s domain, and the system understands utterances by using these models. The framework selects</context>
</contexts>
<marker>Eckert, Gallwitz, Niemann, 1996</marker>
<rawString>Wieland Eckert, Florian Gallwitz, and Heinrich Niemann. 1996. Combining stochastic and linguistic language models for recognition of spontaneous speech. In Proc. ICASSP, pages 423–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER).</title>
<date>1997</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="3409" citStr="Fiscus, 1997" startWordPosition="503" endWordPosition="504">g multiple speech understanding methods is a more effective approach. In this paper, we propose a speech understanding framework called “Multiple Language models and Multiple Understanding models (MLMU)”, in which multiple LMs and LUMs are used, to achieve better performance under the various development situations. It selects the best speech understanding result from the multiple results generated by arbitrary combinations of LMs and LUMs. So far there have been several attempts to improve ASR and speech understanding using multiple speech recognizers and speech understanding modules. ROVER (Fiscus, 1997) tried to improve ASR accuracy by integrating the outputs of multiple ASRs with different acoustic and language mod133 Proceedings of NAACL HLT 2009: Short Papers, pages 133–136, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics LM: Language Model LUM: Language Understanding Model Figure 1: Flow of speech understanding in MLMU els. The work is different from our study in the following two points: it does not deal with speech understanding, and it assumes that each ASR is welldeveloped and achieves high accuracy for a variety of speech inputs. Eckert et al. (1996) u</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER). In Proc. ASRU, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuichiro Fukubayashi</author>
<author>Kazunori Komatani</author>
<author>Mikio Nakano</author>
<author>Kotaro Funakoshi</author>
<author>Hiroshi Tsujino</author>
<author>Tetsuya Ogata</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Rapid prototyping of robust language understanding modules for spoken dialogue systems.</title>
<date>2008</date>
<booktitle>In Proc. IJCNLP,</booktitle>
<pages>210--216</pages>
<contexts>
<context position="10270" citStr="Fukubayashi et al., 2008" startWordPosition="1603" endWordPosition="1606">ram model was trained on 10,000 sentences randomly generated from the grammar. The vocabulary sizes of the grammar LM and the domaindependent statistical LM were both 278. We also used a domain-independent statistical N-gram model for obtaining acoustic scores for utterance verification, which was trained on Web texts (Kawahara et al., 2004). Its vocabulary size was 60,250. The grammar used in the FST was the same as the FSG used as one of the LMs, which was manually written by a system developer. The WFST-based LU was based on a method to estimate WFST parameters with a small amount of data (Fukubayashi et al., 2008). Its parameters were estimated by using 105 utterances of just one user. The keyphrase extractor extracts as many concepts as possible from an ASR result on the basis of a grammar while ignoring words that do not match the grammar. 4.2 Target Data for Evaluation We used 3,055 utterances in the rent-a-car reservation domain (Nakano et al., 2007). We used Julius (ver. 4.0.2) as the speech recognizer and a 3000- state phonetic tied-mixture (PTM) triphone model as the acoustic model&apos;. ASR accuracy in mora accuracy when using the FSG and the N-gram model were 71.9% and 75.5% respectively. We used </context>
</contexts>
<marker>Fukubayashi, Komatani, Nakano, Funakoshi, Tsujino, Ogata, Okuno, 2008</marker>
<rawString>Yuichiro Fukubayashi, Kazunori Komatani, Mikio Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tetsuya Ogata, and Hiroshi G. Okuno. 2008. Rapid prototyping of robust language understanding modules for spoken dialogue systems. In Proc. IJCNLP, pages 210– 216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Hahn</author>
<author>Patrick Lehnen</author>
<author>Hermann Ney</author>
</authors>
<title>System combination for spoken language understanding.</title>
<date>2008</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>236--239</pages>
<contexts>
<context position="4159" citStr="Hahn et al. (2008)" startWordPosition="622" endWordPosition="625">f NAACL HLT 2009: Short Papers, pages 133–136, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics LM: Language Model LUM: Language Understanding Model Figure 1: Flow of speech understanding in MLMU els. The work is different from our study in the following two points: it does not deal with speech understanding, and it assumes that each ASR is welldeveloped and achieves high accuracy for a variety of speech inputs. Eckert et al. (1996) used multiple LMs to deal with both in-grammar utterances and out-of-grammar utterances, but did not mention language understanding. Hahn et al. (2008) used multiple LUMs, but just a single language model. 2 Speech Understanding Framework MLMU MLMU is a framework by which system developers can use multiple speech understanding methods by preparing multiple LMs and multiple LUMs. Figure 1 illustrates the flow of speech understanding in MLMU. System developers list available LMs and LUMs for each system’s domain, and the system understands utterances by using these models. The framework selects one understanding result from multiple results or calculates a confidence score of the result by using the generated multiple understanding results. ML</context>
</contexts>
<marker>Hahn, Lehnen, Ney, 2008</marker>
<rawString>Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008. System combination for spoken language understanding. In Proc. Interspeech, pages 236–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatsuya Kawahara</author>
<author>Akinobu Lee</author>
<author>Kazuya Takeda</author>
<author>Katsunobu Itou</author>
<author>Kiyohiro Shikano</author>
</authors>
<title>Recent progress of open-source LVCSR Engine Julius and Japanese model repository. In</title>
<date>2004</date>
<booktitle>Proc. ICSLP,</booktitle>
<pages>3069--3072</pages>
<contexts>
<context position="9988" citStr="Kawahara et al., 2004" startWordPosition="1548" endWordPosition="1552">on of Fi4 (Fi4 / ∑i Fi5) Fi7: average # concepts (1 ∑i #concept;) Fib: max. # concepts (max (#concept;) ) Fi9: min. # concepts (min (#concept;) ) 4.1 Preparing LMs and LUMs The finite-state grammar rules were written in sentence units manually. A domain-dependent statistical N-gram model was trained on 10,000 sentences randomly generated from the grammar. The vocabulary sizes of the grammar LM and the domaindependent statistical LM were both 278. We also used a domain-independent statistical N-gram model for obtaining acoustic scores for utterance verification, which was trained on Web texts (Kawahara et al., 2004). Its vocabulary size was 60,250. The grammar used in the FST was the same as the FSG used as one of the LMs, which was manually written by a system developer. The WFST-based LU was based on a method to estimate WFST parameters with a small amount of data (Fukubayashi et al., 2008). Its parameters were estimated by using 105 utterances of just one user. The keyphrase extractor extracts as many concepts as possible from an ASR result on the basis of a grammar while ignoring words that do not match the grammar. 4.2 Target Data for Evaluation We used 3,055 utterances in the rent-a-car reservation</context>
</contexts>
<marker>Kawahara, Lee, Takeda, Itou, Shikano, 2004</marker>
<rawString>Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Katsunobu Itou, and Kiyohiro Shikano. 2004. Recent progress of open-source LVCSR Engine Julius and Japanese model repository. In Proc. ICSLP, pages 3069–3072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Flexible mixed-initiative dialogue management using concept-level confidence measures of speech recognizer output.</title>
<date>2000</date>
<booktitle>In Proc. COLING,</booktitle>
<volume>1</volume>
<pages>467--473</pages>
<contexts>
<context position="6252" citStr="Komatani and Kawahara, 2000" startWordPosition="946" endWordPosition="949">standing results of MLMU will include the correct result than a case when a single understanding model is used. The understanding results of MLMU will be helpful in many ways. We used them to achieve better understanding accuracy by selecting the most reliable one. This selection is based on features concerning ASR results and language understanding results. It is also possible to delay the selection, holding multiple understanding result candidates that will be disambiguated as the dialogue proceeds (Bohus, 2004). Furthermore, confidence scores, which enable an efficient dialogue management (Komatani and Kawahara, 2000), can be calculated by ranking these results or by voting on them, by using multiple speech understanding results. The understanding results can be used in the discourse understanding module and the dialogue management module. They can choose one of the understanding results depending on the dialogue situation. 3 Implementation 3.1 Available Language Models and Language Understanding Models We implemented MLMU as a library of RIMETK, which is a toolkit for building multi-domain spoken dialogue systems (Nakano et al., 2008). With the current implementation, developers can use the following LMs:</context>
<context position="8919" citStr="Komatani and Kawahara, 2000" startWordPosition="1373" endWordPosition="1377">itted using training data. The independent variables Fi1, Fi2, ..., Fim are listed in Table 1. In the table, n indicates the number of understanding results, that is, n = 6 in this paper’s experiment. Here, we denote the features as Fi1, Fi2,..., Fim. Features from Fi1 to Fi3 represent characteristics of ASR results. The acoustic scores were normalized by utterance durations in seconds. These features are used for verifying its ASR result. Features from Fi4 to Fig represent characteristics of LU results. Features from Fi4 to Fi6 are defined on the basis of the concept-based confidence scores (Komatani and Kawahara, 2000). 4 Preliminary Experiment We conducted a preliminary experiment to show the potential of the framework by using the two LMs and three LUMs noted in Section 3.1. Table 1: Features from speech understanding result i Fi1: acoustic score of ASR Fi2: difference between Fi1 and acoustic score of ASR for utterance verification Fi3: utterance duration [sec.] Fi4: average confidence scores for concepts in i Fi5: average of Fi4 (1 ∑Z Fi4) Fi6: proportion of Fi4 (Fi4 / ∑i Fi5) Fi7: average # concepts (1 ∑i #concept;) Fib: max. # concepts (max (#concept;) ) Fi9: min. # concepts (min (#concept;) ) 4.1 Pre</context>
</contexts>
<marker>Komatani, Kawahara, 2000</marker>
<rawString>Kazunori Komatani and Tatsuya Kawahara. 2000. Flexible mixed-initiative dialogue management using concept-level confidence measures of speech recognizer output. In Proc. COLING, volume 1, pages 467– 473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Nakano</author>
</authors>
<title>Yuka Nagano, Kotaro Funakoshi, Toshihiko Ito, Kenji Araki, Yuji Hasegawa, and Hiroshi Tsujino.</title>
<date>2007</date>
<booktitle>In Proc. SIGdial,</booktitle>
<pages>120--123</pages>
<marker>Nakano, 2007</marker>
<rawString>Mikio Nakano, Yuka Nagano, Kotaro Funakoshi, Toshihiko Ito, Kenji Araki, Yuji Hasegawa, and Hiroshi Tsujino. 2007. Analysis of user reactions to turn-taking failures in spoken dialogue systems. In Proc. SIGdial, pages 120–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Nakano</author>
<author>Kotaro Funakoshi</author>
<author>Yuji Hasegawa</author>
<author>Hiroshi Tsujino</author>
</authors>
<title>A framework for building conversational agents based on a multi-expert model.</title>
<date>2008</date>
<booktitle>In Proc. SIGdial,</booktitle>
<pages>88--91</pages>
<contexts>
<context position="6780" citStr="Nakano et al., 2008" startWordPosition="1028" endWordPosition="1031">nfidence scores, which enable an efficient dialogue management (Komatani and Kawahara, 2000), can be calculated by ranking these results or by voting on them, by using multiple speech understanding results. The understanding results can be used in the discourse understanding module and the dialogue management module. They can choose one of the understanding results depending on the dialogue situation. 3 Implementation 3.1 Available Language Models and Language Understanding Models We implemented MLMU as a library of RIMETK, which is a toolkit for building multi-domain spoken dialogue systems (Nakano et al., 2008). With the current implementation, developers can use the following LMs: 1. A LM based on finite-state grammar (FSG) 2. A domain-dependent statistical N-gram model (N-gram) and the following LUMs: 1. Finite-state transducer (FST) 2. Weighted FST (WFST) 3. Keyphrase-extractor (extractor). System developers can use multiple finite-stategrammar-based LMs or N-gram-based LMs, and utterance ASR component LM 1 LM 2 LM n LU component LUM 1 LUM 2 LUM n speech understanding results integration component confidence result 134 also multiple FSTs and WFSTs. They can specify the combination for each domain</context>
</contexts>
<marker>Nakano, Funakoshi, Hasegawa, Tsujino, 2008</marker>
<rawString>Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and Hiroshi Tsujino. 2008. A framework for building conversational agents based on a multi-expert model. In Proc. SIGdial, pages 88–91.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>