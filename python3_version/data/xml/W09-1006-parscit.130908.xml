<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.896919">
A note on contextual binary feature grammars
</title>
<author confidence="0.980021">
Alexander Clark Remi Eyraud and Amaury Habrard
</author>
<affiliation confidence="0.939752">
Department of Computer Science Laboratoire d&apos;Informatique Fondamentale
Royal Holloway, University of London de Marseille, CNRS,
alexc@cs.rhul.ac.uk Aix-Marseille University, France
</affiliation>
<email confidence="0.996228">
remi.eyraud,amaury.habrard@lif.univ-mrs.fr
</email>
<sectionHeader confidence="0.995259" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9865174">
Contextual Binary Feature Grammars
were recently proposed by (Clark et al.,
2008) as a learnable representation for
richly structured context-free and con-
text sensitive languages. In this pa-
per we examine the representational
power of the formalism, its relationship
to other standard formalisms and lan-
guage classes, and its appropriateness
for modelling natural language.
</bodyText>
<sectionHeader confidence="0.997907" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999860282608696">
An important issue that concerns both natu-
ral language processing and machine learning
is the ability to learn suitable structures of a
language from a finite sample. There are two
major points that have to be taken into ac-
count in order to define a learning method use-
ful for the two fields: first the method should
rely on intrinsic properties of the language it-
self, rather than syntactic properties of the
representation. Secondly, it must be possible
to associate some semantics to the structural
elements in a natural way.
Grammatical inference is clearly an impor-
tant technology for NLP as it will provide a
foundation for theoretically well-founded un-
supervised learning of syntax, and thus avoid
the annotation bottleneck and the limitations
of working with small hand-labelled treebanks.
Recent advances in context-free grammati-
cal inference have established that there are
large learnable classes of context-free lan-
guages. In this paper, we focus on the ba-
sic representation used by the recent approach
proposed in (Clark et al., 2008). The authors
consider a formalism called Contextual Binary
Feature Grammars (CBFG) which defines a
class of grammars using contexts as features
instead of classical non terminals. The use of
features is interesting from an NLP point of
view because we can associate some semantics
to them, and because we can represent com-
plex, structured syntactic categories. The no-
tion of contexts is relevant from a grammatical
inference standpoint since they are easily ob-
servable from a finite sample. In this paper
we establish some basic language theoretic re-
sults about the class of exact Contextual Bi-
nary Feature Grammars (defined in Section 3),
in particular their relationship to the Chomsky
hierarchy: exact CBFGs are those where the
contextual features are associated to all the
possible strings that can appear in the corre-
sponding contexts of the language defined by
the grammar.
The main results of this paper are proofs
that the class of exact CBFGs:
</bodyText>
<listItem confidence="0.984015">
• properly includes the regular languages
(Section 5),
• does not include some context-free lan-
guages (Section 6),
• and does include some non context-free
languages (Section 7).
</listItem>
<bodyText confidence="0.999957285714286">
Thus, this class of exact CBFGs is orthog-
onal to the classic Chomsky hierarchy but
can represent a very large class of languages.
Moreover, it has been shown that this class
is efficiently learnable. This class is therefore
an interesting candidate for modeling natural
language and deserves further investigation.
</bodyText>
<sectionHeader confidence="0.988513" genericHeader="method">
2 Basic Notation
</sectionHeader>
<bodyText confidence="0.999978857142857">
We consider a finite alphabet E, and E* the
free monoid generated by E. A is the empty
string, and a language is a subset of E*. We
will write the concatenation of u and v as uv,
and similarly for sets of strings. u E E* is a
substring of v E E* if there are strings l, r E E*
such that v = lur.
</bodyText>
<note confidence="0.3272785">
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 33–40,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999224">
33
</page>
<bodyText confidence="0.999956866666667">
A context is an element of E* × E*. For a
string u and a context f = (l, r) we write f O
u = lur; the insertion or wrapping operation.
We extend this to sets of strings and contexts
in the natural way. A context is also known in
structuralist linguistics as an environment.
The set of contexts, or distribution, of a
string u of a language L is, CL(u) = {(l, r) ∈
E* × E*|lur ∈ L}. We will often drop the
subscript where there is no ambiguity. We
define the syntactic congruence as u ≡L v iff
CL(u) = CL(v). The equivalence classes un-
der this relation are the congruence classes of
the language. In general we will assume that
A is not a member of any language.
</bodyText>
<sectionHeader confidence="0.9913665" genericHeader="method">
3 Contextual Binary Feature
Grammars
</sectionHeader>
<bodyText confidence="0.9846355">
Most definitions and lemmas of this section
were first introduced in (Clark et al., 2008).
</bodyText>
<subsectionHeader confidence="0.993865">
3.1 Definition
</subsectionHeader>
<bodyText confidence="0.999674142857143">
Before the presentation of the formalism, we
give some results about contexts to help to
give an intuition of the representation. The
basic insight behind CBFGs is that there is a
relation between the contexts of a string w and
the contexts of its substrings. This is given by
the following trivial lemma:
</bodyText>
<construct confidence="0.881975714285714">
Lemma 1. For any language L and for any
strings u, u&apos;, v, v&apos; if C(u) = C(u&apos;) and C(v) =
C(v&apos;), then C(uv) = C(u&apos;v&apos;).
We can also consider a slightly stronger result:
Lemma 2. For any language L and for any
strings u, u&apos;, v, v&apos; if C(u) ⊆ C(u&apos;) and C(v) ⊆
C(v&apos;), then C(uv) ⊆ C(u&apos;v&apos;).
</construct>
<bodyText confidence="0.977471217391304">
C(u) ⊆ C(u&apos;) means that we can replace
any occurrence of u in a sentence, with a u&apos;,
without affecting the grammaticality, but not
necessarily vice versa. Note that none of these
strings need to correspond to non-terminals:
this is valid for any fragment of a sentence.
We will give a simplified example from En-
glish syntax: the pronoun it can occur every-
where that the pronoun him can, but not vice
versa1. Thus given a sentence “I gave him
away”, we can substitute it for him, to get the
&apos;This example does not account for a number of syn-
tactic and semantic phenomena, particularly the distri-
bution of reflexive anaphors.
grammatical sentence I gave it away, but we
cannot reverse the process. For example, given
the sentence it is raining, we cannot substi-
tute him for it, as we will get the ungrammat-
ical sentence him is raining. Thus we observe
C(him) C C(it).
Looking at Lemma 2 we can also say that,
if we have some finite set of strings K, where
we know the contexts, then:
</bodyText>
<equation confidence="0.9807332">
Corollary 1.
�
C(w) ⊇
u&apos;,v&apos;:
u&apos;v&apos;=w
</equation>
<bodyText confidence="0.9982006">
This is the basis of the representation: a
word w is characterised by its set of contexts.
We can compute the representation of w, from
the representation of its parts u&apos;, v&apos;, by looking
at all of the other matching strings u and v
where we understand how they combine (with
subset inclusion). In order to illustrate this
concept, we give here a simple example.
Consider the language {anbn|n &gt; 0} and
the set K = {aabb, ab, abb, aab, a, b}. Suppose
we want to compute the set of contexts of
aaabbb, Since C(abb) ⊆ C(aabbb), and vacu-
ously C(a) ⊆ C(a), we know that C(aabb) ⊆
C(aaabbb). More generally, the contexts of ab
can represent anbn, those of aab the strings
an+1bn and the ones of abb the strings anbn+1.
The key relationships are given by context
set inclusion. Contextual binary feature gram-
mars allow a proper definition of the combina-
tion of context inclusion:
</bodyText>
<construct confidence="0.996446125">
Definition 1. A Contextual Binary Feature
Grammar (CBFG) G is a tuple hF, P, PL, Ei.
F is a finite set of contexts, called features,
where we write C = 2F for the power set of F
defining the categories of the grammar, P ⊆
C × C × C is a finite set of productions that
we write x → yz where x, y, z ∈ C and PL ⊆
C × E is a set of lexical rules, written x → a.
</construct>
<bodyText confidence="0.917139">
Normally PL contains exactly one production
for each letter in the alphabet (the lexicon).
</bodyText>
<figure confidence="0.70951175">
A CBFG G defines recursively a map fG
U U C(uv)
uEK: vEK:
C(u)CC(u&apos;) C(v)CC(v&apos;)
</figure>
<page confidence="0.762229">
34
</page>
<equation confidence="0.983519125">
from E* → C as follows:
fG(A) = ∅ (1)
�fG(w) = c iff |w |= 1
(c—w)EPL
�fG(w) = U x iff |w |&gt; 1.
u,v:uv=w X—YzEP:
YCfG(u)n
zCfG(v)
</equation>
<bodyText confidence="0.997165">
We give here more explanation about the
map fG. It defines in fact the analysis of a
string by a CBFG. A rule z → xy is applied
to analyse a string w if there is a cut uv = w
s.t. x ⊆ fG(u) and y ⊆ fG(v), recall that x
and y are sets of contexts. Intuitively, the re-
lation given by the production rule is linked
with Lemma 2: z is included in the set of fea-
tures of w = uv. From this relationship, for
any (l, r) ∈ z we have lwr ∈ L(G).
The complete computation of fG is then jus-
tified by Corollary 1: fG(w) defines all the
possible features associated by G to w with all
the possible cuts uv = w (i.e. all the possible
derivations).
Finally, the natural way to define the mem-
bership of a string w in L(G) is to have the
</bodyText>
<construct confidence="0.477347">
context (A, A) ∈ fG(w) which implies that
AuA = u ∈ L(G).
Definition 2. The language defined by a
CBFG G is the set of all strings that are as-
signed the empty context: L(G) = {u|(A, A) ∈
fG(u)}.
</construct>
<bodyText confidence="0.999964153846154">
As we saw before, we are interested in cases
where there is a correspondence between the
language theoretic interpretation of a context,
and the occurrence of that context as a feature
in the grammar. From the basic definition of
a CBFG, we do not require any specific con-
dition on the features of the grammar, except
that a feature is associated to a string if the
string appears in the context defined by the
feature. However, we can also require that fG
defines exactly all the possible features that
can be associated to a given string according
to the underlying language.
</bodyText>
<equation confidence="0.8780978">
Definition 3. Given a finite set of contexts
F = {(l1, r1), ... , (ln, rn)} and a language L
we can define the context feature map FL :
E* → 2F which is just the map u 7→ {(l, r) ∈
F|lur ∈ L} = CL(u) ∩ F.
</equation>
<bodyText confidence="0.999768">
Using this definition, we now need a cor-
respondence between the language theoretic
context feature map FL and the representa-
tion in the CBFG fG.
</bodyText>
<construct confidence="0.919016">
Definition 4. A CBFG G is exact if for all
u ∈ E*, fG(u) = FL(G)(u).
</construct>
<bodyText confidence="0.999676">
Exact CBFGs are a more limited formalism
than CBFGs themselves; without any limits
on the interpretation of the features, we can
define a class of formalisms that is equal to
the class of Conjunctive Grammars (see Sec-
tion 4). However, exactness is an important
notion because it allows to associate intrinsic
components of a language to strings. Contexts
are easily observable from a sample and more-
over it is only when the features correspond to
the contexts that distributional learning algo-
rithms can infer the structure of the language.
A basic example of such a learning algorithm
is given in (Clark et al., 2008).
</bodyText>
<subsectionHeader confidence="0.996616">
3.2 A Parsing Example
</subsectionHeader>
<bodyText confidence="0.957285285714286">
To clarify the relationship with CFG
parsing, we will give a simple worked
example. Consider the CBFG G =
h{(A, A), (aab, A), (A, b), (A, abb), (a, A)(aab, A)},
P, PL, {a, b}i with PL =
{{(A, b), (A, abb)} → a, {(a, A), (aab, A)} → b}
and P =
</bodyText>
<equation confidence="0.71168375">
{{(A, A)} → {(A, b)}{(aab, A)},
{(A, A)} → {(A, abb)}{(a, A)},
{(A, b)} → {(A, abb)}{(A, A)},
{(a, A)} → {(A, A)}{(aab, A)}}.
</equation>
<bodyText confidence="0.999831">
If we want to parse the string w = aabb the
usual way is to have a bottom-up approach.
This means that we recursively compute the
fG map on the substrings of w in order to
check whether (A, A) belongs to fG(w).
The Figure 1 graphically gives the main
steps of the computation of fG(aabb). Ba-
sically there are two ways to split aabb that
allow the derivation of the empty context:
aab|b and a|abb. The first one correspond
to the top part of the figure while the sec-
ond one is drawn at the bottom. We can
see for instance that the empty context be-
longs to fG(ab) thanks to the rule {(A, A)} →
{(A, abb)}{(a, A)}: {(A, abb)} ⊆ fG(a) and
{(a, A)} ⊆ fG(b). But for symmetrical reasons
</bodyText>
<page confidence="0.996543">
35
</page>
<bodyText confidence="0.992883666666667">
the result can also be obtained using the rule
{(A, A)} → {(A, b)}{(aab, A)}.
As we trivially have fG(aa) = fG(bb) = ∅,
since no right-hand side contains the concate-
nation of the same two features, an induction
proof can be written to show that (A, A) ∈
</bodyText>
<figure confidence="0.9579655">
fG(w) ⇔ w ∈ {a&apos;b&apos; : n &gt; 0}.
{(�,b),(�,abb)} {(�,b),(�,abb)} {(a,�),(aab,�)} {(a,�),(aab,�)}
a a b b
{(�,b),(�,abb)} {(�,b),(�,abb)} {(a,�),(aab,�)} {(a,�),(aab,�)}
</figure>
<figureCaption confidence="0.950388">
Figure 1: The two derivations to obtain (A, A)
in fG(aabb) in the grammar G.
</figureCaption>
<bodyText confidence="0.999912285714286">
This is a simple example that illustrates
the parsing of a string given a CBFG. This
example does not characterize the power of
CBFG since no right handside part is com-
posed of more than one context. A more inter-
esting, example with a context-sensitive lan-
guage, will be presented in Section 7.
</bodyText>
<sectionHeader confidence="0.988522" genericHeader="method">
4 Non exact CBFGs
</sectionHeader>
<bodyText confidence="0.999839">
The aim here is to study the expressive power
of CBFG compare to other formalism recently
introduced. Though the inference can be done
only for exact CBFG, where features are di-
rectly linked with observable contexts, it is
still worth having a look at the more general
characteristics of CBFG. For instance, it is in-
teresting to note that several formalisms in-
troduced with the aim of representing natural
languages share strong links with CBFG.
</bodyText>
<subsectionHeader confidence="0.526704">
Range Concatenation Grammars
</subsectionHeader>
<bodyText confidence="0.919330333333333">
Range Concatenation Grammars are a very
powerful formalism (Boullier, 2000), that is a
current area of research in NLP.
</bodyText>
<construct confidence="0.984198">
Lemma 3. For every CBFG G, there is
a non-erasing positive range concatenation
grammar of arity one, in 2-var form that de-
fines the same language.
</construct>
<bodyText confidence="0.9891799">
Proof. Suppose G = hF, P, PL, Ei. Define
a RCG with a set of predicates equal to F
and the following clauses, and the two vari-
ables U, V . For each production x → yz in
P, for each f ∈ x, where y = {g1,... gi},
z = {h1,... hj} add clauses
f(UV ) → g1(U),... gi(U),h1(V ),... hj(V ).
For each lexical production {f1 ... fk} → a
add clauses fi(a) → c. It is straightforward
to verify that f(w) ` c iff f ∈ fG(w).
</bodyText>
<subsectionHeader confidence="0.422954">
Conjunctive Grammar
</subsectionHeader>
<bodyText confidence="0.988161666666667">
A more exact correspondence is to the class of
Conjunctive Grammars (Okhotin, 2001), in-
vented independently of RCGs. For every ev-
ery language L generated by a conjunctive
grammar there is a CBFG representing L#
(where the special character # is not included
in the original alphabet).
Suppose we have a conjunctive grammar
G = hE, N, P, 5i in binary normal form (as
defined in (Okhotin, 2003)). We construct the
equivalent CBFG G&apos; = hF, P&apos;, PL, Ei as fol-
lowed:
</bodyText>
<listItem confidence="0.991472833333333">
• For every letter a we add a context (la, ra)
to F such that laara ∈ L;
• For every rules X → a in P, we create a
rule {(la, ra)} → a in PL.
• For every non terminal X ∈ N, for every
rule X → P1Q1&amp; ... &amp;P,,,Q,,, we add dis-
tinct contexts {(lPiQi, rPiQi)} to F, such
that for all i it exists ui, lPiQiuirPiQi ∈ L
�
and PiQi ⇒G ui;
• Let FX,j = {(lPiQi, rPiQi) : ∀i} the
set of contexts corresponding to the
</listItem>
<bodyText confidence="0.670189">
jth rule applicable to X. For all
</bodyText>
<equation confidence="0.79412535">
fG(aabb) ? {(A,A)}
Rule: (A,A) —. (A,b) (aab,A)
Rule: (A,b) —. (A,abb) (A,A)
fG(aab) ? {(A,b)}
Rule: (A,A) —. (A,abb) (a,A)
fG(ab) ? {(A,A)}
f.
f.
f.
f.
f. f. f. f.
Rule: (A,A) —. (A,b) (aab,A)
fG(ab) ? {(A,A)}
Rule: (a,A) —. (A,A) (aab,A)
fG(abb) ? {(a,A)}
Rule: (A,A) —. (A,abb) (a,A)
fG(aabb) ? {(A,A)}
36
(lPiQi, rPiQi) E FX,j, we add to P&apos; the
rules (lPiQi, rPiQi) —* FPi,kFQi,l (bk,l).
</equation>
<listItem confidence="0.99861075">
• We add a new context (w, A) to F such
that S =*=&gt;.G w and (w, A) —* # to PL;
• For all j, we add to P&apos; the rule (A, A) �
FS,jf(w, A)}.
</listItem>
<bodyText confidence="0.9952115">
It can be shown that this construction gives
an equivalent CBFG.
</bodyText>
<sectionHeader confidence="0.998628" genericHeader="method">
5 Regular Languages
</sectionHeader>
<bodyText confidence="0.999936833333333">
Any regular language can be defined by an ex-
act CBFG. In order to show this we will pro-
pose an approach defining a canonical form for
representing any regular language.
Suppose we have a regular language L, we
consider the left and right residual languages:
</bodyText>
<equation confidence="0.99984">
u−1L = fw|uw E L} (4)
Lu−1 = fw|wu E L} (5)
</equation>
<bodyText confidence="0.963946387096774">
They define two congruencies: if l, l&apos; E u−1L
(resp. r, r&apos; E Lu−1) then for all w E E*, lw E
L iff l&apos;w E L (resp. wr E L iff wr&apos; E L).
For any u E E*, let lmin(u) be the lexico-
graphically shortest element such that l−1
minL =
u−1L. The number of such lmin is finite by
the Myhil-Nerode theorem, we denote by Lmin
this set, i.e. flmin(u)|u E E*}. We de-
fine symmetrically Rmin for the right residuals
(Lr−1
min = Lu−1).
We define the set of contexts as:
F(L) = Lmin X Rmin- (6)
F(L) is clearly finite by construction.
If we consider the regular language de-
fined by the deterministic finite automata
of Figure 2, we obtain Lmin = fA, a, b}
and Rmin = fA, b, ab} and thus F(L) =
f(A, A), (a, A), (b, A), (A, b), (a, b), (b, b), (A, ab),
(a, ab), (b, ab)}.
By considering this set of features, we
can prove (using arguments about congruence
classes) that for any strings u, v such that
FL(u) D FL(v), then CL(u) D CL(v). This
means the set of feature F is sufficient to rep-
resent context inclusion, we call this property
the fiduciality.
Note that the number of congruence classes
of a regular language is finite. Each congru-
ence class is represented by a set of contexts
</bodyText>
<figureCaption confidence="0.791466">
Figure 2: Example of a DFA. The left residuals
</figureCaption>
<bodyText confidence="0.858111333333333">
are defined by A−1L, a−1L, b−1L and the right
ones by LA−1, Lb−1, Lab−1 (note here that
La−1 = LA−1).
FL(u). Let KL be finite set of strings formed
by taking the lexicographically shortest string
from each congruence class. The final gram-
mar can be obtained by combining elements
of KL. For every pair of strings u, v E KL, we
define a rule
</bodyText>
<equation confidence="0.996821">
FL(uv) —* FL(u), FL(v) (7)
</equation>
<bodyText confidence="0.932347714285714">
and we add lexical productions of the form
FL(a) —* a, a E E.
Lemma 4. For all w E E*, fG(w) = FL(w).
Proof. (Sketch) Proof in two steps: bw E
E*, FL(w) C fG(w) and fG(w) C FL(w). Each
step is made by induction on the length of w
and uses the rules created to build the gram-
mar, the derivation process of a CBFG and
the fiduciality for the second step. The key
point rely on the fact that when a string w is
parsed by a CBFG G, there exists a cut of w
in uv = w (u, v E E*) and a rule z —* xy in G
such that x C fG(u) and y C fG(v). The rule
z —* xy is also obtained from a substring from
the set used to build the grammar using the
FL map. By inductive hypothesis you obtain
inclusion between fG and FL on u and v.
For the language of Figure 2, the following
set is sufficient to build an exact CBGF:
fa, b, aa, ab, ba, aab, bb, bba} (this corresponds
to all the substrings of aab and bba). We have:
</bodyText>
<equation confidence="0.953952375">
FL(a) = F(L)\f(A, A), (a, A)} — a
FL(b) = F(L) — b
FL(aa) = FL(a) — FL(a), FL(a)
FL(ab) = F(L) — FL(a), FL(b) = FL(a), F(L)
FL(ba) = F(L) — FL(b), FL(a) = F(L), FL(a)
FL(bb) = F(L) — FL(b), FL(b) = F(L), F(L)
37
FL(aab) = FL(bba) = FL(ab) = FL(ba)
</equation>
<bodyText confidence="0.999922571428571">
The approach presented here gives a canon-
ical form for representing a regular language
by an exact CBFG. Moreover, this is is com-
plete in the sense that every context of every
substring will be represented by some element
of F(L): this CBFG will completely model the
relation between contexts and substrings.
</bodyText>
<sectionHeader confidence="0.996609" genericHeader="method">
6 Context-Free Languages
</sectionHeader>
<bodyText confidence="0.995269">
We now consider the relationship between
CFGs and CBFGs.
</bodyText>
<construct confidence="0.979469">
Definition 5. A context-free grammar (CFG)
is a quadruple G = (E, V, P, S). E is a fi-
nite alphabet, V is a set of non terminals
</construct>
<equation confidence="0.647197">
(E ∩ V = ∅), P ⊆ V × (V ∪ E)+ is a finite
set of productions, S ∈ V is the start symbol.
</equation>
<bodyText confidence="0.989139125">
In the following, we will suppose that a CFG
is represented in Chomsky Normal Form, i.e.
every production is in the form N → UW with
N,U,W ∈ V orN → a with a ∈ E.
We will write uNv ⇒G uαv if there is a pro-
duction N → α ∈ P. ∗⇒G is the reflexive tran-
sitive closure of ⇒G. The language defined by
a CFG GisL(G)={w∈E∗|S ∗ ⇒Gw}.
</bodyText>
<subsectionHeader confidence="0.99808">
6.1 A Simple Characterization
</subsectionHeader>
<bodyText confidence="0.963675558823529">
A simple approach to try to represent a CFG
by a CBFG is to define a bijection between the
set of non terminals and the set of context fea-
tures. Informally we define each non terminal
by a single context and rewrite the productions
of the grammar in the CBFG form.
To build the set of contexts F, it is sufficient
to choose |V  |contexts such that a bijection bC
can be defined between V and F with bC(N) =
(l, r) implies that S ⇒∗ lNr. Note that we fix
bT(S) = (A, A).
Then, we can define a CBFG
hF, P0, P0L, Ei, where P0 = {bT(N) →
bT(U)bT(W)|N → UW ∈ P} and
P0L = {bT(N) → a|N → a ∈ P, a ∈ E}.
A similar proof showing that this construction
produces an equivalent CBFG can be found
in (Clark et al., 2008).
If this approach allows a simple syntactical
convertion of a CFG into a CBFG, it is not
relevant from an NLP point of view. Though
we associate a non-terminal to a context, this
may not correspond to the intrinsic property
of the underlying language. A context could
be associated with many non-terminals and we
choose only one. For example, the context
(He is, A) allows both noun phrases and ad-
jective phrases. In formal terms, the resulting
CBFG is not exact. Then, with the bijection
we introduced before, we are not able to char-
acterize the non-terminals by the contexts in
which they could appear. This is clearly what
we don’t want here and we are more interested
in the relationship with exact CBFG.
</bodyText>
<subsectionHeader confidence="0.989932">
6.2 Not all CFLs have an exact CBFG
</subsectionHeader>
<bodyText confidence="0.999987555555556">
We will show here that the class of context-
free grammars is not strictly included in the
class of exact CBFGs. First, the grammar
defined in Section 3.2 is an exact CBFG for
the context-free and non regular language
{anbn|n &gt; 0}, showing the class of exact
CBFG has some elements in the class of CFGs.
We give now a context-free language L that
can not be defined by an exact CBFG:
</bodyText>
<equation confidence="0.985172">
L = {anb|n &gt; 0} ∪ {amcn|n &gt; m &gt; 0}.
</equation>
<bodyText confidence="0.999932066666667">
Suppose that there exists an exact CBFG that
recognizes it and let N be the length of the
biggest feature (i.e. the longuest left part of
the feature). For any sufficiently large k &gt;
N, the sequences ck and ck+1 share the same
features: FL(ck) = FL(ck+1). Since the CBFG
is exact we have FL(b) ⊆ FL(ck). Thus any
derivation of ak+1b could be a derivation of
ak+1ck which does not belong to the language.
However, this restriction does not mean that
the class of exact CBFG is too restrictive for
modelling natural languages. Indeed, the ex-
ample we have given is highly unnatural and
such phenomena appear not to occur in at-
tested natural languages.
</bodyText>
<sectionHeader confidence="0.999639" genericHeader="method">
7 Context-Sensitive Languages
</sectionHeader>
<bodyText confidence="0.99998375">
We now show that there are some exact
CBFGs that are not context-free. In particu-
lar, we define a language closely related to the
MIX language (consisting of strings with an
equal number of a&apos;s, b&apos;s and c&apos;s in any order)
which is known to be non context-free, and
indeed is conjectured to be outside the class
of indexed grammars (Boullier, 2003).
</bodyText>
<page confidence="0.980117">
38
</page>
<equation confidence="0.9124898">
Let M = {(a, b, c)∗}, we consider the language
L = Labc∪Lab∪Lac∪{a0a, b0b, c0c, dd0, ee0, ff0}:
Lab = {wd|w ∈ M,|w|a = |w|b},
Lac = {we|w ∈ M, |w|a = |w|c},
Labc = {wf|w ∈ M, |w|a = |w|b = |w|c}.
</equation>
<bodyText confidence="0.9999936">
In order to define a CBFG recognizing L, we
have to select features (contexts) that can rep-
resent exactly the intrinsic components of the
languages composing L. We propose to use the
following set of features for each sublanguages:
</bodyText>
<listItem confidence="0.943367285714286">
• For Lab: (λ, d) and (λ, ad), (λ, bd).
• For Lac: (λ, e) and (λ, ae), (λ, ce).
• For Labc: (λ, f).
• For the letters a0, b0, c0, a, b, c we add:
(λ, a), (λ, b), (λ, c), (a0, λ), (b0, λ), (c0, λ).
• For the letters d, e, f, d0, e0, f0 we add;
(λ, d0), (λ, e0), (λ, f0), (d, λ), (e, λ), (f, λ).
</listItem>
<bodyText confidence="0.998193608695652">
Here, Lab will be represented by (λ, d), but we
will use (λ, ad), (λ, bd) to define the internal
derivations of elements of Lab. The same idea
holds for Lac with (λ, e) and (λ, ae), (λ, ce).
For the lexical rules and in order to have an
exact CBFG, note the special case for a, b, c:
{(λ, bd), (λ, ce), (a0, λ)} → a
{(λ, ad), (b0, λ)} → b
{(λ, ad), (λ, ae), (c0, λ)} → c
For the nine other letters, each one is defined
with only one context like {(λ, d0)} → d.
For the production rules, the most impor-
tant one is: (λ, λ) → {(λ, d), (λ, e)}, {(λ, f0)}.
Indeed, this rule, with the presence of two
contexts in one of categories, means that an
element of the language has to be derived
so that it has a prefix u such that fG(u) ⊇
{(λ, d), (λ, e)}. This means u is both an ele-
ment of Lab and Lac. This rule represents the
language Labc since {(λ, f0)} can only repre-
sent the letter f.
The other parts of the language will be
defined by the following rules:
</bodyText>
<equation confidence="0.998683375">
(λ, λ) → {(λ, d)}, {(λ, d0)},
(λ, λ) → {(λ, e)}, {(λ, e0)},
(λ, λ) → {(λ, a)}, {(λ, bd), (λ, ce), (a0, λ)},
(λ, λ) → {(λ, b)}, {(λ, ad), (b0, λ)},
(λ, λ) → {(λ, c)}, {(λ, ad), (λ, ae), (c0, λ)},
(λ, λ) → {(λ, d0)}, {(d, λ)},
(λ, λ) → {(λ, e0)}, {(e, λ)},
(λ, λ) → {(λ, f0)}, {(f, λ)}.
</equation>
<bodyText confidence="0.999995233333333">
This set of rules is incomplete, since for rep-
resenting Lab, the grammar must contain the
rules ensuring to have the same number of a&apos;s
and b&apos;s, and similarly for Lac. To lighten the
presentation here, the complete grammar is
presented in Annex.
We claim this is an exact CBFG for a
context-sensitive language. L is not context-
free since if we intersect L with the regular
language {E∗d}, we get an instance of the
non context-free MIX language (with d ap-
pended). The exactness comes from the fact
that we chose the contexts in order to ensure
that strings belonging to a sublanguage can
not belong to another one and that the deriva-
tion of a substring will provide all the possible
correct features with the help of the union of
all the possible derivations.
Note that the Mix language on its own is
probably not definable by an exact CBFG: it
is only when other parts of the language can
distributionally define the appropriate partial
structures that we can get context sensitive
languages. Far from being a limitation of this
formalism (a bug), we argue this is a feature:
it is only in rather exceptional circumstances
that we will get properly context sensitive lan-
guages. This formalism thus potentially ac-
counts not just for the existence of non context
free natural language but also for their rarity.
</bodyText>
<sectionHeader confidence="0.994203" genericHeader="method">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999593058823529">
The chart in Figure 3 summarises the different
relationship shown in this paper. The substi-
tutable languages (Clark and Eyraud, 2007)
and the very simple ones (Yokomori, 2003)
form two different learnable class of languages.
There is an interesting relationship with Mar-
cus External Contextual Grammars (Mitrana,
2005): if we defined the language of a CBFG
to be the set {fG(u) O u : u ∈ E∗} we would
be taking some steps towards contextual gram-
mars.
In this paper we have discussed the weak
generative power of Exact Contextual Binary
Feature Grammars; we conjecture that the
class of natural language stringsets lie in this
class. ECBFGs are efficiently learnable (see
(Clark et al., 2008) for details) which is a com-
</bodyText>
<page confidence="0.999209">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.397374">
<title confidence="0.999516">A note on contextual binary feature grammars</title>
<author confidence="0.950797">Clark Remi Eyraud Habrard</author>
<affiliation confidence="0.757687666666667">Department of Computer Science Laboratoire d&apos;Informatique Fondamentale Royal Holloway, University of London de Marseille, CNRS, University,</affiliation>
<email confidence="0.995916">remi.eyraud,amaury.habrard@lif.univ-mrs.fr</email>
<abstract confidence="0.996418818181818">Contextual Binary Feature Grammars were recently proposed by (Clark et al., 2008) as a learnable representation for richly structured context-free and context sensitive languages. In this paper we examine the representational power of the formalism, its relationship to other standard formalisms and language classes, and its appropriateness for modelling natural language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>