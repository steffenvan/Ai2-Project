<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.953726">
Building Semantic Perceptron Net for Topic Spotting
</title>
<author confidence="0.968801">
Jimin Liu and Tat-Seng Chua
</author>
<affiliation confidence="0.994529">
School of Computing
National University of Singapore
</affiliation>
<address confidence="0.956994">
SINGAPORE 117543
</address>
<email confidence="0.995677">
{liujm, chuats}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.998588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996546875">
This paper presents an approach to
automatically build a semantic
perceptron net (SPN) for topic spotting.
It uses context at the lower layer to
select the exact meaning of key words,
and employs a combination of context,
co-occurrence statistics and thesaurus to
group the distributed but semantically
related words within a topic to form
basic semantic nodes. The semantic
nodes are then used to infer the topic
within an input document. Experiments
on Reuters 21578 data set demonstrate
that SPN is able to capture the semantics
of topics, and it performs well on topic
spotting task.
</bodyText>
<sectionHeader confidence="0.999015" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999886529411765">
Topic spotting is the problem of identifying the
presence of a predefined topic in a text document.
More formally, given a set of n topics together with
a collection of documents, the task is to determine
for each document the probability that one or more
topics is present in the document. Topic spotting
may be used to automatically assign subject codes
to newswire stories, filter electronic emails and on-
line news, and pre-screen document in information
retrieval and information extraction applications.
Topic spotting, and its related problem of text
categorization, has been a hot area of research for
over a decade. A large number of techniques have
been proposed to tackle the problem, including:
regression model, nearest neighbor classification,
Bayesian probabilistic model, decision tree,
inductive rule learning, neural network, on-line
learning, and, support vector machine (Yang &amp; Liu,
1999; Tzeras &amp; Hartmann, 1993). Most of these
methods are word-based and consider only the
relationships between the features and topics, but
not the relationships among features.
It is well known that the performance of the
word-based methods is greatly affected by the lack
of linguistic understanding, and, in particular, the
inability to handle synonymy and polysemy. A
number of simple linguistic techniques has been
developed to alleviate such problems, ranging from
the use of stemming, lexical chain and thesaurus
(Jing &amp; Tzoukermann, 1999; Green, 1999), to
word-sense disambiguation (Chen &amp; Chang, 1998;
Leacock et al, 1998; Ide &amp; Veronis, 1998) and
context (Cohen &amp; Singer, 1999; Jing &amp;
Tzoukermann, 1999).
The connectionist approach has been widely
used to extract knowledge in a wide range of
information processing tasks including natural
language processing, information retrieval and
image understanding (Anderson, 1983; Lee &amp;
Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp;
Terman, 1995). Because the connectionist
approach closely resembling human cognition
process in text processing, it seems natural to adopt
this approach, in conjunction with linguistic
analysis, to perform topic spotting. However, there
have been few attempts in this direction. This is
mainly because of difficulties in automatically
constructing the semantic networks for the topics.
In this paper, we propose an approach to
automatically build a semantic perceptron net
(SPN) for topic spotting. The SPN is a
connectionist model with hierarchical structure. It
uses a combination of context, co-occurrence
statistics and thesaurus to group the distributed but
semantically related words to form basic semantic
nodes. The semantic nodes are then used to identify
the topic. This paper discusses the design,
implementation and testing of an SPN for topic
spotting.
The paper is organized as follows. Section 2
discusses the topic representation, which is the
prototype structure for SPN. Sections 3 &amp; 4
respectively discuss our approach to extract the
semantic correlations between words, and build
semantic groups and topic tree. Section 5 describes
the building and training of SPN, while Section 6
presents the experiment results. Finally, Section 7
concludes the paper.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="method">
2. Topic Representation
</sectionHeader>
<bodyText confidence="0.972015043478261">
The frame of Minsky (1975) is a well-known
knowledge representation technique. A frame
represents a high-level concept as a collection of
slots, where each slot describes one aspect of the
concept. The situation is similar in topic spotting.
For example, the topic “water” may have many
aspects (or sub-topics). One sub-topic may be
about “water supply”, while the other is about
“water and environment protection”, and so on.
These sub-topics may have some common
attributes, such as the word “water”, and each sub-
topic may be further sub-divided into finer sub-
topics, etc.
The above points to a hierarchical topic
representation, which corresponds to the hierarchy
of document classes (Figure 1). In the model, the
contents of the topics and sub-topics (shown as
circles) are modeled by a set of attributes, which is
simply a group of semantically related words
(shown as solid elliptical shaped bags or
rectangles). The context (shown as dotted ellipses)
is used to identify the exact meaning of a word.
common attribute
</bodyText>
<figureCaption confidence="0.997615">
Figure 1. Topic representation
</figureCaption>
<bodyText confidence="0.999917678571428">
Hofmann (1998) presented a word occurrence
based cluster abstraction model that learns a
hierarchical topic representation. However, the
method is not suitable when the set of training
examples is sparse. To avoid the problem of
automatically constructing the hierarchical model,
Tong et al (1987) required the users to supply the
model, which is used as queries in the system.
Most automated methods, however, avoided this
problem by modeling the topic as a feature vector,
rule set, or instantiated example (Yang &amp; Liu,
1999). These methods typically treat each word
feature as independent, and seldom consider
linguistic factors such as the context or lexical
chain relations among the features. As a result,
these methods are not good at discriminating a
large number of documents that typically lie near
the boundary of two or more topics.
In order to facilitate the automatic extraction
and modeling of the semantic aspects of topics, we
adopt a compromise approach. We model the topic
as a tree of concepts as shown in Figure 1.
However, we consider only one level of hierarchy
built from groups of semantically related words.
These semantic groups may not correspond strictly
to sub-topics within the domain. Figure 2 shows an
example of an automatically constructed topic tree
on “water”.
</bodyText>
<figureCaption confidence="0.994064">
Figure 2. An example of a topic tree
</figureCaption>
<bodyText confidence="0.9997208">
In Figure 2, node “a” contains the common
feature set of the topic; while nodes “b”, “c” and
“d” are related to sub-topics on “water supply”,
“rainfall”, and “water and environment protection”
respectively. Node “e” is the context of the word
“plant”, and node “f” is the context of the word
“bank”. Here we use training to automatically
resolve the corresponding relationship between a
node and an attribute, and the context word to be
used to select the exact meaning of a word. From
this representation, we observe that:
a) Nodes “c” and “d” are closely related and may
not be fully separable. In fact, it is sometimes
difficult even for human experts to decide how
to divide them into separate topics.
</bodyText>
<figure confidence="0.99776806060606">
topic
a word
Sub-topic
Aspect attribute
the context of a word
Topic
water
d
Basic Semantic
Nodes
water
rain
rainfall
dry
a
c
e
price
agreement
water
ton
f
water
river
tourist
Contexts
provide
costumer
corporation
b plant
waste
environment
bank
</figure>
<listItem confidence="0.9068618">
b) The same word, such as “water”, may appear in
both the context node and the basic semantic
node.
c) Some words use context to resolve their
meanings, while many do not need context.
</listItem>
<sectionHeader confidence="0.817827" genericHeader="method">
3. Semantic Correlations
</sectionHeader>
<bodyText confidence="0.9971915">
Although there exists many methods to derive the
semantic correlations between words (Lee, 1999;
Lin, 1998; Karov &amp; Edelman, 1998; Resnik, 1995;
Dagan et al, 1995), we adopt a relatively simple
and yet practical and effective approach to derive
three topic -oriented semantic correlations:
thesaurus-based, co-occurrence-based and context-
based correlation.
</bodyText>
<subsectionHeader confidence="0.999528">
3.1 Thesaurus based correlation
</subsectionHeader>
<bodyText confidence="0.99112054">
WordNet is an electronic thesaurus popularly used
in many researches on lexical semantic acquisition,
and word sense disambiguation (Green, 1999;
Leacock et al, 1998). In WordNet, the sense of a
word is represented by a list of synonyms (synset),
and the lexical information is represented in the
form of a semantic network.
However, it is well known that the granularity
of semantic meanings of words in WordNet is often
too fine for practical use. We thus need to enlarge
the semantic granularity of words in practical
applications. For example, given a topic on
“children education”, it is highly likely that the
word “child” will be a key term. However, the
concept “child” can be expressed in many
semantically related terms, such as “boy”, “girl”,
“kid”, “child”, “youngster”, etc. In this case, it
might not be necessary to distinguish the different
meaning among these words, nor the different
senses within each word. It is, however, important
to group all these words into a large synset {child,
boy, girl, kid, youngster}, and use the synset to
model the dominant but more general meaning of
these words in the context.
In general, it is reasonable and often useful to
group lexically related words together to represent
a more general concept. Here, two words are
considered to be lexically related if they are related
to by the “is_a”, “part_of”, “member_of”, or
“antonym” relations, or if they belong to the same
synset. Figure 3 lists the lexical relations that we
considered, and the examples.
Since in our experiment, there are many
antonyms co-occur within the topic, we also group
antonyms together to identify a topic. Moreover, if
a word had two senses of, say, sense-1 and sense-2.
And if there are two separate words that are
lexically related to this word by sense-1 and sense-
2 respectively, we simply group these words
together and do not attempt to distinguish the two
different senses. The reason is because if a word is
so important to be chosen as the keyword of a
topic, then it should only have one dominant
meaning in that topic. The idea that a keyword
should have only one dominant meaning in a topic
is also suggested in Church &amp; Yarowsky (1992).
synset is_a part_of member_of antonym
Based on the above discussion, we compute the
thesaurus-based correlation between the two terms
t1 and t2, in topic Ti, as:
</bodyText>
<equation confidence="0.9974724">
1 (t1 and t2 are in the same synset, or t1=t2)
0.8 (t1 and t2 have “antonym” relation)
0..5 (t1 and t2 have relations of “is_a”,
“part_of”, or “member_of”)
0 (others)
</equation>
<subsectionHeader confidence="0.999895">
3.2 Co-occurrence based correlation
</subsectionHeader>
<bodyText confidence="0.9996368125">
Co-occurrence relationship is like the global
context of words. Using co-occurrence statistics,
Veling &amp; van der Weerd (1999) was able to find
many interesting conceptual groups in the Reuters-
2178 text corpus. Examples of the conceptual
groups found include: {water, rainfall, dry},
{bomb, injured, explosion, injuries}, and {cola,
PEP, Pepsi, Pespi-cola, Pepsico}. These groups
are meaningful, and are able to capture the
important concepts within the corpus.
Since in general, high co-occurrence words are
likely to be used together to represent (or describe)
a certain concept, it is reasonable to group them
together to form a large semantic node. Thus for
topic Ti, the co-occurrence-based correlation of two
terms, t1 and t2, is computed as:
</bodyText>
<equation confidence="0.993273333333333">
Rco (t1,t2)=df (i)(t1 ∧ t2)df &amp;quot;(t1 ∨ t 2 ) (2)where ( 1 2 )
df (i) t ∧t ( ( 1 2)
df(i) t ∨t ) is the fraction of
</equation>
<bodyText confidence="0.701579">
documents in Ti that contains t1 and (or) t2.
</bodyText>
<subsectionHeader confidence="0.997986">
3.3 Context based correlation
</subsectionHeader>
<bodyText confidence="0.994065166666667">
Broadly speaking, there are three kinds of context:
domain, topic and local contexts (Ide &amp; Vernois,
1998). Domain context requires extensive
knowledge of domain and is not considered in this
paper. Topic context can be modeled
approximately using the co-occurrence
</bodyText>
<figure confidence="0.905310333333333">
corn
maize
pe
</figure>
<figureCaption confidence="0.985652">
Figure 3: Examples of lexical relationship
</figureCaption>
<figure confidence="0.8720880625">
r
zinc
per
metal
tree
leaf
family
son
p
import
export
=
R (i ) t t
( 1 , 2 )
L
(1)
</figure>
<bodyText confidence="0.997906272727273">
relationships between the words in the topic. In this
section, we will define the local context explicitly.
The local context of a word t is often defined as
the set of non-trivial words near t. Here a word wd
is said to be near t if their word distance is less than
a given threshold, which is set to be 5 in our
experiment.
We represent the local context of term tj in topic
Ti by a context vector cv(i)(tj). To derive cv(i)(tj), we
first rank all candidate context words of ti by their
density values:
</bodyText>
<equation confidence="0.997260545454546">
r jk = m wd n t
( ) ( )( )/ ( )( j )
i i i (3)
j k
where ( )
n i t is the number of occurrence of tj in
( )
jTi, and ( )
mj wd is the number of occurrences of
( ) k
i
</equation>
<bodyText confidence="0.883224">
wdk near tj. We then select from the ranking, the top
ten words as the context of tj in Ti as:
</bodyText>
<equation confidence="0.966169777777778">
( )
i ( ) { ( , ) ,( , ), . .. ,( , )
( ) (i ) ( )
i ( ) ( )
r( ) i
cv t = wd i i i
wd (4)
j r wd r
j1 j 1 j 2 j 2 j 10 j 10
</equation>
<bodyText confidence="0.999917285714286">
When the training sample is sufficiently large,
the context vector will have good statistic
meanings. Noting again that an important word to a
topic should have only one dominant meaning
within that topic, and this meaning should be
reflected by its context. We can thus draw the
conclusion that if two words have a very high
context similarity within a topic, it will have a high
possibility that they are semantic related. Therefore
it is reasonable to group them together to form a
larger semantic node. We thus compute the
context-based correlation between two term t1 and
t2 in topic Ti as:
where
</bodyText>
<subsubsectionHeader confidence="0.814444">
s
</subsubsectionHeader>
<bodyText confidence="0.9664378">
For example, in Reuters 21578 corpus,
and
are context-related words
within the topic
This is because they have
</bodyText>
<equation confidence="0.960745307692308">
very
i ( )
i
m k =
( ) arg max (
( )
R wd wd
i ( )
co , )
1 k 2 s
“company”
“corp”
“acq”.
</equation>
<bodyText confidence="0.941484">
similar context of “say, header, acquire,
contract”.
</bodyText>
<sectionHeader confidence="0.925773" genericHeader="method">
4. Semantic Groups &amp; Topic Tree
</sectionHeader>
<bodyText confidence="0.827344583333333">
main keyword. Here we use the semantic
and any other
term tk. Thus:
For each pair
set
if
and
or
and
where
For all tk with
we form a seman
</bodyText>
<equation confidence="0.905881586206896">
{t1,t2,
Ti.
tj,
df(i)(tj)
cv(i)(tj),
df(i)(tj)
Ti
tj.
df(i)(tj)
of tj
Ti.
Gj
tj
tj
(tj,tk),k=1,..n,
Link(tj,tk)=1
( i)
RL(tj,tk)&gt;0,
df(i)(tj)&gt;d0
R(i)
co (tj,tk)&gt;d1
df(i)(tj)&gt;d2
(i )
Rc (tj,tk)&gt;d3.
d0,d1,d2,
Link(tj,tk)=1,
tic
group centered around tj denoted by:
= {t j1 , t j 2 ,..., tj kj } ⊆ {t1,t2,..., tn } (6)
</equation>
<bodyText confidence="0.487399">
documents, the stages involved in finding the
semantic groups for each topic are given below.
</bodyText>
<figure confidence="0.975747">
A) Extract all distinct terms
..tn} from the
training document set for topic
For each term
compute its
and
where
is defined as the fraction of documents in
that
contain
In other words,
gives the
conditional probability
appearing in
B) Derive the semantic group
using
as the
Here
is the main keyword of node
and is
denoted by
C) Calculate the information value
of each
basic seman
tj
Gj
main(Gj)=tj.
inf(i)(Gj)
</figure>
<bodyText confidence="0.955591333333333">
tic group. First we compute the
information value of each tj:
and
the number of topics. Thus 1/N denotes
the probability that a term is in any class, and
denotes the normalized conditional probability
of
in Ti. Only those terms whose normalized
conditional probability is higher than
will
have a positive information value.
The information value of the semantic group
is simply the summation of information value of
its constituent terms weighted by their
maximum seman
</bodyText>
<equation confidence="0.912813289473684">
N is
pij
tj
1/N
Gj
tic correlation with tj as:
R t t
( )
i c ( ,
1 2
∑ ( )
i( )
i ( )
i
R wd wd
co ( ,
1 2 ( ) )
k m k
* ( ) ( )
i
i *
r r
1 k
10
(5)
)
=
1
=
k
)
2m( k
(
� )2
1 /2 (
))2
[∑
r
]
r
i
2 k
k
/ 2
k
1
(7)
}
N
correlations defined in Section 3 to derive the
semantic relationship between
inf( ) ( ) () ( ) * max{0,
i t df t
= i
j j pij
df
=
or,
( )
i
df
( )
i
d3 are predefined thresholds.
(tk)
)
k
t
(
j
1
=
pij
N
k
inf (i)(G j ) = ∑ [wok * inf (i) (tk )] (8)
</equation>
<table confidence="0.496605606060606">
There are many methods that attempt to construct
the conceptual representation of a topic from the
original data set (Veling
der Weerd, 1999;
Baker
1998; Pereira et al, 1993). In
this Section, we will describe our
approach to finding basic semantic groups an
&amp; van
&amp; McCallum,
semantic-based
d
constructing the topic tree. Given a set of training
Gj
where
∑
j
k=1
D) Select the essential seman
wherew = R t t R t t R t t
i
( ) max{ ( ) ( , ), ( )( , ) , ( )( , ) }
i i i
jk co j k c j k L j
tic groups using the
following algorithm:
a) Initialize:
S
{G 1, G 1,...,
} , Groups
←
Gn
←Φ ,
</table>
<listItem confidence="0.875596333333333">
b) Select the semantic group with highest
information value:
← arg max (inf (i) (Gk ))
</listItem>
<equation confidence="0.674167">
k G S
k ∈
</equation>
<listItem confidence="0.371293363636364">
c) Terminate if inf(i)(Gj) is less than a
predefined threshold d4.
d) Add Gj into the set Groups:
S= S− G j, and Groups ← Groups ∪ {G j}
e) Eliminate those groups in S whose key terms
appear in the selected group Gj. That is:
For each Gk ∈ S , if main(Gk) ∈ Gj , then
S ← S − { Gk}
f) Eliminate those terms in remaining groups in
S that are found in the selected group Gj.
That is:
</listItem>
<equation confidence="0.431353">
For each Gk ∈ S, Gk ← Gk − G j ,
and if Gk = Φ, then S←S−{ Gk}
g) If S = Φ then stop; else go to step (b).
</equation>
<bodyText confidence="0.999494571428571">
In the above grouping algorithm, the predefined
thresholds d0,d1,d2,d3 are used to control the size of
each group, and d4 is used to control the number of
groups.
The set of basic semantic groups found then
forms the sub-topics of a 2-layered topic tree as
illustrated in Figure 2.
</bodyText>
<sectionHeader confidence="0.903656" genericHeader="method">
5. Building and Training of SPN
</sectionHeader>
<bodyText confidence="0.999649363636364">
The Combination of local perception and global
arbitrator has been applied to solve perception
problems (Wang &amp; Terman, 1995; Liu &amp; Shi,
2000). Here we adopt the same strategy for topic
spotting. For each topic, we construct a local
perceptron net (LPN), which is designed for a
particular topic. We use a global expert (GE) to
arbitrate all decisions of LPNs and to model the
relationships between topics. Here we discuss the
design of both LPN and GE, and their training
processes.
</bodyText>
<subsectionHeader confidence="0.994502">
5.1 Local Perceptron Net (LPN)
</subsectionHeader>
<bodyText confidence="0.998262461538461">
We derive the LPN directly from the topic tree as
discussed in Sectio n 2 (see Figure 2). Each LPN is
a multi-layer feed-forward neural network with a
typical structure as shown in Figure 4.
In Figure 4, xij represents the feature value of
keyword wdij in the ith semantic group; xijk’s (where
k=1,...10) represent the feature values of the context
words wdijk‘s of keyword wdij; and aij denotes the
meaning of keyword wdij as determined by its
context. Ai corresponds to the ith basic semantic
node. The weights wi, wij, and wijk and biases èi and
èij are learned from training, and y(i)(x) is the output
of the network.
</bodyText>
<figureCaption confidence="0.976481">
Figure 4: The architecture of LPN for topic i
</figureCaption>
<bodyText confidence="0.908293">
Given a document:
</bodyText>
<equation confidence="0.801011">
x = {(xij,cvij)  |i=1,2,...m, j=1,...ij}
</equation>
<bodyText confidence="0.9996936">
where m is the number of basic semantic nodes, ij
is the number of key terms contained in the ith
semantic node, and cvij={xij1,xij2... xijkij } is the
context of term xij. The output y(i) =y(i)(x) is
calculated as follows:
</bodyText>
<equation confidence="0.9991572">
m y i y i x
( )
= ( ) ( ) (9)
= ∑ w A
i i
</equation>
<bodyText confidence="0.9903658">
Equation (10) expresses the fact that only if a
key term is present in the document (i.e. xij &gt; 0), its
context needs to be checked.
For each topic Ti, there is a corresponding net
y(i)=y(i)(x) and a threshold q(i). The pair of (y(i)(x),
q(i)) is a local binary classifier for Ti such that:
If y(i)(x)-q(i) &gt; 0, then Ti is present; otherwise
Ti is not present in document x.
From the procedures employed to building the
topic tree, we know that each feature is in fact an
evidence to support the occurrence of the topic.
This gives us the suggestion that the activation
function for each node in the LPN should be a non-
decreasing function of the inputs. Thus we impose
a weight constraint on the LPN as:
</bodyText>
<equation confidence="0.533121">
wi&gt;0, wij&gt;0, wijk&gt;0 (12)
</equation>
<subsectionHeader confidence="0.98714">
5.2 Global expert (GE)
</subsectionHeader>
<bodyText confidence="0.999184714285714">
Since there are relations among topics, and LPNs
do not have global information, it is inevitable that
LPNs will make wrong decisions. In order to
overcome this problem, we use a global expert
(GE) to arbitrate al local decisions. Figure 5
illustrates the use of global expert to combine the
outputs of LPNs.
</bodyText>
<figure confidence="0.999590872727273">
q (i)
y(i)
Class
xij
xijk
wi
Semantic
group
Ai
w ij
key term
aij
wijk
Basic
meaning
Context
q ij
xijk
∈ cvij
)
ji
)
ji
j
=
=
i
and
1 exp(
−
ij
∑
ij
∑
j
wia
1
w
(11)
1
=
1
=
−
1 exp(
+
i
a
1
where aij = xij `
1 exp[ (
+ − ∑ wijk `xijk−qy)]
(10)
A i
j
</figure>
<figureCaption confidence="0.999979">
Figure 5: The architecture of global expert
</figureCaption>
<bodyText confidence="0.998426333333333">
Given a document x, we first use each LPN to
make a local decision. We then combine the
outputs of LPNs as follows:
</bodyText>
<equation confidence="0.994582875">
( ) ( ) ( )
i i ( ) ( ) ( )
j i
= ( q ) [ W y
− + ∑
i j
Y y ( − q ) − Θ
ij
</equation>
<bodyText confidence="0.999257">
where Wij’s are the weights between the global
arbitrator i and the jth LPN; and ( i)
</bodyText>
<equation confidence="0.511093">
Θ ’s are the
</equation>
<bodyText confidence="0.9675575625">
global bias. From the result of Equation (13), we
have:
If Y(i) &gt; 0; then topic Ti is present; otherwise
Ti is not present in document x
The use of Equation (13) implies that:
a) If a LPN is not activated, i.e., y(i) £ q(i), then its
output is not used in the GE. Thus it will not
affect the output of other LPN.
b) The weight Wij models the relationship or
correlation between topic i and j. If Wij &gt; 0, it
means that if document x is related to Tj, it may
also have some contribution ( Wij) to topic Tj. On
the other hand, if Wij &lt; 0, it means the two
topics are negatively correlated, and a document
x will not be related to both Tj and Ti.
The overall structure of SPN is as follows:
</bodyText>
<figure confidence="0.798860333333333">
Global Expert
Local Perception
Input document
</figure>
<figureCaption confidence="0.993648">
Figure 6: Overall structure of SPN
</figureCaption>
<subsectionHeader confidence="0.989961">
5.3 The Training of SPN
</subsectionHeader>
<bodyText confidence="0.9944455">
In order to adopt SPN for topic spotting, we
employ the well-known BP algorithm to derive the
optimal weights and biases in SPN. The training
phase is divided to two stages. The first stage
learns a LPN for each topic, while the second stage
trains the GE. As the BP algorithm is rather
standard, we will discuss only the error functions
that we employ to guide the training process.
In topic spotting, the goal is to achieve both
high recall and precision. In particular, we want to
allow y(x) to be as large (or as small) as possible in
cases when there is no error, or when +
</bodyText>
<equation confidence="0.9932146">
x∈ Ω and
y(x) &gt; q (or −
x ∈ Ω and y( x) &lt; q ). Here +
Ω and −
Ω
</equation>
<bodyText confidence="0.999974666666667">
denote the positive and negative training document
sets respectively. To achieve this, we adopt a new
error function as follows to train the LPN:
</bodyText>
<equation confidence="0.9667735">
E w
( , , ,
ijk ij ij i
q w w
⎧ 1
⎪⎨ ( ) ( )
2
− &lt;q
e q
+ q
x x
( , )
x = 2 , and
⎪⎩ 0 ( )
x ≥ q
e − (x,q) =e+(−x,−q )
Equation (14) defines a piecewise differentiable
error function. The coefficients
|Ω−|
+  ||
Ω+
 |Ω + |are used to ensure that the contributions
 ||
Ω +
−  ||
Ω+
</equation>
<bodyText confidence="0.99980675">
of positive and negative examples are equal.
After the training, we choose the node with the
biggest wi value as the common attribute node.
Also, we trim the topic representation by removing
those words or context words with very small wij or
wijk values.
We adopt the following error function to train
GE:
</bodyText>
<equation confidence="0.999338909090909">
n + −(15)
E W
( , ) [
Θ = ∑
∑ e ( ( ), )
Y x Θ + ∑ e Y x Θ
i i i
( ( ), )]
ij i i i i
i=1 ∈Ω+ ∈Ω −
x i x
</equation>
<bodyText confidence="0.9871335">
where +
Ωi is the set of positive examples of Ti.
</bodyText>
<sectionHeader confidence="0.997054" genericHeader="evaluation">
6. Experiment and Discussion
</sectionHeader>
<bodyText confidence="0.999976555555556">
We employ the ModApte Split version of Reuters-
21578 corpus to test our method. In order to ensure
that the training is meaningful, we select only those
classes that have at least one document in each of
the training and test sets. This results in 90 classes
in both the training and test sets. After eliminating
documents that do not belong to any of these 90
classes, we obtain a training set of 7,770
documents and a test set of 3,019 documents.
From the set of training documents, we derive the
set of semantic nodes for each topic using the
procedures outlined in Section 4. From the training
set, we found that the average number of semantic
nodes for each topic is 132, and the average
number of terms in each node is 2.4. For
illustration, Table 1 lists some examples of the
semantic nodes that we found. From table 1, we
can draw the following general observations.
</bodyText>
<figure confidence="0.984377769230769">
y (1) − q (1) y−q y −q
(i) (i) (j) (j)
Θ
(i)
Y(i)
Wji
i
≠
j
y
(j)−q(j
)&gt;0
y(i)
Y(i)
x
 ||
Ω −
,
=
q)
∑e+ ( ( ), )
y x q
+
∈Ω
(14)
|Ω−|+|Ω+|x
+
 ||
Ω+
∑
e − ( ( ), )
y x q
−
∈Ω
|Ω−|+|Ω+|x
] (13)
where
− |
 |Ω and
</figure>
<table confidence="0.990471916666667">
Node Semantic Node Method used Topic
ID (SN) to find SNs
1 wheat 1 Wheat
2 import, export, 1,2,3
output
3 farmer, production, 2
mln, ton
4 disease, insect, pest 2
5 fall, fell, rise, rose 3 Wpi
Method 1 – by looking up WordNet
Method 2 – by analyzing co-occurrence correlation
Method 3 – by analyzing context correlation
</table>
<tableCaption confidence="0.999885">
Table 1: Examples of semantic nodes
</tableCaption>
<bodyText confidence="0.99986118">
a) Under the topic “wheat”, we list four semantic
nodes. Node 1 contains the common attribute
set of the topic. Node 2 is related to the “buying
and selling of wheat”. Node 3 is related to
“wheat production”; and node 4 is related to
“the effects of insect on wheat production”. The
results show that the automatically extracted
basic semantic nodes are meaningful and are
able to capture most semantics of a topic.
b) Node 1 originally contains two terms “wheat”
and “corn” that belong to the same synset found
by looking up WordNet. However, in the
training stage, the weight of the word “corn”
was found to be very small in topic “wheat”,
and hence it was removed from the semantic
group. This is similar to the discourse based
word sense disambiguation.
c) The granularity of information expressed by the
semantic nodes may not be the same as what
human expert produces. For example, it is
possible that a human expert may divide node 2
into two nodes {import} and {export, output}.
d) Node 5 contains four words and is formed by
analyzing context. Each context vector of the
four words has the same two components:
“price” and “digital number”. Meanwhile,
“rise” and “fall” can also be grouped together
by “antonym” relation. “fell” is actually the past
tense of “fall”. This means that by comparing
context, it is possible to group together those
words with grammatical variations without
performing grammatical analysis.
Table 2 summarizes the results of SPN in terms
of macro and micro F1 values (see Yang &amp; Liu
(1999) for definitions of the macro and micro F1
values). For comparison purpose, the Table also
lists the results of other TC methods as reported in
Yang &amp; Liu (1999). From the table, it can be seen
that the SPN method achieves the best macF1
value. This indicates that the method performs well
on classes with a small number of training samples.
In terms of the micro F1 measures, SPN out-
performs NB, NNet, LSF and KNN, while posting
a slightly lower performance than that of SVM.
The results are encouraging as they are rather
preliminary. We expect the results to improve
further by tuning the system ranging from the
initial values of various parameters, to the choice
of error functions, context, grouping algorithm, and
the structures of topic tree and SPN.
</bodyText>
<table confidence="0.999377285714286">
Method MicR MicP micF1 macF1
SVM 0.8120 0.9137 0.8599 0.5251
KNN 0.8339 0.8807 0.8567 0.5242
LSF 0.8507 0.8489 0.8498 0.5008
NNet 0.7842 0.8785 0.8287 0.3763
NB 0.7688 0.8245 0.7956 0.3886
SPN 0.8402 0.8743 0.8569 0.6275
</table>
<tableCaption confidence="0.999896">
Table 2. The performance comparison
</tableCaption>
<sectionHeader confidence="0.97466" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.999966260869565">
In this paper, we proposed an approach to
automatically build semantic perceptron net (SPN)
for topic spotting. The SPN is a connectionist
model in which context is used to select the exact
meaning of a word. By analyzing the context and
co-occurrence statistics, and by looking up
thesaurus, it is able to group the distributed but
semantic related words together to form basic
semantic nodes. Experiments on Reuters 21578
show that, to some extent, SPN is able to capture
the semantics of topics and it performs well on
topic spotting task.
It is well known that human expert, whose most
prominent characteristic is the ability to understand
text documents, have a strong natural ability to spot
topics in documents. We are, however, unclear
about the nature of human cognition, and with the
present state-of-art natural language processing
technology, it is still difficult to get an in-depth
understanding of a text passage. We believe that
our proposed approach provides a promising
compromise between full understanding and no
understanding.
</bodyText>
<sectionHeader confidence="0.980212" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9999786">
The authors would like to acknowledge the support
of the National Science and Technology Board, and
the Ministry of Education of Singapore for the
provision of a research grant RP3989903 under
which this research is carried out.
</bodyText>
<sectionHeader confidence="0.998479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948148148148">
J.R. Anderson (1983). A Spreading Activation
Theory of Memory. J. of Verbal Learning &amp;
Verbal Behavior, 22(3):261-295.
L.D. Baker &amp; A.K. McCallum (1998).
Distributional Clustering of Words for Text
Classification. SIGIR’98.
J.N. Chen &amp; J.S. Chang (1998). Topic Clustering
of MRD Senses based on Information Retrieval
Technique. Comp Linguistic, 24(1), 62-95.
G.W.K. Church &amp; D. Yarowsky (1992). One Sense
per Discourse. Proc. of 4th DARPA Speech and
Natural Language Workshop. 233-237.
W.W. Cohen &amp; Y. Singer (1999). Context-
Sensitive Learning Method for Text
Categorization. ACM Trans. on Information
Systems, 17(2), 141-173, Apr.
I. Dagan, S. Marcus &amp; S. Markovitch (1995).
Contextual Word Similarity and Estimation
from Sparse Data. Computer speech and
Language, 9:123-152.
S.J. Green (1999). Building Hypertext Links by
Computing Semantic Similarity. IEEE Trans on
Knowledge &amp; Data Engr, 11(5).
T. Hofmann (1998). Learning and Representing
Topic, a Hierarchical Mixture Model for Word
Occurrences in Document Databases.
Workshop on Learning from Text and the
Web, CMU.
N. Ide &amp; J. Veronis (1998). Introduction to the
Special Issue on Word Sense Disambiguation:
the State of Art. Comp Linguistics, 24(1), 1-39.
H. Jing &amp; E. Tzoukermann (1999). Information
Retrieval based on Context Distance and
Morphology. SIGIR’99, 90-96.
Y. Karov &amp; S. Edelman (1998). Similarity-based
Word Sense Disambiguation, Computational
Linguistics, 24(1), 41-59.
C. Leacock &amp; M. Chodorow &amp; G. Miller (1998).
Using Corpus Statistics and WordNet for Sense
Identification. Comp. Linguistic, 24(1), 147-
165.
L. Lee (1999). Measure of Distributional
Similarity. Proc of 37th Annual Meeting of
ACL.
J. Lee &amp; D. Dubin (1999). Context-Sensitive
Vocabulary Mapping with a Spreading
Activation Network. SIGIR’99, 198-205.
D. Lin (1998). Automatic Retrieval and Clustering
of Similar Words. In COLING-ACL’98, 768-
773.
J. Liu &amp; Z. Shi (2000). Extracting Prominent
Shape by Local Interactions and Global
Optimizations. CVPRIP’2000, USA.
M.A. Minsky (1975). A Framework for
Representing Knowledge. In: Winston P (eds).
“The psychology of computer vision”,
McGraw-Hill, New York, 211-277.
F.C.N. Pereira, N.Z. Tishby &amp; L. Lee (1993).
Distributional Clustering of English Words.
ACL’93, 183-190.
P. Resnik (1995). Using Information Content to
Evaluate Semantic Similarity in a Taxonomy.
Proc of IJCAI-95, 448-453.
S. Sarkas &amp; K.L. Boyer (1995). Using Perceptual
Inference Network to Manage Vision
Processes. Computer Vision &amp; Image
Understanding, 62(1), 27-46.
R. Tong, L. Appelbaum, V. Askman &amp; J.
Cunningham (1987). Conceptual Information
Retrieval using RUBRIC. SIGIR’87, 247– 253.
K. Tzeras &amp; S. Hartmann (1993). Automatic
Indexing based on Bayesian Inference
Networks. SIGIR’93, 22-34.
A. Veling &amp; P. van der Weerd (1999). Conceptual
Grouping in Word Co-occurrence Networks.
IJCAI 99: 694-701.
D. Wang &amp; D. Terman (1995). Locally Excitatory
Globally Inhibitory Oscillator Networks. IEEE
Trans. Neural Network. 6(1).
Y. Yang &amp; X. Liu (1999). Re-examination of Text
Categorization. SIGIR’99, 43-49.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935775">
<title confidence="0.999982">Building Semantic Perceptron Net for Topic Spotting</title>
<author confidence="0.99236">Jimin Liu</author>
<author confidence="0.99236">Tat-Seng Chua</author>
<affiliation confidence="0.9996835">School of Computing National University of Singapore</affiliation>
<address confidence="0.976841">SINGAPORE 117543</address>
<email confidence="0.976328">chuats}@comp.nus.edu.sg</email>
<abstract confidence="0.999283">This paper presents an approach to automatically build a semantic perceptron net (SPN) for topic spotting. It uses context at the lower layer to select the exact meaning of key words, and employs a combination of context, co-occurrence statistics and thesaurus to group the distributed but semantically related words within a topic to form basic semantic nodes. The semantic nodes are then used to infer the topic within an input document. Experiments on Reuters 21578 data set demonstrate that SPN is able to capture the semantics of topics, and it performs well on topic spotting task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Anderson</author>
</authors>
<title>A Spreading Activation Theory of Memory.</title>
<date>1983</date>
<journal>J. of Verbal Learning &amp; Verbal Behavior,</journal>
<pages>22--3</pages>
<contexts>
<context position="2631" citStr="Anderson, 1983" startWordPosition="398" endWordPosition="399">lar, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been few attempts in this direction. This is mainly because of difficulties in automatically constructing the semantic networks for the topics. In this paper, we propose an approach to automatically build a semantic perceptron net (SPN) for topic spotting. The SPN is a connectionist model with hierarchical </context>
</contexts>
<marker>Anderson, 1983</marker>
<rawString>J.R. Anderson (1983). A Spreading Activation Theory of Memory. J. of Verbal Learning &amp; Verbal Behavior, 22(3):261-295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L D Baker</author>
<author>A K McCallum</author>
</authors>
<title>Distributional Clustering of Words for Text Classification.</title>
<date>1998</date>
<marker>Baker, McCallum, 1998</marker>
<rawString>L.D. Baker &amp; A.K. McCallum (1998). Distributional Clustering of Words for Text Classification. SIGIR’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Chen</author>
<author>J S Chang</author>
</authors>
<date>1998</date>
<booktitle>Topic Clustering of MRD Senses based on Information Retrieval Technique. Comp Linguistic,</booktitle>
<volume>24</volume>
<issue>1</issue>
<pages>62--95</pages>
<contexts>
<context position="2308" citStr="Chen &amp; Chang, 1998" startWordPosition="349" endWordPosition="352">&amp; Liu, 1999; Tzeras &amp; Hartmann, 1993). Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. Howev</context>
</contexts>
<marker>Chen, Chang, 1998</marker>
<rawString>J.N. Chen &amp; J.S. Chang (1998). Topic Clustering of MRD Senses based on Information Retrieval Technique. Comp Linguistic, 24(1), 62-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W K Church</author>
<author>D Yarowsky</author>
</authors>
<title>One Sense per Discourse.</title>
<date>1992</date>
<booktitle>Proc. of 4th DARPA Speech and Natural Language Workshop.</booktitle>
<pages>233--237</pages>
<contexts>
<context position="10099" citStr="Church &amp; Yarowsky (1992)" startWordPosition="1610" endWordPosition="1613">ms co-occur within the topic, we also group antonyms together to identify a topic. Moreover, if a word had two senses of, say, sense-1 and sense-2. And if there are two separate words that are lexically related to this word by sense-1 and sense2 respectively, we simply group these words together and do not attempt to distinguish the two different senses. The reason is because if a word is so important to be chosen as the keyword of a topic, then it should only have one dominant meaning in that topic. The idea that a keyword should have only one dominant meaning in a topic is also suggested in Church &amp; Yarowsky (1992). synset is_a part_of member_of antonym Based on the above discussion, we compute the thesaurus-based correlation between the two terms t1 and t2, in topic Ti, as: 1 (t1 and t2 are in the same synset, or t1=t2) 0.8 (t1 and t2 have “antonym” relation) 0..5 (t1 and t2 have relations of “is_a”, “part_of”, or “member_of”) 0 (others) 3.2 Co-occurrence based correlation Co-occurrence relationship is like the global context of words. Using co-occurrence statistics, Veling &amp; van der Weerd (1999) was able to find many interesting conceptual groups in the Reuters2178 text corpus. Examples of the concept</context>
</contexts>
<marker>Church, Yarowsky, 1992</marker>
<rawString>G.W.K. Church &amp; D. Yarowsky (1992). One Sense per Discourse. Proc. of 4th DARPA Speech and Natural Language Workshop. 233-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
<author>Y Singer</author>
</authors>
<title>ContextSensitive Learning Method for Text Categorization.</title>
<date>1999</date>
<journal>ACM Trans. on Information Systems,</journal>
<volume>17</volume>
<issue>2</issue>
<pages>141--173</pages>
<contexts>
<context position="2385" citStr="Cohen &amp; Singer, 1999" startWordPosition="363" endWordPosition="366"> and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been few attempts in this direction. This is mainly because of</context>
</contexts>
<marker>Cohen, Singer, 1999</marker>
<rawString>W.W. Cohen &amp; Y. Singer (1999). ContextSensitive Learning Method for Text Categorization. ACM Trans. on Information Systems, 17(2), 141-173, Apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>S Marcus</author>
<author>S Markovitch</author>
</authors>
<date>1995</date>
<booktitle>Contextual Word Similarity and Estimation from Sparse Data. Computer speech and Language,</booktitle>
<pages>9--123</pages>
<contexts>
<context position="7693" citStr="Dagan et al, 1995" startWordPosition="1212" endWordPosition="1215"> topic a word Sub-topic Aspect attribute the context of a word Topic water d Basic Semantic Nodes water rain rainfall dry a c e price agreement water ton f water river tourist Contexts provide costumer corporation b plant waste environment bank b) The same word, such as “water”, may appear in both the context node and the basic semantic node. c) Some words use context to resolve their meanings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov &amp; Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic network. However, it is well known that the granularity of</context>
</contexts>
<marker>Dagan, Marcus, Markovitch, 1995</marker>
<rawString>I. Dagan, S. Marcus &amp; S. Markovitch (1995). Contextual Word Similarity and Estimation from Sparse Data. Computer speech and Language, 9:123-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Green</author>
</authors>
<title>Building Hypertext Links by Computing Semantic Similarity.</title>
<date>1999</date>
<journal>IEEE Trans on Knowledge &amp; Data Engr,</journal>
<volume>11</volume>
<issue>5</issue>
<contexts>
<context position="2258" citStr="Green, 1999" startWordPosition="344" endWordPosition="345">learning, and, support vector machine (Yang &amp; Liu, 1999; Tzeras &amp; Hartmann, 1993). Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with lin</context>
<context position="8063" citStr="Green, 1999" startWordPosition="1262" endWordPosition="1263">olve their meanings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov &amp; Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic network. However, it is well known that the granularity of semantic meanings of words in WordNet is often too fine for practical use. We thus need to enlarge the semantic granularity of words in practical applications. For example, given a topic on “children education”, it is highly likely that the word “child” will be a key term. However, the concept “child” can be expressed in many semantically related terms, such as “boy”</context>
</contexts>
<marker>Green, 1999</marker>
<rawString>S.J. Green (1999). Building Hypertext Links by Computing Semantic Similarity. IEEE Trans on Knowledge &amp; Data Engr, 11(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Learning and Representing Topic, a Hierarchical Mixture Model for Word Occurrences in Document Databases.</title>
<date>1998</date>
<booktitle>Workshop on Learning from Text and the Web,</booktitle>
<location>CMU.</location>
<contexts>
<context position="5049" citStr="Hofmann (1998)" startWordPosition="774" endWordPosition="775">me common attributes, such as the word “water”, and each subtopic may be further sub-divided into finer subtopics, etc. The above points to a hierarchical topic representation, which corresponds to the hierarchy of document classes (Figure 1). In the model, the contents of the topics and sub-topics (shown as circles) are modeled by a set of attributes, which is simply a group of semantically related words (shown as solid elliptical shaped bags or rectangles). The context (shown as dotted ellipses) is used to identify the exact meaning of a word. common attribute Figure 1. Topic representation Hofmann (1998) presented a word occurrence based cluster abstraction model that learns a hierarchical topic representation. However, the method is not suitable when the set of training examples is sparse. To avoid the problem of automatically constructing the hierarchical model, Tong et al (1987) required the users to supply the model, which is used as queries in the system. Most automated methods, however, avoided this problem by modeling the topic as a feature vector, rule set, or instantiated example (Yang &amp; Liu, 1999). These methods typically treat each word feature as independent, and seldom consider l</context>
</contexts>
<marker>Hofmann, 1998</marker>
<rawString>T. Hofmann (1998). Learning and Representing Topic, a Hierarchical Mixture Model for Word Occurrences in Document Databases. Workshop on Learning from Text and the Web, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Veronis</author>
</authors>
<title>Introduction to the Special Issue on Word Sense Disambiguation: the State of Art.</title>
<date>1998</date>
<journal>Comp Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>1--39</pages>
<contexts>
<context position="2351" citStr="Ide &amp; Veronis, 1998" startWordPosition="357" endWordPosition="360">st of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been few attempts in this di</context>
</contexts>
<marker>Ide, Veronis, 1998</marker>
<rawString>N. Ide &amp; J. Veronis (1998). Introduction to the Special Issue on Word Sense Disambiguation: the State of Art. Comp Linguistics, 24(1), 1-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>E Tzoukermann</author>
</authors>
<date>1999</date>
<booktitle>Information Retrieval based on Context Distance and Morphology. SIGIR’99,</booktitle>
<pages>90--96</pages>
<contexts>
<context position="2244" citStr="Jing &amp; Tzoukermann, 1999" startWordPosition="340" endWordPosition="343">, neural network, on-line learning, and, support vector machine (Yang &amp; Liu, 1999; Tzeras &amp; Hartmann, 1993). Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjun</context>
</contexts>
<marker>Jing, Tzoukermann, 1999</marker>
<rawString>H. Jing &amp; E. Tzoukermann (1999). Information Retrieval based on Context Distance and Morphology. SIGIR’99, 90-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Karov</author>
<author>S Edelman</author>
</authors>
<title>Similarity-based Word Sense Disambiguation,</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>41--59</pages>
<contexts>
<context position="7659" citStr="Karov &amp; Edelman, 1998" startWordPosition="1206" endWordPosition="1209"> to divide them into separate topics. topic a word Sub-topic Aspect attribute the context of a word Topic water d Basic Semantic Nodes water rain rainfall dry a c e price agreement water ton f water river tourist Contexts provide costumer corporation b plant waste environment bank b) The same word, such as “water”, may appear in both the context node and the basic semantic node. c) Some words use context to resolve their meanings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov &amp; Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic network. However, it is </context>
</contexts>
<marker>Karov, Edelman, 1998</marker>
<rawString>Y. Karov &amp; S. Edelman (1998). Similarity-based Word Sense Disambiguation, Computational Linguistics, 24(1), 41-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>G Miller</author>
</authors>
<title>Using Corpus Statistics and WordNet for Sense Identification.</title>
<date>1998</date>
<journal>Comp. Linguistic,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>147--165</pages>
<contexts>
<context position="2329" citStr="Leacock et al, 1998" startWordPosition="353" endWordPosition="356">&amp; Hartmann, 1993). Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been f</context>
<context position="8085" citStr="Leacock et al, 1998" startWordPosition="1264" endWordPosition="1267">anings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov &amp; Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic network. However, it is well known that the granularity of semantic meanings of words in WordNet is often too fine for practical use. We thus need to enlarge the semantic granularity of words in practical applications. For example, given a topic on “children education”, it is highly likely that the word “child” will be a key term. However, the concept “child” can be expressed in many semantically related terms, such as “boy”, “girl”, “kid”, “chil</context>
</contexts>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>C. Leacock &amp; M. Chodorow &amp; G. Miller (1998). Using Corpus Statistics and WordNet for Sense Identification. Comp. Linguistic, 24(1), 147-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Measure of Distributional Similarity.</title>
<date>1999</date>
<booktitle>Proc of 37th Annual Meeting of ACL.</booktitle>
<contexts>
<context position="7625" citStr="Lee, 1999" startWordPosition="1202" endWordPosition="1203"> experts to decide how to divide them into separate topics. topic a word Sub-topic Aspect attribute the context of a word Topic water d Basic Semantic Nodes water rain rainfall dry a c e price agreement water ton f water river tourist Contexts provide costumer corporation b plant waste environment bank b) The same word, such as “water”, may appear in both the context node and the basic semantic node. c) Some words use context to resolve their meanings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov &amp; Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>L. Lee (1999). Measure of Distributional Similarity. Proc of 37th Annual Meeting of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>D Dubin</author>
</authors>
<title>Context-Sensitive Vocabulary Mapping with a Spreading Activation Network.</title>
<date>1999</date>
<volume>99</volume>
<pages>198--205</pages>
<contexts>
<context position="2650" citStr="Lee &amp; Dubin, 1999" startWordPosition="400" endWordPosition="403">ty to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been few attempts in this direction. This is mainly because of difficulties in automatically constructing the semantic networks for the topics. In this paper, we propose an approach to automatically build a semantic perceptron net (SPN) for topic spotting. The SPN is a connectionist model with hierarchical structure. It uses </context>
</contexts>
<marker>Lee, Dubin, 1999</marker>
<rawString>J. Lee &amp; D. Dubin (1999). Context-Sensitive Vocabulary Mapping with a Spreading Activation Network. SIGIR’99, 198-205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In COLING-ACL’98,</booktitle>
<pages>768--773</pages>
<contexts>
<context position="7636" citStr="Lin, 1998" startWordPosition="1204" endWordPosition="1205"> decide how to divide them into separate topics. topic a word Sub-topic Aspect attribute the context of a word Topic water d Basic Semantic Nodes water rain rainfall dry a c e price agreement water ton f water river tourist Contexts provide costumer corporation b plant waste environment bank b) The same word, such as “water”, may appear in both the context node and the basic semantic node. c) Some words use context to resolve their meanings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov &amp; Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic n</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin (1998). Automatic Retrieval and Clustering of Similar Words. In COLING-ACL’98, 768-773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liu</author>
<author>Z Shi</author>
</authors>
<title>Extracting Prominent Shape by Local Interactions and Global Optimizations.</title>
<date>2000</date>
<location>CVPRIP’2000, USA.</location>
<contexts>
<context position="17117" citStr="Liu &amp; Shi, 2000" startWordPosition="3041" endWordPosition="3044"> S that are found in the selected group Gj. That is: For each Gk ∈ S, Gk ← Gk − G j , and if Gk = Φ, then S←S−{ Gk} g) If S = Φ then stop; else go to step (b). In the above grouping algorithm, the predefined thresholds d0,d1,d2,d3 are used to control the size of each group, and d4 is used to control the number of groups. The set of basic semantic groups found then forms the sub-topics of a 2-layered topic tree as illustrated in Figure 2. 5. Building and Training of SPN The Combination of local perception and global arbitrator has been applied to solve perception problems (Wang &amp; Terman, 1995; Liu &amp; Shi, 2000). Here we adopt the same strategy for topic spotting. For each topic, we construct a local perceptron net (LPN), which is designed for a particular topic. We use a global expert (GE) to arbitrate all decisions of LPNs and to model the relationships between topics. Here we discuss the design of both LPN and GE, and their training processes. 5.1 Local Perceptron Net (LPN) We derive the LPN directly from the topic tree as discussed in Sectio n 2 (see Figure 2). Each LPN is a multi-layer feed-forward neural network with a typical structure as shown in Figure 4. In Figure 4, xij represents the feat</context>
</contexts>
<marker>Liu, Shi, 2000</marker>
<rawString>J. Liu &amp; Z. Shi (2000). Extracting Prominent Shape by Local Interactions and Global Optimizations. CVPRIP’2000, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Minsky</author>
</authors>
<title>A Framework for Representing Knowledge. In: Winston P (eds). “The psychology of computer vision”,</title>
<date>1975</date>
<pages>211--277</pages>
<publisher>McGraw-Hill,</publisher>
<location>New York,</location>
<contexts>
<context position="4006" citStr="Minsky (1975)" startWordPosition="607" endWordPosition="608"> nodes. The semantic nodes are then used to identify the topic. This paper discusses the design, implementation and testing of an SPN for topic spotting. The paper is organized as follows. Section 2 discusses the topic representation, which is the prototype structure for SPN. Sections 3 &amp; 4 respectively discuss our approach to extract the semantic correlations between words, and build semantic groups and topic tree. Section 5 describes the building and training of SPN, while Section 6 presents the experiment results. Finally, Section 7 concludes the paper. 2. Topic Representation The frame of Minsky (1975) is a well-known knowledge representation technique. A frame represents a high-level concept as a collection of slots, where each slot describes one aspect of the concept. The situation is similar in topic spotting. For example, the topic “water” may have many aspects (or sub-topics). One sub-topic may be about “water supply”, while the other is about “water and environment protection”, and so on. These sub-topics may have some common attributes, such as the word “water”, and each subtopic may be further sub-divided into finer subtopics, etc. The above points to a hierarchical topic representa</context>
</contexts>
<marker>Minsky, 1975</marker>
<rawString>M.A. Minsky (1975). A Framework for Representing Knowledge. In: Winston P (eds). “The psychology of computer vision”, McGraw-Hill, New York, 211-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>N Z Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional Clustering of English Words.</title>
<date>1993</date>
<volume>93</volume>
<pages>183--190</pages>
<contexts>
<context position="15663" citStr="Pereira et al, 1993" startWordPosition="2726" endWordPosition="2729">orrelation with tj as: R t t ( ) i c ( , 1 2 ∑ ( ) i( ) i ( ) i R wd wd co ( , 1 2 ( ) ) k m k * ( ) ( ) i i * r r 1 k 10 (5) ) = 1 = k ) 2m( k ( � )2 1 /2 ( ))2 [∑ r ] r i 2 k k / 2 k 1 (7) } N correlations defined in Section 3 to derive the semantic relationship between inf( ) ( ) () ( ) * max{0, i t df t = i j j pij df = or, ( ) i df ( ) i d3 are predefined thresholds. (tk) ) k t ( j 1 = pij N k inf (i)(G j ) = ∑ [wok * inf (i) (tk )] (8) There are many methods that attempt to construct the conceptual representation of a topic from the original data set (Veling der Weerd, 1999; Baker 1998; Pereira et al, 1993). In this Section, we will describe our approach to finding basic semantic groups an &amp; van &amp; McCallum, semantic-based d constructing the topic tree. Given a set of training Gj where ∑ j k=1 D) Select the essential seman wherew = R t t R t t R t t i ( ) max{ ( ) ( , ), ( )( , ) , ( )( , ) } i i i jk co j k c j k L j tic groups using the following algorithm: a) Initialize: S {G 1, G 1,..., } , Groups ← Gn ←Φ , b) Select the semantic group with highest information value: ← arg max (inf (i) (Gk )) k G S k ∈ c) Terminate if inf(i)(Gj) is less than a predefined threshold d4. d) Add Gj into the set G</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F.C.N. Pereira, N.Z. Tishby &amp; L. Lee (1993). Distributional Clustering of English Words. ACL’93, 183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy. Proc of IJCAI-95,</title>
<date>1995</date>
<pages>448--453</pages>
<contexts>
<context position="7673" citStr="Resnik, 1995" startWordPosition="1210" endWordPosition="1211">parate topics. topic a word Sub-topic Aspect attribute the context of a word Topic water d Basic Semantic Nodes water rain rainfall dry a c e price agreement water ton f water river tourist Contexts provide costumer corporation b plant waste environment bank b) The same word, such as “water”, may appear in both the context node and the basic semantic node. c) Some words use context to resolve their meanings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov &amp; Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic network. However, it is well known tha</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik (1995). Using Information Content to Evaluate Semantic Similarity in a Taxonomy. Proc of IJCAI-95, 448-453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sarkas</author>
<author>K L Boyer</author>
</authors>
<title>Using Perceptual Inference Network to Manage Vision Processes.</title>
<date>1995</date>
<journal>Computer Vision &amp; Image Understanding,</journal>
<volume>62</volume>
<issue>1</issue>
<pages>27--46</pages>
<contexts>
<context position="2672" citStr="Sarkas &amp; Boyer, 1995" startWordPosition="404" endWordPosition="407">my and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been few attempts in this direction. This is mainly because of difficulties in automatically constructing the semantic networks for the topics. In this paper, we propose an approach to automatically build a semantic perceptron net (SPN) for topic spotting. The SPN is a connectionist model with hierarchical structure. It uses a combination of conte</context>
</contexts>
<marker>Sarkas, Boyer, 1995</marker>
<rawString>S. Sarkas &amp; K.L. Boyer (1995). Using Perceptual Inference Network to Manage Vision Processes. Computer Vision &amp; Image Understanding, 62(1), 27-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tong</author>
<author>L Appelbaum</author>
<author>V Askman</author>
<author>J Cunningham</author>
</authors>
<date>1987</date>
<booktitle>Conceptual Information Retrieval using RUBRIC. SIGIR’87,</booktitle>
<pages>247--253</pages>
<contexts>
<context position="5332" citStr="Tong et al (1987)" startWordPosition="814" endWordPosition="817">pics and sub-topics (shown as circles) are modeled by a set of attributes, which is simply a group of semantically related words (shown as solid elliptical shaped bags or rectangles). The context (shown as dotted ellipses) is used to identify the exact meaning of a word. common attribute Figure 1. Topic representation Hofmann (1998) presented a word occurrence based cluster abstraction model that learns a hierarchical topic representation. However, the method is not suitable when the set of training examples is sparse. To avoid the problem of automatically constructing the hierarchical model, Tong et al (1987) required the users to supply the model, which is used as queries in the system. Most automated methods, however, avoided this problem by modeling the topic as a feature vector, rule set, or instantiated example (Yang &amp; Liu, 1999). These methods typically treat each word feature as independent, and seldom consider linguistic factors such as the context or lexical chain relations among the features. As a result, these methods are not good at discriminating a large number of documents that typically lie near the boundary of two or more topics. In order to facilitate the automatic extraction and </context>
</contexts>
<marker>Tong, Appelbaum, Askman, Cunningham, 1987</marker>
<rawString>R. Tong, L. Appelbaum, V. Askman &amp; J. Cunningham (1987). Conceptual Information Retrieval using RUBRIC. SIGIR’87, 247– 253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tzeras</author>
<author>S Hartmann</author>
</authors>
<title>Automatic Indexing based on Bayesian Inference Networks.</title>
<date>1993</date>
<volume>93</volume>
<pages>22--34</pages>
<contexts>
<context position="1727" citStr="Tzeras &amp; Hartmann, 1993" startWordPosition="261" endWordPosition="264">y be used to automatically assign subject codes to newswire stories, filter electronic emails and online news, and pre-screen document in information retrieval and information extraction applications. Topic spotting, and its related problem of text categorization, has been a hot area of research for over a decade. A large number of techniques have been proposed to tackle the problem, including: regression model, nearest neighbor classification, Bayesian probabilistic model, decision tree, inductive rule learning, neural network, on-line learning, and, support vector machine (Yang &amp; Liu, 1999; Tzeras &amp; Hartmann, 1993). Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 19</context>
</contexts>
<marker>Tzeras, Hartmann, 1993</marker>
<rawString>K. Tzeras &amp; S. Hartmann (1993). Automatic Indexing based on Bayesian Inference Networks. SIGIR’93, 22-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Veling</author>
<author>P van der Weerd</author>
</authors>
<title>Conceptual Grouping in Word Co-occurrence Networks.</title>
<date>1999</date>
<journal>IJCAI</journal>
<volume>99</volume>
<pages>694--701</pages>
<marker>Veling, van der Weerd, 1999</marker>
<rawString>A. Veling &amp; P. van der Weerd (1999). Conceptual Grouping in Word Co-occurrence Networks. IJCAI 99: 694-701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wang</author>
<author>D Terman</author>
</authors>
<title>Locally Excitatory Globally Inhibitory Oscillator Networks.</title>
<date>1995</date>
<journal>IEEE Trans. Neural Network.</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="2694" citStr="Wang &amp; Terman, 1995" startWordPosition="408" endWordPosition="411">ber of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee &amp; Dubin, 1999; Sarkas &amp; Boyer, 1995; Wang &amp; Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been few attempts in this direction. This is mainly because of difficulties in automatically constructing the semantic networks for the topics. In this paper, we propose an approach to automatically build a semantic perceptron net (SPN) for topic spotting. The SPN is a connectionist model with hierarchical structure. It uses a combination of context, co-occurrence stat</context>
<context position="17099" citStr="Wang &amp; Terman, 1995" startWordPosition="3037" endWordPosition="3040">n remaining groups in S that are found in the selected group Gj. That is: For each Gk ∈ S, Gk ← Gk − G j , and if Gk = Φ, then S←S−{ Gk} g) If S = Φ then stop; else go to step (b). In the above grouping algorithm, the predefined thresholds d0,d1,d2,d3 are used to control the size of each group, and d4 is used to control the number of groups. The set of basic semantic groups found then forms the sub-topics of a 2-layered topic tree as illustrated in Figure 2. 5. Building and Training of SPN The Combination of local perception and global arbitrator has been applied to solve perception problems (Wang &amp; Terman, 1995; Liu &amp; Shi, 2000). Here we adopt the same strategy for topic spotting. For each topic, we construct a local perceptron net (LPN), which is designed for a particular topic. We use a global expert (GE) to arbitrate all decisions of LPNs and to model the relationships between topics. Here we discuss the design of both LPN and GE, and their training processes. 5.1 Local Perceptron Net (LPN) We derive the LPN directly from the topic tree as discussed in Sectio n 2 (see Figure 2). Each LPN is a multi-layer feed-forward neural network with a typical structure as shown in Figure 4. In Figure 4, xij r</context>
</contexts>
<marker>Wang, Terman, 1995</marker>
<rawString>D. Wang &amp; D. Terman (1995). Locally Excitatory Globally Inhibitory Oscillator Networks. IEEE Trans. Neural Network. 6(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>X Liu</author>
</authors>
<title>Re-examination of Text Categorization.</title>
<date>1999</date>
<volume>99</volume>
<pages>43--49</pages>
<contexts>
<context position="1701" citStr="Yang &amp; Liu, 1999" startWordPosition="257" endWordPosition="260"> Topic spotting may be used to automatically assign subject codes to newswire stories, filter electronic emails and online news, and pre-screen document in information retrieval and information extraction applications. Topic spotting, and its related problem of text categorization, has been a hot area of research for over a decade. A large number of techniques have been proposed to tackle the problem, including: regression model, nearest neighbor classification, Bayesian probabilistic model, decision tree, inductive rule learning, neural network, on-line learning, and, support vector machine (Yang &amp; Liu, 1999; Tzeras &amp; Hartmann, 1993). Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chan</context>
<context position="5562" citStr="Yang &amp; Liu, 1999" startWordPosition="853" endWordPosition="856"> to identify the exact meaning of a word. common attribute Figure 1. Topic representation Hofmann (1998) presented a word occurrence based cluster abstraction model that learns a hierarchical topic representation. However, the method is not suitable when the set of training examples is sparse. To avoid the problem of automatically constructing the hierarchical model, Tong et al (1987) required the users to supply the model, which is used as queries in the system. Most automated methods, however, avoided this problem by modeling the topic as a feature vector, rule set, or instantiated example (Yang &amp; Liu, 1999). These methods typically treat each word feature as independent, and seldom consider linguistic factors such as the context or lexical chain relations among the features. As a result, these methods are not good at discriminating a large number of documents that typically lie near the boundary of two or more topics. In order to facilitate the automatic extraction and modeling of the semantic aspects of topics, we adopt a compromise approach. We model the topic as a tree of concepts as shown in Figure 1. However, we consider only one level of hierarchy built from groups of semantically related </context>
<context position="25443" citStr="Yang &amp; Liu (1999)" startWordPosition="4699" endWordPosition="4702"> human expert may divide node 2 into two nodes {import} and {export, output}. d) Node 5 contains four words and is formed by analyzing context. Each context vector of the four words has the same two components: “price” and “digital number”. Meanwhile, “rise” and “fall” can also be grouped together by “antonym” relation. “fell” is actually the past tense of “fall”. This means that by comparing context, it is possible to group together those words with grammatical variations without performing grammatical analysis. Table 2 summarizes the results of SPN in terms of macro and micro F1 values (see Yang &amp; Liu (1999) for definitions of the macro and micro F1 values). For comparison purpose, the Table also lists the results of other TC methods as reported in Yang &amp; Liu (1999). From the table, it can be seen that the SPN method achieves the best macF1 value. This indicates that the method performs well on classes with a small number of training samples. In terms of the micro F1 measures, SPN outperforms NB, NNet, LSF and KNN, while posting a slightly lower performance than that of SVM. The results are encouraging as they are rather preliminary. We expect the results to improve further by tuning the system r</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Y. Yang &amp; X. Liu (1999). Re-examination of Text Categorization. SIGIR’99, 43-49.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>