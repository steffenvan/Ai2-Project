<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.971182">
Third-order Variational Reranking on Packed-Shared Dependency Forests
</title>
<author confidence="0.990629">
Katsuhiko Hayashi†, Taro Watanabe‡, Masayuki Asahara†, Yuji Matsumoto††Nara Insutitute of Science and Technology
</author>
<affiliation confidence="0.983164">
Ikoma, Nara, 630-0192, Japan
‡National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.778732">
Sorakugun, Kyoto, 619-0289, Japan
</address>
<email confidence="0.9771435">
{katsuhiko-h,masayu-a,matsu}@is.naist.jp
taro.watanabe@nict.go.jp
</email>
<sectionHeader confidence="0.981015" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999798941176471">
We propose a novel forest reranking algorithm
for discriminative dependency parsing based
on a variant of Eisner’s generative model. In
our framework, we define two kinds of gener-
ative model for reranking. One is learned from
training data offline and the other from a for-
est generated by a baseline parser on the fly.
The final prediction in the reranking stage is
performed using linear interpolation of these
models and discriminative model. In order to
efficiently train the model from and decode
on a hypergraph data structure representing a
forest, we apply extended inside/outside and
Iiterbi algorithms. Experimental results show
that our proposed forest reranking algorithm
achieves significant improvement when com-
pared with conventional approaches.
</bodyText>
<sectionHeader confidence="0.994271" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999804627906977">
Recently, much of research on statistical parsing
has been focused on k-best (or forest) reranking
(Collins, 2000; Charniak and Johnson, 2005; Huang,
2008). Typically, reranking methods first generate
a list of top-k candidates (or a forest) from a base-
line system, then rerank the candidates with arbi-
trary features that are intractable within the baseline
system. In the reranking framework, the baseline
system is usually modeled with a generative model,
and a discriminative model is used for reranking.
Sangati et al. (2009) reversed the usual order of the
two models for dependency parsing by employing
a generative model to rescore the k-best candidates
provided by a discriminative model. They use a vari-
ant of Eisner’s generative model C (Eisner, 1996b;
Eisner, 1996a) for reranking and extend it to capture
higher-order information than Eisner’s second-order
generative model. Their reranking model showed
large improvements in dependency parsing accu-
racy. They reported that the discriminative model is
very effective at filtering out bad candidates, while
the generative model is able to further refine the se-
lection among the few best candidates.
In this paper, we propose a forest generative
reranking algorithm, opposed to Sangati et al.
(2009)’s approach which reranks only k-best candi-
dates. Forests usually encode better candidates more
compactly than k-best lists (Huang, 2008). More-
over, our reranking uses not only a generative model
obtained from training data, but also a sentence spe-
cific generative model learned from a forest. In the
reranking stage, we use linearly combined model
of these models. We call this variational rerank-
ing model. The model proposed in this paper is
factored in the third-order structure, therefore, its
non-locality makes it difficult to perform the rerank-
ing with an usual 1-best Iiterbi search. To solve
this problem, we also propose a new search algo-
rithm, which is inspired by the third-order dynamic
programming parsing algorithm (Koo and Collins,
2010). This algorithm enables us an exact 1-best
reranking without any approximation. We summa-
rize our contributions in this paper as follows.
</bodyText>
<listItem confidence="0.9974766">
• To extend k-best to forest generative reranking.
• We introduce variational reranking which is a
combination approach of generative reranking
and variational decoding (Li et al., 2009).
• To obtain 1-best tree in the reranking stage, we
</listItem>
<page confidence="0.447836">
1479
</page>
<note confidence="0.760981">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479–1488,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
top0,8
saw1,8:V
I1
</note>
<page confidence="0.953488">
,2:
</page>
<bodyText confidence="0.999482666666667">
propose an exact 1-best search algorithm with
the third-order model.
In experiments on English Penn Treebank data,
we show that our proposed methods bring signif-
icant improvement to dependency parsing. More-
over, our variational reranking framework achieves
consistent improvement, compared to conventional
approaches, such as simple k-best and forest-based
generative reranking algorithms.
</bodyText>
<sectionHeader confidence="0.980275" genericHeader="method">
2 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.9990066">
Given an input sentence x E X, the task of statis-
tical dependency parsing is to predict output depen-
dencies y� for x. The task is usually modeled within a
discriminative framework, defined by the following
equation:
</bodyText>
<equation confidence="0.99341575">
y� = argmax s(x, y)
yEY
= argmax AT - F(y, x) (1)
yEY
</equation>
<bodyText confidence="0.9991698">
where Y is the output space, A is a parameter vector,
and F() is a set of feature functions.
We denote a set of candidates as G(x). By using
G(x), the conditional probability p(y|x) is typically
derived as follows:
</bodyText>
<equation confidence="0.987447">
N gir
x eγ·s (x,y) = eγ·s(x,y) 2
p(y |) — Z(x) FI yEG(x) eγ·s(x,y) ( )
</equation>
<bodyText confidence="0.999904666666667">
where s(x, y) is the score function shown in Eq.1
and γ is a scaling factor to adjust the sharpness of
the distribution and Z(x) is a normarization factor.
</bodyText>
<subsectionHeader confidence="0.996742">
2.1 Hypergraph Representation
</subsectionHeader>
<bodyText confidence="0.997755888888889">
We propose to encode many hypotheses in a com-
pact representation called dependency forest. While
there may be exponentially many dependency trees,
the forest represents them in polynomial space. A
dependency forest (or tree) can be defined as a hy-
pergraph data strucure HG (Tu et al., 2010).
Figure 1 shows an example of a hypergraph for a
dependency tree. A shaded hyperedge e is defined
as the following form:
</bodyText>
<equation confidence="0.8014735">
e : ((I1,2, girl3,5, with5,8), saw1,8).
1480
</equation>
<tableCaption confidence="0.621890555555556">
Table 1: An event list of tri-sibling model whose event
space is v|h, sib, tsib, dir, extracted from hyperedge e in
Figure 1. EOC is an end symbol of sequence.
eventspace
I I saw NONE NONE L
EOC I saw I NONE L
girl I saw NONE NONE R
with I saw girl NONE R
EOC I saw with girl R
</tableCaption>
<bodyText confidence="0.917075">
the probability of our model q is defined as follows:
</bodyText>
<equation confidence="0.9988525">
q(vl|C(vl)) · q(vl)
q(vr|C(vr)) · q(vr) (3)
</equation>
<bodyText confidence="0.999319153846154">
where |tailsL(e) |and |tailsR(e) |are the number of
left and right children of v, vl and vr are the left and
right child of position l and r in each side. C(v) is
a context event space of v. We explain the context
event space later in more detail. The probability of
the entire dependency tree y is recursively computed
by q(y(top)) where y(top) denotes a top node of y.
The probability q(v|C(v)) is dependent on a con-
text space C(v) for a node v. We define two kinds of
context spaces. First, we define a tri-sibling model
whose context space consists of the head node, sib-
ling node, tri-sibling node and direction of a node
v:
</bodyText>
<equation confidence="0.999167">
q1(v|C(v)) = q1(v|h,sib, tsib, dir) (4)
</equation>
<bodyText confidence="0.999885777777778">
where h, sib and tsib are head, sibling and tri-sibling
node of v, and dir is a direction of v from h. Table
1 shows an example of an event list of the tri-sibling
model, which is extracted from hyperedge e in Fig-
ure 1. EOC indicates the end of the left or right child
sequence. This is factored in a tri-sibling structure
shown in the left side of Figure 2.
Eq.4 is further decomposed into a product of the
form consisting of three terms:
</bodyText>
<equation confidence="0.3766084">
q1(v|h,sib, tsib, dir) (5)
= q1(dist(v, h), wrd(v), tag(v)|h, sib, tsib, dir)
= q1(tag(v)|h, sib, tsib, dir)
×q1(wrd(v)|tag(v), h, sib, tsib, dir)
×q1(dist(v, h)|wrd(v), tag(v), h, sib, tsib, dir)
</equation>
<bodyText confidence="0.999976125">
where tag(v) and wrd(v) are the POS-tag and word
of v and dist(v, h) is the distance between positions
of v and h. The values of dist(v, h) are partitioned
into 4 categories: 1, 2, 3 − 6, 7 − ∞.
Second, following Sangati et al. (2009), we define
a grandsibling model whose context space consists
of the head node, sibling node, grandparent node and
direction of a node v.
</bodyText>
<equation confidence="0.999813">
q2(v|C(v)) = q2(v|h,sib, g,dir) (6)
</equation>
<bodyText confidence="0.6055945">
where g is a grandparent node of v. Analogous to
Eq.5, Eq.6 is decomposed into three terms:
</bodyText>
<construct confidence="0.855482">
q2(v|h, sib, g, dir) (7)
= q2(dist(v, h), wrd(v), tag(v)|h, sib, g, dir)
= q2(tag(v)|h, sib, g, dir)
×q2(wrd(v)|tag(v), h, sib, g, dir)
×q2(dist(v, h)|wrd(v), tag(v), h, sib, g, dir)
</construct>
<bodyText confidence="0.9976719375">
where notations are the same as those in Eq.5 with
the exception of tri-sibling tsib and grandparent g.
This model is factored in a grandsibling structure
shown in the right side of Figure 2.
The direct estimation of tri-sibling and grandsib-
ling models from a corpus suffers from serious data
sparseness issues. To overcome this, Eisner (1996a)
proposed a back-off strategy which reduces the con-
ditioning of a model. We show the reductions list
for each term of two models in Table 2. The usage
of reductions list is identical to Eisner (1996a) and
readers may refer to it for further details.
The final prediction is performed using a log-
linear interpolated model. It interpolates the base-
line discriminative model and two (tri-sibling and
grandsibling) generative models.
</bodyText>
<equation confidence="0.995383333333333">
2
log qn(top(y))Bn
� log p(y|x)Bbase (8)
</equation>
<bodyText confidence="0.999914666666667">
where 0 are parameters to adjust the weight of each
term in prediction. These parameters are tuned using
MERT algorithm (Och, 2003) on development data
using a criterion of accuracy maximization. The rea-
son why we chose MERT is that it effectively tunes
dense parameters with a line search algorithm.
</bodyText>
<equation confidence="0.988494909090909">
|tailsL(e)|
�
l=1
q(v) =
|tailsR(e)|
�×
r=1
y� = argmax
yEG(x)
�
n=1
</equation>
<page confidence="0.490744">
1481
</page>
<tableCaption confidence="0.992003666666667">
Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word,
POS-tag for a node. d indicates the direction. The first reduction on the list keeps all or most of the original condition;
later reductions throw away more and more of this information.
</tableCaption>
<table confidence="0.989938428571429">
tri-sibling grandsibling
1-st term 2-nd term 3-rd term 1-st term 2-nd term 3-rd term
wt(h),wt(sib),wt(tsib),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),wt(g),d wt(h),t(sib),d wt(v),t(h),t(sib),d
wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d
t(h),wt(sib),t(tsib),d — — t(h),wt(sib),t(g),d — —
wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d
t(h),t(sib),t(tsib),d — — t(h),t(sib),t(g),d — —
</table>
<tableCaption confidence="0.710249">
Figure 2: The left side denotes tri-sibling structure and
the right side denotes grandsibling structure.
Table 3: A summarization of the model factorization and
order
</tableCaption>
<table confidence="0.953255285714286">
first-order McDonald et al. (2005)
second-order Eisner (1996a)
(sibling) McDonald et al. (2005)
third-order tri-sibling model
(tri-sibling) Model 2 (Koo and Collins, 2010)
third-order grandsibling model (Sangati et al., 2009)
(grandsibling) Model 1 (Koo and Collins, 2010)
</table>
<subsectionHeader confidence="0.996421">
3.2 Exact Search Algorithm
</subsectionHeader>
<bodyText confidence="0.999977708333334">
Our baseline discriminative model uses first- and
second-order features provided in (McDonald et al.,
2005; McDonald and Pereira, 2006). Therefore,
both our tri-sibling model and baseline discrimina-
tive model integrate local features that are factored
in one hyperedge. On the other hand, the grandsib-
ling model has non-local features because the grand-
parent is not factored in one hyperedge. We sum-
marize the order of each model in Table 3. Our
reranking models are generative versions of Koo and
Collins (2010)’s third-order factorization model.
Non-locality of weight function makes it difficult
to perform the search of Eq.8 with an usual exact
Viterbi 1-best algorithm. One solution to resolve
the intractability is an approximate k-best Viterbi
search. For a constituent parser, Huang (2008) ap-
plied cube pruning techniques to forest reranking
with non-local features. Cube pruning is originally
proposed for the decoding of statistical machine
translation (SMT) with an integrated n-gram lan-
guage model (Chiang, 2007). It is an approximate
k-best Viterbi search algorithm using beam search
and lazy computation (Huang and Chiang, 2005).
In the case of a dependency parser, Koo and
Collins (2010) proposed dynamic-programming-
based third-order parsing algorithm, which enumer-
ates all grandparents with an additional loop. Our
hypergraph based search algorithm for Eq.8 share
the same spirit to their third-order parsing algo-
rithm since the grandsibling model is similar to their
model 1 in that it is factored in grandsibling struc-
ture. Algorithm 1 shows the search algorithm. This
is almost the same bottom-up 1-best Viterbi algo-
rithm except an additional loop in line 4. Line 4 ref-
erences outgoing edge e′ of node h from a set of out-
going edges OE(h). tails(e) contains a node v, the
sibling node sib and tri-sibling node tsib of v, more-
over, the head of e′ (head(e′)) is the grandparent for
v and sib. Thus, in line 5, we can capture tri-sibling
and grandsibling information and compute the cur-
rent inside estimate of Eq.8.
In our actual implementation, each score of com-
ponents in Eq.8 is represented as a cost. This is writ-
ten as a shortest path search algorithm with a tropi-
cal (real) semiring framework (Mohri, 2002; Huang,
2006). Therefore, ® denotes the min operater and �
denotes the + operater. The function f is defined as
follows:
</bodyText>
<equation confidence="0.994462333333333">
|e|
f(d(v1, e), ... , d(v|e|, e))) = NA d(vi, e) (9)
i=1
</equation>
<bodyText confidence="0.9991532">
where d(vi, e) denotes the current estimate of the
best cost for a pair of node vi and a hyperedge e.
® sums the best cost of a pair of a sub span node
and hyperedge e. Each ctsib and cgsib in line 5 and
7 indicates the cost of tri-sibling and grandsibling
</bodyText>
<figure confidence="0.9613495">
h tsib sib v
9 h sib v
1482
Algorithm 1 Exact DP-Search Algorithm(HG(x))
</figure>
<listItem confidence="0.912283142857143">
1: for h E V in bottom-up topological order do
2: for e E IE(h) do
// tails(e) is {v1, ... , v|e |}.
4: for e′ E OE(h) do
5: d(h, e′) = ®f(d(v1, e), ... , d(v|e|, e)) ® we ® ctsib(h, tails(e)) ® c9sib(head(e′), h, tails(e))
6: if h == top then
7: d(h) = ®f(d(v1, e), ... , d(v|e|, e)) ® we ® ctsib(h, tails(e))
</listItem>
<bodyText confidence="0.999865888888889">
model. we indicates the cost of hyperedge e com-
puted from a baseline discriminative model. Lines
6-7 denote the calculation of the best cost for a top
node. We do not compute the cost of the grandsib-
ling model when h is top node because top node has
no outgoing edges.
Our baseline k-best second-order parser is imple-
mented using Huang and Chiang (2005)’s algorithm
2 whose time complexity is O(m3 +mk log k). Koo
and Collins (2010)’s third-order parser has O(m4)
time complexity and is theoretically slower than our
baseline k-best parser for a long sentence. Our
search algorithm is based on the third-order parsing
algorithm, but, the search space is previously shrank
by a baseline parser’s k-best approximation and a
forest pruning algorithm presented in the next sec-
tion. Therefore, the time efficiency of our reranking
is unimpaired.
</bodyText>
<subsectionHeader confidence="0.998803">
3.3 Forest Pruning
</subsectionHeader>
<bodyText confidence="0.99996075">
Charniak and Johnson (2005) and Huang (2008)
proposed forest pruning algorithms to reduce the
size of a forest. Huang (2008)’s pruning algo-
rithm uses a 1-best Iiterbi inside/outside algorithm
to compute an inside probability β(v) and an out-
side probability α(v), while Charniak and Johnson
(2005) use the usual inside/outside algorithm.
In our experiments, we use Charniak and Johnson
(2005)’s forest pruning criterion because the varia-
tional model needs traditional inside/outside proba-
bilities for its ML estimation. We prune away all
hyperedges that have score &lt; ρ for a threshold ρ.
</bodyText>
<equation confidence="0.9764135">
αβ(e)
score = β(top). (10)
</equation>
<bodyText confidence="0.989153">
Following Huang (2008), we also prune away nodes
with all incoming and outgoing hyperedges pruned.
</bodyText>
<sectionHeader confidence="0.997428" genericHeader="method">
4 Variational Reranking Model
</sectionHeader>
<bodyText confidence="0.90012175">
In place of a maximum a posteriori (MAP) decision
based on Eq.2, the minimum Bayes risk (MBR) deci-
sion rule (Titov and Henderson, 2006) is commonly
used and defined as following equation:
</bodyText>
<equation confidence="0.944317">
�
y� = argmin loss(y, y′)p(y′|x) (11)
yEG(x) y′EG(x)
</equation>
<bodyText confidence="0.999828">
where loss(y, y′) represents a loss function2. As an
alternative to the MBR decision rule, Li et al. (2009)
proposed a variational decision rule that rescores
candidates with an approximate distribution q* E Q.
</bodyText>
<equation confidence="0.9991135">
y� = argmax q*(y) (12)
yEG(x)
</equation>
<bodyText confidence="0.81684">
where q* minimizes the KL divergence KL(p||q)
</bodyText>
<equation confidence="0.9995485">
q* = argmin KL(p||q)
qEQ
�=argmax
qEQ yEG(x) plog q (13)
</equation>
<bodyText confidence="0.9997453">
where each p and q represents p(y|x) and q(y). For
SMT systems, q* is modeled by n-gram language
model over output strings. While the decoding based
on q* is an approximation of intractable MAP de-
coding3, it works as a rescoring function for candi-
dates generated from a baseline model. Here, we
propose to apply the variational decision rule to de-
pendency parsing. For dependency parsing, we can
choose to model q* as the tri-sibling and grandsib-
ling generative models in section 3.
</bodyText>
<footnote confidence="0.693214857142857">
2In case of dependency parsing, Titov and Henderson (2006)
proposed that a loss function is simply defined using a depen-
dency attachment score.
3In SMT, a marginalization of all derivations which yield
a paticular translation needs to be carried out for each trans-
lation. This makes the MAP decoding NP-hard in SMT. This
variational approximate framework can be applied to other tasks
</footnote>
<table confidence="0.714371">
collapsing spurious ambiguity, such as latent-variable parsing
(Matsuzaki et al., 2005).
1483
Algorithm 2 DP-ML Estimation(HG(x))
</table>
<listItem confidence="0.906782631578947">
1: run inside and outside algorithm on HG(x)
2: for v E V do
3: for e E IE(v) do /�
4: ctsib = pe &apos; α (v) /0 (top)
5: for u E tails (e)/�do
6: ctsib = ctsib &apos; N (u)
7: for e′ E IE(u) do
8: cgsib = pe &apos; pe′ &apos; α(v)/β(top)
9: for u′ E tails(e) \ u do
10: cgsib = cgsib &apos; β(u′)
11: for u′′ E tails(e′) do
12: cgsib = cgsib &apos; β(u′′)
13: for u′′ E tails(e′) do
14: c2(u′′|C(u′′))+ = cgsib
15: c2(C(u′′))+ = cgsib
16: for u E tails(e) do
17: c1(u|C(u))+ = ctsib
18: c1(C(u))+ = ctsib
19: MLE estimate q∗1 , q∗2 using formula Eq.14
</listItem>
<subsectionHeader confidence="0.915032">
4.1 ML Estimation from a Forest
</subsectionHeader>
<bodyText confidence="0.99748575">
q*(v|C(v)) is estimated from a forest using a max-
imum likelihood estimation (MLE). The count of
events is no longer an integer count, but an expected
count under p, which is formulated as follows:
</bodyText>
<equation confidence="0.995694833333333">
c(v|C(v))
q*(v|C(v)) =
c(C(v))
E
y p(y|x)c&amp;quot;jC(&amp;quot;)(y) =(14)
Ey p(y|x)cC(&amp;quot;)(y)
</equation>
<bodyText confidence="0.999957294117647">
where ce(y) is the number of event e in y. The es-
timation of Eq.14 can be efficiently performed on a
hypergraph data structure HG(x) of a forest.
Algorithm 2 shows the estimation algorithm.
First, it runs the inside/outside algorithm on HG(x).
We denote inside weight for a node v as β(v) and
outside weight as α(v). For each hyperedge e, we
denote cgsib as the posterior weight for computing
expected count c1 of events in the tri-sibling model
q*1. Lines 16-18 compute c1 for all events occuring
in a hyperedge e.
The expected count c2 needed for the estimation
of grandsibling model q*2 is extracted in lines 7-15.
c2 for a grandsibling model must be extracted over
two hyperedges e and e′ because it needs grandpar-
ent information. Lines 8-12 show the algorithm to
compute the posterior weight cgsib of e and e′, which
</bodyText>
<figure confidence="0.8595955">
0 200 400 600 800 1000 1200 1400
the number of hyperedges per sentence
</figure>
<figureCaption confidence="0.989549">
Figure 3: The relationship between tha data size (the
number of hyperedges) and oracle scores on develop-
ment data: Forests encode candidates with high accuracy
scores more compactly than k-best lists.
</figureCaption>
<bodyText confidence="0.974178956521739">
is similar to that to compute the posterior weight
of rules of tree substitution grammars used in tree-
based MT systems (Mi and Huang, 2008). Lines
13-15 compute expected counts c2 of events occur-
ing over two hyperedges e and e′. Finally, line 19
estimates q*1 and q*2 using the form in Eq.14.
Li et al. (2009) assumes n-gram locality of the
forest to efficiently train the model, namely, the
baseline n-gram model has larger n than that of vari-
ational n-gram model. In our case, grandsibling lo-
cality is not embedded in the forest generated from
the baseline parser. Therefore, we need to reference
incoming hyperedges of tail nodes in line 7.
y* of Eq.12 may be locally appropriate but glob-
ally inadequate because q* only approximates p.
Therefore, we log-linearly combine q* with a global
generative model estimated from the training data
and the baseline discriminative model.
2
Algorithm1 is also applicable to the decoding of
Eq.15. Note that this framework is a combination of
variational decoding and generative reranking. We
call this framework variational reranking.
</bodyText>
<figure confidence="0.969942904761905">
100
99
98
97
96
95
94
93
92
p=0.001
k=20
&amp;quot;kbest&amp;quot;
&amp;quot;forest&amp;quot;
k=100
Unlabeled Accuracy
yˆ = argmax � log qn(top(y))Bn
yEG(x) n=1
2 n
+1: log q* n(top(y))B�
n=1 +log p(y|x)Bbase (15)
1484
</figure>
<tableCaption confidence="0.991354666666667">
Table 4: The statistics of forests and 20-best lists on de-
velopment data: this shows the average number of hyper-
edges and nodes per sentence and oracle scores.
</tableCaption>
<table confidence="0.5594674">
forest 20-best
pruning threshold p = 10−3 —
ave. num of hyperedges 180.67 255.04
ave. num of nodes 135.74 491.42
oracle scores 98.76 96.78
</table>
<sectionHeader confidence="0.98928" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999982961538462">
Experiments are performed on English Penn Tree-
bank data. We split WSJ part of the Treebank into
sections 02-21 for training, sections 22 for develop-
ment, sections 23 for testing. We use Yamada and
Matsumoto (2003)’s head rules to convert phrase
structure to dependency structure. We obtain k-best
lists and forests generated from the baseline discrim-
inative model which has the same feature set as pro-
vided in (McDonald et al., 2005), using the second-
order Eisner algorithms. We use MIRA for training
as it is one of the learning algorithms that achieves
the best performance in dependency parsing. We set
the scaling factory = 1.0.
We also train a generative reranking model from
the training data. To reduce the data sparseness
problem, we use the back-off strategy proposed in
(Eisner, 1996a). Parameters 0 are trained using
MERT (Och, 2003) and for each sentence in the de-
velopment data, 300-best dependency trees are ex-
tracted from its forest. Our variational reranking
does not need much time to train the model be-
cause the training is performed over not the train-
ing data (39832 sentences) but the development data
(1700 sentences)4. After MERT was performed un-
til the convergence, the variational reranking finally
achieved a 94.5 accuracy score on development data.
</bodyText>
<subsectionHeader confidence="0.992663">
5.1 k-best Lists vs. Forests
</subsectionHeader>
<bodyText confidence="0.975555333333333">
Figure 3 shows the relationship between the size of
data structure (the number of hyperedges) and accu-
racy scores on development data. Obviously, forests
can encode a large number of potential candidates
more compactly than k-best lists. This means that
4To generate forests, sentences are parsed only once before
the training. MERT is performed over the forests. We can also
apply a more efficient hypergraph MERT algorithm (Kumar et
al., 2009) to the training than a simple MERT algorithm.
for reranking, there is more possibility of selecting
good candidates in forests than k-best lists.
Table 4 shows the statistics of forests and 20-
best lists on development data. This setting, thresh-
old p = 10−3 for pruning, is also used for testing.
Forests, which have an average of 180.67 hyper-
edges per sentence, achieve oracle score of 98.76,
which is about 1.0% higher than the 96.78 oracle
score of 20-best lists with 255.04 hyperedges per
sentence. Though the size of forests is smaller than
that of k-best lists, the oracle scores of forests are
much higher than those of k-best lists.
</bodyText>
<subsectionHeader confidence="0.999264">
5.2 The Performance of Reranking
</subsectionHeader>
<bodyText confidence="0.999984821428572">
First, we compare the performance of variational de-
coding with that of MBR decoding. The results are
shown in Table 5. Variational decoding outperforms
MBR decodings. However, compared with base-
line, the gains of variational and MBR decoding are
small. Second, we also compare the performance of
variational reranking with k-best and forest gener-
ative reranking algorithms. Table 6 shows that our
variational reranking framework achieves the high-
est accuracy scores.
Being different from the decoding framework,
reranking achieves significant improvements. This
result is intuitively reasonable because the rerank-
ing model obtained from training data has the ability
to select a globally consistent candidate, while the
variational approximate model obtained from a for-
est only supports selecting a locally consistent can-
didate. On the other hand, the fact that variational
reranking achieves the best results clearly indicates
that the combination of sentence specific generative
model and that obtained from training data is suc-
cessful in selecting both locally and globally appro-
priate candidate from a forest.
Table 7 shows the parsing time (on 2.66GHz
Quad-Core Xeon) of the baseline k-best, generative
reranking and variational reranking parsers (java im-
plemented). The variational reranking parser con-
tains the following procedures.
</bodyText>
<listItem confidence="0.99627275">
1. k-best forest creation (baseline)
2. Estimation of variational model
3. Forest pruning
4. Search with the third-order model
</listItem>
<bodyText confidence="0.859008">
Our reranking parser incurred little overhead to the
</bodyText>
<page confidence="0.778873">
1485
</page>
<tableCaption confidence="0.72351275">
Table 5: The comparison of the decoding frameworks:
MBR decoding seeks a candidate which has the high-
est accuracy scores over a forest (Kumar et al., 2009).
Variational decoding is performed based on Eq.8.
</tableCaption>
<table confidence="0.999734333333333">
Eval Unlabeled
����������
Decoding
baseline 91.9
MBR (8-best forest) 91.99
Variational (8-best forest) 92.17
</table>
<tableCaption confidence="0.9222895">
Table 6: The comparison of the reranking frameworks:
Generative means k-best or forest reranking algorithm
based on a generative model estimated from a corpus.
Variational reranking is performed based on Eq.15.
</tableCaption>
<table confidence="0.9997788">
Eval Unlabeled
Reranking
Generative (8-best) 92.66
Generative (8-best forest) 92.72
Variational (8-best forest) 92.87
</table>
<tableCaption confidence="0.983834">
Table 7: The parsing time (CPU second per sentence) and
accuracy score of the baseline k-best, generative rerank-
ing and variational reranking parsers
</tableCaption>
<table confidence="0.999714142857143">
k baseline generative variational
2 0.09 (91.9) +0.03 (92.67) +0.05 (92.76)
4 0.1 (91.9) +0.05 (92.68) +0.09 (92.81)
8 0.13 (91.9) +0.06 (92.72) +0.11 (92.87)
16 0.18 (91.9) +0.07 (92.75) +0.12 (92.89)
32 0.29 (91.9) +0.07 (92.73) +0.13 (92.89)
64 0.54 (91.9) +0.08 (92.72) +0.15 (92.87)
</table>
<tableCaption confidence="0.991282666666667">
Table 8: The comparison of tri-sibling and grandsibling
models: the performance of the grandsibling model out-
performs that of the tri-sibling model.
</tableCaption>
<table confidence="0.9526782">
Eval Unlabeled
M l
ode
tri-sibling 92.63
grandsibling 92.74
</table>
<bodyText confidence="0.837204333333333">
baseline parser in terms of runtime. This means that
our reranking parser can parse sentences at reason-
able times.
</bodyText>
<subsectionHeader confidence="0.9972045">
5.3 The Effects of Third-order Factors and
Error Analysis
</subsectionHeader>
<bodyText confidence="0.998545769230769">
From results in section 5.2, our variational rerank-
ing model achieves higher accuracy scores than the
others. To analyze the factors that improve accu-
racy scores, we further investigate whether varia-
tional reranking is performed better with the tri-
sibling or grandsibling model. Table 8 indicates that
grandsibling model achieves a larger gain than that
of tri-sibling model. Table 9 shows the examples
whose accuracy scores improved by the grandsib-
ling model. For example, the dependency relation-
ship from Verb to Noun phrase was corrected by our
proposed model.
On the other hand, many errors remain still in
</bodyText>
<tableCaption confidence="0.977176666666667">
Table 10: Comparison of our best result (using 16-best
forests) with other best-performing Systems on the whole
section 23
</tableCaption>
<note confidence="0.9021009">
Parser English
McDonald et al. (2005) 90.9
McDonald and Pereira (2006) 91.5
Koo et al. (2008) standard 92.02
Huang and Sagae (2010) 92.1
Koo and Collins (2010) model1 93.04
Koo and Collins (2010) model2 92.93
this work 92.89
Koo et al. (2008) semi-sup 93.16
Suzuki et al. (2009) 93.79
</note>
<bodyText confidence="0.9997025">
our results. In our experiments, 48% of sentences
which contain errors have Prepositional word errors.
In fact, well-known PP-Attachment is a problem to
be solved for natural language parsers. Other re-
maining errors are caused by symbols such as .,:“”().
45% sentences contain such a dependency mistake.
Adding features to solve these problems may poten-
tially improve our parser more.
</bodyText>
<subsectionHeader confidence="0.997707">
5.4 Comparison with Other Systems
</subsectionHeader>
<bodyText confidence="0.999721230769231">
Table 10 shows the comparison of the performance
of variational reranking (16-best forests) with that of
other systems. Our method outperforms supervised
parsers with second-order features, and achieves
comparable results compared to a parser with third-
order features (Koo and Collins, 2010). We can not
directly compare our method with semi-supervised
parsers such as Koo et al. (2008)’s semi-sup and
Suzuki et al. (2009), because ours does not use addi-
tional unlabeled data for training. The model trained
from unlabeled data can be easily incorporated into
our reranking framework. We plan to investigate
semi-supervised learning in future work.
</bodyText>
<page confidence="0.75959">
1486
</page>
<tableCaption confidence="0.9982425">
Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational
reranking parsers. The underlined portions show the effect of the grandsibling model.
</tableCaption>
<table confidence="0.961156875">
sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy .
correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4
baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4
proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4
sent (No.283) Many called it simply a contrast in styles .
correct 2 0 2 6 6 2 6 7 2
baseline 2 0 2 2 6 2 6 7 2
proposed 2 0 2 6 6 2 6 7 2
</table>
<sectionHeader confidence="0.999781" genericHeader="evaluation">
6 Related Work
</sectionHeader>
<bodyText confidence="0.998924470588235">
Collins (2000) and Charniak and Johnson (2005)
proposed a reranking algorithm for constituent
parsers. Huang (2008) extended it to a forest rerank-
ing algorithm with non-local features. Our frame-
work is for a dependency parser and the decoding in
the reranking stage is done with an exact 1-best dy-
namic programming algorithm. Sangati et al. (2009)
proposed a k-best generative reranking algorithm for
dependency parsing. In this paper, we use a similar
generative model, but combined with a variational
model learned on the fly. Moreover, our framework
is applicable to forests, not k-best lists.
Koo and Collins (2010) presented third-order de-
pendency parsing algorithm. Their model 1 is de-
fined by an enclosing grandsibling for each sibling
or grandchild part used in Carreras (2007). Our
grandsibling model is similar to the model 1, but
ours is defined by a generative model. The decod-
ing in the reranking stage is also similar to the pars-
ing algorithm of their model 1. In order to capture
grandsibling factors, our decoding calculates inside
probablities for not the current head node but each
pair of the node and its outgoing edges.
Titov and Henderson (2006) reported that the
MBR approach could be applied to a projective de-
pendency parser. In the field of SMT, for an approx-
imation of MAP decoding, Li et al. (2009) proposed
variational decoding and Kumar et al. (2009) pre-
sented hypergraph MBR decoding. Our variational
model is inspired by the study of Li et al. (2009) and
we apply it to a dependency parser in order to select
better candidates with third-order information. We
also propose an efficient algorithm to estimate the
non-local third-order model structure.
</bodyText>
<sectionHeader confidence="0.998381" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999984173913044">
In this paper, we propose a novel forest reranking
algorithm for dependency parsing. Our reranking
algorithm is a combination approach of generative
reranking and variational decoding. The search al-
gorithm in the reranking stage can be performed
using dynamic programming algorithm. Our vari-
ational reranking is aimed at selecting a candidate
from a forest, which is correct both in local and
global. Our experimental results show more signif-
icant improvements than conventional approaches,
such as k-best and forest generative reranking.
In the future, we plan to investigate more ap-
propriate generative models for reranking. PP-
Attachment is one of the most difficult problems
for a natural language parser. We plan to exam-
ine to model such a complex structure (granduncle)
(Goldberg and Elhadad, 2010) or higher-order struc-
ture than third-order for reranking which is compu-
tationally expensive for a baseline parser. As we
mentioned in Section 5.4, we also plan to incorpo-
rate semi-supervised learning into our framework,
which may potentially improve our reranking per-
formance.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999553428571429">
We would like to thank Graham Neubig and Masashi
Shimbo for their helpful comments and to the anony-
mous reviewers for their effort of reviewing our pa-
per and giving valuable comments. This work was
supported in part by Grant-in-Aid for Japan Society
for the Promotion of Science (JSPS) Research Fel-
lowship for Young Scientists.
</bodyText>
<page confidence="0.68468">
1487
</page>
<sectionHeader confidence="0.994668" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998879259740259">
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. the CoNLL-
EMNLP, pages 957–961.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. the 43rd ACL, pages 173–180.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33:201–228.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. the ICML.
J. M. Eisner. 1996a. An empirical comparison of prob-
ability models for dependency grammar. In Technical
Report, pages 1–18.
J. M. Eisner. 1996b. Three new probabilistic models for
dependency parsing: An exploration. In Proc. the 16th
COLING, pages 340–345.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proc. the HLT-NAACL, pages 742–750.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. the IWPT, pages 53–64.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. the ACL,
pages 1077–1086.
L. Huang. 2006. Dynamic programming al-
gorithms in semiring and hypergraph frame-
works. Qualification Exam Report, pages 1–19.
http://www.cis.upenn.edu/ lhuang3/wpe2/.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. the 46th ACL,
pages 586–594.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. the 48th ACL, pages 1–11.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. the ACL,
pages 595–603.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. the 47th ACL, pages 163–171.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In Proc.
the 47th ACL, pages 593–601.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic cfg with latent annotations. In Proc. the ACL, pages
75–82.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL, pages 81–88.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
the 43rd ACL, pages 91–98.
H. Mi and L. Huang. 2008. Forest-based translation rule
extraction. In Proceedings of EMNLP, pages 206–
214.
M. Mohri. 2002. Semiring framework and algorithms
for shortest-distance problems. Automata, Languages
and Combinatorics, 7:321–350.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. the 41st ACL, pages
160–167.
F. Sangati, W. Zuidema, and R. Bod. 2009. A generative
re-ranking model for dependency parsing. In Proc. the
11th IWPT, pages 238–241.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proc. the
EMNLP, pages 551–560.
I. Titov and J. Henderson. 2006. Bayes risk minimiza-
tion in natural language parsing. In Technical Report,
pages 1–9.
Z. Tu, Y. Liu, Y. Hwang, Q. Liu, and S. Lin. 2010. De-
pendency forest for statistical machine translation. In
Proc. the 23rd COLING, pages 1092–1100.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
the IWPT, pages 195–206.
</reference>
<page confidence="0.771921">
1488
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.375953">
<title confidence="0.949368">Third-order Variational Reranking on Packed-Shared Dependency Forests</title>
<author confidence="0.653733">Taro Masayuki Yuji Insutitute of Science</author>
<affiliation confidence="0.739573">Ikoma, Nara, 630-0192, Institute of Information and Communications</affiliation>
<address confidence="0.880436">Sorakugun, Kyoto, 619-0289,</address>
<email confidence="0.978231">taro.watanabe@nict.go.jp</email>
<abstract confidence="0.985884777777778">propose a novel reranking for discriminative dependency parsing based on a variant of Eisner’s generative model. In our framework, we define two kinds of generative model for reranking. One is learned from training data offline and the other from a forest generated by a baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a we apply extended Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. the CoNLLEMNLP,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="28753" citStr="Carreras (2007)" startWordPosition="4730" endWordPosition="4731">ocal features. Our framework is for a dependency parser and the decoding in the reranking stage is done with an exact 1-best dynamic programming algorithm. Sangati et al. (2009) proposed a k-best generative reranking algorithm for dependency parsing. In this paper, we use a similar generative model, but combined with a variational model learned on the fly. Moreover, our framework is applicable to forests, not k-best lists. Koo and Collins (2010) presented third-order dependency parsing algorithm. Their model 1 is defined by an enclosing grandsibling for each sibling or grandchild part used in Carreras (2007). Our grandsibling model is similar to the model 1, but ours is defined by a generative model. The decoding in the reranking stage is also similar to the parsing algorithm of their model 1. In order to capture grandsibling factors, our decoding calculates inside probablities for not the current head node but each pair of the node and its outgoing edges. Titov and Henderson (2006) reported that the MBR approach could be applied to a projective dependency parser. In the field of SMT, for an approximation of MAP decoding, Li et al. (2009) proposed variational decoding and Kumar et al. (2009) pres</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proc. the CoNLLEMNLP, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. the 43rd ACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1302" citStr="Charniak and Johnson, 2005" startWordPosition="172" endWordPosition="175"> baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Iiterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative mo</context>
<context position="14097" citStr="Charniak and Johnson (2005)" startWordPosition="2293" endWordPosition="2296">g edges. Our baseline k-best second-order parser is implemented using Huang and Chiang (2005)’s algorithm 2 whose time complexity is O(m3 +mk log k). Koo and Collins (2010)’s third-order parser has O(m4) time complexity and is theoretically slower than our baseline k-best parser for a long sentence. Our search algorithm is based on the third-order parsing algorithm, but, the search space is previously shrank by a baseline parser’s k-best approximation and a forest pruning algorithm presented in the next section. Therefore, the time efficiency of our reranking is unimpaired. 3.3 Forest Pruning Charniak and Johnson (2005) and Huang (2008) proposed forest pruning algorithms to reduce the size of a forest. Huang (2008)’s pruning algorithm uses a 1-best Iiterbi inside/outside algorithm to compute an inside probability β(v) and an outside probability α(v), while Charniak and Johnson (2005) use the usual inside/outside algorithm. In our experiments, we use Charniak and Johnson (2005)’s forest pruning criterion because the variational model needs traditional inside/outside probabilities for its ML estimation. We prune away all hyperedges that have score &lt; ρ for a threshold ρ. αβ(e) score = β(top). (10) Following Hua</context>
<context position="28014" citStr="Charniak and Johnson (2005)" startWordPosition="4611" endWordPosition="4614">ork. 1486 Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational reranking parsers. The underlined portions show the effect of the grandsibling model. sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy . correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4 baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4 proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4 sent (No.283) Many called it simply a contrast in styles . correct 2 0 2 6 6 2 6 7 2 baseline 2 0 2 2 6 2 6 7 2 proposed 2 0 2 6 6 2 6 7 2 6 Related Work Collins (2000) and Charniak and Johnson (2005) proposed a reranking algorithm for constituent parsers. Huang (2008) extended it to a forest reranking algorithm with non-local features. Our framework is for a dependency parser and the decoding in the reranking stage is done with an exact 1-best dynamic programming algorithm. Sangati et al. (2009) proposed a k-best generative reranking algorithm for dependency parsing. In this paper, we use a similar generative model, but combined with a variational model learned on the fly. Moreover, our framework is applicable to forests, not k-best lists. Koo and Collins (2010) presented third-order depe</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and maxent discriminative reranking. In Proc. the 43rd ACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--201</pages>
<contexts>
<context position="11187" citStr="Chiang, 2007" startWordPosition="1773" endWordPosition="1774">rder of each model in Table 3. Our reranking models are generative versions of Koo and Collins (2010)’s third-order factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate k-best Viterbi search. For a constituent parser, Huang (2008) applied cube pruning techniques to forest reranking with non-local features. Cube pruning is originally proposed for the decoding of statistical machine translation (SMT) with an integrated n-gram language model (Chiang, 2007). It is an approximate k-best Viterbi search algorithm using beam search and lazy computation (Huang and Chiang, 2005). In the case of a dependency parser, Koo and Collins (2010) proposed dynamic-programmingbased third-order parsing algorithm, which enumerates all grandparents with an additional loop. Our hypergraph based search algorithm for Eq.8 share the same spirit to their third-order parsing algorithm since the grandsibling model is similar to their model 1 in that it is factored in grandsibling structure. Algorithm 1 shows the search algorithm. This is almost the same bottom-up 1-best V</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33:201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. the ICML.</booktitle>
<contexts>
<context position="1274" citStr="Collins, 2000" startWordPosition="170" endWordPosition="171"> generated by a baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Iiterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a varia</context>
<context position="27982" citStr="Collins (2000)" startWordPosition="4608" endWordPosition="4609">earning in future work. 1486 Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational reranking parsers. The underlined portions show the effect of the grandsibling model. sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy . correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4 baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4 proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4 sent (No.283) Many called it simply a contrast in styles . correct 2 0 2 6 6 2 6 7 2 baseline 2 0 2 2 6 2 6 7 2 proposed 2 0 2 6 6 2 6 7 2 6 Related Work Collins (2000) and Charniak and Johnson (2005) proposed a reranking algorithm for constituent parsers. Huang (2008) extended it to a forest reranking algorithm with non-local features. Our framework is for a dependency parser and the decoding in the reranking stage is done with an exact 1-best dynamic programming algorithm. Sangati et al. (2009) proposed a k-best generative reranking algorithm for dependency parsing. In this paper, we use a similar generative model, but combined with a variational model learned on the fly. Moreover, our framework is applicable to forests, not k-best lists. Koo and Collins (</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In Proc. the ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Eisner</author>
</authors>
<title>An empirical comparison of probability models for dependency grammar. In</title>
<date>1996</date>
<tech>Technical Report,</tech>
<pages>1--18</pages>
<contexts>
<context position="1921" citStr="Eisner, 1996" startWordPosition="273" endWordPosition="274">, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner, 1996b; Eisner, 1996a) for reranking and extend it to capture higher-order information than Eisner’s second-order generative model. Their reranking model showed large improvements in dependency parsing accuracy. They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selection among the few best candidates. In this paper, we propose a forest generative reranking algorithm, opposed to Sangati et al. (2009)’s approach which reranks only k-best candidates. Forests usually encode better candidates more compa</context>
<context position="8092" citStr="Eisner (1996" startWordPosition="1320" endWordPosition="1321"> g is a grandparent node of v. Analogous to Eq.5, Eq.6 is decomposed into three terms: q2(v|h, sib, g, dir) (7) = q2(dist(v, h), wrd(v), tag(v)|h, sib, g, dir) = q2(tag(v)|h, sib, g, dir) ×q2(wrd(v)|tag(v), h, sib, g, dir) ×q2(dist(v, h)|wrd(v), tag(v), h, sib, g, dir) where notations are the same as those in Eq.5 with the exception of tri-sibling tsib and grandparent g. This model is factored in a grandsibling structure shown in the right side of Figure 2. The direct estimation of tri-sibling and grandsibling models from a corpus suffers from serious data sparseness issues. To overcome this, Eisner (1996a) proposed a back-off strategy which reduces the conditioning of a model. We show the reductions list for each term of two models in Table 2. The usage of reductions list is identical to Eisner (1996a) and readers may refer to it for further details. The final prediction is performed using a loglinear interpolated model. It interpolates the baseline discriminative model and two (tri-sibling and grandsibling) generative models. 2 log qn(top(y))Bn � log p(y|x)Bbase (8) where 0 are parameters to adjust the weight of each term in prediction. These parameters are tuned using MERT algorithm (Och, 2</context>
<context position="9923" citStr="Eisner (1996" startWordPosition="1585" endWordPosition="1586">erm 3-rd term wt(h),wt(sib),wt(tsib),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),wt(g),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d t(h),wt(sib),t(tsib),d — — t(h),wt(sib),t(g),d — — wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d t(h),t(sib),t(tsib),d — — t(h),t(sib),t(g),d — — Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order McDonald et al. (2005) second-order Eisner (1996a) (sibling) McDonald et al. (2005) third-order tri-sibling model (tri-sibling) Model 2 (Koo and Collins, 2010) third-order grandsibling model (Sangati et al., 2009) (grandsibling) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent i</context>
<context position="20684" citStr="Eisner, 1996" startWordPosition="3425" endWordPosition="3426"> use Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. We obtain k-best lists and forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al., 2005), using the secondorder Eisner algorithms. We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing. We set the scaling factory = 1.0. We also train a generative reranking model from the training data. To reduce the data sparseness problem, we use the back-off strategy proposed in (Eisner, 1996a). Parameters 0 are trained using MERT (Och, 2003) and for each sentence in the development data, 300-best dependency trees are extracted from its forest. Our variational reranking does not need much time to train the model because the training is performed over not the training data (39832 sentences) but the development data (1700 sentences)4. After MERT was performed until the convergence, the variational reranking finally achieved a 94.5 accuracy score on development data. 5.1 k-best Lists vs. Forests Figure 3 shows the relationship between the size of data structure (the number of hypered</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. M. Eisner. 1996a. An empirical comparison of probability models for dependency grammar. In Technical Report, pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. the 16th COLING,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="1921" citStr="Eisner, 1996" startWordPosition="273" endWordPosition="274">, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner, 1996b; Eisner, 1996a) for reranking and extend it to capture higher-order information than Eisner’s second-order generative model. Their reranking model showed large improvements in dependency parsing accuracy. They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selection among the few best candidates. In this paper, we propose a forest generative reranking algorithm, opposed to Sangati et al. (2009)’s approach which reranks only k-best candidates. Forests usually encode better candidates more compa</context>
<context position="8092" citStr="Eisner (1996" startWordPosition="1320" endWordPosition="1321"> g is a grandparent node of v. Analogous to Eq.5, Eq.6 is decomposed into three terms: q2(v|h, sib, g, dir) (7) = q2(dist(v, h), wrd(v), tag(v)|h, sib, g, dir) = q2(tag(v)|h, sib, g, dir) ×q2(wrd(v)|tag(v), h, sib, g, dir) ×q2(dist(v, h)|wrd(v), tag(v), h, sib, g, dir) where notations are the same as those in Eq.5 with the exception of tri-sibling tsib and grandparent g. This model is factored in a grandsibling structure shown in the right side of Figure 2. The direct estimation of tri-sibling and grandsibling models from a corpus suffers from serious data sparseness issues. To overcome this, Eisner (1996a) proposed a back-off strategy which reduces the conditioning of a model. We show the reductions list for each term of two models in Table 2. The usage of reductions list is identical to Eisner (1996a) and readers may refer to it for further details. The final prediction is performed using a loglinear interpolated model. It interpolates the baseline discriminative model and two (tri-sibling and grandsibling) generative models. 2 log qn(top(y))Bn � log p(y|x)Bbase (8) where 0 are parameters to adjust the weight of each term in prediction. These parameters are tuned using MERT algorithm (Och, 2</context>
<context position="9923" citStr="Eisner (1996" startWordPosition="1585" endWordPosition="1586">erm 3-rd term wt(h),wt(sib),wt(tsib),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),wt(g),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d t(h),wt(sib),t(tsib),d — — t(h),wt(sib),t(g),d — — wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d t(h),t(sib),t(tsib),d — — t(h),t(sib),t(g),d — — Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order McDonald et al. (2005) second-order Eisner (1996a) (sibling) McDonald et al. (2005) third-order tri-sibling model (tri-sibling) Model 2 (Koo and Collins, 2010) third-order grandsibling model (Sangati et al., 2009) (grandsibling) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent i</context>
<context position="20684" citStr="Eisner, 1996" startWordPosition="3425" endWordPosition="3426"> use Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. We obtain k-best lists and forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al., 2005), using the secondorder Eisner algorithms. We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing. We set the scaling factory = 1.0. We also train a generative reranking model from the training data. To reduce the data sparseness problem, we use the back-off strategy proposed in (Eisner, 1996a). Parameters 0 are trained using MERT (Och, 2003) and for each sentence in the development data, 300-best dependency trees are extracted from its forest. Our variational reranking does not need much time to train the model because the training is performed over not the training data (39832 sentences) but the development data (1700 sentences)4. After MERT was performed until the convergence, the variational reranking finally achieved a 94.5 accuracy score on development data. 5.1 k-best Lists vs. Forests Figure 3 shows the relationship between the size of data structure (the number of hypered</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. M. Eisner. 1996b. Three new probabilistic models for dependency parsing: An exploration. In Proc. the 16th COLING, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. the HLT-NAACL,</booktitle>
<pages>742--750</pages>
<contexts>
<context position="30468" citStr="Goldberg and Elhadad, 2010" startWordPosition="5007" endWordPosition="5010">ecoding. The search algorithm in the reranking stage can be performed using dynamic programming algorithm. Our variational reranking is aimed at selecting a candidate from a forest, which is correct both in local and global. Our experimental results show more significant improvements than conventional approaches, such as k-best and forest generative reranking. In the future, we plan to investigate more appropriate generative models for reranking. PPAttachment is one of the most difficult problems for a natural language parser. We plan to examine to model such a complex structure (granduncle) (Goldberg and Elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser. As we mentioned in Section 5.4, we also plan to incorporate semi-supervised learning into our framework, which may potentially improve our reranking performance. Acknowledgments We would like to thank Graham Neubig and Masashi Shimbo for their helpful comments and to the anonymous reviewers for their effort of reviewing our paper and giving valuable comments. This work was supported in part by Grant-in-Aid for Japan Society for the Promotion of Science (JSPS) Research Fellowship </context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Y. Goldberg and M. Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Proc. the HLT-NAACL, pages 742–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. the IWPT,</booktitle>
<pages>53--64</pages>
<contexts>
<context position="11305" citStr="Huang and Chiang, 2005" startWordPosition="1789" endWordPosition="1792">d-order factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate k-best Viterbi search. For a constituent parser, Huang (2008) applied cube pruning techniques to forest reranking with non-local features. Cube pruning is originally proposed for the decoding of statistical machine translation (SMT) with an integrated n-gram language model (Chiang, 2007). It is an approximate k-best Viterbi search algorithm using beam search and lazy computation (Huang and Chiang, 2005). In the case of a dependency parser, Koo and Collins (2010) proposed dynamic-programmingbased third-order parsing algorithm, which enumerates all grandparents with an additional loop. Our hypergraph based search algorithm for Eq.8 share the same spirit to their third-order parsing algorithm since the grandsibling model is similar to their model 1 in that it is factored in grandsibling structure. Algorithm 1 shows the search algorithm. This is almost the same bottom-up 1-best Viterbi algorithm except an additional loop in line 4. Line 4 references outgoing edge e′ of node h from a set of outgo</context>
<context position="13563" citStr="Huang and Chiang (2005)" startWordPosition="2210" endWordPosition="2213">er do 2: for e E IE(h) do // tails(e) is {v1, ... , v|e |}. 4: for e′ E OE(h) do 5: d(h, e′) = ®f(d(v1, e), ... , d(v|e|, e)) ® we ® ctsib(h, tails(e)) ® c9sib(head(e′), h, tails(e)) 6: if h == top then 7: d(h) = ®f(d(v1, e), ... , d(v|e|, e)) ® we ® ctsib(h, tails(e)) model. we indicates the cost of hyperedge e computed from a baseline discriminative model. Lines 6-7 denote the calculation of the best cost for a top node. We do not compute the cost of the grandsibling model when h is top node because top node has no outgoing edges. Our baseline k-best second-order parser is implemented using Huang and Chiang (2005)’s algorithm 2 whose time complexity is O(m3 +mk log k). Koo and Collins (2010)’s third-order parser has O(m4) time complexity and is theoretically slower than our baseline k-best parser for a long sentence. Our search algorithm is based on the third-order parsing algorithm, but, the search space is previously shrank by a baseline parser’s k-best approximation and a forest pruning algorithm presented in the next section. Therefore, the time efficiency of our reranking is unimpaired. 3.3 Forest Pruning Charniak and Johnson (2005) and Huang (2008) proposed forest pruning algorithms to reduce the</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>L. Huang and D. Chiang. 2005. Better k-best parsing. In Proc. the IWPT, pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proc. the ACL,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="26170" citStr="Huang and Sagae (2010)" startWordPosition="4278" endWordPosition="4281">e trisibling or grandsibling model. Table 8 indicates that grandsibling model achieves a larger gain than that of tri-sibling model. Table 9 shows the examples whose accuracy scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in Table 10: Comparison of our best result (using 16-best forests) with other best-performing Systems on the whole section 23 Parser English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Koo et al. (2008) standard 92.02 Huang and Sagae (2010) 92.1 Koo and Collins (2010) model1 93.04 Koo and Collins (2010) model2 92.93 this work 92.89 Koo et al. (2008) semi-sup 93.16 Suzuki et al. (2009) 93.79 our results. In our experiments, 48% of sentences which contain errors have Prepositional word errors. In fact, well-known PP-Attachment is a problem to be solved for natural language parsers. Other remaining errors are caused by symbols such as .,:“”(). 45% sentences contain such a dependency mistake. Adding features to solve these problems may potentially improve our parser more. 5.4 Comparison with Other Systems Table 10 shows the comparis</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proc. the ACL, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Dynamic programming algorithms in semiring and hypergraph frameworks. Qualification Exam Report,</title>
<date>2006</date>
<pages>1--19</pages>
<contexts>
<context position="12404" citStr="Huang, 2006" startWordPosition="1980" endWordPosition="1981">i algorithm except an additional loop in line 4. Line 4 references outgoing edge e′ of node h from a set of outgoing edges OE(h). tails(e) contains a node v, the sibling node sib and tri-sibling node tsib of v, moreover, the head of e′ (head(e′)) is the grandparent for v and sib. Thus, in line 5, we can capture tri-sibling and grandsibling information and compute the current inside estimate of Eq.8. In our actual implementation, each score of components in Eq.8 is represented as a cost. This is written as a shortest path search algorithm with a tropical (real) semiring framework (Mohri, 2002; Huang, 2006). Therefore, ® denotes the min operater and � denotes the + operater. The function f is defined as follows: |e| f(d(v1, e), ... , d(v|e|, e))) = NA d(vi, e) (9) i=1 where d(vi, e) denotes the current estimate of the best cost for a pair of node vi and a hyperedge e. ® sums the best cost of a pair of a sub span node and hyperedge e. Each ctsib and cgsib in line 5 and 7 indicates the cost of tri-sibling and grandsibling h tsib sib v 9 h sib v 1482 Algorithm 1 Exact DP-Search Algorithm(HG(x)) 1: for h E V in bottom-up topological order do 2: for e E IE(h) do // tails(e) is {v1, ... , v|e |}. 4: f</context>
</contexts>
<marker>Huang, 2006</marker>
<rawString>L. Huang. 2006. Dynamic programming algorithms in semiring and hypergraph frameworks. Qualification Exam Report, pages 1–19. http://www.cis.upenn.edu/ lhuang3/wpe2/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. the 46th ACL,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="1316" citStr="Huang, 2008" startWordPosition="176" endWordPosition="177"> The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Iiterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner,</context>
<context position="2557" citStr="Huang, 2008" startWordPosition="366" endWordPosition="367">ranking and extend it to capture higher-order information than Eisner’s second-order generative model. Their reranking model showed large improvements in dependency parsing accuracy. They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selection among the few best candidates. In this paper, we propose a forest generative reranking algorithm, opposed to Sangati et al. (2009)’s approach which reranks only k-best candidates. Forests usually encode better candidates more compactly than k-best lists (Huang, 2008). Moreover, our reranking uses not only a generative model obtained from training data, but also a sentence specific generative model learned from a forest. In the reranking stage, we use linearly combined model of these models. We call this variational reranking model. The model proposed in this paper is factored in the third-order structure, therefore, its non-locality makes it difficult to perform the reranking with an usual 1-best Iiterbi search. To solve this problem, we also propose a new search algorithm, which is inspired by the third-order dynamic programming parsing algorithm (Koo an</context>
<context position="10960" citStr="Huang (2008)" startWordPosition="1740" endWordPosition="1741">ne discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent is not factored in one hyperedge. We summarize the order of each model in Table 3. Our reranking models are generative versions of Koo and Collins (2010)’s third-order factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate k-best Viterbi search. For a constituent parser, Huang (2008) applied cube pruning techniques to forest reranking with non-local features. Cube pruning is originally proposed for the decoding of statistical machine translation (SMT) with an integrated n-gram language model (Chiang, 2007). It is an approximate k-best Viterbi search algorithm using beam search and lazy computation (Huang and Chiang, 2005). In the case of a dependency parser, Koo and Collins (2010) proposed dynamic-programmingbased third-order parsing algorithm, which enumerates all grandparents with an additional loop. Our hypergraph based search algorithm for Eq.8 share the same spirit t</context>
<context position="14114" citStr="Huang (2008)" startWordPosition="2298" endWordPosition="2299">ond-order parser is implemented using Huang and Chiang (2005)’s algorithm 2 whose time complexity is O(m3 +mk log k). Koo and Collins (2010)’s third-order parser has O(m4) time complexity and is theoretically slower than our baseline k-best parser for a long sentence. Our search algorithm is based on the third-order parsing algorithm, but, the search space is previously shrank by a baseline parser’s k-best approximation and a forest pruning algorithm presented in the next section. Therefore, the time efficiency of our reranking is unimpaired. 3.3 Forest Pruning Charniak and Johnson (2005) and Huang (2008) proposed forest pruning algorithms to reduce the size of a forest. Huang (2008)’s pruning algorithm uses a 1-best Iiterbi inside/outside algorithm to compute an inside probability β(v) and an outside probability α(v), while Charniak and Johnson (2005) use the usual inside/outside algorithm. In our experiments, we use Charniak and Johnson (2005)’s forest pruning criterion because the variational model needs traditional inside/outside probabilities for its ML estimation. We prune away all hyperedges that have score &lt; ρ for a threshold ρ. αβ(e) score = β(top). (10) Following Huang (2008), we als</context>
<context position="18458" citStr="Huang, 2008" startWordPosition="3051" endWordPosition="3052">for a grandsibling model must be extracted over two hyperedges e and e′ because it needs grandparent information. Lines 8-12 show the algorithm to compute the posterior weight cgsib of e and e′, which 0 200 400 600 800 1000 1200 1400 the number of hyperedges per sentence Figure 3: The relationship between tha data size (the number of hyperedges) and oracle scores on development data: Forests encode candidates with high accuracy scores more compactly than k-best lists. is similar to that to compute the posterior weight of rules of tree substitution grammars used in treebased MT systems (Mi and Huang, 2008). Lines 13-15 compute expected counts c2 of events occuring over two hyperedges e and e′. Finally, line 19 estimates q*1 and q*2 using the form in Eq.14. Li et al. (2009) assumes n-gram locality of the forest to efficiently train the model, namely, the baseline n-gram model has larger n than that of variational n-gram model. In our case, grandsibling locality is not embedded in the forest generated from the baseline parser. Therefore, we need to reference incoming hyperedges of tail nodes in line 7. y* of Eq.12 may be locally appropriate but globally inadequate because q* only approximates p. </context>
<context position="28083" citStr="Huang (2008)" startWordPosition="4622" endWordPosition="4623">on 23 from baseline and variational reranking parsers. The underlined portions show the effect of the grandsibling model. sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy . correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4 baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4 proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4 sent (No.283) Many called it simply a contrast in styles . correct 2 0 2 6 6 2 6 7 2 baseline 2 0 2 2 6 2 6 7 2 proposed 2 0 2 6 6 2 6 7 2 6 Related Work Collins (2000) and Charniak and Johnson (2005) proposed a reranking algorithm for constituent parsers. Huang (2008) extended it to a forest reranking algorithm with non-local features. Our framework is for a dependency parser and the decoding in the reranking stage is done with an exact 1-best dynamic programming algorithm. Sangati et al. (2009) proposed a k-best generative reranking algorithm for dependency parsing. In this paper, we use a similar generative model, but combined with a variational model learned on the fly. Moreover, our framework is applicable to forests, not k-best lists. Koo and Collins (2010) presented third-order dependency parsing algorithm. Their model 1 is defined by an enclosing gr</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>L. Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. the 46th ACL, pages 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. the 48th ACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="3173" citStr="Koo and Collins, 2010" startWordPosition="464" endWordPosition="467"> 2008). Moreover, our reranking uses not only a generative model obtained from training data, but also a sentence specific generative model learned from a forest. In the reranking stage, we use linearly combined model of these models. We call this variational reranking model. The model proposed in this paper is factored in the third-order structure, therefore, its non-locality makes it difficult to perform the reranking with an usual 1-best Iiterbi search. To solve this problem, we also propose a new search algorithm, which is inspired by the third-order dynamic programming parsing algorithm (Koo and Collins, 2010). This algorithm enables us an exact 1-best reranking without any approximation. We summarize our contributions in this paper as follows. • To extend k-best to forest generative reranking. • We introduce variational reranking which is a combination approach of generative reranking and variational decoding (Li et al., 2009). • To obtain 1-best tree in the reranking stage, we 1479 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479–1488, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics top0,8 saw1,8:V I1 ,2</context>
<context position="10034" citStr="Koo and Collins, 2010" startWordPosition="1598" endWordPosition="1601">,t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d t(h),wt(sib),t(tsib),d — — t(h),wt(sib),t(g),d — — wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d t(h),t(sib),t(tsib),d — — t(h),t(sib),t(g),d — — Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order McDonald et al. (2005) second-order Eisner (1996a) (sibling) McDonald et al. (2005) third-order tri-sibling model (tri-sibling) Model 2 (Koo and Collins, 2010) third-order grandsibling model (Sangati et al., 2009) (grandsibling) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent is not factored in one hyperedge. We summarize the order of each model in Table 3. Our reranking models are gene</context>
<context position="11365" citStr="Koo and Collins (2010)" startWordPosition="1800" endWordPosition="1803">makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate k-best Viterbi search. For a constituent parser, Huang (2008) applied cube pruning techniques to forest reranking with non-local features. Cube pruning is originally proposed for the decoding of statistical machine translation (SMT) with an integrated n-gram language model (Chiang, 2007). It is an approximate k-best Viterbi search algorithm using beam search and lazy computation (Huang and Chiang, 2005). In the case of a dependency parser, Koo and Collins (2010) proposed dynamic-programmingbased third-order parsing algorithm, which enumerates all grandparents with an additional loop. Our hypergraph based search algorithm for Eq.8 share the same spirit to their third-order parsing algorithm since the grandsibling model is similar to their model 1 in that it is factored in grandsibling structure. Algorithm 1 shows the search algorithm. This is almost the same bottom-up 1-best Viterbi algorithm except an additional loop in line 4. Line 4 references outgoing edge e′ of node h from a set of outgoing edges OE(h). tails(e) contains a node v, the sibling nod</context>
<context position="13642" citStr="Koo and Collins (2010)" startWordPosition="2224" endWordPosition="2227"> 5: d(h, e′) = ®f(d(v1, e), ... , d(v|e|, e)) ® we ® ctsib(h, tails(e)) ® c9sib(head(e′), h, tails(e)) 6: if h == top then 7: d(h) = ®f(d(v1, e), ... , d(v|e|, e)) ® we ® ctsib(h, tails(e)) model. we indicates the cost of hyperedge e computed from a baseline discriminative model. Lines 6-7 denote the calculation of the best cost for a top node. We do not compute the cost of the grandsibling model when h is top node because top node has no outgoing edges. Our baseline k-best second-order parser is implemented using Huang and Chiang (2005)’s algorithm 2 whose time complexity is O(m3 +mk log k). Koo and Collins (2010)’s third-order parser has O(m4) time complexity and is theoretically slower than our baseline k-best parser for a long sentence. Our search algorithm is based on the third-order parsing algorithm, but, the search space is previously shrank by a baseline parser’s k-best approximation and a forest pruning algorithm presented in the next section. Therefore, the time efficiency of our reranking is unimpaired. 3.3 Forest Pruning Charniak and Johnson (2005) and Huang (2008) proposed forest pruning algorithms to reduce the size of a forest. Huang (2008)’s pruning algorithm uses a 1-best Iiterbi insid</context>
<context position="26198" citStr="Koo and Collins (2010)" startWordPosition="4283" endWordPosition="4286"> model. Table 8 indicates that grandsibling model achieves a larger gain than that of tri-sibling model. Table 9 shows the examples whose accuracy scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in Table 10: Comparison of our best result (using 16-best forests) with other best-performing Systems on the whole section 23 Parser English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Koo et al. (2008) standard 92.02 Huang and Sagae (2010) 92.1 Koo and Collins (2010) model1 93.04 Koo and Collins (2010) model2 92.93 this work 92.89 Koo et al. (2008) semi-sup 93.16 Suzuki et al. (2009) 93.79 our results. In our experiments, 48% of sentences which contain errors have Prepositional word errors. In fact, well-known PP-Attachment is a problem to be solved for natural language parsers. Other remaining errors are caused by symbols such as .,:“”(). 45% sentences contain such a dependency mistake. Adding features to solve these problems may potentially improve our parser more. 5.4 Comparison with Other Systems Table 10 shows the comparison of the performance of var</context>
<context position="28587" citStr="Koo and Collins (2010)" startWordPosition="4702" endWordPosition="4705">rk Collins (2000) and Charniak and Johnson (2005) proposed a reranking algorithm for constituent parsers. Huang (2008) extended it to a forest reranking algorithm with non-local features. Our framework is for a dependency parser and the decoding in the reranking stage is done with an exact 1-best dynamic programming algorithm. Sangati et al. (2009) proposed a k-best generative reranking algorithm for dependency parsing. In this paper, we use a similar generative model, but combined with a variational model learned on the fly. Moreover, our framework is applicable to forests, not k-best lists. Koo and Collins (2010) presented third-order dependency parsing algorithm. Their model 1 is defined by an enclosing grandsibling for each sibling or grandchild part used in Carreras (2007). Our grandsibling model is similar to the model 1, but ours is defined by a generative model. The decoding in the reranking stage is also similar to the parsing algorithm of their model 1. In order to capture grandsibling factors, our decoding calculates inside probablities for not the current head node but each pair of the node and its outgoing edges. Titov and Henderson (2006) reported that the MBR approach could be applied to </context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proc. the 48th ACL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semisupervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. the ACL,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="26132" citStr="Koo et al. (2008)" startWordPosition="4272" endWordPosition="4275">nking is performed better with the trisibling or grandsibling model. Table 8 indicates that grandsibling model achieves a larger gain than that of tri-sibling model. Table 9 shows the examples whose accuracy scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in Table 10: Comparison of our best result (using 16-best forests) with other best-performing Systems on the whole section 23 Parser English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Koo et al. (2008) standard 92.02 Huang and Sagae (2010) 92.1 Koo and Collins (2010) model1 93.04 Koo and Collins (2010) model2 92.93 this work 92.89 Koo et al. (2008) semi-sup 93.16 Suzuki et al. (2009) 93.79 our results. In our experiments, 48% of sentences which contain errors have Prepositional word errors. In fact, well-known PP-Attachment is a problem to be solved for natural language parsers. Other remaining errors are caused by symbols such as .,:“”(). 45% sentences contain such a dependency mistake. Adding features to solve these problems may potentially improve our parser more. 5.4 Comparison with Oth</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semisupervised dependency parsing. In Proc. the ACL, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Macherey</author>
<author>C Dyer</author>
<author>F Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayesrisk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proc. the 47th ACL,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="21640" citStr="Kumar et al., 2009" startWordPosition="3577" endWordPosition="3580">ntences)4. After MERT was performed until the convergence, the variational reranking finally achieved a 94.5 accuracy score on development data. 5.1 k-best Lists vs. Forests Figure 3 shows the relationship between the size of data structure (the number of hyperedges) and accuracy scores on development data. Obviously, forests can encode a large number of potential candidates more compactly than k-best lists. This means that 4To generate forests, sentences are parsed only once before the training. MERT is performed over the forests. We can also apply a more efficient hypergraph MERT algorithm (Kumar et al., 2009) to the training than a simple MERT algorithm. for reranking, there is more possibility of selecting good candidates in forests than k-best lists. Table 4 shows the statistics of forests and 20- best lists on development data. This setting, threshold p = 10−3 for pruning, is also used for testing. Forests, which have an average of 180.67 hyperedges per sentence, achieve oracle score of 98.76, which is about 1.0% higher than the 96.78 oracle score of 20-best lists with 255.04 hyperedges per sentence. Though the size of forests is smaller than that of k-best lists, the oracle scores of forests a</context>
<context position="23996" citStr="Kumar et al., 2009" startWordPosition="3945" endWordPosition="3948">ly and globally appropriate candidate from a forest. Table 7 shows the parsing time (on 2.66GHz Quad-Core Xeon) of the baseline k-best, generative reranking and variational reranking parsers (java implemented). The variational reranking parser contains the following procedures. 1. k-best forest creation (baseline) 2. Estimation of variational model 3. Forest pruning 4. Search with the third-order model Our reranking parser incurred little overhead to the 1485 Table 5: The comparison of the decoding frameworks: MBR decoding seeks a candidate which has the highest accuracy scores over a forest (Kumar et al., 2009). Variational decoding is performed based on Eq.8. Eval Unlabeled ���������� Decoding baseline 91.9 MBR (8-best forest) 91.99 Variational (8-best forest) 92.17 Table 6: The comparison of the reranking frameworks: Generative means k-best or forest reranking algorithm based on a generative model estimated from a corpus. Variational reranking is performed based on Eq.15. Eval Unlabeled Reranking Generative (8-best) 92.66 Generative (8-best forest) 92.72 Variational (8-best forest) 92.87 Table 7: The parsing time (CPU second per sentence) and accuracy score of the baseline k-best, generative reran</context>
<context position="29348" citStr="Kumar et al. (2009)" startWordPosition="4833" endWordPosition="4836"> used in Carreras (2007). Our grandsibling model is similar to the model 1, but ours is defined by a generative model. The decoding in the reranking stage is also similar to the parsing algorithm of their model 1. In order to capture grandsibling factors, our decoding calculates inside probablities for not the current head node but each pair of the node and its outgoing edges. Titov and Henderson (2006) reported that the MBR approach could be applied to a projective dependency parser. In the field of SMT, for an approximation of MAP decoding, Li et al. (2009) proposed variational decoding and Kumar et al. (2009) presented hypergraph MBR decoding. Our variational model is inspired by the study of Li et al. (2009) and we apply it to a dependency parser in order to select better candidates with third-order information. We also propose an efficient algorithm to estimate the non-local third-order model structure. 7 Conclusions In this paper, we propose a novel forest reranking algorithm for dependency parsing. Our reranking algorithm is a combination approach of generative reranking and variational decoding. The search algorithm in the reranking stage can be performed using dynamic programming algorithm. </context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient minimum error rate training and minimum bayesrisk decoding for translation hypergraphs and lattices. In Proc. the 47th ACL, pages 163–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>J Eisner</author>
<author>S Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. the 47th ACL,</booktitle>
<pages>593--601</pages>
<contexts>
<context position="3497" citStr="Li et al., 2009" startWordPosition="513" endWordPosition="516">hird-order structure, therefore, its non-locality makes it difficult to perform the reranking with an usual 1-best Iiterbi search. To solve this problem, we also propose a new search algorithm, which is inspired by the third-order dynamic programming parsing algorithm (Koo and Collins, 2010). This algorithm enables us an exact 1-best reranking without any approximation. We summarize our contributions in this paper as follows. • To extend k-best to forest generative reranking. • We introduce variational reranking which is a combination approach of generative reranking and variational decoding (Li et al., 2009). • To obtain 1-best tree in the reranking stage, we 1479 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479–1488, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics top0,8 saw1,8:V I1 ,2: propose an exact 1-best search algorithm with the third-order model. In experiments on English Penn Treebank data, we show that our proposed methods bring significant improvement to dependency parsing. Moreover, our variational reranking framework achieves consistent improvement, compared to conventional approaches, such</context>
<context position="15161" citStr="Li et al. (2009)" startWordPosition="2464" endWordPosition="2467">utside probabilities for its ML estimation. We prune away all hyperedges that have score &lt; ρ for a threshold ρ. αβ(e) score = β(top). (10) Following Huang (2008), we also prune away nodes with all incoming and outgoing hyperedges pruned. 4 Variational Reranking Model In place of a maximum a posteriori (MAP) decision based on Eq.2, the minimum Bayes risk (MBR) decision rule (Titov and Henderson, 2006) is commonly used and defined as following equation: � y� = argmin loss(y, y′)p(y′|x) (11) yEG(x) y′EG(x) where loss(y, y′) represents a loss function2. As an alternative to the MBR decision rule, Li et al. (2009) proposed a variational decision rule that rescores candidates with an approximate distribution q* E Q. y� = argmax q*(y) (12) yEG(x) where q* minimizes the KL divergence KL(p||q) q* = argmin KL(p||q) qEQ �=argmax qEQ yEG(x) plog q (13) where each p and q represents p(y|x) and q(y). For SMT systems, q* is modeled by n-gram language model over output strings. While the decoding based on q* is an approximation of intractable MAP decoding3, it works as a rescoring function for candidates generated from a baseline model. Here, we propose to apply the variational decision rule to dependency parsing</context>
<context position="18628" citStr="Li et al. (2009)" startWordPosition="3081" endWordPosition="3084">or weight cgsib of e and e′, which 0 200 400 600 800 1000 1200 1400 the number of hyperedges per sentence Figure 3: The relationship between tha data size (the number of hyperedges) and oracle scores on development data: Forests encode candidates with high accuracy scores more compactly than k-best lists. is similar to that to compute the posterior weight of rules of tree substitution grammars used in treebased MT systems (Mi and Huang, 2008). Lines 13-15 compute expected counts c2 of events occuring over two hyperedges e and e′. Finally, line 19 estimates q*1 and q*2 using the form in Eq.14. Li et al. (2009) assumes n-gram locality of the forest to efficiently train the model, namely, the baseline n-gram model has larger n than that of variational n-gram model. In our case, grandsibling locality is not embedded in the forest generated from the baseline parser. Therefore, we need to reference incoming hyperedges of tail nodes in line 7. y* of Eq.12 may be locally appropriate but globally inadequate because q* only approximates p. Therefore, we log-linearly combine q* with a global generative model estimated from the training data and the baseline discriminative model. 2 Algorithm1 is also applicab</context>
<context position="29294" citStr="Li et al. (2009)" startWordPosition="4825" endWordPosition="4828">ng grandsibling for each sibling or grandchild part used in Carreras (2007). Our grandsibling model is similar to the model 1, but ours is defined by a generative model. The decoding in the reranking stage is also similar to the parsing algorithm of their model 1. In order to capture grandsibling factors, our decoding calculates inside probablities for not the current head node but each pair of the node and its outgoing edges. Titov and Henderson (2006) reported that the MBR approach could be applied to a projective dependency parser. In the field of SMT, for an approximation of MAP decoding, Li et al. (2009) proposed variational decoding and Kumar et al. (2009) presented hypergraph MBR decoding. Our variational model is inspired by the study of Li et al. (2009) and we apply it to a dependency parser in order to select better candidates with third-order information. We also propose an efficient algorithm to estimate the non-local third-order model structure. 7 Conclusions In this paper, we propose a novel forest reranking algorithm for dependency parsing. Our reranking algorithm is a combination approach of generative reranking and variational decoding. The search algorithm in the reranking stage </context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational decoding for statistical machine translation. In Proc. the 47th ACL, pages 593–601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic cfg with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc. the ACL,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="16353" citStr="Matsuzaki et al., 2005" startWordPosition="2659" endWordPosition="2662">sion rule to dependency parsing. For dependency parsing, we can choose to model q* as the tri-sibling and grandsibling generative models in section 3. 2In case of dependency parsing, Titov and Henderson (2006) proposed that a loss function is simply defined using a dependency attachment score. 3In SMT, a marginalization of all derivations which yield a paticular translation needs to be carried out for each translation. This makes the MAP decoding NP-hard in SMT. This variational approximate framework can be applied to other tasks collapsing spurious ambiguity, such as latent-variable parsing (Matsuzaki et al., 2005). 1483 Algorithm 2 DP-ML Estimation(HG(x)) 1: run inside and outside algorithm on HG(x) 2: for v E V do 3: for e E IE(v) do /� 4: ctsib = pe &apos; α (v) /0 (top) 5: for u E tails (e)/�do 6: ctsib = ctsib &apos; N (u) 7: for e′ E IE(u) do 8: cgsib = pe &apos; pe′ &apos; α(v)/β(top) 9: for u′ E tails(e) \ u do 10: cgsib = cgsib &apos; β(u′) 11: for u′′ E tails(e′) do 12: cgsib = cgsib &apos; β(u′′) 13: for u′′ E tails(e′) do 14: c2(u′′|C(u′′))+ = cgsib 15: c2(C(u′′))+ = cgsib 16: for u E tails(e) do 17: c1(u|C(u))+ = ctsib 18: c1(C(u))+ = ctsib 19: MLE estimate q∗1 , q∗2 using formula Eq.14 4.1 ML Estimation from a Forest q</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic cfg with latent annotations. In Proc. the ACL, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="10298" citStr="McDonald and Pereira, 2006" startWordPosition="1635" endWordPosition="1638">— t(h),t(sib),t(g),d — — Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order McDonald et al. (2005) second-order Eisner (1996a) (sibling) McDonald et al. (2005) third-order tri-sibling model (tri-sibling) Model 2 (Koo and Collins, 2010) third-order grandsibling model (Sangati et al., 2009) (grandsibling) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent is not factored in one hyperedge. We summarize the order of each model in Table 3. Our reranking models are generative versions of Koo and Collins (2010)’s third-order factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate</context>
<context position="26109" citStr="McDonald and Pereira (2006)" startWordPosition="4267" endWordPosition="4270">estigate whether variational reranking is performed better with the trisibling or grandsibling model. Table 8 indicates that grandsibling model achieves a larger gain than that of tri-sibling model. Table 9 shows the examples whose accuracy scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in Table 10: Comparison of our best result (using 16-best forests) with other best-performing Systems on the whole section 23 Parser English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Koo et al. (2008) standard 92.02 Huang and Sagae (2010) 92.1 Koo and Collins (2010) model1 93.04 Koo and Collins (2010) model2 92.93 this work 92.89 Koo et al. (2008) semi-sup 93.16 Suzuki et al. (2009) 93.79 our results. In our experiments, 48% of sentences which contain errors have Prepositional word errors. In fact, well-known PP-Attachment is a problem to be solved for natural language parsers. Other remaining errors are caused by symbols such as .,:“”(). 45% sentences contain such a dependency mistake. Adding features to solve these problems may potentially improve our parser more. </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. the 43rd ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="9897" citStr="McDonald et al. (2005)" startWordPosition="1580" endWordPosition="1583">2-nd term 3-rd term 1-st term 2-nd term 3-rd term wt(h),wt(sib),wt(tsib),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),wt(g),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d t(h),wt(sib),t(tsib),d — — t(h),wt(sib),t(g),d — — wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d t(h),t(sib),t(tsib),d — — t(h),t(sib),t(g),d — — Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order McDonald et al. (2005) second-order Eisner (1996a) (sibling) McDonald et al. (2005) third-order tri-sibling model (tri-sibling) Model 2 (Koo and Collins, 2010) third-order grandsibling model (Sangati et al., 2009) (grandsibling) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features</context>
<context position="20324" citStr="McDonald et al., 2005" startWordPosition="3363" endWordPosition="3366">des per sentence and oracle scores. forest 20-best pruning threshold p = 10−3 — ave. num of hyperedges 180.67 255.04 ave. num of nodes 135.74 491.42 oracle scores 98.76 96.78 5 Experiments Experiments are performed on English Penn Treebank data. We split WSJ part of the Treebank into sections 02-21 for training, sections 22 for development, sections 23 for testing. We use Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. We obtain k-best lists and forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al., 2005), using the secondorder Eisner algorithms. We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing. We set the scaling factory = 1.0. We also train a generative reranking model from the training data. To reduce the data sparseness problem, we use the back-off strategy proposed in (Eisner, 1996a). Parameters 0 are trained using MERT (Och, 2003) and for each sentence in the development data, 300-best dependency trees are extracted from its forest. Our variational reranking does not need much time to train the model because the tra</context>
<context position="26076" citStr="McDonald et al. (2005)" startWordPosition="4262" endWordPosition="4265">uracy scores, we further investigate whether variational reranking is performed better with the trisibling or grandsibling model. Table 8 indicates that grandsibling model achieves a larger gain than that of tri-sibling model. Table 9 shows the examples whose accuracy scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in Table 10: Comparison of our best result (using 16-best forests) with other best-performing Systems on the whole section 23 Parser English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Koo et al. (2008) standard 92.02 Huang and Sagae (2010) 92.1 Koo and Collins (2010) model1 93.04 Koo and Collins (2010) model2 92.93 this work 92.89 Koo et al. (2008) semi-sup 93.16 Suzuki et al. (2009) 93.79 our results. In our experiments, 48% of sentences which contain errors have Prepositional word errors. In fact, well-known PP-Attachment is a problem to be solved for natural language parsers. Other remaining errors are caused by symbols such as .,:“”(). 45% sentences contain such a dependency mistake. Adding features to solve these problems may pote</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proc. the 43rd ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mi</author>
<author>L Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="18458" citStr="Mi and Huang, 2008" startWordPosition="3049" endWordPosition="3052">15. c2 for a grandsibling model must be extracted over two hyperedges e and e′ because it needs grandparent information. Lines 8-12 show the algorithm to compute the posterior weight cgsib of e and e′, which 0 200 400 600 800 1000 1200 1400 the number of hyperedges per sentence Figure 3: The relationship between tha data size (the number of hyperedges) and oracle scores on development data: Forests encode candidates with high accuracy scores more compactly than k-best lists. is similar to that to compute the posterior weight of rules of tree substitution grammars used in treebased MT systems (Mi and Huang, 2008). Lines 13-15 compute expected counts c2 of events occuring over two hyperedges e and e′. Finally, line 19 estimates q*1 and q*2 using the form in Eq.14. Li et al. (2009) assumes n-gram locality of the forest to efficiently train the model, namely, the baseline n-gram model has larger n than that of variational n-gram model. In our case, grandsibling locality is not embedded in the forest generated from the baseline parser. Therefore, we need to reference incoming hyperedges of tail nodes in line 7. y* of Eq.12 may be locally appropriate but globally inadequate because q* only approximates p. </context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>H. Mi and L. Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP, pages 206– 214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
</authors>
<title>Semiring framework and algorithms for shortest-distance problems.</title>
<date>2002</date>
<journal>Automata, Languages and Combinatorics,</journal>
<pages>7--321</pages>
<contexts>
<context position="12390" citStr="Mohri, 2002" startWordPosition="1978" endWordPosition="1979">1-best Viterbi algorithm except an additional loop in line 4. Line 4 references outgoing edge e′ of node h from a set of outgoing edges OE(h). tails(e) contains a node v, the sibling node sib and tri-sibling node tsib of v, moreover, the head of e′ (head(e′)) is the grandparent for v and sib. Thus, in line 5, we can capture tri-sibling and grandsibling information and compute the current inside estimate of Eq.8. In our actual implementation, each score of components in Eq.8 is represented as a cost. This is written as a shortest path search algorithm with a tropical (real) semiring framework (Mohri, 2002; Huang, 2006). Therefore, ® denotes the min operater and � denotes the + operater. The function f is defined as follows: |e| f(d(v1, e), ... , d(v|e|, e))) = NA d(vi, e) (9) i=1 where d(vi, e) denotes the current estimate of the best cost for a pair of node vi and a hyperedge e. ® sums the best cost of a pair of a sub span node and hyperedge e. Each ctsib and cgsib in line 5 and 7 indicates the cost of tri-sibling and grandsibling h tsib sib v 9 h sib v 1482 Algorithm 1 Exact DP-Search Algorithm(HG(x)) 1: for h E V in bottom-up topological order do 2: for e E IE(h) do // tails(e) is {v1, ... </context>
</contexts>
<marker>Mohri, 2002</marker>
<rawString>M. Mohri. 2002. Semiring framework and algorithms for shortest-distance problems. Automata, Languages and Combinatorics, 7:321–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. the 41st ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="8696" citStr="Och, 2003" startWordPosition="1419" endWordPosition="1420"> (1996a) proposed a back-off strategy which reduces the conditioning of a model. We show the reductions list for each term of two models in Table 2. The usage of reductions list is identical to Eisner (1996a) and readers may refer to it for further details. The final prediction is performed using a loglinear interpolated model. It interpolates the baseline discriminative model and two (tri-sibling and grandsibling) generative models. 2 log qn(top(y))Bn � log p(y|x)Bbase (8) where 0 are parameters to adjust the weight of each term in prediction. These parameters are tuned using MERT algorithm (Och, 2003) on development data using a criterion of accuracy maximization. The reason why we chose MERT is that it effectively tunes dense parameters with a line search algorithm. |tailsL(e)| � l=1 q(v) = |tailsR(e)| �× r=1 y� = argmax yEG(x) � n=1 1481 Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word, POS-tag for a node. d indicates the direction. The first reduction on the list keeps all or most of the original condition; later reductions throw away more and more of this information. tri-sibling grandsibling 1-st term 2-nd term 3-rd term 1</context>
<context position="20735" citStr="Och, 2003" startWordPosition="3433" endWordPosition="3434">ert phrase structure to dependency structure. We obtain k-best lists and forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al., 2005), using the secondorder Eisner algorithms. We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing. We set the scaling factory = 1.0. We also train a generative reranking model from the training data. To reduce the data sparseness problem, we use the back-off strategy proposed in (Eisner, 1996a). Parameters 0 are trained using MERT (Och, 2003) and for each sentence in the development data, 300-best dependency trees are extracted from its forest. Our variational reranking does not need much time to train the model because the training is performed over not the training data (39832 sentences) but the development data (1700 sentences)4. After MERT was performed until the convergence, the variational reranking finally achieved a 94.5 accuracy score on development data. 5.1 k-best Lists vs. Forests Figure 3 shows the relationship between the size of data structure (the number of hyperedges) and accuracy scores on development data. Obvio</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. the 41st ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sangati</author>
<author>W Zuidema</author>
<author>R Bod</author>
</authors>
<title>A generative re-ranking model for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. the 11th IWPT,</booktitle>
<pages>238--241</pages>
<contexts>
<context position="1690" citStr="Sangati et al. (2009)" startWordPosition="233" endWordPosition="236">achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner, 1996b; Eisner, 1996a) for reranking and extend it to capture higher-order information than Eisner’s second-order generative model. Their reranking model showed large improvements in dependency parsing accuracy. They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selection </context>
<context position="7301" citStr="Sangati et al. (2009)" startWordPosition="1186" endWordPosition="1189">eft or right child sequence. This is factored in a tri-sibling structure shown in the left side of Figure 2. Eq.4 is further decomposed into a product of the form consisting of three terms: q1(v|h,sib, tsib, dir) (5) = q1(dist(v, h), wrd(v), tag(v)|h, sib, tsib, dir) = q1(tag(v)|h, sib, tsib, dir) ×q1(wrd(v)|tag(v), h, sib, tsib, dir) ×q1(dist(v, h)|wrd(v), tag(v), h, sib, tsib, dir) where tag(v) and wrd(v) are the POS-tag and word of v and dist(v, h) is the distance between positions of v and h. The values of dist(v, h) are partitioned into 4 categories: 1, 2, 3 − 6, 7 − ∞. Second, following Sangati et al. (2009), we define a grandsibling model whose context space consists of the head node, sibling node, grandparent node and direction of a node v. q2(v|C(v)) = q2(v|h,sib, g,dir) (6) where g is a grandparent node of v. Analogous to Eq.5, Eq.6 is decomposed into three terms: q2(v|h, sib, g, dir) (7) = q2(dist(v, h), wrd(v), tag(v)|h, sib, g, dir) = q2(tag(v)|h, sib, g, dir) ×q2(wrd(v)|tag(v), h, sib, g, dir) ×q2(dist(v, h)|wrd(v), tag(v), h, sib, g, dir) where notations are the same as those in Eq.5 with the exception of tri-sibling tsib and grandparent g. This model is factored in a grandsibling struct</context>
<context position="10088" citStr="Sangati et al., 2009" startWordPosition="1605" endWordPosition="1608">(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d t(h),wt(sib),t(tsib),d — — t(h),wt(sib),t(g),d — — wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d t(h),t(sib),t(tsib),d — — t(h),t(sib),t(g),d — — Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order McDonald et al. (2005) second-order Eisner (1996a) (sibling) McDonald et al. (2005) third-order tri-sibling model (tri-sibling) Model 2 (Koo and Collins, 2010) third-order grandsibling model (Sangati et al., 2009) (grandsibling) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent is not factored in one hyperedge. We summarize the order of each model in Table 3. Our reranking models are generative versions of Koo and Collins (2010)’s third-orde</context>
<context position="28315" citStr="Sangati et al. (2009)" startWordPosition="4660" endWordPosition="4663">orrect 3 3 4 0 4 5 6 4 11 11 12 8 12 4 baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4 proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4 sent (No.283) Many called it simply a contrast in styles . correct 2 0 2 6 6 2 6 7 2 baseline 2 0 2 2 6 2 6 7 2 proposed 2 0 2 6 6 2 6 7 2 6 Related Work Collins (2000) and Charniak and Johnson (2005) proposed a reranking algorithm for constituent parsers. Huang (2008) extended it to a forest reranking algorithm with non-local features. Our framework is for a dependency parser and the decoding in the reranking stage is done with an exact 1-best dynamic programming algorithm. Sangati et al. (2009) proposed a k-best generative reranking algorithm for dependency parsing. In this paper, we use a similar generative model, but combined with a variational model learned on the fly. Moreover, our framework is applicable to forests, not k-best lists. Koo and Collins (2010) presented third-order dependency parsing algorithm. Their model 1 is defined by an enclosing grandsibling for each sibling or grandchild part used in Carreras (2007). Our grandsibling model is similar to the model 1, but ours is defined by a generative model. The decoding in the reranking stage is also similar to the parsing </context>
</contexts>
<marker>Sangati, Zuidema, Bod, 2009</marker>
<rawString>F. Sangati, W. Zuidema, and R. Bod. 2009. A generative re-ranking model for dependency parsing. In Proc. the 11th IWPT, pages 238–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. the EMNLP,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="26317" citStr="Suzuki et al. (2009)" startWordPosition="4304" endWordPosition="4307">e examples whose accuracy scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in Table 10: Comparison of our best result (using 16-best forests) with other best-performing Systems on the whole section 23 Parser English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Koo et al. (2008) standard 92.02 Huang and Sagae (2010) 92.1 Koo and Collins (2010) model1 93.04 Koo and Collins (2010) model2 92.93 this work 92.89 Koo et al. (2008) semi-sup 93.16 Suzuki et al. (2009) 93.79 our results. In our experiments, 48% of sentences which contain errors have Prepositional word errors. In fact, well-known PP-Attachment is a problem to be solved for natural language parsers. Other remaining errors are caused by symbols such as .,:“”(). 45% sentences contain such a dependency mistake. Adding features to solve these problems may potentially improve our parser more. 5.4 Comparison with Other Systems Table 10 shows the comparison of the performance of variational reranking (16-best forests) with that of other systems. Our method outperforms supervised parsers with second-</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In Proc. the EMNLP, pages 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>Bayes risk minimization in natural language parsing.</title>
<date>2006</date>
<booktitle>In Technical Report,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="14948" citStr="Titov and Henderson, 2006" startWordPosition="2428" endWordPosition="2431">lity α(v), while Charniak and Johnson (2005) use the usual inside/outside algorithm. In our experiments, we use Charniak and Johnson (2005)’s forest pruning criterion because the variational model needs traditional inside/outside probabilities for its ML estimation. We prune away all hyperedges that have score &lt; ρ for a threshold ρ. αβ(e) score = β(top). (10) Following Huang (2008), we also prune away nodes with all incoming and outgoing hyperedges pruned. 4 Variational Reranking Model In place of a maximum a posteriori (MAP) decision based on Eq.2, the minimum Bayes risk (MBR) decision rule (Titov and Henderson, 2006) is commonly used and defined as following equation: � y� = argmin loss(y, y′)p(y′|x) (11) yEG(x) y′EG(x) where loss(y, y′) represents a loss function2. As an alternative to the MBR decision rule, Li et al. (2009) proposed a variational decision rule that rescores candidates with an approximate distribution q* E Q. y� = argmax q*(y) (12) yEG(x) where q* minimizes the KL divergence KL(p||q) q* = argmin KL(p||q) qEQ �=argmax qEQ yEG(x) plog q (13) where each p and q represents p(y|x) and q(y). For SMT systems, q* is modeled by n-gram language model over output strings. While the decoding based o</context>
<context position="29135" citStr="Titov and Henderson (2006)" startWordPosition="4795" endWordPosition="4798">r framework is applicable to forests, not k-best lists. Koo and Collins (2010) presented third-order dependency parsing algorithm. Their model 1 is defined by an enclosing grandsibling for each sibling or grandchild part used in Carreras (2007). Our grandsibling model is similar to the model 1, but ours is defined by a generative model. The decoding in the reranking stage is also similar to the parsing algorithm of their model 1. In order to capture grandsibling factors, our decoding calculates inside probablities for not the current head node but each pair of the node and its outgoing edges. Titov and Henderson (2006) reported that the MBR approach could be applied to a projective dependency parser. In the field of SMT, for an approximation of MAP decoding, Li et al. (2009) proposed variational decoding and Kumar et al. (2009) presented hypergraph MBR decoding. Our variational model is inspired by the study of Li et al. (2009) and we apply it to a dependency parser in order to select better candidates with third-order information. We also propose an efficient algorithm to estimate the non-local third-order model structure. 7 Conclusions In this paper, we propose a novel forest reranking algorithm for depen</context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>I. Titov and J. Henderson. 2006. Bayes risk minimization in natural language parsing. In Technical Report, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Tu</author>
<author>Y Liu</author>
<author>Y Hwang</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Dependency forest for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proc. the 23rd COLING,</booktitle>
<pages>1092--1100</pages>
<contexts>
<context position="5218" citStr="Tu et al., 2010" startWordPosition="795" endWordPosition="798"> the conditional probability p(y|x) is typically derived as follows: N gir x eγ·s (x,y) = eγ·s(x,y) 2 p(y |) — Z(x) FI yEG(x) eγ·s(x,y) ( ) where s(x, y) is the score function shown in Eq.1 and γ is a scaling factor to adjust the sharpness of the distribution and Z(x) is a normarization factor. 2.1 Hypergraph Representation We propose to encode many hypotheses in a compact representation called dependency forest. While there may be exponentially many dependency trees, the forest represents them in polynomial space. A dependency forest (or tree) can be defined as a hypergraph data strucure HG (Tu et al., 2010). Figure 1 shows an example of a hypergraph for a dependency tree. A shaded hyperedge e is defined as the following form: e : ((I1,2, girl3,5, with5,8), saw1,8). 1480 Table 1: An event list of tri-sibling model whose event space is v|h, sib, tsib, dir, extracted from hyperedge e in Figure 1. EOC is an end symbol of sequence. eventspace I I saw NONE NONE L EOC I saw I NONE L girl I saw NONE NONE R with I saw girl NONE R EOC I saw with girl R the probability of our model q is defined as follows: q(vl|C(vl)) · q(vl) q(vr|C(vr)) · q(vr) (3) where |tailsL(e) |and |tailsR(e) |are the number of left </context>
</contexts>
<marker>Tu, Liu, Hwang, Liu, Lin, 2010</marker>
<rawString>Z. Tu, Y. Liu, Y. Hwang, Q. Liu, and S. Lin. 2010. Dependency forest for statistical machine translation. In Proc. the 23rd COLING, pages 1092–1100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. the IWPT,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="20104" citStr="Yamada and Matsumoto (2003)" startWordPosition="3327" endWordPosition="3330"> yˆ = argmax � log qn(top(y))Bn yEG(x) n=1 2 n +1: log q* n(top(y))B� n=1 +log p(y|x)Bbase (15) 1484 Table 4: The statistics of forests and 20-best lists on development data: this shows the average number of hyperedges and nodes per sentence and oracle scores. forest 20-best pruning threshold p = 10−3 — ave. num of hyperedges 180.67 255.04 ave. num of nodes 135.74 491.42 oracle scores 98.76 96.78 5 Experiments Experiments are performed on English Penn Treebank data. We split WSJ part of the Treebank into sections 02-21 for training, sections 22 for development, sections 23 for testing. We use Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. We obtain k-best lists and forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al., 2005), using the secondorder Eisner algorithms. We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing. We set the scaling factory = 1.0. We also train a generative reranking model from the training data. To reduce the data sparseness problem, we use the back-off strategy proposed in (Eisner, 1996a). Parameters 0 are</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. the IWPT, pages 195–206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>