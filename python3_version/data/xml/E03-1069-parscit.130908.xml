<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000783">
<title confidence="0.933589">
A Shallow Model of Backchannel Continuers in Spoken Dialogue
</title>
<author confidence="0.990893">
Nicola Cathcart Jean Carletta and Ewan Klein
</author>
<affiliation confidence="0.9806095">
Canon Research Centre Europe School of Informatics
Bracknell, UK University of Edinburgh
</affiliation>
<email confidence="0.996206">
nicolac@cre.canon.co.uk {jeanc,ewan}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.996621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909444444445">
Spoken dialogue systems would be more
acceptable if they were able to produce
backchannel continuers such as mm-hmm in
naturalistic locations during the user&apos;s utter-
ances. Using the HCRC Map Task Cor-
pus as our data source, we describe mod-
els for predicting these locations using only
limited processing and features of the user&apos;s
speech that are commonly available, and
which therefore could be used as a low-
cost improvement for current systems. The
baseline model inserts continuers after a pre-
determined number of words. One fur-
ther model correlates back-channel contin-
uers with pause duration, while a second pre-
dicts their occurrence using trigram POS fre-
quencies. Combining these two models gives
the best results.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994366604166667">
In a spoken dialogue between people, the participants
use simple utterances such as yeah, a totty wee bit aye
and mm-hnint to signal that communication is work-
ing. Without this feedback, the partner may assume
that he has not been understood and reformulate his ut-
terance. Following Yngve (1970), we will use the term
backchannel for such utterances. Although these can
be substantive because they can repeat material from
the partner&apos;s utterance (Clark and Schaefer, 1991), e.g.,
Right, okay, I&apos;m below the fiat rocks, we will adopt (Jo-
rafsky et al., 1998)&apos;s terminology of continuer. We
will take this to refer to the class of backchannel ut-
terances, with minimal content, used to clearly signal
that the speaker should continue with her current turn.
(Yankelovich et al., 1995) point out that users of speech
interface systems need feedback, too, especially since
the system&apos;s silence could mean either of two very dif-
ferent things: that it is waiting for user input, in which
case the user should speak, or that it is still processing
information, in which case the user should not. How-
ever, any feedback must come at the right time or else
it risks disrupting the speaker and ultimately, delaying
task completion (Hirasawa et al., 1999).
Most of our data, including the examples given
above, are drawn from the HCRC Map Task Corpus,
described in more detail in Section 3. Clearly these di-
alogues are significantly more complex than the kind of
interactions supported by current commercial spoken
dialogue systems, where the length of user utterances
is severely constrained. What kind of system would in-
volve potentially lengthy user instructions comparable
to those found in the Map Task? Lauria et al. (2001),
Lemon et al. (2002), and Theobalt et al. (2002) describe
work on building spoken dialogue systems for convers-
ing with mobile robots, and this is a setting where com-
plex instructions naturally arise. For example, in one
scenario,1 users attempt to teach routes and route seg-
ments to a robot. (1) is a portion of such an instruction.
(1) okay go to the end of the road and turn left and
erm ... and then carry on down that road
and then turn ... take your second left where
the trees are on the corner
We describe a shallow model, based on human dia-
logue data, for predicting where to place backchannel
feedback. The model deliberately requires only simple
processing on information that spoken dialogue sys-
tems already keep as history, and is intended to support
a low-cost improvement to existing technology.
</bodyText>
<footnote confidence="0.741504">
&apos;For details, see the description of the IBL Project pre-
sented on http: //www. ltg. ed. ac . uk/dsea/.
</footnote>
<page confidence="0.992861">
51
</page>
<bodyText confidence="0.992757414893617">
2 Where are backchannels thought to speech n-grams, pitch, and FO contour in the immedi-
occur? ately preceding context, and pauses at the location it-
self.
There are two literatures we can draw on to inspire our
model: linguistic theory that predicts where backchan-
nels will occur because of the purpose they serve, and
past corpus-based attempts to model backchannel loca-
tions.
Theoretically, backchannel continuers will be most
interpretable by the speaker if they occur at or before
an utterance reaches a pragmatic completion— that is,
where a segment is &amp;quot;interpretable as a complete con-
versational action within its specific context&amp;quot; (Du Bois
et al., 1993)(p. 147) — but not too early in the utter-
ance. This is because planning the content of an ut-
terance, formulating it, articulating it, and monitoring
the partner&apos;s understanding are all parallel processes,
with monitoring kicking in when planning ends (Lev-
elt, 1998).
Classically, pragmatic completions yield transition
relevance places, or TRPs for short, where the current
hearer can take over the main channel of communica-
tion by taking a turn (Sacks et al., 1974), for instance,
to clear up something that he does not understand. If
the current hearer chooses to take over, then a &amp;quot;turn ex-
change&amp;quot; is said to occur. If the current hearer chooses
not to take over, instead remaining passive or giving
feedback through, e.g., a nod, grimace, or backchan-
nel continuer, then the speaker must decide whether
to go back or go on. Of course, it is possible for the
hearer first to give feedback and subsequently to de-
cide to take a turn. So we would expect speakers to be
able to receive backchannel continuers at TRPs, espe-
cially when they do not lead to turn exchange, or be-
fore TRPs in, say, the second half of their utterance. In
their updating of the classic model, Ford and Thomp-
son (1996)(p. 144) describe &amp;quot;complex transition rele-
vance points (cTRPs)&amp;quot; as confluences where intention,
intonation, and grammatical structure are all complete.
For them, an utterance is grammatically complete if it
&amp;quot;could be interpreted as a complete clause ... with an
overt or directly recoverable predicate&amp;quot;.
Since speakers can always add phrases after the
predicate, grammatical completion is necessary but not
sufficient to make a cTRP. Thus linguistic theory sug-
gests that knowing where to find TRPs will help one
know where to place backchannel continuers, and that
pragmatics, grammar and intonation are all useful cues.
In addition to this theorizing, there have been a
number of previous corpus-based studies that have at-
tempted to describe or model the location of backchan-
nel continuers, TRPs, and turn exchanges, by reference
to the preceding context. These have tended to concen-
trate on easy-to-measure phenomena that clearly relate
to grammatical and intonational completion: part-of-
Denny (1985) was concerned with describing the pre-
ceding context of only those turn exchanges at
which there were pauses of over 65ms, and partic-
ularly those at which backchannel continuers oc-
curred. In her description, she considered pitch
rise and fall, speaker and auditor gaze, gesture,
-filled pauses&amp;quot; such as mm-hmm, and grammati-
cal completion.
Koiso et al. (1998), working in a Japanese replication
of the same corpus on which our results are based,
used all pauses over 100ms as an operational
definition of when turn exchange is possible —
that is, of TRPs — and considered predictors of
whether or not turn exchange occurred at a TRP,
and, when it did not, whether or not the hearer
produced a backchannel continuer.2 They used
as predictors the immediately preceding part-of-
speech plus prosodic features: duration of the fi-
nal phoneme, FO contour, peak FO, energy pat-
tern, and peak energy. They found that the best
single predictor of either phenomena was the pre-
ceding part-of-speech tag, but that combining the
prosodic features gave better results, or, prefer-
ably, augmenting the part-of-speech tag with the
combined prosody features. Turn exchange was
indicated by interjections, sentence-final particles,
and imperative and conclusive verb forms, to-
gether with a rise or fall in intonation. Hearer use
of a backchannel continuer was indicated by con-
junctive and case/adverbial particles and adverbial
verb forms, coupled with the FO contours flat-fall
and rise-fall.
Ward &amp; Tsukahara (2000) modeled the location of
backchannel continuers in Japanese and English
coversation simply by inserting them wherever the
other speaker produced a region of low pitch last-
ing 110ms. This model is motivated by the obser-
vation that such regions often accompany gram-
matical completion. Their model achieved 18%
</bodyText>
<footnote confidence="0.7971076">
2The identification of long pauses with TRPs, although
understandable in the context of informing work on spoken
dialogue systems, is somewhat at odds with previous think-
ing about turn-taking. Although turn-taking behaviour is cul-
turally dependent , human dialogue is generally considered
remarkable for how little silence there can be between turns.
A previous study of Map Task data (Bull and Aylett, 1998),
bears up Sacks, Schegloff and Jefferson&apos;s original (1974) ob-
servation that turns often latch, with no perceivable silent gap,
or that they even overlap.
</footnote>
<page confidence="0.998703">
52
</page>
<bodyText confidence="0.996898714285714">
accuracy for English and 34% for Japanese.3
Although none of these studies is performing exactly
the same task as we are, they jointly suggest a range
of features that could be included in our model. For
example, FO contour would clearly be useful in pre-
dicting backchannel location. However, the challenge
of extracting appropriate prosodic features from a pitch
tracker lay outside the scope of the research effort re-
ported here. Moreover, the multimodal features con-
sidered by Denny seemed too far from the current state-
of-the-art in speech recognition systems to be of im-
mediate practical interest. Therefore, for this work, we
restrict ourselves to pause duration and part-of-speech
tag sequences as inputs.
</bodyText>
<sectionHeader confidence="0.963946" genericHeader="method">
3 Corpus Analysis
</sectionHeader>
<bodyText confidence="0.99993178125">
For our modelling, we use the HCRC Map Task Corpus
(Anderson et al., 1991),4 a set of 128 task-oriented di-
alogues between human speakers of Scottish English,
lasting six minutes on average. In half of the conver-
sations the participants could see each others&apos; faces; in
the other half, this was prevented by a screen. We ig-
nore this distinction, combining data from the two con-
ditions. Although participants must cooperate to com-
plete the task, their roles are somewhat unbalanced,
with one participant, the &amp;quot;instruction Giver&amp;quot;, dominat-
ing their planning For this reason, all of our analysis
considers where the &amp;quot;instruction Follower&amp;quot; produces
backchannel continuers in relation to the instruction
Giver&apos;s speech.
At the most basic level, a Map Task dialogue rep-
resents each participant&apos;s behaviour separately as a
sequence of time-stamped silences, noises (such as
coughing), and speech tokens, to which part-of-speech
tagging has been applied. The part-of-speech tag set is
based on a version of the Brown Corpus tag set which
was modified slightly to better accommodate the cor-
pus ((McKelvie, 2001)). These together allow us to
calculate our input features.
We identify Giver TRPs using existing dialogue
structure coding. The Map Task Corpus has been seg-
mented by hand into dialogue moves, as described in
(Carletta et al., 1997). With the exception of moves
in the &amp;quot;acknowledge&amp;quot;, &amp;quot;ready&amp;quot;, and &amp;quot;align&amp;quot; categories,
each move represents one utterance that is either prag-
matically complete or, rarely, abandoned. In this sys-
tem, a ready move is essentially a discourse marker that
pre-initiates some larger move, usually an instruction
</bodyText>
<footnote confidence="0.9635632">
3Their paper does not specify how these figures are to be
interpreted in terms of precision and recall.
4The transcriptions and coding for the Map Task Cor-
pus are available from http: //www.hcrc.ed.ac.uk/
dialogue/maptask.html.
</footnote>
<table confidence="0.999623666666667">
Acknowledgement Frequency % of Total
right 1226 29%
okay 587 14%
mm-hmm 462 11%
uh 332 8%
right okay 267 6%
yeah 155 4%
oh right 42 &lt;1%
mm 39 &lt;1%
oh 29 &lt;1%
okay right 19 &lt;1%
aye 17 &lt;1%
</table>
<tableCaption confidence="0.999789">
Table 1: Frequency of Acknowledgements
</tableCaption>
<bodyText confidence="0.999823434782608">
(as in OK, go to the left of the swamp...), and an align
move is usually added to the end of a move to elicit ex-
plicit feedback from the partner (as in, Go to the left of
the swamp, OK?). We treat move boundaries as TRPs
in our processing, ignoring the two exceptions above
which consist predominantly of one-word moves. Fail-
ure to remove them affects only our baseline model.
The acknowledge move was used to locate
backchannel continuers. In this system, all backchan-
nel continuers are acknowledge moves, but not all ac-
knowledge moves are backchannel continuers; follow-
ing Clark and Schaefer (1991), they include some-
what more substantive ways of moving the conversa-
tion forward, such as paraphrasing the speaker&apos;s utter-
ance repeating part or all of it verbatim, or accepting
its contents. To identify the instruction Follower&apos;s
backchannel continuers, we filtered the list of their ac-
knowledge moves by removing any that contained con-
tent words or words that generally convey acceptance
such as alright. Table 1 gives the most frequent forms
of backchannel continuers resulting from this process,
which differ somewhat from Jurafsky et al.&apos;s (1998)
analysis of American speech.
</bodyText>
<sectionHeader confidence="0.853514" genericHeader="method">
4 Description of Models
</sectionHeader>
<subsectionHeader confidence="0.752332">
4.1 Baseline Model
</subsectionHeader>
<bodyText confidence="0.999966666666667">
For our baseline model, we planned to insert a
backchannel continuer after every n words, for some
plausible value of n. This seemed to be the simplest
choice in its own right. However, the choice can also be
justified as follows. We expect backchannel continuers
to be placed at or before intonational phrase bound-
aries, since these are a primary indicator for TRPs.
Spotting these boundaries requires a pitch tracker, but
in at least one corpus of spoken English, they are
known to occur every five to fifteen syllables (Knowles
et al., 1996). We decided to approximate syllables by
words. Thus, from each of our Follower backchannels,
</bodyText>
<page confidence="0.992581">
53
</page>
<bodyText confidence="0.9969782">
we can measure the number of words back to the last
Follower backchannel continuer, or Giver TRP, as de-
termined by move boundaries. Figure 1 shows the re-
sulting frequency distribution for the number of Giver
words between Follower backchannel continuers.
</bodyText>
<figure confidence="0.879449666666667">
111111111111111,„.„......
0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 47 56
Number of Words
</figure>
<figureCaption confidence="0.9611295">
Figure 1: The Number of Giver Words between a Move
Boundary and a Backchannel Continuer
</figureCaption>
<bodyText confidence="0.999877458333333">
In addition to the inclusion of the &amp;quot;ready&amp;quot; and
&amp;quot;align&amp;quot; categories (discussed in Section 3), the trigram
&lt;s&gt; &lt;a f f &gt; &lt;bc&gt; accounts for a continuer occuring
after one word. The part-of-speech tag &lt;af f&gt; (affir-
mative) refers to interjections such as right, okay, mm-
hmm, uh-huh, yes and no. Affirmative acknowledge-
ments produced in these circumstances are intended to
convey that the Follower has understood the preceding
command and is now ready to move on to the next task.
Several models were built that inserted a continuer
after n words. The value of n was determined by the
frequency of continuers occurring in the data. The vari-
able n increased by one iteration for each model and
ranged from four to ten inclusively. The Precision, Re-
call and F-measure values were found for each model
and can be seen in Figure 2. This graph shows all
three evaluation metrics for each of the seven models.
The smaller the value of n, the more frequently the
continuers are inserted. In the model where n equals
four, there are 7,147 continuers inserted, but only 3,300
where n equals ten. This is reflected in the recall curve.
The highest F-measure score was produced by pre-
dicting a continuer at the mode frequency of every
seven words. The score is only 6%.
</bodyText>
<subsectionHeader confidence="0.683816">
4.2 Pause Duration Model
</subsectionHeader>
<bodyText confidence="0.999931666666667">
Our next model is based simply on pause duration,
working from the premise that backchannel contin-
uers often occur at TRPs, and that TRPs often contain
</bodyText>
<figure confidence="0.963746111111111">
25 -
- - - - Precision
— -• — Recall
— F-measure
&amp;quot; &apos;•••
5 - A- — — —A- — — — — —A
0
4 5 6 1 8 9 10
Number of Words
</figure>
<figureCaption confidence="0.980536">
Figure 2: Values for Number of Words
</figureCaption>
<table confidence="0.998647">
Threshold Prec. Recall F-meas.
0.9 22 59 32
1.0 22 55 31
1.1 22 51 31
</table>
<tableCaption confidence="0.998228">
Table 2: Highest Performing Pause Duration Models
</tableCaption>
<bodyText confidence="0.999711476190476">
pauses. As we explained in our discussion of (Koiso et
al., 1998), this premise is common, but controversial.
Figure 3 compares the durations of the 12% of instruc-
tion Giver pauses that overlap with Follower backchan-
nel contributes with the durations of the majority that
do not.5
Of course, a real-time system cannot wait to see ex-
actly how a long a pause turns out to have been be-
fore deciding whether or not to produce a backchan-
nel continuer. In our data, 50% of the pauses lacking
backchannel continuers are less than 500ms; moreover,
only 11% of pauses this short do attract continuers. For
this reason, we apply a threshold; the model works by
producing a backchannel for all pauses once they reach
a certain length. Eleven models were run, starting with
a threshold of below 400ms, and increasing the thresh-
old value in increments of 100ms.
Table 2 shows the values for the highest perform-
ing models. The model that only inserts continuers in
pauses over 900 milliseconds has the highest overall
score. This model was applied to the test set.
</bodyText>
<subsectionHeader confidence="0.929886">
4.3 n-gram Part-of-Speech Model
</subsectionHeader>
<bodyText confidence="0.999985333333333">
Separating the data into training, validation and test
sets was carried out by generating a random dialogue
ID. The IDs are in the format q [ 1 —8 ] [ en] c [ 1 —8 1.
A random number was produced for each variable and
the files were moved into the relevant directory. The
division was approximately 50% training, 30% vali-
</bodyText>
<footnote confidence="0.995878">
5For technical reasons to do with the corpus markup,
we counted noises that occurred between instruction Giver
moves as pauses, but not noises that occurred within moves.
</footnote>
<figure confidence="0.9675859">
450
400
350
300
`8) 2.50
g. 200
150
100
50
0
</figure>
<page confidence="0.676624">
54
</page>
<figureCaption confidence="0.998444">
Figure 3: Comparison of Pause Duration
</figureCaption>
<figure confidence="0.9958045">
140
120 -
100 -
E 80
a
r 60 -
0 _
no
40 -
20 -
0.0 0.2 0.4 0.6 0.8
(a) Duration of Pauses with Continuer
IIIIIIIIIIIIIIIIuI
0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3.0
Duration in seconds
(b) Duration of Pauses without Continuers
</figure>
<bodyText confidence="0.998866565217391">
dation and 20% test data. The validation data was
necessary for building the CMU-Cambridge language
model, but was concatenated with the training set for
the other models.
The model was forced to back-off to a unigram af-
ter seeing the continuer tag &lt;bc&gt;, since we did not
want this tag to be used as a predictor for any other
n-grams. Each move was considered a sentence and
given a context tag of &lt;s&gt; and &lt;/ a&gt; for the start and
end of a move respectively, with one move per line.
Within the model design the &lt; s &gt; cue automatically
causes a forced back-off to a bigram so that the in-
formation before the beginning of a sentence is dis-
regarded. This ensured that each sentence was kept
as a separate entity; since Follower moves other than
acknowledge moves were not represented, sentences
were not necessarily in consecutive order.
There are seven occurrences of P(&lt;bc&gt; ) with a
back-off value of one. This shows the result of the
forced back-off after a continuer tag, and is applied to
instances where two continuers appear consecutively.
Twenty-one continuers were predicted by the trigram
&lt;s&gt; &lt;aff&gt; &lt;bc&gt;. This trigram reflects the manoeu-
</bodyText>
<table confidence="0.999611272727273">
Ttigram Probability Freq./million
P(&lt;bc&gt; NNS &lt;pau&gt;) 0.263 134.42
P(&lt;bc&gt; PPO &lt;pau&gt;) 0.220 82.83
P(&lt;bc&gt; NN &lt;pau&gt;) 0.185 627.64
P(&lt;bc&gt; PD &lt;pau&gt;) 0.170 33.34
P(‹bc&gt; AP &lt;pau&gt;) 0.150 3.95
P(&lt;bc&gt; PN &lt;pau&gt;) 0.115 14.66
P(&lt;bc&gt; RP &lt;pau&gt;) 0.010 113.10
P(&lt;bc&gt; LI &lt;pail&gt;) 01)98 25 44
P(&lt;bc&gt; CC PPG) 0.091 0.74
P(&lt;bc&gt; DO &lt;pau&gt;) 0.091 4.61
</table>
<tableCaption confidence="0.9019485">
Table 3: Discounted Trigram Frequencies in the CMU-
Cambridge Language Model
</tableCaption>
<bodyText confidence="0.997150181818182">
vre &amp;quot;Follower query + Giver affirmative + Follower
continuer&amp;quot;, discussed in Section 4.1, and accounts for
some examples of a continuer occurring after only one
word.
The ten highest trigram probability counts (using
Witten Bell discounting) can be seen in Table 3. The
sequence most likely to predict a continuer is a plural
noun (NNS) followed by a pause, while sequences con-
sisting of singular noun (NN) plus pause come third.
Together, this shows that nouns (either singular or plu-
ral) before a pause are good indicators of a backchannel
continuer. The tags PPO, PD and PN all represent pro-
nouns and before a pause they make up the second most
probable group for predicting a continuer.
A model was built using the three most frequent tri-
grams as predictors. A second model was constructed
using all of the ten most frequent trigrams in Table 3.
The aim of this model was to see if increasing the num-
ber of factors used in prediction would significantly im-
prove the coverage whilst also maintaining a high ac-
curacy. A continuer was inserted after the occurrence
of any of these trigrams in the data.
</bodyText>
<subsectionHeader confidence="0.841085">
4.4 Combined Model
</subsectionHeader>
<bodyText confidence="0.999964375">
The pause duration model was designed to differentiate
between pauses that contained continuers and pauses
that didn&apos;t. Combining the models could be used to
filter out the instances where the combination of tags
would be more likely to predict an end of move bound-
ary. More precisely a combination of the two models
would use the language model to predict the syntactic
sequences most likely to determine continuer insertion,
and within these, use the pause duration threshold to
filter out pauses that are more indicative of an end-of-
move boundary.
It is evident from the language model that pause
plays an important role in the prediction of continuers.
A quarter of all relevant trigrams consist of a part-of-
speech tag followed by a pause. This figure includes
the most frequent trigrams and those with the highest
</bodyText>
<figure confidence="0.994951833333333">
01111116
1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0
Duration in seconds
4000
3500
3000
t• 2500
2000
1500
1000
500
0
</figure>
<page confidence="0.992155">
55
</page>
<table confidence="0.9997818">
&gt; 0.6s &gt; 0.9s
three ten three ten
precision 27% 20% 29% 23%
recall 38% 60% 33% 51%
F-measure 32% 30% 31% 32%
</table>
<tableCaption confidence="0.999929">
Table 4: Comparison of Combined Models
</tableCaption>
<bodyText confidence="0.999948875">
probabilities. Moreover the trigrams that predict con-
tinuers are also good predictors of end of move. Us-
ing a specified threshold the pause duration model fil-
ters out the pauses that are most likely to occur before
the end of a move.It could therefore be supposed that
combining both the trigram model and the pause du-
ration model should improve the precision and recall.
Since this would effectively be cutting out a number of
the pauses, a smaller pause duration might be prefer-
able as the higher coverage would compensate for the
more concentrated search area. Another way of coun-
terbalancing this effect could be to use the Ten Trigram
model, which would increase the number of pauses to
which the threshold rule could be applied. A number
of combination models were built using both the Top
Three Trigram and the Top Ten Trigram models and
a pause threshold duration of 500-100ms inclusively.
The graphs in Figure 4 show the precision, recall and
F-measure results for all the boost models. Graphs A
and B demonstrate that the Three Trigram model had
consistently higher precision and lower recall scores.
Graph C shows that the F-measure values for the Three
Trigram models are higher than the Ten Trigram mod-
els for the lower threshold values. The values cross
at a threshold of 0.7 seconds, after which the Ten Tri-
gram model has the highest F-measure. Finding the
ideal compromise between the parameters is difficult
to achieve automatically. The F-measure for the Three
Trigram model at a threshold of 600 milliseconds is
identical to that of the Ten Trigram model at thresholds
of 800, 900 and 1000 milliseconds. Using the Ten Tri-
gram model provides the best precision, but the Three
Trigram model has a higher recall. For both models the
600 ms threshold has the highest recall, and 900ms the
highest precision.
A comparison of these two thresholds can be seen
in Table 4. Without carrying out a human evaluation
of these models it would be hard to decide between a
Three Trigram model with a pause threshold of 600ms
and a Ten Trigram model with a threshold of 900ms.
</bodyText>
<sectionHeader confidence="0.996051" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999462">
The best possible evaluation method, given our aim of
low-cost technological improvement, would be to test
the acceptability of a dialogue system before and after
</bodyText>
<figureCaption confidence="0.717431">
Figure 4: Comparison of Parameters for the Combined
Method
</figureCaption>
<table confidence="0.370096">
--+— Three
- - * - - Ten
--+— Three
- - * - - Ten
0.5 0.6 0.7 0.8 0.9 1.0
Pause Cut-off Point (secs)
F-Measure
— — Three
- - - - Ten
0.5 0.6 0.7 0.8 0.9 1.0
Pause Cut-off Point (secs)
</table>
<bodyText confidence="0.9990286875">
our models have been incorporated. A potential sec-
ond best option, having humans judge the naturalness
of the models&apos; results independent from a dialogue sys-
tem, is problematic. Conversational naturalness must
be judged in a reasonable amount of left and right-
hand context. We could doctor a conversation by ex-
cising the real follower&apos;s backchannel continuers and
re-inserting randomly selected ones where each model
predicts, but the results would be judged unnatural be-
cause of the knock-on effects on subsequent utterances.
A speaker&apos;s timings differ depending on whether or
not his partner produces a backchannel, and it is dif-
ficult to test system insertion of a backchannel where
the follower actually produces a more substantive ut-
terance. Thus we have chosen the less explanatory but
time-honoured evaluation method of comparing the be-
</bodyText>
<figure confidence="0.997929653846154">
Precision
35
&lt;1.)&apos;
A 30-
25
:12) 20 -
15
— -4- — -0
- -
••
0.5 0.6 0.7 0.8 0.9 1.0
Pause Cut-off Point (secs)
Recall
Percentage Values
65
55 -
45 -
35 -
25
34
32
Ia-
U.&apos; 30-
c, 28-
26
\
</figure>
<page confidence="0.7735">
56
</page>
<table confidence="0.747465">
Precision 39%
Recall 36%
F-measure 37%
</table>
<tableCaption confidence="0.937269">
Table 6: Results of the best model on high backchannel
rate data
</tableCaption>
<bodyText confidence="0.999651161290323">
haviour of our models to what the humans in the corpus
do.
One difficulty with evaluating a model such as
ours is that human speakers differ markedly in their
own backchanneling behaviour. As Ward and Tsuka-
hara (2000) remark, &amp;quot;a rule can predict opportunities,
but respondents do not choose to produce back-channel
feedback at every opportunity&amp;quot;. Because we cannot
identify the opportunities that humans pass up, we do
the second best thing: cite results both in general and
for a relatively high level of backchannel in the corpus.
Our reasoning here is that the more backchannels an
individual produces, the fewer opportunities they are
likely to have passed up.
The models were run on previously unseen test data,
the results of which can be seen in Table 5. All models
improved on the training models. The baseline model
was the worst performer with an F-measure of only 7%.
The trigram part-of-speech model and the pause dura-
tion models had very similar results, with the pause du-
ration model proving to be a slightly better predictor.
The combined model improved the F-measure and im-
portantly the precision. The best model was a five-fold
improvement over the baseline.
If we now modify our test set so that it repro-
duces the behaviour of a speaker with a higher rate
of backchannel, we see signficantly improved results.
Thus, running the model on the dialogue containing
eighty backchannel continuers gives a much higher pre-
cision rate, improving upon the best model by 10% as
can be seen in Table 6.
</bodyText>
<subsectionHeader confidence="0.670531">
5.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999984784313726">
A number of cases turn up as errors in this evaluation
which would not affect the performance of a dialogue
system using the model to produce backchannel con-
tinuers.
First, the model sometimes posits a backchannel
continuer when the route follower actually produces
something that has the same effect, but is more sub-
stantive (such as a repetition of some of the giver&apos;s con-
tent). Although the follower&apos;s actual utterance provides
better evidence of grounding than the system&apos;s simple
one, modelling the choice of which type of grounding
response to produce would be rather tricky for what is
likely to be little performance gain.
Second, the model sometimes posits a backchannel
continuer when the route follower produces a more
substantive, content-ful move. This can be when the
follower is not happy for the dialogue to move on, or it
can be when the giver has just asked as a question. Of
course, a dialogue system using our model would be
able to catch these cases because it would know when
it wishes to speak, even though by itself, our simple
model does not.
Third, a pause was said to contain a backchannel
continuer only if the backchannel started or ended
within the pause. Instances where the backchannel
started slightly before the pause would give the trigram
POS &lt;bc&gt; &lt;pau&gt;. This particular trigram would
not have been found by the language model; after a
backchannel backing-off was applied, forcing the lan-
guage model to count the pause as a unigram. However,
after missing this location, the model might well place
a backchannel slightly later, during the pause. Chang-
ing the location of a backchannel by 500ms does not
affect whether or not it was perceived as natural (Ward
and Tsukahara, 2000). Thus our evaluation technique
overrepresents these misses.
Finally, some of the cases that show up as errors
in the evaluation are correct, but the dialogue move
coding from which we derived the actual locations of
backchannel continuers is not. There is a systematic
confusion in our move system between pre-initiating
ready moves and acknowledgments (Carletta et al.,
1997). These moves share the same realizations, so
coders often disagree on which of the two labels to use,
especially for the acknowledgments that lack content
words and therefore which we counted as backchannel
continuers. Even if one accepts the theoretical distinc-
tion, a system&apos;s behaviour would be perceived as cor-
rect if it were to produce something that sounds like a
pre-initiator at the same location as a human one, no
matter what the system thinks it is.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999856733333333">
In general there has been very little work carried out on
building systems that are capable of placing backchan-
nels. In this paper, we investigated various methods
of predicting the placement of backchannel continuers,
using only limited processing and information that is
readily available to current spoken dialogue systems.
Pause duration and a statistical part-of-speech language
model were examined A method combining these two
models achieved the best F-measure of 35% and im-
proved on the baseline five-fold. The best previous sys-
tem (Ward and Tsukahara, 2000) used as its sole pre-
dictor regions of low pitch and produced an accuracy
of 18% for English.
While our results may not be comparable to other
work carried out in the field of natural language pro-
</bodyText>
<page confidence="0.997235">
57
</page>
<table confidence="0.9996544">
Baseline Trigram Pause Combined
10 Tri +&gt; .9s 3 Tri +&gt; .6s
Precision 4% 22% 22% 25% 29%
Recall 13% 50% 58% 51% 43%
F-measure 7% 30% 32% 33% 35%
</table>
<tableCaption confidence="0.999745">
Table 5: Results of the Models on the Test Data
</tableCaption>
<bodyText confidence="0.999951384615385">
cessing, where scores of 90% or above are not uncom-
mon for tasks such as part-of-speech tagging and sta-
tistical parsing, this can be at least partly explained by
the fact that humans vary widely in how many of their
opportunities for placing a backchannel continuer they
actually realize. Our model could potentially be im-
proved by adding words to parts-of-speech in the lan-
guage model; Ward and Tsukahara (2000) suggest that
about half the occurrences of backchannel are elicited
by speaker-produced cues. Beyond this, improvements
may well require changes to the history that a dialogue
system keeps, together with the addition of prosodic
information.
</bodyText>
<sectionHeader confidence="0.997412" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999892364864865">
A. H. Anderson, M. Bader, Bard E. G., E. Boyle, G. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,
J. Miller, C. Sotillo, H. Thompson, and R. Weinert. 1991.
The HCRC Map Task Corpus. Language and Speech,
34(4):351-366.
M. C. Bull and M. P. Aylett. 1998. An analysis of the tim-
ing of turn-taking in a corpus of goal-orientated dialogue.
In R. H. Mannell and J. Robert-Ribes, editors, Proceed-
ings of ICSLP-98, volume 4, pages 1175-1178, Sydney,
Australia. Australian Speech Science and Technology As-
sociation (ASSTA).
J. Carletta, A. Isard, S. Isard, J. Kowtko, G. Doherty-
Sneddon, and A. Anderson. 1997. The reliability of a di-
alogue structure coding scheme. Computational Linguis-
tics, 23:13-31.
H. Clark and E. Schaefer. 1991. Contributing to discourse.
Cognitive Science, 13:259-294.
R. Denny. 1985. Pragmatically marked and unmarked forms
of speaking-turn exchange. In S. Duncan and D. Fiske, ed-
itors, Interaction Structure and Strategy, pages 135-172.
Cambridge University Press.
J. Du Bois, S. Schuetze-Coburn, D. Paolino, and S. Cum-
ming. 1993. Outline of discourse transcription. In J. Ed-
wards and M. Lampert, editors, Talking Data: Transcrip-
tion and Coding Methods for Language Research. Hills-
dale.
C. Ford and S. Thompson. 1996. Interactional units in con-
versation: syntactic, intonational and pragmatic resources
for the management of turns. In E. Ochs, E. A. Schegloff,
and S. A. Thompson, editors, Interaction and Grammar,
chapter 3. CUP, Cambridge.
J. Hirasawa, M. Nakano, T. Kawabata, and K. Aikawa. 1999.
Effects of system barge-in responses on user impressions.
In Sixth European Conference on Speech Communication
and Technology, volume 3, pages 1391-1394.
D. Jurafsky, E. Shriberg, B. Fox, and T. Curl. 1998. Lex-
ical, prosodic, and syntactic cues for dialog acts. In
ACL/COLING-98 Workshop on Discourse Relations and
Discourse Markers.
G. Knowles, A. Wichmann, and P. Alderson, editors. 1996.
Working with Speech: Perspectives on Research into the
Lancaster/IBM Spoken English Corpus. Longman.
H. Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, and Y. Den.
1998. An analysis of turn-taking and backchannels. lan-
guage and Speech, 23:296-321.
S. Lamia, G. Bugmann, T. Kyriacou, J. Bos, and E. Klein.
2001. Training personal robots using natural language in-
struction. IEEE Intelligent Systems, 16(3):38-45.
L. Lemon, A. Gruenstein, and S. Peters. 2002. Collaborative
activities and multi-tasking in dialogue systems. Traite-
ment Automatique des Langues, 43(2): 131-154.
W. J. M. Levelt. 1998. Speaking: From Intention to Articu-
lation. MIT Press, Boston, MA.
D McKelvie. 2001. Part of speech tag set used for MT cor-
pus. Technical report, HCRC. Available from www. it g
ed.ac.uk/-amyi/maptask/mt-tag-set.ps.
H. Sacks, E.A. Schegloff, and G. Jefferson. 1974. A simplest
systematics for the organization of turn taking for conver-
sation. Language, 50(4), pages 696-735.
C. Theobalt, J. Bos, T. Chapman, A. Espinosa-Romero,
M. Fraser, G. Hayes, E. Klein, T. Oka, and R. Reeve.
2002. Talking to Godot: Dialogue with a mobile robot.
In Proceedings of IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS 2002), pages 1338-
1343.
N. Ward and W. Tsukahara. 2000. Prosodic features which
cue back-channel responses in English and Japanese.
Journal of Pragmatics, 23:1177-1207.
N. Yankelovich, G-A. Levow, and M. Marx. 1995. Design-
ing SpeechActs: Issues in speech user interfaces. In CHI
Conference on Human Factors in Computing Systems.
V.H. Yngve. 1970. On getting a word in edgewise. In Pa-
pers from the Sixth Regional Meeting, Chicago Linguistic
Society, pages 567-577.
</reference>
<page confidence="0.999258">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933780">
<title confidence="0.998568">A Shallow Model of Backchannel Continuers in Spoken Dialogue</title>
<author confidence="0.999494">Cathcart Jean Carletta Klein</author>
<affiliation confidence="0.9991755">Canon Research Centre Europe School of Informatics University of Edinburgh</affiliation>
<email confidence="0.957764">nicolac@cre.canon.co.uk{jeanc,ewan}@inf.ed.ac.uk</email>
<abstract confidence="0.998825052631579">Spoken dialogue systems would be more acceptable if they were able to produce continuers such as naturalistic locations during the user&apos;s utterances. Using the HCRC Map Task Corpus as our data source, we describe models for predicting these locations using only limited processing and features of the user&apos;s speech that are commonly available, and which therefore could be used as a lowcost improvement for current systems. The baseline model inserts continuers after a predetermined number of words. One further model correlates back-channel continuers with pause duration, while a second predicts their occurrence using trigram POS frequencies. Combining these two models gives the best results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A H Anderson</author>
<author>M Bader</author>
<author>E G Bard</author>
<author>E Boyle</author>
<author>G Doherty</author>
<author>S Garrod</author>
<author>S Isard</author>
<author>J Kowtko</author>
<author>J McAllister</author>
<author>J Miller</author>
<author>C Sotillo</author>
<author>H Thompson</author>
<author>R Weinert</author>
</authors>
<title>The HCRC Map Task Corpus. Language and Speech,</title>
<date>1991</date>
<pages>34--4</pages>
<contexts>
<context position="9645" citStr="Anderson et al., 1991" startWordPosition="1561" endWordPosition="1564">ed in our model. For example, FO contour would clearly be useful in predicting backchannel location. However, the challenge of extracting appropriate prosodic features from a pitch tracker lay outside the scope of the research effort reported here. Moreover, the multimodal features considered by Denny seemed too far from the current stateof-the-art in speech recognition systems to be of immediate practical interest. Therefore, for this work, we restrict ourselves to pause duration and part-of-speech tag sequences as inputs. 3 Corpus Analysis For our modelling, we use the HCRC Map Task Corpus (Anderson et al., 1991),4 a set of 128 task-oriented dialogues between human speakers of Scottish English, lasting six minutes on average. In half of the conversations the participants could see each others&apos; faces; in the other half, this was prevented by a screen. We ignore this distinction, combining data from the two conditions. Although participants must cooperate to complete the task, their roles are somewhat unbalanced, with one participant, the &amp;quot;instruction Giver&amp;quot;, dominating their planning For this reason, all of our analysis considers where the &amp;quot;instruction Follower&amp;quot; produces backchannel continuers in relat</context>
</contexts>
<marker>Anderson, Bader, Bard, Boyle, Doherty, Garrod, Isard, Kowtko, McAllister, Miller, Sotillo, Thompson, Weinert, 1991</marker>
<rawString>A. H. Anderson, M. Bader, Bard E. G., E. Boyle, G. Doherty, S. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller, C. Sotillo, H. Thompson, and R. Weinert. 1991. The HCRC Map Task Corpus. Language and Speech, 34(4):351-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C Bull</author>
<author>M P Aylett</author>
</authors>
<title>An analysis of the timing of turn-taking in a corpus of goal-orientated dialogue.</title>
<date>1998</date>
<booktitle>Proceedings of ICSLP-98,</booktitle>
<volume>4</volume>
<pages>1175--1178</pages>
<editor>In R. H. Mannell and J. Robert-Ribes, editors,</editor>
<location>Sydney, Australia.</location>
<contexts>
<context position="8682" citStr="Bull and Aylett, 1998" startWordPosition="1405" endWordPosition="1408">inserting them wherever the other speaker produced a region of low pitch lasting 110ms. This model is motivated by the observation that such regions often accompany grammatical completion. Their model achieved 18% 2The identification of long pauses with TRPs, although understandable in the context of informing work on spoken dialogue systems, is somewhat at odds with previous thinking about turn-taking. Although turn-taking behaviour is culturally dependent , human dialogue is generally considered remarkable for how little silence there can be between turns. A previous study of Map Task data (Bull and Aylett, 1998), bears up Sacks, Schegloff and Jefferson&apos;s original (1974) observation that turns often latch, with no perceivable silent gap, or that they even overlap. 52 accuracy for English and 34% for Japanese.3 Although none of these studies is performing exactly the same task as we are, they jointly suggest a range of features that could be included in our model. For example, FO contour would clearly be useful in predicting backchannel location. However, the challenge of extracting appropriate prosodic features from a pitch tracker lay outside the scope of the research effort reported here. Moreover, </context>
</contexts>
<marker>Bull, Aylett, 1998</marker>
<rawString>M. C. Bull and M. P. Aylett. 1998. An analysis of the timing of turn-taking in a corpus of goal-orientated dialogue. In R. H. Mannell and J. Robert-Ribes, editors, Proceedings of ICSLP-98, volume 4, pages 1175-1178, Sydney, Australia. Australian Speech Science and Technology Association (ASSTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>A Isard</author>
<author>S Isard</author>
<author>J Kowtko</author>
<author>G DohertySneddon</author>
<author>A Anderson</author>
</authors>
<title>The reliability of a dialogue structure coding scheme.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--13</pages>
<contexts>
<context position="10903" citStr="Carletta et al., 1997" startWordPosition="1760" endWordPosition="1763">h. At the most basic level, a Map Task dialogue represents each participant&apos;s behaviour separately as a sequence of time-stamped silences, noises (such as coughing), and speech tokens, to which part-of-speech tagging has been applied. The part-of-speech tag set is based on a version of the Brown Corpus tag set which was modified slightly to better accommodate the corpus ((McKelvie, 2001)). These together allow us to calculate our input features. We identify Giver TRPs using existing dialogue structure coding. The Map Task Corpus has been segmented by hand into dialogue moves, as described in (Carletta et al., 1997). With the exception of moves in the &amp;quot;acknowledge&amp;quot;, &amp;quot;ready&amp;quot;, and &amp;quot;align&amp;quot; categories, each move represents one utterance that is either pragmatically complete or, rarely, abandoned. In this system, a ready move is essentially a discourse marker that pre-initiates some larger move, usually an instruction 3Their paper does not specify how these figures are to be interpreted in terms of precision and recall. 4The transcriptions and coding for the Map Task Corpus are available from http: //www.hcrc.ed.ac.uk/ dialogue/maptask.html. Acknowledgement Frequency % of Total right 1226 29% okay 587 14% mm-</context>
<context position="28554" citStr="Carletta et al., 1997" startWordPosition="4828" endWordPosition="4831">owever, after missing this location, the model might well place a backchannel slightly later, during the pause. Changing the location of a backchannel by 500ms does not affect whether or not it was perceived as natural (Ward and Tsukahara, 2000). Thus our evaluation technique overrepresents these misses. Finally, some of the cases that show up as errors in the evaluation are correct, but the dialogue move coding from which we derived the actual locations of backchannel continuers is not. There is a systematic confusion in our move system between pre-initiating ready moves and acknowledgments (Carletta et al., 1997). These moves share the same realizations, so coders often disagree on which of the two labels to use, especially for the acknowledgments that lack content words and therefore which we counted as backchannel continuers. Even if one accepts the theoretical distinction, a system&apos;s behaviour would be perceived as correct if it were to produce something that sounds like a pre-initiator at the same location as a human one, no matter what the system thinks it is. 6 Conclusion In general there has been very little work carried out on building systems that are capable of placing backchannels. In this </context>
</contexts>
<marker>Carletta, Isard, Isard, Kowtko, DohertySneddon, Anderson, 1997</marker>
<rawString>J. Carletta, A. Isard, S. Isard, J. Kowtko, G. DohertySneddon, and A. Anderson. 1997. The reliability of a dialogue structure coding scheme. Computational Linguistics, 23:13-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Clark</author>
<author>E Schaefer</author>
</authors>
<title>Contributing to discourse.</title>
<date>1991</date>
<journal>Cognitive Science,</journal>
<pages>13--259</pages>
<contexts>
<context position="1462" citStr="Clark and Schaefer, 1991" startWordPosition="221" endWordPosition="224">use duration, while a second predicts their occurrence using trigram POS frequencies. Combining these two models gives the best results. 1 Introduction In a spoken dialogue between people, the participants use simple utterances such as yeah, a totty wee bit aye and mm-hnint to signal that communication is working. Without this feedback, the partner may assume that he has not been understood and reformulate his utterance. Following Yngve (1970), we will use the term backchannel for such utterances. Although these can be substantive because they can repeat material from the partner&apos;s utterance (Clark and Schaefer, 1991), e.g., Right, okay, I&apos;m below the fiat rocks, we will adopt (Jorafsky et al., 1998)&apos;s terminology of continuer. We will take this to refer to the class of backchannel utterances, with minimal content, used to clearly signal that the speaker should continue with her current turn. (Yankelovich et al., 1995) point out that users of speech interface systems need feedback, too, especially since the system&apos;s silence could mean either of two very different things: that it is waiting for user input, in which case the user should speak, or that it is still processing information, in which case the use</context>
<context position="12259" citStr="Clark and Schaefer (1991)" startWordPosition="1991" endWordPosition="1994">ency of Acknowledgements (as in OK, go to the left of the swamp...), and an align move is usually added to the end of a move to elicit explicit feedback from the partner (as in, Go to the left of the swamp, OK?). We treat move boundaries as TRPs in our processing, ignoring the two exceptions above which consist predominantly of one-word moves. Failure to remove them affects only our baseline model. The acknowledge move was used to locate backchannel continuers. In this system, all backchannel continuers are acknowledge moves, but not all acknowledge moves are backchannel continuers; following Clark and Schaefer (1991), they include somewhat more substantive ways of moving the conversation forward, such as paraphrasing the speaker&apos;s utterance repeating part or all of it verbatim, or accepting its contents. To identify the instruction Follower&apos;s backchannel continuers, we filtered the list of their acknowledge moves by removing any that contained content words or words that generally convey acceptance such as alright. Table 1 gives the most frequent forms of backchannel continuers resulting from this process, which differ somewhat from Jurafsky et al.&apos;s (1998) analysis of American speech. 4 Description of Mo</context>
</contexts>
<marker>Clark, Schaefer, 1991</marker>
<rawString>H. Clark and E. Schaefer. 1991. Contributing to discourse. Cognitive Science, 13:259-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Denny</author>
</authors>
<title>Pragmatically marked and unmarked forms of speaking-turn exchange.</title>
<date>1985</date>
<booktitle>Interaction Structure and Strategy,</booktitle>
<pages>135--172</pages>
<editor>In S. Duncan and D. Fiske, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6472" citStr="Denny (1985)" startWordPosition="1056" endWordPosition="1058">ecessary but not sufficient to make a cTRP. Thus linguistic theory suggests that knowing where to find TRPs will help one know where to place backchannel continuers, and that pragmatics, grammar and intonation are all useful cues. In addition to this theorizing, there have been a number of previous corpus-based studies that have attempted to describe or model the location of backchannel continuers, TRPs, and turn exchanges, by reference to the preceding context. These have tended to concentrate on easy-to-measure phenomena that clearly relate to grammatical and intonational completion: part-ofDenny (1985) was concerned with describing the preceding context of only those turn exchanges at which there were pauses of over 65ms, and particularly those at which backchannel continuers occurred. In her description, she considered pitch rise and fall, speaker and auditor gaze, gesture, -filled pauses&amp;quot; such as mm-hmm, and grammatical completion. Koiso et al. (1998), working in a Japanese replication of the same corpus on which our results are based, used all pauses over 100ms as an operational definition of when turn exchange is possible — that is, of TRPs — and considered predictors of whether or not </context>
</contexts>
<marker>Denny, 1985</marker>
<rawString>R. Denny. 1985. Pragmatically marked and unmarked forms of speaking-turn exchange. In S. Duncan and D. Fiske, editors, Interaction Structure and Strategy, pages 135-172. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Du Bois</author>
<author>S Schuetze-Coburn</author>
<author>D Paolino</author>
<author>S Cumming</author>
</authors>
<title>Outline of discourse transcription.</title>
<date>1993</date>
<booktitle>Talking Data: Transcription and Coding Methods for Language Research.</booktitle>
<editor>In J. Edwards and M. Lampert, editors,</editor>
<location>Hillsdale.</location>
<contexts>
<context position="4280" citStr="Bois et al., 1993" startWordPosition="696" endWordPosition="699">h n-grams, pitch, and FO contour in the immedioccur? ately preceding context, and pauses at the location itself. There are two literatures we can draw on to inspire our model: linguistic theory that predicts where backchannels will occur because of the purpose they serve, and past corpus-based attempts to model backchannel locations. Theoretically, backchannel continuers will be most interpretable by the speaker if they occur at or before an utterance reaches a pragmatic completion— that is, where a segment is &amp;quot;interpretable as a complete conversational action within its specific context&amp;quot; (Du Bois et al., 1993)(p. 147) — but not too early in the utterance. This is because planning the content of an utterance, formulating it, articulating it, and monitoring the partner&apos;s understanding are all parallel processes, with monitoring kicking in when planning ends (Levelt, 1998). Classically, pragmatic completions yield transition relevance places, or TRPs for short, where the current hearer can take over the main channel of communication by taking a turn (Sacks et al., 1974), for instance, to clear up something that he does not understand. If the current hearer chooses to take over, then a &amp;quot;turn exchange&amp;quot; </context>
</contexts>
<marker>Bois, Schuetze-Coburn, Paolino, Cumming, 1993</marker>
<rawString>J. Du Bois, S. Schuetze-Coburn, D. Paolino, and S. Cumming. 1993. Outline of discourse transcription. In J. Edwards and M. Lampert, editors, Talking Data: Transcription and Coding Methods for Language Research. Hillsdale.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ford</author>
<author>S Thompson</author>
</authors>
<title>Interactional units in conversation: syntactic, intonational and pragmatic resources for the management of turns. In</title>
<date>1996</date>
<booktitle>Interaction and Grammar, chapter 3. CUP,</booktitle>
<editor>E. Ochs, E. A. Schegloff, and S. A. Thompson, editors,</editor>
<location>Cambridge.</location>
<contexts>
<context position="5470" citStr="Ford and Thompson (1996)" startWordPosition="903" endWordPosition="907">take over, then a &amp;quot;turn exchange&amp;quot; is said to occur. If the current hearer chooses not to take over, instead remaining passive or giving feedback through, e.g., a nod, grimace, or backchannel continuer, then the speaker must decide whether to go back or go on. Of course, it is possible for the hearer first to give feedback and subsequently to decide to take a turn. So we would expect speakers to be able to receive backchannel continuers at TRPs, especially when they do not lead to turn exchange, or before TRPs in, say, the second half of their utterance. In their updating of the classic model, Ford and Thompson (1996)(p. 144) describe &amp;quot;complex transition relevance points (cTRPs)&amp;quot; as confluences where intention, intonation, and grammatical structure are all complete. For them, an utterance is grammatically complete if it &amp;quot;could be interpreted as a complete clause ... with an overt or directly recoverable predicate&amp;quot;. Since speakers can always add phrases after the predicate, grammatical completion is necessary but not sufficient to make a cTRP. Thus linguistic theory suggests that knowing where to find TRPs will help one know where to place backchannel continuers, and that pragmatics, grammar and intonation </context>
</contexts>
<marker>Ford, Thompson, 1996</marker>
<rawString>C. Ford and S. Thompson. 1996. Interactional units in conversation: syntactic, intonational and pragmatic resources for the management of turns. In E. Ochs, E. A. Schegloff, and S. A. Thompson, editors, Interaction and Grammar, chapter 3. CUP, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirasawa</author>
<author>M Nakano</author>
<author>T Kawabata</author>
<author>K Aikawa</author>
</authors>
<title>Effects of system barge-in responses on user impressions.</title>
<date>1999</date>
<booktitle>In Sixth European Conference on Speech Communication and Technology,</booktitle>
<volume>3</volume>
<pages>1391--1394</pages>
<contexts>
<context position="2230" citStr="Hirasawa et al., 1999" startWordPosition="352" endWordPosition="355"> class of backchannel utterances, with minimal content, used to clearly signal that the speaker should continue with her current turn. (Yankelovich et al., 1995) point out that users of speech interface systems need feedback, too, especially since the system&apos;s silence could mean either of two very different things: that it is waiting for user input, in which case the user should speak, or that it is still processing information, in which case the user should not. However, any feedback must come at the right time or else it risks disrupting the speaker and ultimately, delaying task completion (Hirasawa et al., 1999). Most of our data, including the examples given above, are drawn from the HCRC Map Task Corpus, described in more detail in Section 3. Clearly these dialogues are significantly more complex than the kind of interactions supported by current commercial spoken dialogue systems, where the length of user utterances is severely constrained. What kind of system would involve potentially lengthy user instructions comparable to those found in the Map Task? Lauria et al. (2001), Lemon et al. (2002), and Theobalt et al. (2002) describe work on building spoken dialogue systems for conversing with mobile</context>
</contexts>
<marker>Hirasawa, Nakano, Kawabata, Aikawa, 1999</marker>
<rawString>J. Hirasawa, M. Nakano, T. Kawabata, and K. Aikawa. 1999. Effects of system barge-in responses on user impressions. In Sixth European Conference on Speech Communication and Technology, volume 3, pages 1391-1394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>E Shriberg</author>
<author>B Fox</author>
<author>T Curl</author>
</authors>
<title>Lexical, prosodic, and syntactic cues for dialog acts.</title>
<date>1998</date>
<booktitle>In ACL/COLING-98 Workshop on Discourse Relations and Discourse Markers.</booktitle>
<marker>Jurafsky, Shriberg, Fox, Curl, 1998</marker>
<rawString>D. Jurafsky, E. Shriberg, B. Fox, and T. Curl. 1998. Lexical, prosodic, and syntactic cues for dialog acts. In ACL/COLING-98 Workshop on Discourse Relations and Discourse Markers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Knowles</author>
<author>A Wichmann</author>
<author>P Alderson</author>
<author>editors</author>
</authors>
<date>1996</date>
<booktitle>Working with Speech: Perspectives on Research into the Lancaster/IBM Spoken English</booktitle>
<publisher>Corpus. Longman.</publisher>
<contexts>
<context position="13427" citStr="Knowles et al., 1996" startWordPosition="2181" endWordPosition="2184">998) analysis of American speech. 4 Description of Models 4.1 Baseline Model For our baseline model, we planned to insert a backchannel continuer after every n words, for some plausible value of n. This seemed to be the simplest choice in its own right. However, the choice can also be justified as follows. We expect backchannel continuers to be placed at or before intonational phrase boundaries, since these are a primary indicator for TRPs. Spotting these boundaries requires a pitch tracker, but in at least one corpus of spoken English, they are known to occur every five to fifteen syllables (Knowles et al., 1996). We decided to approximate syllables by words. Thus, from each of our Follower backchannels, 53 we can measure the number of words back to the last Follower backchannel continuer, or Giver TRP, as determined by move boundaries. Figure 1 shows the resulting frequency distribution for the number of Giver words between Follower backchannel continuers. 111111111111111,„.„...... 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 47 56 Number of Words Figure 1: The Number of Giver Words between a Move Boundary and a Backchannel Continuer In addition to the inclusion of the &amp;quot;ready&amp;quot; and &amp;quot;align&amp;quot; categories (dis</context>
</contexts>
<marker>Knowles, Wichmann, Alderson, editors, 1996</marker>
<rawString>G. Knowles, A. Wichmann, and P. Alderson, editors. 1996. Working with Speech: Perspectives on Research into the Lancaster/IBM Spoken English Corpus. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Koiso</author>
<author>Y Horiuchi</author>
<author>S Tutiya</author>
<author>A Ichikawa</author>
<author>Y Den</author>
</authors>
<title>An analysis of turn-taking and backchannels. language and Speech,</title>
<date>1998</date>
<pages>23--296</pages>
<contexts>
<context position="6830" citStr="Koiso et al. (1998)" startWordPosition="1113" endWordPosition="1116">e or model the location of backchannel continuers, TRPs, and turn exchanges, by reference to the preceding context. These have tended to concentrate on easy-to-measure phenomena that clearly relate to grammatical and intonational completion: part-ofDenny (1985) was concerned with describing the preceding context of only those turn exchanges at which there were pauses of over 65ms, and particularly those at which backchannel continuers occurred. In her description, she considered pitch rise and fall, speaker and auditor gaze, gesture, -filled pauses&amp;quot; such as mm-hmm, and grammatical completion. Koiso et al. (1998), working in a Japanese replication of the same corpus on which our results are based, used all pauses over 100ms as an operational definition of when turn exchange is possible — that is, of TRPs — and considered predictors of whether or not turn exchange occurred at a TRP, and, when it did not, whether or not the hearer produced a backchannel continuer.2 They used as predictors the immediately preceding part-ofspeech plus prosodic features: duration of the final phoneme, FO contour, peak FO, energy pattern, and peak energy. They found that the best single predictor of either phenomena was the</context>
<context position="15722" citStr="Koiso et al., 1998" startWordPosition="2602" endWordPosition="2605">was produced by predicting a continuer at the mode frequency of every seven words. The score is only 6%. 4.2 Pause Duration Model Our next model is based simply on pause duration, working from the premise that backchannel continuers often occur at TRPs, and that TRPs often contain 25 - - - - - Precision — -• — Recall — F-measure &amp;quot; &apos;••• 5 - A- — — —A- — — — — —A 0 4 5 6 1 8 9 10 Number of Words Figure 2: Values for Number of Words Threshold Prec. Recall F-meas. 0.9 22 59 32 1.0 22 55 31 1.1 22 51 31 Table 2: Highest Performing Pause Duration Models pauses. As we explained in our discussion of (Koiso et al., 1998), this premise is common, but controversial. Figure 3 compares the durations of the 12% of instruction Giver pauses that overlap with Follower backchannel contributes with the durations of the majority that do not.5 Of course, a real-time system cannot wait to see exactly how a long a pause turns out to have been before deciding whether or not to produce a backchannel continuer. In our data, 50% of the pauses lacking backchannel continuers are less than 500ms; moreover, only 11% of pauses this short do attract continuers. For this reason, we apply a threshold; the model works by producing a ba</context>
</contexts>
<marker>Koiso, Horiuchi, Tutiya, Ichikawa, Den, 1998</marker>
<rawString>H. Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, and Y. Den. 1998. An analysis of turn-taking and backchannels. language and Speech, 23:296-321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lamia</author>
<author>G Bugmann</author>
<author>T Kyriacou</author>
<author>J Bos</author>
<author>E Klein</author>
</authors>
<title>Training personal robots using natural language instruction.</title>
<date>2001</date>
<journal>IEEE Intelligent Systems,</journal>
<pages>16--3</pages>
<marker>Lamia, Bugmann, Kyriacou, Bos, Klein, 2001</marker>
<rawString>S. Lamia, G. Bugmann, T. Kyriacou, J. Bos, and E. Klein. 2001. Training personal robots using natural language instruction. IEEE Intelligent Systems, 16(3):38-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lemon</author>
<author>A Gruenstein</author>
<author>S Peters</author>
</authors>
<title>Collaborative activities and multi-tasking in dialogue systems.</title>
<date>2002</date>
<journal>Traitement Automatique des Langues,</journal>
<volume>43</volume>
<issue>2</issue>
<pages>131--154</pages>
<contexts>
<context position="2725" citStr="Lemon et al. (2002)" startWordPosition="432" endWordPosition="435">e at the right time or else it risks disrupting the speaker and ultimately, delaying task completion (Hirasawa et al., 1999). Most of our data, including the examples given above, are drawn from the HCRC Map Task Corpus, described in more detail in Section 3. Clearly these dialogues are significantly more complex than the kind of interactions supported by current commercial spoken dialogue systems, where the length of user utterances is severely constrained. What kind of system would involve potentially lengthy user instructions comparable to those found in the Map Task? Lauria et al. (2001), Lemon et al. (2002), and Theobalt et al. (2002) describe work on building spoken dialogue systems for conversing with mobile robots, and this is a setting where complex instructions naturally arise. For example, in one scenario,1 users attempt to teach routes and route segments to a robot. (1) is a portion of such an instruction. (1) okay go to the end of the road and turn left and erm ... and then carry on down that road and then turn ... take your second left where the trees are on the corner We describe a shallow model, based on human dialogue data, for predicting where to place backchannel feedback. The mode</context>
</contexts>
<marker>Lemon, Gruenstein, Peters, 2002</marker>
<rawString>L. Lemon, A. Gruenstein, and S. Peters. 2002. Collaborative activities and multi-tasking in dialogue systems. Traitement Automatique des Langues, 43(2): 131-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
</authors>
<title>Speaking: From Intention to Articulation.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="4545" citStr="Levelt, 1998" startWordPosition="740" endWordPosition="742">e, and past corpus-based attempts to model backchannel locations. Theoretically, backchannel continuers will be most interpretable by the speaker if they occur at or before an utterance reaches a pragmatic completion— that is, where a segment is &amp;quot;interpretable as a complete conversational action within its specific context&amp;quot; (Du Bois et al., 1993)(p. 147) — but not too early in the utterance. This is because planning the content of an utterance, formulating it, articulating it, and monitoring the partner&apos;s understanding are all parallel processes, with monitoring kicking in when planning ends (Levelt, 1998). Classically, pragmatic completions yield transition relevance places, or TRPs for short, where the current hearer can take over the main channel of communication by taking a turn (Sacks et al., 1974), for instance, to clear up something that he does not understand. If the current hearer chooses to take over, then a &amp;quot;turn exchange&amp;quot; is said to occur. If the current hearer chooses not to take over, instead remaining passive or giving feedback through, e.g., a nod, grimace, or backchannel continuer, then the speaker must decide whether to go back or go on. Of course, it is possible for the heare</context>
</contexts>
<marker>Levelt, 1998</marker>
<rawString>W. J. M. Levelt. 1998. Speaking: From Intention to Articulation. MIT Press, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McKelvie</author>
</authors>
<title>Part of speech tag set used for MT corpus.</title>
<date>2001</date>
<booktitle>Technical report, HCRC. Available from www. it g</booktitle>
<pages>ed.ac.uk/-amyi/maptask/mt-tag-set.ps.</pages>
<contexts>
<context position="10671" citStr="McKelvie, 2001" startWordPosition="1724" endWordPosition="1725"> one participant, the &amp;quot;instruction Giver&amp;quot;, dominating their planning For this reason, all of our analysis considers where the &amp;quot;instruction Follower&amp;quot; produces backchannel continuers in relation to the instruction Giver&apos;s speech. At the most basic level, a Map Task dialogue represents each participant&apos;s behaviour separately as a sequence of time-stamped silences, noises (such as coughing), and speech tokens, to which part-of-speech tagging has been applied. The part-of-speech tag set is based on a version of the Brown Corpus tag set which was modified slightly to better accommodate the corpus ((McKelvie, 2001)). These together allow us to calculate our input features. We identify Giver TRPs using existing dialogue structure coding. The Map Task Corpus has been segmented by hand into dialogue moves, as described in (Carletta et al., 1997). With the exception of moves in the &amp;quot;acknowledge&amp;quot;, &amp;quot;ready&amp;quot;, and &amp;quot;align&amp;quot; categories, each move represents one utterance that is either pragmatically complete or, rarely, abandoned. In this system, a ready move is essentially a discourse marker that pre-initiates some larger move, usually an instruction 3Their paper does not specify how these figures are to be interp</context>
</contexts>
<marker>McKelvie, 2001</marker>
<rawString>D McKelvie. 2001. Part of speech tag set used for MT corpus. Technical report, HCRC. Available from www. it g ed.ac.uk/-amyi/maptask/mt-tag-set.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sacks</author>
<author>E A Schegloff</author>
<author>G Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turn taking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<pages>696--735</pages>
<contexts>
<context position="4746" citStr="Sacks et al., 1974" startWordPosition="771" endWordPosition="774"> pragmatic completion— that is, where a segment is &amp;quot;interpretable as a complete conversational action within its specific context&amp;quot; (Du Bois et al., 1993)(p. 147) — but not too early in the utterance. This is because planning the content of an utterance, formulating it, articulating it, and monitoring the partner&apos;s understanding are all parallel processes, with monitoring kicking in when planning ends (Levelt, 1998). Classically, pragmatic completions yield transition relevance places, or TRPs for short, where the current hearer can take over the main channel of communication by taking a turn (Sacks et al., 1974), for instance, to clear up something that he does not understand. If the current hearer chooses to take over, then a &amp;quot;turn exchange&amp;quot; is said to occur. If the current hearer chooses not to take over, instead remaining passive or giving feedback through, e.g., a nod, grimace, or backchannel continuer, then the speaker must decide whether to go back or go on. Of course, it is possible for the hearer first to give feedback and subsequently to decide to take a turn. So we would expect speakers to be able to receive backchannel continuers at TRPs, especially when they do not lead to turn exchange, </context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>H. Sacks, E.A. Schegloff, and G. Jefferson. 1974. A simplest systematics for the organization of turn taking for conversation. Language, 50(4), pages 696-735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Theobalt</author>
<author>J Bos</author>
<author>T Chapman</author>
<author>A Espinosa-Romero</author>
<author>M Fraser</author>
<author>G Hayes</author>
<author>E Klein</author>
<author>T Oka</author>
<author>R Reeve</author>
</authors>
<title>Talking to Godot: Dialogue with a mobile robot.</title>
<date>2002</date>
<booktitle>In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS</booktitle>
<pages>1338--1343</pages>
<contexts>
<context position="2753" citStr="Theobalt et al. (2002)" startWordPosition="437" endWordPosition="440">se it risks disrupting the speaker and ultimately, delaying task completion (Hirasawa et al., 1999). Most of our data, including the examples given above, are drawn from the HCRC Map Task Corpus, described in more detail in Section 3. Clearly these dialogues are significantly more complex than the kind of interactions supported by current commercial spoken dialogue systems, where the length of user utterances is severely constrained. What kind of system would involve potentially lengthy user instructions comparable to those found in the Map Task? Lauria et al. (2001), Lemon et al. (2002), and Theobalt et al. (2002) describe work on building spoken dialogue systems for conversing with mobile robots, and this is a setting where complex instructions naturally arise. For example, in one scenario,1 users attempt to teach routes and route segments to a robot. (1) is a portion of such an instruction. (1) okay go to the end of the road and turn left and erm ... and then carry on down that road and then turn ... take your second left where the trees are on the corner We describe a shallow model, based on human dialogue data, for predicting where to place backchannel feedback. The model deliberately requires only</context>
</contexts>
<marker>Theobalt, Bos, Chapman, Espinosa-Romero, Fraser, Hayes, Klein, Oka, Reeve, 2002</marker>
<rawString>C. Theobalt, J. Bos, T. Chapman, A. Espinosa-Romero, M. Fraser, G. Hayes, E. Klein, T. Oka, and R. Reeve. 2002. Talking to Godot: Dialogue with a mobile robot. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2002), pages 1338-1343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
<author>W Tsukahara</author>
</authors>
<title>Prosodic features which cue back-channel responses in English and Japanese.</title>
<date>2000</date>
<journal>Journal of Pragmatics,</journal>
<pages>23--1177</pages>
<contexts>
<context position="25151" citStr="Ward and Tsukahara (2000)" startWordPosition="4258" endWordPosition="4262">bstantive utterance. Thus we have chosen the less explanatory but time-honoured evaluation method of comparing the bePrecision 35 &lt;1.)&apos; A 30- 25 :12) 20 - 15 — -4- — -0 - - •• 0.5 0.6 0.7 0.8 0.9 1.0 Pause Cut-off Point (secs) Recall Percentage Values 65 55 - 45 - 35 - 25 34 32 IaU.&apos; 30- c, 28- 26 \ 56 Precision 39% Recall 36% F-measure 37% Table 6: Results of the best model on high backchannel rate data haviour of our models to what the humans in the corpus do. One difficulty with evaluating a model such as ours is that human speakers differ markedly in their own backchanneling behaviour. As Ward and Tsukahara (2000) remark, &amp;quot;a rule can predict opportunities, but respondents do not choose to produce back-channel feedback at every opportunity&amp;quot;. Because we cannot identify the opportunities that humans pass up, we do the second best thing: cite results both in general and for a relatively high level of backchannel in the corpus. Our reasoning here is that the more backchannels an individual produces, the fewer opportunities they are likely to have passed up. The models were run on previously unseen test data, the results of which can be seen in Table 5. All models improved on the training models. The baselin</context>
<context position="28177" citStr="Ward and Tsukahara, 2000" startWordPosition="4770" endWordPosition="4773">d to contain a backchannel continuer only if the backchannel started or ended within the pause. Instances where the backchannel started slightly before the pause would give the trigram POS &lt;bc&gt; &lt;pau&gt;. This particular trigram would not have been found by the language model; after a backchannel backing-off was applied, forcing the language model to count the pause as a unigram. However, after missing this location, the model might well place a backchannel slightly later, during the pause. Changing the location of a backchannel by 500ms does not affect whether or not it was perceived as natural (Ward and Tsukahara, 2000). Thus our evaluation technique overrepresents these misses. Finally, some of the cases that show up as errors in the evaluation are correct, but the dialogue move coding from which we derived the actual locations of backchannel continuers is not. There is a systematic confusion in our move system between pre-initiating ready moves and acknowledgments (Carletta et al., 1997). These moves share the same realizations, so coders often disagree on which of the two labels to use, especially for the acknowledgments that lack content words and therefore which we counted as backchannel continuers. Eve</context>
<context position="29595" citStr="Ward and Tsukahara, 2000" startWordPosition="4995" endWordPosition="4998"> one, no matter what the system thinks it is. 6 Conclusion In general there has been very little work carried out on building systems that are capable of placing backchannels. In this paper, we investigated various methods of predicting the placement of backchannel continuers, using only limited processing and information that is readily available to current spoken dialogue systems. Pause duration and a statistical part-of-speech language model were examined A method combining these two models achieved the best F-measure of 35% and improved on the baseline five-fold. The best previous system (Ward and Tsukahara, 2000) used as its sole predictor regions of low pitch and produced an accuracy of 18% for English. While our results may not be comparable to other work carried out in the field of natural language pro57 Baseline Trigram Pause Combined 10 Tri +&gt; .9s 3 Tri +&gt; .6s Precision 4% 22% 22% 25% 29% Recall 13% 50% 58% 51% 43% F-measure 7% 30% 32% 33% 35% Table 5: Results of the Models on the Test Data cessing, where scores of 90% or above are not uncommon for tasks such as part-of-speech tagging and statistical parsing, this can be at least partly explained by the fact that humans vary widely in how many of</context>
<context position="7966" citStr="Ward &amp; Tsukahara (2000)" startWordPosition="1294" endWordPosition="1297"> and peak energy. They found that the best single predictor of either phenomena was the preceding part-of-speech tag, but that combining the prosodic features gave better results, or, preferably, augmenting the part-of-speech tag with the combined prosody features. Turn exchange was indicated by interjections, sentence-final particles, and imperative and conclusive verb forms, together with a rise or fall in intonation. Hearer use of a backchannel continuer was indicated by conjunctive and case/adverbial particles and adverbial verb forms, coupled with the FO contours flat-fall and rise-fall. Ward &amp; Tsukahara (2000) modeled the location of backchannel continuers in Japanese and English coversation simply by inserting them wherever the other speaker produced a region of low pitch lasting 110ms. This model is motivated by the observation that such regions often accompany grammatical completion. Their model achieved 18% 2The identification of long pauses with TRPs, although understandable in the context of informing work on spoken dialogue systems, is somewhat at odds with previous thinking about turn-taking. Although turn-taking behaviour is culturally dependent , human dialogue is generally considered rem</context>
</contexts>
<marker>Ward, Tsukahara, 2000</marker>
<rawString>N. Ward and W. Tsukahara. 2000. Prosodic features which cue back-channel responses in English and Japanese. Journal of Pragmatics, 23:1177-1207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Yankelovich</author>
<author>G-A Levow</author>
<author>M Marx</author>
</authors>
<title>Designing SpeechActs: Issues in speech user interfaces.</title>
<date>1995</date>
<booktitle>In CHI Conference on Human Factors in Computing Systems.</booktitle>
<contexts>
<context position="1769" citStr="Yankelovich et al., 1995" startWordPosition="273" endWordPosition="276">working. Without this feedback, the partner may assume that he has not been understood and reformulate his utterance. Following Yngve (1970), we will use the term backchannel for such utterances. Although these can be substantive because they can repeat material from the partner&apos;s utterance (Clark and Schaefer, 1991), e.g., Right, okay, I&apos;m below the fiat rocks, we will adopt (Jorafsky et al., 1998)&apos;s terminology of continuer. We will take this to refer to the class of backchannel utterances, with minimal content, used to clearly signal that the speaker should continue with her current turn. (Yankelovich et al., 1995) point out that users of speech interface systems need feedback, too, especially since the system&apos;s silence could mean either of two very different things: that it is waiting for user input, in which case the user should speak, or that it is still processing information, in which case the user should not. However, any feedback must come at the right time or else it risks disrupting the speaker and ultimately, delaying task completion (Hirasawa et al., 1999). Most of our data, including the examples given above, are drawn from the HCRC Map Task Corpus, described in more detail in Section 3. Cle</context>
</contexts>
<marker>Yankelovich, Levow, Marx, 1995</marker>
<rawString>N. Yankelovich, G-A. Levow, and M. Marx. 1995. Designing SpeechActs: Issues in speech user interfaces. In CHI Conference on Human Factors in Computing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V H Yngve</author>
</authors>
<title>On getting a word in edgewise.</title>
<date>1970</date>
<booktitle>In Papers from the Sixth Regional Meeting, Chicago Linguistic Society,</booktitle>
<pages>567--577</pages>
<contexts>
<context position="1284" citStr="Yngve (1970)" startWordPosition="196" endWordPosition="197">vement for current systems. The baseline model inserts continuers after a predetermined number of words. One further model correlates back-channel continuers with pause duration, while a second predicts their occurrence using trigram POS frequencies. Combining these two models gives the best results. 1 Introduction In a spoken dialogue between people, the participants use simple utterances such as yeah, a totty wee bit aye and mm-hnint to signal that communication is working. Without this feedback, the partner may assume that he has not been understood and reformulate his utterance. Following Yngve (1970), we will use the term backchannel for such utterances. Although these can be substantive because they can repeat material from the partner&apos;s utterance (Clark and Schaefer, 1991), e.g., Right, okay, I&apos;m below the fiat rocks, we will adopt (Jorafsky et al., 1998)&apos;s terminology of continuer. We will take this to refer to the class of backchannel utterances, with minimal content, used to clearly signal that the speaker should continue with her current turn. (Yankelovich et al., 1995) point out that users of speech interface systems need feedback, too, especially since the system&apos;s silence could m</context>
</contexts>
<marker>Yngve, 1970</marker>
<rawString>V.H. Yngve. 1970. On getting a word in edgewise. In Papers from the Sixth Regional Meeting, Chicago Linguistic Society, pages 567-577.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>