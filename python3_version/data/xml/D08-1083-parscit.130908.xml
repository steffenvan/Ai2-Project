<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.9971255">
Learning with Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis
</title>
<author confidence="0.957822">
Yejin Choi and Claire Cardie
</author>
<affiliation confidence="0.950262">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.745705">
Ithaca, NY 14853
</address>
<email confidence="0.999343">
{ychoi,cardie}@cs.cornell.edu
</email>
<sectionHeader confidence="0.995754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997797307692308">
Determining the polarity of a sentiment-
bearing expression requires more than a sim-
ple bag-of-words approach. In particular,
words or constituents within the expression
can interact with each other to yield a particu-
lar overall polarity. In this paper, we view such
subsentential interactions in light of composi-
tional semantics, and present a novel learning-
based approach that incorporates structural in-
ference motivated by compositional seman-
tics into the learning procedure. Our exper-
iments show that (1) simple heuristics based
on compositional semantics can perform bet-
ter than learning-based methods that do not in-
corporate compositional semantics (accuracy
of 89.7% vs. 89.1%), but (2) a method that
integrates compositional semantics into learn-
ing performs better than all other alterna-
tives (90.7%). We also find that “content-
word negators”, not widely employed in pre-
vious work, play an important role in de-
termining expression-level polarity. Finally,
in contrast to conventional wisdom, we find
that expression-level classification accuracy
uniformly decreases as additional, potentially
disambiguating, context is considered.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999277">
Determining the polarity of sentiment-bearing ex-
pressions at or below the sentence level requires
more than a simple bag-of-words approach. One of
the difficulties is that words or constituents within
the expression can interact with each other to yield
a particular overall polarity. To facilitate our discus-
sion, consider the following examples:
</bodyText>
<listItem confidence="0.965546">
1: [I did [not]— have any [doubt]— about it.]+
2: [The report [eliminated]— my [doubt]—.]+
3: [They could [not]— [eliminate]— my [doubt]—.]—
</listItem>
<bodyText confidence="0.999827857142857">
In the first example, “doubt” in isolation carries
a negative sentiment, but the overall polarity of the
sentence is positive because there is a negator “not”,
which flips the polarity. In the second example, both
“eliminated” and “doubt” carry negative sentiment
in isolation, but the overall polarity of the sentence
is positive because “eliminated” acts as a negator for
its argument “doubt”. In the last example, there are
effectively two negators – “not” and “eliminated” –
which reverse the polarity of “doubt” twice, result-
ing in the negative polarity for the overall sentence.
These examples demonstrate that words or con-
stituents interact with each other to yield the
expression-level polarity. And a system that sim-
ply takes the majority vote of the polarity of indi-
vidual words will not work well on the above exam-
ples. Indeed, much of the previous learning-based
research on this topic tries to incorporate salient in-
teractions by encoding them as features. One ap-
proach includes features based on contextual va-
lence shifters1 (Polanyi and Zaenen, 2004), which
are words that affect the polarity or intensity of sen-
timent over neighboring text spans (e.g., Kennedy
and Inkpen (2005), Wilson et al. (2005), Shaikh et
al. (2007)). Another approach encodes frequent sub-
sentential patterns (e.g., McDonald et al. (2007)) as
features; these might indirectly capture some of the
subsentential interactions that affect polarity. How-
</bodyText>
<footnote confidence="0.925475">
1For instance, “never”, “nowhere”, “little”, “most”, “lack”,
“scarcely”, “deeply”.
</footnote>
<page confidence="0.938498">
793
</page>
<note confidence="0.962414">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943634146342">
ever, both types of approach are based on learning
models with a flat bag-of-features: some structural
information can be encoded as higher order features,
but the final representation of the input is still a flat
feature vector that is inherently too limited to ade-
quately reflect the complex structural nature of the
underlying subsentential interactions. (Liang et al.,
2008)
Moilanen and Pulman (2007), on the other hand,
handle the structural nature of the interactions more
directly using the ideas from compositional seman-
tics (e.g., Montague (1974), Dowty et al. (1981)). In
short, the Principle of Compositionality states that
the meaning of a compound expression is a func-
tion of the meaning of its parts and of the syntac-
tic rules by which they are combined (e.g., Mon-
tague (1974), Dowty et al. (1981)). And Moilanen
and Pulman (2007) develop a collection of compo-
sition rules to assign a sentiment value to individual
expressions, clauses, or sentences. Their approach
can be viewed as a type of structural inference, but
their hand-written rules have not been empirically
compared to learning-based alternatives, which one
might expect to be more effective in handling some
aspects of the polarity classification task.
In this paper, we begin to close the gap between
learning-based approaches to expression-level po-
larity classification and those founded on composi-
tional semantics: we present a novel learning-based
approach that incorporates structural inference mo-
tivated by compositional semantics into the learning
procedure.
Adopting the view point of compositional seman-
tics, our working assumption is that the polarity of a
sentiment-bearing expression can be determined in a
two-step process: (1) assess the polarities of the con-
stituents of the expression, and then (2) apply a rela-
tively simple set of inference rules to combine them
recursively. Rather than a rigid application of hand-
written compositional inference rules, however, we
hypothesize that an ideal solution to the expression-
level polarity classification task will be a method
that can exploit ideas from compositional seman-
tics while providing the flexibility needed to handle
the complexities of real-world natural language —
exceptions, unknown words, missing semantic fea-
tures, and inaccurate or missing rules. The learning-
based approach proposed in this paper takes a first
step in this direction.
In addition to the novel learning approach, this
paper presents new insights for content-word nega-
tors, which we define as content words that can
negate the polarity of neighboring words or con-
stituents. (e.g., words such as “eliminated” in the
example sentences). Unlike function-word nega-
tors, such as “not” or “never”, content-word nega-
tors have been recognized and utilized less actively
in previous work. (Notable exceptions include e.g.,
Niu et al. (2005), Wilson et al. (2005), and Moilanen
and Pulman (2007).2)
In our experiments, we compare learning- and
non-learning-based approaches to expression-level
polarity classification — with and without com-
positional semantics — and find that (1) simple
heuristics based on compositional semantics outper-
form (89.7% in accuracy) other reasonable heuris-
tics that do not incorporate compositional seman-
tics (87.7%); they can also perform better than sim-
ple learning-based methods that do not incorporate
compositional semantics (89.1%), (2) combining
learning with the heuristic rules based on compo-
sitional semantics further improves the performance
(90.7%), (3) content-word negators play an impor-
tant role in determining the expression-level polar-
ity, and, somewhat surprisingly, we find that (4)
expression-level classification accuracy uniformly
decreases as additional, potentially disambiguating,
context is considered.
In what follows, we first explore heuristic-based
approaches in §2, then we present learning-based ap-
proaches in §3. Next we present experimental results
in §4, followed by related work in §5.
</bodyText>
<sectionHeader confidence="0.999827" genericHeader="method">
2 Heuristic-Based Methods
</sectionHeader>
<bodyText confidence="0.9997938">
This section describes a set of heuristic-based meth-
ods for determining the polarity of a sentiment-
bearing expression. Each assesses the polarity of the
words or constituents using a polarity lexicon that
indicates whether a word has positive or negative
polarity, and finds negators in the given expression
using a negator lexicon. The methods then infer the
expression-level polarity using voting-based heuris-
tics (§ 2.1) or heuristics that incorporate composi-
tional semantics (§2.2). The lexicons are described
</bodyText>
<footnote confidence="0.946105">
2See §5. Related Work for detailed discussion.
</footnote>
<page confidence="0.994079">
794
</page>
<table confidence="0.99153125">
VOTE NEG(1) NEG(N) NEGEX(1) NEGEX(N) COMPO
type of negators none function-word function-word &amp; content-word
maximum # of negations applied 0 1 n 1 n n
scope of negators N/A over the entire expression compositional
</table>
<tableCaption confidence="0.999161">
Table 1: Heuristic methods. (n refers to the number of negators found in a given expression.)
</tableCaption>
<table confidence="0.7089580625">
Rules Examples
1 Polarity( not [arg1] ) = ¬ Polarity( arg1 ) not [bad]arg1.
2 Polarity( [VP] [NP] ) = Compose( [VP], [NP] ) [destroyed]VP [the terrorism]NP.
3 Polarity( [VP1] to [VP2] ) = Compose( [VP1], [VP2] ) [refused]V P1 to [deceive]VP2 the man.
4 Polarity( [adj] to [VP] ) = Compose( [adj], [VP] ) [unlikely]adj to [destroy]V P the planet.
5 Polarity( [NP1] [IN] [NP2] ) = Compose( [NP1], [NP2] ) [lack]NP1 [of]IN [crime]NP2 in rural areas.
6 Polarity( [NP] [VP] ) = Compose( [VP], [NP] ) [pollution]NP [has decreased]V P.
7 Polarity( [NP] be [adj] ) = Compose( [adj], [NP] ) [harm]NP is [minimal]adj.
Definition of Compose( arg1, arg2 )
Compose( arg1, arg2 ) =
For COMPOMC: if (arg1 is a negator) then ¬ Polarity( arg2 )
(COMPOsition with Majority Class) else if (Polarity( arg1 ) == Polarity( arg2 )) then Polarity( arg1 )
else the majority polarity of data
Compose( arg1, arg2 ) =
For COMPOPR: if (arg1 is a negator) then ¬ Polarity( arg2 )
(COMPOsition with PRiority) else Polarity( arg1 )
</table>
<tableCaption confidence="0.996861">
Table 2: Compositional inference rules motivated by compositional semantics.
</tableCaption>
<bodyText confidence="0.631056">
in §2.3.
</bodyText>
<subsectionHeader confidence="0.983383">
2.1 Voting
</subsectionHeader>
<bodyText confidence="0.999091423076923">
We first explore five simple heuristics based on vot-
ing. VOTE is defined as the majority polarity vote
by words in a given expression. That is, we count
the number of positive polarity words and negative
polarity words in a given expression, and assign the
majority polarity to the expression. In the case of a
tie, we default to the prevailing polarity of the data.
For NEG(1), we first determine the majority polar-
ity vote as above, and then if the expression contains
any function-word negator, flip the polarity of the
majority vote once. NEG(N) is similar to NEG(1), ex-
cept we flip the polarity of the majority vote n times
after the majority vote, where n is the number of
function-word negators in a given expression.
NEGEX(1) and NEGEX(N) are defined similarly as
NEG(1) and NEG(N) above, except both function-
word negators and content-word negators are con-
sidered as negators when flipping the polarity of the
majority vote. See Table 1 for summary. Note that a
word can be both a negator and have a negative prior
polarity. For the purpose of voting, if a word is de-
fined as a negator per the voting scheme, then that
word does not participate in the majority vote.
For brevity, we refer to NEG(1) and NEG(N) col-
lectively as NEG, and NEGEX(1) and NEGEX(N) col-
lectively as NEGEX.
</bodyText>
<subsectionHeader confidence="0.999321">
2.2 Compositional semantics
</subsectionHeader>
<bodyText confidence="0.999829888888889">
Whereas the heuristics above use voting-based in-
ference, those below employ a set of hand-written
rules motivated by compositional semantics. Table 2
shows the definition of the rules along with moti-
vating examples. In order to apply a rule, we first
detect a syntactic pattern (e.g., [destroyed]uP [the
terrorism]NP), then apply the Compose function as
defined in Table 2 (e.g., Compose([destroyed], [the
terrorism]) by rule #2).3
</bodyText>
<footnote confidence="0.5493765">
3Our implementation uses part-of-speech tags and function-
words to coarsely determine the patterns. An implementation
</footnote>
<page confidence="0.99466">
795
</page>
<bodyText confidence="0.999785764705882">
Compose first checks whether the first argument is
a negator, and if so, flips the polarity of the second
argument. Otherwise, Compose resolves the polar-
ities of its two arguments. Note that if the second
argument is a negator, we do not flip the polarity of
the first argument, because the first argument in gen-
eral is not in the semantic scope of the negation.4 In-
stead, we treat the second argument as a constituent
with negative polarity.
We experiment with two variations of the Com-
pose function depending on how conflicting polari-
ties are resolved: COMPOMC uses a Compose func-
tion that defaults to the Majority Class of the po-
larity of the data,5 while COMPOPR uses a Compose
function that selects the polarity of the argument that
has higher semantic PRiority. For brevity, we refer
to COMPOPR and COMPOMC collectively as COMPO.
</bodyText>
<subsectionHeader confidence="0.997769">
2.3 Lexicons
</subsectionHeader>
<bodyText confidence="0.913275">
The polarity lexicon is initialized with the lexicon
of Wilson et al. (2005) and then expanded using the
General Inquirer dictionary.6 In particular, a word
contained in at least two of the following categories
is considered as positive: POSITIV, PSTV, POSAFF,
PLEASUR, VIRTUE, INCREAS, and a word contained
in at least one of the following categories is consid-
ered as negative: NEGATIV, NGTV, NEGAFF, PAIN,
VICE, HOSTILE, FAIL, ENLLOSS, WLBLOSS, TRAN-
LOSS.
For the (function- and content-word) negator lex-
icon, we collect a handful of seed words as well as
General Inquirer words that appear in either NOTLW
or DECREAS category. Then we expand the list of
content-negators using the synonym information of
WordNet (Miller, 1995) to take a simple vote among
senses.
based on parse trees might further improve the performance.
4Moilanen and Pulman (2007) provide more detailed dis-
cussion on the semantic scope of negations and the semantic
priorities in resolving polarities.
</bodyText>
<footnote confidence="0.91992">
5The majority polarity of the data we use for our experi-
ments is negative.
6Available at http://www.wjh.harvard.edu/∼inquirer/.
</footnote>
<bodyText confidence="0.504473666666667">
When consulting the General Inquirer dictionary, senses with
less than 5% frequency and senses specific to an idiom are
dropped.
</bodyText>
<sectionHeader confidence="0.999409" genericHeader="method">
3 Learning-Based Methods
</sectionHeader>
<bodyText confidence="0.9999043">
While we expect that a set of hand-written heuristic
rules motivated by compositional semantics can be
effective for determining the polarity of a sentiment-
bearing expression, we do not expect them to be per-
fect. Interpreting natural language is such a com-
plex task that writing a perfect set of rules would
be extremely challenging. Therefore, a more ideal
solution would be a learning-based method that can
exploit ideas from compositional semantics while
providing the flexibility to the rigid application of
the heuristic rules. To this end, we present a novel
learning-based approach that incorporates inference
rules inspired by compositional semantics into the
learning procedure (§3.2). To assess the effect of
compositional semantics in the learning-based meth-
ods, we also experiment with a simple classifica-
tion approach that does not incorporate composi-
tional semantics (§3.1). The details of these two
approaches are elaborated in the following subsec-
tions.
</bodyText>
<subsectionHeader confidence="0.99286">
3.1 Simple Classification (SC)
</subsectionHeader>
<bodyText confidence="0.997019818181818">
Given an expression x consisting of n words xi,
..., xn, the task is to determine the polarity y E
{positive, negative} of x. In our simple binary
classification approach, x is represented as a vec-
tor of features f(x), and the prediction y is given by
argmaxyw·f(x, y), where w is a vector of parameters
learned from training data. In our experiment, we
use an online SVM algorithm called MIRA (Margin
Infused Relaxed Algorithm) (Crammer and Singer,
2003)7 for training.
For each x, we encode the following features:
</bodyText>
<listItem confidence="0.9982061">
• Lexical: We add every word xi in x, and also
add the lemma of xi produced by the CASS
partial parser toolkit (Abney, 1996).
• Dictionary: In order to mitigate the problem of
unseen words in the test data, we add features
that describe word categories based on the Gen-
eral Inquirer dictionary. We add this feature for
each xi that is not a stop word.
• Vote: We experiment with two variations of
voting-related features: for SC-VOTE, we add
</listItem>
<footnote confidence="0.960067333333333">
7We use the Java implementation of this algorithm
available at http://www.seas.upenn.edu/∼strctlrn/StructLearn
/StructLearn.html.
</footnote>
<page confidence="0.990259">
796
</page>
<figure confidence="0.356921">
Definitions of score functions and loss functions
</figure>
<figureCaption confidence="0.87365">
Figure 1: Training procedures. y∗ E {positive, negative} denotes the true label for a given expression x = x1, ..., xn.
z∗ denotes the pseudo gold standard for hidden variables z.
</figureCaption>
<table confidence="0.5303184">
Simple Classification
Classification with Compositional Inference
Find K best z and denote them as i = {z(1), ..., z(K)}
s.t. d i &lt; j, score(z(i)) &gt; score(z(j))
zbad +— mink z(k) s.t. loss compo(y∗, z(k), x) &gt; 0
(if such zbad not found in i, skip parameter update for this.)
If loss compo(y∗, z∗, x) &gt; 0
z9ood +— mink z(k) s.t. loss compo(y∗, z(k), x) = 0
z∗ +— z9ood
(if such z9ood not found in i, stick to the original z∗.)
</table>
<equation confidence="0.950439222222222">
l +— loss compo(y∗, zbad, x) − loss compo(y∗, z∗, x)
w +— update(w, l, z∗, zbad)
y +— argmaxy score(y)
l +— loss flat(y∗, y)
w +— update(w, l, y∗, y)
score(y) := w · f(x, y)
loss flat(y∗, y) := if (y∗ = y) 0 else 1
score(z) := Ei score(zi) := Ei w · f(x, zi, i)
loss compo(y∗, z, x) := if (y∗ = C(x, z)) 0 else 1
</equation>
<bodyText confidence="0.999948">
a feature that indicates the dominant polarity of
words in the given expression, without consid-
ering the effect of negators. For SC-NEGEX,
we count the number of content-word nega-
tors as well as function-word negators to de-
termine whether the final polarity should be
flipped. Then we add a conjunctive feature that
indicates the dominant polarity together with
whether the final polarity should be flipped. For
brevity, we refer to SC-VOTE and SC-NEGEX
collectively as SC.
Notice that in this simple binary classification set-
ting, it is inherently difficult to capture the compo-
sitional structure among words in x, because f(x, y)
is merely a flat bag of features, and the prediction
is governed simply by the dot product of f(x, y) and
the parameter vector w.
</bodyText>
<subsectionHeader confidence="0.9912885">
3.2 Classification with Compositional
Inference (CCI)
</subsectionHeader>
<bodyText confidence="0.999017111111111">
Next, instead of determining y directly from x,
we introduce hidden variables z = (zi,..., zn)
as intermediate decision variables, where zi E
{positive, negative, negator, none}, so that zi
represents whether xi is a word with posi-
tive/negative polarity, or a negator, or none of the
above. For simplicity, we let each intermediate de-
cision variable zi (a) be determined independently
from other intermediate decision variables, and (b)
</bodyText>
<figure confidence="0.871460666666667">
For each token xi,
if xi is a word in the negator lexicon
then z∗i +— negator
else if xi is in the polarity lexicon as negative
then z∗i +— negative
else if xi is in the polarity lexicon as positive
then z∗i +— positive
else
then z∗i +— none
</figure>
<figureCaption confidence="0.999885">
Figure 2: Constructing Soft Gold Standard z∗
</figureCaption>
<bodyText confidence="0.999962588235294">
depend only on the input x, so that zi = argmaxziw·
f(x, zi, i), where f(x, zi, i) is the feature vector en-
coding around the ith word (described on the next
page). Once we determine the intermediate decision
variables, we apply the heuristic rules motivated by
compositional semantics (from Table 2) in order to
obtain the final polarity y of x. That is, y = C(x, z),
where C is the function that applies the composi-
tional inference, either COMPOPR or COMPOMC.
For training, there are two issues we need to
handle: the first issue is dealing with the hidden
variables z. Because the structure of composi-
tional inference C does not allow dynamic program-
ming, it is intractable to perform exact expectation-
maximization style training that requires enumerat-
ing all possible values of the hidden variables z. In-
stead, we propose a simple and tractable training
</bodyText>
<page confidence="0.993213">
797
</page>
<bodyText confidence="0.999122558823529">
rule based on the creation of a soft gold standard for
z. In particular, we exploit the fact that in our task,
we can automatically construct a reasonably accu-
rate gold standard for z, denoted as z*: as shown in
Figure 2, we simply rely on the negator and polar-
ity lexicons. Because z* is not always correct, we
allow the training procedure to replace z* with po-
tentially better assignments as learning proceeds: in
the event that the soft gold standard z* leads to an in-
correct prediction, we search for an assignment that
leads to a correct prediction to replace z*. The exact
procedure is given in Figure 1, and will be discussed
again shortly.
Figure 1 shows how we modify the parameter up-
date rule of MIRA (Crammer and Singer, 2003) to
reflect the aspect of compositional inference. In the
event that the soft gold standard z* leads to an incor-
rect prediction, we search for zgood, the assignment
with highest score that leads to a correct prediction,
and replace z* with zgood. In the event of no such
zgood being found among the K-best assignments of
z, we stick with z*.
The second issue is finding the assignment of z
with the highest score(z) = &amp; w · f(x, zi, i) that
leads to an incorrect prediction y = C(x, z). Be-
cause the structure of compositional inference C
does not allow dynamic programming, finding such
an assignment is again intractable. We resort to enu-
merating only over K-best assignments instead. If
none of the K-best assignments of z leads to an in-
correct prediction y, then we skip the training in-
stance for parameter update.
Features. For each xi in x, we encode the follow-
ing features:
</bodyText>
<listItem confidence="0.995734">
• Lexical: We include the current word xi as well
as the lemma of xi produced by CASS partial
parser toolkit (Abney, 1996). We also add a
boolean feature to indicate whether the current
word is a stop word.
• Dictionary: In order to mitigate the problem
</listItem>
<bodyText confidence="0.9630755625">
with unseen words in the test data, we add fea-
tures that describe word categories based on the
General Inquirer dictionary. We add this fea-
ture for each xi that is not a stop word. We
also add a number of boolean features that pro-
vide following properties of xi using the polar-
ity lexicon and the negator lexicon:
– whether xi is a function-word negator
– whether xi is a content-word negator
– whether xi is a negator of any kind
– the polarity of xi according to Wilson et
al. (2005)’s polarity lexicon
– the polarity of xi according to the lexicon
derived from the General Inquirer dictio-
nary
– conjunction of the above two features
</bodyText>
<listItem confidence="0.9889505">
• Vote: We encode the same vote feature that we
use for SC-NEGEX described in § 3.1.
</listItem>
<bodyText confidence="0.999950428571428">
As in the heuristic-based compositional semantics
approach (§ 2.2), we experiment with two variations
of this learning-based approach: CCI-COMPOPR
and CCI-COMPOMC, whose compositional infer-
ence rules are COMPOPR and COMPOMC respec-
tively. For brevity, we refer to both variations col-
lectively as CCI-COMPO.
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999004">
The experiments below evaluate our heuristic- and
learning-based methods for subsentential sentiment
analysis (§ 4.1). In addition, we explore the role
of context by expanding the boundaries of the
sentiment-bearing expressions (§ 4.2).
</bodyText>
<subsectionHeader confidence="0.999692">
4.1 Evaluation with given boundaries
</subsectionHeader>
<bodyText confidence="0.999948588235294">
For evaluation, we use the Multi-Perspective Ques-
tion Answering (MPQA) corpus (Wiebe et al.,
2005), which consists of 535 newswire documents
manually annotated with phrase-level subjectivity
information. We evaluate on all strong (i.e., inten-
sity of expression is ‘medium’ or higher), sentiment-
bearing (i.e., polarity is ‘positive’ or ‘negative’) ex-
pressions.8 As a result, we can assume the bound-
aries of the expressions are given. Performance is
reported using 10-fold cross-validation on 400 doc-
uments; a separate 135 documents were used as a
development set. Based on pilot experiments on the
development data, we set parameters for MIRA as
follows: slack variable to 0.5, and the number of
incorrect labels (constraints) for each parameter up-
date to 1. The number of iterations (epochs) for
training is set to 1 for simple classification, and to 4
</bodyText>
<footnote confidence="0.5353715">
8We discard expressions with confidence marked as ‘uncer-
tain’.
</footnote>
<page confidence="0.986114">
798
</page>
<table confidence="0.9967702">
Heuristic-Based Learning-Based
VOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCI
(1) (N) EX EX MC PR VOTE NEG COMPO COMPO
(1) (N) EX MC PR
86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7
</table>
<tableCaption confidence="0.996883">
Table 3: Performance (in accuracy) on MPQA dataset.
</tableCaption>
<table confidence="0.999819125">
Data Heuristic-Based Learning-Based
VOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCI
(1) (N) EX EX MC PR VOTE NEG COMPO COMPO
(1) (N) EX MC PR
[-0,+0] 86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7
[-1,+1] 86.4 81.0 81.2 87.2 87.2 89.3 89.0 88.3 88.4 89.5 89.4
[-5,+5] 85.9 79.0 79.4 85.7 85.6 88.2 88.0 86.4 87.1 88.7 88.7
[-oc,+oc] 85.3 75.8 76.9 83.9 83.9 87.0 86.9 85.8 85.8 87.3 87.5
</table>
<tableCaption confidence="0.999916">
Table 4: Performance (in accuracy) on MPQA data set with varying boundaries of expressions.
</tableCaption>
<bodyText confidence="0.997238">
for classification with compositional inference. We
use K = 20 for classification with compositional
inference.
Results. Performance is reported in Table 3. In-
terestingly, the heuristic-based methods NEG (-
82.2%) that only consider function-word negators
perform even worse than VOTE (86.5%), which does
not consider negators. On the other hand, the NEGEX
methods (87.7%) that do consider content-word
negators as well as function-word negators perform
better than VOTE. This confirms the importance of
content-word negators for determining the polari-
ties of expressions. The heuristic-based methods
motivated by compositional semantics COMPO fur-
ther improve the performance over NEGEX, achiev-
ing up to 89.7% accuracy. In fact, these heuris-
tics perform even better than the SC learning-based
methods (- 89.1%). This shows that heuristics that
take into account the compositional structure of the
expression can perform better than learning-based
methods that do not exploit such structure.
Finally, the learning-based methods that in-
corporate compositional inference CCI-COMPO (-
90.7%) perform better than all of the previous
methods. The difference between CCI-COMPOPR
(90.7%) and SC-NEGEX (89.1%) is statistically sig-
nificant at the .05 level by paired t-test. The dif-
ference between COMPO and any other heuristic that
is not based on computational semantics is also
statistically significant. In addition, the difference
between CCICOMPOPR (learning-based) and COM-
POMC (non-learning-based) is statistically signifi-
cant, as is the difference between NEGEX and VOTE.
</bodyText>
<subsectionHeader confidence="0.998617">
4.2 Evaluation with noisy boundaries
</subsectionHeader>
<bodyText confidence="0.999721454545454">
One might wonder whether employing additional
context outside the annotated expression boundaries
could further improve the performance. Indeed, con-
ventional wisdom would say that it is necessary to
employ such contextual information (e.g., Wilson et
al. (2005)). In any case, it is important to determine
whether our results will apply to more real-world
settings where human-annotated expression bound-
aries are not available.
To address these questions, we gradually relax
our previous assumption that the exact boundaries of
expressions are given: for each annotation bound-
ary, we expand the boundary by x words for each
direction, up to sentence boundaries, where x E
11, 5, ocI. We stop expanding the boundary if it
will collide with the boundary of an expression with
a different polarity, so that we can consistently re-
cover the expression-level gold standard for evalua-
tion. This expansion is applied to both the training
and test data, and the performance is reported in Ta-
ble 4. From this experiment, we make the following
observations:
</bodyText>
<listItem confidence="0.625198">
• Expanding the boundaries hurts the perfor-
</listItem>
<page confidence="0.996101">
799
</page>
<bodyText confidence="0.998295035714286">
mance for any method. This shows that most of
relevant context for judging the polarity is con-
tained within the expression boundaries, and
motivates the task of finding the boundaries of
opinion expressions.
• The NEGEX methods perform better than VOTE
only when the expression boundaries are rea-
sonably accurate. When the expression bound-
aries are expanded up to sentence boundaries,
they perform worse than VOTE. We conjecture
this is because the scope of negators tends to be
limited to inside of expression boundaries.
• The COMPO methods always perform better
than any other heuristic-based methods. And
their performance does not decrease as steeply
as the NEGEX methods as the expression
boundaries expand. We conjecture this is be-
cause methods based on compositional seman-
tics can handle the scope of negators more ade-
quately.
• Among the learning-based methods, those that
involve compositional inference (CCI-COMPO)
always perform better than those that do not
(SC) for any boundaries. And learning with
compositional inference tend to perform bet-
ter than the rigid application of heuristic rules
(COMPO), although the relative performance
gain decreases once the boundaries are relaxed.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999943298245614">
The task focused on in this paper is similar to that
of Wilson et al. (2005) in that the general goal of the
task is to determine the polarity in context at a sub-
sentence level. However, Wilson et al. (2005) for-
mulated the task differently by limiting their evalua-
tion to individual words that appear in their polarity
lexicon. Also, their approach was based on a flat bag
of features, and only a few examples of what we call
content-word negators were employed.
Our use of compositional semantics for the task
of polarity classification is preceded by Moilanen
and Pulman (2007), but our work differs in that
we integrate the key idea of compositional seman-
tics into learning-based methods, and that we per-
form empirical comparisons among reasonable al-
ternative approaches. For comparison, we evalu-
ated our approaches on the polarity classification
task from SemEval-07 (Strapparava and Mihalcea,
2007). We achieve 88.6% accuracy with COMPOPR,
90.1% with SCNEGEX, and 87.6% with CCICOM-
POMC.9 There are a number of possible reasons for
our lower performance vs. Moilanen and Pulman
(2007) on this data set. First, SemEval-07 does not
include a training data set for this task, so we use
400 documents from the MPQA corpus instead. In
addition, the SemEval-07 data is very different from
the MPQA data in that (1) the polarity annotation
is given only at the sentence level, (2) the sentences
are shorter, with simpler structure, and not as many
negators as the MPQA sentences, and (3) there are
many more instances with positive polarity than in
the MPQA corpus.
Nairn et al. (2006) also employ a “polarity” prop-
agation algorithm in their approach to the semantic
interpretation of implicatives. However, their notion
of polarity is quite different from that assumed here
and in the literature on sentiment analysis. In partic-
ular, it refers to the degree of “commitment” of the
author to the truth or falsity of a complement clause
for a textual entailment task.
McDonald et al. (2007) use a structured model
to determine the sentence-level polarity and the
document-level polarity simultaneously. But deci-
sions at each sentence level does not consider struc-
tural inference within the sentence.
Among the studies that examined content-word
negators, Niu et al. (2005) manually collected a
small set of such words (referred as “words that
change phases”), but their lexicon was designed
mainly for the medical domain and the type of nega-
tors was rather limited. Wilson et al. (2005) also
manually collected a handful of content-word nega-
tors (referred as “general polarity shifters”), but not
extensively. Moilanen and Pulman (2007) collected
a more extensive set of negators semi-automatically
using WordNet 2.1, but the empirical effect of such
words was not explicitly investigated.
</bodyText>
<footnote confidence="0.664726">
9For lack of space, we only report our performance on in-
stances with strong intensities as defined in Moilanen and Pul-
man (2007), which amounts to only 208 test instances. The
cross-validation set of MPQA contains 4.9k instances.
</footnote>
<page confidence="0.994834">
800
</page>
<sectionHeader confidence="0.999184" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999917642857143">
In this paper, we consider the task of determining
the polarity of a sentiment-bearing expression, con-
sidering the effect of interactions among words or
constituents in light of compositional semantics. We
presented a novel learning-based approach that in-
corporates structural inference motivated by compo-
sitional semantics into the learning procedure. Our
approach can be considered as a small step toward
bridging the gap between computational semantics
and machine learning methods. Our experimen-
tal results suggest that this direction of research is
promising. Future research includes an approach
that learns the compositional inference rules from
data.
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999671625">
This work was supported in part by National Science
Foundation Grants BCS-0624277 and IIS-0535099
and by Department of Homeland Security Grant
N0014-07-1-0152. We also thank Eric Breck, Lil-
lian Lee, Mats Rooth, the members of the Cornell
NLP reading seminar, and the EMNLP reviewers for
insightful comments on the submitted version of the
paper.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999746348484848">
Steven Abney. 1996. Partial parsing via finite-state
cascades. Journal of Natural Language Engineering,
2(4):337344.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR
3:951.
David R. Dowty, Robert E. Wall and Stanley Peters.
1981. Introduction to Montague Semantics.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A Publicly Available Lexical Resource for Opin-
ion Mining. In Proceedings of 5th Conference on Lan-
guage Resources and Evaluation (LREC),.
Percy Liang, Hal Daum´e III and Dan Klein. 2008. Struc-
ture Compilation: Trading Structure for Features. In
International Conference on Machine Learning.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery &amp; Data Mining (KDD-2004).
Alistair Kennedy and Diana Inkpen. 2005. Senti-
ment Classification of Movie and Product Reviews Us-
ing Contextual Valence Shifters. In Proceedings of
FINEXIN 2005, Workshop on the Analysis of Infor-
mal and Formal Information Exchange during Nego-
tiations.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of COLING.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells and Jeff Reynar. 2007. Structured Models for
Fine-to-Coarse Sentiment Analysis. In Proceedings of
Association for Computational Linguistics (ACL) .
George A. Miller. 1995. WordNet: a lexical database for
English. In Communications of the ACM, 38(11):3941
Richard Montague. 1974. Formal Philosophy; Selected
papers of Richard Montague. Yale University Press.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007).
Rowan Nairn, Cleo Condoravdi and Lauri Karttunen
2006. Computing relative polarity for textual infer-
ence. In Inference in Computational Semantics (ICoS-
5).
Yun Niu, Xiaodan Zhu, Jianhua Li and Graeme Hirst.
2005. Analysis of polarity information in medical text.
In Proceedings of the American Medical Informatics
Association 2005 Annual Symposium (AMIA).
Livia Polanyi and Annie Zaenen. 2004. Contextual lex-
ical valence shifters. In Exploring Attitude and Affect
in Text: Theories and Applications: Papers from the
2004 Spring Symposium, AAAI.
Mostafa Shaikh, Helmut Prendinger and Mitsuru
Ishizuka. 2007. Assessing sentiment of text by se-
mantic dependency and contextual valence analysis.
In Proc 2nd Int’l Conf on Affective Computing and In-
telligent Interaction (ACII’07).
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of Se-
mEval.
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. In Language Resources and Evalua-
tion (formerly Computers and the Humanities), 39(2-
3):165210.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP.
</reference>
<page confidence="0.998324">
801
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798995">
<title confidence="0.996479">Learning with Compositional Semantics as Structural Inference Subsentential Sentiment Analysis</title>
<author confidence="0.993598">Yejin Choi</author>
<author confidence="0.993598">Claire</author>
<affiliation confidence="0.959385">Department of Computer Cornell</affiliation>
<address confidence="0.97769">Ithaca, NY</address>
<abstract confidence="0.995783777777778">Determining the polarity of a sentimentbearing expression requires more than a simple bag-of-words approach. In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity. In this paper, we view such interactions in light of composiand present a novel learningbased approach that incorporates structural inference motivated by compositional semantics into the learning procedure. Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%). We also find that “contentword negators”, not widely employed in previous work, play an important role in determining expression-level polarity. Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy additional, potentially disambiguating, context is considered.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial parsing via finite-state cascades.</title>
<date>1996</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="15151" citStr="Abney, 1996" startWordPosition="2367" endWordPosition="2368">ing of n words xi, ..., xn, the task is to determine the polarity y E {positive, negative} of x. In our simple binary classification approach, x is represented as a vector of features f(x), and the prediction y is given by argmaxyw·f(x, y), where w is a vector of parameters learned from training data. In our experiment, we use an online SVM algorithm called MIRA (Margin Infused Relaxed Algorithm) (Crammer and Singer, 2003)7 for training. For each x, we encode the following features: • Lexical: We add every word xi in x, and also add the lemma of xi produced by the CASS partial parser toolkit (Abney, 1996). • Dictionary: In order to mitigate the problem of unseen words in the test data, we add features that describe word categories based on the General Inquirer dictionary. We add this feature for each xi that is not a stop word. • Vote: We experiment with two variations of voting-related features: for SC-VOTE, we add 7We use the Java implementation of this algorithm available at http://www.seas.upenn.edu/∼strctlrn/StructLearn /StructLearn.html. 796 Definitions of score functions and loss functions Figure 1: Training procedures. y∗ E {positive, negative} denotes the true label for a given expres</context>
<context position="20712" citStr="Abney, 1996" startWordPosition="3353" endWordPosition="3354">with the highest score(z) = &amp; w · f(x, zi, i) that leads to an incorrect prediction y = C(x, z). Because the structure of compositional inference C does not allow dynamic programming, finding such an assignment is again intractable. We resort to enumerating only over K-best assignments instead. If none of the K-best assignments of z leads to an incorrect prediction y, then we skip the training instance for parameter update. Features. For each xi in x, we encode the following features: • Lexical: We include the current word xi as well as the lemma of xi produced by CASS partial parser toolkit (Abney, 1996). We also add a boolean feature to indicate whether the current word is a stop word. • Dictionary: In order to mitigate the problem with unseen words in the test data, we add features that describe word categories based on the General Inquirer dictionary. We add this feature for each xi that is not a stop word. We also add a number of boolean features that provide following properties of xi using the polarity lexicon and the negator lexicon: – whether xi is a function-word negator – whether xi is a content-word negator – whether xi is a negator of any kind – the polarity of xi according to Wil</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial parsing via finite-state cascades. Journal of Natural Language Engineering, 2(4):337344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>JMLR</journal>
<pages>3--951</pages>
<contexts>
<context position="14965" citStr="Crammer and Singer, 2003" startWordPosition="2330" endWordPosition="2333">that does not incorporate compositional semantics (§3.1). The details of these two approaches are elaborated in the following subsections. 3.1 Simple Classification (SC) Given an expression x consisting of n words xi, ..., xn, the task is to determine the polarity y E {positive, negative} of x. In our simple binary classification approach, x is represented as a vector of features f(x), and the prediction y is given by argmaxyw·f(x, y), where w is a vector of parameters learned from training data. In our experiment, we use an online SVM algorithm called MIRA (Margin Infused Relaxed Algorithm) (Crammer and Singer, 2003)7 for training. For each x, we encode the following features: • Lexical: We add every word xi in x, and also add the lemma of xi produced by the CASS partial parser toolkit (Abney, 1996). • Dictionary: In order to mitigate the problem of unseen words in the test data, we add features that describe word categories based on the General Inquirer dictionary. We add this feature for each xi that is not a stop word. • Vote: We experiment with two variations of voting-related features: for SC-VOTE, we add 7We use the Java implementation of this algorithm available at http://www.seas.upenn.edu/∼strctl</context>
<context position="19710" citStr="Crammer and Singer, 2003" startWordPosition="3170" endWordPosition="3173">e can automatically construct a reasonably accurate gold standard for z, denoted as z*: as shown in Figure 2, we simply rely on the negator and polarity lexicons. Because z* is not always correct, we allow the training procedure to replace z* with potentially better assignments as learning proceeds: in the event that the soft gold standard z* leads to an incorrect prediction, we search for an assignment that leads to a correct prediction to replace z*. The exact procedure is given in Figure 1, and will be discussed again shortly. Figure 1 shows how we modify the parameter update rule of MIRA (Crammer and Singer, 2003) to reflect the aspect of compositional inference. In the event that the soft gold standard z* leads to an incorrect prediction, we search for zgood, the assignment with highest score that leads to a correct prediction, and replace z* with zgood. In the event of no such zgood being found among the K-best assignments of z, we stick with z*. The second issue is finding the assignment of z with the highest score(z) = &amp; w · f(x, zi, i) that leads to an incorrect prediction y = C(x, z). Because the structure of compositional inference C does not allow dynamic programming, finding such an assignment</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. JMLR 3:951.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
<author>Robert E Wall</author>
<author>Stanley Peters</author>
</authors>
<title>Introduction to Montague Semantics.</title>
<date>1981</date>
<contexts>
<context position="4152" citStr="Dowty et al. (1981)" startWordPosition="613" endWordPosition="616">Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some structural information can be encoded as higher order features, but the final representation of the input is still a flat feature vector that is inherently too limited to adequately reflect the complex structural nature of the underlying subsentential interactions. (Liang et al., 2008) Moilanen and Pulman (2007), on the other hand, handle the structural nature of the interactions more directly using the ideas from compositional semantics (e.g., Montague (1974), Dowty et al. (1981)). In short, the Principle of Compositionality states that the meaning of a compound expression is a function of the meaning of its parts and of the syntactic rules by which they are combined (e.g., Montague (1974), Dowty et al. (1981)). And Moilanen and Pulman (2007) develop a collection of composition rules to assign a sentiment value to individual expressions, clauses, or sentences. Their approach can be viewed as a type of structural inference, but their hand-written rules have not been empirically compared to learning-based alternatives, which one might expect to be more effective in hand</context>
</contexts>
<marker>Dowty, Wall, Peters, 1981</marker>
<rawString>David R. Dowty, Robert E. Wall and Stanley Peters. 1981. Introduction to Montague Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings of 5th Conference on Language Resources and Evaluation (LREC),.</booktitle>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining. In Proceedings of 5th Conference on Language Resources and Evaluation (LREC),.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daum´e</author>
<author>Dan Klein</author>
</authors>
<title>Structure Compilation: Trading Structure for Features.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<marker>Liang, Daum´e, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daum´e III and Dan Klein. 2008. Structure Compilation: Trading Structure for Features. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD-2004).</booktitle>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters.</title>
<date>2005</date>
<booktitle>In Proceedings of FINEXIN 2005, Workshop on the Analysis of Informal and Formal Information Exchange during Negotiations.</booktitle>
<contexts>
<context position="3070" citStr="Kennedy and Inkpen (2005)" startWordPosition="457" endWordPosition="460">erall sentence. These examples demonstrate that words or constituents interact with each other to yield the expression-level polarity. And a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples. Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features. One approach includes features based on contextual valence shifters1 (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy and Inkpen (2005), Wilson et al. (2005), Shaikh et al. (2007)). Another approach encodes frequent subsentential patterns (e.g., McDonald et al. (2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity. How1For instance, “never”, “nowhere”, “little”, “most”, “lack”, “scarcely”, “deeply”. 793 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, Honolulu, October 2008.c�2008 Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some str</context>
</contexts>
<marker>Kennedy, Inkpen, 2005</marker>
<rawString>Alistair Kennedy and Diana Inkpen. 2005. Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters. In Proceedings of FINEXIN 2005, Workshop on the Analysis of Informal and Formal Information Exchange during Negotiations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured Models for Fine-to-Coarse Sentiment Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL) .</booktitle>
<contexts>
<context position="3203" citStr="McDonald et al. (2007)" startWordPosition="477" endWordPosition="480">nd a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples. Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features. One approach includes features based on contextual valence shifters1 (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy and Inkpen (2005), Wilson et al. (2005), Shaikh et al. (2007)). Another approach encodes frequent subsentential patterns (e.g., McDonald et al. (2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity. How1For instance, “never”, “nowhere”, “little”, “most”, “lack”, “scarcely”, “deeply”. 793 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, Honolulu, October 2008.c�2008 Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some structural information can be encoded as higher order features, but the final representation of the input is still a flat feature vector</context>
<context position="29709" citStr="McDonald et al. (2007)" startWordPosition="4803" endWordPosition="4806">ce level, (2) the sentences are shorter, with simpler structure, and not as many negators as the MPQA sentences, and (3) there are many more instances with positive polarity than in the MPQA corpus. Nairn et al. (2006) also employ a “polarity” propagation algorithm in their approach to the semantic interpretation of implicatives. However, their notion of polarity is quite different from that assumed here and in the literature on sentiment analysis. In particular, it refers to the degree of “commitment” of the author to the truth or falsity of a complement clause for a textual entailment task. McDonald et al. (2007) use a structured model to determine the sentence-level polarity and the document-level polarity simultaneously. But decisions at each sentence level does not consider structural inference within the sentence. Among the studies that examined content-word negators, Niu et al. (2005) manually collected a small set of such words (referred as “words that change phases”), but their lexicon was designed mainly for the medical domain and the type of negators was rather limited. Wilson et al. (2005) also manually collected a handful of content-word negators (referred as “general polarity shifters”), b</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells and Jeff Reynar. 2007. Structured Models for Fine-to-Coarse Sentiment Analysis. In Proceedings of Association for Computational Linguistics (ACL) .</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>In Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="12981" citStr="Miller, 1995" startWordPosition="2023" endWordPosition="2024">quirer dictionary.6 In particular, a word contained in at least two of the following categories is considered as positive: POSITIV, PSTV, POSAFF, PLEASUR, VIRTUE, INCREAS, and a word contained in at least one of the following categories is considered as negative: NEGATIV, NGTV, NEGAFF, PAIN, VICE, HOSTILE, FAIL, ENLLOSS, WLBLOSS, TRANLOSS. For the (function- and content-word) negator lexicon, we collect a handful of seed words as well as General Inquirer words that appear in either NOTLW or DECREAS category. Then we expand the list of content-negators using the synonym information of WordNet (Miller, 1995) to take a simple vote among senses. based on parse trees might further improve the performance. 4Moilanen and Pulman (2007) provide more detailed discussion on the semantic scope of negations and the semantic priorities in resolving polarities. 5The majority polarity of the data we use for our experiments is negative. 6Available at http://www.wjh.harvard.edu/∼inquirer/. When consulting the General Inquirer dictionary, senses with less than 5% frequency and senses specific to an idiom are dropped. 3 Learning-Based Methods While we expect that a set of hand-written heuristic rules motivated by </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. In Communications of the ACM, 38(11):3941</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Formal Philosophy; Selected papers of Richard Montague.</title>
<date>1974</date>
<publisher>Yale University Press.</publisher>
<contexts>
<context position="4131" citStr="Montague (1974)" startWordPosition="611" endWordPosition="612">ober 2008.c�2008 Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some structural information can be encoded as higher order features, but the final representation of the input is still a flat feature vector that is inherently too limited to adequately reflect the complex structural nature of the underlying subsentential interactions. (Liang et al., 2008) Moilanen and Pulman (2007), on the other hand, handle the structural nature of the interactions more directly using the ideas from compositional semantics (e.g., Montague (1974), Dowty et al. (1981)). In short, the Principle of Compositionality states that the meaning of a compound expression is a function of the meaning of its parts and of the syntactic rules by which they are combined (e.g., Montague (1974), Dowty et al. (1981)). And Moilanen and Pulman (2007) develop a collection of composition rules to assign a sentiment value to individual expressions, clauses, or sentences. Their approach can be viewed as a type of structural inference, but their hand-written rules have not been empirically compared to learning-based alternatives, which one might expect to be m</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Richard Montague. 1974. Formal Philosophy; Selected papers of Richard Montague. Yale University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment Composition.</title>
<date>2007</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP</booktitle>
<contexts>
<context position="3980" citStr="Moilanen and Pulman (2007)" startWordPosition="586" endWordPosition="589">e”, “most”, “lack”, “scarcely”, “deeply”. 793 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, Honolulu, October 2008.c�2008 Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some structural information can be encoded as higher order features, but the final representation of the input is still a flat feature vector that is inherently too limited to adequately reflect the complex structural nature of the underlying subsentential interactions. (Liang et al., 2008) Moilanen and Pulman (2007), on the other hand, handle the structural nature of the interactions more directly using the ideas from compositional semantics (e.g., Montague (1974), Dowty et al. (1981)). In short, the Principle of Compositionality states that the meaning of a compound expression is a function of the meaning of its parts and of the syntactic rules by which they are combined (e.g., Montague (1974), Dowty et al. (1981)). And Moilanen and Pulman (2007) develop a collection of composition rules to assign a sentiment value to individual expressions, clauses, or sentences. Their approach can be viewed as a type </context>
<context position="6482" citStr="Moilanen and Pulman (2007)" startWordPosition="974" endWordPosition="977">rate or missing rules. The learningbased approach proposed in this paper takes a first step in this direction. In addition to the novel learning approach, this paper presents new insights for content-word negators, which we define as content words that can negate the polarity of neighboring words or constituents. (e.g., words such as “eliminated” in the example sentences). Unlike function-word negators, such as “not” or “never”, content-word negators have been recognized and utilized less actively in previous work. (Notable exceptions include e.g., Niu et al. (2005), Wilson et al. (2005), and Moilanen and Pulman (2007).2) In our experiments, we compare learning- and non-learning-based approaches to expression-level polarity classification — with and without compositional semantics — and find that (1) simple heuristics based on compositional semantics outperform (89.7% in accuracy) other reasonable heuristics that do not incorporate compositional semantics (87.7%); they can also perform better than simple learning-based methods that do not incorporate compositional semantics (89.1%), (2) combining learning with the heuristic rules based on compositional semantics further improves the performance (90.7%), (3)</context>
<context position="13105" citStr="Moilanen and Pulman (2007)" startWordPosition="2041" endWordPosition="2044">s positive: POSITIV, PSTV, POSAFF, PLEASUR, VIRTUE, INCREAS, and a word contained in at least one of the following categories is considered as negative: NEGATIV, NGTV, NEGAFF, PAIN, VICE, HOSTILE, FAIL, ENLLOSS, WLBLOSS, TRANLOSS. For the (function- and content-word) negator lexicon, we collect a handful of seed words as well as General Inquirer words that appear in either NOTLW or DECREAS category. Then we expand the list of content-negators using the synonym information of WordNet (Miller, 1995) to take a simple vote among senses. based on parse trees might further improve the performance. 4Moilanen and Pulman (2007) provide more detailed discussion on the semantic scope of negations and the semantic priorities in resolving polarities. 5The majority polarity of the data we use for our experiments is negative. 6Available at http://www.wjh.harvard.edu/∼inquirer/. When consulting the General Inquirer dictionary, senses with less than 5% frequency and senses specific to an idiom are dropped. 3 Learning-Based Methods While we expect that a set of hand-written heuristic rules motivated by compositional semantics can be effective for determining the polarity of a sentimentbearing expression, we do not expect the</context>
<context position="28302" citStr="Moilanen and Pulman (2007)" startWordPosition="4572" endWordPosition="4575">nce the boundaries are relaxed. 5 Related Work The task focused on in this paper is similar to that of Wilson et al. (2005) in that the general goal of the task is to determine the polarity in context at a subsentence level. However, Wilson et al. (2005) formulated the task differently by limiting their evaluation to individual words that appear in their polarity lexicon. Also, their approach was based on a flat bag of features, and only a few examples of what we call content-word negators were employed. Our use of compositional semantics for the task of polarity classification is preceded by Moilanen and Pulman (2007), but our work differs in that we integrate the key idea of compositional semantics into learning-based methods, and that we perform empirical comparisons among reasonable alternative approaches. For comparison, we evaluated our approaches on the polarity classification task from SemEval-07 (Strapparava and Mihalcea, 2007). We achieve 88.6% accuracy with COMPOPR, 90.1% with SCNEGEX, and 87.6% with CCICOMPOMC.9 There are a number of possible reasons for our lower performance vs. Moilanen and Pulman (2007) on this data set. First, SemEval-07 does not include a training data set for this task, so</context>
<context position="30355" citStr="Moilanen and Pulman (2007)" startWordPosition="4902" endWordPosition="4905">del to determine the sentence-level polarity and the document-level polarity simultaneously. But decisions at each sentence level does not consider structural inference within the sentence. Among the studies that examined content-word negators, Niu et al. (2005) manually collected a small set of such words (referred as “words that change phases”), but their lexicon was designed mainly for the medical domain and the type of negators was rather limited. Wilson et al. (2005) also manually collected a handful of content-word negators (referred as “general polarity shifters”), but not extensively. Moilanen and Pulman (2007) collected a more extensive set of negators semi-automatically using WordNet 2.1, but the empirical effect of such words was not explicitly investigated. 9For lack of space, we only report our performance on instances with strong intensities as defined in Moilanen and Pulman (2007), which amounts to only 208 test instances. The cross-validation set of MPQA contains 4.9k instances. 800 6 Conclusion In this paper, we consider the task of determining the polarity of a sentiment-bearing expression, considering the effect of interactions among words or constituents in light of compositional semanti</context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Karo Moilanen and Stephen Pulman. 2007. Sentiment Composition. In Proceedings of Recent Advances in Natural Language Processing (RANLP 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rowan Nairn</author>
</authors>
<title>Cleo Condoravdi and Lauri Karttunen</title>
<date>2006</date>
<booktitle>In Inference in Computational Semantics (ICoS5).</booktitle>
<marker>Nairn, 2006</marker>
<rawString>Rowan Nairn, Cleo Condoravdi and Lauri Karttunen 2006. Computing relative polarity for textual inference. In Inference in Computational Semantics (ICoS5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Niu</author>
<author>Xiaodan Zhu</author>
<author>Jianhua Li</author>
<author>Graeme Hirst</author>
</authors>
<title>Analysis of polarity information in medical text.</title>
<date>2005</date>
<booktitle>In Proceedings of the American Medical Informatics Association 2005 Annual Symposium (AMIA).</booktitle>
<contexts>
<context position="6428" citStr="Niu et al. (2005)" startWordPosition="965" endWordPosition="968"> words, missing semantic features, and inaccurate or missing rules. The learningbased approach proposed in this paper takes a first step in this direction. In addition to the novel learning approach, this paper presents new insights for content-word negators, which we define as content words that can negate the polarity of neighboring words or constituents. (e.g., words such as “eliminated” in the example sentences). Unlike function-word negators, such as “not” or “never”, content-word negators have been recognized and utilized less actively in previous work. (Notable exceptions include e.g., Niu et al. (2005), Wilson et al. (2005), and Moilanen and Pulman (2007).2) In our experiments, we compare learning- and non-learning-based approaches to expression-level polarity classification — with and without compositional semantics — and find that (1) simple heuristics based on compositional semantics outperform (89.7% in accuracy) other reasonable heuristics that do not incorporate compositional semantics (87.7%); they can also perform better than simple learning-based methods that do not incorporate compositional semantics (89.1%), (2) combining learning with the heuristic rules based on compositional s</context>
<context position="29991" citStr="Niu et al. (2005)" startWordPosition="4844" endWordPosition="4847">the semantic interpretation of implicatives. However, their notion of polarity is quite different from that assumed here and in the literature on sentiment analysis. In particular, it refers to the degree of “commitment” of the author to the truth or falsity of a complement clause for a textual entailment task. McDonald et al. (2007) use a structured model to determine the sentence-level polarity and the document-level polarity simultaneously. But decisions at each sentence level does not consider structural inference within the sentence. Among the studies that examined content-word negators, Niu et al. (2005) manually collected a small set of such words (referred as “words that change phases”), but their lexicon was designed mainly for the medical domain and the type of negators was rather limited. Wilson et al. (2005) also manually collected a handful of content-word negators (referred as “general polarity shifters”), but not extensively. Moilanen and Pulman (2007) collected a more extensive set of negators semi-automatically using WordNet 2.1, but the empirical effect of such words was not explicitly investigated. 9For lack of space, we only report our performance on instances with strong intens</context>
</contexts>
<marker>Niu, Zhu, Li, Hirst, 2005</marker>
<rawString>Yun Niu, Xiaodan Zhu, Jianhua Li and Graeme Hirst. 2005. Analysis of polarity information in medical text. In Proceedings of the American Medical Informatics Association 2005 Annual Symposium (AMIA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Annie Zaenen</author>
</authors>
<title>Contextual lexical valence shifters.</title>
<date>2004</date>
<booktitle>In Exploring Attitude and Affect in Text: Theories and Applications: Papers from the 2004 Spring Symposium, AAAI.</booktitle>
<contexts>
<context position="2941" citStr="Polanyi and Zaenen, 2004" startWordPosition="436" endWordPosition="439">wo negators – “not” and “eliminated” – which reverse the polarity of “doubt” twice, resulting in the negative polarity for the overall sentence. These examples demonstrate that words or constituents interact with each other to yield the expression-level polarity. And a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples. Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features. One approach includes features based on contextual valence shifters1 (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy and Inkpen (2005), Wilson et al. (2005), Shaikh et al. (2007)). Another approach encodes frequent subsentential patterns (e.g., McDonald et al. (2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity. How1For instance, “never”, “nowhere”, “little”, “most”, “lack”, “scarcely”, “deeply”. 793 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, Honolulu, October 2008.c�2008 Associat</context>
</contexts>
<marker>Polanyi, Zaenen, 2004</marker>
<rawString>Livia Polanyi and Annie Zaenen. 2004. Contextual lexical valence shifters. In Exploring Attitude and Affect in Text: Theories and Applications: Papers from the 2004 Spring Symposium, AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mostafa Shaikh</author>
</authors>
<title>Helmut Prendinger and Mitsuru Ishizuka.</title>
<date>2007</date>
<booktitle>In Proc 2nd Int’l Conf on Affective Computing and Intelligent Interaction (ACII’07).</booktitle>
<marker>Shaikh, 2007</marker>
<rawString>Mostafa Shaikh, Helmut Prendinger and Mitsuru Ishizuka. 2007. Assessing sentiment of text by semantic dependency and contextual valence analysis. In Proc 2nd Int’l Conf on Affective Computing and Intelligent Interaction (ACII’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Rada Mihalcea</author>
</authors>
<title>Semeval2007 task 14: Affective text.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="28626" citStr="Strapparava and Mihalcea, 2007" startWordPosition="4620" endWordPosition="4623">individual words that appear in their polarity lexicon. Also, their approach was based on a flat bag of features, and only a few examples of what we call content-word negators were employed. Our use of compositional semantics for the task of polarity classification is preceded by Moilanen and Pulman (2007), but our work differs in that we integrate the key idea of compositional semantics into learning-based methods, and that we perform empirical comparisons among reasonable alternative approaches. For comparison, we evaluated our approaches on the polarity classification task from SemEval-07 (Strapparava and Mihalcea, 2007). We achieve 88.6% accuracy with COMPOPR, 90.1% with SCNEGEX, and 87.6% with CCICOMPOMC.9 There are a number of possible reasons for our lower performance vs. Moilanen and Pulman (2007) on this data set. First, SemEval-07 does not include a training data set for this task, so we use 400 documents from the MPQA corpus instead. In addition, the SemEval-07 data is very different from the MPQA data in that (1) the polarity annotation is given only at the sentence level, (2) the sentences are shorter, with simpler structure, and not as many negators as the MPQA sentences, and (3) there are many mor</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>Carlo Strapparava and Rada Mihalcea. 2007. Semeval2007 task 14: Affective text. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<booktitle>In Language Resources and Evaluation (formerly Computers and the Humanities),</booktitle>
<pages>39--2</pages>
<contexts>
<context position="22257" citStr="Wiebe et al., 2005" startWordPosition="3608" endWordPosition="3611">periment with two variations of this learning-based approach: CCI-COMPOPR and CCI-COMPOMC, whose compositional inference rules are COMPOPR and COMPOMC respectively. For brevity, we refer to both variations collectively as CCI-COMPO. 4 Experiments The experiments below evaluate our heuristic- and learning-based methods for subsentential sentiment analysis (§ 4.1). In addition, we explore the role of context by expanding the boundaries of the sentiment-bearing expressions (§ 4.2). 4.1 Evaluation with given boundaries For evaluation, we use the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005), which consists of 535 newswire documents manually annotated with phrase-level subjectivity information. We evaluate on all strong (i.e., intensity of expression is ‘medium’ or higher), sentimentbearing (i.e., polarity is ‘positive’ or ‘negative’) expressions.8 As a result, we can assume the boundaries of the expressions are given. Performance is reported using 10-fold cross-validation on 400 documents; a separate 135 documents were used as a development set. Based on pilot experiments on the development data, we set parameters for MIRA as follows: slack variable to 0.5, and the number of inc</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. In Language Resources and Evaluation (formerly Computers and the Humanities), 39(2-3):165210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="3092" citStr="Wilson et al. (2005)" startWordPosition="461" endWordPosition="464">les demonstrate that words or constituents interact with each other to yield the expression-level polarity. And a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples. Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features. One approach includes features based on contextual valence shifters1 (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy and Inkpen (2005), Wilson et al. (2005), Shaikh et al. (2007)). Another approach encodes frequent subsentential patterns (e.g., McDonald et al. (2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity. How1For instance, “never”, “nowhere”, “little”, “most”, “lack”, “scarcely”, “deeply”. 793 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, Honolulu, October 2008.c�2008 Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some structural information ca</context>
<context position="6450" citStr="Wilson et al. (2005)" startWordPosition="969" endWordPosition="972">antic features, and inaccurate or missing rules. The learningbased approach proposed in this paper takes a first step in this direction. In addition to the novel learning approach, this paper presents new insights for content-word negators, which we define as content words that can negate the polarity of neighboring words or constituents. (e.g., words such as “eliminated” in the example sentences). Unlike function-word negators, such as “not” or “never”, content-word negators have been recognized and utilized less actively in previous work. (Notable exceptions include e.g., Niu et al. (2005), Wilson et al. (2005), and Moilanen and Pulman (2007).2) In our experiments, we compare learning- and non-learning-based approaches to expression-level polarity classification — with and without compositional semantics — and find that (1) simple heuristics based on compositional semantics outperform (89.7% in accuracy) other reasonable heuristics that do not incorporate compositional semantics (87.7%); they can also perform better than simple learning-based methods that do not incorporate compositional semantics (89.1%), (2) combining learning with the heuristic rules based on compositional semantics further impro</context>
<context position="12329" citStr="Wilson et al. (2005)" startWordPosition="1918" endWordPosition="1921"> in general is not in the semantic scope of the negation.4 Instead, we treat the second argument as a constituent with negative polarity. We experiment with two variations of the Compose function depending on how conflicting polarities are resolved: COMPOMC uses a Compose function that defaults to the Majority Class of the polarity of the data,5 while COMPOPR uses a Compose function that selects the polarity of the argument that has higher semantic PRiority. For brevity, we refer to COMPOPR and COMPOMC collectively as COMPO. 2.3 Lexicons The polarity lexicon is initialized with the lexicon of Wilson et al. (2005) and then expanded using the General Inquirer dictionary.6 In particular, a word contained in at least two of the following categories is considered as positive: POSITIV, PSTV, POSAFF, PLEASUR, VIRTUE, INCREAS, and a word contained in at least one of the following categories is considered as negative: NEGATIV, NGTV, NEGAFF, PAIN, VICE, HOSTILE, FAIL, ENLLOSS, WLBLOSS, TRANLOSS. For the (function- and content-word) negator lexicon, we collect a handful of seed words as well as General Inquirer words that appear in either NOTLW or DECREAS category. Then we expand the list of content-negators usi</context>
<context position="21329" citStr="Wilson et al. (2005)" startWordPosition="3468" endWordPosition="3471">96). We also add a boolean feature to indicate whether the current word is a stop word. • Dictionary: In order to mitigate the problem with unseen words in the test data, we add features that describe word categories based on the General Inquirer dictionary. We add this feature for each xi that is not a stop word. We also add a number of boolean features that provide following properties of xi using the polarity lexicon and the negator lexicon: – whether xi is a function-word negator – whether xi is a content-word negator – whether xi is a negator of any kind – the polarity of xi according to Wilson et al. (2005)’s polarity lexicon – the polarity of xi according to the lexicon derived from the General Inquirer dictionary – conjunction of the above two features • Vote: We encode the same vote feature that we use for SC-NEGEX described in § 3.1. As in the heuristic-based compositional semantics approach (§ 2.2), we experiment with two variations of this learning-based approach: CCI-COMPOPR and CCI-COMPOMC, whose compositional inference rules are COMPOPR and COMPOMC respectively. For brevity, we refer to both variations collectively as CCI-COMPO. 4 Experiments The experiments below evaluate our heuristic</context>
<context position="25677" citStr="Wilson et al. (2005)" startWordPosition="4144" endWordPosition="4147">y paired t-test. The difference between COMPO and any other heuristic that is not based on computational semantics is also statistically significant. In addition, the difference between CCICOMPOPR (learning-based) and COMPOMC (non-learning-based) is statistically significant, as is the difference between NEGEX and VOTE. 4.2 Evaluation with noisy boundaries One might wonder whether employing additional context outside the annotated expression boundaries could further improve the performance. Indeed, conventional wisdom would say that it is necessary to employ such contextual information (e.g., Wilson et al. (2005)). In any case, it is important to determine whether our results will apply to more real-world settings where human-annotated expression boundaries are not available. To address these questions, we gradually relax our previous assumption that the exact boundaries of expressions are given: for each annotation boundary, we expand the boundary by x words for each direction, up to sentence boundaries, where x E 11, 5, ocI. We stop expanding the boundary if it will collide with the boundary of an expression with a different polarity, so that we can consistently recover the expression-level gold sta</context>
<context position="27799" citStr="Wilson et al. (2005)" startWordPosition="4486" endWordPosition="4489">methods as the expression boundaries expand. We conjecture this is because methods based on compositional semantics can handle the scope of negators more adequately. • Among the learning-based methods, those that involve compositional inference (CCI-COMPO) always perform better than those that do not (SC) for any boundaries. And learning with compositional inference tend to perform better than the rigid application of heuristic rules (COMPO), although the relative performance gain decreases once the boundaries are relaxed. 5 Related Work The task focused on in this paper is similar to that of Wilson et al. (2005) in that the general goal of the task is to determine the polarity in context at a subsentence level. However, Wilson et al. (2005) formulated the task differently by limiting their evaluation to individual words that appear in their polarity lexicon. Also, their approach was based on a flat bag of features, and only a few examples of what we call content-word negators were employed. Our use of compositional semantics for the task of polarity classification is preceded by Moilanen and Pulman (2007), but our work differs in that we integrate the key idea of compositional semantics into learning</context>
<context position="30205" citStr="Wilson et al. (2005)" startWordPosition="4881" endWordPosition="4884">mmitment” of the author to the truth or falsity of a complement clause for a textual entailment task. McDonald et al. (2007) use a structured model to determine the sentence-level polarity and the document-level polarity simultaneously. But decisions at each sentence level does not consider structural inference within the sentence. Among the studies that examined content-word negators, Niu et al. (2005) manually collected a small set of such words (referred as “words that change phases”), but their lexicon was designed mainly for the medical domain and the type of negators was rather limited. Wilson et al. (2005) also manually collected a handful of content-word negators (referred as “general polarity shifters”), but not extensively. Moilanen and Pulman (2007) collected a more extensive set of negators semi-automatically using WordNet 2.1, but the empirical effect of such words was not explicitly investigated. 9For lack of space, we only report our performance on instances with strong intensities as defined in Moilanen and Pulman (2007), which amounts to only 208 test instances. The cross-validation set of MPQA contains 4.9k instances. 800 6 Conclusion In this paper, we consider the task of determinin</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HLT/EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>