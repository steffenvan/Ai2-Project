<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002677">
<title confidence="0.991741">
Hybrid Document Indexing with Spectral Embedding
</title>
<author confidence="0.99701">
Irina Matveeva
</author>
<affiliation confidence="0.998345">
Department of Computer Science
University of Chicago
</affiliation>
<address confidence="0.808753">
Chicago, IL 60637
</address>
<email confidence="0.998927">
matveeva@cs.uchicago.edu
</email>
<author confidence="0.984239">
Gina-Anne Levow
</author>
<affiliation confidence="0.9982805">
Department of Computer Science
University of Chicago
</affiliation>
<address confidence="0.807797">
Chicago, IL 60637
</address>
<email confidence="0.997266">
levow@cs.uchicago.edu
</email>
<sectionHeader confidence="0.995598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946571428571">
Document representation has a large im-
pact on the performance of document re-
trieval and clustering algorithms. We
propose a hybrid document indexing
scheme that combines the traditional bag-
of-words representation with spectral em-
bedding. This method accounts for the
specifics of the document collection and
also uses semantic similarity information
based on a large scale statistical analysis.
Clustering experiments showed improve-
ments over the traditional tf-idf represen-
tation and over the spectral methods based
solely on the document collection.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869305555556">
Capturing semantic relations between words in a
document representation is a difficult problem. Dif-
ferent approaches tried to overcome the term inde-
pendence assumption of the bag-of-words represen-
tation (Salton and McGill, 1983) for example by us-
ing distributional term clusters (Slonim and Tishby,
2000) and expanding the document vectors with
synonyms, see (Levow et al., 2005). Since content
words can be combined into semantic classes there
has been a considerable interest in low-dimensional
term and document representations.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is one of the best known dimensionality
reduction algorithms. In the LSA space documents
are indexed with latent semantic concepts. LSA
showed large performance improvements over the
traditional tf-idf representation on small document
collections (Deerwester et al., 1990) but often does
not perform well on large heterogeneous collections.
LSA maps all words to low dimensional vectors.
However, the notion of semantic relatedness is de-
fined differently for subsets of the vocabulary. In ad-
dition, the numerical information, abbreviations and
the documents’ style may be very good indicators of
their topic. However, this information is no longer
available after the dimensionality reduction.
We use a hybrid approach to document indexing
to address these issues. We keep the notion of la-
tent semantic concepts and also try to preserve the
specifics of the document collection. We use a low-
dimensional representation only for nouns and rep-
resent the rest of the document’s content as tf-idf
vectors.
The rest of the paper is organized as follows. Sec-
tion 2 discusses our approach. Section 3 reports the
experimental results. We conclude in section 4.
</bodyText>
<sectionHeader confidence="0.967579" genericHeader="method">
2 Hybrid Document Indexing
</sectionHeader>
<bodyText confidence="0.999980272727273">
This section gives the general idea of our approach.
We divide the vocabulary into two sets: nouns and
the rest of the vocabulary. We use a method of spec-
tral embedding, as described below and compute a
low-dimensional representation for documents using
only the nouns. We also compute a tf-idf represen-
tation for documents using the other set of words.
Since we can treat each latent semantic concept in
the low-dimensional representation as part of the vo-
cabulary, we combine the two vector representations
for each document by concatenating them.
</bodyText>
<page confidence="0.990931">
113
</page>
<note confidence="0.521553">
Proceedings of NAACL HLT 2007, Companion Volume, pages 113–116,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.994207">
2.1 Spectral Embedding
</subsectionHeader>
<bodyText confidence="0.999966090909091">
Spectral methods comprise a family of algorithms
that use a matrix of pair-wise similarities S and per-
form its spectral analysis, such as the eigenvalue de-
composition, to embed terms and documents in a
low-dimensional vector space. S = UEUT, where
the columns of U are its eigenvectors and E is a di-
agonal matrix with the eigenvalues.
If we have a matrix of pair-wise word similarities
S, its first k eigenvectors Uk will be used to repre-
sent the words in the latent semantic space. Seman-
tically related words will have high association with
the same latent concepts and their corresponding
vectors will be similar. Moreover, the vector similar-
ity between the word vectors will optimally preserve
the original similarities (Cox and Cox, 2001).
We use two approaches to compute spectral em-
bedding for nouns. Latent Semantic Analysis
(LSA) (Deerwester et al., 1990) and Generalized La-
tent Semantic Analysis (GLSA) (Matveeva et al.,
2005). For both we used the eigenvalue decomposi-
tion as the embedding step. The difference is in the
similarities matrix which we are trying to preserve.
</bodyText>
<subsectionHeader confidence="0.997992">
2.2 Distributional Term Similarity
</subsectionHeader>
<bodyText confidence="0.999980384615385">
LSA and GLSA begin with a matrix of pair-wise
term similarities S, compute its eigenvectors U and
use the first k of them to represent terms and doc-
uments, for details see (Deerwester et al., 1990;
Matveeva et al., 2005). The main difference in our
implementation of these algorithms is the matrix of
pair-wise word similarities. Since our representation
will try to preserve them it is important to have a ma-
trix of similarities which is linguistically motivated.
LSA uses the matrix of pair-wise similarities
which is based on document vectors. For two words
wi and wj in the document collection containing n
documents dk, the similarity is computed as
</bodyText>
<equation confidence="0.981924666666667">
S(wi,wj) =
E tf(wi, dk)idf(wi) * tf(wj, dk)idf(wj),
k=1:n
</equation>
<bodyText confidence="0.999957933333333">
where tf(wi, dk) is the term frequency for wi in
dk and idf(wi) is the inverse document frequency
weight for wi. LSA is a special case of spectral em-
bedding restricted to one type of term similarities
and dimensionality reduction method.
GLSA (Matveeva et al., 2005) generalizes the
idea of latent semantic space. It proposes to use
different types of similarity matrix and spectral em-
bedding methods to compute a latent space which is
closer to true semantic similarities. One way to do
so is to use a more appropriate similarities matrix S.
PMI We use point-wise mutual information (PMI)
to compute the matrix S. PMI between random vari-
ables representing the words wi and wj is computed
as
</bodyText>
<equation confidence="0.999941">
P(Wi = 1, Wj = 1)
PMI(wi, wj) = log
P(Wi = 1)P(Wj = 1).
</equation>
<bodyText confidence="0.85663525">
Thus, for GLSA, S(wi, wj) = PMI(wi, wj).
Co-occurrence Proximity An advantage of PMI
is the notion of proximity. The co-occurrence statis-
tics for PMI are typically computed using a sliding
window. Thus, PMI will be large only for words
that co-occur within a small fixed context. Our ex-
periments show that this is a better approximation to
true semantic similarities.
</bodyText>
<subsectionHeader confidence="0.996248">
2.3 Document Indexing
</subsectionHeader>
<bodyText confidence="0.999849">
We have two sets of the vocabulary terms: a set of
nouns, N, and the other words, T. We compute tf-idf
document vectors indexed with the words in T:
</bodyText>
<equation confidence="0.719929">
di = (αi(w1),αi(w2),...,αi(w|T|)),
</equation>
<bodyText confidence="0.99763225">
where αi(wt) = tf(wt, di) * idf(wt).
We also compute a k-dimensional representation
with latent concepts ci as a weighted linear combi-
nation of LSA or GLSA term vectors ~wt:
</bodyText>
<equation confidence="0.9992405">
�~di = (c1, ..., ck) = αi(wt) * ~wt,
t=1:|T |
</equation>
<bodyText confidence="0.999834">
We concatenate these two representations to gener-
ate a hybrid indexing of documents:
</bodyText>
<equation confidence="0.650337">
di = (αi(w1),...,αi(w|T|),c1,...ck)
</equation>
<sectionHeader confidence="0.999319" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999258">
We performed document clustering experiments to
validate our approach.
</bodyText>
<page confidence="0.987073">
114
</page>
<table confidence="0.993264">
Subset m-n #topics min #d max #d av. #d
5-10 19 6 10 8.2
50-150 21 55 150 94.7
500-1000 2 544 844 694.0
1000-5000 3 1367 2083 1792.3
</table>
<tableCaption confidence="0.992834">
Table 1: TDT2 topic subsets containing between m
</tableCaption>
<bodyText confidence="0.979763333333333">
and n documents: the number of topics per subset,
the minimum, the maximum and the average number
of documents per topic in each subset.
</bodyText>
<table confidence="0.99744725">
Indexing
All words Nouns Hybrid
tf-idf, LSA tf-idfN tf-idf+GLSAN
GLSA, GLSA local GLSAN
</table>
<tableCaption confidence="0.599896">
Table 2: Indexing schemes: with full vocabulary
(All), only nouns (Nouns) and the combination.
Data We used the TDT2 collection1 of news arti-
cles from six news agencies in 1998. We used only
10,329 documents that are assigned to one topic.
TDT2 documents are distributed over topics very
</tableCaption>
<bodyText confidence="0.996355238095238">
unevenly. We used subsets of the TDT2 topics that
contain between m and n documents, see Table 1.
We used the Lemur toolkit2 with stemming and stop
words list for the tf-idf indexing, Bikel’s parser3 to
obtain the set of nouns and the PLAPACK pack-
age (Bientinesi et al., 2003) to compute the eigen-
value decomposition.
Global vs. Local Similarity To obtain the PMI
values for GLSA we used the TDT2 collection, de-
noted as GLSAlocal. Since co-occurrence statistics
based on larger collections gives abetter approxima-
tion to linguistic similarities, we also used 700,000
documents from the English GigaWord collection,
denoted as GLSA and GLSAN. We used a window
of size 8.
Representations For each document we com-
puted 7 representations, see Table 2. The vocabulary
size we used with the tf-idf indexing was 114,127.
For computational reasons we used the set of words
that occurred in at least 20 documents with our spec-
tral methods. We used 17,633 words for index-
</bodyText>
<footnote confidence="0.999833333333333">
1http://nist.gov/speech/tests/tdt/tdt98/
2http://www.lemurproject.org/
3http://www.cis.upenn.edu/ dbikel/software.html
</footnote>
<bodyText confidence="0.970174470588235">
ing with LSA and GLSAlocal and 17,572 words for
GLSA. We also indexed documents using only the
15,325 nouns: tf-idfN and GLSAN. The hybrid rep-
resentation was computed using the tf-idf indexing
without nouns and the GLSAN nouns vectors.
Evaluation We used the minimum squared
residue co-clustering algorithm 4. We report two
evaluation measures: accuracy and the F1-score.
The clustering algorithm assigns each document to
a cluster. We map the cluster id’s to topic labels
using the Munkres assignment algorithm (Munkres,
1957) and compute the accuracy as the ratio of the
correctly assigned labels.
The F1 score for cluster ci labeled with topic ti is
computed using F1 = 2(p∗r) (p+r)where p is precision
and r is recall. For clusters C = (c1, ..., cn) and
topics T = (t1, ..., tn) we compute the total score:
</bodyText>
<equation confidence="0.76513">
Nt max F1(c, t).
N c∈C
</equation>
<bodyText confidence="0.9998785">
Nt is the number of documents belonging to the
topic t and N is the total number of documents. This
measure accounts for the topic size and also corrects
the topic assignments to clusters by using the max.
</bodyText>
<sectionHeader confidence="0.99886" genericHeader="evaluation">
4 Results and Conclusion
</sectionHeader>
<bodyText confidence="0.9983871">
Table 3 shows that the spectral methods outperform
the tf-idf representations and have smaller variance.
We report the performance for four subsets. The
subset 5−10 has a large number of topics, each with
a similar number of documents. The subset 50−150
has a large number of topics with a less even distri-
bution of documents. 500 − 1000 and 1000 − 5000
have a couple of large topics. We ran the clustering
over 30 random initializations. To eliminate the ef-
fect of the initial conditions on the performance we
also used one document per cluster to seed the initial
assignment for the 5 − 10 subset.
All methods have the worst performance for the
5−10 subset. The best performance is for the subset
500−1000. LSA and GLSAlocal indexing are com-
puted based on the TDT2 collection. GLSAlocal has
better average performance which confirms that the
co-occurrence proximity is important for distribu-
tional similarity. The GLSA indexing computed us-
ing a large corpus performs significantly worse than
</bodyText>
<footnote confidence="0.6837">
4http://www.cs.utexas.edu/users/dml/Software/cocluster.html
</footnote>
<equation confidence="0.809450333333333">
1]
F1(C,T) =
t∈T
</equation>
<page confidence="0.995354">
115
</page>
<table confidence="0.999852909090909">
All words LSA GLSAlocal GLSA onlyN GLSAN Hybrid
5-10 acc 0.56(0.11) 0.69(0.07) 0.78(0.05) 0.60(0.05) 0.63(0.05) 0.76(0.05) 0.82(0.05)
F1 0.60(0.09) 0.73(0.05) 0.81(0.04) 0.64(0.05) 0.67(0.05) 0.80(0.04) 0.85(0.04)
50-150 acc 0.75(0.05) 0.73(0.06) 0.80(0.05) 0.70(0.04) 0.68(0.04) 0.80(0.04) 0.87(0.04)
F1 0.80(0.04) 0.78(0.05) 0.84(0.04) 0.75(0.04) 0.75(0.03) 0.84(0.04) 0.90(0.03)
500-1000 acc 0.95(0.03) 0.98(0.00) 0.99(0.00) 0.97(0.00) 0.97(0.00) 0.99(0.00) 1.00(0.00)
F1 0.95(0.03) 0.98(0.00) 0.99(0.00) 0.97(0.00) 0.97(0.00) 0.99(0.00) 1.00(0.00)
1000-5000 acc 0.86(0.11) 0.88(0.04) 0.88(0.13) 0.92(0.08) 0.82(0.06) 0.92(0.00) 0.96(0.07)
F1 0.88(0.07) 0.88(0.03) 0.90(0.09) 0.93(0.06) 0.82(0.04) 0.92(0.00) 0.97(0.05)
5-10s acc 0.932 0.919 0.986 0.932 0.980 0.980 0.992
F1 0.933 0.927 0.986 0.932 0.979 0.979 0.992
</table>
<tableCaption confidence="0.999276">
Table 3: Clustering accuracy (first row) and F1 score (second row) for each indexing scheme. The measures
</tableCaption>
<bodyText confidence="0.975280055555556">
are averaged over 30 random initiations of the clustering algorithm, the standard deviation is shown in
brackets. For the last experiment, 5-10s, we used one document per cluster as the initial assignment.
GLSAlocal on the heterogeneous 5−10 and 50−150
subsets and performs similarly for the other two. It
supports our intuition that the document’s style and
word distribution within the collection are important
and may get lost, especially if we use a document
collection with a different word distribution to esti-
mate the similarities matrix S.
The tf-idf indexing with nouns only, onlyN, has
good performance compared to the all-words index-
ing. The semantic similarity between nouns seems
to be collection independent. The GLSAN index-
ing is significantly better than onlyN and tf-idf in
most cases and performs similar to GLSAlocal. By
using GLSAN we computed the embedding for
more nouns that we could keep in the GLSAlocal
and GLSA representations. Nouns convey impor-
tant topic membership information and it is advan-
tageous to use as many of them as possible.
We observed the same performance relation when
we used labels to make the initial cluster assign-
ment, see 5 − 10s in Table 3. tf-idf, GLSA and LSA
performed similarly, GLSAlocal and GLSAN per-
formed better with the hybrid scheme being the best.
The hybrid indexing significantly outperforms tf-
idf, LSA and GLSA on three subsets. This shows the
benefits of using the spectral embedding to discover
the semantic relations between nouns and keeping
the rest of the document content as tf-idf representa-
tion to preserve other indicators of its topic member-
ship. By combining two representations the hybrid
indexing scheme defines a more complex notion of
similarity between documents. For nouns it uses the
semantic proximity in the space of latent semantic
classes and for other words it uses term-matching.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999765407407407">
Paolo Bientinesi, Inderjit S. Dhilon, and Robert A. van de
Geijn. 2003. A parallel eigensolver for dense sym-
metric matrices based on multiple relatively robust
representations. UT CS Technical Report TR-03-26.
Trevor F. Cox and Micheal A. Cox. 2001. Multidimen-
sional Scaling. CRC/Chapman and Hall.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391–407.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management: Special Issue on Cross-language Infor-
mation Retrieval.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized latent semantic
analysis for term representation. In Proc. ofRANLP.
J. Munkres. 1957. Algorithms for the assignment and
transportation problems. SIAM, 5(1):32–38.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Research and Development in Infor-
mation Retrieval, pages 208–215.
</reference>
<page confidence="0.999023">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.256052">
<title confidence="0.999286">Hybrid Document Indexing with Spectral Embedding</title>
<author confidence="0.97442">Irina</author>
<affiliation confidence="0.92149">Department of Computer University of Chicago, IL</affiliation>
<email confidence="0.99796">matveeva@cs.uchicago.edu</email>
<author confidence="0.707231">Gina-Anne</author>
<affiliation confidence="0.9994845">Department of Computer University of</affiliation>
<address confidence="0.498126">Chicago, IL</address>
<email confidence="0.999202">levow@cs.uchicago.edu</email>
<abstract confidence="0.998451866666667">Document representation has a large impact on the performance of document retrieval and clustering algorithms. We propose a hybrid document indexing scheme that combines the traditional bagof-words representation with spectral embedding. This method accounts for the specifics of the document collection and also uses semantic similarity information based on a large scale statistical analysis. Clustering experiments showed improveover the traditional representation and over the spectral methods based solely on the document collection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paolo Bientinesi</author>
<author>Inderjit S Dhilon</author>
<author>Robert A van de Geijn</author>
</authors>
<title>A parallel eigensolver for dense symmetric matrices based on multiple relatively robust representations.</title>
<date>2003</date>
<tech>UT CS Technical Report TR-03-26.</tech>
<marker>Bientinesi, Dhilon, van de Geijn, 2003</marker>
<rawString>Paolo Bientinesi, Inderjit S. Dhilon, and Robert A. van de Geijn. 2003. A parallel eigensolver for dense symmetric matrices based on multiple relatively robust representations. UT CS Technical Report TR-03-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor F Cox</author>
<author>Micheal A Cox</author>
</authors>
<title>Multidimensional Scaling. CRC/Chapman and Hall.</title>
<date>2001</date>
<contexts>
<context position="4072" citStr="Cox and Cox, 2001" startWordPosition="618" endWordPosition="621">ysis, such as the eigenvalue decomposition, to embed terms and documents in a low-dimensional vector space. S = UEUT, where the columns of U are its eigenvectors and E is a diagonal matrix with the eigenvalues. If we have a matrix of pair-wise word similarities S, its first k eigenvectors Uk will be used to represent the words in the latent semantic space. Semantically related words will have high association with the same latent concepts and their corresponding vectors will be similar. Moreover, the vector similarity between the word vectors will optimally preserve the original similarities (Cox and Cox, 2001). We use two approaches to compute spectral embedding for nouns. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and Generalized Latent Semantic Analysis (GLSA) (Matveeva et al., 2005). For both we used the eigenvalue decomposition as the embedding step. The difference is in the similarities matrix which we are trying to preserve. 2.2 Distributional Term Similarity LSA and GLSA begin with a matrix of pair-wise term similarities S, compute its eigenvectors U and use the first k of them to represent terms and documents, for details see (Deerwester et al., 1990; Matveeva et al., 2005). T</context>
</contexts>
<marker>Cox, Cox, 2001</marker>
<rawString>Trevor F. Cox and Micheal A. Cox. 2001. Multidimensional Scaling. CRC/Chapman and Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1433" citStr="Deerwester et al., 1990" startWordPosition="197" endWordPosition="200">document collection. 1 Introduction Capturing semantic relations between words in a document representation is a difficult problem. Different approaches tried to overcome the term independence assumption of the bag-of-words representation (Salton and McGill, 1983) for example by using distributional term clusters (Slonim and Tishby, 2000) and expanding the document vectors with synonyms, see (Levow et al., 2005). Since content words can be combined into semantic classes there has been a considerable interest in low-dimensional term and document representations. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. In the LSA space documents are indexed with latent semantic concepts. LSA showed large performance improvements over the traditional tf-idf representation on small document collections (Deerwester et al., 1990) but often does not perform well on large heterogeneous collections. LSA maps all words to low dimensional vectors. However, the notion of semantic relatedness is defined differently for subsets of the vocabulary. In addition, the numerical information, abbreviations and the documents’ style may be very good indicators of the</context>
<context position="4193" citStr="Deerwester et al., 1990" startWordPosition="637" endWordPosition="640">T, where the columns of U are its eigenvectors and E is a diagonal matrix with the eigenvalues. If we have a matrix of pair-wise word similarities S, its first k eigenvectors Uk will be used to represent the words in the latent semantic space. Semantically related words will have high association with the same latent concepts and their corresponding vectors will be similar. Moreover, the vector similarity between the word vectors will optimally preserve the original similarities (Cox and Cox, 2001). We use two approaches to compute spectral embedding for nouns. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and Generalized Latent Semantic Analysis (GLSA) (Matveeva et al., 2005). For both we used the eigenvalue decomposition as the embedding step. The difference is in the similarities matrix which we are trying to preserve. 2.2 Distributional Term Similarity LSA and GLSA begin with a matrix of pair-wise term similarities S, compute its eigenvectors U and use the first k of them to represent terms and documents, for details see (Deerwester et al., 1990; Matveeva et al., 2005). The main difference in our implementation of these algorithms is the matrix of pair-wise word similarities. Since our repr</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
<author>Douglas W Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Dictionary-based techniques for cross-language information retrieval.</title>
<date>2005</date>
<booktitle>Information Processing and Management: Special Issue on Cross-language Information Retrieval.</booktitle>
<contexts>
<context position="1224" citStr="Levow et al., 2005" startWordPosition="168" endWordPosition="171">similarity information based on a large scale statistical analysis. Clustering experiments showed improvements over the traditional tf-idf representation and over the spectral methods based solely on the document collection. 1 Introduction Capturing semantic relations between words in a document representation is a difficult problem. Different approaches tried to overcome the term independence assumption of the bag-of-words representation (Salton and McGill, 1983) for example by using distributional term clusters (Slonim and Tishby, 2000) and expanding the document vectors with synonyms, see (Levow et al., 2005). Since content words can be combined into semantic classes there has been a considerable interest in low-dimensional term and document representations. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. In the LSA space documents are indexed with latent semantic concepts. LSA showed large performance improvements over the traditional tf-idf representation on small document collections (Deerwester et al., 1990) but often does not perform well on large heterogeneous collections. LSA maps all words to low dimensional vectors. Ho</context>
</contexts>
<marker>Levow, Oard, Resnik, 2005</marker>
<rawString>Gina-Anne Levow, Douglas W. Oard, and Philip Resnik. 2005. Dictionary-based techniques for cross-language information retrieval. Information Processing and Management: Special Issue on Cross-language Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Matveeva</author>
<author>Gina-Anne Levow</author>
<author>Ayman Farahat</author>
<author>Christian Royer</author>
</authors>
<title>Generalized latent semantic analysis for term representation. In</title>
<date>2005</date>
<booktitle>Proc. ofRANLP.</booktitle>
<contexts>
<context position="4265" citStr="Matveeva et al., 2005" startWordPosition="648" endWordPosition="651">with the eigenvalues. If we have a matrix of pair-wise word similarities S, its first k eigenvectors Uk will be used to represent the words in the latent semantic space. Semantically related words will have high association with the same latent concepts and their corresponding vectors will be similar. Moreover, the vector similarity between the word vectors will optimally preserve the original similarities (Cox and Cox, 2001). We use two approaches to compute spectral embedding for nouns. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and Generalized Latent Semantic Analysis (GLSA) (Matveeva et al., 2005). For both we used the eigenvalue decomposition as the embedding step. The difference is in the similarities matrix which we are trying to preserve. 2.2 Distributional Term Similarity LSA and GLSA begin with a matrix of pair-wise term similarities S, compute its eigenvectors U and use the first k of them to represent terms and documents, for details see (Deerwester et al., 1990; Matveeva et al., 2005). The main difference in our implementation of these algorithms is the matrix of pair-wise word similarities. Since our representation will try to preserve them it is important to have a matrix of</context>
</contexts>
<marker>Matveeva, Levow, Farahat, Royer, 2005</marker>
<rawString>Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and Christian Royer. 2005. Generalized latent semantic analysis for term representation. In Proc. ofRANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Munkres</author>
</authors>
<title>Algorithms for the assignment and transportation problems.</title>
<date>1957</date>
<journal>SIAM,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="9216" citStr="Munkres, 1957" startWordPosition="1467" endWordPosition="1468">t/tdt98/ 2http://www.lemurproject.org/ 3http://www.cis.upenn.edu/ dbikel/software.html ing with LSA and GLSAlocal and 17,572 words for GLSA. We also indexed documents using only the 15,325 nouns: tf-idfN and GLSAN. The hybrid representation was computed using the tf-idf indexing without nouns and the GLSAN nouns vectors. Evaluation We used the minimum squared residue co-clustering algorithm 4. We report two evaluation measures: accuracy and the F1-score. The clustering algorithm assigns each document to a cluster. We map the cluster id’s to topic labels using the Munkres assignment algorithm (Munkres, 1957) and compute the accuracy as the ratio of the correctly assigned labels. The F1 score for cluster ci labeled with topic ti is computed using F1 = 2(p∗r) (p+r)where p is precision and r is recall. For clusters C = (c1, ..., cn) and topics T = (t1, ..., tn) we compute the total score: Nt max F1(c, t). N c∈C Nt is the number of documents belonging to the topic t and N is the total number of documents. This measure accounts for the topic size and also corrects the topic assignments to clusters by using the max. 4 Results and Conclusion Table 3 shows that the spectral methods outperform the tf-idf </context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>J. Munkres. 1957. Algorithms for the assignment and transportation problems. SIAM, 5(1):32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="1073" citStr="Salton and McGill, 1983" startWordPosition="144" endWordPosition="147">he traditional bagof-words representation with spectral embedding. This method accounts for the specifics of the document collection and also uses semantic similarity information based on a large scale statistical analysis. Clustering experiments showed improvements over the traditional tf-idf representation and over the spectral methods based solely on the document collection. 1 Introduction Capturing semantic relations between words in a document representation is a difficult problem. Different approaches tried to overcome the term independence assumption of the bag-of-words representation (Salton and McGill, 1983) for example by using distributional term clusters (Slonim and Tishby, 2000) and expanding the document vectors with synonyms, see (Levow et al., 2005). Since content words can be combined into semantic classes there has been a considerable interest in low-dimensional term and document representations. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. In the LSA space documents are indexed with latent semantic concepts. LSA showed large performance improvements over the traditional tf-idf representation on small document coll</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Slonim</author>
<author>Naftali Tishby</author>
</authors>
<title>Document clustering using word clusters via the information bottleneck method.</title>
<date>2000</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>208--215</pages>
<contexts>
<context position="1149" citStr="Slonim and Tishby, 2000" startWordPosition="156" endWordPosition="159">od accounts for the specifics of the document collection and also uses semantic similarity information based on a large scale statistical analysis. Clustering experiments showed improvements over the traditional tf-idf representation and over the spectral methods based solely on the document collection. 1 Introduction Capturing semantic relations between words in a document representation is a difficult problem. Different approaches tried to overcome the term independence assumption of the bag-of-words representation (Salton and McGill, 1983) for example by using distributional term clusters (Slonim and Tishby, 2000) and expanding the document vectors with synonyms, see (Levow et al., 2005). Since content words can be combined into semantic classes there has been a considerable interest in low-dimensional term and document representations. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. In the LSA space documents are indexed with latent semantic concepts. LSA showed large performance improvements over the traditional tf-idf representation on small document collections (Deerwester et al., 1990) but often does not perform well on large h</context>
</contexts>
<marker>Slonim, Tishby, 2000</marker>
<rawString>Noam Slonim and Naftali Tishby. 2000. Document clustering using word clusters via the information bottleneck method. In Research and Development in Information Retrieval, pages 208–215.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>