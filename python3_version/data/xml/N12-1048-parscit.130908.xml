<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000148">
<title confidence="0.953086">
Real-time Incremental Speech-to-Speech Translation of Dialogs
</title>
<author confidence="0.688171">
Srinivas Bangalore, Vivek Kumar Rangarajan Sridhar, Prakash Kolan
Ladan Golipour, Aura Jimenez
</author>
<affiliation confidence="0.577505">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.792026">
180 Park Avenue
Florham Park, NJ 07932, USA
</address>
<email confidence="0.996891">
vkumar,srini,pkolan,ladan,aura@research.att.com
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999515875">
In a conventional telephone conversation be-
tween two speakers of the same language, the
interaction is real-time and the speakers pro-
cess the information stream incrementally. In
this work, we address the problem of incre-
mental speech-to-speech translation (S2S) that
enables cross-lingual communication between
two remote participants over a telephone. We
investigate the problem in a novel real-time
Session Initiation Protocol (SIP) based S2S
framework. The speech translation is per-
formed incrementally based on generation of
partial hypotheses from speech recognition.
We describe the statistical models comprising
the S2S system and the SIP architecture for
enabling real-time two-way cross-lingual dia-
log. We present dialog experiments performed
in this framework and study the tradeoff in ac-
curacy versus latency in incremental speech
translation. Experimental results demonstrate
that high quality translations can be generated
with the incremental approach with approxi-
mately half the latency associated with non-
incremental approach.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998319">
In recent years, speech-to-speech translation (S2S)
technology has played an increasingly important
role in narrowing the language barrier in cross-
lingual interpersonal communication. The improve-
ments in automatic speech recognition (ASR), statis-
tical machine translation (MT), and, text-to-speech
synthesis (TTS) technology has facilitated the serial
binding of these individual components to achieve
S2S translation of acceptable quality.
Prior work on S2S translation has primarily fo-
cused on providing either one-way or two-way trans-
lation on a single device (Waibel et al., 2003; Zhou
et al., 2003). Typically, the user interface requires
the participant(s) to choose the source and target lan-
guage apriori. The nature of communication, either
single user talking or turn taking between two users
can result in a one-way or cross-lingual dialog inter-
action. In most systems, the necessity to choose the
directionality of translation for each turn does take
away from a natural dialog flow. Furthermore, single
interface based S2S translation (embedded or cloud-
based) is not suitable for cross-lingual communica-
tion when participants are geographically distant, a
scenario more likely in a global setting. In such a
scenario, it is imperative to provide real-time and
low latency communication.
In a conventional telephone conversation between
two speakers of the same language, the interaction
is real-time and the speakers process the informa-
tion stream incrementally. Similarly, cross-lingual
dialog between two remote participants will greatly
benefit through incremental translation. While in-
cremental decoding for text translation has been
addressed previously in (Furuse and Iida, 1996;
Sankaran et al., 2010), we address the problem in
a speech-to-speech translation setting for enabling
real-time cross-lingual dialog. We address the prob-
lem of incrementality in a novel session initiation
protocol (SIP) based S2S translation system that en-
ables two people to interact and engage in cross-
lingual dialog over a telephone (mobile phone or
landline). Our system performs incremental speech
recognition and translation, allowing for low latency
interaction that provides an ideal setting for remote
dialog aimed at accomplishing a task.
We present previous work in this area in Section 2
and introduce the problem of incremental translation
in Section 3. We describe the statistical models used
in the S2S translation framework in Section 4 fol-
lowed by a description of the SIP communication
</bodyText>
<page confidence="0.656742">
437
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 437–445,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999934">
framework for real-time translation in Section 5. In
Section 6, we describe the basic call flow of our sys-
tem following which we present dialog experiments
performed using our framework in Section 8. Fi-
nally, we conclude in Section 9 along with directions
for future work.
</bodyText>
<sectionHeader confidence="0.993419" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999975191489362">
Most previous work on speech-to-speech transla-
tion systems has focused on a single device model,
i.e., the user interface for translation is on one de-
vice (Waibel et al., 1991; Metze et al., 2002; Zhou
et al., 2003; Waibel et al., 2003). The device typi-
cally supports multiple source-target language pairs.
A user typically chooses the directionality of transla-
tion and a toggle feature is used to switch the direc-
tionality. However, this requires physical presence
of the two conversants in one location.
On the other hand, text chat between users over
cell phones has become increasingly popular in the
last decade. While the language used in the inter-
action is typically monolingual, there have been at-
tempts to use statistical machine translation to en-
able cross-lingual text communication (Chen and
Raman, 2008). But this introduces a significant
overhead as the users need to type in the responses
for each turn. Moreover, statistical translation sys-
tems are typically unable to cope with telegraphic
text present in chat messages. A more user friendly
approach would be to use speech as the modality for
communication.
One of the first attempts for two-way S2S trans-
lation over a telephone between two potentially re-
mote participants was made as part of the Verbmobil
project (Wahlster, 2000). The system was restricted
to certain topics and speech was the only modality.
Furthermore, the spontaneous translation of dialogs
was not incremental. One of the first attempts at in-
cremental text translation was demonstrated in (Fu-
ruse and Iida, 1996) using a transfer-driven machine
translation approach. More recently, an incremen-
tal decoding framework for text translation was pre-
sented in (Sankaran et al., 2010). To the best of
our knowledge, incremental speech-to-speech trans-
lation in a dialog setting has not been addressed in
prior work. In this work, we address this problem
using first of a kind SIP-based large vocabulary S2S
translation system that can work with both smart-
phones and landlines. The speech translation is per-
formed incrementally based on generation of partial
hypotheses from speech recognition. Our system
displays the recognized and translated text in an in-
cremental fashion. The use of SIP-based technology
also supports an open form of cross-lingual dialog
without the need for attention phrases.
</bodyText>
<sectionHeader confidence="0.9759005" genericHeader="method">
3 Incremental Speech-to-Speech
Translation
</sectionHeader>
<bodyText confidence="0.999588538461539">
In most statistical machine translation systems, the
input source text is translated in entirety, i.e., the
search for the optimal target string is constrained
on the knowledge of the entire source string. How-
ever, in applications such as language learning and
real-time speech-to-speech translation, incremen-
tally translating the source text or speech can pro-
vide seamless communication and understanding
with low latency. Let us assume that the input string
(either text or speech recognition hypothesis) is f =
f1, · · · , fJ and the target string is e = e1, · · · , eI.
Among all possible target sentences, we will choose
the one with highest probability:
</bodyText>
<equation confidence="0.970517">
=
(f) arg max Pr(e|f) (1)
e
</equation>
<bodyText confidence="0.955949076923077">
In an incremental translation framework, we do not
observe the entire string f. Instead, we observe Q3
sequences, S = s1 · · · sk · · · sQ3, i.e., each sequence
sk = [fjkfjk+1 ··· fj(k+1)−11, i1 = 1,�Q3+1 =
J + 11. Let the translation of each foreign sequence
sk be denoted by tk = [eikeik+1 ··· ei(k+1)−11, i1 =
1, iQ3+1 = I +1. Given this setting, we can perform
decoding using three different approaches. Assum-
ing that each partial source input is translated inde-
pendently, i.e., chunk-wise translation, we get,
We call the decoding in Eq. 2 as partial decoding.
The other option is to translate the partial source in-
e
</bodyText>
<footnote confidence="0.8192035">
1For simplicity, we assume that the incremental and non-
incremental hypotheses are equal in length
</footnote>
<equation confidence="0.9960095">
e(f) = argmax Pr(t1|s1) · · · arg max Pr(tk|sk)
t1 tk (2)
</equation>
<page confidence="0.625156">
438
</page>
<bodyText confidence="0.401863">
put conditioned on the history, i.e.,
</bodyText>
<equation confidence="0.999233666666667">
e(f) = argmax
ti
Pr(tk|s1, ··· , sk, t7, ··· , tk−1) (3)
</equation>
<bodyText confidence="0.9999048">
where tz denotes the best translation for source se-
quence si. We term the result obtained through Eq. 3
as continue-partial. The third option is to wait for
all the partials to be generated and then decode the
source string which we call complete decoding, i.e.,
</bodyText>
<equation confidence="0.9842895">
e(f) = argmax Pr(e|s1, ··· , sk) (4)
e
</equation>
<bodyText confidence="0.999945764705882">
Typically, the hypothesis e� will be more accurate
than e�� as the translation process is non-incremental.
In the best case, one can obtain e�� = e. While the de-
coding described in Eq. 2 has the lowest latency, it
is likely to result in inferior performance in compari-
son to Eq. 1 that will have higher latency. One of the
main issues in incremental speech-to-speech trans-
lation is that the translated sequences need to be im-
mediately synthesized. Hence, there is tradeoff be-
tween the amount of latency versus accuracy as the
synthesized audio cannot be revoked in case of long
distance reordering. In this work, we focus on incre-
mental speech translation and defer the problem of
incremental synthesis to future work. We investigate
the problem of incrementality using a novel SIP-
based S2S translation system, the details of which
we discuss in the subsequent sections.
</bodyText>
<sectionHeader confidence="0.9866815" genericHeader="method">
4 Speech-to-Speech Translation
Components
</sectionHeader>
<bodyText confidence="0.998623333333333">
In this section, we describe the training data, pre-
processing steps and statistical models used in the
S2S system.
</bodyText>
<subsectionHeader confidence="0.999311">
4.1 Automatic Speech Recognition
</subsectionHeader>
<bodyText confidence="0.999993785714286">
We use the AT&amp;T WATSONSM real-time speech
recognizer (Goffin et al., 2004) as the speech recog-
nition module. WATSONSM uses context-dependent
continuous density hidden Markov models (HMM)
for acoustic modeling and finite-state networks for
network optimization and search. The acoustic mod-
els are Gaussian mixture tied-state three-state left-
to-right HMMs. All the acoustic models in this work
were initially trained using the Maximum Likeli-
hood Estimation (MLE) criterion, and followed by
discriminative training through Minimum Phone Er-
ror (MPE) criterion. We also employed Gaussian
Selection (Bocchieri, 1993) to decrease the real-time
factor during the recognition procedure.
The acoustic models for English and Span-
ish were mainly trained on short utterances in
the respective language, acquired from SMS and
search applications on smartphones. The amount
of training data for the English acoustic model
is around 900 hours of speech, while the data
for training the Spanish is approximately half that
of the English model. We used a total of 107
phonemes for the English acoustic model, com-
posed of digit-specific, alpha-specific, and general
English phonemes. Digit-specific and alpha-specific
phonemes were applied to improve the recognition
accuracy of digits and alphas in the speech. The
number of phonemes for Spanish was 34, and, no
digit- or alpha-specific phonemes were included.
The pronunciation dictionary for English is a hand-
labeled dictionary, with pronunciation for unseen
words being predicted using custom rules. A rule-
based dictionary was used for Spanish.
We use AT&amp;T FSM toolkit (Mohri et al., 1997)
to train a trigram language model (LM). The lan-
guage model was linearly interpolated from 18 and
17 components for English and Spanish, respec-
tively. The data for the the LM components was
obtained from several sources that included LDC,
Web, and monolingual portion of the parallel data
described in section 4.2. An elaborate set of lan-
guage specific tokenization and normalization rules
was used to clean the corpora. The normalization
included spelling corrections, conversion of numer-
als into words while accounting for telephone num-
bers, ordinal, and, cardinal categories, punctuation,
etc. The interpolation was performed by tuning the
language model weights on a development set us-
ing perplexity metric. The development set was 500
sentences selected randomly from the IWSLT cor-
pus (Paul, 2006). The training vocabulary size for
English acoustic model is 140k and for the language
model is 300k. For the Spanish model, the train-
ing vocabulary size is 92k, while for testing, the
language model includes 370k distinct words. In
our experiments, the decoding and LM vocabularies
</bodyText>
<figure confidence="0.687955">
Pr(t1|s1)···
arg max
tk
</figure>
<page confidence="0.992957">
439
</page>
<bodyText confidence="0.938135">
were the same.
</bodyText>
<subsectionHeader confidence="0.993924">
4.2 Machine Translation
</subsectionHeader>
<bodyText confidence="0.999992888888889">
The phrase-based translation experiments reported
in this work was performed using the Moses2
toolkit (Koehn et al., 2007) for statistical machine
translation. Training the translation model starts
from the parallel sentences from which we learn
word alignments by using GIZA++ toolkit (Och
and Ney, 2003). The bidirectional word alignments
obtained using GIZA++ were consolidated by us-
ing the grow-diag-final option in Moses. Subse-
quently, we learn phrases (maximum length of 7)
from the consolidated word alignments. A lexical-
ized reordering model (msd-bidirectional-fe option
in Moses) was used for reordering the phrases in
addition to the standard distance based reordering
(distortion-limit of 6). The language models were
interpolated Kneser-Ney discounted trigram models,
all constructed using the SRILM toolkit (Stolcke,
2002). Minimum error rate training (MERT) was
performed on a development set to optimize the fea-
ture weights of the log-linear model used in trans-
lation. During decoding, the unknown words were
preserved in the hypotheses.
The parallel corpus for phrase-based transla-
tion was obtained from a variety of sources: eu-
roparl (Koehn, 2005), jrc-acquis corpus (Steinberger
et al., 2006), opensubtitle corpus (Tiedemann and
Lars Nygaard, 2004), web crawling as well as hu-
man translation. The statistics of the data used for
English-Spanish is shown in Table 1. About 30% of
the training data was obtained from the Web (Ran-
garajan Sridhar et al., 2011). The development set
(identical to the one used in ASR) was used in
MERT training as well as perplexity based optimiza-
tion of the interpolated language model. The lan-
guage model for MT and ASR was constructed from
identical data.
</bodyText>
<subsectionHeader confidence="0.997766">
4.3 Text-to-speech synthesis
</subsectionHeader>
<bodyText confidence="0.9992172">
The translated sentence from the machine trans-
lation component is synthesized using the AT&amp;T
Natural VoicesTM text-to-speech synthesis en-
gine (Beutnagel et al., 1999). The system uses unit
selection synthesis with half phones as the basic
</bodyText>
<footnote confidence="0.913393">
2http://www.statmt.org/moses
</footnote>
<table confidence="0.9995434">
en-es
Data statistics en es
# Sentences 7792118 7792118
# Words 98347681 111006109
Vocabulary 501450 516906
</table>
<tableCaption confidence="0.9351225">
Table 1: Parallel data used for training translation
models
</tableCaption>
<bodyText confidence="0.73249">
units. The database was recorded by professional
speakers of the language. We are currently using fe-
male voices for English as well as Spanish.
</bodyText>
<sectionHeader confidence="0.9955705" genericHeader="method">
5 SIP Communication Framework for
Real-time S2S Translation
</sectionHeader>
<bodyText confidence="0.999983173913043">
The SIP communication framework for real-time
language translation comprises of three main com-
ponents. Session Initiation Protocol (SIP) is becom-
ing the de-facto standard for signaling control for
streaming applications such as Voice over IP. We
present a SIP communication framework that uses
Real-time Transport Protocol (RTP) for packetiz-
ing multimedia content and User Datagram Proto-
col (UDP) for delivering the content. In this work,
the content we focus on is speech and text infor-
mation exchanged between two speakers in a cross-
lingual dialog. For two users conversing in two dif-
ferent languages (e.g., English and Spanish), the me-
dia channels between them will be established as
shown in Figure 1. In Figure 1, each client (UA) is
responsible for recognition, translation, and synthe-
sis of one language input. E.g., the English-Spanish
UA recognizes English text, converts it into Spanish,
and produces output Spanish audio. Similarly, the
Spanish-English UA is responsible for recognition
of Spanish speech input, converting it into English,
and producing output English audio. We describe
the underlying architecture of the system below.
</bodyText>
<subsectionHeader confidence="0.951774">
5.1 Architecture
</subsectionHeader>
<listItem confidence="0.991495857142857">
1. End point SIP user agents: These are the SIP
end points that exchange SIP signaling mes-
sages with the SIP Application server (AS) for
call control.
2. SIP User Agents: Provide a SIP interface to the
core AT&amp;T WATSONSM engine that incorpo-
rates acoustic and language models for speech
</listItem>
<page confidence="0.995692">
440
</page>
<figureCaption confidence="0.9108615">
Figure 1: SIP communication framework used for real-time speech-to-speech translation. The example
shows the setup between two participants in English(en) and Spanish (es)
</figureCaption>
<figure confidence="0.981319">
Caller
(English)
Callee
(Spanish)
Callee English Audio (Translated)
SIP Channel for Signaling Setup and Text (recognized + translated)
Media Channel for RTP Audio
Caller Spanish Audio (Translated)
SIP UA
(en-&gt;es)
SIP UA
(es-&gt;en)
Caller Eng Audio
APP
SERVER
Callee Spanish Audio
recognition.
</figure>
<listItem confidence="0.583042">
3. SIP Application Server (AS): A standard SIP
B2BUA (back to back user agent) that receives
</listItem>
<bodyText confidence="0.996288033333333">
SIP signaling messages and forwards them to
the intended destination. The machine transla-
tion component (server running Moses (Koehn
et al., 2007)) is invoked from the AS.
In our communication framework, the SIP AS re-
ceives a call request from the calling party. The AS
infers the language preference of the calling party
from the user profile database and forwards the call
to the called party. Based on the response, AS in-
fers the language preference of the called party from
the user profile database. If the languages of the
calling and called parties are different, the AS in-
vites two SIP UAs into the call context. The AS ex-
changes media parameters derived from the calling
and called party SIP messages with that of the SIP
UAs. The AS then forwards the media parameters
of the UAs to the end user SIP agents.
The AS, the end user SIP UAs, and the SIP UAs
are all RFC 3261 SIP standard compliant. The end
user SIP UAs are developed using PJSIP stack that
uses PJMedia for RTP packetization of audio and
network transmission. For our testing, we have
implemented the end user SIP UAs to run on Ap-
ple IOS devices. The AS is developed using E4SS
(Echarts for SIP Servlets) software and deployed on
Sailfin Java container. It is deployed on a Linux box
installed with Cent OS version 5. The SIP UAs are
written in python for interfacing with external SIP
devices, and use proprietary protocol for interfacing
with the core AT&amp;T WATSONSM engine.
</bodyText>
<sectionHeader confidence="0.99781" genericHeader="method">
6 Typical Call Flow
</sectionHeader>
<bodyText confidence="0.999521222222222">
Figure 2 shows the typical call flow involved in set-
ting up the cross-lingual dialog. The caller chooses
the number of the callee from the address book or
enters it using the keypad. Subsequently, the call is
initiated and the underlying SIP channels are estab-
lished to facilitate the call. The users can then con-
verse in their native language with the hypotheses
displayed in an IM-like fashion. The messages of
the caller appear on the left side of the screen while
those of the callee appear on the right. Both the
recognition and translation hypotheses are displayed
incrementally for each side of the conversation. In
our experiments, the caller and the callee naturally
followed a protocol of listening to the other party’s
synthesized output before speaking once they were
accustomed to the interface. One of the issues dur-
ing speech recognition is that, the user can poten-
tially start speaking as the TTS output from the other
</bodyText>
<page confidence="0.998853">
441
</page>
<figureCaption confidence="0.974997">
Figure 2: Illustration of call flow. The call is established using SIP and the real-time conversation appears
in the bubbles in a manner similar to Instant Messaging. For illustration purposes, the caller (Spanish) and
callee (English) are assumed to have set their language preferences in the setup menu.
</figureCaption>
<bodyText confidence="0.827602666666667">
participant is being played. We address the feedback
problem from the TTS output by muting the micro-
phone when TTS output is played.
</bodyText>
<sectionHeader confidence="0.986046" genericHeader="method">
7 Dialog Data
</sectionHeader>
<bodyText confidence="0.9999960625">
The system described above provides a natural way
to collect cross-lingual dialog data. We used our
system to collect a corpus of 40 scripted dialogs in
English and Spanish. A bilingual (English-Spanish)
speaker created dialog scenarios in the travel and
hospitality domain and the scripted dialog was used
as reference material in the call. Two subjects partic-
ipated in the data collection, a male English speaker
and female Spanish speaker. The subjects were in-
structed to read the lines verbatim. However, due to
ASR errors, the subjects had to repeat or improvise
few turns (about 10%) to sustain the dialog. The av-
erage number of turns per scenario in the collected
corpus is 13; 6 and 7 turns per scenario for English
and Spanish, respectively. An example dialog be-
tween two speakers is shown in Table 2.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<bodyText confidence="0.99985">
In this section, we describe speech translation ex-
periments performed on the dialog corpus collected
through our system. We present baseline results fol-
lowed by results of incremental translation.
</bodyText>
<subsectionHeader confidence="0.982694">
8.1 Baseline Experiments
</subsectionHeader>
<bodyText confidence="0.9993815">
The models described in Section 4 were used to es-
tablish baseline results on the dialog corpus. No
</bodyText>
<listItem confidence="0.85140875">
A: Hello, I am calling from room four twenty one
the T.V. is not working. Do you think you can send
someone to fix it please?
B: Si, Se˜nor enseguida enviamos a alguien para que
la arregle. Si no le cambiaremos de habitaci´on.
A: Thank you very much.
B: Estamos aqu para servirle. Ll´amenos si necesita
algo m´as.
</listItem>
<tableCaption confidence="0.985317">
Table 2: Example of a sample dialog scenario.
</tableCaption>
<bodyText confidence="0.999920277777778">
contextual information was used in these experi-
ments, i.e., the audio utterances were decoded in-
dependently. The ASR WER for English and Span-
ish sides of the dialogs is shown in Figure 3. The
average WER for English and Spanish side of the
conversations is 27.73% and 22.83%, respectively.
The recognized utterances were subsequently trans-
lated using the MT system described above. The
MT performance in terms of Translation Edit Rate
(TER) (Snover et al., 2006) and BLEU (Papineni
et al., 2002) is shown in Figure 4. The MT per-
formance is shown across all the turns for both ref-
erence transcriptions and ASR output. The results
show that the performance of the Spanish-English
MT model is better in comparison to the English-
Spanish model on the dialog corpus. The perfor-
mance on ASR input drops by about 18% compared
to translation on reference text.
</bodyText>
<page confidence="0.994464">
442
</page>
<figure confidence="0.998683772727273">
34.0
28.21 26.96
25.5 23.87
17.0
8.5
0
33.58 Spanish-English
English-Spanish
Spanish-English
70.0
63.42
English-Spanish
55.34
59.19
52.5
47.26
BLEU
TER
35.0
17.5
0
Reference ASR Reference ASR
</figure>
<figureCaption confidence="0.99554775">
Figure 4: TER (%) and BLEU of English-Spanish and Spanish-English MT models on reference transcripts
and ASR output
Figure 3: WER (%) of English and Spanish acoustic
models on the dialog corpus
</figureCaption>
<subsectionHeader confidence="0.998577">
8.2 Segmentation of ASR output for MT
</subsectionHeader>
<bodyText confidence="0.9997381">
Turn taking in a dialog typically involves the sub-
jects speaking one or more utterances in a turn.
Since, machine translation systems are trained on
chunked parallel texts (40 words or less), it is ben-
eficial to segment the ASR hypotheses before trans-
lation. Previous studies have shown significant im-
provements in translation performance through the
segmentation of ASR hypotheses (Matusov et al.,
2007). We experimented with the notion of seg-
mentation defined by silence frames in the ASR out-
put. A threshold of 8-10 frames (100 ms) was found
to be suitable for segmenting the ASR output into
sentence chunks. We did not use any lexical fea-
tures for segmenting the turns. The BLEU scores for
different silence thresholds used in segmentation is
shown in Figure 5. The BLEU scores improvement
for Spanish-English is 1.6 BLEU points higher than
the baseline model using no segmentation. The im-
provement for English-Spanish is smaller but statis-
tically significant. Analysis of the dialogs revealed
that the English speaker tended to speak his turns
without pausing across utterance chunks while the
Spanish speaker paused a lot more. The results in-
dicate that in a typical dialog interaction, if the par-
ticipants observe inter-utterance pause (80-100 ms)
within a turn, it serves as a good marker for segmen-
tation. Further, exploiting such information can po-
tentially result in improvements in MT performance
as the model is typically trained on sentence level
parallel text.
</bodyText>
<figure confidence="0.98541625">
BLEU 21.0
19.2
17.4
15.6
13.8
12.0
50 80 110 140 170 200 500
Silence threshold for segmentation (ms)
</figure>
<figureCaption confidence="0.991798666666667">
Figure 5: BLEU score of English-Spanish and
Spanish-English MT models on the ASR output us-
ing silence segmentation
</figureCaption>
<subsectionHeader confidence="0.967726">
8.3 Incremental Speech Translation Results
</subsectionHeader>
<bodyText confidence="0.9996916">
Figure 6 shows the BLEU score for incremental
speech translation described in Section 3. In the fig-
ure, partial refers to Eq. 2, continue-partial refers to
Eq. 3 and complete refers to Eq. 4. The continue-
partials option was exercised by using the continue-
</bodyText>
<figure confidence="0.9929716">
30.0
25.21 25.20 24.86 24.80
24.34 24.27 23.87
28.76 28.76 28.21 28.18
27.76 27.62
26.96
28.2
26.4
24.6
22.8
</figure>
<page confidence="0.701612">
443
</page>
<figure confidence="0.9890065">
10 20 30 40 50 60 70 80 90 100 200 300 400 500 600 700 800 900 1000
Speech Recognizer timeouts (msec)
</figure>
<figureCaption confidence="0.9917955">
Figure 6: BLEU score (Spanish-English) for incremental speech translation across varying timeout periods
in the speech recognizer
</figureCaption>
<figure confidence="0.990516428571428">
Partial (Eq. 1)
25.00
Moses ‘continue-partial’ (Eq. 2)
Complete (Eq. 3)
0
2.78
5.56
22.22
19.44
16.67
13.89
11.11
8.33
BLEU
</figure>
<bodyText confidence="0.999639294117647">
partial-translation parameter in Moses (Koehn et al.,
2007). The partial hypotheses are generated as a
function of speech recognizer timeouts. Timeout is
defined as the time interval with which the speech
recognizer generates partial hypotheses. For each
timeout interval, the speech recognizer may or may
not generate a partial result based on the search path
at that instant in time. As the timeout interval in-
creases, the performance of incremental translation
approaches that of non-incremental translation. The
key is to choose an operating point such that the
user perception of latency is minimal with accept-
able BLEU score. It is interesting that very good
performance can be attained at a timeout of 500 ms
in comparison with non-incremental speech trans-
lation, i.e., the latency can be reduced in half with
acceptable translation quality. The continue-partial
option in Moses performs slightly better than the
partial case as it conditions the decision on prior
source input as well as translation.
In Table 3, we present the latency measurements
of the various components in our framework. We do
not have a row for ASR since it is not possible to get
the start time for each recognition run as the RTP
packets are continuously flowing in the SIP frame-
work. The latency between various system compo-
nents is very low (5-30 ms). While the average time
taken for translation (incremental) is Pt� 100 ms, the
TTS takes the longest time as it is non-incremental
in the current work. It can also been seen that the
average time taken for generating incremental MT
output is half that of TTS that is non-incremental.
The overall results show that the communication in
our SIP-based framework has low latency.
</bodyText>
<table confidence="0.8794496">
Components Caller Callee Average
ASR output to MT input 6.8 0.1 3.4
MT 100.4 108.8 104.6
MT output to TTS 22.1 33.1 27.6
TTS 246 160.3 203.1
</table>
<tableCaption confidence="0.8249985">
Table 3: Latency measurements (in ms) for the S2S
components in the real-time SIP framework.
</tableCaption>
<sectionHeader confidence="0.995699" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999918823529412">
In this paper, we introduced the problem of incre-
mental speech-to-speech translation and presented
first of a kind two-way real-time speech-to-speech
translation system based on SIP that incorporates
the notion of incrementality. We presented details
about the SIP framework and demonstrated the typ-
ical call flow in our application. We also presented
a dialog corpus collected using our framework and
benchmarked the performance of the system. Our
framework allows for incremental speech transla-
tion and can provide low latency translation. We
are currently working on improving the accuracy of
incremental translation. We are also exploring new
algorithms for performing reordering aware incre-
mental speech-to-speech translation, i.e., translating
source phrases such that text-to-speech synthesis can
be rendered incrementally.
</bodyText>
<page confidence="0.998697">
444
</page>
<sectionHeader confidence="0.998348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998322304878049">
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou, and
A. Syrdal. 1999. The AT&amp;T Next-Gen TTS sys-
tem. In Proceedings of Joint Meeting of ASA, EAA
and DEGA.
E. Bocchieri. 1993. Vector quantization for the efficient
computation of continuous density likelihoods. Pro-
ceedings of ICASSP.
Charles L. Chen and T. V. Raman. 2008. Axsjax: a talk-
ing translation bot using google im: bringing web-2.0
applications to life. In Proceedings of the 2008 inter-
national cross-disciplinary conference on Web acces-
sibility (W4A).
O. Furuse and H. Iida. 1996. Incremental translation uti-
lizing constituent boundary patterns. In Proc. of Col-
ing ’96.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani Tur, Andrej Ljolje, and Sarangarajan
Parthasarathy. 2004. The AT&amp;T Watson Speech Rec-
ognizer. Technical report, September.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Shen W.,
C. Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings ofACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
E. Matusov, D. Hillard, M. Magimai-Doss, D. Hakkani-
T´ur, M. Ostendorf, and H. Ney. 2007. Improving
speech translation with automatic boundary predic-
tion. In Proceedings of Interspeech.
F. Metze, J. McDonough, H. Soltau, A. Waibel, A. Lavie,
S. Burger, C. Langley, L. Levin, T. Schultz, F. Pianesi,
R. Cattoni, G. Lazzari, N. Mana, and E. Pianta. 2002.
The NESPOLE! speech-to-speech translation system.
M. Mohri, F. Pereira, and M. Riley. 1997. Att
general-purpose finite-state machine software tools,
http://www.research.att.com/sw/tools/fsm/.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
M. Paul. 2006. Overview of the iwslt 2006 evaluation
campaign. In Proceedings of the International Work-
shop of Spoken Language Translation, Kyoto, Japan.
V. K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore.
2011. A scalable approach to building a parallel cor-
pus from the Web. In Proceedings of Interspeech.
B. Sankaran, A. Grewal, and A. Sarkar. 2010. Incre-
mental decoding for phrase-based statistical machine
translation. In Proceedings of the fifth Workshop on
Statistical Machine Translation and Metrics.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of AMTA.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, and D. Tufis. 2006. The JRC-Acquis: A multi-
lingual aligned parallel corpus with 20+ languages. In
Proceedings of LREC.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proceedings of ICSLP.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS
corpus - parallel &amp; free. In Proceedings of LREC.
Wolfgang Wahlster, editor. 2000. Verbmobil: Founda-
tions of Speech-to-Speech Translation. Springer.
A. Waibel, A. N. Jain, A. E. McNair, H. Saito, A. G.
Hauptmann, and J. Tebelskis. 1991. JANUS: a
speech-to-speech translation system using connection-
ist and symbolic processing strategies. In Proceedings
of ICASSP, pages 793–796, Los Alamitos, CA, USA.
A. Waibel, A. Badran, A. W. Black, R. Frederk-
ing, G. Gates, A. Lavie, L. Levin, K. Lenzo,
L. M. Tomokiyo, J. Reichert, T. Schultz, W. Dorcas,
M. Woszczyna, and J. Zhang. 2003. Speechalator:
two-way speech-to-speech translation on a consumer
PDA. In Proceedings of the European Conference on
Speech Communication and Technology, pages 369–
372.
B. Zhou, Y. Gao, J. Sorenson, D. Dechelotte, and
M. Picheny. 2003. A hand-held speech-to-speech
translation system. In Proceedings of ASRU.
</reference>
<page confidence="0.999109">
445
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.300900">
<title confidence="0.994502">Real-time Incremental Speech-to-Speech Translation of Dialogs</title>
<author confidence="0.680776">Srinivas Bangalore</author>
<author confidence="0.680776">Vivek Kumar Rangarajan Sridhar</author>
<author confidence="0.680776">Prakash Ladan Golipour</author>
<author confidence="0.680776">Aura</author>
<affiliation confidence="0.858164">AT&amp;T Labs -</affiliation>
<address confidence="0.9955685">180 Park Florham Park, NJ 07932,</address>
<email confidence="0.999754">vkumar,srini,pkolan,ladan,aura@research.att.com</email>
<abstract confidence="0.99078788">In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. In this work, we address the problem of incremental speech-to-speech translation (S2S) that enables cross-lingual communication between two remote participants over a telephone. We investigate the problem in a novel real-time Session Initiation Protocol (SIP) based S2S framework. The speech translation is performed incrementally based on generation of partial hypotheses from speech recognition. We describe the statistical models comprising the S2S system and the SIP architecture for enabling real-time two-way cross-lingual dialog. We present dialog experiments performed in this framework and study the tradeoff in accuracy versus latency in incremental speech translation. Experimental results demonstrate that high quality translations can be generated with the incremental approach with approximately half the latency associated with nonincremental approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Beutnagel</author>
<author>A Conkie</author>
<author>J Schroeter</author>
<author>Y Stylianou</author>
<author>A Syrdal</author>
</authors>
<title>The AT&amp;T Next-Gen TTS system.</title>
<date>1999</date>
<booktitle>In Proceedings of Joint Meeting of ASA, EAA and DEGA.</booktitle>
<contexts>
<context position="14336" citStr="Beutnagel et al., 1999" startWordPosition="2228" endWordPosition="2231">ing as well as human translation. The statistics of the data used for English-Spanish is shown in Table 1. About 30% of the training data was obtained from the Web (Rangarajan Sridhar et al., 2011). The development set (identical to the one used in ASR) was used in MERT training as well as perplexity based optimization of the interpolated language model. The language model for MT and ASR was constructed from identical data. 4.3 Text-to-speech synthesis The translated sentence from the machine translation component is synthesized using the AT&amp;T Natural VoicesTM text-to-speech synthesis engine (Beutnagel et al., 1999). The system uses unit selection synthesis with half phones as the basic 2http://www.statmt.org/moses en-es Data statistics en es # Sentences 7792118 7792118 # Words 98347681 111006109 Vocabulary 501450 516906 Table 1: Parallel data used for training translation models units. The database was recorded by professional speakers of the language. We are currently using female voices for English as well as Spanish. 5 SIP Communication Framework for Real-time S2S Translation The SIP communication framework for real-time language translation comprises of three main components. Session Initiation Prot</context>
</contexts>
<marker>Beutnagel, Conkie, Schroeter, Stylianou, Syrdal, 1999</marker>
<rawString>M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou, and A. Syrdal. 1999. The AT&amp;T Next-Gen TTS system. In Proceedings of Joint Meeting of ASA, EAA and DEGA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bocchieri</author>
</authors>
<title>Vector quantization for the efficient computation of continuous density likelihoods.</title>
<date>1993</date>
<booktitle>Proceedings of ICASSP.</booktitle>
<contexts>
<context position="10270" citStr="Bocchieri, 1993" startWordPosition="1600" endWordPosition="1601">ition We use the AT&amp;T WATSONSM real-time speech recognizer (Goffin et al., 2004) as the speech recognition module. WATSONSM uses context-dependent continuous density hidden Markov models (HMM) for acoustic modeling and finite-state networks for network optimization and search. The acoustic models are Gaussian mixture tied-state three-state leftto-right HMMs. All the acoustic models in this work were initially trained using the Maximum Likelihood Estimation (MLE) criterion, and followed by discriminative training through Minimum Phone Error (MPE) criterion. We also employed Gaussian Selection (Bocchieri, 1993) to decrease the real-time factor during the recognition procedure. The acoustic models for English and Spanish were mainly trained on short utterances in the respective language, acquired from SMS and search applications on smartphones. The amount of training data for the English acoustic model is around 900 hours of speech, while the data for training the Spanish is approximately half that of the English model. We used a total of 107 phonemes for the English acoustic model, composed of digit-specific, alpha-specific, and general English phonemes. Digit-specific and alpha-specific phonemes we</context>
</contexts>
<marker>Bocchieri, 1993</marker>
<rawString>E. Bocchieri. 1993. Vector quantization for the efficient computation of continuous density likelihoods. Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles L Chen</author>
<author>T V Raman</author>
</authors>
<title>Axsjax: a talking translation bot using google im: bringing web-2.0 applications to life.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 international cross-disciplinary conference on Web accessibility (W4A).</booktitle>
<contexts>
<context position="5173" citStr="Chen and Raman, 2008" startWordPosition="759" endWordPosition="762">; Zhou et al., 2003; Waibel et al., 2003). The device typically supports multiple source-target language pairs. A user typically chooses the directionality of translation and a toggle feature is used to switch the directionality. However, this requires physical presence of the two conversants in one location. On the other hand, text chat between users over cell phones has become increasingly popular in the last decade. While the language used in the interaction is typically monolingual, there have been attempts to use statistical machine translation to enable cross-lingual text communication (Chen and Raman, 2008). But this introduces a significant overhead as the users need to type in the responses for each turn. Moreover, statistical translation systems are typically unable to cope with telegraphic text present in chat messages. A more user friendly approach would be to use speech as the modality for communication. One of the first attempts for two-way S2S translation over a telephone between two potentially remote participants was made as part of the Verbmobil project (Wahlster, 2000). The system was restricted to certain topics and speech was the only modality. Furthermore, the spontaneous translat</context>
</contexts>
<marker>Chen, Raman, 2008</marker>
<rawString>Charles L. Chen and T. V. Raman. 2008. Axsjax: a talking translation bot using google im: bringing web-2.0 applications to life. In Proceedings of the 2008 international cross-disciplinary conference on Web accessibility (W4A).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Furuse</author>
<author>H Iida</author>
</authors>
<title>Incremental translation utilizing constituent boundary patterns.</title>
<date>1996</date>
<booktitle>In Proc. of Coling ’96.</booktitle>
<contexts>
<context position="3034" citStr="Furuse and Iida, 1996" startWordPosition="423" endWordPosition="426">not suitable for cross-lingual communication when participants are geographically distant, a scenario more likely in a global setting. In such a scenario, it is imperative to provide real-time and low latency communication. In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. Similarly, cross-lingual dialog between two remote participants will greatly benefit through incremental translation. While incremental decoding for text translation has been addressed previously in (Furuse and Iida, 1996; Sankaran et al., 2010), we address the problem in a speech-to-speech translation setting for enabling real-time cross-lingual dialog. We address the problem of incrementality in a novel session initiation protocol (SIP) based S2S translation system that enables two people to interact and engage in crosslingual dialog over a telephone (mobile phone or landline). Our system performs incremental speech recognition and translation, allowing for low latency interaction that provides an ideal setting for remote dialog aimed at accomplishing a task. We present previous work in this area in Section </context>
<context position="5910" citStr="Furuse and Iida, 1996" startWordPosition="877" endWordPosition="881">atistical translation systems are typically unable to cope with telegraphic text present in chat messages. A more user friendly approach would be to use speech as the modality for communication. One of the first attempts for two-way S2S translation over a telephone between two potentially remote participants was made as part of the Verbmobil project (Wahlster, 2000). The system was restricted to certain topics and speech was the only modality. Furthermore, the spontaneous translation of dialogs was not incremental. One of the first attempts at incremental text translation was demonstrated in (Furuse and Iida, 1996) using a transfer-driven machine translation approach. More recently, an incremental decoding framework for text translation was presented in (Sankaran et al., 2010). To the best of our knowledge, incremental speech-to-speech translation in a dialog setting has not been addressed in prior work. In this work, we address this problem using first of a kind SIP-based large vocabulary S2S translation system that can work with both smartphones and landlines. The speech translation is performed incrementally based on generation of partial hypotheses from speech recognition. Our system displays the re</context>
</contexts>
<marker>Furuse, Iida, 1996</marker>
<rawString>O. Furuse and H. Iida. 1996. Incremental translation utilizing constituent boundary patterns. In Proc. of Coling ’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Goffin</author>
</authors>
<title>Cyril Allauzen, Enrico Bocchieri, Dilek Hakkani Tur, Andrej Ljolje, and Sarangarajan Parthasarathy.</title>
<date>2004</date>
<tech>Technical report,</tech>
<marker>Goffin, 2004</marker>
<rawString>Vincent Goffin, Cyril Allauzen, Enrico Bocchieri, Dilek Hakkani Tur, Andrej Ljolje, and Sarangarajan Parthasarathy. 2004. The AT&amp;T Watson Speech Recognizer. Technical report, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C J Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="12558" citStr="Koehn et al., 2007" startWordPosition="1956" endWordPosition="1959">el weights on a development set using perplexity metric. The development set was 500 sentences selected randomly from the IWSLT corpus (Paul, 2006). The training vocabulary size for English acoustic model is 140k and for the language model is 300k. For the Spanish model, the training vocabulary size is 92k, while for testing, the language model includes 370k distinct words. In our experiments, the decoding and LM vocabularies Pr(t1|s1)··· arg max tk 439 were the same. 4.2 Machine Translation The phrase-based translation experiments reported in this work was performed using the Moses2 toolkit (Koehn et al., 2007) for statistical machine translation. Training the translation model starts from the parallel sentences from which we learn word alignments by using GIZA++ toolkit (Och and Ney, 2003). The bidirectional word alignments obtained using GIZA++ were consolidated by using the grow-diag-final option in Moses. Subsequently, we learn phrases (maximum length of 7) from the consolidated word alignments. A lexicalized reordering model (msd-bidirectional-fe option in Moses) was used for reordering the phrases in addition to the standard distance based reordering (distortion-limit of 6). The language model</context>
<context position="16967" citStr="Koehn et al., 2007" startWordPosition="2630" endWordPosition="2633">on. The example shows the setup between two participants in English(en) and Spanish (es) Caller (English) Callee (Spanish) Callee English Audio (Translated) SIP Channel for Signaling Setup and Text (recognized + translated) Media Channel for RTP Audio Caller Spanish Audio (Translated) SIP UA (en-&gt;es) SIP UA (es-&gt;en) Caller Eng Audio APP SERVER Callee Spanish Audio recognition. 3. SIP Application Server (AS): A standard SIP B2BUA (back to back user agent) that receives SIP signaling messages and forwards them to the intended destination. The machine translation component (server running Moses (Koehn et al., 2007)) is invoked from the AS. In our communication framework, the SIP AS receives a call request from the calling party. The AS infers the language preference of the calling party from the user profile database and forwards the call to the called party. Based on the response, AS infers the language preference of the called party from the user profile database. If the languages of the calling and called parties are different, the AS invites two SIP UAs into the call context. The AS exchanges media parameters derived from the calling and called party SIP messages with that of the SIP UAs. The AS the</context>
<context position="24985" citStr="Koehn et al., 2007" startWordPosition="3983" endWordPosition="3986">d complete refers to Eq. 4. The continuepartials option was exercised by using the continue30.0 25.21 25.20 24.86 24.80 24.34 24.27 23.87 28.76 28.76 28.21 28.18 27.76 27.62 26.96 28.2 26.4 24.6 22.8 443 10 20 30 40 50 60 70 80 90 100 200 300 400 500 600 700 800 900 1000 Speech Recognizer timeouts (msec) Figure 6: BLEU score (Spanish-English) for incremental speech translation across varying timeout periods in the speech recognizer Partial (Eq. 1) 25.00 Moses ‘continue-partial’ (Eq. 2) Complete (Eq. 3) 0 2.78 5.56 22.22 19.44 16.67 13.89 11.11 8.33 BLEU partial-translation parameter in Moses (Koehn et al., 2007). The partial hypotheses are generated as a function of speech recognizer timeouts. Timeout is defined as the time interval with which the speech recognizer generates partial hypotheses. For each timeout interval, the speech recognizer may or may not generate a partial result based on the search path at that instant in time. As the timeout interval increases, the performance of incremental translation approaches that of non-incremental translation. The key is to choose an operating point such that the user perception of latency is minimal with acceptable BLEU score. It is interesting that very</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, Shen W., C. Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="13600" citStr="Koehn, 2005" startWordPosition="2112" endWordPosition="2113">sd-bidirectional-fe option in Moses) was used for reordering the phrases in addition to the standard distance based reordering (distortion-limit of 6). The language models were interpolated Kneser-Ney discounted trigram models, all constructed using the SRILM toolkit (Stolcke, 2002). Minimum error rate training (MERT) was performed on a development set to optimize the feature weights of the log-linear model used in translation. During decoding, the unknown words were preserved in the hypotheses. The parallel corpus for phrase-based translation was obtained from a variety of sources: europarl (Koehn, 2005), jrc-acquis corpus (Steinberger et al., 2006), opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), web crawling as well as human translation. The statistics of the data used for English-Spanish is shown in Table 1. About 30% of the training data was obtained from the Web (Rangarajan Sridhar et al., 2011). The development set (identical to the one used in ASR) was used in MERT training as well as perplexity based optimization of the interpolated language model. The language model for MT and ASR was constructed from identical data. 4.3 Text-to-speech synthesis The translated sentence from t</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>D Hillard</author>
<author>M Magimai-Doss</author>
<author>D HakkaniT´ur</author>
<author>M Ostendorf</author>
<author>H Ney</author>
</authors>
<title>Improving speech translation with automatic boundary prediction.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<marker>Matusov, Hillard, Magimai-Doss, HakkaniT´ur, Ostendorf, Ney, 2007</marker>
<rawString>E. Matusov, D. Hillard, M. Magimai-Doss, D. HakkaniT´ur, M. Ostendorf, and H. Ney. 2007. Improving speech translation with automatic boundary prediction. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Metze</author>
<author>J McDonough</author>
<author>H Soltau</author>
<author>A Waibel</author>
<author>A Lavie</author>
<author>S Burger</author>
<author>C Langley</author>
<author>L Levin</author>
<author>T Schultz</author>
<author>F Pianesi</author>
<author>R Cattoni</author>
<author>G Lazzari</author>
<author>N Mana</author>
<author>E Pianta</author>
</authors>
<title>The NESPOLE! speech-to-speech translation system.</title>
<date>2002</date>
<contexts>
<context position="4552" citStr="Metze et al., 2002" startWordPosition="660" endWordPosition="663">tics: Human Language Technologies, pages 437–445, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics framework for real-time translation in Section 5. In Section 6, we describe the basic call flow of our system following which we present dialog experiments performed using our framework in Section 8. Finally, we conclude in Section 9 along with directions for future work. 2 Previous Work Most previous work on speech-to-speech translation systems has focused on a single device model, i.e., the user interface for translation is on one device (Waibel et al., 1991; Metze et al., 2002; Zhou et al., 2003; Waibel et al., 2003). The device typically supports multiple source-target language pairs. A user typically chooses the directionality of translation and a toggle feature is used to switch the directionality. However, this requires physical presence of the two conversants in one location. On the other hand, text chat between users over cell phones has become increasingly popular in the last decade. While the language used in the interaction is typically monolingual, there have been attempts to use statistical machine translation to enable cross-lingual text communication (</context>
</contexts>
<marker>Metze, McDonough, Soltau, Waibel, Lavie, Burger, Langley, Levin, Schultz, Pianesi, Cattoni, Lazzari, Mana, Pianta, 2002</marker>
<rawString>F. Metze, J. McDonough, H. Soltau, A. Waibel, A. Lavie, S. Burger, C. Langley, L. Levin, T. Schultz, F. Pianesi, R. Cattoni, G. Lazzari, N. Mana, and E. Pianta. 2002. The NESPOLE! speech-to-speech translation system.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>F Pereira</author>
<author>M Riley</author>
</authors>
<title>Att general-purpose finite-state machine software tools,</title>
<date>1997</date>
<location>http://www.research.att.com/sw/tools/fsm/.</location>
<contexts>
<context position="11284" citStr="Mohri et al., 1997" startWordPosition="1756" endWordPosition="1759">of the English model. We used a total of 107 phonemes for the English acoustic model, composed of digit-specific, alpha-specific, and general English phonemes. Digit-specific and alpha-specific phonemes were applied to improve the recognition accuracy of digits and alphas in the speech. The number of phonemes for Spanish was 34, and, no digit- or alpha-specific phonemes were included. The pronunciation dictionary for English is a handlabeled dictionary, with pronunciation for unseen words being predicted using custom rules. A rulebased dictionary was used for Spanish. We use AT&amp;T FSM toolkit (Mohri et al., 1997) to train a trigram language model (LM). The language model was linearly interpolated from 18 and 17 components for English and Spanish, respectively. The data for the the LM components was obtained from several sources that included LDC, Web, and monolingual portion of the parallel data described in section 4.2. An elaborate set of language specific tokenization and normalization rules was used to clean the corpora. The normalization included spelling corrections, conversion of numerals into words while accounting for telephone numbers, ordinal, and, cardinal categories, punctuation, etc. The</context>
</contexts>
<marker>Mohri, Pereira, Riley, 1997</marker>
<rawString>M. Mohri, F. Pereira, and M. Riley. 1997. Att general-purpose finite-state machine software tools, http://www.research.att.com/sw/tools/fsm/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12741" citStr="Och and Ney, 2003" startWordPosition="1983" endWordPosition="1986">glish acoustic model is 140k and for the language model is 300k. For the Spanish model, the training vocabulary size is 92k, while for testing, the language model includes 370k distinct words. In our experiments, the decoding and LM vocabularies Pr(t1|s1)··· arg max tk 439 were the same. 4.2 Machine Translation The phrase-based translation experiments reported in this work was performed using the Moses2 toolkit (Koehn et al., 2007) for statistical machine translation. Training the translation model starts from the parallel sentences from which we learn word alignments by using GIZA++ toolkit (Och and Ney, 2003). The bidirectional word alignments obtained using GIZA++ were consolidated by using the grow-diag-final option in Moses. Subsequently, we learn phrases (maximum length of 7) from the consolidated word alignments. A lexicalized reordering model (msd-bidirectional-fe option in Moses) was used for reordering the phrases in addition to the standard distance based reordering (distortion-limit of 6). The language models were interpolated Kneser-Ney discounted trigram models, all constructed using the SRILM toolkit (Stolcke, 2002). Minimum error rate training (MERT) was performed on a development se</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="21682" citStr="Papineni et al., 2002" startWordPosition="3438" endWordPosition="3441">nk you very much. B: Estamos aqu para servirle. Ll´amenos si necesita algo m´as. Table 2: Example of a sample dialog scenario. contextual information was used in these experiments, i.e., the audio utterances were decoded independently. The ASR WER for English and Spanish sides of the dialogs is shown in Figure 3. The average WER for English and Spanish side of the conversations is 27.73% and 22.83%, respectively. The recognized utterances were subsequently translated using the MT system described above. The MT performance in terms of Translation Edit Rate (TER) (Snover et al., 2006) and BLEU (Papineni et al., 2002) is shown in Figure 4. The MT performance is shown across all the turns for both reference transcriptions and ASR output. The results show that the performance of the Spanish-English MT model is better in comparison to the EnglishSpanish model on the dialog corpus. The performance on ASR input drops by about 18% compared to translation on reference text. 442 34.0 28.21 26.96 25.5 23.87 17.0 8.5 0 33.58 Spanish-English English-Spanish Spanish-English 70.0 63.42 English-Spanish 55.34 59.19 52.5 47.26 BLEU TER 35.0 17.5 0 Reference ASR Reference ASR Figure 4: TER (%) and BLEU of English-Spanish a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
</authors>
<title>Overview of the iwslt 2006 evaluation campaign.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop of Spoken Language Translation,</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="12086" citStr="Paul, 2006" startWordPosition="1883" endWordPosition="1884">btained from several sources that included LDC, Web, and monolingual portion of the parallel data described in section 4.2. An elaborate set of language specific tokenization and normalization rules was used to clean the corpora. The normalization included spelling corrections, conversion of numerals into words while accounting for telephone numbers, ordinal, and, cardinal categories, punctuation, etc. The interpolation was performed by tuning the language model weights on a development set using perplexity metric. The development set was 500 sentences selected randomly from the IWSLT corpus (Paul, 2006). The training vocabulary size for English acoustic model is 140k and for the language model is 300k. For the Spanish model, the training vocabulary size is 92k, while for testing, the language model includes 370k distinct words. In our experiments, the decoding and LM vocabularies Pr(t1|s1)··· arg max tk 439 were the same. 4.2 Machine Translation The phrase-based translation experiments reported in this work was performed using the Moses2 toolkit (Koehn et al., 2007) for statistical machine translation. Training the translation model starts from the parallel sentences from which we learn word</context>
</contexts>
<marker>Paul, 2006</marker>
<rawString>M. Paul. 2006. Overview of the iwslt 2006 evaluation campaign. In Proceedings of the International Workshop of Spoken Language Translation, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V K Rangarajan Sridhar</author>
<author>L Barbosa</author>
<author>S Bangalore</author>
</authors>
<title>A scalable approach to building a parallel corpus from the Web. In</title>
<date>2011</date>
<booktitle>Proceedings of Interspeech.</booktitle>
<contexts>
<context position="13910" citStr="Sridhar et al., 2011" startWordPosition="2161" endWordPosition="2164">te training (MERT) was performed on a development set to optimize the feature weights of the log-linear model used in translation. During decoding, the unknown words were preserved in the hypotheses. The parallel corpus for phrase-based translation was obtained from a variety of sources: europarl (Koehn, 2005), jrc-acquis corpus (Steinberger et al., 2006), opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), web crawling as well as human translation. The statistics of the data used for English-Spanish is shown in Table 1. About 30% of the training data was obtained from the Web (Rangarajan Sridhar et al., 2011). The development set (identical to the one used in ASR) was used in MERT training as well as perplexity based optimization of the interpolated language model. The language model for MT and ASR was constructed from identical data. 4.3 Text-to-speech synthesis The translated sentence from the machine translation component is synthesized using the AT&amp;T Natural VoicesTM text-to-speech synthesis engine (Beutnagel et al., 1999). The system uses unit selection synthesis with half phones as the basic 2http://www.statmt.org/moses en-es Data statistics en es # Sentences 7792118 7792118 # Words 98347681</context>
</contexts>
<marker>Sridhar, Barbosa, Bangalore, 2011</marker>
<rawString>V. K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore. 2011. A scalable approach to building a parallel corpus from the Web. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sankaran</author>
<author>A Grewal</author>
<author>A Sarkar</author>
</authors>
<title>Incremental decoding for phrase-based statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the fifth Workshop on Statistical Machine Translation and Metrics.</booktitle>
<contexts>
<context position="3058" citStr="Sankaran et al., 2010" startWordPosition="427" endWordPosition="430">lingual communication when participants are geographically distant, a scenario more likely in a global setting. In such a scenario, it is imperative to provide real-time and low latency communication. In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. Similarly, cross-lingual dialog between two remote participants will greatly benefit through incremental translation. While incremental decoding for text translation has been addressed previously in (Furuse and Iida, 1996; Sankaran et al., 2010), we address the problem in a speech-to-speech translation setting for enabling real-time cross-lingual dialog. We address the problem of incrementality in a novel session initiation protocol (SIP) based S2S translation system that enables two people to interact and engage in crosslingual dialog over a telephone (mobile phone or landline). Our system performs incremental speech recognition and translation, allowing for low latency interaction that provides an ideal setting for remote dialog aimed at accomplishing a task. We present previous work in this area in Section 2 and introduce the prob</context>
<context position="6075" citStr="Sankaran et al., 2010" startWordPosition="902" endWordPosition="905">e modality for communication. One of the first attempts for two-way S2S translation over a telephone between two potentially remote participants was made as part of the Verbmobil project (Wahlster, 2000). The system was restricted to certain topics and speech was the only modality. Furthermore, the spontaneous translation of dialogs was not incremental. One of the first attempts at incremental text translation was demonstrated in (Furuse and Iida, 1996) using a transfer-driven machine translation approach. More recently, an incremental decoding framework for text translation was presented in (Sankaran et al., 2010). To the best of our knowledge, incremental speech-to-speech translation in a dialog setting has not been addressed in prior work. In this work, we address this problem using first of a kind SIP-based large vocabulary S2S translation system that can work with both smartphones and landlines. The speech translation is performed incrementally based on generation of partial hypotheses from speech recognition. Our system displays the recognized and translated text in an incremental fashion. The use of SIP-based technology also supports an open form of cross-lingual dialog without the need for atten</context>
</contexts>
<marker>Sankaran, Grewal, Sarkar, 2010</marker>
<rawString>B. Sankaran, A. Grewal, and A. Sarkar. 2010. Incremental decoding for phrase-based statistical machine translation. In Proceedings of the fifth Workshop on Statistical Machine Translation and Metrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="21649" citStr="Snover et al., 2006" startWordPosition="3432" endWordPosition="3435">biaremos de habitaci´on. A: Thank you very much. B: Estamos aqu para servirle. Ll´amenos si necesita algo m´as. Table 2: Example of a sample dialog scenario. contextual information was used in these experiments, i.e., the audio utterances were decoded independently. The ASR WER for English and Spanish sides of the dialogs is shown in Figure 3. The average WER for English and Spanish side of the conversations is 27.73% and 22.83%, respectively. The recognized utterances were subsequently translated using the MT system described above. The MT performance in terms of Translation Edit Rate (TER) (Snover et al., 2006) and BLEU (Papineni et al., 2002) is shown in Figure 4. The MT performance is shown across all the turns for both reference transcriptions and ASR output. The results show that the performance of the Spanish-English MT model is better in comparison to the EnglishSpanish model on the dialog corpus. The performance on ASR input drops by about 18% compared to translation on reference text. 442 34.0 28.21 26.96 25.5 23.87 17.0 8.5 0 33.58 Spanish-English English-Spanish Spanish-English 70.0 63.42 English-Spanish 55.34 59.19 52.5 47.26 BLEU TER 35.0 17.5 0 Reference ASR Reference ASR Figure 4: TER </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>B Pouliquen</author>
<author>A Widiger</author>
<author>C Ignat</author>
<author>T Erjavec</author>
<author>D Tufis</author>
</authors>
<title>The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="13646" citStr="Steinberger et al., 2006" startWordPosition="2116" endWordPosition="2119">es) was used for reordering the phrases in addition to the standard distance based reordering (distortion-limit of 6). The language models were interpolated Kneser-Ney discounted trigram models, all constructed using the SRILM toolkit (Stolcke, 2002). Minimum error rate training (MERT) was performed on a development set to optimize the feature weights of the log-linear model used in translation. During decoding, the unknown words were preserved in the hypotheses. The parallel corpus for phrase-based translation was obtained from a variety of sources: europarl (Koehn, 2005), jrc-acquis corpus (Steinberger et al., 2006), opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), web crawling as well as human translation. The statistics of the data used for English-Spanish is shown in Table 1. About 30% of the training data was obtained from the Web (Rangarajan Sridhar et al., 2011). The development set (identical to the one used in ASR) was used in MERT training as well as perplexity based optimization of the interpolated language model. The language model for MT and ASR was constructed from identical data. 4.3 Text-to-speech synthesis The translated sentence from the machine translation component is synthesize</context>
</contexts>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis, 2006</marker>
<rawString>R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, and D. Tufis. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP.</booktitle>
<contexts>
<context position="13271" citStr="Stolcke, 2002" startWordPosition="2059" endWordPosition="2060">nces from which we learn word alignments by using GIZA++ toolkit (Och and Ney, 2003). The bidirectional word alignments obtained using GIZA++ were consolidated by using the grow-diag-final option in Moses. Subsequently, we learn phrases (maximum length of 7) from the consolidated word alignments. A lexicalized reordering model (msd-bidirectional-fe option in Moses) was used for reordering the phrases in addition to the standard distance based reordering (distortion-limit of 6). The language models were interpolated Kneser-Ney discounted trigram models, all constructed using the SRILM toolkit (Stolcke, 2002). Minimum error rate training (MERT) was performed on a development set to optimize the feature weights of the log-linear model used in translation. During decoding, the unknown words were preserved in the hypotheses. The parallel corpus for phrase-based translation was obtained from a variety of sources: europarl (Koehn, 2005), jrc-acquis corpus (Steinberger et al., 2006), opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), web crawling as well as human translation. The statistics of the data used for English-Spanish is shown in Table 1. About 30% of the training data was obtained from th</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
<author>L Lars Nygaard</author>
</authors>
<title>The OPUS corpus - parallel &amp; free.</title>
<date>2004</date>
<booktitle>In Proceedings of</booktitle>
<editor>LREC. Wolfgang Wahlster, editor.</editor>
<publisher>Springer.</publisher>
<marker>Tiedemann, Nygaard, 2004</marker>
<rawString>J. Tiedemann and L. Lars Nygaard. 2004. The OPUS corpus - parallel &amp; free. In Proceedings of LREC. Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of Speech-to-Speech Translation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waibel</author>
<author>A N Jain</author>
<author>A E McNair</author>
<author>H Saito</author>
<author>A G Hauptmann</author>
<author>J Tebelskis</author>
</authors>
<title>JANUS: a speech-to-speech translation system using connectionist and symbolic processing strategies.</title>
<date>1991</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>793--796</pages>
<location>Los Alamitos, CA, USA.</location>
<contexts>
<context position="4532" citStr="Waibel et al., 1991" startWordPosition="656" endWordPosition="659">Computational Linguistics: Human Language Technologies, pages 437–445, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics framework for real-time translation in Section 5. In Section 6, we describe the basic call flow of our system following which we present dialog experiments performed using our framework in Section 8. Finally, we conclude in Section 9 along with directions for future work. 2 Previous Work Most previous work on speech-to-speech translation systems has focused on a single device model, i.e., the user interface for translation is on one device (Waibel et al., 1991; Metze et al., 2002; Zhou et al., 2003; Waibel et al., 2003). The device typically supports multiple source-target language pairs. A user typically chooses the directionality of translation and a toggle feature is used to switch the directionality. However, this requires physical presence of the two conversants in one location. On the other hand, text chat between users over cell phones has become increasingly popular in the last decade. While the language used in the interaction is typically monolingual, there have been attempts to use statistical machine translation to enable cross-lingual </context>
</contexts>
<marker>Waibel, Jain, McNair, Saito, Hauptmann, Tebelskis, 1991</marker>
<rawString>A. Waibel, A. N. Jain, A. E. McNair, H. Saito, A. G. Hauptmann, and J. Tebelskis. 1991. JANUS: a speech-to-speech translation system using connectionist and symbolic processing strategies. In Proceedings of ICASSP, pages 793–796, Los Alamitos, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waibel</author>
<author>A Badran</author>
<author>A W Black</author>
<author>R Frederking</author>
<author>G Gates</author>
<author>A Lavie</author>
<author>L Levin</author>
<author>K Lenzo</author>
<author>L M Tomokiyo</author>
<author>J Reichert</author>
<author>T Schultz</author>
<author>W Dorcas</author>
<author>M Woszczyna</author>
<author>J Zhang</author>
</authors>
<title>Speechalator: two-way speech-to-speech translation on a consumer PDA.</title>
<date>2003</date>
<booktitle>In Proceedings of the European Conference on Speech Communication and Technology,</booktitle>
<pages>369--372</pages>
<contexts>
<context position="1921" citStr="Waibel et al., 2003" startWordPosition="259" endWordPosition="262">ncremental approach. 1 Introduction In recent years, speech-to-speech translation (S2S) technology has played an increasingly important role in narrowing the language barrier in crosslingual interpersonal communication. The improvements in automatic speech recognition (ASR), statistical machine translation (MT), and, text-to-speech synthesis (TTS) technology has facilitated the serial binding of these individual components to achieve S2S translation of acceptable quality. Prior work on S2S translation has primarily focused on providing either one-way or two-way translation on a single device (Waibel et al., 2003; Zhou et al., 2003). Typically, the user interface requires the participant(s) to choose the source and target language apriori. The nature of communication, either single user talking or turn taking between two users can result in a one-way or cross-lingual dialog interaction. In most systems, the necessity to choose the directionality of translation for each turn does take away from a natural dialog flow. Furthermore, single interface based S2S translation (embedded or cloudbased) is not suitable for cross-lingual communication when participants are geographically distant, a scenario more l</context>
<context position="4593" citStr="Waibel et al., 2003" startWordPosition="668" endWordPosition="671">s 437–445, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics framework for real-time translation in Section 5. In Section 6, we describe the basic call flow of our system following which we present dialog experiments performed using our framework in Section 8. Finally, we conclude in Section 9 along with directions for future work. 2 Previous Work Most previous work on speech-to-speech translation systems has focused on a single device model, i.e., the user interface for translation is on one device (Waibel et al., 1991; Metze et al., 2002; Zhou et al., 2003; Waibel et al., 2003). The device typically supports multiple source-target language pairs. A user typically chooses the directionality of translation and a toggle feature is used to switch the directionality. However, this requires physical presence of the two conversants in one location. On the other hand, text chat between users over cell phones has become increasingly popular in the last decade. While the language used in the interaction is typically monolingual, there have been attempts to use statistical machine translation to enable cross-lingual text communication (Chen and Raman, 2008). But this introduce</context>
</contexts>
<marker>Waibel, Badran, Black, Frederking, Gates, Lavie, Levin, Lenzo, Tomokiyo, Reichert, Schultz, Dorcas, Woszczyna, Zhang, 2003</marker>
<rawString>A. Waibel, A. Badran, A. W. Black, R. Frederking, G. Gates, A. Lavie, L. Levin, K. Lenzo, L. M. Tomokiyo, J. Reichert, T. Schultz, W. Dorcas, M. Woszczyna, and J. Zhang. 2003. Speechalator: two-way speech-to-speech translation on a consumer PDA. In Proceedings of the European Conference on Speech Communication and Technology, pages 369– 372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhou</author>
<author>Y Gao</author>
<author>J Sorenson</author>
<author>D Dechelotte</author>
<author>M Picheny</author>
</authors>
<title>A hand-held speech-to-speech translation system.</title>
<date>2003</date>
<booktitle>In Proceedings of ASRU.</booktitle>
<contexts>
<context position="1941" citStr="Zhou et al., 2003" startWordPosition="263" endWordPosition="266">1 Introduction In recent years, speech-to-speech translation (S2S) technology has played an increasingly important role in narrowing the language barrier in crosslingual interpersonal communication. The improvements in automatic speech recognition (ASR), statistical machine translation (MT), and, text-to-speech synthesis (TTS) technology has facilitated the serial binding of these individual components to achieve S2S translation of acceptable quality. Prior work on S2S translation has primarily focused on providing either one-way or two-way translation on a single device (Waibel et al., 2003; Zhou et al., 2003). Typically, the user interface requires the participant(s) to choose the source and target language apriori. The nature of communication, either single user talking or turn taking between two users can result in a one-way or cross-lingual dialog interaction. In most systems, the necessity to choose the directionality of translation for each turn does take away from a natural dialog flow. Furthermore, single interface based S2S translation (embedded or cloudbased) is not suitable for cross-lingual communication when participants are geographically distant, a scenario more likely in a global se</context>
<context position="4571" citStr="Zhou et al., 2003" startWordPosition="664" endWordPosition="667"> Technologies, pages 437–445, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics framework for real-time translation in Section 5. In Section 6, we describe the basic call flow of our system following which we present dialog experiments performed using our framework in Section 8. Finally, we conclude in Section 9 along with directions for future work. 2 Previous Work Most previous work on speech-to-speech translation systems has focused on a single device model, i.e., the user interface for translation is on one device (Waibel et al., 1991; Metze et al., 2002; Zhou et al., 2003; Waibel et al., 2003). The device typically supports multiple source-target language pairs. A user typically chooses the directionality of translation and a toggle feature is used to switch the directionality. However, this requires physical presence of the two conversants in one location. On the other hand, text chat between users over cell phones has become increasingly popular in the last decade. While the language used in the interaction is typically monolingual, there have been attempts to use statistical machine translation to enable cross-lingual text communication (Chen and Raman, 200</context>
</contexts>
<marker>Zhou, Gao, Sorenson, Dechelotte, Picheny, 2003</marker>
<rawString>B. Zhou, Y. Gao, J. Sorenson, D. Dechelotte, and M. Picheny. 2003. A hand-held speech-to-speech translation system. In Proceedings of ASRU.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>