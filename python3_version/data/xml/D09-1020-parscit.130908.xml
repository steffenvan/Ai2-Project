<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000101">
<title confidence="0.962059">
Subjectivity Word Sense Disambiguation
</title>
<author confidence="0.980546">
Cem Akkaya and Janyce Wiebe Rada Mihalcea
</author>
<affiliation confidence="0.999402">
University of Pittsburgh University of North Texas
</affiliation>
<email confidence="0.99762">
icem,wiebel@cs.pitt.edu rada@cs.unt.edu
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999760083333333">
This paper investigates a new task, subjec-
tivity word sense disambiguation (SWSD),
which is to automatically determine which
word instances in a corpus are being used
with subjective senses, and which are be-
ing used with objective senses. We pro-
vide empirical evidence that SWSD is
more feasible than full word sense dis-
ambiguation, and that it can be exploited
to improve the performance of contextual
subjectivity and sentiment analysis sys-
tems.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999829">
The automatic extraction of opinions, emotions,
and sentiments in text (subjectivity analysis) to
support applications such as product review min-
ing, summarization, question answering, and in-
formation extraction is an active area of research
in NLP.
Many approaches to opinion, sentiment, and
subjectivity analysis rely on lexicons of words that
may be used to express subjectivity. Examples of
such words are the following (in bold):
</bodyText>
<equation confidence="0.854442">
(1) He is a disease to every team he has gone to.
</equation>
<bodyText confidence="0.994152055555556">
Converting to SMF is a headache.
The concert left me cold.
That guy is such a pain.
Knowing the meaning (and thus subjectivity) of
these words would help a system recognize the
negative sentiments in these sentences.
Most subjectivity lexicons are compiled as lists
of keywords, rather than word meanings (senses).
However, many keywords have both subjective
and objective senses. False hits – subjectivity
clues used with objective senses – are a signifi-
cant source of error in subjectivity and sentiment
analysis. For example, even though the follow-
ing sentence contains all of the negative keywords
above, it is nevertheless objective, as they are all
false hits:
(2) Early symptoms of the disease include severe
headaches, red eyes, fevers and cold chills, body
pain, and vomiting.
To tackle this source of error, we define a
new task, subjectivity word sense disambigua-
tion (SWSD), which is to automatically determine
which word instances in a corpus are being used
with subjective senses, and which are being used
with objective senses. We hypothesize that SWSD
is more feasible than full word sense disambigua-
tion, because it is more coarse grained – often, the
exact sense need not be pinpointed. We also hy-
pothesize that SWSD can be exploited to improve
the performance of contextual subjectivity analy-
sis systems via sense-aware classification.
The paper consists of two parts. In the first
part, we build and evaluate a targeted supervised
SWSD system that aims to disambiguate members
of a subjectivity lexicon. It labels clue instances as
having a subjective sense or an objective sense in
context. The system relies on common machine
learning features for word sense disambiguation
(WSD). The performance is substantially above
both baseline and the performance of full WSD
on the same data, suggesting that the task is feasi-
ble, and that subjectivity provides a natural coarse-
grained grouping of senses.
The second part demonstrates the promise of
SWSD for contextual subjectivity analysis. First,
we show that subjectivity sense ambiguity is
highly prevalent in the MPQA opinion-annotated
corpus (Wiebe et al., 2005; Wilson, 2008), thus
establishing the potential benefit of performing
SWSD. Then, we exploit SWSD to improve per-
formance on several subjectivity analysis tasks,
from subjective/objective sentence-level classi-
fication to positive/negative/neutral expression-
level classification. To our knowledge, this is the
</bodyText>
<page confidence="0.970083">
190
</page>
<note confidence="0.9966095">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190–199,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.993154333333333">
first attempt to explicitly use sense-level subjec-
tivity tags in contextual subjectivity and sentiment
analysis.
</bodyText>
<sectionHeader confidence="0.957502" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999226090909091">
We adopt the definitions of subjective and objec-
tive from (Wiebe et al., 2005; Wiebe and Mi-
halcea, 2006; Wilson, 2008). Subjective expres-
sions are words and phrases being used to ex-
press mental and emotional states, such as spec-
ulations, evaluations, sentiments, and beliefs. A
general covering term for such states is private
state (Quirk et al., 1985), an internal state that
cannot be directly observed or verified by others.
(Wiebe and Mihalcea, 2006) give the following
examples:
</bodyText>
<listItem confidence="0.558998">
(3) His alarm grew.
</listItem>
<bodyText confidence="0.922895666666667">
He absorbed the information quickly.
UCC/Disciples leaders roundly condemned the
Iranian President’s verbal assault on Israel.
</bodyText>
<subsectionHeader confidence="0.60337">
What’s the catch?
</subsectionHeader>
<bodyText confidence="0.999767896551724">
Polarity (also called semantic orientation) is
also important to NLP applications. In review
mining, for example, we want to know whether
an opinion about a product is positive or negative.
Nonetheless, as argued by (Wiebe and Mihalcea,
2006; Su and Markert, 2008), there are also mo-
tivations for a separate subjective/objective (S/O)
classification.
First, expressions may be subjective but not
have any particular polarity. An example given by
(Wilson et al., 2005a) is Jerome says the hospi-
tal feels no different than a hospital in the states.
An NLP application system may want to find a
wide range of private states attributed to a person,
such as their motivations, thoughts, and specula-
tions, in addition to their positive and negative sen-
timents. Second, benefits for sentiment analysis
can be realized by decomposing the problem into
S/O (or neutral versus polar) and polarity classifi-
cation (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Wilson et al., 2005a; Kim and Hovy,
2006). We will see further evidence of this in Sec-
tion 4.2.3 in this paper.
The contextual subjectivity analysis experi-
ments in Section 4 include both S/O and polarity
classifications. The data used in those experiments
is from the MPQA Corpus (Wiebe et al., 2005;
Wilson, 2008),1 which consists of texts from the
world press annotated for subjective expressions.
</bodyText>
<footnote confidence="0.993632">
1Available at http://www.cs.pitt.edu/mpqa
</footnote>
<bodyText confidence="0.9698601">
In the MPQA Corpus, subjective expressions of
varying lengths are marked, from single words to
long phrases. In addition, other properties are an-
notated, including polarity.
For SWSD, we need the notions of subjective
and objective senses of words in a dictionary. We
adopt the definitions from (Wiebe and Mihalcea,
2006), who describe the annotation scheme as fol-
lows. Classifying a sense as S means that, when
the sense is used in a text or conversation, one ex-
pects it to express subjectivity, and also that the
phrase or sentence containing it expresses subjec-
tivity. As noted in (Wiebe and Mihalcea, 2006),
sentences containing objective senses may not be
objective. Thus, objective senses are defined as
follows: Classifying a sense as O means that,
when the sense is used in a text or conversation,
one does not expect it to express subjectivity and,
if the phrase or sentence containing it is subjective,
the subjectivity is due to something else. Finally,
classifying a sense as B means it covers both sub-
jective and objective usages.
The following subjective examples are given in
(Wiebe and Mihalcea, 2006):
His alarm grew.
alarm, dismay, consternation – (fear resulting from the aware-
ness of danger)
_&gt; fear, fearfulness, fright – (an emotion experienced in
anticipation of some specific pain or danger (usually ac-
companied by a desire to flee or fight))
</bodyText>
<subsectionHeader confidence="0.852738">
What’s the catch?
</subsectionHeader>
<bodyText confidence="0.933106666666667">
catch – (a hidden drawback; “it sounds good but what’s the
catch?”)
_&gt; drawback – (the quality of being a hindrance; “he
pointed out all the drawbacks to my plan”)
They give the following objective examples:
The alarm went off.
alarm, warning device, alarm system – (a device that signals
the occurrence of some undesirable event)
_&gt; device – (an instrumentality invented for a particu-
lar purpose; “the device is small enough to wear on your
wrist”; “a device intended to conserve water”)
He sold his catch at the market.
catch, haul – (the quantity that was caught; “the catch was
only 10 fish”)
_&gt; indefinite quantity – (an estimated quantity)
Wiebe and Mihalcea performed an agreement
study and report that good agreement (κ=0.74) can
be achieved between human annotators labeling
the subjectivity of senses. For a similar task, (Su
and Markert, 2008) also report good agreement
(κ=0.79).
</bodyText>
<page confidence="0.999105">
191
</page>
<sectionHeader confidence="0.9984755" genericHeader="method">
3 Subjectivity Word Sense
Disambiguation
</sectionHeader>
<subsectionHeader confidence="0.999972">
3.1 Task Definition and Method
</subsectionHeader>
<bodyText confidence="0.979826928571429">
We now turn to SWSD, and our method for per-
forming it.
Note that SWSD is midway between pure dic-
tionary classification and pure contextual interpre-
tation. For SWSD, the context of the word is con-
sidered in order to perform the task, but the sub-
jectivity is determined solely by the dictionary. In
contrast, full contextual interpretation can deviate
from a sense’s subjectivity label in the dictionary.
As noted above, words used with objective senses
may appear in subjective expressions. For exam-
ple, an SWSD system would label the following
examples of alarm as S, O and O, respectively. On
the other hand, a sentence-level subjectivity clas-
sifier would label the sentences as S, S, and O, re-
spectively.
(4) His alarm grew.
Will someone shut that darn alarm off?
The alarm went off.
We use a supervised approach to SWSD. We
train a different classifier for each lexicon entry
for which we have training data. Thus, our ap-
proach is like targeted WSD (in contrast to all-
words WSD), with two labels: S and O.
We borrow machine learning features which
have been successfully used in WSD. Specifically,
given an ambiguous target word, we use the fol-
lowing features from (Mihalcea, 2002):
</bodyText>
<equation confidence="0.794326333333333">
CW : the target word itself
CP : POS of the target word
CF : surrounding context of 3 words and their POS
</equation>
<bodyText confidence="0.9314865">
HNP : the head of the noun phrase to which the
target word belongs
NB : the first noun before the target word
VB : the first verb before the target word
NA : the first noun after the target word
VA : the first verb after the target word
SK : at most 10 context words occurring at least 5
times; determined for each sense
</bodyText>
<subsectionHeader confidence="0.999821">
3.2 Lexicon and Data
</subsectionHeader>
<bodyText confidence="0.9995688">
Our target words are members of a subjectivity
lexicon, because, since they are in such a lexicon,
we know they have subjective usages. Specifically,
we use the lexicon of (Wilson et al., 2005b; Wil-
son, 2008).2 The entries have been divided into
</bodyText>
<footnote confidence="0.981103">
2Available at http://www.cs.pitt.edu/mpqa
</footnote>
<bodyText confidence="0.984741265306123">
those that are strongly subjective (strongsubj) and
those that are weakly subjective (weaksubj), re-
flecting their reliability as subjectivity clues. The
sources of the entries in the lexicon are identified
in (Wilson, 2008). In the second part of this pa-
per, we evaluate systems against the MPQA cor-
pus. Wilson also uses this corpus for her eval-
uations. To enable this, entries were added to
the lexicon independently from the MPQA corpus
(that is, none of the entries were derived using the
MPQA corpus).
The training and test data for SWSD consists of
word instances in a corpus labeled as S or O, indi-
cating whether they are used with a subjective or
objective sense. Because we do not have data la-
beled with the S/O coarse-grained senses and we
did not want to undertake the annotation effort at
this stage, we created an annotated corpus by com-
bining two types of sense annotations: (1) labels
of senses within a dictionary as S or O (i.e., sub-
jectivity sense labels), and (2) sense tags of word
instances in a corpus (i.e., sense-tagged data). The
subjectivity sense labels are used to collapse the
sense labels in the sense-tagged data into the two
new senses, S and O.
Our sense-tagged data are the lexical sample
corpora (training and test data) from SENSEVAL1
(Kilgarriff and Palmer, 2000), SENSEVAL2 (Preiss
and Yarowsky, 2001), and SENSEVAL3 (Mihal-
cea and Edmonds, 2004). We selected all of the
SENSEVAL words that are also in the subjectivity
lexicon, and labeled their dictionary senses as S,
O, or B according to the annotation scheme de-
scribed above in Section 2. We did this subjectiv-
ity sense labeling according to the sense inventory
of the underlying corpus (Hector for SENSEVAL1;
WordNet1.7 for SENSEVAL2; and WordNet1.7.1
for SENSEVAL3).
Among the words, we found that 11 are not
ambiguous - either they have only S or only O
senses (in the corresponding sense inventory), or
the senses of their instances in the SENSEVAL data
are all S or all O. So as not to inflate our results, we
removed those 11 from the data, leaving 39 words.
In addition, we excluded the senses labeled B (a to-
tal of 10 senses). This leaves a total of 372 senses:
9 words (64 senses) from SENSEVAL1, 18 words
(201 senses) from SENSEVAL2, and 12 words (107
senses) from SENSEVAL3.
</bodyText>
<page confidence="0.991363">
192
</page>
<table confidence="0.9987838">
Base Acc SP SR SF OP OR OF IB EB(%)
All 79.9 88.3 89.3 89.1 89.2 87.1 87.4 87.2 8.4 41.8
S1 57.9 80.7 81.1 78.3 79.7 80.2 82.9 81.5 22.8 54.2
S2 81.1 87.3 86.5 85.2 85.8 87.9 89.0 88.4 6.2 32.8
S3 95.0 96.4 96.5 99.0 97.7 96.3 87.8 91.8 1.4 28.0
</table>
<tableCaption confidence="0.953526666666667">
Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,
SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute
improvement in Acc over Base; EB is percent error reduction in Acc.
</tableCaption>
<subsectionHeader confidence="0.997891">
3.3 SWSD Experiments
</subsectionHeader>
<bodyText confidence="0.999973957446809">
In this section, we evaluate our SWSD system, and
compare its performance to an WSD system on the
same data.
Note that, although generally in the SENSEVAL
datasets, training and test data are provided sep-
arately, a few target words from SENSEVAL1 do
not have both training and testing data. Thus, we
opted to combine the training and test data into one
dataset, and then perform 10-fold cross validation
experiments.
For our classifier, we use the SVM classifier
from the Weka package (Witten and Frank., 2005)
with its default settings.
We were interested in how well the system
would perform on more and less ambiguous
words. Thus, we split the words into three sub-
sets according to their majority-class baselines,
and report separate results: S1 (9 words), S2 (18
words), and S3 (12 words) have majority-class
baselines in the intervals [50%,70%) , [70%,90%),
and [90%,100%), respectively.
Table 1 contains the results, giving the overall
results (micro averages), as well as results for the
subsets S1, S2, and S3.
The improvement for SWSD over baseline is
especially high for the less skewed set, S1. This
is very encouraging because these words are the
more ambiguous words, and thus are the ones that
most need SWSD (assuming the SENSEVAL pri-
ors are similar to the priors in the corpus). The
average error reduction over baseline for S1 words
is 54.2%. Even for the more skewed sets S2 and
S3, reductions are 32.8% and 28.0%, respectively,
with an overall reduction of 41.8%.
To compare SWSD with WSD, we re-ran the
10-fold cross validation experiments, but this time
using the original sense labels, rather than S
and O. The (micro-averaged) accuracy is 67.9%,
much lower than the overall accuracy for SWSD
(88.3%).
The positive results provide evidence that
SWSD is a feasible variant of WSD, and that the
S/O sense groupings are natural ones, since the
system is able to learn to distinguish between them
with high accuracy. There is also potential for im-
provement by using a richer feature set, including
subjectivity features.
</bodyText>
<sectionHeader confidence="0.9205985" genericHeader="method">
4 Opinion Analysis with Subjectivity
Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.999941636363636">
In this section, we explore the promise of SWSD
for contextual subjectivity analysis. First, we pro-
vide evidence that a subjectivity lexicon can have
substantial coverage of the subjective expressions
in a corpus, yet still be responsible for significant
subjectivity sense ambiguity in that corpus. Then,
we exploit SWSD in several contextual opinion
analysis systems, comparing the performance of
sense-aware and non-sense-aware versions. They
are all variations of components of the Opinion-
Finder opinion recognition system.3
</bodyText>
<subsectionHeader confidence="0.9063295">
4.1 Coverage and Ambiguity of Lexicon
Entries in the MPQA Corpus
</subsectionHeader>
<bodyText confidence="0.9995989375">
In this section, we consider the distribution of lex-
icon entries in the MPQA corpus.
The lexicon covers a substantial subset of the
subjective expressions in the corpus: 67.1% of the
subjective expressions contain one or more lexi-
con entries.
On the other hand, fully 42.9% of the instances
of the lexicon entries in the MPQA corpus are
not in subjective expressions. An instance that
is not in a subjective expression is, by definition,
being used with an objective sense. Thus, these
instances are false hits of subjectivity clues. As
mentioned above, the entries in the lexicon have
been pre-classified as either more (strongsubj) or
less (weaksubj) reliable. We see this difference re-
flected in their degree of ambiguity – 53% of the
</bodyText>
<footnote confidence="0.998092">
3Available at http://www.cs.pitt.edu/opin
</footnote>
<page confidence="0.998102">
193
</page>
<bodyText confidence="0.999957666666667">
weaksubj instances are false hits, while only 22%
of the strongsubj instances are.
The high coverage of the lexicon demonstrates
its potential usefulness for opinion analysis sys-
tems, while its degree of ambiguity, in the form of
false hits in a subjectivity annotated corpus, shows
the potential benefit to opinion analysis of per-
forming SWSD.
As mentioned above, our experiments involve
only lexicon entries that are covered by the SEN-
SEVAL data, as we did not perform manual sense
tagging for this work. We have hope to expand
the system’s coverage in the future, as more word-
sense tagged data is produced (e.g., ONTONOTES
(Hovy et al., 2006)). We also have evidence that a
moderate amount of manual annotation would be
worth the effort. For example, let us order the lexi-
con entries from highest to lowest by frequency in
the MPQA corpus. The top 20 are responsible for
25% of all false hits in the corpus; the top 40 are
responsible for 34%; and the top 80 are responsi-
ble for 44%. If the SWSD system could be trained
for these words, the potential impact on reducing
false hits could be substantial, especially consid-
ering the good performance of the SWSD system
on the more ambiguous words. Note that we do
not want to simply discard these clues. The top 20
cover 9.4% of all subjective expressions; the top
40 cover 15.4%; and the top 80 cover 29.5%. Note
that SWSD only needs the data annotated with the
coarse-grained binary labels, which should be less
time consuming to produce than full word sense
tags.
</bodyText>
<subsectionHeader confidence="0.966271">
4.2 Contextual Classification
</subsectionHeader>
<bodyText confidence="0.999953481481481">
We found in Section 3.3 that SWSD is a feasible
task and then in Section 4.1 that there is a great
deal of subjectivity sense ambiguity in a standard
subjectivity-annotated corpus (MPQA). We now
turn to exploiting the results of SWSD to automat-
ically recognize subjectivity and sentiment in the
MPQA corpus.
A motivation for using the MPQA data is that
many types of classifiers have been evaluated on
it, and we can directly test the effect of SWSD on
these classifiers.
Note that, for the SWSD experiments, the num-
ber of words does not limit the amount of data,
as SENSEVAL provides data for each word. How-
ever, the only parts of the MPQA corpus for which
SWSD could affect performance is the subset con-
taining instances of the words in the SWSD sys-
tem’s coverage. Thus, for the classifiers in this
section, the data used is the SenMPQA dataset,
which consists of the sentences in the MPQA Cor-
pus that contain at least one instance of the 39 key-
words. There are 689 such sentences (containing,
in total, 723 instances of the 39 keywords).
Even though this dataset is smaller than the one
used above, it gives us enough data to draw con-
clusions according to McNemar’s test for statisti-
cal significance.
</bodyText>
<subsubsectionHeader confidence="0.525475">
4.2.1 Rule-based Classifier
</subsubsectionHeader>
<bodyText confidence="0.997479871794872">
We first apply SWSD to the rule-based classifier
from (Riloff and Wiebe, 2003). The classifier,
which is a sentence-level S/O classifier, has low
subjective and objective recall but high subjective
and objective precision. It is useful for creating
training data for subsequent processing by apply-
ing it to large amounts of unannotated data.
The classifier is a good candidate for directly
measuring the effects of SWSD on contextual sub-
jectivity analysis, because it classifies sentences
only by looking for the presence of subjectivity
keywords. Performance will improve if false hits
can be ignored.
The classifier labels a sentence as S if it contains
two or more strongsubj clues. On the other hand,
it considers three conditions to classify a sentence
as O: there are no strongsubj clues in the current
sentence, there are together at most one strongsubj
clue in the previous and next sentence, and there
are together at most 2 weaksubj clues in the cur-
rent, previous, and next sentence. A sentence that
is not labeled S or O is labeled unknown.
The rule-based classifier is made sense aware
by making it blind to the target word instances la-
beled O by the SWSD system, as these represent
false hits of subjectivity keywords. We compare
this sense-aware method (SE), with the original
classifier (ORB), in order to see if SWSD would
improve performance. We also built another modi-
fied rule-based classifier RE to demonstrate the ef-
fect of randomly ignoring subjectivity keywords.
RE ignores a keyword instance randomly with a
probability of 0.429, the expected value of false
hits in the MPQA corpus. The results are listed in
Table 2.
The rule-based classifier looks for the presence
of the keywords to find subjective sentences and
for the absence of the keywords to find objective
sentences. It is obvious that a variant working on
</bodyText>
<page confidence="0.996778">
194
</page>
<table confidence="0.975272">
Acc OP OR OF SP SR SF
ORB 27.0 50.0 4.1 7.6 92.7 36.0 51.8
SE 28.3 62.1 9.3 16.1 92.7 35.8 51.6
RE 27.6 48.4 7.7 13.3 92.6 35.4 51.2
</table>
<tableCaption confidence="0.828982">
Table 2: Effect of SWSD on the rule-based classi-
fiers.
</tableCaption>
<bodyText confidence="0.999676052631579">
fewer keyword instances than ORB will always
have the same or higher objective recall and the
same or lower subjective recall than ORB. That is
the case for both SE and RE. The real benefit we
see is in objective precision, which is substantially
higher for SE than ORB. For our experiments, OP
gives a better idea of the impact of SWSD, be-
cause most of the keyword instances SWSD dis-
ambiguates are weaksubj clues, and weaksubj key-
words figure more prominently in objective classi-
fication. On the other hand, RE has both lower OP
and SP than ORB. Note that accuracy for all three
systems is low, because all unknown predictions
are counted as incorrect.
These findings suggest that SWSD performs
well on disambiguating keyword instances in the
MPQA corpus,4 and demonstrates a positive im-
pact of SWSD on sentence-level subjectivity clas-
sification.
</bodyText>
<subsubsectionHeader confidence="0.465864">
4.2.2 Subjective/Objective Classifier
</subsubsectionHeader>
<bodyText confidence="0.9997454">
We now move to more fine-grained expression-
level subjectivity classification. Since sentences
often contain multiple subjective expressions,
expression-level classification is more informative
than sentence-level classification.
The classifier in this section is an implementa-
tion of the neutral/polar supervised classifier of
(Wilson et al., 2005a) (using the same features),
except that the classes are S/O rather than neu-
tral/polar. These classifiers label instances of lex-
icon entries. The gold standard is defined on the
MPQA Corpus as follows: If an instance is in a
subjective expression, it is contextually S. If the
instance is in an objective expression, it is contex-
tually O. We evaluate the system on the 723 clue
instances in the SenMPQA dataset.
We incorporate SWSD information into the
contextual subjectivity classifier in a straight-
forward fashion: outputs are modified according
to simple, intuitive rules.
</bodyText>
<footnote confidence="0.8916565">
4which we cannot evaluate directly, as the MPQA corpus
is not sense tagged.
</footnote>
<bodyText confidence="0.969841377358491">
Our strategy is defined by the relation between
sense subjectivity and contextual subjectivity and
involves two rules, R1 and R2.
We know that a keyword instance used with a
S sense must be in a subjective expression. R1 is
to simply trust SWSD: If the contextual classifier
labels an instance as O, but SWSD determines that
it has an S sense, then R1 flips the contextual clas-
sifier’s label to S.
Things are not as simple in the case of O senses,
since they may appear in both subjective and ob-
jective expressions. We will state R2, and then ex-
plain it: If the contextual classifier labels an in-
stance as S, but (1) SWSD determines that it has
an O sense, (2) the contextual classifier’s confi-
dence is low, and (3) there is no other subjective
keyword in the same expression, then R2 flips the
contextual classifier’s label to O. First, consider
confidence: though a keyword with an O sense
may appear in either subjective or objective ex-
pressions, it is more likely to appear in an objec-
tive expression. We assume that this is reflected
to some extent in the contextual classifier’s confi-
dence. Second, if a keyword with an O sense ap-
pears in a subjective expression, then the subjec-
tivity is not due to that keyword but rather due to
something else. Thus, the presence of another lex-
icon entry “explains away” the presence of the O
sense in the subjective expression, and we do not
want SWSD to overrule the contextual classifier.
Only when the contextual classifier isn’t certain
and only when there isn’t another keyword does
R2 flip the label to O.
Our definition of low confidence is in terms
of the label weights assigned by BoosTexter
(Schapire and Singer, 2000), which is the under-
lying machine learning algorithm of the classifier.
We use the difference between the largest label
weight and the second largest label weight as a
measure of confidence, as suggested in the Boos-
Texter documentation. The threshold we use is
0.0008.5
We apply the contextual classifier and the
SWSD system to the data, and compare the per-
formance of the original system (OS/O) and three
sense-aware variants: one using only R1, one us-
5As will be noted below, we experimented with three
thresholds for the classifier in Section 4.2.3, with no signif-
icant difference in accuracy. Here, we simply adopt 0.0008,
without further experimentation. In addition, we did not ex-
periment with other conditions than those incorporated in the
two rules in this section and the two rules in Section 4.2.3
below.
</bodyText>
<page confidence="0.994619">
195
</page>
<table confidence="0.9979362">
Acc OP OR OF SP SR SF
OS/O 75.4 68.0 62.9 65.4 79.2 82.7 80.9
R1 77.7 75.5 58.8 66.1 78.6 88.8 83.4
R2 79.0 67.3 83.9 74.7 89.0 76.1 82.0
R1R2 81.3 72.5 79.8 75.9 87.4 82.2 84.8
</table>
<tableCaption confidence="0.967285">
Table 3: Effect of SWSD on the subjec-
tive/objective classifier
</tableCaption>
<bodyText confidence="0.99982575">
ing only R2, and one using both (R1R2). The re-
sults are in Table 3. The R1 variant shows an im-
provement of 2.3 points in accuracy (a 9.4% error
reduction). The R2 variant shows an improvement
of 3.6 points in accuracy (a 14.6% error reduc-
tion). Applying both rules (R1R2) gives an im-
provement of 5.9 percentage points in accuracy (a
24% error reduction).
In our case, a paired t-test is not appropriate
to measure statistical significance, as we are not
doing multiple runs. Thus, we apply McNemar’s
test, which is a non-parametric method for algo-
rithms that can be executed only once, meaning
training once and testing once (Dietterich, 1998).
For R1, the improvement in accuracy is statisti-
cally significant at the p &lt; .05 level. For R2 and
R1R2, the improvement in accuracy is statistically
significant at the p &lt; .01 level. Moreover, in all
cases, we see improvement in both objective and
subjective F-measure.
</bodyText>
<subsectionHeader confidence="0.682632">
4.2.3 Contextual Polarity Classifier
</subsectionHeader>
<bodyText confidence="0.999272808219179">
We now apply SWSD to contextual polarity clas-
sification (positive/negative/neutral), in the hope
that avoiding false hits of subjectivity keywords
will also lead to performance improvement in con-
textual sentiment analysis.
We use an implementation of the classifier of
(Wilson et al., 2005a). This classifier labels in-
stances of lexicon entries. The gold standard is
defined on the MPQA Corpus as follows: If an
instance is in a positive subjective expression, it
is contextually positive (Ps); if in a negative sub-
jective expression, it is contextually negative (Ng);
and if it is in an objective expression or a neu-
tral subjective expression, then it is contextually
N(eutral). As above, we evaluate the system on
the keyword instances in the SenMPQA dataset.
Wilson et al. use a two step approach. The first
step classifies keyword instances as being in a po-
lar (positive or negative) or a neutral context. The
first step is performed by the neutral/polar classi-
fier mentioned above in Section 4.2.2. The sec-
ond step decides the contextual polarity (positive
or negative) of the instances classified as polar in
the first step, and is performed by a separate clas-
sifier.
To make a sense-aware version of the system,
we use rules to change some of the answers of the
neutral/polar classifier.
Unfortunately, we cannot simply trust SWSD
when it labels a keyword as an S sense, because an
S sense might be in a N(eutral) expression (since
there are neutral subjective expressions). But, an
S sense is more likely to appear in a P(olar) ex-
pression. Thus, we consider confidence (rule R3):
If the contextual classifier labels an instance as N,
but SWSD determines it has an S sense and the
contextual classifier’s confidence is low,6 then R3
flips the contextual classifier’s label to P.
Rule R4 is analogous to R2 in the previous sec-
tion: If the contextual classifier labels an instance
as P, but (1) SWSD determines that it has an O
sense, (2) the contextual classifier’s confidence is
low, and (3) there is no other subjective keyword in
the same expression, then R2 flips the contextual
classifier’s label to N.
We compare the performance of the original
neutral/polar classifier (ON/P) and sense-aware
variants using R3 and R4. The results are in Table
4. This time, the table does not include a combined
method, because only R4 improves performance.
This is consistent with the finding in (Wilson et
al., 2005a) that most errors are caused by subjec-
tivity keywords with non-neutral prior polarity ap-
pearing in phrases with neutral contextual polarity.
R4 targets these cases. It is promising to see that
SWSD provides enough information to fix some of
them. There is a 2.6 point improvement in accu-
racy (a 12.4% error reduction). The improvement
in accuracy is statistically significant at the p &lt;
.01 level with McNemar’s test. The improvement
in accuracy is accompanied by improvements in
both neutral and polar F-measure.
We wanted to see if the improvements in the
6As in the previous section, low confidence is defined
in terms of the difference between the largest label weight
and the second largest label weight assigned by BoosTexter.
We tried three thresholds, 0.0007, 0.0008, and 0.0009, re-
sulting in only a slight difference in accuracy: 0.0007 and
0.0009 both give 81.5 accuracy compared to 81.6 accuracy
for 0.0008. We report results using 0.0008, though the ac-
curacy using the other thresholds is statistically significantly
better than the accuracy of the original classifier at the same
level.
</bodyText>
<page confidence="0.995915">
196
</page>
<table confidence="0.995402666666667">
Acc NP NR NF NgP NgR NgF PsP PsR PsF
OPs/Ng/N 77.6 80.9 94.6 87.2 60.4 29.4 39.5 52.2 32.4 40.0
R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0
</table>
<tableCaption confidence="0.98409">
Table 5: Effect of SWSD on the contextual polarity classifier
</tableCaption>
<table confidence="0.99986025">
Acc NP NR NF PP PR PF
ON/P 79.0 81.5 92.5 86.7 65.8 40.7 50.3
R3 70.0 83.7 73.8 78.4 44.4 59.3 50.8
R4 81.6 81.7 96.8 88.6 81.1 38.6 52.3
</table>
<tableCaption confidence="0.8386405">
Table 4: Effect of SWSD on the neutral/polar clas-
sifier
</tableCaption>
<bodyText confidence="0.999804785714286">
first step of Wilson et al’s system can be propa-
gated to their second step, yielding an overall im-
provement in positive /negative/neutral (Ps/Ng/N)
classification.
The sense-aware variant of the overall two-part
system is the same as the original except that we
apply R4 to the output of the first step (flipping
some of the neutral/polar classifier’s P labels to
N). Thus, since the second step in Wilson et al.’s
classifier processes only those instances labeled P
in the first step, in the sense-aware system, fewer
instances are passed from the first to the second
step.
Table 5 reports results for the original sys-
tem (OPs/Ng/N) and the sense-aware variant (R4).
These results are for the entire SenMPQA dataset,
not just those labeled P in the first step.
The accuracy improves 3 percentage points (a
13.4% error reduction). The improvement in accu-
racy is statistically significant at the p &lt; .01 level
with McNemar’s test. We see the real benefit when
we look at the precision of the positive and neg-
ative classes. Negative precision goes from 60.4
to 82.1 and positive precision goes from 52.2 to
68.6, with no loss in recall. This is evidence that
the SWSD system is doing a good job of removing
some false hits of subjectivity clues that harm the
original version of the system.
</bodyText>
<sectionHeader confidence="0.98425" genericHeader="method">
5 Comparisons to Previous Work
</sectionHeader>
<bodyText confidence="0.999975595744681">
Several researchers exploit lexical resources for
contextual subjectivity and sentiment analysis.
These systems typically look for the presence of
subjective or sentiment-bearing words in the text.
They may rely only on this information (e.g.,
(Turney, 2002; Whitelaw et al., 2005; Riloff and
Wiebe, 2003)), or they may combine it with addi-
tional information as well (e.g., (Yu and Hatzivas-
siloglou, 2003; Kim and Hovy, 2004; Bloom et al.,
2007; Wilson et al., 2005a)). We apply SWSD to
some of those systems to show the effect of SWSD
on contextual subjectivity and sentiment analysis.
Another set of related work is on subjectivity
and polarity labeling of word senses (e.g. (Esuli
and Sebastiani, 2006; Andreevskaia and Bergler,
2006; Wiebe and Mihalcea, 2006; Su and Markert,
2008)). They label senses of words in a dictionary.
In comparison, we label senses of word instances
in a corpus.
Moreover, our work extends findings in (Wiebe
and Mihalcea, 2006) and (Su and Markert, 2008).
(Wiebe and Mihalcea, 2006) demonstrates that
subjectivity is a property that can be associated
with word senses. We show that it is a natural
grouping of word senses and that it provides a
principled way for clustering senses. They also
demonstrate that subjectivity helps with WSD. We
show that a coarse-grained WSD variant (SWSD)
helps with subjectivity and sentiment analysis.
Both (Wiebe and Mihalcea, 2006) and (Su and
Markert, 2008) show that even reliable subjectiv-
ity clues have objective senses. We demonstrate
that this ambiguity is also prevalent in a corpus.
Several researchers (e.g., (Palmer et al., 2004;
Navigli, 2006; Snow et al., 2007; Hovy et al.,
2006)) work on reducing the granularity of sense
inventories for WSD. They aim for a more coarse-
grained sense inventory to overcome performance
shortcomings related to fine-grained sense distinc-
tions. Our work is similar in the sense that we
reduce all senses of a word to two senses (S/O).
The difference is the criterion driving the group-
ing. Related work concentrates on syntactic and
semantic similarity between senses to group them.
In contrast, our grouping is driven by subjectivity
with a specific application area in mind, namely
subjectivity and sentiment analysis.
</bodyText>
<sectionHeader confidence="0.998966" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999623333333333">
We introduced the task of subjectivity word sense
disambiguation (SWSD), and evaluated a super-
vised method inspired by research in WSD. The
</bodyText>
<page confidence="0.994061">
197
</page>
<bodyText confidence="0.999984793103448">
system achieves high accuracy, especially on
highly ambiguous words, and substantially outper-
forms WSD on the same data. The positive results
provide evidence that SWSD is a feasible variant
of WSD, and that the S/O sense groupings are nat-
ural ones.
We also explored the promise of SWSD for con-
textual subjectivity analysis. We showed that a
subjectivity lexicon can have substantial coverage
of the subjective expressions in the corpus, yet
still be responsible for significant sense ambiguity.
This demonstrates the potential benefit to opin-
ion analysis of performing SWSD. We then ex-
ploit SWSD in several contextual opinion analysis
systems, including positive/negative/neutral senti-
ment classification. Improvements in performance
were realized for all of the systems.
We plan several future directions which promise
to further increase the impact of SWSD on sub-
jectivity and sentiment analysis. We will manu-
ally annotate a moderate number of strategically
chosen words, namely frequent ones which are
highly ambiguous. In addition, we will add fea-
tures to the SWSD system reflecting the subjec-
tivity of the surrounding context. Finally, there
are more sophisticated strategies to explore for
improving subjectivity and sentiment analysis via
SWSD than the simple, intuitive rules we began
with in this paper.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999735571428571">
This material is based in part upon work supported
by National Science Foundation awards #0840632
and #0840608. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of the National Science
Foundation.
</bodyText>
<sectionHeader confidence="0.998411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999675338028169">
A. Andreevskaia and S. Bergler. 2006. Mining word-
net for a fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In (EACL-2006).
K. Bloom, N. Garg, and S. Argamon. 2007. Extracting
appraisal expressions. In HLT-NAACL 2007, pages
308–315, Rochester, NY.
T. G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning al-
gorithms. Neural Computation, 10:1895–1923.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In (LREC-06), Genova, IT.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, New York City.
A. Kilgarriff and M. Palmer, editors. 2000. Com-
puter and the Humanities. Special issue: SENSE-
VAL. Evaluating Word Sense Disambiguation pro-
grams, volume 34, April.
S.-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In (COLING 2004), pages 1267–
1373, Geneva, Switzerland.
S.-M. Kim and E. Hovy. 2006. Identifying and analyz-
ing judgment opinions. In (HLT/NAACL-06), pages
200–207, New York, New York.
R. Mihalcea and P. Edmonds, editors. 2004. Pro-
ceedings of SENSEVAL-3, Association for Compu-
tational Linguistics Workshop, Barcelona, Spain.
R. Mihalcea. 2002. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan, August.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia.
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004.
Different sense granularities for different applica-
tions. In HLT-NAACL 2004 Workshop: 2nd Work-
shop on Scalable Natural Language Understanding,
Boston, Massachusetts.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In (ACL-04), pages 271–
278, Barcelona, ES. Association for Computational
Linguistics.
J. Preiss and D. Yarowsky, editors. 2001. Pro-
ceedings of SENSEVAL-2, Association for Compu-
tational Linguistics Workshop, Toulouse, France.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In (EMNLP-2003),
pages 105–112, Sapporo, Japan.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135–168.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007.
Learning to merge word senses. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), Prague,
Czech Republic.
F. Su and K. Markert. 2008. From word to sense: a
case study of subjectivity recognition. In (COLING-
2008), Manchester.
</reference>
<page confidence="0.985345">
198
</page>
<reference confidence="0.989069906976744">
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 417–424, Philadelphia.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Us-
ing appraisal groups for sentiment analysis. In Pro-
ceedings of CIKM-05, the ACM SIGIR Conference
on Information and Knowledge Management, Bre-
men, DE.
J. Wiebe and R. Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, Syd-
ney, Australia.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Anno-
tating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164–
210.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005a. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In (HLT/EMNLP-2005), pages 347–354,
Vancouver, Canada.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J.
Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005b. OpinionFinder: A system for subjec-
tivity analysis. In Proc. Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP-
2005) Companion Volume (software demonstration).
T. Wilson. 2008. Fine-grained Subjectivity and Sen-
timent Analysis: Recognizing the Intensity, Polarity,
and Attitudes ofprivate states. Ph.D. thesis, Intelli-
gent Systems Program, University of Pittsburgh.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, June.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-03), pages 129–
136, Sapporo, Japan.
</reference>
<page confidence="0.998912">
199
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.853253">
<title confidence="0.999658">Subjectivity Word Sense Disambiguation</title>
<author confidence="0.998123">Akkaya Wiebe Rada Mihalcea</author>
<affiliation confidence="0.997782">University of Pittsburgh University of North Texas</affiliation>
<email confidence="0.999815">rada@cs.unt.edu</email>
<abstract confidence="0.988954692307692">paper investigates a new task, subjecword sense disambiguation which is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. We provide empirical evidence that SWSD is more feasible than full word sense disambiguation, and that it can be exploited to improve the performance of contextual subjectivity and sentiment analysis systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Andreevskaia</author>
<author>S Bergler</author>
</authors>
<title>Mining wordnet for a fuzzy sentiment: Sentiment tag extraction from wordnet glosses.</title>
<date>2006</date>
<booktitle>In (EACL-2006).</booktitle>
<contexts>
<context position="32836" citStr="Andreevskaia and Bergler, 2006" startWordPosition="5481" endWordPosition="5484">stems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subje</context>
</contexts>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>A. Andreevskaia and S. Bergler. 2006. Mining wordnet for a fuzzy sentiment: Sentiment tag extraction from wordnet glosses. In (EACL-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bloom</author>
<author>N Garg</author>
<author>S Argamon</author>
</authors>
<title>Extracting appraisal expressions.</title>
<date>2007</date>
<booktitle>In HLT-NAACL 2007,</booktitle>
<pages>308--315</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="32544" citStr="Bloom et al., 2007" startWordPosition="5434" endWordPosition="5437">dence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivit</context>
</contexts>
<marker>Bloom, Garg, Argamon, 2007</marker>
<rawString>K. Bloom, N. Garg, and S. Argamon. 2007. Extracting appraisal expressions. In HLT-NAACL 2007, pages 308–315, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<pages>10--1895</pages>
<contexts>
<context position="26547" citStr="Dietterich, 1998" startWordPosition="4425" endWordPosition="4426"> both (R1R2). The results are in Table 3. The R1 variant shows an improvement of 2.3 points in accuracy (a 9.4% error reduction). The R2 variant shows an improvement of 3.6 points in accuracy (a 14.6% error reduction). Applying both rules (R1R2) gives an improvement of 5.9 percentage points in accuracy (a 24% error reduction). In our case, a paired t-test is not appropriate to measure statistical significance, as we are not doing multiple runs. Thus, we apply McNemar’s test, which is a non-parametric method for algorithms that can be executed only once, meaning training once and testing once (Dietterich, 1998). For R1, the improvement in accuracy is statistically significant at the p &lt; .05 level. For R2 and R1R2, the improvement in accuracy is statistically significant at the p &lt; .01 level. Moreover, in all cases, we see improvement in both objective and subjective F-measure. 4.2.3 Contextual Polarity Classifier We now apply SWSD to contextual polarity classification (positive/negative/neutral), in the hope that avoiding false hits of subjectivity keywords will also lead to performance improvement in contextual sentiment analysis. We use an implementation of the classifier of (Wilson et al., 2005a)</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>T. G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10:1895–1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In (LREC-06),</booktitle>
<location>Genova, IT.</location>
<contexts>
<context position="32804" citStr="Esuli and Sebastiani, 2006" startWordPosition="5477" endWordPosition="5480">sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A. Esuli and F. Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In (LREC-06), Genova, IT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<location>New York City.</location>
<contexts>
<context position="17118" citStr="Hovy et al., 2006" startWordPosition="2808" endWordPosition="2811">e hits, while only 22% of the strongsubj instances are. The high coverage of the lexicon demonstrates its potential usefulness for opinion analysis systems, while its degree of ambiguity, in the form of false hits in a subjectivity annotated corpus, shows the potential benefit to opinion analysis of performing SWSD. As mentioned above, our experiments involve only lexicon entries that are covered by the SENSEVAL data, as we did not perform manual sense tagging for this work. We have hope to expand the system’s coverage in the future, as more wordsense tagged data is produced (e.g., ONTONOTES (Hovy et al., 2006)). We also have evidence that a moderate amount of manual annotation would be worth the effort. For example, let us order the lexicon entries from highest to lowest by frequency in the MPQA corpus. The top 20 are responsible for 25% of all false hits in the corpus; the top 40 are responsible for 34%; and the top 80 are responsible for 44%. If the SWSD system could be trained for these words, the potential impact on reducing false hits could be substantial, especially considering the good performance of the SWSD system on the more ambiguous words. Note that we do not want to simply discard thes</context>
<context position="33760" citStr="Hovy et al., 2006" startWordPosition="5631" endWordPosition="5634">ity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis. Both (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008) show that even reliable subjectivity clues have objective senses. We demonstrate that this ambiguity is also prevalent in a corpus. Several researchers (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)) work on reducing the granularity of sense inventories for WSD. They aim for a more coarsegrained sense inventory to overcome performance shortcomings related to fine-grained sense distinctions. Our work is similar in the sense that we reduce all senses of a word to two senses (S/O). The difference is the criterion driving the grouping. Related work concentrates on syntactic and semantic similarity between senses to group them. In contrast, our grouping is driven by subjectivity with a specific application area in mind, namely subjectivity and sentiment analysis. 6 Conclusions and Future Work</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>M Palmer</author>
<author>editors</author>
</authors>
<date>2000</date>
<booktitle>Computer and the Humanities. Special issue: SENSEVAL. Evaluating Word Sense Disambiguation programs,</booktitle>
<volume>34</volume>
<marker>Kilgarriff, Palmer, editors, 2000</marker>
<rawString>A. Kilgarriff and M. Palmer, editors. 2000. Computer and the Humanities. Special issue: SENSEVAL. Evaluating Word Sense Disambiguation programs, volume 34, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-M Kim</author>
<author>E Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In (COLING 2004),</booktitle>
<pages>1267--1373</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="32524" citStr="Kim and Hovy, 2004" startWordPosition="5430" endWordPosition="5433"> recall. This is evidence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstra</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>S.-M. Kim and E. Hovy. 2004. Determining the sentiment of opinions. In (COLING 2004), pages 1267– 1373, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-M Kim</author>
<author>E Hovy</author>
</authors>
<title>Identifying and analyzing judgment opinions.</title>
<date>2006</date>
<booktitle>In (HLT/NAACL-06),</booktitle>
<pages>200--207</pages>
<location>New York, New York.</location>
<contexts>
<context position="5495" citStr="Kim and Hovy, 2006" startWordPosition="849" endWordPosition="852">e subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications. The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions. 1Available at http://www.cs.pitt.edu/mpqa In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases. In addition, other properties are annotated, including polarity. For SWSD, we need the not</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>S.-M. Kim and E. Hovy. 2006. Identifying and analyzing judgment opinions. In (HLT/NAACL-06), pages 200–207, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Edmonds</author>
<author>editors</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of SENSEVAL-3, Association for Computational Linguistics Workshop,</booktitle>
<location>Barcelona, Spain.</location>
<marker>Mihalcea, Edmonds, editors, 2004</marker>
<rawString>R. Mihalcea and P. Edmonds, editors. 2004. Proceedings of SENSEVAL-3, Association for Computational Linguistics Workshop, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Instance based learning with automatic feature selection applied to Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002),</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="9433" citStr="Mihalcea, 2002" startWordPosition="1504" endWordPosition="1505"> and O, respectively. On the other hand, a sentence-level subjectivity classifier would label the sentences as S, S, and O, respectively. (4) His alarm grew. Will someone shut that darn alarm off? The alarm went off. We use a supervised approach to SWSD. We train a different classifier for each lexicon entry for which we have training data. Thus, our approach is like targeted WSD (in contrast to allwords WSD), with two labels: S and O. We borrow machine learning features which have been successfully used in WSD. Specifically, given an ambiguous target word, we use the following features from (Mihalcea, 2002): CW : the target word itself CP : POS of the target word CF : surrounding context of 3 words and their POS HNP : the head of the noun phrase to which the target word belongs NB : the first noun before the target word VB : the first verb before the target word NA : the first noun after the target word VA : the first verb after the target word SK : at most 10 context words occurring at least 5 times; determined for each sense 3.2 Lexicon and Data Our target words are members of a subjectivity lexicon, because, since they are in such a lexicon, we know they have subjective usages. Specifically, </context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>R. Mihalcea. 2002. Instance based learning with automatic feature selection applied to Word Sense Disambiguation. In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002), Taipei, Taiwan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost word sense disambiguation performance.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="33721" citStr="Navigli, 2006" startWordPosition="5625" endWordPosition="5626"> 2006) demonstrates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis. Both (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008) show that even reliable subjectivity clues have objective senses. We demonstrate that this ambiguity is also prevalent in a corpus. Several researchers (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)) work on reducing the granularity of sense inventories for WSD. They aim for a more coarsegrained sense inventory to overcome performance shortcomings related to fine-grained sense distinctions. Our work is similar in the sense that we reduce all senses of a word to two senses (S/O). The difference is the criterion driving the grouping. Related work concentrates on syntactic and semantic similarity between senses to group them. In contrast, our grouping is driven by subjectivity with a specific application area in mind, namely subjectivity and sentiment </context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>R. Navigli. 2006. Meaningful clustering of senses helps boost word sense disambiguation performance. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>O Babko-Malaya</author>
<author>H T Dang</author>
</authors>
<title>Different sense granularities for different applications.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: 2nd Workshop on Scalable Natural Language Understanding,</booktitle>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="33706" citStr="Palmer et al., 2004" startWordPosition="5621" endWordPosition="5624"> (Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis. Both (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008) show that even reliable subjectivity clues have objective senses. We demonstrate that this ambiguity is also prevalent in a corpus. Several researchers (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)) work on reducing the granularity of sense inventories for WSD. They aim for a more coarsegrained sense inventory to overcome performance shortcomings related to fine-grained sense distinctions. Our work is similar in the sense that we reduce all senses of a word to two senses (S/O). The difference is the criterion driving the grouping. Related work concentrates on syntactic and semantic similarity between senses to group them. In contrast, our grouping is driven by subjectivity with a specific application area in mind, namely subjectivity</context>
</contexts>
<marker>Palmer, Babko-Malaya, Dang, 2004</marker>
<rawString>M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004. Different sense granularities for different applications. In HLT-NAACL 2004 Workshop: 2nd Workshop on Scalable Natural Language Understanding, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In (ACL-04),</booktitle>
<pages>271--278</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona, ES.</location>
<contexts>
<context position="5452" citStr="Pang and Lee, 2004" startWordPosition="841" endWordPosition="844">) classification. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications. The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions. 1Available at http://www.cs.pitt.edu/mpqa In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases. In addition, other properties are annotated, in</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In (ACL-04), pages 271– 278, Barcelona, ES. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>Proceedings of SENSEVAL-2, Association for Computational Linguistics Workshop,</booktitle>
<editor>J. Preiss and D. Yarowsky, editors.</editor>
<location>Toulouse, France.</location>
<marker>2001</marker>
<rawString>J. Preiss and D. Yarowsky, editors. 2001. Proceedings of SENSEVAL-2, Association for Computational Linguistics Workshop, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="4204" citStr="Quirk et al., 1985" startWordPosition="646" endWordPosition="649">of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190–199, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP first attempt to explicitly use sense-level subjectivity tags in contextual subjectivity and sentiment analysis. 2 Background We adopt the definitions of subjective and objective from (Wiebe et al., 2005; Wiebe and Mihalcea, 2006; Wilson, 2008). Subjective expressions are words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs. A general covering term for such states is private state (Quirk et al., 1985), an internal state that cannot be directly observed or verified by others. (Wiebe and Mihalcea, 2006) give the following examples: (3) His alarm grew. He absorbed the information quickly. UCC/Disciples leaders roundly condemned the Iranian President’s verbal assault on Israel. What’s the catch? Polarity (also called semantic orientation) is also important to NLP applications. In review mining, for example, we want to know whether an opinion about a product is positive or negative. Nonetheless, as argued by (Wiebe and Mihalcea, 2006; Su and Markert, 2008), there are also motivations for a sepa</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions. In</title>
<date>2003</date>
<booktitle>(EMNLP-2003),</booktitle>
<pages>105--112</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="19330" citStr="Riloff and Wiebe, 2003" startWordPosition="3198" endWordPosition="3201"> is the subset containing instances of the words in the SWSD system’s coverage. Thus, for the classifiers in this section, the data used is the SenMPQA dataset, which consists of the sentences in the MPQA Corpus that contain at least one instance of the 39 keywords. There are 689 such sentences (containing, in total, 723 instances of the 39 keywords). Even though this dataset is smaller than the one used above, it gives us enough data to draw conclusions according to McNemar’s test for statistical significance. 4.2.1 Rule-based Classifier We first apply SWSD to the rule-based classifier from (Riloff and Wiebe, 2003). The classifier, which is a sentence-level S/O classifier, has low subjective and objective recall but high subjective and objective precision. It is useful for creating training data for subsequent processing by applying it to large amounts of unannotated data. The classifier is a good candidate for directly measuring the effects of SWSD on contextual subjectivity analysis, because it classifies sentences only by looking for the presence of subjectivity keywords. Performance will improve if false hits can be ignored. The classifier labels a sentence as S if it contains two or more strongsubj</context>
<context position="32405" citStr="Riloff and Wiebe, 2003" startWordPosition="5409" endWordPosition="5412">d negative classes. Negative precision goes from 60.4 to 82.1 and positive precision goes from 52.2 to 68.6, with no loss in recall. This is evidence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, o</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>E. Riloff and J. Wiebe. 2003. Learning extraction patterns for subjective expressions. In (EMNLP-2003), pages 105–112, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="24843" citStr="Schapire and Singer, 2000" startWordPosition="4126" endWordPosition="4129">e extent in the contextual classifier’s confidence. Second, if a keyword with an O sense appears in a subjective expression, then the subjectivity is not due to that keyword but rather due to something else. Thus, the presence of another lexicon entry “explains away” the presence of the O sense in the subjective expression, and we do not want SWSD to overrule the contextual classifier. Only when the contextual classifier isn’t certain and only when there isn’t another keyword does R2 flip the label to O. Our definition of low confidence is in terms of the label weights assigned by BoosTexter (Schapire and Singer, 2000), which is the underlying machine learning algorithm of the classifier. We use the difference between the largest label weight and the second largest label weight as a measure of confidence, as suggested in the BoosTexter documentation. The threshold we use is 0.0008.5 We apply the contextual classifier and the SWSD system to the data, and compare the performance of the original system (OS/O) and three sense-aware variants: one using only R1, one us5As will be noted below, we experimented with three thresholds for the classifier in Section 4.2.3, with no significant difference in accuracy. Her</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>S Prakash</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Learning to merge word senses.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="33740" citStr="Snow et al., 2007" startWordPosition="5627" endWordPosition="5630">ates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis. Both (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008) show that even reliable subjectivity clues have objective senses. We demonstrate that this ambiguity is also prevalent in a corpus. Several researchers (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)) work on reducing the granularity of sense inventories for WSD. They aim for a more coarsegrained sense inventory to overcome performance shortcomings related to fine-grained sense distinctions. Our work is similar in the sense that we reduce all senses of a word to two senses (S/O). The difference is the criterion driving the grouping. Related work concentrates on syntactic and semantic similarity between senses to group them. In contrast, our grouping is driven by subjectivity with a specific application area in mind, namely subjectivity and sentiment analysis. 6 Conclus</context>
</contexts>
<marker>Snow, Prakash, Jurafsky, Ng, 2007</marker>
<rawString>R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007. Learning to merge word senses. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Su</author>
<author>K Markert</author>
</authors>
<title>From word to sense: a case study of subjectivity recognition.</title>
<date>2008</date>
<booktitle>In (COLING2008),</booktitle>
<location>Manchester.</location>
<contexts>
<context position="4765" citStr="Su and Markert, 2008" startWordPosition="731" endWordPosition="734">g term for such states is private state (Quirk et al., 1985), an internal state that cannot be directly observed or verified by others. (Wiebe and Mihalcea, 2006) give the following examples: (3) His alarm grew. He absorbed the information quickly. UCC/Disciples leaders roundly condemned the Iranian President’s verbal assault on Israel. What’s the catch? Polarity (also called semantic orientation) is also important to NLP applications. In review mining, for example, we want to know whether an opinion about a product is positive or negative. Nonetheless, as argued by (Wiebe and Mihalcea, 2006; Su and Markert, 2008), there are also motivations for a separate subjective/objective (S/O) classification. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versu</context>
<context position="8135" citStr="Su and Markert, 2008" startWordPosition="1282" endWordPosition="1285">, warning device, alarm system – (a device that signals the occurrence of some undesirable event) _&gt; device – (an instrumentality invented for a particular purpose; “the device is small enough to wear on your wrist”; “a device intended to conserve water”) He sold his catch at the market. catch, haul – (the quantity that was caught; “the catch was only 10 fish”) _&gt; indefinite quantity – (an estimated quantity) Wiebe and Mihalcea performed an agreement study and report that good agreement (κ=0.74) can be achieved between human annotators labeling the subjectivity of senses. For a similar task, (Su and Markert, 2008) also report good agreement (κ=0.79). 191 3 Subjectivity Word Sense Disambiguation 3.1 Task Definition and Method We now turn to SWSD, and our method for performing it. Note that SWSD is midway between pure dictionary classification and pure contextual interpretation. For SWSD, the context of the word is considered in order to perform the task, but the subjectivity is determined solely by the dictionary. In contrast, full contextual interpretation can deviate from a sense’s subjectivity label in the dictionary. As noted above, words used with objective senses may appear in subjective expressio</context>
<context position="32885" citStr="Su and Markert, 2008" startWordPosition="5489" endWordPosition="5492">timent-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis. Both (Wiebe and M</context>
</contexts>
<marker>Su, Markert, 2008</marker>
<rawString>F. Su and K. Markert. 2008. From word to sense: a case study of subjectivity recognition. In (COLING2008), Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>417--424</pages>
<location>Philadelphia.</location>
<contexts>
<context position="32357" citStr="Turney, 2002" startWordPosition="5403" endWordPosition="5404">k at the precision of the positive and negative classes. Negative precision goes from 60.4 to 82.1 and positive precision goes from 52.2 to 68.6, with no loss in recall. This is evidence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label s</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 417–424, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Whitelaw</author>
<author>N Garg</author>
<author>S Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM-05, the ACM SIGIR Conference on Information and Knowledge Management,</booktitle>
<location>Bremen, DE.</location>
<contexts>
<context position="32380" citStr="Whitelaw et al., 2005" startWordPosition="5405" endWordPosition="5408">sion of the positive and negative classes. Negative precision goes from 60.4 to 82.1 and positive precision goes from 52.2 to 68.6, with no loss in recall. This is evidence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances</context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>C. Whitelaw, N. Garg, and S. Argamon. 2005. Using appraisal groups for sentiment analysis. In Proceedings of CIKM-05, the ACM SIGIR Conference on Information and Knowledge Management, Bremen, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="3954" citStr="Wiebe and Mihalcea, 2006" startWordPosition="605" endWordPosition="609"> SWSD. Then, we exploit SWSD to improve performance on several subjectivity analysis tasks, from subjective/objective sentence-level classification to positive/negative/neutral expressionlevel classification. To our knowledge, this is the 190 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190–199, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP first attempt to explicitly use sense-level subjectivity tags in contextual subjectivity and sentiment analysis. 2 Background We adopt the definitions of subjective and objective from (Wiebe et al., 2005; Wiebe and Mihalcea, 2006; Wilson, 2008). Subjective expressions are words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs. A general covering term for such states is private state (Quirk et al., 1985), an internal state that cannot be directly observed or verified by others. (Wiebe and Mihalcea, 2006) give the following examples: (3) His alarm grew. He absorbed the information quickly. UCC/Disciples leaders roundly condemned the Iranian President’s verbal assault on Israel. What’s the catch? Polarity (also called semantic orientation) is also i</context>
<context position="6217" citStr="Wiebe and Mihalcea, 2006" startWordPosition="961" endWordPosition="964">y analysis experiments in Section 4 include both S/O and polarity classifications. The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions. 1Available at http://www.cs.pitt.edu/mpqa In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases. In addition, other properties are annotated, including polarity. For SWSD, we need the notions of subjective and objective senses of words in a dictionary. We adopt the definitions from (Wiebe and Mihalcea, 2006), who describe the annotation scheme as follows. Classifying a sense as S means that, when the sense is used in a text or conversation, one expects it to express subjectivity, and also that the phrase or sentence containing it expresses subjectivity. As noted in (Wiebe and Mihalcea, 2006), sentences containing objective senses may not be objective. Thus, objective senses are defined as follows: Classifying a sense as O means that, when the sense is used in a text or conversation, one does not expect it to express subjectivity and, if the phrase or sentence containing it is subjective, the subj</context>
<context position="32862" citStr="Wiebe and Mihalcea, 2006" startWordPosition="5485" endWordPosition="5488">sence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment anal</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>J. Wiebe and R. Mihalcea. 2006. Word sense and subjectivity. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<booktitle>Language Resources and Evaluation (formerly Computers and the Humanities),</booktitle>
<pages>39--2</pages>
<contexts>
<context position="3260" citStr="Wiebe et al., 2005" startWordPosition="508" endWordPosition="511">y lexicon. It labels clue instances as having a subjective sense or an objective sense in context. The system relies on common machine learning features for word sense disambiguation (WSD). The performance is substantially above both baseline and the performance of full WSD on the same data, suggesting that the task is feasible, and that subjectivity provides a natural coarsegrained grouping of senses. The second part demonstrates the promise of SWSD for contextual subjectivity analysis. First, we show that subjectivity sense ambiguity is highly prevalent in the MPQA opinion-annotated corpus (Wiebe et al., 2005; Wilson, 2008), thus establishing the potential benefit of performing SWSD. Then, we exploit SWSD to improve performance on several subjectivity analysis tasks, from subjective/objective sentence-level classification to positive/negative/neutral expressionlevel classification. To our knowledge, this is the 190 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190–199, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP first attempt to explicitly use sense-level subjectivity tags in contextual subjectivity and sentiment analysis. 2 Background We adopt </context>
<context position="5753" citStr="Wiebe et al., 2005" startWordPosition="893" endWordPosition="896"> a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications. The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions. 1Available at http://www.cs.pitt.edu/mpqa In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases. In addition, other properties are annotated, including polarity. For SWSD, we need the notions of subjective and objective senses of words in a dictionary. We adopt the definitions from (Wiebe and Mihalcea, 2006), who describe the annotation scheme as follows. Classifying a sense as S means that, when the sense is used in a text or conversation, </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation (formerly Computers and the Humanities), 39(2/3):164– 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In (HLT/EMNLP-2005),</booktitle>
<pages>347--354</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4967" citStr="Wilson et al., 2005" startWordPosition="761" endWordPosition="764">m grew. He absorbed the information quickly. UCC/Disciples leaders roundly condemned the Iranian President’s verbal assault on Israel. What’s the catch? Polarity (also called semantic orientation) is also important to NLP applications. In review mining, for example, we want to know whether an opinion about a product is positive or negative. Nonetheless, as argued by (Wiebe and Mihalcea, 2006; Su and Markert, 2008), there are also motivations for a separate subjective/objective (S/O) classification. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. T</context>
<context position="10075" citStr="Wilson et al., 2005" startWordPosition="1628" endWordPosition="1631">itself CP : POS of the target word CF : surrounding context of 3 words and their POS HNP : the head of the noun phrase to which the target word belongs NB : the first noun before the target word VB : the first verb before the target word NA : the first noun after the target word VA : the first verb after the target word SK : at most 10 context words occurring at least 5 times; determined for each sense 3.2 Lexicon and Data Our target words are members of a subjectivity lexicon, because, since they are in such a lexicon, we know they have subjective usages. Specifically, we use the lexicon of (Wilson et al., 2005b; Wilson, 2008).2 The entries have been divided into 2Available at http://www.cs.pitt.edu/mpqa those that are strongly subjective (strongsubj) and those that are weakly subjective (weaksubj), reflecting their reliability as subjectivity clues. The sources of the entries in the lexicon are identified in (Wilson, 2008). In the second part of this paper, we evaluate systems against the MPQA corpus. Wilson also uses this corpus for her evaluations. To enable this, entries were added to the lexicon independently from the MPQA corpus (that is, none of the entries were derived using the MPQA corpus)</context>
<context position="22518" citStr="Wilson et al., 2005" startWordPosition="3723" endWordPosition="3726">n predictions are counted as incorrect. These findings suggest that SWSD performs well on disambiguating keyword instances in the MPQA corpus,4 and demonstrates a positive impact of SWSD on sentence-level subjectivity classification. 4.2.2 Subjective/Objective Classifier We now move to more fine-grained expressionlevel subjectivity classification. Since sentences often contain multiple subjective expressions, expression-level classification is more informative than sentence-level classification. The classifier in this section is an implementation of the neutral/polar supervised classifier of (Wilson et al., 2005a) (using the same features), except that the classes are S/O rather than neutral/polar. These classifiers label instances of lexicon entries. The gold standard is defined on the MPQA Corpus as follows: If an instance is in a subjective expression, it is contextually S. If the instance is in an objective expression, it is contextually O. We evaluate the system on the 723 clue instances in the SenMPQA dataset. We incorporate SWSD information into the contextual subjectivity classifier in a straightforward fashion: outputs are modified according to simple, intuitive rules. 4which we cannot evalu</context>
<context position="27145" citStr="Wilson et al., 2005" startWordPosition="4516" endWordPosition="4519">ce (Dietterich, 1998). For R1, the improvement in accuracy is statistically significant at the p &lt; .05 level. For R2 and R1R2, the improvement in accuracy is statistically significant at the p &lt; .01 level. Moreover, in all cases, we see improvement in both objective and subjective F-measure. 4.2.3 Contextual Polarity Classifier We now apply SWSD to contextual polarity classification (positive/negative/neutral), in the hope that avoiding false hits of subjectivity keywords will also lead to performance improvement in contextual sentiment analysis. We use an implementation of the classifier of (Wilson et al., 2005a). This classifier labels instances of lexicon entries. The gold standard is defined on the MPQA Corpus as follows: If an instance is in a positive subjective expression, it is contextually positive (Ps); if in a negative subjective expression, it is contextually negative (Ng); and if it is in an objective expression or a neutral subjective expression, then it is contextually N(eutral). As above, we evaluate the system on the keyword instances in the SenMPQA dataset. Wilson et al. use a two step approach. The first step classifies keyword instances as being in a polar (positive or negative) o</context>
<context position="29269" citStr="Wilson et al., 2005" startWordPosition="4877" endWordPosition="4880">is analogous to R2 in the previous section: If the contextual classifier labels an instance as P, but (1) SWSD determines that it has an O sense, (2) the contextual classifier’s confidence is low, and (3) there is no other subjective keyword in the same expression, then R2 flips the contextual classifier’s label to N. We compare the performance of the original neutral/polar classifier (ON/P) and sense-aware variants using R3 and R4. The results are in Table 4. This time, the table does not include a combined method, because only R4 improves performance. This is consistent with the finding in (Wilson et al., 2005a) that most errors are caused by subjectivity keywords with non-neutral prior polarity appearing in phrases with neutral contextual polarity. R4 targets these cases. It is promising to see that SWSD provides enough information to fix some of them. There is a 2.6 point improvement in accuracy (a 12.4% error reduction). The improvement in accuracy is statistically significant at the p &lt; .01 level with McNemar’s test. The improvement in accuracy is accompanied by improvements in both neutral and polar F-measure. We wanted to see if the improvements in the 6As in the previous section, low confide</context>
<context position="32565" citStr="Wilson et al., 2005" startWordPosition="5438" endWordPosition="5441">system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005a. Recognizing contextual polarity in phrase-level sentiment analysis. In (HLT/EMNLP-2005), pages 347–354, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>P Hoffmann</author>
<author>S Somasundaran</author>
<author>J Kessler</author>
<author>J Wiebe</author>
<author>Y Choi</author>
<author>C Cardie</author>
<author>E Riloff</author>
<author>S Patwardhan</author>
</authors>
<title>OpinionFinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proc. Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP2005) Companion Volume (software demonstration).</booktitle>
<contexts>
<context position="4967" citStr="Wilson et al., 2005" startWordPosition="761" endWordPosition="764">m grew. He absorbed the information quickly. UCC/Disciples leaders roundly condemned the Iranian President’s verbal assault on Israel. What’s the catch? Polarity (also called semantic orientation) is also important to NLP applications. In review mining, for example, we want to know whether an opinion about a product is positive or negative. Nonetheless, as argued by (Wiebe and Mihalcea, 2006; Su and Markert, 2008), there are also motivations for a separate subjective/objective (S/O) classification. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. T</context>
<context position="10075" citStr="Wilson et al., 2005" startWordPosition="1628" endWordPosition="1631">itself CP : POS of the target word CF : surrounding context of 3 words and their POS HNP : the head of the noun phrase to which the target word belongs NB : the first noun before the target word VB : the first verb before the target word NA : the first noun after the target word VA : the first verb after the target word SK : at most 10 context words occurring at least 5 times; determined for each sense 3.2 Lexicon and Data Our target words are members of a subjectivity lexicon, because, since they are in such a lexicon, we know they have subjective usages. Specifically, we use the lexicon of (Wilson et al., 2005b; Wilson, 2008).2 The entries have been divided into 2Available at http://www.cs.pitt.edu/mpqa those that are strongly subjective (strongsubj) and those that are weakly subjective (weaksubj), reflecting their reliability as subjectivity clues. The sources of the entries in the lexicon are identified in (Wilson, 2008). In the second part of this paper, we evaluate systems against the MPQA corpus. Wilson also uses this corpus for her evaluations. To enable this, entries were added to the lexicon independently from the MPQA corpus (that is, none of the entries were derived using the MPQA corpus)</context>
<context position="22518" citStr="Wilson et al., 2005" startWordPosition="3723" endWordPosition="3726">n predictions are counted as incorrect. These findings suggest that SWSD performs well on disambiguating keyword instances in the MPQA corpus,4 and demonstrates a positive impact of SWSD on sentence-level subjectivity classification. 4.2.2 Subjective/Objective Classifier We now move to more fine-grained expressionlevel subjectivity classification. Since sentences often contain multiple subjective expressions, expression-level classification is more informative than sentence-level classification. The classifier in this section is an implementation of the neutral/polar supervised classifier of (Wilson et al., 2005a) (using the same features), except that the classes are S/O rather than neutral/polar. These classifiers label instances of lexicon entries. The gold standard is defined on the MPQA Corpus as follows: If an instance is in a subjective expression, it is contextually S. If the instance is in an objective expression, it is contextually O. We evaluate the system on the 723 clue instances in the SenMPQA dataset. We incorporate SWSD information into the contextual subjectivity classifier in a straightforward fashion: outputs are modified according to simple, intuitive rules. 4which we cannot evalu</context>
<context position="27145" citStr="Wilson et al., 2005" startWordPosition="4516" endWordPosition="4519">ce (Dietterich, 1998). For R1, the improvement in accuracy is statistically significant at the p &lt; .05 level. For R2 and R1R2, the improvement in accuracy is statistically significant at the p &lt; .01 level. Moreover, in all cases, we see improvement in both objective and subjective F-measure. 4.2.3 Contextual Polarity Classifier We now apply SWSD to contextual polarity classification (positive/negative/neutral), in the hope that avoiding false hits of subjectivity keywords will also lead to performance improvement in contextual sentiment analysis. We use an implementation of the classifier of (Wilson et al., 2005a). This classifier labels instances of lexicon entries. The gold standard is defined on the MPQA Corpus as follows: If an instance is in a positive subjective expression, it is contextually positive (Ps); if in a negative subjective expression, it is contextually negative (Ng); and if it is in an objective expression or a neutral subjective expression, then it is contextually N(eutral). As above, we evaluate the system on the keyword instances in the SenMPQA dataset. Wilson et al. use a two step approach. The first step classifies keyword instances as being in a polar (positive or negative) o</context>
<context position="29269" citStr="Wilson et al., 2005" startWordPosition="4877" endWordPosition="4880">is analogous to R2 in the previous section: If the contextual classifier labels an instance as P, but (1) SWSD determines that it has an O sense, (2) the contextual classifier’s confidence is low, and (3) there is no other subjective keyword in the same expression, then R2 flips the contextual classifier’s label to N. We compare the performance of the original neutral/polar classifier (ON/P) and sense-aware variants using R3 and R4. The results are in Table 4. This time, the table does not include a combined method, because only R4 improves performance. This is consistent with the finding in (Wilson et al., 2005a) that most errors are caused by subjectivity keywords with non-neutral prior polarity appearing in phrases with neutral contextual polarity. R4 targets these cases. It is promising to see that SWSD provides enough information to fix some of them. There is a 2.6 point improvement in accuracy (a 12.4% error reduction). The improvement in accuracy is statistically significant at the p &lt; .01 level with McNemar’s test. The improvement in accuracy is accompanied by improvements in both neutral and polar F-measure. We wanted to see if the improvements in the 6As in the previous section, low confide</context>
<context position="32565" citStr="Wilson et al., 2005" startWordPosition="5438" endWordPosition="5441">system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that </context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005b. OpinionFinder: A system for subjectivity analysis. In Proc. Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP2005) Companion Volume (software demonstration).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
</authors>
<title>Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes ofprivate states.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Intelligent Systems Program, University of Pittsburgh.</institution>
<contexts>
<context position="3275" citStr="Wilson, 2008" startWordPosition="512" endWordPosition="513"> clue instances as having a subjective sense or an objective sense in context. The system relies on common machine learning features for word sense disambiguation (WSD). The performance is substantially above both baseline and the performance of full WSD on the same data, suggesting that the task is feasible, and that subjectivity provides a natural coarsegrained grouping of senses. The second part demonstrates the promise of SWSD for contextual subjectivity analysis. First, we show that subjectivity sense ambiguity is highly prevalent in the MPQA opinion-annotated corpus (Wiebe et al., 2005; Wilson, 2008), thus establishing the potential benefit of performing SWSD. Then, we exploit SWSD to improve performance on several subjectivity analysis tasks, from subjective/objective sentence-level classification to positive/negative/neutral expressionlevel classification. To our knowledge, this is the 190 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190–199, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP first attempt to explicitly use sense-level subjectivity tags in contextual subjectivity and sentiment analysis. 2 Background We adopt the definitions</context>
<context position="5768" citStr="Wilson, 2008" startWordPosition="897" endWordPosition="898">heir motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications. The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions. 1Available at http://www.cs.pitt.edu/mpqa In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases. In addition, other properties are annotated, including polarity. For SWSD, we need the notions of subjective and objective senses of words in a dictionary. We adopt the definitions from (Wiebe and Mihalcea, 2006), who describe the annotation scheme as follows. Classifying a sense as S means that, when the sense is used in a text or conversation, one expects it </context>
<context position="10091" citStr="Wilson, 2008" startWordPosition="1632" endWordPosition="1634"> target word CF : surrounding context of 3 words and their POS HNP : the head of the noun phrase to which the target word belongs NB : the first noun before the target word VB : the first verb before the target word NA : the first noun after the target word VA : the first verb after the target word SK : at most 10 context words occurring at least 5 times; determined for each sense 3.2 Lexicon and Data Our target words are members of a subjectivity lexicon, because, since they are in such a lexicon, we know they have subjective usages. Specifically, we use the lexicon of (Wilson et al., 2005b; Wilson, 2008).2 The entries have been divided into 2Available at http://www.cs.pitt.edu/mpqa those that are strongly subjective (strongsubj) and those that are weakly subjective (weaksubj), reflecting their reliability as subjectivity clues. The sources of the entries in the lexicon are identified in (Wilson, 2008). In the second part of this paper, we evaluate systems against the MPQA corpus. Wilson also uses this corpus for her evaluations. To enable this, entries were added to the lexicon independently from the MPQA corpus (that is, none of the entries were derived using the MPQA corpus). The training a</context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>T. Wilson. 2008. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes ofprivate states. Ph.D. thesis, Intelligent Systems Program, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques, Second Edition.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<marker>Witten, Frank, 2005</marker>
<rawString>I. Witten and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, Second Edition. Morgan Kaufmann, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP-03),</booktitle>
<pages>129--136</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5432" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="837" endWordPosition="840">arate subjective/objective (S/O) classification. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications. The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions. 1Available at http://www.cs.pitt.edu/mpqa In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases. In addition, other properti</context>
<context position="32504" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="5425" endWordPosition="5429">m 52.2 to 68.6, with no loss in recall. This is evidence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihal</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Conference on Empirical Methods in Natural Language Processing (EMNLP-03), pages 129– 136, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>