<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99114">
Mildly Non-Projective Dependency Grammar
</title>
<author confidence="0.998775">
Marco Kuhlmann*
</author>
<affiliation confidence="0.986202">
Uppsala University
</affiliation>
<bodyText confidence="0.949045214285714">
Syntactic representations based on word-to-word dependencies have a long-standing tradition
in descriptive linguistics, and receive considerable interest in many applications. Nevertheless,
dependency syntax has remained something of an island from a formal point of view. Moreover,
most formalisms available for dependency grammar are restricted to projective analyses, and
thus not able to support natural accounts of phenomena such as wh-movement and cross–serial
dependencies. In this article we present a formalism for non-projective dependency grammar
in the framework of linear context-free rewriting systems. A characteristic property of our
formalism is a close correspondence between the non-projectivity of the dependency trees
admitted by a grammar on the one hand, and the parsing complexity of the grammar on the
other. We show that parsing with unrestricted grammars is intractable. We therefore study two
constraints on non-projectivity, block-degree and well-nestedness. Jointly, these two constraints
define a class of “mildly” non-projective dependency grammars that can be parsed in polynomial
time. An evaluation on five dependency treebanks shows that these grammars have a good
coverage of empirical data.
</bodyText>
<sectionHeader confidence="0.99583" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999497571428572">
Syntactic representations based on word-to-word dependencies have a long-standing
tradition in descriptive linguistics. Since the seminal work of Tesni`ere (1959), they
have become the basis for several linguistic theories, such as Functional Generative
Description (Sgall, Hajiˇcov´a, and Panevov´a 1986), Meaning–Text Theory (Mel’ˇcuk 1988),
and Word Grammar (Hudson 2007). In recent years they have also been used for a wide
range of practical applications, such as information extraction, machine translation, and
question answering. We ascribe the widespread interest in dependency structures to
their intuitive appeal, their conceptual simplicity, and in particular to the availability of
accurate and efficient dependency parsers for a wide range of languages (Buchholz and
Marsi 2006; Nivre et al. 2007).
Although there exist both a considerable practical interest and an extensive lin-
guistic literature, dependency syntax has remained something of an island from a
formal point of view. In particular, there are relatively few results that bridge between
dependency syntax and other traditions, such as phrase structure or categorial syntax.
</bodyText>
<note confidence="0.465078">
∗ Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.
</note>
<email confidence="0.948532">
E-mail: marco.kuhlmann@lingfil.uu.se.
</email>
<note confidence="0.838514">
Submission received: 17 December 2009; revised submission received: 3 April 2012; accepted for publication:
24 May 2012.
doi:10.1162/COLI a 00125
© 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 2
</note>
<figureCaption confidence="0.984545">
Figure 1
</figureCaption>
<bodyText confidence="0.967227344827586">
Nested dependencies and cross–serial dependencies.
This makes it hard to gauge the similarities and differences between the paradigms,
and hampers the exchange of linguistic resources and computational methods. An
overarching goal of this article is to bring dependency grammar closer to the mainland
of formal study.
One of the few bridging results for dependency grammar is thanks to Gaifman
(1965), who studied a formalism that we will refer to as Hays–Gaifman grammar, and
proved it to be weakly equivalent to context-free phrase structure grammar. Although
this result is of fundamental importance from a theoretical point of view, its practical
usefulness is limited. In particular, Hays–Gaifman grammar is restricted to projective
dependency structures, which is similar to the familiar restriction to contiguous con-
stituents. Yet, non-projective dependencies naturally arise in the analysis of natural
language. One classic example of this is the phenomenon of cross–serial dependencies
in Dutch. In this language, the nominal arguments of verbs that also select an infinitival
complement occur in the same order as the verbs themselves:
(i) dat Jan1 Piet2 Marie3 zag1 helpen2 lezen3 (Dutch)
that Jan Piet Marie saw help read
‘that Jan saw Piet help Marie read’
In German, the order of the nominal arguments instead inverts the verb order:
(ii) dass Jan1 Piet2 Marie3 lesen3 helfen2 sah1 (German)
that Jan Piet Marie read help saw
Figure 1 shows dependency trees for the two examples.1 The German linearization
gives rise to a projective structure, where the verb–argument dependencies are nested
within each other, whereas the Dutch linearization induces a non-projective structure
with crossing edges. To account for such structures we need to turn to formalisms more
expressive than Hays–Gaifman grammars.
In this article we present a formalism for non-projective dependency grammar
based on linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi
1987; Weir 1988). This framework was introduced to facilitate the comparison of various
</bodyText>
<footnote confidence="0.953290666666667">
1 We draw the nodes of a dependency tree as circles, and the edges as arrows pointing towards the
dependent (away from the root node). Following Hays (1964), we use dotted lines to help us keep
track of the positions of the nodes in the linear order, and to associate nodes with lexical items.
</footnote>
<page confidence="0.988977">
356
</page>
<note confidence="0.591964">
Kuhlmann Mildly Non-Projective Dependency Grammar
</note>
<bodyText confidence="0.99985502">
grammar formalisms, including standard context-free grammar, tree-adjoining gram-
mar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman and
Baldridge 2011). It also comprises, among others, multiple context-free grammars (Seki
et al. 1991), minimalist grammars (Michaelis 1998), and simple range concatenation
grammars (Boullier 2004).
The article is structured as follows. In Section 2 we provide the technical back-
ground to our work; in particular, we introduce our terminology and notation for linear
context-free rewriting systems. An LCFRS generates a set of terms (formal expressions)
which are interpreted as derivation trees of objects from some domain. Each term also
has a secondary interpretation under which it denotes a tuple of strings, representing
the string yield of the derived object. In Section 3 we introduce the central notion of a
lexicalized linear context-free rewriting system, which is an LCFRS in which each rule
of the grammar is associated with an overt lexical item, representing a syntactic head
(cf. Schabes, Abeill´e, and Joshi 1988 and Schabes 1990). We show that this property gives
rise to an additional interpretation under which each term denotes a dependency tree
on its yield. With this interpretation, lexicalized LCFRSs can be used as dependency
grammars.
In Section 4 we show how to acquire lexicalized LCFRSs from dependency tree-
banks. This works in much the same way as the extraction of context-free grammars
from phrase structure treebanks (cf. Charniak 1996), except that the derivation trees of
dependency trees are not immediately accessible in the treebank. We therefore present
an efficient algorithm for computing a canonical derivation tree for an input depen-
dency tree; from this derivation tree, the rules of the grammar can be extracted in a
straightforward way. The algorithm was originally published by Kuhlmann and Satta
(2009). It produces a restricted type of lexicalized LCFRS that we call “canonical.” In
Section 5 we provide a declarative characterization of this class of grammars, and show
that every lexicalized LCFRS is (strongly) equivalent to a canonical one, in the sense that
it induces the same set of dependency trees.
In Section 6 we present a simple parsing algorithm for LCFRSs. Although the
runtime of this algorithm is polynomial in the length of the sentence, the degree of
the polynomial depends on two grammar-specific measures called fan-out and rank.
We show that even in the restricted case of canonical grammars, parsing is an NP-
hard problem. It is important therefore to keep the fan-out and the rank of a grammar
as low as possible, and much of the recent work on LCFRSs has been devoted to
the development of techniques that optimize parsing complexity in various scenarios
G´omez-Rodr´ıguez and Satta 2009; G´omez-Rodr´ıguez et al. 2009; Kuhlmann and Satta
2009; Gildea 2010; G´omez-Rodr´ıguez, Kuhlmann, and Satta 2010; Sagot and Satta 2010;
and Crescenzi et al. 2011).
In this article we explore the impact of non-projectivity on parsing complexity. In
Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS,
a measure called block-degree (or gap-degree) (Holan et al. 1998). Although there is
no theoretical upper bound on the block-degree of the dependency trees needed for
linguistic analysis, we provide evidence from several dependency treebanks showing
that, from a practical point of view, this upper bound can be put at a value of as low as 2.
In Section 8 we study a second constraint on non-projectivity called well-nestedness
(Bodirsky, Kuhlmann, and M¨ohl 2005), and show that its presence facilitates tractable
parsing. This comes at the cost of a small loss in coverage on treebank data. Bounded
block-degree and well-nestedness jointly define a class of “mildly” non-projective
dependency grammars that can be parsed in polynomial time.
Section 9 summarizes our main contributions and concludes the article.
</bodyText>
<page confidence="0.990665">
357
</page>
<note confidence="0.80024">
Computational Linguistics Volume 39, Number 2
</note>
<sectionHeader confidence="0.954505" genericHeader="method">
2. Technical Background
</sectionHeader>
<bodyText confidence="0.999950166666667">
We assume basic familiarity with linear context-free rewriting systems (see, e.g., Vijay-
Shanker, Weir, and Joshi 1987 and Weir 1988) and only review the terminology and
notation that we use in this article.
A linear context-free rewriting system (LCFRS) is a structure G = (N, E, P, S)
where N is a set of nonterminals, E is a set of function symbols, P is a finite set of
production rules, and S E N is a distinguished start symbol. Rules take the form
</bodyText>
<equation confidence="0.995591">
A0 -+ f(A1,...,Am) (1)
</equation>
<bodyText confidence="0.999819285714286">
where f is a function symbol and the Ai are nonterminals. Rules are used for rewriting
in the same way as in a context-free grammar, with the function symbols acting as
terminals. The outcome of the rewriting process is a set T(G) of terms, tree-formed
expressions built from function symbols. Each term is then associated with a string
yield, more specifically a tuple of strings. For this, every function symbol f comes with
a yield function that specifies how to compute the yield of a term f (t1, ... , tm) from the
yields of its subterms ti. Yield functions are defined by equations
</bodyText>
<equation confidence="0.998587">
f((x1,1,...,x1,k1),...,(xm,1,...,xm,km)) = (α1,...,αk0) (2)
</equation>
<bodyText confidence="0.999842833333333">
where the tuple on the right-hand side consists of strings over the variables on the
left-hand side and some given alphabet of yield symbols, and contains exactly one
occurrence of each variable. For a yield function f defined by an equation of this form,
we say that f is of type k1 · · · km -+ k0, denoted by f : k1 · · · km -+ k0. To guarantee that
the string yield of a term is well-defined, each nonterminal A is associated with a
fan-out ϕ(A) &gt; 1, and it is required that for every rule (1),
</bodyText>
<equation confidence="0.693444">
f : ϕ(A1)··· ϕ(Am) -+ ϕ(A0)
</equation>
<bodyText confidence="0.909276375">
In Equation (2), the values m and k0 are called the rank and the fan-out off, respectively.
The rank and the fan-out of an LCFRS are the maximal rank and fan-out of its yield
functions.
Example 1
Figure 2 shows an example of an LCFRS for the language { (anbncndn)  |n &gt; 0 }.
Equation (2) is uniquely determined by the tuple on the right-hand side of the
equation. We call this tuple the template of the yield function f, and use it as the
canonical function symbol for f. This gives rise to a compact notation for LCFRSs,
</bodyText>
<figureCaption confidence="0.686702">
Figure 2
</figureCaption>
<bodyText confidence="0.295048">
An LCFRS that generates the yield language { (anbncndn)  |n &gt; 0 }.
</bodyText>
<page confidence="0.990942">
358
</page>
<note confidence="0.604996">
Kuhlmann Mildly Non-Projective Dependency Grammar
</note>
<bodyText confidence="0.947478333333333">
illustrated in the right column of Figure 2. In this notation, to save some subscripts,
we use the following shorthands for variables: x and x1 for x1,1; x2 for x1,2; x3 for x1,3;
y and y1 for x2,1; y2 for x2,2; y3 for x2,3.
</bodyText>
<figure confidence="0.893283625">
3. Lexicalized LCFRSs as Dependency Grammars
Recall the following examples for verb–argument dependencies in German and Dutch
from Section 1:
(iii) dass Jan1 Piet2 Marie3 lesen3 helfen2 sah1 (German)
that Jan Piet Marie read help saw
(iv) dat Jan1 Piet2 Marie3 zag1 helpen2 lezen3 (Dutch)
that Jan Piet Marie saw help read
‘that Jan saw Piet help Marie read’
</figure>
<bodyText confidence="0.7865292">
Figure 3 shows the production rules of two linear context-free rewriting systems (one for
German, one for Dutch) that generate these examples. The grammars are lexicalized in
the sense that each of their yield functions is associated with a lexical item, such as sah or
zag (cf. Schabes, Abeill´e, and Joshi 1988 and Schabes 1990). Productions with lexicalized
yield functions can be read as dependency rules. For example, the rules
</bodyText>
<equation confidence="0.936986">
V -+ (x y sah)(N, V) (German) V -+ (x y1 zag y2)(N, V) (Dutch)
</equation>
<bodyText confidence="0.99614625">
can be read as stating that the verb to see requires two dependents, one noun (N) and
one verb (V). Based on this reading, every term generated by a lexicalized LCFRS
does not only yield a tuple of strings, but also induces a dependency tree on these
strings: Each parent–child relation in the term represents a dependency between the
associated lexical items (cf. Rambow and Joshi 1997). Thus every lexicalized LCFRS can
be reinterpreted as a dependency grammar. To illustrate the idea, Figure 4 shows (the
tree representations of) two terms generated by the grammars G1 and G2, together with
the dependency trees induced by them. Note that these are the same trees that we gave
for (iii) and (iv) in Figure 1.
Our goal for the remainder of this section is to make the notion of induction formally
precise. To this end we will reinterpret the yield functions of lexicalized LCFRSs as
operations on dependency trees.
</bodyText>
<footnote confidence="0.4786985">
Figure 3
Lexicalized linear context-free rewriting systems.
</footnote>
<page confidence="0.977646">
359
</page>
<figure confidence="0.871147">
Computational Linguistics Volume 39, Number 2
</figure>
<figureCaption confidence="0.970112">
Figure 4
</figureCaption>
<bodyText confidence="0.787152">
Lexicalized linear context-free rewriting systems induce dependency trees.
</bodyText>
<subsectionHeader confidence="0.997647">
3.1 Dependency Trees
</subsectionHeader>
<bodyText confidence="0.999532285714286">
By a dependency tree, we mean a pair (w, D), where w� is a tuple of strings, and D is
a tree-shaped graph whose nodes correspond to the occurrences of symbols in w, and
whose edges represent dependency relations between these occurrences. We identify
occurrences in w� by pairs (i,j) of integers, where i indexes the component of w� that
contains the occurrence, and j specifies the linear position of the occurrence within
that component. We can then formally define a dependency graph for a tuple of
strings
</bodyText>
<equation confidence="0.976964">
w� = (a1,1 ··· a1,n1, ... , ak,1 ··· ak,nk)
as a directed graph G = (V, E) where
V = I (i,j)  |1 &lt; i &lt; k, 1 &lt; j &lt; ni } and E C V x V
</equation>
<bodyText confidence="0.976955">
We use u and v as variables for nodes, and denote edges (u, v) as u -+ v. A dependency
tree D for w� is a dependency graph for w� in which there exists a root node r such
that for any node u, there is exactly one directed path from r to u. A dependency
tree is called simple if w� consists of a single string w. In this case, we write the de-
pendency tree as (w, D), and identify occurrences by their linear positions j in w, with
</bodyText>
<equation confidence="0.768457">
1 &lt; j &lt; |w|.
Example 2
</equation>
<bodyText confidence="0.9748965">
Figure 5 shows examples of dependency trees. In pictures of such structures we use
dashed boxes to group nodes that correspond to occurrences from the same tuple
</bodyText>
<page confidence="0.988029">
360
</page>
<figure confidence="0.78718">
Kuhlmann Mildly Non-Projective Dependency Grammar
</figure>
<figureCaption confidence="0.6454215">
Figure 5
Dependency trees.
</figureCaption>
<bodyText confidence="0.909503">
component; however, we usually omit the box when there is only one component.
Writing Di as Di = (Vi,Ei) we have:
</bodyText>
<equation confidence="0.99997075">
V1 = {(1,1)} E1 = {}
V2 = {(1,1), (1,2)} E2 = {(1,1) (1,2)}
V3 = {(1,1), (2,1)} E3 = {(1,1) (2, 1)}
V4 = {(1,1), (3,1)} E4 = {(1,1) (3, 1)}
</equation>
<bodyText confidence="0.968865142857143">
We use standard terminology from graph theory for dependency trees and the
relations between their nodes. In particular, for a node u, the set of descendants of u,
which we denote by Lu], is the set of nodes that can be reached from u by following a
directed path consisting of zero or more edges. We write u &lt; v to express that the node u
precedes the node v when reading the yield from left to right. Formally, precedence is
the lexicographical order on occurrences:
(i1, j1) &lt; (i2, j2) if and only if either i1 &lt; i2 or (i1 = i2 and j1 &lt; j2)
</bodyText>
<subsectionHeader confidence="0.999447">
3.2 Operations on Dependency Trees
</subsectionHeader>
<bodyText confidence="0.9549715625">
A yield function f is called lexicalized if its template contains exactly one yield symbol,
representing a lexical item; this symbol is then called the anchor of f. With every
lexicalized yield function f we associate an operation f&apos; on dependency trees as follows.
Let w1, ... , wm, w� be tuples of strings such that
f(A, ... , wm) = w�
and let Di be a dependency tree for wi. By the definition of yield functions, every
occurrence u in an input tuple wi corresponds to exactly one occurrence in the output
tuple w; we denote this occurrence by ¯u. Let G be the dependency graph for w� that
has an edge u¯ v¯ whenever there is an edge u v in some Di, and no other edges.
Because f is lexicalized, there is exactly one occurrence r in the output tuple w� that does
not correspond to any occurrence in some wi; this is the occurrence of the anchor of f.
Let D be the dependency tree for w� that is obtained by adding to the graph G all edges
of the form r ¯ri, where ri is the root node of Di. By this construction, the occurrence r
of the anchor becomes the root node of D, and the root nodes of the input dependency
trees Di become its dependents. We then define
f&apos;((�w1, D1), . . . , (�wm, Dm))= (w,D)
</bodyText>
<page confidence="0.993371">
361
</page>
<figure confidence="0.8490605">
Computational Linguistics Volume 39, Number 2
Figure 6
Operations on dependency trees.
Example 3
</figure>
<figureCaption confidence="0.7928285">
We consider a concrete application of an operation on dependency trees, illustrated in
Figure 6. In this example we have
</figureCaption>
<equation confidence="0.612544">
f = (x1 b,yx2) w1 = (a, e) 792 = (c d) w� = f(�w1, �w2) = (a b, cd e)
</equation>
<bodyText confidence="0.702537333333333">
and the dependency trees D1, D2 are defined as
D1 = ({(1,1), (2, 1)}, {(1,1) -+ (2, 1)}) D2 = ({(1,1), (1, 2)}, {(1,1) -+ (1, 2)})
We show that f&apos;((w1, D1), (792, D2)) = (w, D), where D = (V, E) with
</bodyText>
<equation confidence="0.9382242">
V = {(1,1), (1,2), (2,1), (2,2), (2,3)}
E = {(1,1) -+ (2,3), (1,2) -+ (1,1), (1,2) -+ (2,1), (2,1) -+ (2,2)}
The correspondences between the occurrences u in the input tuples and the occur-
rences u¯ in the output tuple are as follows:
for w1: (1, 1) = (1, 1), (2, 1) = (2,3) for 792: (1, 1) = (2,1), (1,2) = (2,2)
</equation>
<bodyText confidence="0.736079">
By copying the edges from the input dependency trees, we obtain the intermediate
dependency graph G = (V, E&apos;) for w, where
</bodyText>
<equation confidence="0.997649">
E&apos; = {(1,1) -+ (2,3), (2,1) -+ (2,2)}
</equation>
<bodyText confidence="0.999044333333333">
The occurrence r of the anchor b off in w� is (1, 2); the nodes of G that correspond to
the root nodes of D1 and D2 are ¯r1 = (1, 1) and ¯r2 = (2, 1). The dependency tree D is
obtained by adding the edges r -+ ¯r1 and r -+ ¯r2 to G.
</bodyText>
<sectionHeader confidence="0.868499" genericHeader="method">
4. Extraction of Dependency Grammars
</sectionHeader>
<bodyText confidence="0.999635375">
We now show how to extract lexicalized linear context-free rewriting systems from
dependency treebanks. To this end, we adapt the standard technique for extracting
context-free grammars from phrase structure treebanks (Charniak 1996).
Our technique was originally published by Kuhlmann and Satta (2009). In recent
work, Maier and Lichte (2011) have shown how to unify it with a similar technique
for the extraction of range concatenation grammars from discontinuous constituent
structures, due to Maier and Søgaard (2008). To simplify our presentation we restrict
our attention to treebanks containing simple dependency trees.
</bodyText>
<page confidence="0.992273">
362
</page>
<figure confidence="0.798709">
Kuhlmann Mildly Non-Projective Dependency Grammar
</figure>
<figureCaption confidence="0.998294">
Figure 7
</figureCaption>
<bodyText confidence="0.997064444444444">
A dependency tree and one of its construction trees.
To extract a lexicalized LCFRS from a dependency treebank we proceed as follows.
First, for each dependency tree (w, D) in the treebank, we compute a construction tree,
a term t over yield functions that induces (w, D). Then we collect a set of production
rules, one rule for each node of the construction trees. As an example, consider Fig-
ure 7, which shows a dependency tree with one of its construction trees. (The analysis
is taken from K¨ubler, McDonald, and Nivre [2009].) From this construction tree we
extract the following rules. The nonterminals (in bold) represent linear positions of
nodes.
</bodyText>
<equation confidence="0.998629">
1 (A) 5 (on x)(7)
2 (x hearing, y)(1, 5) 6 (the)
3 (x1 is y1 x2 y2)(2,4) 7 (x issue)(6)
4 (scheduled, x)(8) 8 (today)
</equation>
<bodyText confidence="0.9996502">
Rules like these can serve as the starting point for practical systems for data-driven,
non-projective dependency parsing (Maier and Kallmeyer 2010).
Because the extraction of rules from construction trees is straightforward, the prob-
lem that we focus on in this section is how to obtain these trees in the first place. Our
procedure for computing construction trees is based on the concept of “blocks.”
</bodyText>
<subsectionHeader confidence="0.995004">
4.1 Blocks
</subsectionHeader>
<bodyText confidence="0.9242493">
Let D be a dependency tree. A segment of D is a contiguous, non-empty sequence
of nodes of D, all of which belong to the same component of the string yield. Thus
a segment contains its endpoints, as well as all nodes between the endpoints in the
precedence order. For a node u of D, a block of u is a longest segment consisting of
descendants of u. This means that the left endpoint of a block of u either is the first node
in its component, or is preceded by a node that is not a descendant of u. A symmetric
property holds for the right endpoint.
Example 4
Consider the node 2 of the dependency tree in Figure 7. The descendants of 2 fall into
two blocks, marked by the dashed boxes: 12 and 5 6 7.
</bodyText>
<page confidence="0.996638">
363
</page>
<note confidence="0.296484">
Computational Linguistics Volume 39, Number 2
</note>
<bodyText confidence="0.999197666666667">
We use u~ and v~ as variables for blocks. Extending the precedence order on nodes,
we say that a block ut precedes a block v, denoted by ut &lt; v, if the right endpoint of ut
precedes the left endpoint of v.
</bodyText>
<subsectionHeader confidence="0.999047">
4.2 Computing Canonical Construction Trees
</subsectionHeader>
<bodyText confidence="0.9998774">
To obtain a canonical construction tree t for a dependency tree (w, D) we label each
node u of D with a yield function f as follows. Let w~ be the tuple consisting of the blocks
of u, in the order of their precedence, and let 791, ... , 79m be the corresponding tuples for
the children of u. We may view blocks as strings of nodes. Taking this view, we compute
the (unique) yield function g with the property that
</bodyText>
<equation confidence="0.609731">
g(�w1,..., �wm) = w~
</equation>
<bodyText confidence="0.8773635">
The anchor of g is the node u, the rank of g corresponds to the number of children
of u, the variables in the template of g represent the blocks of these children, and the
components of the template represent the blocks of u. To obtain f, we take the template
of g and replace the occurrence of u with the corresponding lexical item.
Example 5
Node 2 of the dependency tree shown in Figure 7 has two children,1 and 5. We have
</bodyText>
<equation confidence="0.97985">
w� = (12,567) 791 = (1) 792 = (567) g = (x 2, y) f = (x hearing, y)
</equation>
<bodyText confidence="0.99933475">
Note that in order to properly define f we need to assume some order on the
children of u. The function g (and hence the construction tree t) is unique up to the
specific choice of this order. In the following we assume that children are ordered from
left to right based on the position of their leftmost descendants.
</bodyText>
<subsectionHeader confidence="0.999857">
4.3 Computing the Blocks of a Dependency Tree
</subsectionHeader>
<bodyText confidence="0.974078529411765">
The algorithmically most interesting part of our extraction procedure is the computation
of the yield function g. The template of g is uniquely determined by the left-to-right
sequence of the endpoints of the blocks of u and its children. An efficient algorithm that
can be used to compute these sequences is given in Table 1.
4.3.1 Description. We start at a virtual root node 1 (line 1) which serves as the parent
of the real root node. For each node next in the precedence order of D, we follow the
shortest path from the current node current to next. To determine this path, we compute
the lowest common ancestor lca of the two nodes (lines 4–5), using a set of markings
on the nodes. At the beginning of each iteration of the for loop in line 2, all ancestors of
current (including the virtual root node 1) are marked; therefore, we find lca by going
upwards from next to the first node that is marked. To restore the loop invariant, we
then unmark all nodes on the path from current to lca (lines 6–9). Each time we move
down from a node to one of its children (line 12), we record the information that next
is the left endpoint of a block of current. Symmetrically, each time we move up from a
node to its parent (lines 8 and 17), we record the information that next − 1 is the right
endpoint of a block of current. The while loop in lines 15–18 takes us from the last node
of the dependency tree back to the node 1.
</bodyText>
<page confidence="0.998096">
364
</page>
<table confidence="0.576619">
Kuhlmann Mildly Non-Projective Dependency Grammar
</table>
<tableCaption confidence="0.993973">
Table 1
</tableCaption>
<bodyText confidence="0.8983525">
Computing the blocks of a simple dependency tree.
Input: a string w and a simple dependency tree D for w
</bodyText>
<listItem confidence="0.99171695">
1: current +- L; mark current
2: for each node next of D from 1 to JwJ do
3: lca +- next; stack +- []
4: while lca is not marked do loop 1
5: push lca to stack; lca +- the parent of lca
6: while current =� lca do loop 2
7: D next − 1 is the right endpoint of a block of current
8: D move up from current to the parent of current
9: unmark current; current +- the parent of current
10: while stack is not empty do loop 3
11: current +- pop stack; mark current
12: D move down from the parent of current to current
13: D next is the left endpoint of a block of current
14: D arrive at next; at this point, current = next
15: while current =� L do loop 4
16: D JwJ is the right endpoint of a block of current
17: D move up from current to the parent of current
18: unmark current; current +- the parent of current
4.3.2 Runtime Analysis. We analyze the runtime of our algorithm. Let m be the total
number of blocks of D. Let us write ni for the total number of iterations of the ith while
</listItem>
<bodyText confidence="0.982577375">
loop, and let n = n1 + n2 + n3 + n4. Under the reasonable assumption that every line in
Table 1 can be executed in constant time, the runtime of the algorithm clearly is in O(n).
Because each iteration of loop 2 and loop 4 determines the right endpoint of a block, we
have n2 + n4 = m. Similarly, as each iteration of loop 3 fixes the left endpoint of a block,
we have n3 = m. To determine n1, we note that every node that is pushed to the auxiliary
stack in loop 1 is popped again in loop 3; therefore, n1 = n3 = m. Putting everything
together, we have n = 3m, and we conclude that the runtime of the algorithm is in O(m).
Note that this runtime is asymptotically optimal for the task we are considering.
</bodyText>
<sectionHeader confidence="0.968308" genericHeader="method">
5. Canonical Grammars
</sectionHeader>
<bodyText confidence="0.999915">
Our extraction technique produces a restricted type of lexicalized linear context-free
rewriting system that we will refer to as “canonical.” In this section we provide a
declarative characterization of these grammars, and show that every lexicalized LCFRS
is equivalent to a canonical one.
</bodyText>
<subsectionHeader confidence="0.998971">
5.1 Definition of Canonical Grammars
</subsectionHeader>
<bodyText confidence="0.995009571428571">
We are interested in a syntactic characterization of the yield functions that can occur
in extracted grammars. We give such a characterization in terms of four properties,
stated in the following. We use the following terminology and notation. Consider a
yield function
f: k1 ··· km - +k , f = (α1,...,αk)
For variables x, y we write x &lt;f y to state that x precedes y in the template of f, that
is, in the string α1 · · · αk. Recall that, in the context of our extraction procedure, the
</bodyText>
<page confidence="0.986212">
365
</page>
<note confidence="0.282882">
Computational Linguistics Volume 39, Number 2
</note>
<bodyText confidence="0.938303307692308">
components in the template of f represent the blocks of a node u, and the variables in
the template represent the blocks of the children of u. For a variable xi,j we call i the
argument index and j the component index of the variable.
Property 1
For all 1 &lt; i1, i2 &lt; m, if i1 &lt; i2 then xi1,1 &lt;f xi2,1.
This property is an artifact of our decision to order the children of a node from left
to right based on the position of their leftmost descendants. A variable with argument
index i represents a block of the ith child of u in that order. An example of a yield
function that does not have Property 1 is (x2,1 x1,1), which defines a kind of “reverse
concatenation operation.”
Property 2
For all 1 &lt; i &lt; m and 1 &lt; j1,j2 &lt; ki, if j1 &lt; j2 then xi,j1 &lt;f xi,j2.
This property reflects that, in our extraction procedure, the variable xi,j represents the
jth block of the ith child of u, where the blocks of a node are ordered from left to right
based on their precedence. An example of a yield function that violates the property
is (x1,2 x1,1), which defines a kind of swapping operation. In the literature on LCFRSs
and related formalisms, yield functions with Property 2 have been called monotone
(Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer
2010), and non-permuting (Kanazawa 2009).
Property 3
No component o h is the empty string.
This property, which is similar to ε-freeness as known from context-free grammars,
has been discussed for multiple context-free grammars (Seki et al. 1991, Property N3
in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1). For our
extracted grammars it holds because each component o h represents a block, and blocks
are always non-empty.
Property 4
No component o h contains a substring of the form xi,j1xi,j2.
This property, which does not seem to have been discussed in the literature before, is a
reflection of the facts that variables with the same argument index represent blocks of
the same child node, and that these blocks are longest segments of descendants.
A yield function with Properties 1–4 is called canonical. An LCFRS is canonical if
all of its yield functions are canonical.
Lemma 1
A lexicalized LCFRS is canonical if and only if it can be extracted from a dependency
treebank using the technique presented in Section 4.
Proof
We have already argued for the “only if” part of the claim. To prove the “if” part, it
suffices to show that for every canonical, lexicalized yield function f, one can construct
</bodyText>
<page confidence="0.982369">
366
</page>
<bodyText confidence="0.862415">
Kuhlmann Mildly Non-Projective Dependency Grammar
a dependency tree such that the construction tree extracted for this dependency tree
contains f. This is an easy exercise. ■
We conclude by noting that Properties 2–4 are also shared by the treebank grammars
extracted from constituency treebanks using the technique by Maier and Søgaard (2008).
</bodyText>
<subsectionHeader confidence="0.999273">
5.2 Equivalence Between General and Canonical Grammars
</subsectionHeader>
<bodyText confidence="0.9520412">
Two lexicalized LCFRSs are called strongly equivalent if they induce the same set of
dependency trees. We show the following equivalence result:
Lemma 2
For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized
LCFRS G&apos; such that G&apos; is canonical.
Proof
Our proof of this lemma uses two normal-form results about multiple context-free
grammars: Michaelis (2001, Section 2.4) provides a construction that transforms a mul-
tiple context-free grammar into a weakly equivalent multiple context-free grammar in
which all rules satisfy Property 2, and Seki et al. (1991, Lemma 2.2) present a corre-
sponding construction for Property 3. Whereas both constructions are only quoted to
preserve weak equivalence, we can verify that, in the special case where the input
grammar is a lexicalized LCFRS, they also preserve the set of induced dependency trees.
To complete the proof of Lemma 2, we show that every lexicalized LCFRS can be cast
into normal forms that satisfy Property 1 and Property 4. It is not hard then to combine
the four constructions into a single one that simultaneously establishes all properties of
canonical yield functions. ■
Lemma 3
For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized
LCFRS G&apos; such that G&apos; only contains yield functions which satisfy Property 1.
</bodyText>
<subsectionHeader confidence="0.566411">
Proof
</subsectionHeader>
<bodyText confidence="0.964451777777778">
The proof is very simple. Intuitively, Property 1 enforces a canonical naming of the
arguments of yield functions. To establish it, we determine, for every yield function f,
a permutation π that renames the argument indices of the variables occurring in the
template off in such a way that the template meets Property 1. This renaming gives rise
to a modified yield function fπ. We then replace every rule A → f (A1, ... , Am) with the
modified rule A → fπ(Aπ(1), . . . , Aπ(m)). ■
Lemma 4
For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized
LCFRS G&apos; such that G&apos; only contains yield functions which satisfy Property 4.
</bodyText>
<subsectionHeader confidence="0.489839">
Proof
</subsectionHeader>
<bodyText confidence="0.9999702">
The idea behind our construction of the grammar G&apos; is perhaps best illustrated by an
example. Imagine that the grammar G generates the term t shown in Figure 8a. The yield
function f1 = (x1 c x2 x3) at the root node of that term violates Property 4, as its template
contains the offending substring x2 x3. We set up G&apos; in such a way that instead of t it
generates the term t&apos; shown in Figure 8b in which f1 is replaced with the yield function
</bodyText>
<page confidence="0.966699">
367
</page>
<figure confidence="0.887545">
Computational Linguistics Volume 39, Number 2
</figure>
<figureCaption confidence="0.990468">
Figure 8
</figureCaption>
<bodyText confidence="0.998242666666667">
The transformation implemented by the construction of the grammar G&apos; in Lemma 4.
f&apos;1 = (x1 cx2). To obtain f1&apos; from f1 we reduce the offending substring x2 x3 to the single
variable x2. In order to ensure that t and t&apos; induce the same dependency tree (shown in
Figure 8c), we then adapt the function f2 = (x1 b, y, x2) at the first child of the root node:
Dual to the reduction, we replace the two-component sequence y, x2 in the template of f2
with the single component y x2; in this way we get f2� = (x1 b, y x2).
Because adaptation operations may introduce new offending substrings, we need a
recursive algorithm to compute the rules of the grammar G&apos;. Such an algorithm is given
in Table 2. For every rule A -+ f (A1, ... , Am) of G we construct new rules
</bodyText>
<equation confidence="0.795167">
(A,g) -+f�((A1,g1),..., (Am,gm))
</equation>
<bodyText confidence="0.999628333333333">
where g and the gi are yield functions encoding adaptation operations. As an example,
the adaptation of the function f2 in the term t may be encoded into the adaptor function
(x1, x2 x3). The function f2~ can then be written as the composition of this function and f2:
</bodyText>
<equation confidence="0.9653">
f2 = (x1,x2 x3) o f2 = (x1,x2 x3)((x1 b,y,x2)) = (x1 b,yx2)
</equation>
<bodyText confidence="0.999831">
The yield function f~ and the adaptor functions gi are computed based on the template
of the g-adapted yield function f, that is, the composed function g o f. In Table 2 we write
this as f&apos; = reduce(f,g) and gi = adapt(f,g, i), respectively. Let us denote the template of
the adapted function g o f by r. An i-block of r is a maximal, non-empty substring of
some component of r that consists of variables with argument index i. To compute the
template of gi we read the i-blocks of r from left to right and rename the variables by
changing their argument indices from i to 1. To compute the template off, we take the
</bodyText>
<tableCaption confidence="0.700569">
Table 2
</tableCaption>
<bodyText confidence="0.896658">
Computing the production rules of an LCFRS in which all yield functions satisfy Property 4.
Input: a linear context-free rewriting system G = (N, E, P, S)
</bodyText>
<listItem confidence="0.9522548">
1: P� +- 0; agenda +- {(S, (x))}; chart +- 0
2: while agenda is not empty
3: remove some (A, g) from agenda
4: if (A, g) E/ chart then
5: add (A, g) to chart
6: for each rule A - +f (A1, ... , Am) E P do
7: f +- reduce(f,g); gi +- adapt(f,g,i) (1 &lt; i &lt; m)
8: for each i from 1 to m do
9: add (Ai,gi) to agenda
10: add (A,g) -+ f&apos;((A1,g1),..., (Am,gm)) to P&apos;
</listItem>
<page confidence="0.982986">
368
</page>
<bodyText confidence="0.952731">
Kuhlmann Mildly Non-Projective Dependency Grammar
template r and replace the jth i-block with the variable xi,j, for all argument indices i
and component indices j.
Our algorithm is controlled by an agenda and a chart, both containing pairs of
the form (A, g), where A is a nonterminal of G and g is an adaptor function. These
pairs also constitute the nonterminals of the new grammar G&apos;. The fan-out of a non-
terminal is the fan-out of g. The agenda is initialized with the pair (S, (x)) where (x)
is the identity function; this pair also represents the start symbol of G&apos;. To see that
the algorithm terminates, one may observe that the fan-out of every nonterminal (A, g)
added to the agenda is upper-bounded by the fan-out of A. Hence, there are only finitely
many pairs (A,g) that may occur in the chart, and a finite number of iterations of the
while-loop. ■
We conclude by noting that when constructing a canonical grammar, one needs to
be careful about the order in which the individual constructions (for Properties 1–4) are
combined. One order that works is
</bodyText>
<listItem confidence="0.5778535">
Property 3 &lt; Property 4 &lt; Property 2 &lt; Property 1
6. Parsing and Recognition
</listItem>
<bodyText confidence="0.99818825">
Lexicalized linear context-free rewriting systems are able to account for arbitrarily non-
projective dependency trees. This expressiveness comes with a price: In this section we
show that parsing with lexicalized LCFRSs is intractable, unless we are willing to restrict
the class of grammars.
</bodyText>
<subsectionHeader confidence="0.999427">
6.1 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.997128333333333">
To ground our discussion of parsing complexity, we present a simple bottom–up parsing
algorithm for LCFRSs, specified as a grammatical deduction system (Shieber, Schabes,
and Pereira 1995). Several similar algorithms have been described in the literature (Seki
et al. 1991; Bertsch and Nederhof 2001; Kallmeyer 2010). We assume that we are given a
grammar G = (N, Σ, P, S) and a string w = a1 · · · an E V∗ to be parsed.
Item form. The items of the deduction system take the form
</bodyText>
<equation confidence="0.750935">
[A, l1, r1, ... , lk, rk]
</equation>
<bodyText confidence="0.92489325">
where A E N with y(A) = k, and the remaining components are indices identifying the
left and right endpoints of pairwise non-overlapping substrings of w. More formally,
0 &lt; lh &lt; rh &lt; n, and for all h, h&apos; with h =� h&apos;, either rh &lt; lh&apos; or rh, &lt; lh. The intended
interpretation of an item of this form is that A derives a term t E T(G) that yields the
specified substrings of w, that is,
A �∗G t and yield(t) = (al1+1 ···ar1, . . . ,alk+1 · · · ark)
Goal item. The goal item is [S, 0, n]. By this item, there exists a term that can be derived
from the start symbol S and yields the full string (w).
</bodyText>
<page confidence="0.987013">
369
</page>
<note confidence="0.396878">
Computational Linguistics Volume 39, Number 2
</note>
<bodyText confidence="0.454044">
Inference rules. The inference rules of the deduction system are defined based on the
rules in P. Each production rule
</bodyText>
<equation confidence="0.8827085">
A -+ f(A1,...,Am) with f : k1 ··· km -+ k , f = (α1,...,αk)
is converted into a set of inference rules of the form
�~A1, l1,1, r1,1, ... ,l1,k1, r1,k1 ] ··· [Am, lm,1, rm,1,... , lm,km, rm,km
[A, l1, r1, ... , lk, rk]
</equation>
<bodyText confidence="0.9817205">
Each such rule is subject to the following constraints. Let 1 &lt; h &lt; k, v E V∗, 1 &lt; i &lt; m,
and 1 &lt; j &lt; ki. We write δ(l,v) = r to assert that r = l + |v |and that v is the substring
of w between indices l and r.
If αh = v then δ(lh,v) = rh
If v xi,j is a prefix of αh then δ(lh, v) = li,j
If xi,j v is a suffix of αh then δ(ri,j, v) = rh
If xi,j v xi,,j, is an infix of αh then δ(ri,j, v) = li,,j,
These constraints ensure that the substrings corresponding to the premises of the
inference rule can be combined into the substrings corresponding to the conclusion by
means of the yield function f.
Based on the deduction system, a tabular parser for LCFRSs can be implemented
using standard dynamic programming techniques. This parser will compute a packed
representation of the set of all derivation trees that the grammar G assigns to the
string w. Such a packed representation is often called a shared forest (Lang 1994). In
combination with appropriate semirings, the shared forest is useful for many tasks in
syntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009).
</bodyText>
<subsectionHeader confidence="0.999803">
6.2 Parsing Complexity
</subsectionHeader>
<bodyText confidence="0.992878230769231">
We are interested in an upper bound on the runtime of the tabular parser that we have
just presented. We can see that the parser runs in time O(|G||w|c), where |G |denotes
the size of some suitable representation of the grammar G, and c denotes the maximal
number of instantiations of an inference rule (cf. McAllester 2002). Let us write c(f ) for
the specialization of c to inference rules for productions with yield function f. We refer
to this value as the parsing complexity off (cf. Gildea 2010). Then to show an upper
bound on c it suffices to show an upper bound on the parsing complexities of the yield
functions that the parser has to handle. An obvious such upper bound is
c(f ) &lt; 2k + ~m 2ki
i=1
Here we imagine that we could choose each endpoint in Equation (3) independently of
all the others. By virtue of the constraints, however, some of the endpoints cannot be
chosen freely; in particular, some of the substrings may be adjacent. In general, to show
</bodyText>
<figure confidence="0.433080666666667">
(3)
370
Kuhlmann Mildly Non-Projective Dependency Grammar
</figure>
<bodyText confidence="0.9067925">
an upper bound c(f ) &lt; b we specify a strategy for choosing b endpoints, and then argue
that, given the constraints, these choices determine the remaining endpoints.
Lemma 5
For a yield function f : k1 · · · km -+ k we have
</bodyText>
<equation confidence="0.872266">
m
c( f) &lt; k + ki
i=1
Proof
</equation>
<bodyText confidence="0.9998718">
We adopt the following strategy for choosing endpoints: For 1 &lt; i &lt; k, choose the
value of lh. Then, for 1 &lt; i &lt; m and 1 &lt; j &lt; ki, choose the value of ri j. It is not hard
to see that these choices suffice to determine all other endpoints. In particular, each left
endpoint li, j, will be shared either with the left endpoint lh of some component (by
constraint c2), or with some right endpoint ri,j (by constraint c4). ■
</bodyText>
<subsectionHeader confidence="0.992702">
6.3 Universal Recognition
</subsectionHeader>
<bodyText confidence="0.982351032258064">
The runtime of our parsing algorithm for LCFRSs is exponential in both the rank and the
fan-out of the input grammar. One may wonder whether there are parsing algorithms
that can be substantially faster. We now show that the answer to this question is likely
to be negative even if we restrict ourselves to canonical lexicalized LCFRSs. To this end
we study the universal recognition problem for this class of grammars.
The universal recognition problem for a class of linear context-free rewriting
systems is to decide, given a grammar G from the class in question and a string w,
whether G yields (w). A straightforward algorithm for solving this problem is to first
compute the shared forest for G and w, and to return “yes” if and only if the shared
forest is non-empty. Choosing appropriate data structures, the emptiness of shared
forests can be decided in linear time and space with respect to the size of the forest.
Therefore, the computational complexity of universal recognition is upper-bounded by
the complexity of constructing the shared forest. Conversely, parsing cannot be faster
than universal recognition.
In the next three lemmas we prove that the universal recognition problem for
canonical lexicalized LCFRSs is NP-complete unless we restrict ourselves to a class of
grammars where both the fan-out and the rank of the yield functions are bounded by
constants. Lemma 6, which shows that the universal recognition problem of lexicalized
LCFRSs is in NP, distinguishes lexicalized LCFRSs from general LCFRSs, for which the
universal recognition problem is known to be PSPACE-complete (Kaji et al. 1992). The
crucial difference between general and lexicalized LCFRSs is the fact that in the latter,
the size of the generated terms is bounded by the length of the input string. Lemma 7
and Lemma 8, which establish two NP-hardness results for lexicalized LCFRSs, are
stronger versions of the corresponding results for general LCFRSs presented by Satta
(1992), and are proved using similar reductions. They show that the hardness results
hold under significant restrictions of the formalism: to lexicalized form and to canonical
yield functions. Note that, whereas in Section 5.2 we have shown that every lexicalized
LCFRS is equivalent to a canonical one, the normal form transformation increases the
size of the original grammar by a factor that is at least exponential in the fan-out.
Lemma 6
The universal recognition problem of lexicalized LCFRSs is in NP.
</bodyText>
<page confidence="0.981421">
371
</page>
<figure confidence="0.3219885">
Computational Linguistics Volume 39, Number 2
Proof
</figure>
<bodyText confidence="0.99982775">
Let G be a lexicalized LCFRS, and let w be a string. To test whether G yields (w), we
guess a term t E T(G) and check whether t yields (w). Let |t |denote the length of some
string representation of t. Since the yield functions of G are lexicalized, |t |&lt;_ |w||G|. Note
that we have
</bodyText>
<equation confidence="0.945312">
|t |&lt;_ |w||G |&lt;_ |w|2 + 2|w||G |+ |G|2 = (|w |+ |G|)2
</equation>
<bodyText confidence="0.9999236">
Using a simple tabular algorithm, we can verify in time O(|w||G|) whether a candidate
term t belongs to T(G). It is then straightforward to compute the string yield of t in time
O(|w||G|). Thus we have a nondeterministic polynomial-time decider for the universal
recognition problem. ■
For the following two lemmas, recall the decision problem 3SAT, which is known
to be NP-complete. An instance of 3SAT is a Boolean formula φ in conjunctive normal
form where each clause contains exactly three literals, which may be either variables or
negated variables. We write m for the number of distinct variables that occur in φ, and n
for the number of clauses. In the proofs the index i will always range over values from 1
to m, and the index j will range over values from 1 to n.
In order to make the grammars in the following reductions more readable, we use
yield functions with more than one lexical anchor. Our use of these yield functions
is severely restricted, however, and each of our grammars can be transformed into a
proper lexicalized LCFRS without affecting the correctness or polynomial size of the
reductions.
</bodyText>
<subsectionHeader confidence="0.833908">
Lemma 7
</subsectionHeader>
<bodyText confidence="0.999862">
The universal recognition problem for canonical lexicalized LCFRSs with unbounded
fan-out and rank 1 is NP-hard.
</bodyText>
<subsectionHeader confidence="0.794113">
Proof
</subsectionHeader>
<bodyText confidence="0.999996272727273">
To prove this claim, we provide a polynomial-time reduction of 3SAT. The basic idea is
to use the derivations of the grammar to guess truth assignments for the variables, and
to use the feature of unbounded fan-out to ensure that the truth assignment satisfies all
clauses.
Let φ be an instance of 3SAT. We construct a canonical lexicalized LCFRS G and a
string w as follows. Let M denote the m x n matrix with entries Mi,j = (vi, cj), that is,
entries in the same row share the same variable, and entries in the same column share
the same clause. We set up G in such a way that each of its derivations simulates a row-
wise iteration over M. Before visiting a new row, the derivation chooses a truth value
for the corresponding variable, and sticks to that choice until the end of the row. The
string w takes the form
</bodyText>
<equation confidence="0.976702">
w = w1 $ ··· $ wn where wj = cj,1 ··· cj,m cj,1 ··· cj,m
</equation>
<bodyText confidence="0.998074333333333">
This string is built up during the iteration over M in a column-wise fashion, where each
column corresponds to one component of a tuple with fan-out n. More specifically, for
each entry (vi, cj), the derivation generates one of two strings, denoted by γi,j and ¯γi,j:
</bodyText>
<equation confidence="0.851162">
γi,j = cj,i ··· cj,m cj,1 ··· cj,i ¯γi,j = cj,i
</equation>
<page confidence="0.974564">
372
</page>
<note confidence="0.337098">
Kuhlmann Mildly Non-Projective Dependency Grammar
</note>
<bodyText confidence="0.999920375">
The string γi,j is generated only if vi can be used to satisfy cj under the hypothesized
truth assignment. By this construction, every successful derivation of G represents a
truth assignment that satisfies φ. Conversely, using a satisfying truth assignment for φ,
we will be able to construct a derivation of G that yields w.
To see how the traversal of the matrix M can be implemented by the grammar G,
consider the grammar fragment in Figure 9. Each of the rules specifies one possible step
of the iteration for the pair (vi, cj) under the truth assignment vi = true; rules with left-
hand side Fi,j (not shown here) specify possible steps under the assignment vi = false. ■
</bodyText>
<subsectionHeader confidence="0.803294">
Lemma 8
</subsectionHeader>
<bodyText confidence="0.9999055">
The universal recognition problem for canonical lexicalized LCFRSs with unbounded
rank and fan-out 2 is NP-hard.
</bodyText>
<subsectionHeader confidence="0.740153">
Proof
</subsectionHeader>
<bodyText confidence="0.9999676">
We provide another polynomial-time reduction of 3SAT to a grammar G and a string w,
again based on the matrix M mentioned in the previous proof. Also as in the previous
reduction, we set up the grammar G to simulate a row-wise iteration over M. The major
difference this time is that the entries of M are not visited during one long rank 1
derivation, but during mn rather short fan-out 2 subderivations. The string w is
</bodyText>
<equation confidence="0.991372">
w = w&lt;,1 ··· wa,m $ wD,1 ··· wD,n
</equation>
<bodyText confidence="0.994655666666667">
where wa,i = ai,1 ··· ai,n bi,1 ··· bi,n and w&gt;,j = c1,j ··· cm,j c1,j ··· cm,j
During the traversal of M, for each entry (vi, cj), we generate a tuple consisting of two
substrings of w. The right component of the tuple consists of one the two strings γi,j
and ¯γi,j mentioned previously. As before, the string γi,j is generated only if vi can be
used to satisfy cj under the hypothesized truth assignment. The left component consists
of one of two strings, denoted by σi,j and ¯σi,j:
</bodyText>
<equation confidence="0.945801">
σi,1 = ai,1 ··· ai,n bi,1 σi,j = bi,j (1 &lt; j) ¯σi,n = ai,n bi,1 ··· bi,n ¯σi,j = ai,j (j &lt; n)
</equation>
<bodyText confidence="0.9993155">
These strings are generated to represent the truth assignments vi = true and vi =false,
respectively. By this construction, each substring w&lt;,i can be derived in exactly one of
two ways, ensuring a consistent truth assignment for all subderivations that are linked
to the same variable vi.
</bodyText>
<figureCaption confidence="0.850042">
Figure 9
</figureCaption>
<bodyText confidence="0.865215">
A fragment of the grammar used in the proof of Lemma 7.
</bodyText>
<page confidence="0.988997">
373
</page>
<note confidence="0.481582">
Computational Linguistics Volume 39, Number 2
</note>
<bodyText confidence="0.999632571428571">
The grammar G is defined as follows. There is one rather complex rule to rewrite
the start symbol S; this rule sets up the general topology of w. Let I be the m × n matrix
with entries Ii,j = (j − 1)m + i. Define z1 to be the sequence of variables of the form xh,1,
where the argument index i is taken from a row-wise reading of the matrix I; in this
case, the argument indices in x� will simply go up from 1 to mn. Now define z2 to be the
sequence of variables of the form xh,2, where h is taken from a column-wise reading of
the matrix I. Then S can be expanded with the rule
</bodyText>
<equation confidence="0.777629">
S → OX1 $�x2)(V1,1,...,V1,n, ... ,Vm,1, ... ,Vm,n)
</equation>
<bodyText confidence="0.720026">
Note that there is one nonterminal Vi,j for each variable–clause pair (vi, cj). These non-
terminals can be rewritten using the following rules:
</bodyText>
<equation confidence="0.835661">
Vi,1 → (σi,1,x)(Ti,1) Vi,j → (σi,j,x)(Ti,j)
Vi,n → ( ¯σi,n, x)(Fi,n) Vi,j → ( ¯σi,j, x)(Fi,j)
The remaining rules rewrite the nonterminals Ti,j and Fi,j:
Ti,j → (γi,j) (if vi occurs in cj) Ti,j → (¯γi,j)
Fi,j → (γi,j) (if ¯vi occurs in cj) Fi,j → (¯γi,j)
</equation>
<bodyText confidence="0.98398425">
It is not hard to see that both G and w can be constructed in polynomial time. ■
7. Block-Degree
To obtain efficient parsing, we would like to have grammars with as low a fan-out as
possible. Therefore it is interesting to know how low we can go without losing too much
coverage. In lexicalized LCFRSs extracted from dependency treebanks, the fan-out of a
grammar has a structural correspondence in the maximal number of blocks per subtree,
a measure known as “block-degree.” In this section we formally define block-degree,
and evaluate grammar coverage under different bounds on this measure.
</bodyText>
<subsectionHeader confidence="0.999762">
7.1 Definition of Block-Degree
</subsectionHeader>
<bodyText confidence="0.9141378">
Recall the concept of “blocks” that was defined in Section 4.2. The block-degree of a
node u of a dependency tree D is the number of distinct blocks of u. The block-degree
of D is the maximal block-degree of its nodes.2
Example 6
Figure 10 shows two non-projective dependency trees. For D1, consider the node 2. The
descendants of 2 fall into two blocks, marked by the dashed boxes. Because this is the
maximal number of blocks per node in D1, the block-degree of D1 is 2. Similarly, we can
verify that the block-degree of the dependency tree D2 is 3.
2 We note that, instead of counting the blocks of each node, one may also count the gaps between these
blocks and define the “gap-degree” of a dependency tree (Holan et al. 1998).
</bodyText>
<page confidence="0.994822">
374
</page>
<figure confidence="0.765988">
Kuhlmann Mildly Non-Projective Dependency Grammar
</figure>
<figureCaption confidence="0.850446">
Figure 10
Block-degree.
</figureCaption>
<bodyText confidence="0.999718666666667">
A dependency tree is projective if its block-degree is 1. In a projective dependency
tree, each subtree corresponds to a substring of the underlying tuple of strings. In a non-
projective dependency tree, a subtree may span over several, discontinuous substrings.
</bodyText>
<subsectionHeader confidence="0.999607">
7.2 Computing the Block-Degrees
</subsectionHeader>
<bodyText confidence="0.999933666666667">
Using a straightforward extension of the algorithm in Table 1, the block-degrees of all
nodes of a dependency tree D can be computed in time O(m), where m is the total
number of blocks. To compute the block-degree of D, we simply take the maximum
over the degrees of each node. We can also adapt this procedure to test whether D is
projective, by aborting the computation as soon as we discover that some node has
more than one block. The runtime of this test is linear in the number of nodes of D.
</bodyText>
<subsectionHeader confidence="0.989121">
7.3 Block-Degree in Extracted Grammars
</subsectionHeader>
<bodyText confidence="0.999920583333333">
In a lexicalized LCFRS extracted from a dependency treebank, there is a one-to-one
correspondence between the blocks of a node u and the components of the template
of the yield function f extracted for u. In particular, the fan-out of f is exactly the
block-degree of u. As a consequence, any bound on the block-degree of the trees in
the treebank translates into a bound on the fan-out of the extracted grammar. This has
consequences for the generative capacity of the grammars: As Seki et al. (1991) show,
the class of LCFRSs with fan-out k &gt; 1 can generate string languages that cannot be
generated by the class of LCFRSs with fan-out k − 1.
It may be worth emphasizing that the one-to-one correspondence between blocks
and tuple components is a consequence of two characteristic properties of extracted
grammars (Properties 3 and 4), and does not hold for non-canonical lexicalized
LCFRSs.
</bodyText>
<subsectionHeader confidence="0.568529">
Example 7
</subsectionHeader>
<bodyText confidence="0.999982666666667">
The following term induces a two-node dependency tree with block-degree 1, but
contains yield functions with fan-out 2: (a x1 x2)((b, ε)). Note that the yield functions
in this term violate both Property 3 and Property 4.
</bodyText>
<subsectionHeader confidence="0.980531">
7.4 Coverage on Dependency Treebanks
</subsectionHeader>
<bodyText confidence="0.994491">
In order to assess the consequences of different bounds on the fan-out, we now evaluate
the block-degree of dependency trees in real-world data. Specifically, we look into five
</bodyText>
<page confidence="0.995673">
375
</page>
<note confidence="0.488263">
Computational Linguistics Volume 39, Number 2
</note>
<bodyText confidence="0.994439029411764">
dependency treebanks used in the 2006 CoNLL shared task on dependency parsing
(Buchholz and Marsi 2006): the Prague Arabic Dependency Treebank (Hajiˇc et al. 2004),
the Prague Dependency Treebank of Czech (B¨ohmov´a et al. 2003), the Danish Depen-
dency Treebank (Kromann 2003), the Slovene Dependency Treebank (Dˇzeroski et al.
2006), and the Metu-Sabancı treebank of Turkish (Oflazer et al. 2003). The full data used
in the CoNLL shared task also included treebanks that were produced by conversion
of corpora originally annotated with structures other than dependencies, which is a
potential source of “noise” that one has to take into account when interpreting any
findings. Here, we consider only genuine dependency treebanks. More specifically, our
statistics concern the training sections of the treebanks that were set off for the task. For
similar results on other data sets, see Kuhlmann and Nivre (2006), Havelka (2007), and
Maier and Lichte (2011).
Our results are given in Table 3. For each treebank, we list the number of rules
extracted from that treebank, as well as the number of corresponding dependency trees.
We then list the number of rules that we lose if we restrict ourselves to rules with fan-
out = 1, or rules with fan-out &lt; 2, as well as the number of dependency trees that we
lose because their construction trees contain at least one such rule. We count rule tokens,
meaning that two otherwise identical rules are counted twice if they were extracted
from different trees, or from different nodes in the same tree.
By putting the bound at fan-out 1, we lose between 0.74% (Arabic) and 1.75%
(Slovene) of the rules, and between 11.16% (Arabic) and 23.15% (Czech) of the trees
in the treebanks. This loss is quite substantial. If we instead put the bound at fan-out
&lt; 2, then rule loss is reduced by between 94.16% (Turkish) and 99.76% (Arabic), and
tree loss is reduced by between 94.31% (Turkish) and 99.39% (Arabic). This outcome
is surprising. For example, Holan et al. (1998) argue that it is impossible to give a
theoretical upper bound for the block-degree of reasonable dependency analyses of
Czech. Here we find that, if we are ready to accept a loss of as little as 0.02% of the
rules extracted from the Prague Dependency Treebank, and up to 0.5% of the trees, then
such an upper bound can be set at a block-degree as low as 2.
8. Well-Nestedness
The parsing of LCFRSs is exponential both in the fan-out and in the rank of the
grammars. In this section we study “well-nestedness,” another restriction on the non-
projectivity of dependency trees, and show how enforcing this constraint allows us to
restrict our attention to the class of LCFRSs with rank 2.
</bodyText>
<tableCaption confidence="0.995082">
Table 3
</tableCaption>
<table confidence="0.96796775">
Loss in coverage under the restriction to yield functions with fan-out = 1 and fan-out &lt; 2.
fan-out = 1 fan-out &lt; 2
rules trees rules trees rules trees
Arabic 5,839 1,460 411 163 1 1
Czech 1,322,111 72,703 22,283 16,831 328 312
Danish 99,576 5,190 1,229 811 11 9
Slovene 30,284 1,534 530 340 14 11
Turkish 62,507 4,997 924 580 54 33
</table>
<page confidence="0.873384">
376
</page>
<note confidence="0.297835">
Kuhlmann Mildly Non-Projective Dependency Grammar
</note>
<subsectionHeader confidence="0.994964">
8.1 Definition of Well-Nestedness
</subsectionHeader>
<bodyText confidence="0.842057058823529">
Let D be a dependency tree, and let u and v be nodes of D. The descendants of u
and v overlap, denoted by Lu] 0 Lv], if there exist nodes ul, ur E Lu] and vl, vr E Lv]
such that
ul &lt; vl &lt; ur &lt; vr or vl &lt; ul &lt; vr &lt; ur
A dependency tree D is called well-nested if for all pairs of nodes u, v of D
Lu] 0 Lv] implies that Lu] n Lv] =� 0
In other words, Lu] and Lv] may overlap only if u is an ancestor of v, or v is an ancestor
of u. If this implication does not hold, then D is called ill-nested.
Example 8
Figure 11 shows three non-projective dependency trees. Both D1 and D2 are well-nested:
D1 does not contain any overlapping sets of descendants at all. In D2, although L1]
and L2] overlap, it is also the case that L1] D L2]. In contrast, D3 is ill-nested, as
L2] 0 L3] but L2] n L3] = 0
The following lemma characterizes well-nestedness in terms of blocks.
Lemma 9
A dependency tree is ill-nested if and only if it contains two sibling nodes u, v and blocks
u1, u2 of u and 61,62 of v such that
</bodyText>
<equation confidence="0.840814">
u1 &lt; 61 &lt; 42 &lt; 62 (4)
Proof
</equation>
<bodyText confidence="0.941682">
Let D be a dependency tree. Suppose that D contains a configuration of the form (4).
This configuration witnesses that the sets Lu] and Lv] overlap. Because u, v are siblings,
Lu] n Lv] = 0. Therefore we conclude that D is ill-nested. Conversely now, suppose
that D is ill-nested. In this case, there exist two nodes u and v such that
Lu] 0 Lv] and Lu] n Lv] = 0 (*)
</bodyText>
<figureCaption confidence="0.722237">
Figure 11
</figureCaption>
<subsubsectionHeader confidence="0.257864">
Well-nestedness and ill-nestedness.
</subsubsectionHeader>
<page confidence="0.974558">
377
</page>
<note confidence="0.480914">
Computational Linguistics Volume 39, Number 2
</note>
<bodyText confidence="0.9703144">
Here, we may assume u and v to be siblings: otherwise, we may replace either u or v
with its parent node, and property (*) will continue to hold. Because Lu] 0 Lv], there
exist descendants ul, ur E Lu] and vl, vr E Lv] such that
ul &lt; vl &lt; ur &lt; vr or vl &lt; ul &lt; vr &lt; ur
Without loss of generality, assume that we have the first case. The nodes ul and ur belong
to different blocks of u, say u1 and u2; and the nodes vl and vr belong to different blocks
of v, say 61 and 62. Then it is not hard to verify Equation (4). ■
Note that projective dependency trees are always well-nested; in these structures,
every node has exactly one block, so configuration (4) is impossible. For every k &gt; 1,
there are both well-nested and ill-nested dependency trees with block-degree k.
</bodyText>
<subsectionHeader confidence="0.984383">
8.2 Testing for Well-Nestedness
</subsectionHeader>
<bodyText confidence="0.999956769230769">
Based on Lemma 9, testing whether a dependency tree D is well-nested can be done in
time linear in the number of blocks in D using a simple subsequence test as follows. We
run the algorithm given in Table 1, maintaining a stack s[u] for every node u. The first
time we make a down step to u, we push u to the stack for the parent of u; every other
time, we pop the stack for the parent until we either find u as the topmost element, or the
stack becomes empty. In the latter case, we terminate the computation and report that D
is ill-nested; if the computation can be completed without any stack ever becoming
empty, we report that D is well-nested.
To show that the algorithm is sound, suppose that some stack s[p] becomes empty
when making a down step to some child v of p. In this case, the node v must have been
popped from s[p] when making a down step to some other child u of p, and that child
must have already been on the stack before the first down step to v. This witnesses the
existence of a configuration of the form in Equation (4).
</bodyText>
<subsectionHeader confidence="0.983988">
8.3 Well-Nestedness in Extracted Grammars
</subsectionHeader>
<bodyText confidence="0.9610655">
Just like block-degree, well-nestedness can be characterized in terms of yield functions.
Recall the notation x &lt;f y from Section 5.1. A yield function
</bodyText>
<equation confidence="0.6957868">
f: k1 ··· km - +k , f = (α1,...,αk)
is ill-nested if there are argument indices 1 &lt; i1, i2 &lt; m with i1 =� i2 and component
indices 1 &lt; j1, j&apos;1 &lt; ki1, 1 &lt; j2, j&apos; 2 &lt; ki2 such that
xi1,j1 &lt;f xi2,j2 &lt;f xi1,ji &lt;f xi2,j� (5)
2
</equation>
<bodyText confidence="0.9993715">
Otherwise, we say that f is well-nested. As an immediate consequence of Lemma 9, a
restriction to well-nested dependency trees translates into a restriction to well-nested
yield functions in the extracted grammars. This puts them into the class of what
Kanazawa (2009) calls “well-nested multiple context-free grammars.”3 These grammars
</bodyText>
<footnote confidence="0.745454">
3 Kanazawa (2009) calls a multiple context-free grammar well-nested if each of its rules is non-deleting,
non-permuting (our Property 2), and well-nested according to (5).
</footnote>
<page confidence="0.992974">
378
</page>
<note confidence="0.348562">
Kuhlmann Mildly Non-Projective Dependency Grammar
</note>
<bodyText confidence="0.9998593">
have a number of interesting properties that set them apart from general LCFRSs; in
particular, they have a standard pumping lemma (Kanazawa 2009). The yield languages
generated by well-nested multiple context-free grammars form a proper subhierarchy
within the languages generated by general LCFRSs (Kanazawa and Salvati 2010). Per-
haps the most prominent subclass of well-nested LCFRSs is the class of tree-adjoining
grammars (Joshi and Schabes 1997).
Similar to the situation with block-degree, the correspondence between structural
well-nestedness and syntactic well-nestedness is tight only for canonical grammars.
For non-canonical grammars, syntactic well-nestedness alone does not imply structural
well-nestedness, nor the other way around.
</bodyText>
<subsectionHeader confidence="0.98816">
8.4 Coverage on Dependency Treebanks
</subsectionHeader>
<bodyText confidence="0.999992142857143">
To estimate the coverage of well-nested grammars, we extend the evaluation presented
in Section 7.4. Table 4 shows how many rules and trees in the five dependency treebanks
we lose if we restrict ourselves to well-nested yield functions with fan-out &lt; 2. The
losses reported in Table 3 are repeated here for comparison. Although the coverage
of well-nested rules is significantly smaller than the coverage of rules without this
requirement, rule loss is still reduced by between 92.65% (Turkish) and 99.51% (Arabic)
when compared to the fan-out = 1 baseline.
</bodyText>
<subsectionHeader confidence="0.998522">
8.5 Binarization of Well-Nested Grammars
</subsectionHeader>
<bodyText confidence="0.89315575">
Our main interest in well-nestedness comes from the following:
Lemma 10
The universal recognition problem for well-nested lexicalized LCFRS with fan-out k and
unbounded rank can be decided in time
</bodyText>
<equation confidence="0.992588">
(|G |· |w|2k+2)
O
</equation>
<bodyText confidence="0.999620166666667">
To prove this lemma, we will provide an algorithm for the binarization of well-
nested lexicalized LCFRSs. In the context of LCFRSs, a binarization is a procedure for
transforming a grammar into an equivalent one with rank at most 2. Binarization,
either explicit at the level or the grammar or implicit at the level of some parsing
algorithm, is essential for achieving efficient recognition algorithms, in particular the
usual cubic-time algorithms for context-free grammars. Note that our binarization only
</bodyText>
<tableCaption confidence="0.62111175">
Table 4
Loss in coverage under the restriction to yield functions with fan-out = 1, fan-out &lt; 2,
and to well-nested yield functions with fan-out &lt; 2 (last column).
fan-out = 1 fan-out &lt; 2 + well-nested
</tableCaption>
<table confidence="0.997912">
rules trees rules trees rules trees rules trees
Arabic 5,839 1,460 411 163 1 1 2 2
Czech 1,322,111 72,703 22,283 16,831 328 312 407 382
Danish 99,576 5,190 1,229 811 11 9 17 15
Slovene 30,284 1,534 530 340 14 11 17 13
Turkish 62,507 4,997 924 580 54 33 68 43
</table>
<page confidence="0.9212">
379
</page>
<note confidence="0.547885">
Computational Linguistics Volume 39, Number 2
</note>
<bodyText confidence="0.999872363636364">
preserves weak equivalence; in effect, it reduces the universal recognition problem for
well-nested lexicalized LCFRSs to the corresponding problem for well-nested LCFRSs
with rank 2. Many interesting semiring computations on the original grammar can be
simulated on the binarized grammar, however. A direct parsing algorithm for well-
nested dependency trees has been presented by G´omez-Rodr´ıguez, Carroll, and Weir
(2011).
The binarization that we present here is a special case of the binarization proposed
by G´omez-Rodr´ıguez, Kuhlmann, and Satta (2010). They show that every well-nested
LCFRS can be transformed (at the cost of a linear size increase) into a weakly equivalent
one in which all yield functions are either constants (that is, have rank 0) or binary
functions of one of two types:
</bodyText>
<equation confidence="0.9602285">
(x1,. . . , xk1 y1, ... , yk2) : k1 k2 -+ (k1 + k2 − 1) (concatenation) (6)
(x1, ... , xj y1, ... , yk2 xj+1,... , xk1) : k1 k2 -+ (k1 + k2 − 2) (wrapping) (7)
</equation>
<bodyText confidence="0.9999754">
A concatenation function takes a k1-tuple and a k2-tuple and returns the (k1 + k2 − 1)-
tuple that is obtained by concatenating the two arguments. The simplest concatenation
function is the standard concatenation operation (x y). We will write conc : k1 k2 to refer
to a concatenation function of the type given in Equation (6). By counting endpoints, we
see that the parsing complexity of concatenation functions is
</bodyText>
<equation confidence="0.872854">
c(conc : k1 k2) &lt; 2k1 + 2k2 − 1
</equation>
<bodyText confidence="0.971380461538462">
A wrapping function takes a k1-tuple (for some k1 &gt; 2) and a k2-tuple and returns the
(k1 + k2 − 2)-tuple that is obtained by “wrapping” the first argument around the second
argument, filling some gap in the former. The simplest function of this type is (x1 yx2),
which wraps a 2-tuple around a 1-tuple. We write wrap: k1 k2 j to refer to a wrapping
function of the type given in Equation (7). The parsing complexity is
c(wrap : k1 k2 j) &lt; 2k1 + 2k2 − 2 (for all choices of j)
The constants of the binarized grammar have the form (ε), (ε, ε), and (a), where a is the
anchor of some yield function of the original grammar.
8.5.1 Parsing Complexity. Before presenting the actual binarization, we determine the
parsing complexity of the binarized grammar. Because the binarization preserves the
fan-out of the original grammar, and because in a grammar with fan-out k, for con-
catenation functions conc : k1 k2 we have k1 + k2 − 1 &lt; k and for wrapping functions
wrap : k1 k2 j we have k1 + k2 − 2 &lt; k, we can rewrite the general parsing complexities as
</bodyText>
<equation confidence="0.9214105">
c(conc : k1 k2) &lt; 2k1 + 2k2 − 1 = 2(k1 + k2 − 1) + 1 &lt; 2k + 1
c(wrap : k1 k2 j) &lt; 2k1 + 2k2 − 2 = 2(k1 + k2 − 2) + 2 &lt; 2k + 2
</equation>
<bodyText confidence="0.948217">
Thus the maximal parsing complexity in the binarized grammar is 2k + 2; this is
achieved by wrapping operations. This gives the bound stated in Lemma 10.
</bodyText>
<page confidence="0.988318">
380
</page>
<figure confidence="0.604973">
Kuhlmann Mildly Non-Projective Dependency Grammar
</figure>
<figureCaption confidence="0.903187">
Figure 12
</figureCaption>
<bodyText confidence="0.590919">
Binarization of well-nested LCFRSs (complex cases).
</bodyText>
<subsubsectionHeader confidence="0.790211">
8.5.2 Binarization. We now turn to the actual binarization. Consider a rule
</subsubsectionHeader>
<bodyText confidence="0.978801111111111">
A → f(A1,...,Am)
where f is not already a concatenation function, wrapping function, or constant. We
decompose this rule into up to three rules
A → f&apos;(B,C) B → f1(B1,...,Bm1) C → f2(C1, ... ,Cm2)
as follows. We match the template off against one of three cases, shown schematically
in Figure 12. In each case we select a concatenation or wrapping function f&apos; (shown in
the right half of the figure), and split up the template off into two parts defining yield
functions f1 and f2, respectively. In Figure 12, f1 is drawn shaded, and f2 is drawn non-
shaded.4 The split of f partitions the variables that occur in the template, in the sense
</bodyText>
<footnote confidence="0.9448315">
4 In order for these parts to make well-defined templates, we will in general need to rename the variables.
We leave this renaming implicit here.
</footnote>
<page confidence="0.988891">
381
</page>
<note confidence="0.558478">
Computational Linguistics Volume 39, Number 2
</note>
<bodyText confidence="0.937576833333333">
that if for some argument index 1 &lt; i &lt; m, either f1 or f2 contains any variable with
argument index i, then it contains all such variables. The two sequences
B1, ... , Bm1 and C1, ... , Cm2 are obtained from A1, ... , Am
by collecting the nonterminal Ai if the variables with argument index i belong to the
template of f1 and f2, respectively. The nonterminals B and C are fresh nonterminals. We
do not create rules for f1 and f2 if they are identity functions.
</bodyText>
<subsectionHeader confidence="0.584354">
Example 9
</subsectionHeader>
<bodyText confidence="0.926808666666667">
We illustrate the binarization by showing how to transform the rule
A -+ (x1 a x2 y1, y2, y3 x3)(A1, A2)
The template (x1 a x2 y1, y2, y3 x3) is complex and matches Case 3 in Figure 12, because
its first component starts with the variable x1 and its last component ends with the
variable x3. We therefore split the template into two smaller parts (x1 a x2, x3) and
(y1, y2, y3). The function (y1, y2, y3) is an identity. We therefore create two rules:
A -+ f&apos;1(X, A2) , f&apos;1 = wrap : 2 31 = (x1 y1, y2, y3 x2) X -+ (x1 a x2, x3)(A1)
Note that the index j for the wrapping function was chosen to be j = 2 because there
were more component boundaries between x2 and x3 than between x1 and x2. The
template (x1 a x2, x3) requires further decomposition according to Case 3. This time, the
two smaller parts are the identity function (x1,x2,x3) and the constant (a). We therefore
create the following rules:
</bodyText>
<equation confidence="0.728459">
X -+ f&apos;2(A1, Y) , f2&apos; = wrap : 311 = (x1 y x2, x3) Y -+ (a)
</equation>
<bodyText confidence="0.942805333333333">
At this point, the transformation ends.
8.5.3 Correctness. We need to show that the fan-out of the binarized grammar does not
exceed the fan-out of the original grammar. We reason as follows. Starting from some
initial yield function f0 : k1 · · · km -+ k, each step of the binarization decomposes some
yield function f into two new yield functions f1,f2. Let us denote the fan-outs of the
three functions by h, h1, h2, respectively. We have
</bodyText>
<equation confidence="0.9999">
h = h1 + h2 − 1 in Case 1 and Case 2 (8)
h = h1 + h2 − 2 in Case 3 (9)
</equation>
<bodyText confidence="0.999904875">
From Equation (8) it is clear that in Case 1 and Case 2, both h1 and h2 are upper-
bounded by h. In Case 3 we have h1 &gt; 2, which together with Equation (9) implies
that h2 &lt; h. However, h1 is upper-bounded by h only if h2 &gt; 2; if h2 = 1, then h1 may
be greater than h. As an example, consider the decomposition of (x1 a x2) (fan-out 1) into
the wrapping function (x1, x2) (fan-out 2) and the constant (a) (fan-out 1). But because
in Case 3 the index j is chosen to maximize the number of component boundaries
between the variables xi,j and xi,j+1, the assumption h2 = 1 implies that each of the h1
components of f1 contains at least one variable with argument index i—if there were
</bodyText>
<page confidence="0.989526">
382
</page>
<note confidence="0.38187">
Kuhlmann Mildly Non-Projective Dependency Grammar
</note>
<bodyText confidence="0.8145615">
a component without such a variable, then the two variables that surrounded that com-
ponent would have given rise to a different choice of j. Hence we deduce that h1 ≤ ki.
</bodyText>
<sectionHeader confidence="0.866827" genericHeader="method">
9. Conclusion
</sectionHeader>
<bodyText confidence="0.999856736842105">
In this article, we have presented a formalism for non-projective dependency grammar
based on linear context-free rewriting systems, along with a technique for extracting
grammars from dependency treebanks. We have shown that parsing with the full class
of these grammars is intractable. Therefore, we have investigated two constraints on the
non-projectivity of dependency trees, block-degree and well-nestedness. Jointly, these
two constraints define a class of “mildly” non-projective dependency grammars that
can be parsed in polynomial time.
Our results in Sections 7 and 8 allow us to relate the formal power of an LCFRS
to the structural properties of the dependency structures that it induces. Although we
have used this relation to identify a class of dependency grammars that can be parsed
in polynomial time, it also provides us with a new perspective on the question about
the descriptive adequacy of a grammar formalism. This question has traditionally been
discussed on the basis of strong and weak generative capacity (Bresnan et al. 1982;
Huybregts 1984; Shieber 1985). A notion of generative capacity based on dependency
trees makes a useful addition to this discussion, in particular when comparing
formalisms for which no common concept of strong generative capacity exists. As an
example for a result in this direction, see Koller and Kuhlmann (2009).
We have defined the dependency trees that an LCFRS induces by means of a
compositional mapping on the derivations. While we would claim that compositionality
is a generally desirable property, the particular notion of induction is up for discussion.
In particular, our interpretation of derivations may not always be in line with how the
grammar producing these derivations is actually used. One formalism for which such a
mismatch between derivation trees and dependency trees has been pointed out is tree-
adjoining grammar (Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998).
Resolving this mismatch provides an interesting line of future work.
One aspect that we have not discussed here is the linguistic adequacy of block-
degree and well-nestedness. Each of our dependency grammars is restricted to a finite
block-degree. As a consequence of this restriction, our dependency grammars are not
expressive enough to capture linguistic phenomena that require unlimited degrees
of non-projectivity, such as the “scrambling” in German subordinate clauses (Becker,
Rambow, and Niv 1992). The question whether it is reasonable to assume a bound
on the block-degree of dependency trees, perhaps for some performance-based reason,
is open. Likewise, it is not clear whether well-nestedness is a “natural” constraint on
dependency analyses (Chen-Main and Joshi 2010; Maier and Lichte 2011).
Although most of the results that we have presented in this article are of a theo-
retical nature, some of them have found their way into practical systems. In particular,
the extraction technique from Section 4 is used by the data-driven dependency parser
of Maier and Kallmeyer (2010).
</bodyText>
<table confidence="0.894664285714286">
Acknowledgments References
The author gratefully acknowledges Becker, Tilman, Owen Rambow, and
financial support from The Michael Niv. 1992. The derivational
German Research Foundation generative power of formal systems,
(Sonderforschungsbereich 378, or: Scrambling is beyond LCFRS. IRCS
project MI 2) and The Swedish Research Report 92-38, University of Pennsylvania,
Council (diary no. 2008-296). Philadelphia, PA.
</table>
<page confidence="0.902482">
383
</page>
<note confidence="0.560463">
Computational Linguistics Volume 39, Number 2
</note>
<reference confidence="0.997989593220339">
Bertsch, Eberhard and Mark-Jan Nederhof.
2001. On the complexity of some
extensions of RCG parsing. In Proceedings
of the Seventh International Workshop on
Parsing Technologies (IWPT), pages 66–77,
Beijing.
Bodirsky, Manuel, Marco Kuhlmann, and
Mathias M¨ohl. 2005. Well-nested
drawings as models of syntactic structure.
In Proceedings of the 10th Conference on
Formal Grammar (FG) and Ninth Meeting
on Mathematics of Language (MOL),
pages 195–203, Edinburgh.
B¨ohmov´a, Alena, Jan Hajiˇc, Eva Hajiˇcov´a,
and Barbora Hladk´a. 2003. The Prague
Dependency Treebank: A three-level
annotation scenario. In Abeill´e, Anne,
editor. Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers,
Dordrecht, chapter 7, pages 103–127.
Boullier, Pierre. 1998. Proposal for a natural
language processing syntactic backbone.
Rapport de recherche 3342, INRIA
Rocquencourt, Paris, France.
Boullier, Pierre. 2004. Range Concatenation
Grammars. In Harry C. Bunt, John Carroll,
and Giorgio Satta, editors, New
Developments in Parsing Technology,
volume 23 of Text, Speech and Language
Technology. Kluwer Academic Publishers,
Dordrecht, pages 269–289.
Bresnan, Joan, Ronald M. Kaplan, Stanley
Peters, and Annie Zaenen. 1982.
Cross-serial dependencies in Dutch.
Linguistic Inquiry, 13(4):613–635.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of the
Tenth Conference on Computational Natural
Language Learning (CoNLL), pages 149–164,
New York, NY.
Candito, Marie-H´el`ene and Sylvain Kahane.
1998. Can the TAG derivation tree
represent a semantic graph? An answer
in the light of Meaning-Text Theory.
In Proceedings of the Fourth Workshop on
Tree Adjoining Grammars and Related
Formalisms (TAG+), pages 21–24,
Philadelphia, PA.
Charniak, Eugene. 1996. Tree-bank
grammars. In Proceedings of the
13th National Conference on Artificial
Intelligence (AAAI) and Eighth Innovative
Applications of Artificial Intelligence
Conference (IAAI), volume 2,
pages 1031–1036, Portland, OR.
Chen-Main, Joan and Aravind K. Joshi.
2010. Unavoidable ill-nestedness in
natural language and the adequacy of
tree local-MCTAG induced dependency
structures. In Proceedings of the Tenth
International Conference on Tree Adjoining
Grammars and Related Formalisms (TAG+),
New Haven, CT. Available at http://dx.
doi.org/10.1093/logcom/exs012.
Crescenzi, Pierluigi, Daniel Gildea, Andrea
Marino, Gianluca Rossi, and Giorgio Satta.
2011. Optimal head-driven parsing
complexity for linear context-free
rewriting systems. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 450–459, Portland, OR.
Dˇzeroski, Saˇso, Tomaˇz Erjavec, Nina
Ledinek, Petr Pajas, Zdenek ˇZabokrtsky,
and Andreja ˇZele. 2006. Towards a
Slovene dependency treebank. In Fifth
International Conference on Language
Resources and Evaluations (LREC),
pages 1388–1391, Genoa.
Gaifman, Haim. 1965. Dependency systems
and phrase-structure systems. Information
and Control, 8(3):304–337.
Gildea, Daniel. 2010. Optimal parsing
strategies for linear context-free rewriting
systems. In Proceedings of Human Language
Technologies: The 2010 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics (NAACL),
pages 769–776, Los Angeles, CA.
G´omez-Rodriguez, Carlos, John Carroll, and
David J. Weir. 2011. Dependency parsing
schemata and mildly non-projective
dependency parsing. Computational
Linguistics, 37(3):541–586.
G´omez-Rodriguez, Carlos, Marco
Kuhlmann, and Giorgio Satta. 2010.
Efficient parsing of well-nested linear
context-free rewriting systems. In
Proceedings of Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL),
pages 276–284, New Haven, CT.
G´omez-Rodriguez, Carlos, Marco
Kuhlmann, Giorgio Satta, and David J.
Weir. 2009. Optimal reduction of rule
length in linear context-free rewriting
systems. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics (NAACL),
pages 539–547, Boulder, CO.
G´omez-Rodriguez, Carlos and Giorgio
Satta. 2009. An optimal-time binarization
algorithm for linear context-free rewriting
systems with fan-out two. In Proceedings
of the Joint Conference of the 47th Annual
</reference>
<page confidence="0.99678">
384
</page>
<note confidence="0.56535">
Kuhlmann Mildly Non-Projective Dependency Grammar
</note>
<reference confidence="0.98714786440678">
Meeting of the Association for Computational
Linguistics (ACL) and the Fourth
International Joint Conference on Natural
Language Processing of the Asian Federation
of Natural Language Processing (IJCNLP),
pages 985–993, Singapore.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573–605.
Hajiˇc, Jan, Otakar Smrˇz, Petr Zem´anek,
Jan ˇSnaidauf, and Emanuel Beˇska. 2004.
Prague Arabic Dependency Treebank:
Development in data and tools. In
Proceedings of the International Conference on
Arabic Language Resources and Tools,
pages 110–117, Cairo.
Havelka, Jiˇri. 2007. Beyond projectivity:
Multilingual evaluation of constraints and
measures on non-projective structures.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 608–615, Prague.
Hays, David G. 1964. Dependency theory:
A formalism and some observations.
Language, 40(4):511–525.
Holan, Tom´aˇs, Vladislav Kuboˇn, Karel Oliva,
and Martin Pl´atek. 1998. Two useful
measures of word order complexity.
In Proceedings of the Workshop on
Processing of Dependency-Based Grammars,
pages 21–29, Montr´eal.
Hudson, Richard. 2007. Language Networks.
The New Word Grammar. Oxford University
Press, Oxford.
Huybregts, Riny. 1984. The weak inadequacy
of context-free phrase structure grammars.
In Ger de Haan, Mieke Trommelen, and
Wim Zonneveld, editors, Van periferie naar
kern. Foris, Dordrecht, pages 81–99.
Joshi, Aravind K. and Yves Schabes.1997.
Tree-Adjoining Grammars. In Grzegorz
Rozenberg and Arto Salomaa, editors,
Handbook of Formal Languages, volume 3.
Springer, Berlin, pages 69–123.
Kaji, Yuichi, Ryuichi Nakanishi, Hiroyuki
Seki, and Tadao Kasami.1992. The
universal recognition problems for
multiple context-free grammars and for
linear context-free rewriting systems.
IEICE Transactions on Information and
Systems, E75-D(1):78–88.
Kallmeyer, Laura. 2010. Parsing Beyond
Context-Free Grammars. Springer, Berlin.
Kanazawa, Makoto. 2009. The pumping
lemma for well-nested multiple
context-free languages. In Developments
in Language Theory. Proceedings of the
13th International Conference, DLT 2009,
volume 5583 of Lecture Notes in Computer
Science, pages 312–325, Stuttgart.
Kanazawa, Makoto and Sylvain Salvati.
2010. The copying power of well-nested
multiple context-free grammars.
In Adrian-Horia Dediu, Henning Fernau,
and Carlos Martin-Vide, editors, Language
and Automata Theory and Applications.
Proceedings of the 4th International
Conference, LATA 2010, volume 6031
of Lecture Notes in Computer Science,
pages 344–355, Trier.
Koller, Alexander and Marco Kuhlmann.
2009. Dependency trees and the
strong generative capacity of CCG. In
Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 460–468, Athens.
Kracht, Marcus. 2003. The Mathematics
of Language, volume 63 of Studies in
Generative Grammar. Mouton de
Gruyter, Paris.
Kromann, Matthias Trautner. 2003. The
Danish Dependency Treebank and
the underlying linguistic theory. In
Proceedings of the Second Workshop on
Treebanks and Linguistic Theories (TLT),
pages 217–220, V¨axj¨o.
K¨ubler, Sandra, Ryan McDonald, and
Joakim Nivre. 2009. Dependency
Parsing. Synthesis Lectures on Human
Language Technologies. Morgan and
Claypool.
Kuhlmann, Marco and Joakim Nivre.
2006. Mildly non-projective dependency
structures. In Proceedings of the
21st International Conference on
Computational Linguistics (COLING) and
44th Annual Meeting of the Association
for Computational Linguistics (ACL)
Main Conference Poster Sessions,
pages 507–514, Sydney.
Kuhlmann, Marco and Giorgio Satta.
2009. Treebank grammar techniques for
non-projective dependency parsing. In
Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 478–486, Athens.
Lang, Bernard. 1994. Recognition can be
harder than parsing. Computational
Intelligence, 10(4):486–494.
Li, Zhifei and Jason Eisner. 2009.
First- and second-order expectation
semirings with applications to
minimum-risk training on translation
forests. In Proceedings of the 2009
Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 40–51, Singapore.
</reference>
<page confidence="0.967803">
385
</page>
<reference confidence="0.994893428571429">
Computational Linguistics Volume 39, Number 2
Maier, Wolfgang and Laura Kallmeyer. 2010.
Discontinuity and non-projectivity: Using
mildly context-sensitive formalisms for
data-driven parsing. In Proceedings of the
Tenth International Conference on Tree
Adjoining Grammars and Related Formalisms
(TAG+), New Haven, CT.
Maier, Wolfgang and Timm Lichte. 2011.
Characterizing discontinuity in constituent
treebanks. In Philippe de Groote, Markus
Egg, and Laura Kallmeyer, editors,
Formal Grammar. Proceedings of the 14th
International Conference, FG 2009, Revised
Selected Papers, volume 5591 of Lecture
Notes in Computer Science, pages 167–182,
Bordeaux.
Maier, Wolfgang and Anders Søgaard. 2008.
Treebanks and mild context-sensitivity.
In Proceedings of the 13th Conference on
Formal Grammar (FG), pages 61–76,
Hamburg.
McAllester, David. 2002. On the complexity
analysis of static analyses. Journal of the
Association for Computing Machinery,
49(4):512–537.
Mel’ˇcuk, Igor.1988. Dependency Syntax:
Theory and Practice. State University
of New York Press, Albany, NY.
Michaelis, Jens. 1998. Derivational
minimalism is mildly context-sensitive.
In Logical Aspects of Computational
Linguistics, Third International Conference,
LACL 1998, Selected Papers, volume 2014
of Lecture Notes in Computer Science,
pages 179–198, Grenoble.
Michaelis, Jens. 2001. On Formal Properties
of Minimalist Grammars. Ph.D. thesis,
Universit¨at Potsdam, Potsdam,
Germany.
Nivre, Joakim, Johan Hall, Sandra K¨ubler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The CoNLL
2007 shared task on dependency parsing.
In Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing (EMNLP) and Computational
Natural Language Learning (CoNLL),
pages 915–932, Prague.
Oflazer, Kemal, Bilge Say, Dilek Zeynep
Hakkani-T¨ur, and G¨okhan T¨ur. 2003.
Building a Turkish treebank. In Abeill´e,
Anne, editor. Treebanks: Building and
Using Parsed Corpora. Kluwer Academic
Publishers, Dordrecht, chapter 15,
pages 261–277.
Rambow, Owen and Aravind K. Joshi.
1997. A formal look at dependency
grammars and phrase-structure
grammars, with special consideration
of word-order phenomena. In Leo Wanner,
editor, Recent Trends in Meaning-Text
Theory, volume 39 of Studies in Language,
Companion Series. John Benjamins,
Amsterdam, pages 167–190.
Rambow, Owen, K. Vijay-Shanker, and
David J. Weir. 1995. D-Tree grammars.
In Proceedings of the 33rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 151–158,
Cambridge, MA.
Sagot, Benoit and Giorgio Satta. 2010.
Optimal rank reduction for linear
context-free rewriting systems with
fan-out two. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 525–533, Uppsala.
Satta, Giorgio. 1992. Recognition of
linear context-free rewriting systems.
In Proceedings of the 30th Annual
Meeting of the Association for
Computational Linguistics (ACL),
pages 89–95, Newark, DE.
Schabes, Yves. 1990. Mathematical and
Computational Aspects of Lexicalized
Grammars. Ph.D. thesis, University
of Pennsylvania, Philadelphia, PA.
Schabes, Yves, Anne Abeill´e, and
Aravind K. Joshi.1988. Parsing
strategies with ‘lexicalized’ grammars:
Application to tree adjoining grammars.
In Proceedings of the Twelfth International
Conference on Computational Linguistics
(COLING), pages 578–583, Budapest.
Seki, Hiroyuki, Takashi Matsumura,
Mamoru Fujii, and Tadao Kasami.
1991. On Multiple Context-Free
Grammars. Theoretical Computer
Science, 88(2):191–229.
Sgall, Petr, Eva Hajiˇcov´a, and Jarmila
Panevov´a. 1986. The Meaning of the
Sentence in Its Semantic and Pragmatic
Aspects. Springer, Berlin.
Shieber, Stuart M. 1985. Evidence against
the context-freeness of natural language.
Linguistics and Philosophy, 8(3):333–343.
Shieber, Stuart M., Yves Schabes, and
Fernando Pereira. 1995. Principles
and implementation of deductive
parsing. Journal of Logic Programming,
24(1–2):3–36.
Steedman, Mark and Jason Baldridge.
2011. Combinatory categorial grammar.
In Robert D. Borsley and Kersti B¨orjars,
editors, Non-Transformational Syntax:
Formal and Explicit Models of Grammar.
Wiley-Oxford, Blackwell, chapter 5,
pages 181–224.
</reference>
<page confidence="0.984544">
386
</page>
<note confidence="0.207868">
Kuhlmann Mildly Non-Projective Dependency Grammar
</note>
<reference confidence="0.99945595">
Tesni`ere, Lucien. 1959. ´El´ements de syntaxe
structurale. Klinksieck, Paris.
Vijay-Shanker, K., David J. Weir, and
Aravind K. Joshi. 1987. Characterizing
structural descriptions produced by
various grammatical formalisms.
In Proceedings of the 25th Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 104–111,
Stanford, CA.
Villemonte de la Clergerie, ´Eric. 2002.
Parsing mildly context-sensitive
languages with thread automata.
In Proceedings of the 19th International
Conference on Computational Linguistics
(COLING), pages 1–7, Taipei.
Weir, David J. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
</reference>
<page confidence="0.998339">
387
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.579732">
<title confidence="0.648543">Mildly Non-Projective Dependency Grammar</title>
<affiliation confidence="0.998817">Uppsala University</affiliation>
<abstract confidence="0.992216928571429">Syntactic representations based on word-to-word dependencies have a long-standing tradition in descriptive linguistics, and receive considerable interest in many applications. Nevertheless, dependency syntax has remained something of an island from a formal point of view. Moreover, most formalisms available for dependency grammar are restricted to projective analyses, and thus not able to support natural accounts of phenomena such as wh-movement and cross–serial dependencies. In this article we present a formalism for non-projective dependency grammar in the framework of linear context-free rewriting systems. A characteristic property of our formalism is a close correspondence between the non-projectivity of the dependency trees admitted by a grammar on the one hand, and the parsing complexity of the grammar on the other. We show that parsing with unrestricted grammars is intractable. We therefore study two constraints on non-projectivity, block-degree and well-nestedness. Jointly, these two constraints define a class of “mildly” non-projective dependency grammars that can be parsed in polynomial time. An evaluation on five dependency treebanks shows that these grammars have a good coverage of empirical data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eberhard Bertsch</author>
<author>Mark-Jan Nederhof</author>
</authors>
<title>On the complexity of some extensions of RCG parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>66--77</pages>
<location>Beijing.</location>
<contexts>
<context position="36192" citStr="Bertsch and Nederhof 2001" startWordPosition="6311" endWordPosition="6314">Recognition Lexicalized linear context-free rewriting systems are able to account for arbitrarily nonprojective dependency trees. This expressiveness comes with a price: In this section we show that parsing with lexicalized LCFRSs is intractable, unless we are willing to restrict the class of grammars. 6.1 Parsing Algorithm To ground our discussion of parsing complexity, we present a simple bottom–up parsing algorithm for LCFRSs, specified as a grammatical deduction system (Shieber, Schabes, and Pereira 1995). Several similar algorithms have been described in the literature (Seki et al. 1991; Bertsch and Nederhof 2001; Kallmeyer 2010). We assume that we are given a grammar G = (N, Σ, P, S) and a string w = a1 · · · an E V∗ to be parsed. Item form. The items of the deduction system take the form [A, l1, r1, ... , lk, rk] where A E N with y(A) = k, and the remaining components are indices identifying the left and right endpoints of pairwise non-overlapping substrings of w. More formally, 0 &lt; lh &lt; rh &lt; n, and for all h, h&apos; with h =� h&apos;, either rh &lt; lh&apos; or rh, &lt; lh. The intended interpretation of an item of this form is that A derives a term t E T(G) that yields the specified substrings of w, that is, A �∗G t </context>
</contexts>
<marker>Bertsch, Nederhof, 2001</marker>
<rawString>Bertsch, Eberhard and Mark-Jan Nederhof. 2001. On the complexity of some extensions of RCG parsing. In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT), pages 66–77, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Bodirsky</author>
<author>Marco Kuhlmann</author>
<author>Mathias M¨ohl</author>
</authors>
<title>Well-nested drawings as models of syntactic structure.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Conference on Formal Grammar (FG) and Ninth Meeting on Mathematics of Language (MOL),</booktitle>
<pages>195--203</pages>
<location>Edinburgh.</location>
<marker>Bodirsky, Kuhlmann, M¨ohl, 2005</marker>
<rawString>Bodirsky, Manuel, Marco Kuhlmann, and Mathias M¨ohl. 2005. Well-nested drawings as models of syntactic structure. In Proceedings of the 10th Conference on Formal Grammar (FG) and Ninth Meeting on Mathematics of Language (MOL), pages 195–203, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena B¨ohmov´a</author>
<author>Jan Hajiˇc</author>
<author>Eva Hajiˇcov´a</author>
<author>Barbora Hladk´a</author>
</authors>
<title>The Prague Dependency Treebank: A three-level annotation scenario.</title>
<date>2003</date>
<pages>103--127</pages>
<editor>In Abeill´e, Anne, editor.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht,</location>
<note>chapter 7,</note>
<marker>B¨ohmov´a, Hajiˇc, Hajiˇcov´a, Hladk´a, 2003</marker>
<rawString>B¨ohmov´a, Alena, Jan Hajiˇc, Eva Hajiˇcov´a, and Barbora Hladk´a. 2003. The Prague Dependency Treebank: A three-level annotation scenario. In Abeill´e, Anne, editor. Treebanks: Building and Using Parsed Corpora. Kluwer Academic Publishers, Dordrecht, chapter 7, pages 103–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
</authors>
<title>Proposal for a natural language processing syntactic backbone. Rapport de recherche 3342, INRIA Rocquencourt,</title>
<date>1998</date>
<location>Paris, France.</location>
<contexts>
<context position="28358" citStr="Boullier 1998" startWordPosition="4932" endWordPosition="4933">eld function that violates the property is (x1,2 x1,1), which defines a kind of swapping operation. In the literature on LCFRSs and related formalisms, yield functions with Property 2 have been called monotone (Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer 2010), and non-permuting (Kanazawa 2009). Property 3 No component o h is the empty string. This property, which is similar to ε-freeness as known from context-free grammars, has been discussed for multiple context-free grammars (Seki et al. 1991, Property N3 in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1). For our extracted grammars it holds because each component o h represents a block, and blocks are always non-empty. Property 4 No component o h contains a substring of the form xi,j1xi,j2. This property, which does not seem to have been discussed in the literature before, is a reflection of the facts that variables with the same argument index represent blocks of the same child node, and that these blocks are longest segments of descendants. A yield function with Properties 1–4 is called canonical. An LCFRS is canonical if all of its yield functions are canonical. Lemma 1 A lex</context>
</contexts>
<marker>Boullier, 1998</marker>
<rawString>Boullier, Pierre. 1998. Proposal for a natural language processing syntactic backbone. Rapport de recherche 3342, INRIA Rocquencourt, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
</authors>
<title>Range Concatenation Grammars. In</title>
<date>2004</date>
<booktitle>New Developments in Parsing Technology,</booktitle>
<volume>23</volume>
<pages>269--289</pages>
<editor>Harry C. Bunt, John Carroll, and Giorgio Satta, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="5606" citStr="Boullier 2004" startWordPosition="805" endWordPosition="806">owards the dependent (away from the root node). Following Hays (1964), we use dotted lines to help us keep track of the positions of the nodes in the linear order, and to associate nodes with lexical items. 356 Kuhlmann Mildly Non-Projective Dependency Grammar grammar formalisms, including standard context-free grammar, tree-adjoining grammar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman and Baldridge 2011). It also comprises, among others, multiple context-free grammars (Seki et al. 1991), minimalist grammars (Michaelis 1998), and simple range concatenation grammars (Boullier 2004). The article is structured as follows. In Section 2 we provide the technical background to our work; in particular, we introduce our terminology and notation for linear context-free rewriting systems. An LCFRS generates a set of terms (formal expressions) which are interpreted as derivation trees of objects from some domain. Each term also has a secondary interpretation under which it denotes a tuple of strings, representing the string yield of the derived object. In Section 3 we introduce the central notion of a lexicalized linear context-free rewriting system, which is an LCFRS in which eac</context>
</contexts>
<marker>Boullier, 2004</marker>
<rawString>Boullier, Pierre. 2004. Range Concatenation Grammars. In Harry C. Bunt, John Carroll, and Giorgio Satta, editors, New Developments in Parsing Technology, volume 23 of Text, Speech and Language Technology. Kluwer Academic Publishers, Dordrecht, pages 269–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Ronald M Kaplan</author>
<author>Stanley Peters</author>
<author>Annie Zaenen</author>
</authors>
<date>1982</date>
<booktitle>Cross-serial dependencies in Dutch. Linguistic Inquiry,</booktitle>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="70248" citStr="Bresnan et al. 1982" startWordPosition="12435" endWordPosition="12438">constraints define a class of “mildly” non-projective dependency grammars that can be parsed in polynomial time. Our results in Sections 7 and 8 allow us to relate the formal power of an LCFRS to the structural properties of the dependency structures that it induces. Although we have used this relation to identify a class of dependency grammars that can be parsed in polynomial time, it also provides us with a new perspective on the question about the descriptive adequacy of a grammar formalism. This question has traditionally been discussed on the basis of strong and weak generative capacity (Bresnan et al. 1982; Huybregts 1984; Shieber 1985). A notion of generative capacity based on dependency trees makes a useful addition to this discussion, in particular when comparing formalisms for which no common concept of strong generative capacity exists. As an example for a result in this direction, see Koller and Kuhlmann (2009). We have defined the dependency trees that an LCFRS induces by means of a compositional mapping on the derivations. While we would claim that compositionality is a generally desirable property, the particular notion of induction is up for discussion. In particular, our interpretati</context>
</contexts>
<marker>Bresnan, Kaplan, Peters, Zaenen, 1982</marker>
<rawString>Bresnan, Joan, Ronald M. Kaplan, Stanley Peters, and Annie Zaenen. 1982. Cross-serial dependencies in Dutch. Linguistic Inquiry, 13(4):613–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>149--164</pages>
<location>New York, NY.</location>
<contexts>
<context position="2112" citStr="Buchholz and Marsi 2006" startWordPosition="287" endWordPosition="290"> have become the basis for several linguistic theories, such as Functional Generative Description (Sgall, Hajiˇcov´a, and Panevov´a 1986), Meaning–Text Theory (Mel’ˇcuk 1988), and Word Grammar (Hudson 2007). In recent years they have also been used for a wide range of practical applications, such as information extraction, machine translation, and question answering. We ascribe the widespread interest in dependency structures to their intuitive appeal, their conceptual simplicity, and in particular to the availability of accurate and efficient dependency parsers for a wide range of languages (Buchholz and Marsi 2006; Nivre et al. 2007). Although there exist both a considerable practical interest and an extensive linguistic literature, dependency syntax has remained something of an island from a formal point of view. In particular, there are relatively few results that bridge between dependency syntax and other traditions, such as phrase structure or categorial syntax. ∗ Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden. E-mail: marco.kuhlmann@lingfil.uu.se. Submission received: 17 December 2009; revised submission received: 3 April 2012; accepted for publication: 24 May 2012. doi:10</context>
<context position="52613" citStr="Buchholz and Marsi 2006" startWordPosition="9262" endWordPosition="9265">ical lexicalized LCFRSs. Example 7 The following term induces a two-node dependency tree with block-degree 1, but contains yield functions with fan-out 2: (a x1 x2)((b, ε)). Note that the yield functions in this term violate both Property 3 and Property 4. 7.4 Coverage on Dependency Treebanks In order to assess the consequences of different bounds on the fan-out, we now evaluate the block-degree of dependency trees in real-world data. Specifically, we look into five 375 Computational Linguistics Volume 39, Number 2 dependency treebanks used in the 2006 CoNLL shared task on dependency parsing (Buchholz and Marsi 2006): the Prague Arabic Dependency Treebank (Hajiˇc et al. 2004), the Prague Dependency Treebank of Czech (B¨ohmov´a et al. 2003), the Danish Dependency Treebank (Kromann 2003), the Slovene Dependency Treebank (Dˇzeroski et al. 2006), and the Metu-Sabancı treebank of Turkish (Oflazer et al. 2003). The full data used in the CoNLL shared task also included treebanks that were produced by conversion of corpora originally annotated with structures other than dependencies, which is a potential source of “noise” that one has to take into account when interpreting any findings. Here, we consider only gen</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL), pages 149–164, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-H´el`ene Candito</author>
<author>Sylvain Kahane</author>
</authors>
<title>Can the TAG derivation tree represent a semantic graph? An answer in the light of Meaning-Text Theory.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fourth Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+),</booktitle>
<pages>21--24</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="71154" citStr="Candito and Kahane 1998" startWordPosition="12575" endWordPosition="12578">ion, see Koller and Kuhlmann (2009). We have defined the dependency trees that an LCFRS induces by means of a compositional mapping on the derivations. While we would claim that compositionality is a generally desirable property, the particular notion of induction is up for discussion. In particular, our interpretation of derivations may not always be in line with how the grammar producing these derivations is actually used. One formalism for which such a mismatch between derivation trees and dependency trees has been pointed out is treeadjoining grammar (Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998). Resolving this mismatch provides an interesting line of future work. One aspect that we have not discussed here is the linguistic adequacy of blockdegree and well-nestedness. Each of our dependency grammars is restricted to a finite block-degree. As a consequence of this restriction, our dependency grammars are not expressive enough to capture linguistic phenomena that require unlimited degrees of non-projectivity, such as the “scrambling” in German subordinate clauses (Becker, Rambow, and Niv 1992). The question whether it is reasonable to assume a bound on the block-degree of dependency tr</context>
</contexts>
<marker>Candito, Kahane, 1998</marker>
<rawString>Candito, Marie-H´el`ene and Sylvain Kahane. 1998. Can the TAG derivation tree represent a semantic graph? An answer in the light of Meaning-Text Theory. In Proceedings of the Fourth Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+), pages 21–24, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the 13th National Conference on Artificial Intelligence (AAAI) and Eighth Innovative Applications of Artificial Intelligence Conference (IAAI),</booktitle>
<volume>2</volume>
<contexts>
<context position="6779" citStr="Charniak 1996" startWordPosition="992" endWordPosition="993">ng system, which is an LCFRS in which each rule of the grammar is associated with an overt lexical item, representing a syntactic head (cf. Schabes, Abeill´e, and Joshi 1988 and Schabes 1990). We show that this property gives rise to an additional interpretation under which each term denotes a dependency tree on its yield. With this interpretation, lexicalized LCFRSs can be used as dependency grammars. In Section 4 we show how to acquire lexicalized LCFRSs from dependency treebanks. This works in much the same way as the extraction of context-free grammars from phrase structure treebanks (cf. Charniak 1996), except that the derivation trees of dependency trees are not immediately accessible in the treebank. We therefore present an efficient algorithm for computing a canonical derivation tree for an input dependency tree; from this derivation tree, the rules of the grammar can be extracted in a straightforward way. The algorithm was originally published by Kuhlmann and Satta (2009). It produces a restricted type of lexicalized LCFRS that we call “canonical.” In Section 5 we provide a declarative characterization of this class of grammars, and show that every lexicalized LCFRS is (strongly) equiva</context>
<context position="18653" citStr="Charniak 1996" startWordPosition="3130" endWordPosition="3131">es, we obtain the intermediate dependency graph G = (V, E&apos;) for w, where E&apos; = {(1,1) -+ (2,3), (2,1) -+ (2,2)} The occurrence r of the anchor b off in w� is (1, 2); the nodes of G that correspond to the root nodes of D1 and D2 are ¯r1 = (1, 1) and ¯r2 = (2, 1). The dependency tree D is obtained by adding the edges r -+ ¯r1 and r -+ ¯r2 to G. 4. Extraction of Dependency Grammars We now show how to extract lexicalized linear context-free rewriting systems from dependency treebanks. To this end, we adapt the standard technique for extracting context-free grammars from phrase structure treebanks (Charniak 1996). Our technique was originally published by Kuhlmann and Satta (2009). In recent work, Maier and Lichte (2011) have shown how to unify it with a similar technique for the extraction of range concatenation grammars from discontinuous constituent structures, due to Maier and Søgaard (2008). To simplify our presentation we restrict our attention to treebanks containing simple dependency trees. 362 Kuhlmann Mildly Non-Projective Dependency Grammar Figure 7 A dependency tree and one of its construction trees. To extract a lexicalized LCFRS from a dependency treebank we proceed as follows. First, fo</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Charniak, Eugene. 1996. Tree-bank grammars. In Proceedings of the 13th National Conference on Artificial Intelligence (AAAI) and Eighth Innovative Applications of Artificial Intelligence Conference (IAAI), volume 2,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Chen-Main</author>
<author>Aravind K Joshi</author>
</authors>
<title>Unavoidable ill-nestedness in natural language and the adequacy of tree local-MCTAG induced dependency structures.</title>
<date>2010</date>
<booktitle>In Proceedings of the Tenth International Conference on Tree Adjoining Grammars and Related</booktitle>
<pages>pages</pages>
<note>Available at http://dx. doi.org/10.1093/logcom/exs012.</note>
<contexts>
<context position="71935" citStr="Chen-Main and Joshi 2010" startWordPosition="12690" endWordPosition="12693">nd well-nestedness. Each of our dependency grammars is restricted to a finite block-degree. As a consequence of this restriction, our dependency grammars are not expressive enough to capture linguistic phenomena that require unlimited degrees of non-projectivity, such as the “scrambling” in German subordinate clauses (Becker, Rambow, and Niv 1992). The question whether it is reasonable to assume a bound on the block-degree of dependency trees, perhaps for some performance-based reason, is open. Likewise, it is not clear whether well-nestedness is a “natural” constraint on dependency analyses (Chen-Main and Joshi 2010; Maier and Lichte 2011). Although most of the results that we have presented in this article are of a theoretical nature, some of them have found their way into practical systems. In particular, the extraction technique from Section 4 is used by the data-driven dependency parser of Maier and Kallmeyer (2010). Acknowledgments References The author gratefully acknowledges Becker, Tilman, Owen Rambow, and financial support from The Michael Niv. 1992. The derivational German Research Foundation generative power of formal systems, (Sonderforschungsbereich 378, or: Scrambling is beyond LCFRS. IRCS </context>
</contexts>
<marker>Chen-Main, Joshi, 2010</marker>
<rawString>pages 1031–1036, Portland, OR. Chen-Main, Joan and Aravind K. Joshi. 2010. Unavoidable ill-nestedness in natural language and the adequacy of tree local-MCTAG induced dependency structures. In Proceedings of the Tenth International Conference on Tree Adjoining Grammars and Related Formalisms (TAG+), New Haven, CT. Available at http://dx. doi.org/10.1093/logcom/exs012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierluigi Crescenzi</author>
<author>Daniel Gildea</author>
<author>Andrea Marino</author>
<author>Gianluca Rossi</author>
<author>Giorgio Satta</author>
</authors>
<title>Optimal head-driven parsing complexity for linear context-free rewriting systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>450--459</pages>
<location>Portland, OR.</location>
<contexts>
<context position="8233" citStr="Crescenzi et al. 2011" startWordPosition="1224" endWordPosition="1227">e, the degree of the polynomial depends on two grammar-specific measures called fan-out and rank. We show that even in the restricted case of canonical grammars, parsing is an NPhard problem. It is important therefore to keep the fan-out and the rank of a grammar as low as possible, and much of the recent work on LCFRSs has been devoted to the development of techniques that optimize parsing complexity in various scenarios G´omez-Rodr´ıguez and Satta 2009; G´omez-Rodr´ıguez et al. 2009; Kuhlmann and Satta 2009; Gildea 2010; G´omez-Rodr´ıguez, Kuhlmann, and Satta 2010; Sagot and Satta 2010; and Crescenzi et al. 2011). In this article we explore the impact of non-projectivity on parsing complexity. In Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS, a measure called block-degree (or gap-degree) (Holan et al. 1998). Although there is no theoretical upper bound on the block-degree of the dependency trees needed for linguistic analysis, we provide evidence from several dependency treebanks showing that, from a practical point of view, this upper bound can be put at a value of as low as 2. In Section 8 we study a second constraint on non-projectivity called well-nestedne</context>
</contexts>
<marker>Crescenzi, Gildea, Marino, Rossi, Satta, 2011</marker>
<rawString>Crescenzi, Pierluigi, Daniel Gildea, Andrea Marino, Gianluca Rossi, and Giorgio Satta. 2011. Optimal head-driven parsing complexity for linear context-free rewriting systems. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 450–459, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇso Dˇzeroski</author>
</authors>
<title>Tomaˇz Erjavec, Nina Ledinek, Petr Pajas, Zdenek ˇZabokrtsky, and Andreja ˇZele.</title>
<date>2006</date>
<booktitle>In Fifth International Conference on Language Resources and Evaluations (LREC),</booktitle>
<pages>1388--1391</pages>
<location>Genoa.</location>
<marker>Dˇzeroski, 2006</marker>
<rawString>Dˇzeroski, Saˇso, Tomaˇz Erjavec, Nina Ledinek, Petr Pajas, Zdenek ˇZabokrtsky, and Andreja ˇZele. 2006. Towards a Slovene dependency treebank. In Fifth International Conference on Language Resources and Evaluations (LREC), pages 1388–1391, Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haim Gaifman</author>
</authors>
<title>Dependency systems and phrase-structure systems.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="3235" citStr="Gaifman (1965)" startWordPosition="450" endWordPosition="451">9; revised submission received: 3 April 2012; accepted for publication: 24 May 2012. doi:10.1162/COLI a 00125 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 2 Figure 1 Nested dependencies and cross–serial dependencies. This makes it hard to gauge the similarities and differences between the paradigms, and hampers the exchange of linguistic resources and computational methods. An overarching goal of this article is to bring dependency grammar closer to the mainland of formal study. One of the few bridging results for dependency grammar is thanks to Gaifman (1965), who studied a formalism that we will refer to as Hays–Gaifman grammar, and proved it to be weakly equivalent to context-free phrase structure grammar. Although this result is of fundamental importance from a theoretical point of view, its practical usefulness is limited. In particular, Hays–Gaifman grammar is restricted to projective dependency structures, which is similar to the familiar restriction to contiguous constituents. Yet, non-projective dependencies naturally arise in the analysis of natural language. One classic example of this is the phenomenon of cross–serial dependencies in Du</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Gaifman, Haim. 1965. Dependency systems and phrase-structure systems. Information and Control, 8(3):304–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Optimal parsing strategies for linear context-free rewriting systems.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>769--776</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="8138" citStr="Gildea 2010" startWordPosition="1212" endWordPosition="1213">s. Although the runtime of this algorithm is polynomial in the length of the sentence, the degree of the polynomial depends on two grammar-specific measures called fan-out and rank. We show that even in the restricted case of canonical grammars, parsing is an NPhard problem. It is important therefore to keep the fan-out and the rank of a grammar as low as possible, and much of the recent work on LCFRSs has been devoted to the development of techniques that optimize parsing complexity in various scenarios G´omez-Rodr´ıguez and Satta 2009; G´omez-Rodr´ıguez et al. 2009; Kuhlmann and Satta 2009; Gildea 2010; G´omez-Rodr´ıguez, Kuhlmann, and Satta 2010; Sagot and Satta 2010; and Crescenzi et al. 2011). In this article we explore the impact of non-projectivity on parsing complexity. In Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS, a measure called block-degree (or gap-degree) (Holan et al. 1998). Although there is no theoretical upper bound on the block-degree of the dependency trees needed for linguistic analysis, we provide evidence from several dependency treebanks showing that, from a practical point of view, this upper bound can be put at a value of </context>
<context position="38994" citStr="Gildea 2010" startWordPosition="6866" endWordPosition="6867">ny tasks in syntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009). 6.2 Parsing Complexity We are interested in an upper bound on the runtime of the tabular parser that we have just presented. We can see that the parser runs in time O(|G||w|c), where |G |denotes the size of some suitable representation of the grammar G, and c denotes the maximal number of instantiations of an inference rule (cf. McAllester 2002). Let us write c(f ) for the specialization of c to inference rules for productions with yield function f. We refer to this value as the parsing complexity off (cf. Gildea 2010). Then to show an upper bound on c it suffices to show an upper bound on the parsing complexities of the yield functions that the parser has to handle. An obvious such upper bound is c(f ) &lt; 2k + ~m 2ki i=1 Here we imagine that we could choose each endpoint in Equation (3) independently of all the others. By virtue of the constraints, however, some of the endpoints cannot be chosen freely; in particular, some of the substrings may be adjacent. In general, to show (3) 370 Kuhlmann Mildly Non-Projective Dependency Grammar an upper bound c(f ) &lt; b we specify a strategy for choosing b endpoints, a</context>
</contexts>
<marker>Gildea, 2010</marker>
<rawString>Gildea, Daniel. 2010. Optimal parsing strategies for linear context-free rewriting systems. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 769–776, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodriguez</author>
<author>John Carroll</author>
<author>David J Weir</author>
</authors>
<title>Dependency parsing schemata and mildly non-projective dependency parsing.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>3</issue>
<marker>G´omez-Rodriguez, Carroll, Weir, 2011</marker>
<rawString>G´omez-Rodriguez, Carlos, John Carroll, and David J. Weir. 2011. Dependency parsing schemata and mildly non-projective dependency parsing. Computational Linguistics, 37(3):541–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodriguez</author>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing of well-nested linear context-free rewriting systems.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>276--284</pages>
<location>New Haven, CT.</location>
<marker>G´omez-Rodriguez, Kuhlmann, Satta, 2010</marker>
<rawString>G´omez-Rodriguez, Carlos, Marco Kuhlmann, and Giorgio Satta. 2010. Efficient parsing of well-nested linear context-free rewriting systems. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 276–284, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodriguez</author>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
<author>David J Weir</author>
</authors>
<title>Optimal reduction of rule length in linear context-free rewriting systems.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>539--547</pages>
<location>Boulder, CO.</location>
<marker>G´omez-Rodriguez, Kuhlmann, Satta, Weir, 2009</marker>
<rawString>G´omez-Rodriguez, Carlos, Marco Kuhlmann, Giorgio Satta, and David J. Weir. 2009. Optimal reduction of rule length in linear context-free rewriting systems. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 539–547, Boulder, CO.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Carlos G´omez-Rodriguez</author>
<author>Giorgio Satta</author>
</authors>
<title>An optimal-time binarization algorithm for linear context-free rewriting systems with fan-out two.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the Fourth International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (IJCNLP),</booktitle>
<pages>985--993</pages>
<marker>G´omez-Rodriguez, Satta, 2009</marker>
<rawString>G´omez-Rodriguez, Carlos and Giorgio Satta. 2009. An optimal-time binarization algorithm for linear context-free rewriting systems with fan-out two. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the Fourth International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (IJCNLP), pages 985–993, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="38447" citStr="Goodman 1999" startWordPosition="6770" endWordPosition="6771">ng to the premises of the inference rule can be combined into the substrings corresponding to the conclusion by means of the yield function f. Based on the deduction system, a tabular parser for LCFRSs can be implemented using standard dynamic programming techniques. This parser will compute a packed representation of the set of all derivation trees that the grammar G assigns to the string w. Such a packed representation is often called a shared forest (Lang 1994). In combination with appropriate semirings, the shared forest is useful for many tasks in syntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009). 6.2 Parsing Complexity We are interested in an upper bound on the runtime of the tabular parser that we have just presented. We can see that the parser runs in time O(|G||w|c), where |G |denotes the size of some suitable representation of the grammar G, and c denotes the maximal number of instantiations of an inference rule (cf. McAllester 2002). Let us write c(f ) for the specialization of c to inference rules for productions with yield function f. We refer to this value as the parsing complexity off (cf. Gildea 2010). Then to show an upper bound on c it suffices to sho</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Goodman, Joshua. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Otakar Smrˇz</author>
<author>Petr Zem´anek</author>
<author>Jan ˇSnaidauf</author>
<author>Emanuel Beˇska</author>
</authors>
<title>Prague Arabic Dependency Treebank: Development in data and tools.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Arabic Language Resources and Tools,</booktitle>
<pages>110--117</pages>
<location>Cairo.</location>
<marker>Hajiˇc, Smrˇz, Zem´anek, ˇSnaidauf, Beˇska, 2004</marker>
<rawString>Hajiˇc, Jan, Otakar Smrˇz, Petr Zem´anek, Jan ˇSnaidauf, and Emanuel Beˇska. 2004. Prague Arabic Dependency Treebank: Development in data and tools. In Proceedings of the International Conference on Arabic Language Resources and Tools, pages 110–117, Cairo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiˇri Havelka</author>
</authors>
<title>Beyond projectivity: Multilingual evaluation of constraints and measures on non-projective structures.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>608--615</pages>
<location>Prague.</location>
<contexts>
<context position="53438" citStr="Havelka (2007)" startWordPosition="9391" endWordPosition="9392">roski et al. 2006), and the Metu-Sabancı treebank of Turkish (Oflazer et al. 2003). The full data used in the CoNLL shared task also included treebanks that were produced by conversion of corpora originally annotated with structures other than dependencies, which is a potential source of “noise” that one has to take into account when interpreting any findings. Here, we consider only genuine dependency treebanks. More specifically, our statistics concern the training sections of the treebanks that were set off for the task. For similar results on other data sets, see Kuhlmann and Nivre (2006), Havelka (2007), and Maier and Lichte (2011). Our results are given in Table 3. For each treebank, we list the number of rules extracted from that treebank, as well as the number of corresponding dependency trees. We then list the number of rules that we lose if we restrict ourselves to rules with fanout = 1, or rules with fan-out &lt; 2, as well as the number of dependency trees that we lose because their construction trees contain at least one such rule. We count rule tokens, meaning that two otherwise identical rules are counted twice if they were extracted from different trees, or from different nodes in th</context>
</contexts>
<marker>Havelka, 2007</marker>
<rawString>Havelka, Jiˇri. 2007. Beyond projectivity: Multilingual evaluation of constraints and measures on non-projective structures. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 608–615, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Hays</author>
</authors>
<title>Dependency theory: A formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<volume>40</volume>
<issue>4</issue>
<contexts>
<context position="5061" citStr="Hays (1964)" startWordPosition="729" endWordPosition="730">h other, whereas the Dutch linearization induces a non-projective structure with crossing edges. To account for such structures we need to turn to formalisms more expressive than Hays–Gaifman grammars. In this article we present a formalism for non-projective dependency grammar based on linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987; Weir 1988). This framework was introduced to facilitate the comparison of various 1 We draw the nodes of a dependency tree as circles, and the edges as arrows pointing towards the dependent (away from the root node). Following Hays (1964), we use dotted lines to help us keep track of the positions of the nodes in the linear order, and to associate nodes with lexical items. 356 Kuhlmann Mildly Non-Projective Dependency Grammar grammar formalisms, including standard context-free grammar, tree-adjoining grammar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman and Baldridge 2011). It also comprises, among others, multiple context-free grammars (Seki et al. 1991), minimalist grammars (Michaelis 1998), and simple range concatenation grammars (Boullier 2004). The article is structured as follows. In Section 2 we</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>Hays, David G. 1964. Dependency theory: A formalism and some observations. Language, 40(4):511–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Holan</author>
<author>Vladislav Kuboˇn</author>
<author>Karel Oliva</author>
<author>Martin Pl´atek</author>
</authors>
<title>Two useful measures of word order complexity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Processing of Dependency-Based Grammars,</booktitle>
<pages>21--29</pages>
<location>Montr´eal.</location>
<marker>Holan, Kuboˇn, Oliva, Pl´atek, 1998</marker>
<rawString>Holan, Tom´aˇs, Vladislav Kuboˇn, Karel Oliva, and Martin Pl´atek. 1998. Two useful measures of word order complexity. In Proceedings of the Workshop on Processing of Dependency-Based Grammars, pages 21–29, Montr´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<title>Language Networks. The New Word Grammar.</title>
<date>2007</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1695" citStr="Hudson 2007" startWordPosition="228" endWordPosition="229">constraints define a class of “mildly” non-projective dependency grammars that can be parsed in polynomial time. An evaluation on five dependency treebanks shows that these grammars have a good coverage of empirical data. 1. Introduction Syntactic representations based on word-to-word dependencies have a long-standing tradition in descriptive linguistics. Since the seminal work of Tesni`ere (1959), they have become the basis for several linguistic theories, such as Functional Generative Description (Sgall, Hajiˇcov´a, and Panevov´a 1986), Meaning–Text Theory (Mel’ˇcuk 1988), and Word Grammar (Hudson 2007). In recent years they have also been used for a wide range of practical applications, such as information extraction, machine translation, and question answering. We ascribe the widespread interest in dependency structures to their intuitive appeal, their conceptual simplicity, and in particular to the availability of accurate and efficient dependency parsers for a wide range of languages (Buchholz and Marsi 2006; Nivre et al. 2007). Although there exist both a considerable practical interest and an extensive linguistic literature, dependency syntax has remained something of an island from a </context>
</contexts>
<marker>Hudson, 2007</marker>
<rawString>Hudson, Richard. 2007. Language Networks. The New Word Grammar. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Riny Huybregts</author>
</authors>
<title>The weak inadequacy of context-free phrase structure grammars.</title>
<date>1984</date>
<booktitle>In Ger de Haan, Mieke Trommelen, and Wim Zonneveld, editors, Van periferie naar kern. Foris,</booktitle>
<pages>81--99</pages>
<location>Dordrecht,</location>
<contexts>
<context position="70264" citStr="Huybregts 1984" startWordPosition="12439" endWordPosition="12440">class of “mildly” non-projective dependency grammars that can be parsed in polynomial time. Our results in Sections 7 and 8 allow us to relate the formal power of an LCFRS to the structural properties of the dependency structures that it induces. Although we have used this relation to identify a class of dependency grammars that can be parsed in polynomial time, it also provides us with a new perspective on the question about the descriptive adequacy of a grammar formalism. This question has traditionally been discussed on the basis of strong and weak generative capacity (Bresnan et al. 1982; Huybregts 1984; Shieber 1985). A notion of generative capacity based on dependency trees makes a useful addition to this discussion, in particular when comparing formalisms for which no common concept of strong generative capacity exists. As an example for a result in this direction, see Koller and Kuhlmann (2009). We have defined the dependency trees that an LCFRS induces by means of a compositional mapping on the derivations. While we would claim that compositionality is a generally desirable property, the particular notion of induction is up for discussion. In particular, our interpretation of derivation</context>
</contexts>
<marker>Huybregts, 1984</marker>
<rawString>Huybregts, Riny. 1984. The weak inadequacy of context-free phrase structure grammars. In Ger de Haan, Mieke Trommelen, and Wim Zonneveld, editors, Van periferie naar kern. Foris, Dordrecht, pages 81–99.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes 1997</author>
</authors>
<title>Tree-Adjoining Grammars.</title>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--123</pages>
<publisher>Springer,</publisher>
<location>Berlin,</location>
<contexts>
<context position="13064" citStr="Joshi 1997" startWordPosition="2065" endWordPosition="2066">g (cf. Schabes, Abeill´e, and Joshi 1988 and Schabes 1990). Productions with lexicalized yield functions can be read as dependency rules. For example, the rules V -+ (x y sah)(N, V) (German) V -+ (x y1 zag y2)(N, V) (Dutch) can be read as stating that the verb to see requires two dependents, one noun (N) and one verb (V). Based on this reading, every term generated by a lexicalized LCFRS does not only yield a tuple of strings, but also induces a dependency tree on these strings: Each parent–child relation in the term represents a dependency between the associated lexical items (cf. Rambow and Joshi 1997). Thus every lexicalized LCFRS can be reinterpreted as a dependency grammar. To illustrate the idea, Figure 4 shows (the tree representations of) two terms generated by the grammars G1 and G2, together with the dependency trees induced by them. Note that these are the same trees that we gave for (iii) and (iv) in Figure 1. Our goal for the remainder of this section is to make the notion of induction formally precise. To this end we will reinterpret the yield functions of lexicalized LCFRSs as operations on dependency trees. Figure 3 Lexicalized linear context-free rewriting systems. 359 Comput</context>
</contexts>
<marker>Joshi, 1997, </marker>
<rawString>Joshi, Aravind K. and Yves Schabes.1997. Tree-Adjoining Grammars. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages, volume 3. Springer, Berlin, pages 69–123.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yuichi Kaji</author>
<author>Ryuichi Nakanishi</author>
</authors>
<title>Hiroyuki Seki, and Tadao Kasami.1992. The universal recognition problems for multiple context-free grammars and for linear context-free rewriting systems.</title>
<journal>IEICE Transactions on Information and Systems,</journal>
<pages>75--1</pages>
<marker>Kaji, Nakanishi, </marker>
<rawString>Kaji, Yuichi, Ryuichi Nakanishi, Hiroyuki Seki, and Tadao Kasami.1992. The universal recognition problems for multiple context-free grammars and for linear context-free rewriting systems. IEICE Transactions on Information and Systems, E75-D(1):78–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
</authors>
<title>Parsing Beyond Context-Free Grammars.</title>
<date>2010</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="20032" citStr="Kallmeyer 2010" startWordPosition="3353" endWordPosition="3354">n rules, one rule for each node of the construction trees. As an example, consider Figure 7, which shows a dependency tree with one of its construction trees. (The analysis is taken from K¨ubler, McDonald, and Nivre [2009].) From this construction tree we extract the following rules. The nonterminals (in bold) represent linear positions of nodes. 1 (A) 5 (on x)(7) 2 (x hearing, y)(1, 5) 6 (the) 3 (x1 is y1 x2 y2)(2,4) 7 (x issue)(6) 4 (scheduled, x)(8) 8 (today) Rules like these can serve as the starting point for practical systems for data-driven, non-projective dependency parsing (Maier and Kallmeyer 2010). Because the extraction of rules from construction trees is straightforward, the problem that we focus on in this section is how to obtain these trees in the first place. Our procedure for computing construction trees is based on the concept of “blocks.” 4.1 Blocks Let D be a dependency tree. A segment of D is a contiguous, non-empty sequence of nodes of D, all of which belong to the same component of the string yield. Thus a segment contains its endpoints, as well as all nodes between the endpoints in the precedence order. For a node u of D, a block of u is a longest segment consisting of de</context>
<context position="28043" citStr="Kallmeyer 2010" startWordPosition="4884" endWordPosition="4885">ration.” Property 2 For all 1 &lt; i &lt; m and 1 &lt; j1,j2 &lt; ki, if j1 &lt; j2 then xi,j1 &lt;f xi,j2. This property reflects that, in our extraction procedure, the variable xi,j represents the jth block of the ith child of u, where the blocks of a node are ordered from left to right based on their precedence. An example of a yield function that violates the property is (x1,2 x1,1), which defines a kind of swapping operation. In the literature on LCFRSs and related formalisms, yield functions with Property 2 have been called monotone (Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer 2010), and non-permuting (Kanazawa 2009). Property 3 No component o h is the empty string. This property, which is similar to ε-freeness as known from context-free grammars, has been discussed for multiple context-free grammars (Seki et al. 1991, Property N3 in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1). For our extracted grammars it holds because each component o h represents a block, and blocks are always non-empty. Property 4 No component o h contains a substring of the form xi,j1xi,j2. This property, which does not seem to have been discussed in the literature befo</context>
<context position="36209" citStr="Kallmeyer 2010" startWordPosition="6315" endWordPosition="6316">ear context-free rewriting systems are able to account for arbitrarily nonprojective dependency trees. This expressiveness comes with a price: In this section we show that parsing with lexicalized LCFRSs is intractable, unless we are willing to restrict the class of grammars. 6.1 Parsing Algorithm To ground our discussion of parsing complexity, we present a simple bottom–up parsing algorithm for LCFRSs, specified as a grammatical deduction system (Shieber, Schabes, and Pereira 1995). Several similar algorithms have been described in the literature (Seki et al. 1991; Bertsch and Nederhof 2001; Kallmeyer 2010). We assume that we are given a grammar G = (N, Σ, P, S) and a string w = a1 · · · an E V∗ to be parsed. Item form. The items of the deduction system take the form [A, l1, r1, ... , lk, rk] where A E N with y(A) = k, and the remaining components are indices identifying the left and right endpoints of pairwise non-overlapping substrings of w. More formally, 0 &lt; lh &lt; rh &lt; n, and for all h, h&apos; with h =� h&apos;, either rh &lt; lh&apos; or rh, &lt; lh. The intended interpretation of an item of this form is that A derives a term t E T(G) that yields the specified substrings of w, that is, A �∗G t and yield(t) = (a</context>
</contexts>
<marker>Kallmeyer, 2010</marker>
<rawString>Kallmeyer, Laura. 2010. Parsing Beyond Context-Free Grammars. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Kanazawa</author>
</authors>
<title>The pumping lemma for well-nested multiple context-free languages. In Developments in Language Theory.</title>
<date>2009</date>
<booktitle>Proceedings of the 13th International Conference, DLT</booktitle>
<volume>5583</volume>
<pages>312--325</pages>
<location>Stuttgart.</location>
<contexts>
<context position="28078" citStr="Kanazawa 2009" startWordPosition="4888" endWordPosition="4889">m and 1 &lt; j1,j2 &lt; ki, if j1 &lt; j2 then xi,j1 &lt;f xi,j2. This property reflects that, in our extraction procedure, the variable xi,j represents the jth block of the ith child of u, where the blocks of a node are ordered from left to right based on their precedence. An example of a yield function that violates the property is (x1,2 x1,1), which defines a kind of swapping operation. In the literature on LCFRSs and related formalisms, yield functions with Property 2 have been called monotone (Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer 2010), and non-permuting (Kanazawa 2009). Property 3 No component o h is the empty string. This property, which is similar to ε-freeness as known from context-free grammars, has been discussed for multiple context-free grammars (Seki et al. 1991, Property N3 in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1). For our extracted grammars it holds because each component o h represents a block, and blocks are always non-empty. Property 4 No component o h contains a substring of the form xi,j1xi,j2. This property, which does not seem to have been discussed in the literature before, is a reflection of the facts th</context>
<context position="59649" citStr="Kanazawa (2009)" startWordPosition="10571" endWordPosition="10572">l-nestedness can be characterized in terms of yield functions. Recall the notation x &lt;f y from Section 5.1. A yield function f: k1 ··· km - +k , f = (α1,...,αk) is ill-nested if there are argument indices 1 &lt; i1, i2 &lt; m with i1 =� i2 and component indices 1 &lt; j1, j&apos;1 &lt; ki1, 1 &lt; j2, j&apos; 2 &lt; ki2 such that xi1,j1 &lt;f xi2,j2 &lt;f xi1,ji &lt;f xi2,j� (5) 2 Otherwise, we say that f is well-nested. As an immediate consequence of Lemma 9, a restriction to well-nested dependency trees translates into a restriction to well-nested yield functions in the extracted grammars. This puts them into the class of what Kanazawa (2009) calls “well-nested multiple context-free grammars.”3 These grammars 3 Kanazawa (2009) calls a multiple context-free grammar well-nested if each of its rules is non-deleting, non-permuting (our Property 2), and well-nested according to (5). 378 Kuhlmann Mildly Non-Projective Dependency Grammar have a number of interesting properties that set them apart from general LCFRSs; in particular, they have a standard pumping lemma (Kanazawa 2009). The yield languages generated by well-nested multiple context-free grammars form a proper subhierarchy within the languages generated by general LCFRSs (Kana</context>
</contexts>
<marker>Kanazawa, 2009</marker>
<rawString>Kanazawa, Makoto. 2009. The pumping lemma for well-nested multiple context-free languages. In Developments in Language Theory. Proceedings of the 13th International Conference, DLT 2009, volume 5583 of Lecture Notes in Computer Science, pages 312–325, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Kanazawa</author>
<author>Sylvain Salvati</author>
</authors>
<title>The copying power of well-nested multiple context-free grammars.</title>
<date>2010</date>
<booktitle>Language and Automata Theory and Applications. Proceedings of the 4th International Conference, LATA 2010,</booktitle>
<volume>6031</volume>
<pages>344--355</pages>
<editor>In Adrian-Horia Dediu, Henning Fernau, and Carlos Martin-Vide, editors,</editor>
<location>Trier.</location>
<contexts>
<context position="60271" citStr="Kanazawa and Salvati 2010" startWordPosition="10654" endWordPosition="10657">009) calls “well-nested multiple context-free grammars.”3 These grammars 3 Kanazawa (2009) calls a multiple context-free grammar well-nested if each of its rules is non-deleting, non-permuting (our Property 2), and well-nested according to (5). 378 Kuhlmann Mildly Non-Projective Dependency Grammar have a number of interesting properties that set them apart from general LCFRSs; in particular, they have a standard pumping lemma (Kanazawa 2009). The yield languages generated by well-nested multiple context-free grammars form a proper subhierarchy within the languages generated by general LCFRSs (Kanazawa and Salvati 2010). Perhaps the most prominent subclass of well-nested LCFRSs is the class of tree-adjoining grammars (Joshi and Schabes 1997). Similar to the situation with block-degree, the correspondence between structural well-nestedness and syntactic well-nestedness is tight only for canonical grammars. For non-canonical grammars, syntactic well-nestedness alone does not imply structural well-nestedness, nor the other way around. 8.4 Coverage on Dependency Treebanks To estimate the coverage of well-nested grammars, we extend the evaluation presented in Section 7.4. Table 4 shows how many rules and trees in</context>
</contexts>
<marker>Kanazawa, Salvati, 2010</marker>
<rawString>Kanazawa, Makoto and Sylvain Salvati. 2010. The copying power of well-nested multiple context-free grammars. In Adrian-Horia Dediu, Henning Fernau, and Carlos Martin-Vide, editors, Language and Automata Theory and Applications. Proceedings of the 4th International Conference, LATA 2010, volume 6031 of Lecture Notes in Computer Science, pages 344–355, Trier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Marco Kuhlmann</author>
</authors>
<title>Dependency trees and the strong generative capacity of CCG.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>460--468</pages>
<location>Athens.</location>
<contexts>
<context position="70565" citStr="Koller and Kuhlmann (2009)" startWordPosition="12484" endWordPosition="12487">ntify a class of dependency grammars that can be parsed in polynomial time, it also provides us with a new perspective on the question about the descriptive adequacy of a grammar formalism. This question has traditionally been discussed on the basis of strong and weak generative capacity (Bresnan et al. 1982; Huybregts 1984; Shieber 1985). A notion of generative capacity based on dependency trees makes a useful addition to this discussion, in particular when comparing formalisms for which no common concept of strong generative capacity exists. As an example for a result in this direction, see Koller and Kuhlmann (2009). We have defined the dependency trees that an LCFRS induces by means of a compositional mapping on the derivations. While we would claim that compositionality is a generally desirable property, the particular notion of induction is up for discussion. In particular, our interpretation of derivations may not always be in line with how the grammar producing these derivations is actually used. One formalism for which such a mismatch between derivation trees and dependency trees has been pointed out is treeadjoining grammar (Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998). Resolving</context>
</contexts>
<marker>Koller, Kuhlmann, 2009</marker>
<rawString>Koller, Alexander and Marco Kuhlmann. 2009. Dependency trees and the strong generative capacity of CCG. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 460–468, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Kracht</author>
</authors>
<title>The Mathematics of Language,</title>
<date>2003</date>
<booktitle>Studies in Generative Grammar. Mouton de Gruyter,</booktitle>
<volume>63</volume>
<location>Paris.</location>
<contexts>
<context position="27984" citStr="Kracht 2003" startWordPosition="4876" endWordPosition="4877">1,1), which defines a kind of “reverse concatenation operation.” Property 2 For all 1 &lt; i &lt; m and 1 &lt; j1,j2 &lt; ki, if j1 &lt; j2 then xi,j1 &lt;f xi,j2. This property reflects that, in our extraction procedure, the variable xi,j represents the jth block of the ith child of u, where the blocks of a node are ordered from left to right based on their precedence. An example of a yield function that violates the property is (x1,2 x1,1), which defines a kind of swapping operation. In the literature on LCFRSs and related formalisms, yield functions with Property 2 have been called monotone (Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer 2010), and non-permuting (Kanazawa 2009). Property 3 No component o h is the empty string. This property, which is similar to ε-freeness as known from context-free grammars, has been discussed for multiple context-free grammars (Seki et al. 1991, Property N3 in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1). For our extracted grammars it holds because each component o h represents a block, and blocks are always non-empty. Property 4 No component o h contains a substring of the form xi,j1xi,j2. This property, which </context>
</contexts>
<marker>Kracht, 2003</marker>
<rawString>Kracht, Marcus. 2003. The Mathematics of Language, volume 63 of Studies in Generative Grammar. Mouton de Gruyter, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Trautner Kromann</author>
</authors>
<title>The Danish Dependency Treebank and the underlying linguistic theory.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second Workshop on Treebanks and Linguistic Theories (TLT),</booktitle>
<pages>217--220</pages>
<contexts>
<context position="52785" citStr="Kromann 2003" startWordPosition="9290" endWordPosition="9291"> the yield functions in this term violate both Property 3 and Property 4. 7.4 Coverage on Dependency Treebanks In order to assess the consequences of different bounds on the fan-out, we now evaluate the block-degree of dependency trees in real-world data. Specifically, we look into five 375 Computational Linguistics Volume 39, Number 2 dependency treebanks used in the 2006 CoNLL shared task on dependency parsing (Buchholz and Marsi 2006): the Prague Arabic Dependency Treebank (Hajiˇc et al. 2004), the Prague Dependency Treebank of Czech (B¨ohmov´a et al. 2003), the Danish Dependency Treebank (Kromann 2003), the Slovene Dependency Treebank (Dˇzeroski et al. 2006), and the Metu-Sabancı treebank of Turkish (Oflazer et al. 2003). The full data used in the CoNLL shared task also included treebanks that were produced by conversion of corpora originally annotated with structures other than dependencies, which is a potential source of “noise” that one has to take into account when interpreting any findings. Here, we consider only genuine dependency treebanks. More specifically, our statistics concern the training sections of the treebanks that were set off for the task. For similar results on other dat</context>
</contexts>
<marker>Kromann, 2003</marker>
<rawString>Kromann, Matthias Trautner. 2003. The Danish Dependency Treebank and the underlying linguistic theory. In Proceedings of the Second Workshop on Treebanks and Linguistic Theories (TLT), pages 217–220, V¨axj¨o.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing. Synthesis Lectures on Human Language Technologies.</title>
<date>2009</date>
<publisher>Morgan</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>K¨ubler, Sandra, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Synthesis Lectures on Human Language Technologies. Morgan and Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Mildly non-projective dependency structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association for Computational Linguistics (ACL) Main Conference Poster Sessions,</booktitle>
<pages>507--514</pages>
<location>Sydney.</location>
<contexts>
<context position="53422" citStr="Kuhlmann and Nivre (2006)" startWordPosition="9387" endWordPosition="9390">e Dependency Treebank (Dˇzeroski et al. 2006), and the Metu-Sabancı treebank of Turkish (Oflazer et al. 2003). The full data used in the CoNLL shared task also included treebanks that were produced by conversion of corpora originally annotated with structures other than dependencies, which is a potential source of “noise” that one has to take into account when interpreting any findings. Here, we consider only genuine dependency treebanks. More specifically, our statistics concern the training sections of the treebanks that were set off for the task. For similar results on other data sets, see Kuhlmann and Nivre (2006), Havelka (2007), and Maier and Lichte (2011). Our results are given in Table 3. For each treebank, we list the number of rules extracted from that treebank, as well as the number of corresponding dependency trees. We then list the number of rules that we lose if we restrict ourselves to rules with fanout = 1, or rules with fan-out &lt; 2, as well as the number of dependency trees that we lose because their construction trees contain at least one such rule. We count rule tokens, meaning that two otherwise identical rules are counted twice if they were extracted from different trees, or from diffe</context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Kuhlmann, Marco and Joakim Nivre. 2006. Mildly non-projective dependency structures. In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association for Computational Linguistics (ACL) Main Conference Poster Sessions, pages 507–514, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
</authors>
<title>Treebank grammar techniques for non-projective dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>478--486</pages>
<location>Athens.</location>
<contexts>
<context position="7160" citStr="Kuhlmann and Satta (2009)" startWordPosition="1049" endWordPosition="1052">can be used as dependency grammars. In Section 4 we show how to acquire lexicalized LCFRSs from dependency treebanks. This works in much the same way as the extraction of context-free grammars from phrase structure treebanks (cf. Charniak 1996), except that the derivation trees of dependency trees are not immediately accessible in the treebank. We therefore present an efficient algorithm for computing a canonical derivation tree for an input dependency tree; from this derivation tree, the rules of the grammar can be extracted in a straightforward way. The algorithm was originally published by Kuhlmann and Satta (2009). It produces a restricted type of lexicalized LCFRS that we call “canonical.” In Section 5 we provide a declarative characterization of this class of grammars, and show that every lexicalized LCFRS is (strongly) equivalent to a canonical one, in the sense that it induces the same set of dependency trees. In Section 6 we present a simple parsing algorithm for LCFRSs. Although the runtime of this algorithm is polynomial in the length of the sentence, the degree of the polynomial depends on two grammar-specific measures called fan-out and rank. We show that even in the restricted case of canonic</context>
<context position="18722" citStr="Kuhlmann and Satta (2009)" startWordPosition="3138" endWordPosition="3141">) for w, where E&apos; = {(1,1) -+ (2,3), (2,1) -+ (2,2)} The occurrence r of the anchor b off in w� is (1, 2); the nodes of G that correspond to the root nodes of D1 and D2 are ¯r1 = (1, 1) and ¯r2 = (2, 1). The dependency tree D is obtained by adding the edges r -+ ¯r1 and r -+ ¯r2 to G. 4. Extraction of Dependency Grammars We now show how to extract lexicalized linear context-free rewriting systems from dependency treebanks. To this end, we adapt the standard technique for extracting context-free grammars from phrase structure treebanks (Charniak 1996). Our technique was originally published by Kuhlmann and Satta (2009). In recent work, Maier and Lichte (2011) have shown how to unify it with a similar technique for the extraction of range concatenation grammars from discontinuous constituent structures, due to Maier and Søgaard (2008). To simplify our presentation we restrict our attention to treebanks containing simple dependency trees. 362 Kuhlmann Mildly Non-Projective Dependency Grammar Figure 7 A dependency tree and one of its construction trees. To extract a lexicalized LCFRS from a dependency treebank we proceed as follows. First, for each dependency tree (w, D) in the treebank, we compute a construct</context>
</contexts>
<marker>Kuhlmann, Satta, 2009</marker>
<rawString>Kuhlmann, Marco and Giorgio Satta. 2009. Treebank grammar techniques for non-projective dependency parsing. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 478–486, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Recognition can be harder than parsing.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="38303" citStr="Lang 1994" startWordPosition="6749" endWordPosition="6750">f αh then δ(ri,j, v) = rh If xi,j v xi,,j, is an infix of αh then δ(ri,j, v) = li,,j, These constraints ensure that the substrings corresponding to the premises of the inference rule can be combined into the substrings corresponding to the conclusion by means of the yield function f. Based on the deduction system, a tabular parser for LCFRSs can be implemented using standard dynamic programming techniques. This parser will compute a packed representation of the set of all derivation trees that the grammar G assigns to the string w. Such a packed representation is often called a shared forest (Lang 1994). In combination with appropriate semirings, the shared forest is useful for many tasks in syntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009). 6.2 Parsing Complexity We are interested in an upper bound on the runtime of the tabular parser that we have just presented. We can see that the parser runs in time O(|G||w|c), where |G |denotes the size of some suitable representation of the grammar G, and c denotes the maximal number of instantiations of an inference rule (cf. McAllester 2002). Let us write c(f ) for the specialization of c to inference rules for productions wi</context>
</contexts>
<marker>Lang, 1994</marker>
<rawString>Lang, Bernard. 1994. Recognition can be harder than parsing. Computational Intelligence, 10(4):486–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<journal>Computational Linguistics</journal>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<volume>39</volume>
<pages>40--51</pages>
<contexts>
<context position="38468" citStr="Li and Eisner 2009" startWordPosition="6772" endWordPosition="6775">ises of the inference rule can be combined into the substrings corresponding to the conclusion by means of the yield function f. Based on the deduction system, a tabular parser for LCFRSs can be implemented using standard dynamic programming techniques. This parser will compute a packed representation of the set of all derivation trees that the grammar G assigns to the string w. Such a packed representation is often called a shared forest (Lang 1994). In combination with appropriate semirings, the shared forest is useful for many tasks in syntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009). 6.2 Parsing Complexity We are interested in an upper bound on the runtime of the tabular parser that we have just presented. We can see that the parser runs in time O(|G||w|c), where |G |denotes the size of some suitable representation of the grammar G, and c denotes the maximal number of instantiations of an inference rule (cf. McAllester 2002). Let us write c(f ) for the specialization of c to inference rules for productions with yield function f. We refer to this value as the parsing complexity off (cf. Gildea 2010). Then to show an upper bound on c it suffices to show an upper bound on t</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Li, Zhifei and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 40–51, Singapore. Computational Linguistics Volume 39, Number 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Laura Kallmeyer</author>
</authors>
<title>Discontinuity and non-projectivity: Using mildly context-sensitive formalisms for data-driven parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Tenth International Conference on Tree Adjoining Grammars and Related Formalisms (TAG+),</booktitle>
<location>New Haven, CT.</location>
<contexts>
<context position="20032" citStr="Maier and Kallmeyer 2010" startWordPosition="3351" endWordPosition="3354"> production rules, one rule for each node of the construction trees. As an example, consider Figure 7, which shows a dependency tree with one of its construction trees. (The analysis is taken from K¨ubler, McDonald, and Nivre [2009].) From this construction tree we extract the following rules. The nonterminals (in bold) represent linear positions of nodes. 1 (A) 5 (on x)(7) 2 (x hearing, y)(1, 5) 6 (the) 3 (x1 is y1 x2 y2)(2,4) 7 (x issue)(6) 4 (scheduled, x)(8) 8 (today) Rules like these can serve as the starting point for practical systems for data-driven, non-projective dependency parsing (Maier and Kallmeyer 2010). Because the extraction of rules from construction trees is straightforward, the problem that we focus on in this section is how to obtain these trees in the first place. Our procedure for computing construction trees is based on the concept of “blocks.” 4.1 Blocks Let D be a dependency tree. A segment of D is a contiguous, non-empty sequence of nodes of D, all of which belong to the same component of the string yield. Thus a segment contains its endpoints, as well as all nodes between the endpoints in the precedence order. For a node u of D, a block of u is a longest segment consisting of de</context>
</contexts>
<marker>Maier, Kallmeyer, 2010</marker>
<rawString>Maier, Wolfgang and Laura Kallmeyer. 2010. Discontinuity and non-projectivity: Using mildly context-sensitive formalisms for data-driven parsing. In Proceedings of the Tenth International Conference on Tree Adjoining Grammars and Related Formalisms (TAG+), New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Timm Lichte</author>
</authors>
<title>Characterizing discontinuity in constituent treebanks.</title>
<date>2011</date>
<booktitle>Formal Grammar. Proceedings of the 14th International Conference, FG 2009, Revised Selected Papers,</booktitle>
<volume>5591</volume>
<pages>167--182</pages>
<editor>In Philippe de Groote, Markus Egg, and Laura Kallmeyer, editors,</editor>
<location>Bordeaux.</location>
<contexts>
<context position="18763" citStr="Maier and Lichte (2011)" startWordPosition="3145" endWordPosition="3148">-+ (2,2)} The occurrence r of the anchor b off in w� is (1, 2); the nodes of G that correspond to the root nodes of D1 and D2 are ¯r1 = (1, 1) and ¯r2 = (2, 1). The dependency tree D is obtained by adding the edges r -+ ¯r1 and r -+ ¯r2 to G. 4. Extraction of Dependency Grammars We now show how to extract lexicalized linear context-free rewriting systems from dependency treebanks. To this end, we adapt the standard technique for extracting context-free grammars from phrase structure treebanks (Charniak 1996). Our technique was originally published by Kuhlmann and Satta (2009). In recent work, Maier and Lichte (2011) have shown how to unify it with a similar technique for the extraction of range concatenation grammars from discontinuous constituent structures, due to Maier and Søgaard (2008). To simplify our presentation we restrict our attention to treebanks containing simple dependency trees. 362 Kuhlmann Mildly Non-Projective Dependency Grammar Figure 7 A dependency tree and one of its construction trees. To extract a lexicalized LCFRS from a dependency treebank we proceed as follows. First, for each dependency tree (w, D) in the treebank, we compute a construction tree, a term t over yield functions t</context>
<context position="53467" citStr="Maier and Lichte (2011)" startWordPosition="9394" endWordPosition="9397">and the Metu-Sabancı treebank of Turkish (Oflazer et al. 2003). The full data used in the CoNLL shared task also included treebanks that were produced by conversion of corpora originally annotated with structures other than dependencies, which is a potential source of “noise” that one has to take into account when interpreting any findings. Here, we consider only genuine dependency treebanks. More specifically, our statistics concern the training sections of the treebanks that were set off for the task. For similar results on other data sets, see Kuhlmann and Nivre (2006), Havelka (2007), and Maier and Lichte (2011). Our results are given in Table 3. For each treebank, we list the number of rules extracted from that treebank, as well as the number of corresponding dependency trees. We then list the number of rules that we lose if we restrict ourselves to rules with fanout = 1, or rules with fan-out &lt; 2, as well as the number of dependency trees that we lose because their construction trees contain at least one such rule. We count rule tokens, meaning that two otherwise identical rules are counted twice if they were extracted from different trees, or from different nodes in the same tree. By putting the b</context>
<context position="71959" citStr="Maier and Lichte 2011" startWordPosition="12694" endWordPosition="12697">f our dependency grammars is restricted to a finite block-degree. As a consequence of this restriction, our dependency grammars are not expressive enough to capture linguistic phenomena that require unlimited degrees of non-projectivity, such as the “scrambling” in German subordinate clauses (Becker, Rambow, and Niv 1992). The question whether it is reasonable to assume a bound on the block-degree of dependency trees, perhaps for some performance-based reason, is open. Likewise, it is not clear whether well-nestedness is a “natural” constraint on dependency analyses (Chen-Main and Joshi 2010; Maier and Lichte 2011). Although most of the results that we have presented in this article are of a theoretical nature, some of them have found their way into practical systems. In particular, the extraction technique from Section 4 is used by the data-driven dependency parser of Maier and Kallmeyer (2010). Acknowledgments References The author gratefully acknowledges Becker, Tilman, Owen Rambow, and financial support from The Michael Niv. 1992. The derivational German Research Foundation generative power of formal systems, (Sonderforschungsbereich 378, or: Scrambling is beyond LCFRS. IRCS project MI 2) and The Sw</context>
</contexts>
<marker>Maier, Lichte, 2011</marker>
<rawString>Maier, Wolfgang and Timm Lichte. 2011. Characterizing discontinuity in constituent treebanks. In Philippe de Groote, Markus Egg, and Laura Kallmeyer, editors, Formal Grammar. Proceedings of the 14th International Conference, FG 2009, Revised Selected Papers, volume 5591 of Lecture Notes in Computer Science, pages 167–182, Bordeaux.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Anders Søgaard</author>
</authors>
<title>Treebanks and mild context-sensitivity.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th Conference on Formal Grammar (FG),</booktitle>
<pages>61--76</pages>
<location>Hamburg.</location>
<contexts>
<context position="18941" citStr="Maier and Søgaard (2008)" startWordPosition="3172" endWordPosition="3175">ree D is obtained by adding the edges r -+ ¯r1 and r -+ ¯r2 to G. 4. Extraction of Dependency Grammars We now show how to extract lexicalized linear context-free rewriting systems from dependency treebanks. To this end, we adapt the standard technique for extracting context-free grammars from phrase structure treebanks (Charniak 1996). Our technique was originally published by Kuhlmann and Satta (2009). In recent work, Maier and Lichte (2011) have shown how to unify it with a similar technique for the extraction of range concatenation grammars from discontinuous constituent structures, due to Maier and Søgaard (2008). To simplify our presentation we restrict our attention to treebanks containing simple dependency trees. 362 Kuhlmann Mildly Non-Projective Dependency Grammar Figure 7 A dependency tree and one of its construction trees. To extract a lexicalized LCFRS from a dependency treebank we proceed as follows. First, for each dependency tree (w, D) in the treebank, we compute a construction tree, a term t over yield functions that induces (w, D). Then we collect a set of production rules, one rule for each node of the construction trees. As an example, consider Figure 7, which shows a dependency tree w</context>
<context position="29623" citStr="Maier and Søgaard (2008)" startWordPosition="5140" endWordPosition="5143">f it can be extracted from a dependency treebank using the technique presented in Section 4. Proof We have already argued for the “only if” part of the claim. To prove the “if” part, it suffices to show that for every canonical, lexicalized yield function f, one can construct 366 Kuhlmann Mildly Non-Projective Dependency Grammar a dependency tree such that the construction tree extracted for this dependency tree contains f. This is an easy exercise. ■ We conclude by noting that Properties 2–4 are also shared by the treebank grammars extracted from constituency treebanks using the technique by Maier and Søgaard (2008). 5.2 Equivalence Between General and Canonical Grammars Two lexicalized LCFRSs are called strongly equivalent if they induce the same set of dependency trees. We show the following equivalence result: Lemma 2 For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized LCFRS G&apos; such that G&apos; is canonical. Proof Our proof of this lemma uses two normal-form results about multiple context-free grammars: Michaelis (2001, Section 2.4) provides a construction that transforms a multiple context-free grammar into a weakly equivalent multiple context-free grammar in which all rules</context>
</contexts>
<marker>Maier, Søgaard, 2008</marker>
<rawString>Maier, Wolfgang and Anders Søgaard. 2008. Treebanks and mild context-sensitivity. In Proceedings of the 13th Conference on Formal Grammar (FG), pages 61–76, Hamburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McAllester</author>
</authors>
<title>On the complexity analysis of static analyses.</title>
<date>2002</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>49</volume>
<issue>4</issue>
<contexts>
<context position="38817" citStr="McAllester 2002" startWordPosition="6834" endWordPosition="6835">ar G assigns to the string w. Such a packed representation is often called a shared forest (Lang 1994). In combination with appropriate semirings, the shared forest is useful for many tasks in syntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009). 6.2 Parsing Complexity We are interested in an upper bound on the runtime of the tabular parser that we have just presented. We can see that the parser runs in time O(|G||w|c), where |G |denotes the size of some suitable representation of the grammar G, and c denotes the maximal number of instantiations of an inference rule (cf. McAllester 2002). Let us write c(f ) for the specialization of c to inference rules for productions with yield function f. We refer to this value as the parsing complexity off (cf. Gildea 2010). Then to show an upper bound on c it suffices to show an upper bound on the parsing complexities of the yield functions that the parser has to handle. An obvious such upper bound is c(f ) &lt; 2k + ~m 2ki i=1 Here we imagine that we could choose each endpoint in Equation (3) independently of all the others. By virtue of the constraints, however, some of the endpoints cannot be chosen freely; in particular, some of the sub</context>
</contexts>
<marker>McAllester, 2002</marker>
<rawString>McAllester, David. 2002. On the complexity analysis of static analyses. Journal of the Association for Computing Machinery, 49(4):512–537.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Igor 1988 Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<publisher>State University of New York Press,</publisher>
<location>Albany, NY.</location>
<marker>Mel’ˇcuk, </marker>
<rawString>Mel’ˇcuk, Igor.1988. Dependency Syntax: Theory and Practice. State University of New York Press, Albany, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Michaelis</author>
</authors>
<title>Derivational minimalism is mildly context-sensitive.</title>
<date>1998</date>
<booktitle>In Logical Aspects of Computational Linguistics, Third International Conference, LACL 1998, Selected Papers,</booktitle>
<volume>2014</volume>
<pages>179--198</pages>
<location>Grenoble.</location>
<contexts>
<context position="5549" citStr="Michaelis 1998" startWordPosition="798" endWordPosition="799">ndency tree as circles, and the edges as arrows pointing towards the dependent (away from the root node). Following Hays (1964), we use dotted lines to help us keep track of the positions of the nodes in the linear order, and to associate nodes with lexical items. 356 Kuhlmann Mildly Non-Projective Dependency Grammar grammar formalisms, including standard context-free grammar, tree-adjoining grammar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman and Baldridge 2011). It also comprises, among others, multiple context-free grammars (Seki et al. 1991), minimalist grammars (Michaelis 1998), and simple range concatenation grammars (Boullier 2004). The article is structured as follows. In Section 2 we provide the technical background to our work; in particular, we introduce our terminology and notation for linear context-free rewriting systems. An LCFRS generates a set of terms (formal expressions) which are interpreted as derivation trees of objects from some domain. Each term also has a secondary interpretation under which it denotes a tuple of strings, representing the string yield of the derived object. In Section 3 we introduce the central notion of a lexicalized linear cont</context>
</contexts>
<marker>Michaelis, 1998</marker>
<rawString>Michaelis, Jens. 1998. Derivational minimalism is mildly context-sensitive. In Logical Aspects of Computational Linguistics, Third International Conference, LACL 1998, Selected Papers, volume 2014 of Lecture Notes in Computer Science, pages 179–198, Grenoble.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Michaelis</author>
</authors>
<title>On Formal Properties of Minimalist Grammars.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at Potsdam,</institution>
<location>Potsdam, Germany.</location>
<contexts>
<context position="27970" citStr="Michaelis 2001" startWordPosition="4874" endWordPosition="4875">rty 1 is (x2,1 x1,1), which defines a kind of “reverse concatenation operation.” Property 2 For all 1 &lt; i &lt; m and 1 &lt; j1,j2 &lt; ki, if j1 &lt; j2 then xi,j1 &lt;f xi,j2. This property reflects that, in our extraction procedure, the variable xi,j represents the jth block of the ith child of u, where the blocks of a node are ordered from left to right based on their precedence. An example of a yield function that violates the property is (x1,2 x1,1), which defines a kind of swapping operation. In the literature on LCFRSs and related formalisms, yield functions with Property 2 have been called monotone (Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer 2010), and non-permuting (Kanazawa 2009). Property 3 No component o h is the empty string. This property, which is similar to ε-freeness as known from context-free grammars, has been discussed for multiple context-free grammars (Seki et al. 1991, Property N3 in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1). For our extracted grammars it holds because each component o h represents a block, and blocks are always non-empty. Property 4 No component o h contains a substring of the form xi,j1xi,j2. This pr</context>
<context position="30063" citStr="Michaelis (2001" startWordPosition="5208" endWordPosition="5209">ise. ■ We conclude by noting that Properties 2–4 are also shared by the treebank grammars extracted from constituency treebanks using the technique by Maier and Søgaard (2008). 5.2 Equivalence Between General and Canonical Grammars Two lexicalized LCFRSs are called strongly equivalent if they induce the same set of dependency trees. We show the following equivalence result: Lemma 2 For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized LCFRS G&apos; such that G&apos; is canonical. Proof Our proof of this lemma uses two normal-form results about multiple context-free grammars: Michaelis (2001, Section 2.4) provides a construction that transforms a multiple context-free grammar into a weakly equivalent multiple context-free grammar in which all rules satisfy Property 2, and Seki et al. (1991, Lemma 2.2) present a corresponding construction for Property 3. Whereas both constructions are only quoted to preserve weak equivalence, we can verify that, in the special case where the input grammar is a lexicalized LCFRS, they also preserve the set of induced dependency trees. To complete the proof of Lemma 2, we show that every lexicalized LCFRS can be cast into normal forms that satisfy P</context>
</contexts>
<marker>Michaelis, 2001</marker>
<rawString>Michaelis, Jens. 2001. On Formal Properties of Minimalist Grammars. Ph.D. thesis, Universit¨at Potsdam, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing (EMNLP) and Computational Natural Language Learning (CoNLL),</booktitle>
<pages>915--932</pages>
<location>Prague.</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Nivre, Joakim, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing (EMNLP) and Computational Natural Language Learning (CoNLL), pages 915–932, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Bilge Say, Dilek Zeynep Hakkani-T¨ur, and G¨okhan T¨ur.</title>
<date>2003</date>
<volume>15</volume>
<pages>261--277</pages>
<editor>In Abeill´e, Anne, editor.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, chapter</location>
<marker>Oflazer, 2003</marker>
<rawString>Oflazer, Kemal, Bilge Say, Dilek Zeynep Hakkani-T¨ur, and G¨okhan T¨ur. 2003. Building a Turkish treebank. In Abeill´e, Anne, editor. Treebanks: Building and Using Parsed Corpora. Kluwer Academic Publishers, Dordrecht, chapter 15, pages 261–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Aravind K Joshi</author>
</authors>
<title>A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena.</title>
<date>1997</date>
<booktitle>Recent Trends in Meaning-Text Theory, volume 39 of Studies in Language, Companion Series. John Benjamins,</booktitle>
<pages>167--190</pages>
<editor>In Leo Wanner, editor,</editor>
<location>Amsterdam,</location>
<contexts>
<context position="13064" citStr="Rambow and Joshi 1997" startWordPosition="2063" endWordPosition="2066">s sah or zag (cf. Schabes, Abeill´e, and Joshi 1988 and Schabes 1990). Productions with lexicalized yield functions can be read as dependency rules. For example, the rules V -+ (x y sah)(N, V) (German) V -+ (x y1 zag y2)(N, V) (Dutch) can be read as stating that the verb to see requires two dependents, one noun (N) and one verb (V). Based on this reading, every term generated by a lexicalized LCFRS does not only yield a tuple of strings, but also induces a dependency tree on these strings: Each parent–child relation in the term represents a dependency between the associated lexical items (cf. Rambow and Joshi 1997). Thus every lexicalized LCFRS can be reinterpreted as a dependency grammar. To illustrate the idea, Figure 4 shows (the tree representations of) two terms generated by the grammars G1 and G2, together with the dependency trees induced by them. Note that these are the same trees that we gave for (iii) and (iv) in Figure 1. Our goal for the remainder of this section is to make the notion of induction formally precise. To this end we will reinterpret the yield functions of lexicalized LCFRSs as operations on dependency trees. Figure 3 Lexicalized linear context-free rewriting systems. 359 Comput</context>
</contexts>
<marker>Rambow, Joshi, 1997</marker>
<rawString>Rambow, Owen and Aravind K. Joshi. 1997. A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena. In Leo Wanner, editor, Recent Trends in Meaning-Text Theory, volume 39 of Studies in Language, Companion Series. John Benjamins, Amsterdam, pages 167–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
</authors>
<title>D-Tree grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>151--158</pages>
<location>Cambridge, MA.</location>
<marker>Rambow, Vijay-Shanker, Weir, 1995</marker>
<rawString>Rambow, Owen, K. Vijay-Shanker, and David J. Weir. 1995. D-Tree grammars. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 151–158, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
<author>Giorgio Satta</author>
</authors>
<title>Optimal rank reduction for linear context-free rewriting systems with fan-out two.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>525--533</pages>
<location>Uppsala.</location>
<contexts>
<context position="8205" citStr="Sagot and Satta 2010" startWordPosition="1219" endWordPosition="1222"> the length of the sentence, the degree of the polynomial depends on two grammar-specific measures called fan-out and rank. We show that even in the restricted case of canonical grammars, parsing is an NPhard problem. It is important therefore to keep the fan-out and the rank of a grammar as low as possible, and much of the recent work on LCFRSs has been devoted to the development of techniques that optimize parsing complexity in various scenarios G´omez-Rodr´ıguez and Satta 2009; G´omez-Rodr´ıguez et al. 2009; Kuhlmann and Satta 2009; Gildea 2010; G´omez-Rodr´ıguez, Kuhlmann, and Satta 2010; Sagot and Satta 2010; and Crescenzi et al. 2011). In this article we explore the impact of non-projectivity on parsing complexity. In Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS, a measure called block-degree (or gap-degree) (Holan et al. 1998). Although there is no theoretical upper bound on the block-degree of the dependency trees needed for linguistic analysis, we provide evidence from several dependency treebanks showing that, from a practical point of view, this upper bound can be put at a value of as low as 2. In Section 8 we study a second constraint on non-proje</context>
</contexts>
<marker>Sagot, Satta, 2010</marker>
<rawString>Sagot, Benoit and Giorgio Satta. 2010. Optimal rank reduction for linear context-free rewriting systems with fan-out two. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 525–533, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Satta</author>
</authors>
<title>Recognition of linear context-free rewriting systems.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>89--95</pages>
<location>Newark, DE.</location>
<contexts>
<context position="42195" citStr="Satta (1992)" startWordPosition="7421" endWordPosition="7422">nded by constants. Lemma 6, which shows that the universal recognition problem of lexicalized LCFRSs is in NP, distinguishes lexicalized LCFRSs from general LCFRSs, for which the universal recognition problem is known to be PSPACE-complete (Kaji et al. 1992). The crucial difference between general and lexicalized LCFRSs is the fact that in the latter, the size of the generated terms is bounded by the length of the input string. Lemma 7 and Lemma 8, which establish two NP-hardness results for lexicalized LCFRSs, are stronger versions of the corresponding results for general LCFRSs presented by Satta (1992), and are proved using similar reductions. They show that the hardness results hold under significant restrictions of the formalism: to lexicalized form and to canonical yield functions. Note that, whereas in Section 5.2 we have shown that every lexicalized LCFRS is equivalent to a canonical one, the normal form transformation increases the size of the original grammar by a factor that is at least exponential in the fan-out. Lemma 6 The universal recognition problem of lexicalized LCFRSs is in NP. 371 Computational Linguistics Volume 39, Number 2 Proof Let G be a lexicalized LCFRS, and let w b</context>
</contexts>
<marker>Satta, 1992</marker>
<rawString>Satta, Giorgio. 1992. Recognition of linear context-free rewriting systems. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics (ACL), pages 89–95, Newark, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and Computational Aspects of Lexicalized Grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6356" citStr="Schabes 1990" startWordPosition="925" endWordPosition="926">minology and notation for linear context-free rewriting systems. An LCFRS generates a set of terms (formal expressions) which are interpreted as derivation trees of objects from some domain. Each term also has a secondary interpretation under which it denotes a tuple of strings, representing the string yield of the derived object. In Section 3 we introduce the central notion of a lexicalized linear context-free rewriting system, which is an LCFRS in which each rule of the grammar is associated with an overt lexical item, representing a syntactic head (cf. Schabes, Abeill´e, and Joshi 1988 and Schabes 1990). We show that this property gives rise to an additional interpretation under which each term denotes a dependency tree on its yield. With this interpretation, lexicalized LCFRSs can be used as dependency grammars. In Section 4 we show how to acquire lexicalized LCFRSs from dependency treebanks. This works in much the same way as the extraction of context-free grammars from phrase structure treebanks (cf. Charniak 1996), except that the derivation trees of dependency trees are not immediately accessible in the treebank. We therefore present an efficient algorithm for computing a canonical deri</context>
<context position="12511" citStr="Schabes 1990" startWordPosition="1968" endWordPosition="1969">or verb–argument dependencies in German and Dutch from Section 1: (iii) dass Jan1 Piet2 Marie3 lesen3 helfen2 sah1 (German) that Jan Piet Marie read help saw (iv) dat Jan1 Piet2 Marie3 zag1 helpen2 lezen3 (Dutch) that Jan Piet Marie saw help read ‘that Jan saw Piet help Marie read’ Figure 3 shows the production rules of two linear context-free rewriting systems (one for German, one for Dutch) that generate these examples. The grammars are lexicalized in the sense that each of their yield functions is associated with a lexical item, such as sah or zag (cf. Schabes, Abeill´e, and Joshi 1988 and Schabes 1990). Productions with lexicalized yield functions can be read as dependency rules. For example, the rules V -+ (x y sah)(N, V) (German) V -+ (x y1 zag y2)(N, V) (Dutch) can be read as stating that the verb to see requires two dependents, one noun (N) and one verb (V). Based on this reading, every term generated by a lexicalized LCFRS does not only yield a tuple of strings, but also induces a dependency tree on these strings: Each parent–child relation in the term represents a dependency between the associated lexical items (cf. Rambow and Joshi 1997). Thus every lexicalized LCFRS can be reinterpr</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Schabes, Yves. 1990. Mathematical and Computational Aspects of Lexicalized Grammars. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeill´e</author>
<author>Aravind K Joshi 1988</author>
</authors>
<title>Parsing strategies with ‘lexicalized’ grammars: Application to tree adjoining grammars.</title>
<booktitle>In Proceedings of the Twelfth International Conference on Computational Linguistics (COLING),</booktitle>
<pages>578--583</pages>
<location>Budapest.</location>
<marker>Schabes, Abeill´e, 1988, </marker>
<rawString>Schabes, Yves, Anne Abeill´e, and Aravind K. Joshi.1988. Parsing strategies with ‘lexicalized’ grammars: Application to tree adjoining grammars. In Proceedings of the Twelfth International Conference on Computational Linguistics (COLING), pages 578–583, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On Multiple Context-Free Grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<volume>88</volume>
<issue>2</issue>
<contexts>
<context position="5511" citStr="Seki et al. 1991" startWordPosition="792" endWordPosition="795">of various 1 We draw the nodes of a dependency tree as circles, and the edges as arrows pointing towards the dependent (away from the root node). Following Hays (1964), we use dotted lines to help us keep track of the positions of the nodes in the linear order, and to associate nodes with lexical items. 356 Kuhlmann Mildly Non-Projective Dependency Grammar grammar formalisms, including standard context-free grammar, tree-adjoining grammar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman and Baldridge 2011). It also comprises, among others, multiple context-free grammars (Seki et al. 1991), minimalist grammars (Michaelis 1998), and simple range concatenation grammars (Boullier 2004). The article is structured as follows. In Section 2 we provide the technical background to our work; in particular, we introduce our terminology and notation for linear context-free rewriting systems. An LCFRS generates a set of terms (formal expressions) which are interpreted as derivation trees of objects from some domain. Each term also has a secondary interpretation under which it denotes a tuple of strings, representing the string yield of the derived object. In Section 3 we introduce the centr</context>
<context position="28283" citStr="Seki et al. 1991" startWordPosition="4919" endWordPosition="4922">e are ordered from left to right based on their precedence. An example of a yield function that violates the property is (x1,2 x1,1), which defines a kind of swapping operation. In the literature on LCFRSs and related formalisms, yield functions with Property 2 have been called monotone (Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer 2010), and non-permuting (Kanazawa 2009). Property 3 No component o h is the empty string. This property, which is similar to ε-freeness as known from context-free grammars, has been discussed for multiple context-free grammars (Seki et al. 1991, Property N3 in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1). For our extracted grammars it holds because each component o h represents a block, and blocks are always non-empty. Property 4 No component o h contains a substring of the form xi,j1xi,j2. This property, which does not seem to have been discussed in the literature before, is a reflection of the facts that variables with the same argument index represent blocks of the same child node, and that these blocks are longest segments of descendants. A yield function with Properties 1–4 is called canonical. An LC</context>
<context position="30265" citStr="Seki et al. (1991" startWordPosition="5237" endWordPosition="5240"> General and Canonical Grammars Two lexicalized LCFRSs are called strongly equivalent if they induce the same set of dependency trees. We show the following equivalence result: Lemma 2 For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized LCFRS G&apos; such that G&apos; is canonical. Proof Our proof of this lemma uses two normal-form results about multiple context-free grammars: Michaelis (2001, Section 2.4) provides a construction that transforms a multiple context-free grammar into a weakly equivalent multiple context-free grammar in which all rules satisfy Property 2, and Seki et al. (1991, Lemma 2.2) present a corresponding construction for Property 3. Whereas both constructions are only quoted to preserve weak equivalence, we can verify that, in the special case where the input grammar is a lexicalized LCFRS, they also preserve the set of induced dependency trees. To complete the proof of Lemma 2, we show that every lexicalized LCFRS can be cast into normal forms that satisfy Property 1 and Property 4. It is not hard then to combine the four constructions into a single one that simultaneously establishes all properties of canonical yield functions. ■ Lemma 3 For every lexical</context>
<context position="36165" citStr="Seki et al. 1991" startWordPosition="6307" endWordPosition="6310"> 1 6. Parsing and Recognition Lexicalized linear context-free rewriting systems are able to account for arbitrarily nonprojective dependency trees. This expressiveness comes with a price: In this section we show that parsing with lexicalized LCFRSs is intractable, unless we are willing to restrict the class of grammars. 6.1 Parsing Algorithm To ground our discussion of parsing complexity, we present a simple bottom–up parsing algorithm for LCFRSs, specified as a grammatical deduction system (Shieber, Schabes, and Pereira 1995). Several similar algorithms have been described in the literature (Seki et al. 1991; Bertsch and Nederhof 2001; Kallmeyer 2010). We assume that we are given a grammar G = (N, Σ, P, S) and a string w = a1 · · · an E V∗ to be parsed. Item form. The items of the deduction system take the form [A, l1, r1, ... , lk, rk] where A E N with y(A) = k, and the remaining components are indices identifying the left and right endpoints of pairwise non-overlapping substrings of w. More formally, 0 &lt; lh &lt; rh &lt; n, and for all h, h&apos; with h =� h&apos;, either rh &lt; lh&apos; or rh, &lt; lh. The intended interpretation of an item of this form is that A derives a term t E T(G) that yields the specified substri</context>
<context position="51621" citStr="Seki et al. (1991)" startWordPosition="9103" endWordPosition="9106">an one block. The runtime of this test is linear in the number of nodes of D. 7.3 Block-Degree in Extracted Grammars In a lexicalized LCFRS extracted from a dependency treebank, there is a one-to-one correspondence between the blocks of a node u and the components of the template of the yield function f extracted for u. In particular, the fan-out of f is exactly the block-degree of u. As a consequence, any bound on the block-degree of the trees in the treebank translates into a bound on the fan-out of the extracted grammar. This has consequences for the generative capacity of the grammars: As Seki et al. (1991) show, the class of LCFRSs with fan-out k &gt; 1 can generate string languages that cannot be generated by the class of LCFRSs with fan-out k − 1. It may be worth emphasizing that the one-to-one correspondence between blocks and tuple components is a consequence of two characteristic properties of extracted grammars (Properties 3 and 4), and does not hold for non-canonical lexicalized LCFRSs. Example 7 The following term induces a two-node dependency tree with block-degree 1, but contains yield functions with fan-out 2: (a x1 x2)((b, ε)). Note that the yield functions in this term violate both Pr</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Seki, Hiroyuki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On Multiple Context-Free Grammars. Theoretical Computer Science, 88(2):191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<title>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>Sgall, Petr, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Evidence against the context-freeness of natural language.</title>
<date>1985</date>
<journal>Linguistics and Philosophy,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="70279" citStr="Shieber 1985" startWordPosition="12441" endWordPosition="12442">” non-projective dependency grammars that can be parsed in polynomial time. Our results in Sections 7 and 8 allow us to relate the formal power of an LCFRS to the structural properties of the dependency structures that it induces. Although we have used this relation to identify a class of dependency grammars that can be parsed in polynomial time, it also provides us with a new perspective on the question about the descriptive adequacy of a grammar formalism. This question has traditionally been discussed on the basis of strong and weak generative capacity (Bresnan et al. 1982; Huybregts 1984; Shieber 1985). A notion of generative capacity based on dependency trees makes a useful addition to this discussion, in particular when comparing formalisms for which no common concept of strong generative capacity exists. As an example for a result in this direction, see Koller and Kuhlmann (2009). We have defined the dependency trees that an LCFRS induces by means of a compositional mapping on the derivations. While we would claim that compositionality is a generally desirable property, the particular notion of induction is up for discussion. In particular, our interpretation of derivations may not alway</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, Stuart M. 1985. Evidence against the context-freeness of natural language. Linguistics and Philosophy, 8(3):333–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Shieber, Stuart M., Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1–2):3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Jason Baldridge</author>
</authors>
<title>Combinatory categorial grammar.</title>
<date>2011</date>
<booktitle>Non-Transformational Syntax: Formal and Explicit Models of Grammar. Wiley-Oxford, Blackwell, chapter 5,</booktitle>
<pages>181--224</pages>
<editor>In Robert D. Borsley and Kersti B¨orjars, editors,</editor>
<contexts>
<context position="5427" citStr="Steedman and Baldridge 2011" startWordPosition="780" endWordPosition="783">, Weir, and Joshi 1987; Weir 1988). This framework was introduced to facilitate the comparison of various 1 We draw the nodes of a dependency tree as circles, and the edges as arrows pointing towards the dependent (away from the root node). Following Hays (1964), we use dotted lines to help us keep track of the positions of the nodes in the linear order, and to associate nodes with lexical items. 356 Kuhlmann Mildly Non-Projective Dependency Grammar grammar formalisms, including standard context-free grammar, tree-adjoining grammar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman and Baldridge 2011). It also comprises, among others, multiple context-free grammars (Seki et al. 1991), minimalist grammars (Michaelis 1998), and simple range concatenation grammars (Boullier 2004). The article is structured as follows. In Section 2 we provide the technical background to our work; in particular, we introduce our terminology and notation for linear context-free rewriting systems. An LCFRS generates a set of terms (formal expressions) which are interpreted as derivation trees of objects from some domain. Each term also has a secondary interpretation under which it denotes a tuple of strings, repr</context>
</contexts>
<marker>Steedman, Baldridge, 2011</marker>
<rawString>Steedman, Mark and Jason Baldridge. 2011. Combinatory categorial grammar. In Robert D. Borsley and Kersti B¨orjars, editors, Non-Transformational Syntax: Formal and Explicit Models of Grammar. Wiley-Oxford, Blackwell, chapter 5, pages 181–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale.</title>
<date>1959</date>
<location>Klinksieck, Paris.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>Tesni`ere, Lucien. 1959. ´El´ements de syntaxe structurale. Klinksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>104--111</pages>
<location>Stanford, CA.</location>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>Vijay-Shanker, K., David J. Weir, and Aravind K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics (ACL), pages 104–111, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Villemonte de la Clergerie</author>
<author>´Eric</author>
</authors>
<title>Parsing mildly context-sensitive languages with thread automata.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1--7</pages>
<location>Taipei.</location>
<marker>Clergerie, ´Eric, 2002</marker>
<rawString>Villemonte de la Clergerie, ´Eric. 2002. Parsing mildly context-sensitive languages with thread automata. In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 1–7, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>Characterizing Mildly Context-Sensitive Grammar Formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4833" citStr="Weir 1988" startWordPosition="691" endWordPosition="692">n2 sah1 (German) that Jan Piet Marie read help saw Figure 1 shows dependency trees for the two examples.1 The German linearization gives rise to a projective structure, where the verb–argument dependencies are nested within each other, whereas the Dutch linearization induces a non-projective structure with crossing edges. To account for such structures we need to turn to formalisms more expressive than Hays–Gaifman grammars. In this article we present a formalism for non-projective dependency grammar based on linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987; Weir 1988). This framework was introduced to facilitate the comparison of various 1 We draw the nodes of a dependency tree as circles, and the edges as arrows pointing towards the dependent (away from the root node). Following Hays (1964), we use dotted lines to help us keep track of the positions of the nodes in the linear order, and to associate nodes with lexical items. 356 Kuhlmann Mildly Non-Projective Dependency Grammar grammar formalisms, including standard context-free grammar, tree-adjoining grammar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman and Baldridge 2011). It a</context>
<context position="9428" citStr="Weir 1988" startWordPosition="1409" endWordPosition="1410">d well-nestedness (Bodirsky, Kuhlmann, and M¨ohl 2005), and show that its presence facilitates tractable parsing. This comes at the cost of a small loss in coverage on treebank data. Bounded block-degree and well-nestedness jointly define a class of “mildly” non-projective dependency grammars that can be parsed in polynomial time. Section 9 summarizes our main contributions and concludes the article. 357 Computational Linguistics Volume 39, Number 2 2. Technical Background We assume basic familiarity with linear context-free rewriting systems (see, e.g., VijayShanker, Weir, and Joshi 1987 and Weir 1988) and only review the terminology and notation that we use in this article. A linear context-free rewriting system (LCFRS) is a structure G = (N, E, P, S) where N is a set of nonterminals, E is a set of function symbols, P is a finite set of production rules, and S E N is a distinguished start symbol. Rules take the form A0 -+ f(A1,...,Am) (1) where f is a function symbol and the Ai are nonterminals. Rules are used for rewriting in the same way as in a context-free grammar, with the function symbols acting as terminals. The outcome of the rewriting process is a set T(G) of terms, tree-formed ex</context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>Weir, David J. 1988. Characterizing Mildly Context-Sensitive Grammar Formalisms. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>