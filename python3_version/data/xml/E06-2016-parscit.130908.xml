<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018193">
<note confidence="0.720633333333333">
The GOD model
Alfio Massimiliano Gliozzo
ITC-irst
</note>
<address confidence="0.652717">
Trento, Italy
</address>
<email confidence="0.98994">
gliozzo@itc.it
</email>
<sectionHeader confidence="0.981931" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998373888888889">
GOD (General Ontology Discovery) is an
unsupervised system to extract semantic
relations among domain specific entities
and concepts from texts. Operationally,
it acts as a search engine returning a set
of true predicates regarding the query in-
stead of the usual ranked list of relevant
documents. Our approach relies on two
basic assumptions: (i) paradigmatic rela-
tions can be established only among terms
in the same Semantic Domain an (ii) they
can be inferred from texts by analyzing the
Subject-Verb-Object patterns where two
domain specific terms co-occur. A quali-
tative analysis of the system output shows
that GOD provide true, informative and
meaningful relations in a very efficient
way.
</bodyText>
<sectionHeader confidence="0.997137" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990828320754717">
GOD (General Ontology Discovery) is an un-
supervised system to extract semantic relations
among domain specific entities and concepts from
texts. Operationally, it acts as a search engine re-
turning a set of true predicates regarding the query
instead of the usual ranked list of relevant docu-
ments. Such predicates can be perceived as a set
of semantic relations explaining the domain of the
query, i.e. a set of binary predicated involving do-
main specific entities and concepts. Entities and
concepts are referred to by domain specific terms,
and the relations among them are expressed by the
verbs of which they are arguments.
To illustrate the functionality of the system, be-
low we report an example for the query God.
god:
lord hear prayer
god is creator
god have mercy
faith reverences god
lord have mercy
jesus_christ is god
god banishing him
god commanded israelites
god was trinity
abraham believed god
god requires abraham
god supply human_need
god is holy
noah obeyed god
From a different perspective, GOD is first of all
a general system for ontology learning from texts
(Buitelaar et al., 2005). Likewise current state-
of-the-art methodologies for non-hierarchical re-
lation extraction it exploits shallow parsing tech-
niques to identify syntactic patterns involving do-
main specific entities (Reinberger et al., 2004),
and statistical association measures to detect rel-
evant relations (Ciaramita et al., 2005). In con-
trast to them, it does not require any domain spe-
cific collection of texts, allowing the user to de-
scribe the domain of interest by simply typing
short queries. This feature is of great advantage
from a practical point of view: it is obviously more
easy to formulate short queries than to collect huge
amounts of domain specific texts.
Even if, in principle, an ontology is supposed to
represent a domain by a hierarchy of concepts and
entities, in this paper we concentrate only on the
non-hyrarchical relation extraction process. In ad-
dition, in this work we do not address the problem
of associating synonyms to the same concept (e.g.
god and lord in the example above).
</bodyText>
<page confidence="0.997205">
147
</page>
<bodyText confidence="0.994337166666667">
In this paper we just concentrate on describ-
ing our general framework for ontology learning,
postponing the solution of the already mentioned
problems. The good quality of the results and the
well foundedness of the GOD framework motivate
our future work.
</bodyText>
<sectionHeader confidence="0.816541" genericHeader="method">
2 The GOD algorithm
</sectionHeader>
<bodyText confidence="0.964961037037037">
The basic assumption of the GOD model is that
paradigmatic relations can be established only
among terms in the same Semantic Domain, while
concepts belonging to different fields are mainly
unrelated (Gliozzo, 2005). Such relations can
be identified by considering Subject-Verb-Object
(SVO) patterns involving domain specific terms
(i.e. syntagmatic relations).
When a query Q = (q1, q2,. .. , qn) is formu-
lated, GOD operates as follows:
Domain Discovery Retrieve the ranked list
dom(Q) = (t1, t2,... , tk) of domain spe-
cific terms such that sim(ti, Q) &gt; 00, where
sim(Q, t) is a similarity function capturing
domain proximity and 00 is the domain
specificity threshold.
Relation Extraction For each SVO pattern in-
volving two different terms ti E dom(Q) and
tj E dom(Q) such that the term ti occurs in
the subject position and the term tj occurs in
the object position return the relation tivtj if
score(ti, v, tj) &gt; 000, where score(ti, v, tj)
measures the syntagmatic association among
ti, v and tj.
In Subsection 2.1 we describe into details the
Domain Discovery step. Subsection 2.2 is about
the relation extraction step.
</bodyText>
<subsectionHeader confidence="0.996743">
2.1 Domain Discovery
</subsectionHeader>
<bodyText confidence="0.999401722222222">
Semantic Domains (Magnini et al., 2002) are clus-
ters of very closely related concepts, lexicalized
by domain specific terms. Word senses are de-
termined and delimited only by the meanings of
other words in the same domain. Words belonging
to a limited number of domains are called domain
words. Domain words can be disambiguated by
simply identifying the domain of the text.
As a consequence, concepts belonging to dif-
ferent domains are basically unrelated. This ob-
servation is crucial from a methodological point
of view, allowing us to perform a large scale struc-
tural analysis of the whole lexicon of a language,
otherwise computationally infeasible. In fact, re-
stricting the attention to a particular domain is a
way to reduce the complexity of the overall rela-
tion extraction task, that is evidently quadratic in
the number of terms.
Domain information can be expressed by ex-
ploiting Domain Models (DMs) (Gliozzo et al.,
2005). A DM is represented by a k x k0 rectan-
gular matrix D, containing the domain relevance
for each term with respect to each domain, where
k is the cardinality of the vocabulary, and k0 is the
size of the Domain Set.
DMs can be acquired from texts in a totally
unsupervised way by exploiting a lexical coher-
ence assumption (Gliozzo, 2005). To this aim,
term clustering algorithms can be adopted: each
cluster represents a Semantic Domain. The de-
gree of association among terms and clusters, es-
timated by the learning algorithm, provides a do-
main relevance function. For our experiments we
adopted a clustering strategy based on Latent Se-
mantic Analysis, following the methodology de-
scribed in (Gliozzo, 2005). This operation is done
off-line, and can be efficiently performed on large
corpora. To filter out noise, we considered only
those terms having a frequency higher than 5 in
the corpus.
Once a DM has been defined by the matrix D,
the Domain Space is a k0 dimensional space, in
which both texts and terms are associated to Do-
main Vectors (DVs), i.e. vectors representing their
domain relevances with respect to each domain.
The DV �t0i for the term ti E V is the ith row of D,
where V = {t1, t2, ... , tkI is the vocabulary of
the corpus. The similarity among DVs in the Do-
main Space is estimated by means of the cosine
operation.
When a query Q = (q1, q2, ... , qn) is formu-
lated, its DV Q0 is estimated by
and then compared to the DVs of each term ti E V
by adopting the cosine similarity metric
</bodyText>
<equation confidence="0.856497">
sim(ti, Q) = cos(�t0i, ch (2)
</equation>
<bodyText confidence="0.9900834">
where �t0i and �
q0j are the DVs for the terms ti and
qj, respectively.
All those terms whose similarity with the query
is above the domain specificity threshold 00 are
</bodyText>
<equation confidence="0.987008166666667">
�
q0 (1)
j
n
�Q0 =
j=1
</equation>
<page confidence="0.983146">
148
</page>
<bodyText confidence="0.99992025">
then returned as an output of the function dom(Q).
Empirically, we fixed this threshold to 0.5. In gen-
eral, the higher the domain specificity threshold,
the higher the relevance of the discovered relations
for the query (see Section 3), increasing accuracy
while reducing recall. In the previous example,
dom(god) returns the terms lord, prayer, creator
and mercy, among the others.
</bodyText>
<subsectionHeader confidence="0.989116">
2.2 Relation extraction
</subsectionHeader>
<bodyText confidence="0.999704311111111">
As a second step, the system analyzes all the syn-
tagmatic relations involving the retrieved entities.
To this aim, as an off-line learning step, the sys-
tem acquires Subject-Verb-Object (SVO) patterns
from the training corpus by using regular expres-
sions on the output of a shallow parser.
In particular, GOD extracts the relations tivtj
for each ordered couple of domain specific terms
(ti, tj) such that ti ∈ dom(Q), tj ∈ dom(Q)
and score(ti, v, tj) &gt; 0&amp;quot;. The confidence score
is estimated by adopting the heuristic confidence
measure described in (Reinberger et al., 2004), re-
ported below:
where F(t) is the frequency of the term t in the
corpus, F(t, v) is the frequency of the SV pattern
involving both t and v, F(v, t) is the frequency
of the VO pattern involving both v and t, and
F(ti, v, tj) is the frequency of the SVO pattern in-
volving ti, v and tj. In general, augmenting 0&amp;quot; is a
way to filter out noisy relations, while decreasing
recall.
It is important to remark here that all the ex-
tracted predicates occur at least once in the corpus,
then they have been asserted somewhere. Even if
it is not a sufficient condition to guarantee their
truth, it is reasonable to assume that most of the
sentences in texts express true assertions.
The relation extraction process is performed on-
line for each query, then efficiency is a crucial re-
quirement in this phase. It would be preferable
to avoid an extensive search of the required SVO
patterns, because the number of sentences in the
corpus is huge. To solve this problem we adopted
an inverted relation index, consisting of three hash
tables: the SV(VO) table report, for each term,
the frequency of the SV(VO) patterns where it oc-
curs as a subject(object); the SVO table reports,
for each ordered couple of terms in the corpus,
the frequency of the SVO patterns in which they
co-occur. All the information required to estimate
Formula 3 can then be accessed in a time propor-
tional to the frequencies of the involved terms. In
general, domain specific terms are not very fre-
quent in a generic corpus, allowing a fast compu-
tation in most of the cases.
</bodyText>
<sectionHeader confidence="0.997382" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999662833333333">
Performing a rigorous evaluation of an ontology
learning process is not an easy task (Buitelaar et
al., 2005) and it is outside the goals of this paper.
Due to time constraints, we did not performed a
quantitative and objective evaluation of our sys-
tem. In Subsection 3.1 we describe the data and
the NLP tools adopted by the system. In Subsec-
tion 3.2 we comment some example of the system
output, providing a qualitative analysis of the re-
sults after having proposed some evaluation guide-
lines. Finally, in Subsection 3.3 we discuss issues
related to the recall of the system.
</bodyText>
<subsectionHeader confidence="0.997821">
3.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9999836">
To expect high coverage, the system would be
trained on WEB scale corpora. On the other hand,
the analysis of very large corpora needs efficient
preprocessing tools and optimized memory allo-
cation strategies. For the experiments reported in
this paper we adopted the British National Cor-
pus (BNC-Consortium, 2000), and we parsed each
sentence by exploiting a shallow parser on the out-
put of which we detected SVO patterns by means
of regular expressions1.
</bodyText>
<subsectionHeader confidence="0.999213">
3.2 Accuracy
</subsectionHeader>
<bodyText confidence="0.9997274">
Once a query has been formulated, and a set of
relations has been extracted, it is not clear how to
evaluate the quality of the results. The first four
columns of the example below show the evaluation
we did for the query Karl Marx.
</bodyText>
<table confidence="0.991382">
Karl Marx:
TRIM economic_organisation determines superstructure
TRUM capitalism needs capitalists
FRIM proletariat overthrow bourgeoisie
TRIM marx understood capitalism
???E marx later marxists
TRIM labour_power be production
TRIM societies are class_societies
?RIM private_property equals exploitation
TRIM primitive_societies were classless
TRIM social_relationships form economic_basis
TRIM max_weber criticised marxist_view
</table>
<footnote confidence="0.705557333333333">
1For the experiments reported in this paper we used a
memory-based shallow parser developed at CNTS Antwerp
and ILK Tilburg (Daelemans et al., 1999) together with a set
</footnote>
<bodyText confidence="0.6186215">
of scripts to extract SVO patterns (Reinberger et al., 2004)
kindly put at our disposal by the authors.
</bodyText>
<equation confidence="0.9957504">
F(ti,v,tj)
score(ti, v, tj) =
F(ti,v) F(v,tj) (3)
F(ti) + F(tj)
min(F(ti),F(tj))
</equation>
<page confidence="0.998204">
149
</page>
<table confidence="0.9506242">
TRIM contradictions legitimizes class_structure
?R?E societies is political_level
?R?E class_society where false_consciousness
?RUE social_system containing such_contradictions
TRIM human_societies organizing production
</table>
<bodyText confidence="0.9994696">
Several aspects are addressed: truthfulness (i.e.
True vs. False in the first column), relevance
for the query (i.e. Relevant vs. Not-relevant in
the second column), information content (i.e. In-
formative vs. Uninformative, third column) and
meaningfulness (i.e. Meaningful vs. Error, fourth
column). For most of the test queries, the majority
of the retrieved predicates were true, relevant, in-
formative and meaningful, confirming the quality
of the acquired DM and the validity of the relation
extraction technique2.
From the BNC, GOD was able to extract good
quality information for many different queries in
very different domains, as for example music,
unix, painting and many others.
</bodyText>
<subsectionHeader confidence="0.941262">
3.3 Recall
</subsectionHeader>
<bodyText confidence="0.999943909090909">
An interesting aspect of the behavior of the system
is that if the domain of the query is not well rep-
resented in the corpus, the domain discovery step
retrieves few domain specific terms. As a conse-
quece, just few relations (and sometimes no re-
lations) have been retrieved for most of our test
queries. An analysis of such cases showed that the
low recall was mainly due to the low coverage of
the BNC corpus. We believe that this problem can
be avoided by training the system on larger scale
corpora (e.g. from the Web).
</bodyText>
<sectionHeader confidence="0.931677" genericHeader="conclusions">
4 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.989814153846154">
In this paper we reported the preliminary results
we obtained from the development of GOD, a
system that dynamically acquires ontologies from
texts. In the GOD model, the required domain is
formulated by typing short queries in an Informa-
tion Retrieval style. The system is efficient and
accurate, even if the small size of the corpus pre-
vented us from acquiring domain ontologies for
many queries. For the future, we plan to evaluate
the system in a more rigorous way, by contrast-
ing its output to hand made reference ontologies
for different domains. To improve the coverage of
the system, we are going to train it on WEB scale
2It is worthwhile to remark here that evaluation strongly
depends on the point of view from which the query has
been formulated. For example, the predicate private property
equals exploitation is true in the Marxist view, while it is ob-
viously false with respect to the present economic system.
text collections and to explore the use of super-
vised relation extraction techniques. In addition,
we are improving relation extraction by adopting
a more sophisticated syntactic analisys (e.g. Se-
matic Role Labeling). Finally, we plan to explore
the usefulness of the extracted relations into NLP
systems for Question Answering, Information Ex-
traction and Semantic Entailment.
</bodyText>
<sectionHeader confidence="0.99605" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999939666666667">
This work has been supported by the ONTOTEXT
project, funded by the Autonomous Province of
Trento under the FUP-2004 research program.
Most of the experiments have been performed
during my research stage at the University of
Antwerp. Thanks to Walter Daelemans and Carlo
Strapparava for useful suggestions and comments
and to Marie-Laure Reinberger for having pro-
vided the SVO extraction scripts.
</bodyText>
<sectionHeader confidence="0.997993" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999588962962963">
BNC-Consortium. 2000. British national corpus.
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. On-
tology learning from texts: methods, evaluation and
applications. IOS Press.
M. Ciaramita, A. Gangemi, E. Ratsch, J. Saric, and
I. Rojas. 2005. Unsupervised learning of seman-
tic relations between concepts of a molecular biol-
ogy ontology. In In proceedings of IJCAI-05, Edim-
burgh, Scotland.
W. Daelemans, S. Buchholz, and J. Veenstra. 1999.
Memory-based shallow parsing. In Proceedings of
CoNLL-99.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005.
Domain kernels for word sense disambiguation. In
Proceedings ofACL-05, pages 403–410, Ann Arbor,
Michigan.
A. Gliozzo. 2005. Semantic Domains in Compu-
tational Linguistics. Ph.D. thesis, University of
Trento.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain information
in word sense disambiguation. Natural Language
Engineering, 8(4):359–373.
M.L. Reinberger, P. Spyns, A. J. Pretorius, and
W. Daelemans. 2004. Automatic initiation of an on-
tology. In Proceedings of ODBase‘04, pages 600–
617. Springer-Verlag.
</reference>
<page confidence="0.998304">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.405211">
<title confidence="0.998332">The GOD model</title>
<author confidence="0.998206">Alfio Massimiliano Gliozzo</author>
<email confidence="0.505164">ITC-irst</email>
<address confidence="0.911705">Trento, Italy</address>
<email confidence="0.998882">gliozzo@itc.it</email>
<abstract confidence="0.995377947368421">GOD (General Ontology Discovery) is an unsupervised system to extract semantic relations among domain specific entities and concepts from texts. Operationally, it acts as a search engine returning a set of true predicates regarding the query instead of the usual ranked list of relevant documents. Our approach relies on two basic assumptions: (i) paradigmatic relations can be established only among terms in the same Semantic Domain an (ii) they can be inferred from texts by analyzing the Subject-Verb-Object patterns where two domain specific terms co-occur. A qualitative analysis of the system output shows that GOD provide true, informative and meaningful relations in a very efficient way.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>BNC-Consortium</author>
</authors>
<date>2000</date>
<note>British national corpus.</note>
<contexts>
<context position="10432" citStr="BNC-Consortium, 2000" startWordPosition="1765" endWordPosition="1766"> the NLP tools adopted by the system. In Subsection 3.2 we comment some example of the system output, providing a qualitative analysis of the results after having proposed some evaluation guidelines. Finally, in Subsection 3.3 we discuss issues related to the recall of the system. 3.1 Experimental Settings To expect high coverage, the system would be trained on WEB scale corpora. On the other hand, the analysis of very large corpora needs efficient preprocessing tools and optimized memory allocation strategies. For the experiments reported in this paper we adopted the British National Corpus (BNC-Consortium, 2000), and we parsed each sentence by exploiting a shallow parser on the output of which we detected SVO patterns by means of regular expressions1. 3.2 Accuracy Once a query has been formulated, and a set of relations has been extracted, it is not clear how to evaluate the quality of the results. The first four columns of the example below show the evaluation we did for the query Karl Marx. Karl Marx: TRIM economic_organisation determines superstructure TRUM capitalism needs capitalists FRIM proletariat overthrow bourgeoisie TRIM marx understood capitalism ???E marx later marxists TRIM labour_power</context>
</contexts>
<marker>BNC-Consortium, 2000</marker>
<rawString>BNC-Consortium. 2000. British national corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>P Cimiano</author>
<author>B Magnini</author>
</authors>
<title>Ontology learning from texts: methods, evaluation and applications.</title>
<date>2005</date>
<publisher>IOS Press.</publisher>
<contexts>
<context position="1906" citStr="Buitelaar et al., 2005" startWordPosition="302" endWordPosition="305">ities and concepts are referred to by domain specific terms, and the relations among them are expressed by the verbs of which they are arguments. To illustrate the functionality of the system, below we report an example for the query God. god: lord hear prayer god is creator god have mercy faith reverences god lord have mercy jesus_christ is god god banishing him god commanded israelites god was trinity abraham believed god god requires abraham god supply human_need god is holy noah obeyed god From a different perspective, GOD is first of all a general system for ontology learning from texts (Buitelaar et al., 2005). Likewise current stateof-the-art methodologies for non-hierarchical relation extraction it exploits shallow parsing techniques to identify syntactic patterns involving domain specific entities (Reinberger et al., 2004), and statistical association measures to detect relevant relations (Ciaramita et al., 2005). In contrast to them, it does not require any domain specific collection of texts, allowing the user to describe the domain of interest by simply typing short queries. This feature is of great advantage from a practical point of view: it is obviously more easy to formulate short queries</context>
<context position="9624" citStr="Buitelaar et al., 2005" startWordPosition="1629" endWordPosition="1632">VO) table report, for each term, the frequency of the SV(VO) patterns where it occurs as a subject(object); the SVO table reports, for each ordered couple of terms in the corpus, the frequency of the SVO patterns in which they co-occur. All the information required to estimate Formula 3 can then be accessed in a time proportional to the frequencies of the involved terms. In general, domain specific terms are not very frequent in a generic corpus, allowing a fast computation in most of the cases. 3 Evaluation Performing a rigorous evaluation of an ontology learning process is not an easy task (Buitelaar et al., 2005) and it is outside the goals of this paper. Due to time constraints, we did not performed a quantitative and objective evaluation of our system. In Subsection 3.1 we describe the data and the NLP tools adopted by the system. In Subsection 3.2 we comment some example of the system output, providing a qualitative analysis of the results after having proposed some evaluation guidelines. Finally, in Subsection 3.3 we discuss issues related to the recall of the system. 3.1 Experimental Settings To expect high coverage, the system would be trained on WEB scale corpora. On the other hand, the analysi</context>
</contexts>
<marker>Buitelaar, Cimiano, Magnini, 2005</marker>
<rawString>P. Buitelaar, P. Cimiano, and B. Magnini. 2005. Ontology learning from texts: methods, evaluation and applications. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>A Gangemi</author>
<author>E Ratsch</author>
<author>J Saric</author>
<author>I Rojas</author>
</authors>
<title>Unsupervised learning of semantic relations between concepts of a molecular biology ontology. In</title>
<date>2005</date>
<booktitle>In proceedings of IJCAI-05, Edimburgh,</booktitle>
<location>Scotland.</location>
<contexts>
<context position="2218" citStr="Ciaramita et al., 2005" startWordPosition="344" endWordPosition="347">lord have mercy jesus_christ is god god banishing him god commanded israelites god was trinity abraham believed god god requires abraham god supply human_need god is holy noah obeyed god From a different perspective, GOD is first of all a general system for ontology learning from texts (Buitelaar et al., 2005). Likewise current stateof-the-art methodologies for non-hierarchical relation extraction it exploits shallow parsing techniques to identify syntactic patterns involving domain specific entities (Reinberger et al., 2004), and statistical association measures to detect relevant relations (Ciaramita et al., 2005). In contrast to them, it does not require any domain specific collection of texts, allowing the user to describe the domain of interest by simply typing short queries. This feature is of great advantage from a practical point of view: it is obviously more easy to formulate short queries than to collect huge amounts of domain specific texts. Even if, in principle, an ontology is supposed to represent a domain by a hierarchy of concepts and entities, in this paper we concentrate only on the non-hyrarchical relation extraction process. In addition, in this work we do not address the problem of a</context>
</contexts>
<marker>Ciaramita, Gangemi, Ratsch, Saric, Rojas, 2005</marker>
<rawString>M. Ciaramita, A. Gangemi, E. Ratsch, J. Saric, and I. Rojas. 2005. Unsupervised learning of semantic relations between concepts of a molecular biology ontology. In In proceedings of IJCAI-05, Edimburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>S Buchholz</author>
<author>J Veenstra</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of CoNLL-99.</booktitle>
<contexts>
<context position="11397" citStr="Daelemans et al., 1999" startWordPosition="1904" endWordPosition="1907">the evaluation we did for the query Karl Marx. Karl Marx: TRIM economic_organisation determines superstructure TRUM capitalism needs capitalists FRIM proletariat overthrow bourgeoisie TRIM marx understood capitalism ???E marx later marxists TRIM labour_power be production TRIM societies are class_societies ?RIM private_property equals exploitation TRIM primitive_societies were classless TRIM social_relationships form economic_basis TRIM max_weber criticised marxist_view 1For the experiments reported in this paper we used a memory-based shallow parser developed at CNTS Antwerp and ILK Tilburg (Daelemans et al., 1999) together with a set of scripts to extract SVO patterns (Reinberger et al., 2004) kindly put at our disposal by the authors. F(ti,v,tj) score(ti, v, tj) = F(ti,v) F(v,tj) (3) F(ti) + F(tj) min(F(ti),F(tj)) 149 TRIM contradictions legitimizes class_structure ?R?E societies is political_level ?R?E class_society where false_consciousness ?RUE social_system containing such_contradictions TRIM human_societies organizing production Several aspects are addressed: truthfulness (i.e. True vs. False in the first column), relevance for the query (i.e. Relevant vs. Not-relevant in the second column), info</context>
</contexts>
<marker>Daelemans, Buchholz, Veenstra, 1999</marker>
<rawString>W. Daelemans, S. Buchholz, and J. Veenstra. 1999. Memory-based shallow parsing. In Proceedings of CoNLL-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Giuliano</author>
<author>C Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-05,</booktitle>
<pages>403--410</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="5257" citStr="Gliozzo et al., 2005" startWordPosition="851" endWordPosition="854">n be disambiguated by simply identifying the domain of the text. As a consequence, concepts belonging to different domains are basically unrelated. This observation is crucial from a methodological point of view, allowing us to perform a large scale structural analysis of the whole lexicon of a language, otherwise computationally infeasible. In fact, restricting the attention to a particular domain is a way to reduce the complexity of the overall relation extraction task, that is evidently quadratic in the number of terms. Domain information can be expressed by exploiting Domain Models (DMs) (Gliozzo et al., 2005). A DM is represented by a k x k0 rectangular matrix D, containing the domain relevance for each term with respect to each domain, where k is the cardinality of the vocabulary, and k0 is the size of the Domain Set. DMs can be acquired from texts in a totally unsupervised way by exploiting a lexical coherence assumption (Gliozzo, 2005). To this aim, term clustering algorithms can be adopted: each cluster represents a Semantic Domain. The degree of association among terms and clusters, estimated by the learning algorithm, provides a domain relevance function. For our experiments we adopted a clu</context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>A. Gliozzo, C. Giuliano, and C. Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings ofACL-05, pages 403–410, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
</authors>
<date>2005</date>
<booktitle>Semantic Domains in Computational Linguistics. Ph.D. thesis,</booktitle>
<institution>University of Trento.</institution>
<contexts>
<context position="3395" citStr="Gliozzo, 2005" startWordPosition="544" endWordPosition="545">rk we do not address the problem of associating synonyms to the same concept (e.g. god and lord in the example above). 147 In this paper we just concentrate on describing our general framework for ontology learning, postponing the solution of the already mentioned problems. The good quality of the results and the well foundedness of the GOD framework motivate our future work. 2 The GOD algorithm The basic assumption of the GOD model is that paradigmatic relations can be established only among terms in the same Semantic Domain, while concepts belonging to different fields are mainly unrelated (Gliozzo, 2005). Such relations can be identified by considering Subject-Verb-Object (SVO) patterns involving domain specific terms (i.e. syntagmatic relations). When a query Q = (q1, q2,. .. , qn) is formulated, GOD operates as follows: Domain Discovery Retrieve the ranked list dom(Q) = (t1, t2,... , tk) of domain specific terms such that sim(ti, Q) &gt; 00, where sim(Q, t) is a similarity function capturing domain proximity and 00 is the domain specificity threshold. Relation Extraction For each SVO pattern involving two different terms ti E dom(Q) and tj E dom(Q) such that the term ti occurs in the subject p</context>
<context position="5593" citStr="Gliozzo, 2005" startWordPosition="915" endWordPosition="916"> In fact, restricting the attention to a particular domain is a way to reduce the complexity of the overall relation extraction task, that is evidently quadratic in the number of terms. Domain information can be expressed by exploiting Domain Models (DMs) (Gliozzo et al., 2005). A DM is represented by a k x k0 rectangular matrix D, containing the domain relevance for each term with respect to each domain, where k is the cardinality of the vocabulary, and k0 is the size of the Domain Set. DMs can be acquired from texts in a totally unsupervised way by exploiting a lexical coherence assumption (Gliozzo, 2005). To this aim, term clustering algorithms can be adopted: each cluster represents a Semantic Domain. The degree of association among terms and clusters, estimated by the learning algorithm, provides a domain relevance function. For our experiments we adopted a clustering strategy based on Latent Semantic Analysis, following the methodology described in (Gliozzo, 2005). This operation is done off-line, and can be efficiently performed on large corpora. To filter out noise, we considered only those terms having a frequency higher than 5 in the corpus. Once a DM has been defined by the matrix D, </context>
</contexts>
<marker>Gliozzo, 2005</marker>
<rawString>A. Gliozzo. 2005. Semantic Domains in Computational Linguistics. Ph.D. thesis, University of Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>C Strapparava</author>
<author>G Pezzulo</author>
<author>A Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="4366" citStr="Magnini et al., 2002" startWordPosition="704" endWordPosition="707">sim(Q, t) is a similarity function capturing domain proximity and 00 is the domain specificity threshold. Relation Extraction For each SVO pattern involving two different terms ti E dom(Q) and tj E dom(Q) such that the term ti occurs in the subject position and the term tj occurs in the object position return the relation tivtj if score(ti, v, tj) &gt; 000, where score(ti, v, tj) measures the syntagmatic association among ti, v and tj. In Subsection 2.1 we describe into details the Domain Discovery step. Subsection 2.2 is about the relation extraction step. 2.1 Domain Discovery Semantic Domains (Magnini et al., 2002) are clusters of very closely related concepts, lexicalized by domain specific terms. Word senses are determined and delimited only by the meanings of other words in the same domain. Words belonging to a limited number of domains are called domain words. Domain words can be disambiguated by simply identifying the domain of the text. As a consequence, concepts belonging to different domains are basically unrelated. This observation is crucial from a methodological point of view, allowing us to perform a large scale structural analysis of the whole lexicon of a language, otherwise computationall</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(4):359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Reinberger</author>
<author>P Spyns</author>
<author>A J Pretorius</author>
<author>W Daelemans</author>
</authors>
<title>Automatic initiation of an ontology.</title>
<date>2004</date>
<booktitle>In Proceedings of ODBase‘04,</booktitle>
<pages>600--617</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="2126" citStr="Reinberger et al., 2004" startWordPosition="331" endWordPosition="334"> for the query God. god: lord hear prayer god is creator god have mercy faith reverences god lord have mercy jesus_christ is god god banishing him god commanded israelites god was trinity abraham believed god god requires abraham god supply human_need god is holy noah obeyed god From a different perspective, GOD is first of all a general system for ontology learning from texts (Buitelaar et al., 2005). Likewise current stateof-the-art methodologies for non-hierarchical relation extraction it exploits shallow parsing techniques to identify syntactic patterns involving domain specific entities (Reinberger et al., 2004), and statistical association measures to detect relevant relations (Ciaramita et al., 2005). In contrast to them, it does not require any domain specific collection of texts, allowing the user to describe the domain of interest by simply typing short queries. This feature is of great advantage from a practical point of view: it is obviously more easy to formulate short queries than to collect huge amounts of domain specific texts. Even if, in principle, an ontology is supposed to represent a domain by a hierarchy of concepts and entities, in this paper we concentrate only on the non-hyrarchic</context>
<context position="7967" citStr="Reinberger et al., 2004" startWordPosition="1331" endWordPosition="1334"> mercy, among the others. 2.2 Relation extraction As a second step, the system analyzes all the syntagmatic relations involving the retrieved entities. To this aim, as an off-line learning step, the system acquires Subject-Verb-Object (SVO) patterns from the training corpus by using regular expressions on the output of a shallow parser. In particular, GOD extracts the relations tivtj for each ordered couple of domain specific terms (ti, tj) such that ti ∈ dom(Q), tj ∈ dom(Q) and score(ti, v, tj) &gt; 0&amp;quot;. The confidence score is estimated by adopting the heuristic confidence measure described in (Reinberger et al., 2004), reported below: where F(t) is the frequency of the term t in the corpus, F(t, v) is the frequency of the SV pattern involving both t and v, F(v, t) is the frequency of the VO pattern involving both v and t, and F(ti, v, tj) is the frequency of the SVO pattern involving ti, v and tj. In general, augmenting 0&amp;quot; is a way to filter out noisy relations, while decreasing recall. It is important to remark here that all the extracted predicates occur at least once in the corpus, then they have been asserted somewhere. Even if it is not a sufficient condition to guarantee their truth, it is reasonable</context>
<context position="11478" citStr="Reinberger et al., 2004" startWordPosition="1918" endWordPosition="1921">tion determines superstructure TRUM capitalism needs capitalists FRIM proletariat overthrow bourgeoisie TRIM marx understood capitalism ???E marx later marxists TRIM labour_power be production TRIM societies are class_societies ?RIM private_property equals exploitation TRIM primitive_societies were classless TRIM social_relationships form economic_basis TRIM max_weber criticised marxist_view 1For the experiments reported in this paper we used a memory-based shallow parser developed at CNTS Antwerp and ILK Tilburg (Daelemans et al., 1999) together with a set of scripts to extract SVO patterns (Reinberger et al., 2004) kindly put at our disposal by the authors. F(ti,v,tj) score(ti, v, tj) = F(ti,v) F(v,tj) (3) F(ti) + F(tj) min(F(ti),F(tj)) 149 TRIM contradictions legitimizes class_structure ?R?E societies is political_level ?R?E class_society where false_consciousness ?RUE social_system containing such_contradictions TRIM human_societies organizing production Several aspects are addressed: truthfulness (i.e. True vs. False in the first column), relevance for the query (i.e. Relevant vs. Not-relevant in the second column), information content (i.e. Informative vs. Uninformative, third column) and meaningful</context>
</contexts>
<marker>Reinberger, Spyns, Pretorius, Daelemans, 2004</marker>
<rawString>M.L. Reinberger, P. Spyns, A. J. Pretorius, and W. Daelemans. 2004. Automatic initiation of an ontology. In Proceedings of ODBase‘04, pages 600– 617. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>