<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009425">
<title confidence="0.997734">
Authorship Attribution with Author-aware Topic Models
</title>
<author confidence="0.997698">
Yanir Seroussi Fabian Bohnert Ingrid Zukerman
</author>
<affiliation confidence="0.999649">
Faculty of Information Technology, Monash University
</affiliation>
<address confidence="0.698548">
Clayton, Victoria 3800, Australia
</address>
<email confidence="0.998267">
firstname.lastname@monash.edu
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999537285714286">
Authorship attribution deals with identifying
the authors of anonymous texts. Building on
our earlier finding that the Latent Dirichlet Al-
location (LDA) topic model can be used to
improve authorship attribution accuracy, we
show that employing a previously-suggested
Author-Topic (AT) model outperforms LDA
when applied to scenarios with many authors.
In addition, we define a model that combines
LDA and AT by representing authors and doc-
uments over two disjoint topic sets, and show
that our model outperforms LDA, AT and sup-
port vector machines on datasets with many
authors.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950266666667">
Authorship attribution (AA) has attracted much at-
tention due to its many applications in, e.g., com-
puter forensics, criminal law, military intelligence,
and humanities research (Stamatatos, 2009). The
traditional problem, which is the focus of our work,
is to attribute test texts of unknown authorship to
one of a set of known authors, whose training texts
are supplied in advance (i.e., a supervised classifi-
cation problem). While most of the early work on
AA focused on formal texts with only a few pos-
sible authors, researchers have recently turned their
attention to informal texts and tens to thousands of
authors (Koppel et al., 2011). In parallel, topic mod-
els have gained popularity as a means of analysing
such large text corpora (Blei, 2012). In (Seroussi et
al., 2011), we showed that methods based on Latent
Dirichlet Allocation (LDA) – a popular topic model
by Blei et al. (2003) – yield good AA performance.
However, LDA does not model authors explicitly,
and we are not aware of any previous studies that
apply author-aware topic models to traditional AA.
This paper aims to address this gap.
In addition to being the first (to the best of
our knowledge) to apply Rosen-Zvi et al.’s (2004)
Author-Topic Model (AT) to traditional AA, the
main contribution of this paper is our Disjoint
Author-Document Topic Model (DADT), which ad-
dresses AT’s limitations in the context of AA. We
show that DADT outperforms AT, LDA, and linear
support vector machines on AA with many authors.
</bodyText>
<sectionHeader confidence="0.993653" genericHeader="method">
2 Disjoint Author-Document Topic Model
</sectionHeader>
<bodyText confidence="0.999618210526316">
Background. Our definition of DADT is motivated
by the observation that when authors write texts on
the same issue, specific words must be used (e.g.,
texts about LDA are likely to contain the words
“topic” and “prior”), while other words vary in fre-
quency according to author style. Also, texts by the
same author share similar style markers, indepen-
dently of content (Koppel et al., 2009). DADT aims
to separate document words from author words by
generating them from two disjoint topic sets of T (D)
document topics and T (A) author topics.
Lacoste-Julien et al. (2008) and Ramage et al.
(2009) (among others) also used disjoint topic sets
to represent document labels, and Chemudugunta
et al. (2006) separated corpus-level topics from
document-specific words. However, we are unaware
of any applications of these ideas to AA. The clos-
est work we know of is by Mimno and McCallum
(2008), whose DMR model outperformed AT in AA
</bodyText>
<page confidence="0.972909">
264
</page>
<note confidence="0.9021315">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 264–269,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999723">
Figure 1: The Disjoint Author-Document Topic Model
</figureCaption>
<bodyText confidence="0.980652941176471">
of multi-authored texts (DMR does not use disjoint
topic sets). We use AT rather than DMR, since we
found that AT outperforms DMR in AA of single-
authored texts, which are the focus of this paper.
The Model. Figure 1 shows DADT’s graphical rep-
resentation, with document-related parameters on
the left (the LDA component), and author-related
parameters on the right (the AT component). We de-
fine the model for single-authored texts, but it can be
easily extended to multi-authored texts.
The generative process for DADT is described be-
low. We use D and C to denote the Dirichlet and
categorical distributions respectively, and A, D and
V to denote the number of authors, documents, and
unique vocabulary words respectively. In addition,
we mark each step as coming from either LDA or
AT, or as new in DADT.
</bodyText>
<equation confidence="0.960845384615385">
Global level:
L. For each document topic t, draw a word dis-
tribution φ(D)
t — D (β(D)), where β(D) is a
length-V vector.
A. For each author topic t, draw a word distribu-
tion φ(A)
t — D (β(A)), where β(A) is a length-
V vector.
A. For each author a, draw the author topic dis-
tribution θ(A)
a — D (α(A)), where α(A) is a
length-T(A) vector.
</equation>
<bodyText confidence="0.952160634615385">
D. Draw a distribution over authors χ — D (η),
where η is a length-A vector.
Document level: For each document d:
L. Draw d’s topic distribution θ(D)
d — D (α(D)),
where α(D) is a length-T(D) vector.
D. Draw d’s author ad — C (χ).
D. Draw d’s topic ratio 7rd — Beta (S(A), S(D)),
where S(A) and S(D) are scalars.
Word level: For each word index i in document d:
D. Draw di’s topic indicator ydi — Bernoulli(7rd).
L. If ydi = 0, draw a document topic zdi —
C(θ(D)) and word wdi — C(φ(D)).
di
A. If ydi = 1, draw an author topic zdi — C (θ (d )
and word wdi — C(φ(A)).
zdi
DADT versus AT. DADT might seem similar to
AT with “fictitious” authors, as described by Rosen-
Zvi et al. (2010) (i.e., AT trained with an additional
unique “fictitious” author for each document, allow-
ing it to adapt to individual documents and not only
to authors). However, there are several key differ-
ences between DADT and AT.
First, in DADT author topics are disjoint from
document topics, with different priors for each topic
set. Thus, the number of author topics can be differ-
ent from the number of document topics, enabling
us to vary the number of author topics according to
the number of authors in the corpus.
Second, DADT places different priors on the
word distributions for author topics and document
topics (β(A) and β(D) respectively). Stopwords are
known to be strong indicators of authorship (Kop-
pel et al., 2009), and DADT allows us to use this
knowledge by assigning higher weights to the ele-
ments of β(A) that correspond to stopwords than to
such elements in β(D).
Third, DADT learns the ratio between document
words and author words on a per-document basis,
and makes it possible to specify a prior belief of
what this ratio should be. We found that specify-
ing a prior belief that about 80% of each document
is composed of author words yielded better results
than using AT’s approach, which evenly splits each
document into author and document words.
Fourth, DADT defines the process that generates
authors. This allows us to consider the number
of texts by each author when performing AA. This
also enables the potential use of DADT in a semi-
supervised setting by training on unlabelled texts,
which we plan to explore in the future.
</bodyText>
<sectionHeader confidence="0.993769" genericHeader="method">
3 Authorship Attribution Methods
</sectionHeader>
<bodyText confidence="0.999808">
We experimented with the following AA methods,
using token frequency features, which are good pre-
dictors of authorship (Koppel et al., 2009).
</bodyText>
<figure confidence="0.996424464285714">
±
( )
1/4d
±
( )
77
ydi
ad X
D
)
a(
ed( )
zdi
ea( )
a( )
D
A
T
� (
fit( )
�( )
( )
wdi
d
D
) fit( )
T
( )
</figure>
<page confidence="0.982754">
265
</page>
<bodyText confidence="0.991164024390244">
Baseline: Support Vector Machines (SVMs).
Koppel et al. (2009) showed that SVMs yield
good AA performance. We use linear SVMs in a
one-versus-all setup, as implemented in LIBLIN-
EAR (Fan et al., 2008), reporting results obtained
with the best cost parameter values.
Baseline: LDA + Hellinger (LDA-H). This ap-
proach uses the Hellinger distances of topic dis-
tributions to assign test texts to the closest author.
In (Seroussi et al., 2011), we experimented with two
variants: (1) each author’s texts are concatenated be-
fore building the LDA model; and (2) no concate-
nation is performed. We found that the latter ap-
proach performs poorly in cases with many candi-
date authors. Hence, we use only the former ap-
proach in this paper. Note that when dealing with
single-authored texts, concatenating each author’s
texts yields an LDA model that is equivalent to AT.
AT. Given an inferred AT model (Rosen-Zvi et al.,
2004), we calculate the probability of the test text
words for each author a, assuming it was written
by a, and return the most probable author. We do not
know of any other studies that used AT in this man-
ner for single-authored AA. We expect this method
to outperform LDA-H as it employs AT directly,
rather than relying on an external distance measure.
AT-FA. Same as AT, but built with an additional
unique “fictitious” author for each document.
DADT. Given our DADT model, we assume that the
test text was written by a “new” author, and infer
this author’s topic distribution, the author/document
topic ratio, and the document topic distribution. We
then calculate the probability of each author given
the model’s parameters, the test text words, and the
inferred author/document topic ratio and document
topic distribution. The most probable author is re-
turned. We use this method to avoid inferring the
document-dependent parameters separately for each
author, which is infeasible when many authors ex-
ist. A version that marginalises over these parame-
ters will be explored in future work.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.98986866">
We compare the performance of the methods on
two publicly-available datasets: (1) PAN’11: emails
with 72 authors (Argamon and Juola, 2011);
and (2) Blog: blogs with 19,320 authors (Schler et
al., 2006). These datasets represent realistic scenar-
ios of AA of user-generated texts with many can-
didate authors. For example, Chaski (2005) notes
a case where an employee who was terminated for
sending a racist email claimed that any person with
access to his computer could have sent the email.
Experimental Setup. Experiments on the PAN’11
dataset followed the setup of the PAN’11 competi-
tion (Argamon and Juola, 2011): We trained all the
methods on the given training subset, tuned the pa-
rameters according to the results on the given valida-
tion subset, and ran the tuned methods on the given
testing subset. In the Blog experiments, we used ten-
fold cross validation as in (Seroussi et al., 2011).
We used collapsed Gibbs sampling to train all the
topic models (Griffiths and Steyvers, 2004), run-
ning 4 chains with a burn-in of 1,000 iterations. In
the PAN’11 experiments, we retained 8 samples per
chain with spacing of 100 iterations. In the Blog
experiments, we retained 1 sample per chain due to
runtime constraints. Since we cannot average topic
distribution estimates obtained from training sam-
ples due to topic exchangeability (Steyvers and Grif-
fiths, 2007), we averaged the distances and probabil-
ities calculated from the retained samples. For test
text sampling, we used a burn-in of 100 iterations
and averaged the parameter estimates over the next
100 iterations in a similar manner to Rosen-Zvi et
al. (2010). We found that these settings yield stable
results across different random seed values.
We found that the number of topics has a larger
impact on accuracy than other configurable pa-
rameters. Hence, we used symmetric topic pri-
ors, setting all the elements of α(D) and α(A)
to min{0.1, 5/T (D)} and min{0.1, 5/T(A)} respec-
tively.1 For all models, we set Q,,, = 0.01 for each
word w as the base measure for the prior of words in
topics. Since DADT allows us to encode our prior
knowledge that stopword use is indicative of author-
ship, we set Q(D)
� = 0.01 − c and Q(A)
� = 0.01 + c
for all w, where w is a stopword.2 We set c = 0.009,
which improved accuracy by up to one percentage
point over using c = 0. Finally, we set S(A) = 4.889
and S(D) = 1.222 for DADT. This encodes our prior
</bodyText>
<footnote confidence="0.99768975">
1We tested Wallach et al.’s (2009) method of obtaining
asymmetric priors, but found that it did not improve accuracy.
2We used the stopword list from www.lextek.com/
manuals/onix/stopwords2.html.
</footnote>
<page confidence="0.992723">
266
</page>
<table confidence="0.999910285714286">
Method PAN’11 PAN’11 Blog Blog
Validation Testing Prolific Full
SVM 48.61% 53.31% 33.31% 24.13%
LDA-H 34.95% 42.62% 21.61% 7.94%
AT 46.68% 53.08% 37.56% 23.03%
AT-FA 20.68% 24.23% — —
DADT 54.24% 59.08% 42.51% 27.63%
</table>
<tableCaption confidence="0.999846">
Table 1: Experiment results
</tableCaption>
<bodyText confidence="0.999982648648649">
belief that 0.8 f 0.15 of each document is com-
posed of author words. We found that this yields
better results than an uninformed uniform prior of
S(A) = S(D) = 1(Seroussi et al., 2012). In addition,
we set ,% = 1 for each author a, yielding smoothed
estimates for the corpus distribution of authors X.
To fairly compare the topic-based methods, we
used the same overall number of topics for all the
topic models. We present only the results obtained
with the best topic settings: 100 for PAN’11 and 400
for Blog, with DADT’s author/document topic splits
being 90/10 for PAN’11, and 390/10 for Blog. These
splits allow DADT to de-noise the author represen-
tations by allocating document words to a relatively
small number of document topics. It is worth not-
ing that AT can be seen as an extreme version of
DADT, where all the topics are author topics. A fu-
ture extension is to learn the topic balance automat-
ically, e.g., in a similar manner to Teh et al.’s (2006)
method of inferring the number of topics in LDA.
Results. Table 1 shows the results of our experi-
ments in terms of classification accuracy (i.e., the
percentage of test texts correctly attributed to their
author). The PAN’11 results are shown for the val-
idation and testing subsets, and the Blog results are
shown for a subset containing the 1,000 most prolific
authors and for the full dataset of 19,320 authors.
Our DADT model yielded the best results in all
cases (the differences between DADT and the other
methods are statistically significant according to a
paired two-tailed t-test with p &lt; 0.05). We attribute
DADT’s superior performance to the de-noising ef-
fect of the disjoint topic sets, which appear to yield
author representations of higher predictive quality
than those of the other models.
As expected, AT significantly outperformed
LDA-H. On the other hand, AT-FA performed much
worse than all the other methods on PAN’11, prob-
ably because of the inherent noisiness in using the
same topics to model both authors and documents.
Hence, we did not run AT-FA on the Blog dataset.
DADT’s PAN’11 testing result is close to the
third-best accuracy from the PAN’11 competi-
tion (Argamon and Juola, 2011). However, to the
best of our knowledge, DADT obtained the best
accuracy for a fully-supervised method that uses
only unigram features. Specifically, Kourtis and
Stamatatos (2011), who obtained the highest accu-
racy (65.8%), assumed that all the test texts are
given to the classifier at the same time, and used
this additional information with a semi-supervised
method; while Kern et al. (2011) and Tanguy et al.
(2011), who obtained the second-best (64.2%) and
third-best (59.4%) accuracies respectively, used var-
ious feature types (e.g., features obtained from parse
trees). Further, preprocessing differences make it
hard to compare the methods on a level playing
field. Nonetheless, we note that extending DADT to
enable semi-supervised classification and additional
feature types are promising future work directions.
While all the methods yielded relatively low accu-
racies on Blog due to its size, topic-based methods
were more strongly affected than SVM by the transi-
tion from the 1,000 author subset to the full dataset.
This is probably because topic-based methods use a
single model, making them more sensitive to corpus
size than SVM’s one-versus-all setup that uses one
model per author. Notably, an oracle that chooses
the correct answer between SVM and DADT when
they disagree yields an accuracy of 37.15% on the
full dataset, suggesting it is worthwhile to explore
ensembles that combine the outputs of SVM and
DADT (we tried using DADT topics as additional
SVM features, but this did not outperform DADT).
</bodyText>
<sectionHeader confidence="0.995189" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991833333333">
This paper demonstrated the utility of using author-
aware topic models for AA: AT outperformed LDA,
and our DADT model outperformed LDA, AT and
SVMs in cases with noisy texts and many authors.
We hope that these results will inspire further re-
search into the application of topic models to AA.
</bodyText>
<sectionHeader confidence="0.997911" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.842058">
This research was supported in part by Australian
Research Council grant LP0883416. We thank Mark
Carman for fruitful discussions on topic modelling.
</bodyText>
<page confidence="0.994452">
267
</page>
<sectionHeader confidence="0.994914" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999306245283018">
Shlomo Argamon and Patrick Juola. 2011. Overview of
the international authorship identification competition
at PAN-2011. In CLEF 2011: Proceedings of the 2011
Conference on Multilingual and Multimodal Informa-
tion Access Evaluation (Lab and Workshop Notebook
Papers), Amsterdam, The Netherlands.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3(Jan):993–1022.
David M. Blei. 2012. Probabilistic topic models. Com-
munications of the ACM, 55(4):77–84.
Carole E. Chaski. 2005. Who’s at the keyboard? Au-
thorship attribution in digital evidence investigations.
International Journal of Digital Evidence, 4(1).
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2006. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In NIPS 2006: Proceedings of the 20th Annual Confer-
ence on Neural Information Processing Systems, pages
241–248, Vancouver, BC, Canada.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9(Aug):1871–1874.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228–5235.
Roman Kern, Christin Seifert, Mario Zechner, and
Michael Granitzer. 2011. Vote/veto meta-classifier
for authorship identification. In CLEF 2011: Pro-
ceedings of the 2011 Conference on Multilingual and
Multimodal Information Access Evaluation (Lab and
Workshop Notebook Papers), Amsterdam, The Nether-
lands.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational methods in authorship attribu-
tion. Journal of the American Society for Information
Science and Technology, 60(1):9–26.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83–94.
Ioannis Kourtis and Efstathios Stamatatos. 2011. Au-
thor identification using semi-supervised learning. In
CLEF 2011: Proceedings of the 2011 Conference
on Multilingual and Multimodal Information Access
Evaluation (Lab and Workshop Notebook Papers),
Amsterdam, The Netherlands.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative learning for dimen-
sionality reduction and classification. In NIPS 2008:
Proceedings of the 22nd Annual Conference on Neu-
ral Information Processing Systems, pages 897–904,
Vancouver, BC, Canada.
David Mimno and Andrew McCallum. 2008.
Topic models conditioned on arbitrary features with
Dirichlet-multinomial regression. In UAI 2008: Pro-
ceedings of the 24th Conference on Uncertainty in Ar-
tificial Intelligence, pages 411–418, Helsinki, Finland.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In EMNLP 2009: Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 248–256, Singapore.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for
authors and documents. In UAI 2004: Proceedings of
the 20th Conference on Uncertainty in Artificial Intel-
ligence, pages 487–494, Banff, AB, Canada.
Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas
Griffiths, Padhraic Smyth, and Mark Steyvers. 2010.
Learning author-topic models from text corpora. ACM
Transactions on Information Systems, 28(1):1–38.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and gen-
der on blogging. In Proceedings of AAAI Spring Sym-
posium on Computational Approaches for Analyzing
Weblogs, pages 199–205, Stanford, CA, USA.
Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert.
2011. Authorship attribution with latent Dirichlet allo-
cation. In CoNLL 2011: Proceedings of the 15th Inter-
national Conference on Computational Natural Lan-
guage Learning, pages 181–189, Portland, OR, USA.
Yanir Seroussi, Fabian Bohnert, and Ingrid Zukerman.
2012. Authorship attribution with author-aware topic
models. Technical Report 2012/268, Faculty of Infor-
mation Technology, Monash University, Clayton, VIC,
Australia.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538–556.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. In Thomas K. Landauer, Danielle S.
McNamara, Simon Dennis, and Walter Kintsch, ed-
itors, Handbook of Latent Semantic Analysis, pages
427–448. Lawrence Erlbaum Associates.
Ludovic Tanguy, Assaf Urieli, Basilio Calderone, Nabil
Hathout, and Franck Sajous. 2011. A multitude
of linguistically-rich features for authorship attribu-
tion. In CLEF 2011: Proceedings of the 2011 Con-
ference on Multilingual and Multimodal Information
Access Evaluation (Lab and Workshop Notebook Pa-
pers), Amsterdam, The Netherlands.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
</reference>
<page confidence="0.968284">
268
</page>
<reference confidence="0.998692571428571">
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Hanna M. Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking LDA: Why priors matter. In
NIPS 2009: Proceedings of the 23rd Annual Confer-
ence on Neural Information Processing Systems, pages
1973–1981, Vancouver, BC, Canada.
</reference>
<page confidence="0.998576">
269
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.901329">
<title confidence="0.999798">Authorship Attribution with Author-aware Topic Models</title>
<author confidence="0.99903">Yanir Seroussi Fabian Bohnert Ingrid Zukerman</author>
<affiliation confidence="0.999946">Faculty of Information Technology, Monash University</affiliation>
<address confidence="0.999846">Clayton, Victoria 3800, Australia</address>
<email confidence="0.999958">firstname.lastname@monash.edu</email>
<abstract confidence="0.992296133333333">Authorship attribution deals with identifying the authors of anonymous texts. Building on our earlier finding that the Latent Dirichlet Allocation (LDA) topic model can be used to improve authorship attribution accuracy, we show that employing a previously-suggested Author-Topic (AT) model outperforms LDA when applied to scenarios with many authors. In addition, we define a model that combines LDA and AT by representing authors and documents over two disjoint topic sets, and show that our model outperforms LDA, AT and support vector machines on datasets with many authors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Patrick Juola</author>
</authors>
<title>Overview of the international authorship identification competition at PAN-2011. In</title>
<date>2011</date>
<booktitle>CLEF 2011: Proceedings of the 2011 Conference on Multilingual and Multimodal Information Access Evaluation (Lab and Workshop Notebook Papers),</booktitle>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="9337" citStr="Argamon and Juola, 2011" startWordPosition="1585" endWordPosition="1588">document topic distribution. We then calculate the probability of each author given the model’s parameters, the test text words, and the inferred author/document topic ratio and document topic distribution. The most probable author is returned. We use this method to avoid inferring the document-dependent parameters separately for each author, which is infeasible when many authors exist. A version that marginalises over these parameters will be explored in future work. 4 Evaluation We compare the performance of the methods on two publicly-available datasets: (1) PAN’11: emails with 72 authors (Argamon and Juola, 2011); and (2) Blog: blogs with 19,320 authors (Schler et al., 2006). These datasets represent realistic scenarios of AA of user-generated texts with many candidate authors. For example, Chaski (2005) notes a case where an employee who was terminated for sending a racist email claimed that any person with access to his computer could have sent the email. Experimental Setup. Experiments on the PAN’11 dataset followed the setup of the PAN’11 competition (Argamon and Juola, 2011): We trained all the methods on the given training subset, tuned the parameters according to the results on the given valida</context>
<context position="14219" citStr="Argamon and Juola, 2011" startWordPosition="2422" endWordPosition="2425">t with p &lt; 0.05). We attribute DADT’s superior performance to the de-noising effect of the disjoint topic sets, which appear to yield author representations of higher predictive quality than those of the other models. As expected, AT significantly outperformed LDA-H. On the other hand, AT-FA performed much worse than all the other methods on PAN’11, probably because of the inherent noisiness in using the same topics to model both authors and documents. Hence, we did not run AT-FA on the Blog dataset. DADT’s PAN’11 testing result is close to the third-best accuracy from the PAN’11 competition (Argamon and Juola, 2011). However, to the best of our knowledge, DADT obtained the best accuracy for a fully-supervised method that uses only unigram features. Specifically, Kourtis and Stamatatos (2011), who obtained the highest accuracy (65.8%), assumed that all the test texts are given to the classifier at the same time, and used this additional information with a semi-supervised method; while Kern et al. (2011) and Tanguy et al. (2011), who obtained the second-best (64.2%) and third-best (59.4%) accuracies respectively, used various feature types (e.g., features obtained from parse trees). Further, preprocessing </context>
</contexts>
<marker>Argamon, Juola, 2011</marker>
<rawString>Shlomo Argamon and Patrick Juola. 2011. Overview of the international authorship identification competition at PAN-2011. In CLEF 2011: Proceedings of the 2011 Conference on Multilingual and Multimodal Information Access Evaluation (Lab and Workshop Notebook Papers), Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1713" citStr="Blei et al. (2003)" startWordPosition="260" endWordPosition="263">texts of unknown authorship to one of a set of known authors, whose training texts are supplied in advance (i.e., a supervised classification problem). While most of the early work on AA focused on formal texts with only a few possible authors, researchers have recently turned their attention to informal texts and tens to thousands of authors (Koppel et al., 2011). In parallel, topic models have gained popularity as a means of analysing such large text corpora (Blei, 2012). In (Seroussi et al., 2011), we showed that methods based on Latent Dirichlet Allocation (LDA) – a popular topic model by Blei et al. (2003) – yield good AA performance. However, LDA does not model authors explicitly, and we are not aware of any previous studies that apply author-aware topic models to traditional AA. This paper aims to address this gap. In addition to being the first (to the best of our knowledge) to apply Rosen-Zvi et al.’s (2004) Author-Topic Model (AT) to traditional AA, the main contribution of this paper is our Disjoint Author-Document Topic Model (DADT), which addresses AT’s limitations in the context of AA. We show that DADT outperforms AT, LDA, and linear support vector machines on AA with many authors. 2 </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3(Jan):993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2012</date>
<journal>Communications of the ACM,</journal>
<volume>55</volume>
<issue>4</issue>
<contexts>
<context position="1572" citStr="Blei, 2012" startWordPosition="237" endWordPosition="238">telligence, and humanities research (Stamatatos, 2009). The traditional problem, which is the focus of our work, is to attribute test texts of unknown authorship to one of a set of known authors, whose training texts are supplied in advance (i.e., a supervised classification problem). While most of the early work on AA focused on formal texts with only a few possible authors, researchers have recently turned their attention to informal texts and tens to thousands of authors (Koppel et al., 2011). In parallel, topic models have gained popularity as a means of analysing such large text corpora (Blei, 2012). In (Seroussi et al., 2011), we showed that methods based on Latent Dirichlet Allocation (LDA) – a popular topic model by Blei et al. (2003) – yield good AA performance. However, LDA does not model authors explicitly, and we are not aware of any previous studies that apply author-aware topic models to traditional AA. This paper aims to address this gap. In addition to being the first (to the best of our knowledge) to apply Rosen-Zvi et al.’s (2004) Author-Topic Model (AT) to traditional AA, the main contribution of this paper is our Disjoint Author-Document Topic Model (DADT), which addresses</context>
</contexts>
<marker>Blei, 2012</marker>
<rawString>David M. Blei. 2012. Probabilistic topic models. Communications of the ACM, 55(4):77–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carole E Chaski</author>
</authors>
<title>Who’s at the keyboard? Authorship attribution in digital evidence investigations.</title>
<date>2005</date>
<journal>International Journal of Digital Evidence,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="9532" citStr="Chaski (2005)" startWordPosition="1618" endWordPosition="1619"> The most probable author is returned. We use this method to avoid inferring the document-dependent parameters separately for each author, which is infeasible when many authors exist. A version that marginalises over these parameters will be explored in future work. 4 Evaluation We compare the performance of the methods on two publicly-available datasets: (1) PAN’11: emails with 72 authors (Argamon and Juola, 2011); and (2) Blog: blogs with 19,320 authors (Schler et al., 2006). These datasets represent realistic scenarios of AA of user-generated texts with many candidate authors. For example, Chaski (2005) notes a case where an employee who was terminated for sending a racist email claimed that any person with access to his computer could have sent the email. Experimental Setup. Experiments on the PAN’11 dataset followed the setup of the PAN’11 competition (Argamon and Juola, 2011): We trained all the methods on the given training subset, tuned the parameters according to the results on the given validation subset, and ran the tuned methods on the given testing subset. In the Blog experiments, we used tenfold cross validation as in (Seroussi et al., 2011). We used collapsed Gibbs sampling to tr</context>
</contexts>
<marker>Chaski, 2005</marker>
<rawString>Carole E. Chaski. 2005. Who’s at the keyboard? Authorship attribution in digital evidence investigations. International Journal of Digital Evidence, 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chaitanya Chemudugunta</author>
<author>Padhraic Smyth</author>
<author>Mark Steyvers</author>
</authors>
<title>Modeling general and specific aspects of documents with a probabilistic topic model.</title>
<date>2006</date>
<booktitle>In NIPS 2006: Proceedings of the 20th Annual Conference on Neural Information Processing Systems,</booktitle>
<pages>241--248</pages>
<location>Vancouver, BC,</location>
<contexts>
<context position="3054" citStr="Chemudugunta et al. (2006)" startWordPosition="483" endWordPosition="486">uthors write texts on the same issue, specific words must be used (e.g., texts about LDA are likely to contain the words “topic” and “prior”), while other words vary in frequency according to author style. Also, texts by the same author share similar style markers, independently of content (Koppel et al., 2009). DADT aims to separate document words from author words by generating them from two disjoint topic sets of T (D) document topics and T (A) author topics. Lacoste-Julien et al. (2008) and Ramage et al. (2009) (among others) also used disjoint topic sets to represent document labels, and Chemudugunta et al. (2006) separated corpus-level topics from document-specific words. However, we are unaware of any applications of these ideas to AA. The closest work we know of is by Mimno and McCallum (2008), whose DMR model outperformed AT in AA 264 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 264–269, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: The Disjoint Author-Document Topic Model of multi-authored texts (DMR does not use disjoint topic sets). We use AT rather than DMR, since we found that AT outperforms</context>
</contexts>
<marker>Chemudugunta, Smyth, Steyvers, 2006</marker>
<rawString>Chaitanya Chemudugunta, Padhraic Smyth, and Mark Steyvers. 2006. Modeling general and specific aspects of documents with a probabilistic topic model. In NIPS 2006: Proceedings of the 20th Annual Conference on Neural Information Processing Systems, pages 241–248, Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="7383" citStr="Fan et al., 2008" startWordPosition="1264" endWordPosition="1267">ables the potential use of DADT in a semisupervised setting by training on unlabelled texts, which we plan to explore in the future. 3 Authorship Attribution Methods We experimented with the following AA methods, using token frequency features, which are good predictors of authorship (Koppel et al., 2009). ± ( ) 1/4d ± ( ) 77 ydi ad X D ) a( ed( ) zdi ea( ) a( ) D A T � ( fit( ) �( ) ( ) wdi d D ) fit( ) T ( ) 265 Baseline: Support Vector Machines (SVMs). Koppel et al. (2009) showed that SVMs yield good AA performance. We use linear SVMs in a one-versus-all setup, as implemented in LIBLINEAR (Fan et al., 2008), reporting results obtained with the best cost parameter values. Baseline: LDA + Hellinger (LDA-H). This approach uses the Hellinger distances of topic distributions to assign test texts to the closest author. In (Seroussi et al., 2011), we experimented with two variants: (1) each author’s texts are concatenated before building the LDA model; and (2) no concatenation is performed. We found that the latter approach performs poorly in cases with many candidate authors. Hence, we use only the former approach in this paper. Note that when dealing with single-authored texts, concatenating each aut</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9(Aug):1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="10187" citStr="Griffiths and Steyvers, 2004" startWordPosition="1728" endWordPosition="1731">loyee who was terminated for sending a racist email claimed that any person with access to his computer could have sent the email. Experimental Setup. Experiments on the PAN’11 dataset followed the setup of the PAN’11 competition (Argamon and Juola, 2011): We trained all the methods on the given training subset, tuned the parameters according to the results on the given validation subset, and ran the tuned methods on the given testing subset. In the Blog experiments, we used tenfold cross validation as in (Seroussi et al., 2011). We used collapsed Gibbs sampling to train all the topic models (Griffiths and Steyvers, 2004), running 4 chains with a burn-in of 1,000 iterations. In the PAN’11 experiments, we retained 8 samples per chain with spacing of 100 iterations. In the Blog experiments, we retained 1 sample per chain due to runtime constraints. Since we cannot average topic distribution estimates obtained from training samples due to topic exchangeability (Steyvers and Griffiths, 2007), we averaged the distances and probabilities calculated from the retained samples. For test text sampling, we used a burn-in of 100 iterations and averaged the parameter estimates over the next 100 iterations in a similar mann</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Kern</author>
<author>Christin Seifert</author>
<author>Mario Zechner</author>
<author>Michael Granitzer</author>
</authors>
<title>Vote/veto meta-classifier for authorship identification.</title>
<date>2011</date>
<booktitle>In CLEF 2011: Proceedings of the 2011 Conference on Multilingual and Multimodal Information Access Evaluation (Lab and Workshop Notebook Papers),</booktitle>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="14613" citStr="Kern et al. (2011)" startWordPosition="2484" endWordPosition="2487">ing the same topics to model both authors and documents. Hence, we did not run AT-FA on the Blog dataset. DADT’s PAN’11 testing result is close to the third-best accuracy from the PAN’11 competition (Argamon and Juola, 2011). However, to the best of our knowledge, DADT obtained the best accuracy for a fully-supervised method that uses only unigram features. Specifically, Kourtis and Stamatatos (2011), who obtained the highest accuracy (65.8%), assumed that all the test texts are given to the classifier at the same time, and used this additional information with a semi-supervised method; while Kern et al. (2011) and Tanguy et al. (2011), who obtained the second-best (64.2%) and third-best (59.4%) accuracies respectively, used various feature types (e.g., features obtained from parse trees). Further, preprocessing differences make it hard to compare the methods on a level playing field. Nonetheless, we note that extending DADT to enable semi-supervised classification and additional feature types are promising future work directions. While all the methods yielded relatively low accuracies on Blog due to its size, topic-based methods were more strongly affected than SVM by the transition from the 1,000 </context>
</contexts>
<marker>Kern, Seifert, Zechner, Granitzer, 2011</marker>
<rawString>Roman Kern, Christin Seifert, Mario Zechner, and Michael Granitzer. 2011. Vote/veto meta-classifier for authorship identification. In CLEF 2011: Proceedings of the 2011 Conference on Multilingual and Multimodal Information Access Evaluation (Lab and Workshop Notebook Papers), Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Shlomo Argamon</author>
</authors>
<title>Computational methods in authorship attribution.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>1</issue>
<contexts>
<context position="2740" citStr="Koppel et al., 2009" startWordPosition="431" endWordPosition="434"> Author-Document Topic Model (DADT), which addresses AT’s limitations in the context of AA. We show that DADT outperforms AT, LDA, and linear support vector machines on AA with many authors. 2 Disjoint Author-Document Topic Model Background. Our definition of DADT is motivated by the observation that when authors write texts on the same issue, specific words must be used (e.g., texts about LDA are likely to contain the words “topic” and “prior”), while other words vary in frequency according to author style. Also, texts by the same author share similar style markers, independently of content (Koppel et al., 2009). DADT aims to separate document words from author words by generating them from two disjoint topic sets of T (D) document topics and T (A) author topics. Lacoste-Julien et al. (2008) and Ramage et al. (2009) (among others) also used disjoint topic sets to represent document labels, and Chemudugunta et al. (2006) separated corpus-level topics from document-specific words. However, we are unaware of any applications of these ideas to AA. The closest work we know of is by Mimno and McCallum (2008), whose DMR model outperformed AT in AA 264 Proceedings of the 50th Annual Meeting of the Associatio</context>
<context position="6073" citStr="Koppel et al., 2009" startWordPosition="1019" endWordPosition="1023">dapt to individual documents and not only to authors). However, there are several key differences between DADT and AT. First, in DADT author topics are disjoint from document topics, with different priors for each topic set. Thus, the number of author topics can be different from the number of document topics, enabling us to vary the number of author topics according to the number of authors in the corpus. Second, DADT places different priors on the word distributions for author topics and document topics (β(A) and β(D) respectively). Stopwords are known to be strong indicators of authorship (Koppel et al., 2009), and DADT allows us to use this knowledge by assigning higher weights to the elements of β(A) that correspond to stopwords than to such elements in β(D). Third, DADT learns the ratio between document words and author words on a per-document basis, and makes it possible to specify a prior belief of what this ratio should be. We found that specifying a prior belief that about 80% of each document is composed of author words yielded better results than using AT’s approach, which evenly splits each document into author and document words. Fourth, DADT defines the process that generates authors. T</context>
</contexts>
<marker>Koppel, Schler, Argamon, 2009</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2009. Computational methods in authorship attribution. Journal of the American Society for Information Science and Technology, 60(1):9–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Shlomo Argamon</author>
</authors>
<date>2011</date>
<booktitle>Authorship attribution in the wild. Language Resources and Evaluation,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="1461" citStr="Koppel et al., 2011" startWordPosition="216" endWordPosition="219">n (AA) has attracted much attention due to its many applications in, e.g., computer forensics, criminal law, military intelligence, and humanities research (Stamatatos, 2009). The traditional problem, which is the focus of our work, is to attribute test texts of unknown authorship to one of a set of known authors, whose training texts are supplied in advance (i.e., a supervised classification problem). While most of the early work on AA focused on formal texts with only a few possible authors, researchers have recently turned their attention to informal texts and tens to thousands of authors (Koppel et al., 2011). In parallel, topic models have gained popularity as a means of analysing such large text corpora (Blei, 2012). In (Seroussi et al., 2011), we showed that methods based on Latent Dirichlet Allocation (LDA) – a popular topic model by Blei et al. (2003) – yield good AA performance. However, LDA does not model authors explicitly, and we are not aware of any previous studies that apply author-aware topic models to traditional AA. This paper aims to address this gap. In addition to being the first (to the best of our knowledge) to apply Rosen-Zvi et al.’s (2004) Author-Topic Model (AT) to traditio</context>
</contexts>
<marker>Koppel, Schler, Argamon, 2011</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2011. Authorship attribution in the wild. Language Resources and Evaluation, 45(1):83–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Kourtis</author>
<author>Efstathios Stamatatos</author>
</authors>
<title>Author identification using semi-supervised learning.</title>
<date>2011</date>
<booktitle>In CLEF 2011: Proceedings of the 2011 Conference on Multilingual and Multimodal Information Access Evaluation (Lab and Workshop Notebook Papers),</booktitle>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="14398" citStr="Kourtis and Stamatatos (2011)" startWordPosition="2448" endWordPosition="2451">ive quality than those of the other models. As expected, AT significantly outperformed LDA-H. On the other hand, AT-FA performed much worse than all the other methods on PAN’11, probably because of the inherent noisiness in using the same topics to model both authors and documents. Hence, we did not run AT-FA on the Blog dataset. DADT’s PAN’11 testing result is close to the third-best accuracy from the PAN’11 competition (Argamon and Juola, 2011). However, to the best of our knowledge, DADT obtained the best accuracy for a fully-supervised method that uses only unigram features. Specifically, Kourtis and Stamatatos (2011), who obtained the highest accuracy (65.8%), assumed that all the test texts are given to the classifier at the same time, and used this additional information with a semi-supervised method; while Kern et al. (2011) and Tanguy et al. (2011), who obtained the second-best (64.2%) and third-best (59.4%) accuracies respectively, used various feature types (e.g., features obtained from parse trees). Further, preprocessing differences make it hard to compare the methods on a level playing field. Nonetheless, we note that extending DADT to enable semi-supervised classification and additional feature </context>
</contexts>
<marker>Kourtis, Stamatatos, 2011</marker>
<rawString>Ioannis Kourtis and Efstathios Stamatatos. 2011. Author identification using semi-supervised learning. In CLEF 2011: Proceedings of the 2011 Conference on Multilingual and Multimodal Information Access Evaluation (Lab and Workshop Notebook Papers), Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Fei Sha</author>
<author>Michael I Jordan</author>
</authors>
<title>DiscLDA: Discriminative learning for dimensionality reduction and classification.</title>
<date>2008</date>
<booktitle>In NIPS 2008: Proceedings of the 22nd Annual Conference on Neural Information Processing Systems,</booktitle>
<pages>897--904</pages>
<location>Vancouver, BC,</location>
<contexts>
<context position="2923" citStr="Lacoste-Julien et al. (2008)" startWordPosition="462" endWordPosition="465">h many authors. 2 Disjoint Author-Document Topic Model Background. Our definition of DADT is motivated by the observation that when authors write texts on the same issue, specific words must be used (e.g., texts about LDA are likely to contain the words “topic” and “prior”), while other words vary in frequency according to author style. Also, texts by the same author share similar style markers, independently of content (Koppel et al., 2009). DADT aims to separate document words from author words by generating them from two disjoint topic sets of T (D) document topics and T (A) author topics. Lacoste-Julien et al. (2008) and Ramage et al. (2009) (among others) also used disjoint topic sets to represent document labels, and Chemudugunta et al. (2006) separated corpus-level topics from document-specific words. However, we are unaware of any applications of these ideas to AA. The closest work we know of is by Mimno and McCallum (2008), whose DMR model outperformed AT in AA 264 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 264–269, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: The Disjoint Author-Document Topic </context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008. DiscLDA: Discriminative learning for dimensionality reduction and classification. In NIPS 2008: Proceedings of the 22nd Annual Conference on Neural Information Processing Systems, pages 897–904, Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Topic models conditioned on arbitrary features with Dirichlet-multinomial regression.</title>
<date>2008</date>
<booktitle>In UAI 2008: Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>411--418</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="3240" citStr="Mimno and McCallum (2008)" startWordPosition="514" endWordPosition="517">g to author style. Also, texts by the same author share similar style markers, independently of content (Koppel et al., 2009). DADT aims to separate document words from author words by generating them from two disjoint topic sets of T (D) document topics and T (A) author topics. Lacoste-Julien et al. (2008) and Ramage et al. (2009) (among others) also used disjoint topic sets to represent document labels, and Chemudugunta et al. (2006) separated corpus-level topics from document-specific words. However, we are unaware of any applications of these ideas to AA. The closest work we know of is by Mimno and McCallum (2008), whose DMR model outperformed AT in AA 264 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 264–269, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: The Disjoint Author-Document Topic Model of multi-authored texts (DMR does not use disjoint topic sets). We use AT rather than DMR, since we found that AT outperforms DMR in AA of singleauthored texts, which are the focus of this paper. The Model. Figure 1 shows DADT’s graphical representation, with document-related parameters on the left (the LDA co</context>
</contexts>
<marker>Mimno, McCallum, 2008</marker>
<rawString>David Mimno and Andrew McCallum. 2008. Topic models conditioned on arbitrary features with Dirichlet-multinomial regression. In UAI 2008: Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence, pages 411–418, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora.</title>
<date>2009</date>
<booktitle>In EMNLP 2009: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>248--256</pages>
<contexts>
<context position="2948" citStr="Ramage et al. (2009)" startWordPosition="467" endWordPosition="470">-Document Topic Model Background. Our definition of DADT is motivated by the observation that when authors write texts on the same issue, specific words must be used (e.g., texts about LDA are likely to contain the words “topic” and “prior”), while other words vary in frequency according to author style. Also, texts by the same author share similar style markers, independently of content (Koppel et al., 2009). DADT aims to separate document words from author words by generating them from two disjoint topic sets of T (D) document topics and T (A) author topics. Lacoste-Julien et al. (2008) and Ramage et al. (2009) (among others) also used disjoint topic sets to represent document labels, and Chemudugunta et al. (2006) separated corpus-level topics from document-specific words. However, we are unaware of any applications of these ideas to AA. The closest work we know of is by Mimno and McCallum (2008), whose DMR model outperformed AT in AA 264 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 264–269, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: The Disjoint Author-Document Topic Model of multi-authored t</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora. In EMNLP 2009: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 248–256, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Thomas Griffiths</author>
<author>Mark Steyvers</author>
<author>Padhraic Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In UAI 2004: Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>487--494</pages>
<location>Banff, AB,</location>
<contexts>
<context position="8096" citStr="Rosen-Zvi et al., 2004" startWordPosition="1383" endWordPosition="1386">r (LDA-H). This approach uses the Hellinger distances of topic distributions to assign test texts to the closest author. In (Seroussi et al., 2011), we experimented with two variants: (1) each author’s texts are concatenated before building the LDA model; and (2) no concatenation is performed. We found that the latter approach performs poorly in cases with many candidate authors. Hence, we use only the former approach in this paper. Note that when dealing with single-authored texts, concatenating each author’s texts yields an LDA model that is equivalent to AT. AT. Given an inferred AT model (Rosen-Zvi et al., 2004), we calculate the probability of the test text words for each author a, assuming it was written by a, and return the most probable author. We do not know of any other studies that used AT in this manner for single-authored AA. We expect this method to outperform LDA-H as it employs AT directly, rather than relying on an external distance measure. AT-FA. Same as AT, but built with an additional unique “fictitious” author for each document. DADT. Given our DADT model, we assume that the test text was written by a “new” author, and infer this author’s topic distribution, the author/document topi</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. 2004. The author-topic model for authors and documents. In UAI 2004: Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, pages 487–494, Banff, AB, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Chaitanya Chemudugunta</author>
<author>Thomas Griffiths</author>
<author>Padhraic Smyth</author>
<author>Mark Steyvers</author>
</authors>
<title>Learning author-topic models from text corpora.</title>
<date>2010</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="10816" citStr="Rosen-Zvi et al. (2010)" startWordPosition="1830" endWordPosition="1833">ing 4 chains with a burn-in of 1,000 iterations. In the PAN’11 experiments, we retained 8 samples per chain with spacing of 100 iterations. In the Blog experiments, we retained 1 sample per chain due to runtime constraints. Since we cannot average topic distribution estimates obtained from training samples due to topic exchangeability (Steyvers and Griffiths, 2007), we averaged the distances and probabilities calculated from the retained samples. For test text sampling, we used a burn-in of 100 iterations and averaged the parameter estimates over the next 100 iterations in a similar manner to Rosen-Zvi et al. (2010). We found that these settings yield stable results across different random seed values. We found that the number of topics has a larger impact on accuracy than other configurable parameters. Hence, we used symmetric topic priors, setting all the elements of α(D) and α(A) to min{0.1, 5/T (D)} and min{0.1, 5/T(A)} respectively.1 For all models, we set Q,,, = 0.01 for each word w as the base measure for the prior of words in topics. Since DADT allows us to encode our prior knowledge that stopword use is indicative of authorship, we set Q(D) � = 0.01 − c and Q(A) � = 0.01 + c for all w, where w i</context>
</contexts>
<marker>Rosen-Zvi, Chemudugunta, Griffiths, Smyth, Steyvers, 2010</marker>
<rawString>Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas Griffiths, Padhraic Smyth, and Mark Steyvers. 2010. Learning author-topic models from text corpora. ACM Transactions on Information Systems, 28(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Schler</author>
<author>Moshe Koppel</author>
<author>Shlomo Argamon</author>
<author>James W Pennebaker</author>
</authors>
<title>Effects of age and gender on blogging.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs,</booktitle>
<pages>199--205</pages>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="9400" citStr="Schler et al., 2006" startWordPosition="1596" endWordPosition="1599">ach author given the model’s parameters, the test text words, and the inferred author/document topic ratio and document topic distribution. The most probable author is returned. We use this method to avoid inferring the document-dependent parameters separately for each author, which is infeasible when many authors exist. A version that marginalises over these parameters will be explored in future work. 4 Evaluation We compare the performance of the methods on two publicly-available datasets: (1) PAN’11: emails with 72 authors (Argamon and Juola, 2011); and (2) Blog: blogs with 19,320 authors (Schler et al., 2006). These datasets represent realistic scenarios of AA of user-generated texts with many candidate authors. For example, Chaski (2005) notes a case where an employee who was terminated for sending a racist email claimed that any person with access to his computer could have sent the email. Experimental Setup. Experiments on the PAN’11 dataset followed the setup of the PAN’11 competition (Argamon and Juola, 2011): We trained all the methods on the given training subset, tuned the parameters according to the results on the given validation subset, and ran the tuned methods on the given testing sub</context>
</contexts>
<marker>Schler, Koppel, Argamon, Pennebaker, 2006</marker>
<rawString>Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W. Pennebaker. 2006. Effects of age and gender on blogging. In Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs, pages 199–205, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanir Seroussi</author>
<author>Ingrid Zukerman</author>
<author>Fabian Bohnert</author>
</authors>
<title>Authorship attribution with latent Dirichlet allocation.</title>
<date>2011</date>
<booktitle>In CoNLL 2011: Proceedings of the 15th International Conference on Computational Natural Language Learning,</booktitle>
<pages>181--189</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="1600" citStr="Seroussi et al., 2011" startWordPosition="240" endWordPosition="243">umanities research (Stamatatos, 2009). The traditional problem, which is the focus of our work, is to attribute test texts of unknown authorship to one of a set of known authors, whose training texts are supplied in advance (i.e., a supervised classification problem). While most of the early work on AA focused on formal texts with only a few possible authors, researchers have recently turned their attention to informal texts and tens to thousands of authors (Koppel et al., 2011). In parallel, topic models have gained popularity as a means of analysing such large text corpora (Blei, 2012). In (Seroussi et al., 2011), we showed that methods based on Latent Dirichlet Allocation (LDA) – a popular topic model by Blei et al. (2003) – yield good AA performance. However, LDA does not model authors explicitly, and we are not aware of any previous studies that apply author-aware topic models to traditional AA. This paper aims to address this gap. In addition to being the first (to the best of our knowledge) to apply Rosen-Zvi et al.’s (2004) Author-Topic Model (AT) to traditional AA, the main contribution of this paper is our Disjoint Author-Document Topic Model (DADT), which addresses AT’s limitations in the con</context>
<context position="7620" citStr="Seroussi et al., 2011" startWordPosition="1302" endWordPosition="1305">cy features, which are good predictors of authorship (Koppel et al., 2009). ± ( ) 1/4d ± ( ) 77 ydi ad X D ) a( ed( ) zdi ea( ) a( ) D A T � ( fit( ) �( ) ( ) wdi d D ) fit( ) T ( ) 265 Baseline: Support Vector Machines (SVMs). Koppel et al. (2009) showed that SVMs yield good AA performance. We use linear SVMs in a one-versus-all setup, as implemented in LIBLINEAR (Fan et al., 2008), reporting results obtained with the best cost parameter values. Baseline: LDA + Hellinger (LDA-H). This approach uses the Hellinger distances of topic distributions to assign test texts to the closest author. In (Seroussi et al., 2011), we experimented with two variants: (1) each author’s texts are concatenated before building the LDA model; and (2) no concatenation is performed. We found that the latter approach performs poorly in cases with many candidate authors. Hence, we use only the former approach in this paper. Note that when dealing with single-authored texts, concatenating each author’s texts yields an LDA model that is equivalent to AT. AT. Given an inferred AT model (Rosen-Zvi et al., 2004), we calculate the probability of the test text words for each author a, assuming it was written by a, and return the most p</context>
<context position="10092" citStr="Seroussi et al., 2011" startWordPosition="1713" endWordPosition="1716"> texts with many candidate authors. For example, Chaski (2005) notes a case where an employee who was terminated for sending a racist email claimed that any person with access to his computer could have sent the email. Experimental Setup. Experiments on the PAN’11 dataset followed the setup of the PAN’11 competition (Argamon and Juola, 2011): We trained all the methods on the given training subset, tuned the parameters according to the results on the given validation subset, and ran the tuned methods on the given testing subset. In the Blog experiments, we used tenfold cross validation as in (Seroussi et al., 2011). We used collapsed Gibbs sampling to train all the topic models (Griffiths and Steyvers, 2004), running 4 chains with a burn-in of 1,000 iterations. In the PAN’11 experiments, we retained 8 samples per chain with spacing of 100 iterations. In the Blog experiments, we retained 1 sample per chain due to runtime constraints. Since we cannot average topic distribution estimates obtained from training samples due to topic exchangeability (Steyvers and Griffiths, 2007), we averaged the distances and probabilities calculated from the retained samples. For test text sampling, we used a burn-in of 100</context>
</contexts>
<marker>Seroussi, Zukerman, Bohnert, 2011</marker>
<rawString>Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert. 2011. Authorship attribution with latent Dirichlet allocation. In CoNLL 2011: Proceedings of the 15th International Conference on Computational Natural Language Learning, pages 181–189, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanir Seroussi</author>
<author>Fabian Bohnert</author>
<author>Ingrid Zukerman</author>
</authors>
<title>Authorship attribution with author-aware topic models.</title>
<date>2012</date>
<tech>Technical Report 2012/268,</tech>
<institution>Faculty of Information Technology, Monash University,</institution>
<location>Clayton, VIC, Australia.</location>
<contexts>
<context position="12229" citStr="Seroussi et al., 2012" startWordPosition="2084" endWordPosition="2087">ted Wallach et al.’s (2009) method of obtaining asymmetric priors, but found that it did not improve accuracy. 2We used the stopword list from www.lextek.com/ manuals/onix/stopwords2.html. 266 Method PAN’11 PAN’11 Blog Blog Validation Testing Prolific Full SVM 48.61% 53.31% 33.31% 24.13% LDA-H 34.95% 42.62% 21.61% 7.94% AT 46.68% 53.08% 37.56% 23.03% AT-FA 20.68% 24.23% — — DADT 54.24% 59.08% 42.51% 27.63% Table 1: Experiment results belief that 0.8 f 0.15 of each document is composed of author words. We found that this yields better results than an uninformed uniform prior of S(A) = S(D) = 1(Seroussi et al., 2012). In addition, we set ,% = 1 for each author a, yielding smoothed estimates for the corpus distribution of authors X. To fairly compare the topic-based methods, we used the same overall number of topics for all the topic models. We present only the results obtained with the best topic settings: 100 for PAN’11 and 400 for Blog, with DADT’s author/document topic splits being 90/10 for PAN’11, and 390/10 for Blog. These splits allow DADT to de-noise the author representations by allocating document words to a relatively small number of document topics. It is worth noting that AT can be seen as an</context>
</contexts>
<marker>Seroussi, Bohnert, Zukerman, 2012</marker>
<rawString>Yanir Seroussi, Fabian Bohnert, and Ingrid Zukerman. 2012. Authorship attribution with author-aware topic models. Technical Report 2012/268, Faculty of Information Technology, Monash University, Clayton, VIC, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>A survey of modern authorship attribution methods.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>3</issue>
<contexts>
<context position="1015" citStr="Stamatatos, 2009" startWordPosition="141" endWordPosition="142">used to improve authorship attribution accuracy, we show that employing a previously-suggested Author-Topic (AT) model outperforms LDA when applied to scenarios with many authors. In addition, we define a model that combines LDA and AT by representing authors and documents over two disjoint topic sets, and show that our model outperforms LDA, AT and support vector machines on datasets with many authors. 1 Introduction Authorship attribution (AA) has attracted much attention due to its many applications in, e.g., computer forensics, criminal law, military intelligence, and humanities research (Stamatatos, 2009). The traditional problem, which is the focus of our work, is to attribute test texts of unknown authorship to one of a set of known authors, whose training texts are supplied in advance (i.e., a supervised classification problem). While most of the early work on AA focused on formal texts with only a few possible authors, researchers have recently turned their attention to informal texts and tens to thousands of authors (Koppel et al., 2011). In parallel, topic models have gained popularity as a means of analysing such large text corpora (Blei, 2012). In (Seroussi et al., 2011), we showed tha</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>Efstathios Stamatatos. 2009. A survey of modern authorship attribution methods. Journal of the American Society for Information Science and Technology, 60(3):538–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2007</date>
<booktitle>Handbook of Latent Semantic Analysis,</booktitle>
<pages>427--448</pages>
<editor>In Thomas K. Landauer, Danielle S. McNamara, Simon Dennis, and Walter Kintsch, editors,</editor>
<contexts>
<context position="10560" citStr="Steyvers and Griffiths, 2007" startWordPosition="1787" endWordPosition="1791">validation subset, and ran the tuned methods on the given testing subset. In the Blog experiments, we used tenfold cross validation as in (Seroussi et al., 2011). We used collapsed Gibbs sampling to train all the topic models (Griffiths and Steyvers, 2004), running 4 chains with a burn-in of 1,000 iterations. In the PAN’11 experiments, we retained 8 samples per chain with spacing of 100 iterations. In the Blog experiments, we retained 1 sample per chain due to runtime constraints. Since we cannot average topic distribution estimates obtained from training samples due to topic exchangeability (Steyvers and Griffiths, 2007), we averaged the distances and probabilities calculated from the retained samples. For test text sampling, we used a burn-in of 100 iterations and averaged the parameter estimates over the next 100 iterations in a similar manner to Rosen-Zvi et al. (2010). We found that these settings yield stable results across different random seed values. We found that the number of topics has a larger impact on accuracy than other configurable parameters. Hence, we used symmetric topic priors, setting all the elements of α(D) and α(A) to min{0.1, 5/T (D)} and min{0.1, 5/T(A)} respectively.1 For all models</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Mark Steyvers and Tom Griffiths. 2007. Probabilistic topic models. In Thomas K. Landauer, Danielle S. McNamara, Simon Dennis, and Walter Kintsch, editors, Handbook of Latent Semantic Analysis, pages 427–448. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Tanguy</author>
<author>Assaf Urieli</author>
<author>Basilio Calderone</author>
<author>Nabil Hathout</author>
<author>Franck Sajous</author>
</authors>
<title>A multitude of linguistically-rich features for authorship attribution.</title>
<date>2011</date>
<booktitle>In CLEF 2011: Proceedings of the 2011 Conference on Multilingual and Multimodal Information Access Evaluation (Lab and Workshop Notebook Papers),</booktitle>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="14638" citStr="Tanguy et al. (2011)" startWordPosition="2489" endWordPosition="2492">model both authors and documents. Hence, we did not run AT-FA on the Blog dataset. DADT’s PAN’11 testing result is close to the third-best accuracy from the PAN’11 competition (Argamon and Juola, 2011). However, to the best of our knowledge, DADT obtained the best accuracy for a fully-supervised method that uses only unigram features. Specifically, Kourtis and Stamatatos (2011), who obtained the highest accuracy (65.8%), assumed that all the test texts are given to the classifier at the same time, and used this additional information with a semi-supervised method; while Kern et al. (2011) and Tanguy et al. (2011), who obtained the second-best (64.2%) and third-best (59.4%) accuracies respectively, used various feature types (e.g., features obtained from parse trees). Further, preprocessing differences make it hard to compare the methods on a level playing field. Nonetheless, we note that extending DADT to enable semi-supervised classification and additional feature types are promising future work directions. While all the methods yielded relatively low accuracies on Blog due to its size, topic-based methods were more strongly affected than SVM by the transition from the 1,000 author subset to the full</context>
</contexts>
<marker>Tanguy, Urieli, Calderone, Hathout, Sajous, 2011</marker>
<rawString>Ludovic Tanguy, Assaf Urieli, Basilio Calderone, Nabil Hathout, and Franck Sajous. 2011. A multitude of linguistically-rich features for authorship attribution. In CLEF 2011: Proceedings of the 2011 Conference on Multilingual and Multimodal Information Access Evaluation (Lab and Workshop Notebook Papers), Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Rethinking LDA: Why priors matter.</title>
<date>2009</date>
<booktitle>In NIPS 2009: Proceedings of the 23rd Annual Conference on Neural Information Processing Systems,</booktitle>
<pages>1973--1981</pages>
<location>Vancouver, BC,</location>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>Hanna M. Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA: Why priors matter. In NIPS 2009: Proceedings of the 23rd Annual Conference on Neural Information Processing Systems, pages 1973–1981, Vancouver, BC, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>