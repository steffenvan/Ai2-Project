<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000353">
<title confidence="0.995438">
Latent Semantic Tensor Indexing
for Community-based Question Answering
</title>
<author confidence="0.949183">
Xipeng Qiu, Le Tian, Xuanjing Huang
</author>
<affiliation confidence="0.721909">
Fudan University, 825 Zhangheng Road, Shanghai, China
</affiliation>
<email confidence="0.990807">
xpqiu@fudan.edu.cn, tianlefdu@gmail.com, xjhuang@fudan.edu.cn
</email>
<sectionHeader confidence="0.979921" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999525">
Retrieving similar questions is very
important in community-based ques-
tion answering(CQA). In this paper,
we propose a unified question retrieval
model based on latent semantic index-
ing with tensor analysis, which can cap-
ture word associations among different
parts of CQA triples simultaneously.
Thus, our method can reduce lexical
chasm of question retrieval with the
help of the information of question con-
tent and answer parts. The experimen-
tal result shows that our method out-
performs the traditional methods.
</bodyText>
<sectionHeader confidence="0.996299" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999776611111111">
Community-based (or collaborative) ques-
tion answering(CQA) such as Yahoo! An-
swers1 and Baidu Zhidao2 has become a pop-
ular online service in recent years. Unlike tra-
ditional question answering (QA), information
seekers can post their questions on a CQA
website which are later answered by other
users. However, with the increase of the CQA
archive, there accumulate massive duplicate
questions on CQA websites. One of the pri-
mary reasons is that information seekers can-
not retrieve answers they need and thus post
another new question consequently. There-
fore, it becomes more and more important to
find semantically similar questions.
The major challenge for CQA retrieval is the
lexical gap (or lexical chasm) among the ques-
tions (Jeon et al., 2005b; Xue et al., 2008),
</bodyText>
<footnote confidence="0.999819">
1http://answers.yahoo.com/
2http://zhidao.baidu.com/
</footnote>
<listItem confidence="0.68622775">
Query:
Q: Why is my laptop screen blinking?
Expected:
Q1: How to troubleshoot a flashing
screen on an LCD monitor?
Not Expected:
Q2: How to blinking text on screen
with PowerPoint?
</listItem>
<tableCaption confidence="0.991319">
Table 1: An example on question retrieval
</tableCaption>
<bodyText confidence="0.999929296296296">
as shown in Table 1. Since question-answer
pairs are usually short, the word mismatch-
ing problem is especially important. However,
due to the lexical gap between questions and
answers as well as spam typically existing in
user-generated content, filtering and ranking
answers is very challenging.
The earlier studies mainly focus on generat-
ing redundant features, or finding textual clues
using machine learning techniques; none of
them ever consider questions and their answers
as relational data but instead model them as
independent information. Moreover, they only
consider the answers of the current question,
and ignore any previous knowledge that would
be helpful to bridge the lexical and se mantic
gap.
In recent years, many methods have been
proposed to solve the word mismatching prob-
lem between user questions and the questions
in a QA archive(Blooma and Kurian, 2011),
among which the translation-based (Riezler et
al., 2007; Xue et al., 2008; Zhou et al., 2011)
or syntactic-based approaches (Wang et al.,
2009) methods have been proven to improve
the performance of CQA retrieval.
However, most of these approaches used
</bodyText>
<page confidence="0.987677">
434
</page>
<bodyText confidence="0.971387816326531">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–439,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
pipeline methods: (1) modeling word asso-
ciation; (2) question retrieval combined with
other models, such as vector space model
(VSM), Okapi model (Robertson et al., 1994)
or language model (LM). The pipeline meth-
ods often have many non-trivial experimental
setting and result to be very hard to repro-
duce.
In this paper, we propose a novel unified
retrieval model for CQA, latent semantic
tensor indexing (LSTI), which is an exten-
sion of the conventional latent semantic index-
ing (LSI) (Deerwester et al., 1990). Similar
to LSI, LSTI can integrate the two detached
parts (modeling word association and question
retrieval) into a single model.
In traditional document retrieval, LSI is an
effective method to overcome two of the most
severe constraints on Boolean keyword queries:
synonymy, that is, multiple words with similar
meanings, and polysemy, or words with more
than one meanings.
Usually in a CQA archive, each en-
try (or question) is in the following triple
form:⟨question title, question content,
answer⟩. Because the performance based
solely on the content or the answer part is
less than satisfactory, many works proposed
that additional relevant information should be
provided to help question retrieval(Xue et al.,
2008). For example, if a question title contains
the keyword “why”, the CQA triple, which
contains “because” or “reason” in its answer
part, is more likely to be what the user looks
for.
Since each triple in CQA has three parts, the
natural representation of the CQA collection
is a three-dimensional array, or 3rd-order ten-
sor, rather than a matrix. Based on the tensor
decomposition, we can model the word associ-
ation simultaneously in the pairs: question-
question, question-body and question-answer.
The rest of the paper is organized as fol-
lows: Section 3 introduces the concept of LSI.
Section 4 presents our method. Section 5 de-
scribes the experimental analysis. Section 6
concludes the paper.
</bodyText>
<sectionHeader confidence="0.994906" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999928754716981">
There are some related works on question re-
trieval in CQA. Various query expansion tech-
niques have been studied to solve word mis-
match problems between queries and docu-
ments. The early works on question retrieval
can be traced back to finding similar ques-
tions in Frequently Asked Questions (FAQ)
archives, such as the FAQ finder (Burke et al.,
1997), which usually used statistical and se-
mantic similarity measures to rank FAQs.
Jeon et al. (2005a; 2005b) compared four
different retrieval methods, i.e., the vector
space model(Jijkoun and de Rijke, 2005),
the Okapi BM25 model (Robertson et al.,
1994), the language model, and the trans-
lation model, for question retrieval on CQA
data, and the experimental results showed
that the translation model outperforms the
others. However, they focused only on similar-
ity measures between queries (questions) and
question titles.
In subsequent work (Xue et al., 2008), a
translation-based language model combining
the translation model and the language model
for question retrieval was proposed. The
results showed that translation models help
question retrieval since they could effectively
address the word mismatch problem of ques-
tions. Additionally, they also explored an-
swers in question retrieval.
Duan et al. (2008) proposed a solution that
made use of question structures for retrieval
by building a structure tree for questions in
a category of Yahoo! Answers, which gave
more weight to important phrases in question
matching.
Wang et al. (2009) employed a parser to
build syntactic trees for questions, and ques-
tions were ranked based on the similarity be-
tween their syntactic trees and that of the
query question.
It is worth noting that our method is to-
tally different to the work (Cai et al., 2006)
of the same name. They regard documents
as matrices, or the second order tensors to
generate a low rank approximations of ma-
trices (Ye, 2005). For example, they convert
a 1, 000, 000-dimensional vector of word space
into a 1000 × 1000 matrix. However in our
model, a document is still represented by a
vector. We just project a higher-dimensional
vector to a lower-dimensional vector, but not
a matrix in Cai’s model. A 3rd-order tensor is
</bodyText>
<page confidence="0.996283">
435
</page>
<bodyText confidence="0.9727515">
also introduced in our model for better repre-
sentation for CQA corpus.
</bodyText>
<sectionHeader confidence="0.89276" genericHeader="method">
3 Latent Semantic Indexing
</sectionHeader>
<bodyText confidence="0.99957255">
Latent Semantic Indexing (LSI) (Deer-
wester et al., 1990), also called Latent Seman-
tic Analysis (LSA), is an approach to auto-
matic indexing and information retrieval that
attempts to overcome these problems by map-
ping documents as well as terms to a represen-
tation in the so-called latent semantic space.
The key idea of LSI is to map documents
(and by symmetry terms) to a low dimen-
sional vector space, the latent semantic space.
This mapping is computed by decomposing
the term-document matrix N with SVD, N =
UEVt, where U and V are orthogonal matri-
ces UtU = VtV = I and the diagonal matrix
E contains the singular values of N. The LSA
approximation of N is computed by just keep
the largest K singular values in E, which is
rank K optimal in the sense of the L2-norm.
LSI has proven to result in more robust word
processing in many applications.
</bodyText>
<sectionHeader confidence="0.973182" genericHeader="method">
4 Tensor Analysis for CQA
</sectionHeader>
<subsectionHeader confidence="0.868722">
4.1 Tensor Algebra
</subsectionHeader>
<bodyText confidence="0.998193238095238">
We first introduce the notation and basic
definitions of multilinear algebra. Scalars are
denoted by lower case letters (a, b,... ), vectors
by bold lower case letters (a, b,... ), matri-
ces by bold upper-case letters (A, B,... ), and
higher-order tensors by calligraphic upper-case
letters (A, 13,... ).
A tensor, also known as n-way array, is a
higher order generalization of a vector (first
order tensor) and a matrix (second order ten-
sor). The order of tensor D E RI1xI2x···xI is
N. An element of D is denoted as di1,..., .
An Nth-order tensor can be flattened into
a matrix by N ways. We denote the matrix
D(n) as the mode-n flattening of D (Kolda,
2002).
Similar with a matrix, an Nth-order tensor
can be decomposed through “N-mode singu-
lar value decomposition (SVD)”, which is a an
extension of SVD that expresses the tensor as
the mode-n product of N-orthogonal spaces.
</bodyText>
<equation confidence="0.916547">
D = i X1U1 X2U2 ···XnUn ···XN UN. (1)
</equation>
<bodyText confidence="0.9978742">
Tensor i, known as the core tensor, is analo-
gous to the diagonal singular value matrix in
conventional matrix SVD. i is in general a
full tensor. The core tensor governs the in-
teraction between the mode matrices Un, for
n = 1, ... , N. Mode matrix Un contains the
orthogonal left singular vectors of the mode-n
flattened matrix D(n).
The N-mode SVD algorithm for decompos-
ing D is as follows:
</bodyText>
<listItem confidence="0.985295">
1. For n = 1, ... , N, compute matrix Un in
Eq.(1) by computing the SVD of the flat-
tened matrix D(n) and setting Un to be
the left matrix of the SVD.
2. Solve for the core tensor as follows i =
D X1 UT1 X2 UT2 ··· Xn UTn ··· XN UTN.
</listItem>
<subsectionHeader confidence="0.887482">
4.2 CQA Tensor
</subsectionHeader>
<bodyText confidence="0.994050571428571">
Given a collection of CQA triples, (qi, ci, ai)
(i = 1, ... , K), where qi is the question and
ci and ai are the content and answer of qi
respectively. We can use a 3-order tensor
D E RKx3xT to represent the collection, where
T is the number of terms. The first dimension
corresponds to entries, the second dimension,
to parts and the third dimension, to the terms.
For example, the flattened matrix of CQA
tensor with “terms” direction is composed
by three sub-matrices MTitle, MContent and
MAnswer, as was illustrated in Figure 1. Each
sub-matrix is equivalent to the traditional
document-term matrix.
</bodyText>
<figureCaption confidence="0.9756125">
Figure 1: Flattening CQA tensor with “terms”
(right matrix)and “entries” (bottom matrix)
</figureCaption>
<bodyText confidence="0.890734">
Denote pi,j to be part j of entry i. Then we
</bodyText>
<page confidence="0.998601">
436
</page>
<bodyText confidence="0.999646857142857">
have the term frequency, defined as follows.
where ni,j,k is the number of occurrences of the
considered term (tk) in pi,j, and the denomi-
nator is the sum of number of occurrences of
all terms in pi,j.
The inverse document frequency is a mea-
sure of the general importance of the term.
</bodyText>
<equation confidence="0.999709">
idfj,k = log |K|(3)
1 + Ei I (tk ∈ pi,j)
</equation>
<bodyText confidence="0.946818666666667">
where |K |is the total number of entries and
I(·) is the indicator function.
Then the element di,j,k of tensor D is
</bodyText>
<equation confidence="0.967405">
di,j,k = tfi,j,k × idfj,k. (4)
</equation>
<subsectionHeader confidence="0.927595">
4.4 Question Retrieval
</subsectionHeader>
<bodyText confidence="0.999139333333333">
In order to retrieve similar question effec-
tively, we project each CQA triple DQ ∈
R1×3×T to the term space by
</bodyText>
<equation confidence="0.962561">
Di = Di ×3 U′T Term. (6)
</equation>
<bodyText confidence="0.999956615384616">
Given a new question only with title part,
we can represent it by tensor DQ ∈ R1×3×T,
and its MContent and MAnswer are zero ma-
trices. Then we project DQ to the term space
and get DQ.
Here, DQ and Di are degraded tensors and
can be regarded as matrices. Thus, we can cal-
culate the similarity between DQ and Di with
normalized Frobenius inner product.
For two matrices A and B, the Frobenius
inner product, indicated as A : B, is the
component-wise inner product of two matrices
as though they are vectors.
</bodyText>
<equation confidence="0.536474333333333">
tfi,j,k = ni,j,k
Ei ni,j,k
4.3 Latent Semantic Tensor Indexing �A : B = Ai,jBi,j (7)
For the CQA tensor, we can decompose it i,j
as illustrated in Figure 2.
D = Z ×1 UEntry ×2 UPart ×3 UTerm, (5)
</equation>
<bodyText confidence="0.999887">
where UEntry, UPart and UTerm are left sin-
gular matrices of corresponding flattened ma-
trices. UTerm spans the term space, and we
just use the vectors corresponding to the 1, 000
largest singular values in this paper, denoted
as U′Term.
</bodyText>
<figureCaption confidence="0.993514">
Figure 2: 3-mode SVD of CQA tensor
</figureCaption>
<bodyText confidence="0.999785857142857">
To deal with such a huge sparse data set, we
use singular value decomposition (SVD) im-
plemented in Apache Mahout3 machine learn-
ing library, which is implemented on top
of Apache Hadoop4 using the map/reduce
paradigm and scalable to reasonably large
data sets.
</bodyText>
<footnote confidence="0.999931">
3http://mahout.apache.org/
4http://hadoop.apache.org
</footnote>
<bodyText confidence="0.9812005">
To reduce the affect of length, we use the
normalized Frobenius inner product.
</bodyText>
<equation confidence="0.951369">
√ √ (8)
A : A × B : B
</equation>
<bodyText confidence="0.9979662">
While given a new question both with title
and content parts, MContent is not a zero ma-
trix and could be also employed in the question
retrieval process. A simple strategy is to sum
up the scores of two parts.
</bodyText>
<sectionHeader confidence="0.997046" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.882497">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999976692307692">
We collected the resolved CQA triples from
the “computer” category of Yahoo! Answers
and Baidu Zhidao websites. We just selected
the resolved questions that already have been
given their best answers. The CQA triples are
preprocessed with stopwords removal (Chinese
sentences are segmented into words in advance
by FudanNLP toolkit(Qiu et al., 2013)).
In order to evaluate our retrieval system, we
divide our dataset into two parts. The first
part is used as training dataset; the rest is used
as test dataset for evaluation. The datasets are
shown in Table 2.
</bodyText>
<figure confidence="0.718205">
A : B
A : B =
</figure>
<page confidence="0.990993">
437
</page>
<table confidence="0.993440363636364">
DataSet training test data
data size size
Baidu Zhidao 423k 1000
Yahoo! Answers 300k 1000
Table 2: Statistics of Collected Datasets
Methods MAP
Okapi 0.359
LSI 0.387
(Jeon et al., 2005b) 0.372
(Xue et al., 2008) 0.381
LSTI 0.415
</table>
<tableCaption confidence="0.97254">
Table 3: Retrieval Performance on Dataset
from Yahoo! Answers
</tableCaption>
<subsectionHeader confidence="0.973316">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99993947826087">
We compare our method with two baseline
methods: Okapi BM25 and LSI and two state-
of-the-art methods: (Jeon et al., 2005b)(Xue
et al., 2008). In LSI, we regard each triple
as a single document. Three annotators are
involved in the evaluation process. Given a
returned result, two annotators are asked to
label it with “relevant” or “irrelevant”. If an
annotator considers the returned result seman-
tically equivalent to the queried question, he
labels it as “relevant”; otherwise, it is labeled
as “irrelevant”. If a conflict happens, the third
annotator will make the final judgement.
We use mean average precision (MAP)
to evaluate the effectiveness of each method.
The experiment results are illustrated in Ta-
ble 3 and 4, which show that our method out-
performs the others on both datasets.
The primary reason is that we incorpo-
rate the content of the question body and
the answer parts into the process of ques-
tion retrieval, which should provide addi-
tional relevance information. Different to
</bodyText>
<table confidence="0.945921">
Methods MAP
Okapi 0.423
LSI 0.490
(Jeon et al., 2005b) 0.498
(Xue et al., 2008) 0.512
LSTI 0.523
</table>
<tableCaption confidence="0.9341515">
Table 4: Retrieval Performance on Dataset
from Baidu Zhidao
</tableCaption>
<bodyText confidence="0.999854076923077">
the translation-based methods, our method
can capture the mapping relations in three
parts (question, content and answer) simulta-
neously.
It is worth noting that the problem of data
sparsity is more crucial for LSTI since the size
of a tensor in LSTI is larger than a term-
document matrix in LSI. When the size of data
is small, LSTI tends to just align the common
words and thus cannot find the correspond-
ing relations among the focus words in CQA
triples. Therefore, more CQA triples may re-
sult in better performance for our method.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999207">
In this paper, we proposed a novel re-
trieval approach for community-based QA,
called LSTI, which analyzes the CQA triples
with naturally tensor representation. LSTI
is a unified model and effectively resolves the
problem of lexical chasm for question retrieval.
For future research, we will extend LSTI to
a probabilistic form (Hofmann, 1999) for bet-
ter scalability and investigate its performance
with a larger corpus.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988808">
We would like to thank the anony-
mous reviewers for their valuable com-
ments. This work was funded by NSFC
(No.61003091 and No.61073069) and 973 Pro-
gram (No.2010CB327900).
</bodyText>
<sectionHeader confidence="0.998828" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982550111111111">
M.J. Blooma and J.C. Kurian. 2011. Research
issues in community based question answering.
In PACIS 2011 Proceedings.
R. Burke, K. Hammond, V. Kulyukin, S. Lytinen,
N. Tomuro, and S. Schoenberg. 1997. Ques-
tion answering from frequently asked question
files: Experiences with the faq finder system.
AI Magazine, 18(2):57–66.
Deng Cai, Xiaofei He, and Jiawei Han. 2006. Ten-
sor space model for document analysis. In SI-
GIR ’06: Proceedings of the 29th annual inter-
national ACM SIGIR conference on Research
and development in information retrieval.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K.
Landauer, and R. Harshman. 1990. Index-
ing by latent semantic analysis. Journal of
the American society for information science,
41(6):391–407.
</reference>
<page confidence="0.995029">
438
</page>
<reference confidence="0.998542222222222">
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and
Yong Yu. 2008. Searching questions by iden-
tifying question topic and question focus. In
Proceedings of ACL-08: HLT, pages 156–164,
Columbus, Ohio, June. Association for Compu-
tational Linguistics.
T. Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual in-
ternational ACM SIGIR conference on Research
and development in information retrieval, pages
50–57. ACM Press New York, NY, USA.
J. Jeon, W.B. Croft, and J.H. Lee. 2005a. Find-
ing semantically similar questions based on their
answers. In Proceedings of the 28th annual in-
ternational ACM SIGIR conference on Research
and development in information retrieval, pages
617–618. ACM.
J. Jeon, W.B. Croft, and J.H. Lee. 2005b. Finding
similar questions in large question and answer
archives. Proceedings of the 14th ACM interna-
tional conference on Information and knowledge
management, pages 84–90.
V. Jijkoun and M. de Rijke. 2005. Retrieving an-
swers from frequently asked questions pages on
the web. Proceedings of the 14th ACM interna-
tional conference on Information and knowledge
management, pages 76–83.
T.G. Kolda. 2002. Orthogonal tensor decompo-
sitions. SIAM Journal on Matrix Analysis and
Applications, 23(1):243–255.
Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013.
Fudannlp: A toolkit for chinese natural lan-
guage processing. In Proceedings of ACL.
S. Riezler, A. Vasserman, I. Tsochantaridis,
V. Mittal, and Y. Liu. 2007. Statistical ma-
chine translation for query expansion in answer
retrieval. In Proceedings of the Annual Meeting
of the Association for Computational Linguis-
tics.
S.E. Robertson, S. Walker, S. Jones, M.M.
Hancock-Beaulieu, and M. Gatford. 1994.
Okapi at trec-3. In TREC, pages 109–126.
K. Wang, Z. Ming, and T.S. Chua. 2009. A syn-
tactic tree matching approach to finding similar
questions in community-based QA services. In
Proceedings of the 32nd international ACM SI-
GIR conference on Research and development in
information retrieval, pages 187–194. ACM.
X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval
models for question and answer archives. In Pro-
ceedings of the 31st annual international ACM
SIGIR conference on Research and development
in information retrieval, pages 475–482. ACM.
J.M. Ye. 2005. Generalized low rank approxima-
tions of matrices. Mach. Learn., 61(1):167–191.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011.
Phrase-based translation model for question re-
trieval in community question answer archives.
In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages
653–662. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999269">
439
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.501065">
<title confidence="0.861176333333333">Latent Semantic Tensor for Community-based Question Answering Xipeng Qiu, Le Tian, Xuanjing</title>
<address confidence="0.836415">Fudan University, 825 Zhangheng Road, Shanghai, China</address>
<email confidence="0.980401">xpqiu@fudan.edu.cn,tianlefdu@gmail.com,xjhuang@fudan.edu.cn</email>
<abstract confidence="0.998692933333333">Retrieving similar questions is very important in community-based question answering(CQA). In this paper, we propose a unified question retrieval model based on latent semantic indexing with tensor analysis, which can capture word associations among different parts of CQA triples simultaneously. Thus, our method can reduce lexical chasm of question retrieval with the help of the information of question content and answer parts. The experimental result shows that our method outperforms the traditional methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M J Blooma</author>
<author>J C Kurian</author>
</authors>
<title>Research issues in community based question answering.</title>
<date>2011</date>
<booktitle>In PACIS 2011 Proceedings.</booktitle>
<contexts>
<context position="2688" citStr="Blooma and Kurian, 2011" startWordPosition="403" endWordPosition="406">and ranking answers is very challenging. The earlier studies mainly focus on generating redundant features, or finding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and se mantic gap. In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(Blooma and Kurian, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–439, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics pipeline methods: (1) modeling word association; (2) question retrieval combined with other models, such as vector space model (VSM), Okapi model (Roberts</context>
</contexts>
<marker>Blooma, Kurian, 2011</marker>
<rawString>M.J. Blooma and J.C. Kurian. 2011. Research issues in community based question answering. In PACIS 2011 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Burke</author>
<author>K Hammond</author>
<author>V Kulyukin</author>
<author>S Lytinen</author>
<author>N Tomuro</author>
<author>S Schoenberg</author>
</authors>
<title>Question answering from frequently asked question files: Experiences with the faq finder system.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="5428" citStr="Burke et al., 1997" startWordPosition="839" endWordPosition="842">s: questionquestion, question-body and question-answer. The rest of the paper is organized as follows: Section 3 introduces the concept of LSI. Section 4 presents our method. Section 5 describes the experimental analysis. Section 6 concludes the paper. 2 Related Works There are some related works on question retrieval in CQA. Various query expansion techniques have been studied to solve word mismatch problems between queries and documents. The early works on question retrieval can be traced back to finding similar questions in Frequently Asked Questions (FAQ) archives, such as the FAQ finder (Burke et al., 1997), which usually used statistical and semantic similarity measures to rank FAQs. Jeon et al. (2005a; 2005b) compared four different retrieval methods, i.e., the vector space model(Jijkoun and de Rijke, 2005), the Okapi BM25 model (Robertson et al., 1994), the language model, and the translation model, for question retrieval on CQA data, and the experimental results showed that the translation model outperforms the others. However, they focused only on similarity measures between queries (questions) and question titles. In subsequent work (Xue et al., 2008), a translation-based language model co</context>
</contexts>
<marker>Burke, Hammond, Kulyukin, Lytinen, Tomuro, Schoenberg, 1997</marker>
<rawString>R. Burke, K. Hammond, V. Kulyukin, S. Lytinen, N. Tomuro, and S. Schoenberg. 1997. Question answering from frequently asked question files: Experiences with the faq finder system. AI Magazine, 18(2):57–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deng Cai</author>
<author>Xiaofei He</author>
<author>Jiawei Han</author>
</authors>
<title>Tensor space model for document analysis.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="6833" citStr="Cai et al., 2006" startWordPosition="1061" endWordPosition="1064">dress the word mismatch problem of questions. Additionally, they also explored answers in question retrieval. Duan et al. (2008) proposed a solution that made use of question structures for retrieval by building a structure tree for questions in a category of Yahoo! Answers, which gave more weight to important phrases in question matching. Wang et al. (2009) employed a parser to build syntactic trees for questions, and questions were ranked based on the similarity between their syntactic trees and that of the query question. It is worth noting that our method is totally different to the work (Cai et al., 2006) of the same name. They regard documents as matrices, or the second order tensors to generate a low rank approximations of matrices (Ye, 2005). For example, they convert a 1, 000, 000-dimensional vector of word space into a 1000 × 1000 matrix. However in our model, a document is still represented by a vector. We just project a higher-dimensional vector to a lower-dimensional vector, but not a matrix in Cai’s model. A 3rd-order tensor is 435 also introduced in our model for better representation for CQA corpus. 3 Latent Semantic Indexing Latent Semantic Indexing (LSI) (Deerwester et al., 1990),</context>
</contexts>
<marker>Cai, He, Han, 2006</marker>
<rawString>Deng Cai, Xiaofei He, and Jiawei Han. 2006. Tensor space model for document analysis. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American society for information science,</journal>
<pages>41--6</pages>
<contexts>
<context position="3645" citStr="Deerwester et al., 1990" startWordPosition="551" endWordPosition="554">l Linguistics, pages 434–439, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics pipeline methods: (1) modeling word association; (2) question retrieval combined with other models, such as vector space model (VSM), Okapi model (Robertson et al., 1994) or language model (LM). The pipeline methods often have many non-trivial experimental setting and result to be very hard to reproduce. In this paper, we propose a novel unified retrieval model for CQA, latent semantic tensor indexing (LSTI), which is an extension of the conventional latent semantic indexing (LSI) (Deerwester et al., 1990). Similar to LSI, LSTI can integrate the two detached parts (modeling word association and question retrieval) into a single model. In traditional document retrieval, LSI is an effective method to overcome two of the most severe constraints on Boolean keyword queries: synonymy, that is, multiple words with similar meanings, and polysemy, or words with more than one meanings. Usually in a CQA archive, each entry (or question) is in the following triple form:⟨question title, question content, answer⟩. Because the performance based solely on the content or the answer part is less than satisfactor</context>
<context position="7432" citStr="Deerwester et al., 1990" startWordPosition="1162" endWordPosition="1166">e work (Cai et al., 2006) of the same name. They regard documents as matrices, or the second order tensors to generate a low rank approximations of matrices (Ye, 2005). For example, they convert a 1, 000, 000-dimensional vector of word space into a 1000 × 1000 matrix. However in our model, a document is still represented by a vector. We just project a higher-dimensional vector to a lower-dimensional vector, but not a matrix in Cai’s model. A 3rd-order tensor is 435 also introduced in our model for better representation for CQA corpus. 3 Latent Semantic Indexing Latent Semantic Indexing (LSI) (Deerwester et al., 1990), also called Latent Semantic Analysis (LSA), is an approach to automatic indexing and information retrieval that attempts to overcome these problems by mapping documents as well as terms to a representation in the so-called latent semantic space. The key idea of LSI is to map documents (and by symmetry terms) to a low dimensional vector space, the latent semantic space. This mapping is computed by decomposing the term-document matrix N with SVD, N = UEVt, where U and V are orthogonal matrices UtU = VtV = I and the diagonal matrix E contains the singular values of N. The LSA approximation of N</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huizhong Duan</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Yong Yu</author>
</authors>
<title>Searching questions by identifying question topic and question focus.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>156--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6344" citStr="Duan et al. (2008)" startWordPosition="976" endWordPosition="979">el, for question retrieval on CQA data, and the experimental results showed that the translation model outperforms the others. However, they focused only on similarity measures between queries (questions) and question titles. In subsequent work (Xue et al., 2008), a translation-based language model combining the translation model and the language model for question retrieval was proposed. The results showed that translation models help question retrieval since they could effectively address the word mismatch problem of questions. Additionally, they also explored answers in question retrieval. Duan et al. (2008) proposed a solution that made use of question structures for retrieval by building a structure tree for questions in a category of Yahoo! Answers, which gave more weight to important phrases in question matching. Wang et al. (2009) employed a parser to build syntactic trees for questions, and questions were ranked based on the similarity between their syntactic trees and that of the query question. It is worth noting that our method is totally different to the work (Cai et al., 2006) of the same name. They regard documents as matrices, or the second order tensors to generate a low rank approx</context>
</contexts>
<marker>Duan, Cao, Lin, Yu, 2008</marker>
<rawString>Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong Yu. 2008. Searching questions by identifying question topic and question focus. In Proceedings of ACL-08: HLT, pages 156–164, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>50--57</pages>
<publisher>ACM Press</publisher>
<location>New York, NY, USA.</location>
<marker>Hofmann, 1999</marker>
<rawString>T. Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50–57. ACM Press New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
<author>J H Lee</author>
</authors>
<title>Finding semantically similar questions based on their answers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>617--618</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1510" citStr="Jeon et al., 2005" startWordPosition="222" endWordPosition="225">rvice in recent years. Unlike traditional question answering (QA), information seekers can post their questions on a CQA website which are later answered by other users. However, with the increase of the CQA archive, there accumulate massive duplicate questions on CQA websites. One of the primary reasons is that information seekers cannot retrieve answers they need and thus post another new question consequently. Therefore, it becomes more and more important to find semantically similar questions. The major challenge for CQA retrieval is the lexical gap (or lexical chasm) among the questions (Jeon et al., 2005b; Xue et al., 2008), 1http://answers.yahoo.com/ 2http://zhidao.baidu.com/ Query: Q: Why is my laptop screen blinking? Expected: Q1: How to troubleshoot a flashing screen on an LCD monitor? Not Expected: Q2: How to blinking text on screen with PowerPoint? Table 1: An example on question retrieval as shown in Table 1. Since question-answer pairs are usually short, the word mismatching problem is especially important. However, due to the lexical gap between questions and answers as well as spam typically existing in user-generated content, filtering and ranking answers is very challenging. The e</context>
<context position="5525" citStr="Jeon et al. (2005" startWordPosition="855" endWordPosition="858">ws: Section 3 introduces the concept of LSI. Section 4 presents our method. Section 5 describes the experimental analysis. Section 6 concludes the paper. 2 Related Works There are some related works on question retrieval in CQA. Various query expansion techniques have been studied to solve word mismatch problems between queries and documents. The early works on question retrieval can be traced back to finding similar questions in Frequently Asked Questions (FAQ) archives, such as the FAQ finder (Burke et al., 1997), which usually used statistical and semantic similarity measures to rank FAQs. Jeon et al. (2005a; 2005b) compared four different retrieval methods, i.e., the vector space model(Jijkoun and de Rijke, 2005), the Okapi BM25 model (Robertson et al., 1994), the language model, and the translation model, for question retrieval on CQA data, and the experimental results showed that the translation model outperforms the others. However, they focused only on similarity measures between queries (questions) and question titles. In subsequent work (Xue et al., 2008), a translation-based language model combining the translation model and the language model for question retrieval was proposed. The res</context>
<context position="13585" citStr="Jeon et al., 2005" startWordPosition="2281" endWordPosition="2284">tions that already have been given their best answers. The CQA triples are preprocessed with stopwords removal (Chinese sentences are segmented into words in advance by FudanNLP toolkit(Qiu et al., 2013)). In order to evaluate our retrieval system, we divide our dataset into two parts. The first part is used as training dataset; the rest is used as test dataset for evaluation. The datasets are shown in Table 2. A : B A : B = 437 DataSet training test data data size size Baidu Zhidao 423k 1000 Yahoo! Answers 300k 1000 Table 2: Statistics of Collected Datasets Methods MAP Okapi 0.359 LSI 0.387 (Jeon et al., 2005b) 0.372 (Xue et al., 2008) 0.381 LSTI 0.415 Table 3: Retrieval Performance on Dataset from Yahoo! Answers 5.2 Evaluation We compare our method with two baseline methods: Okapi BM25 and LSI and two stateof-the-art methods: (Jeon et al., 2005b)(Xue et al., 2008). In LSI, we regard each triple as a single document. Three annotators are involved in the evaluation process. Given a returned result, two annotators are asked to label it with “relevant” or “irrelevant”. If an annotator considers the returned result semantically equivalent to the queried question, he labels it as “relevant”; otherwise,</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>J. Jeon, W.B. Croft, and J.H. Lee. 2005a. Finding semantically similar questions based on their answers. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 617–618. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
<author>J H Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>84--90</pages>
<contexts>
<context position="1510" citStr="Jeon et al., 2005" startWordPosition="222" endWordPosition="225">rvice in recent years. Unlike traditional question answering (QA), information seekers can post their questions on a CQA website which are later answered by other users. However, with the increase of the CQA archive, there accumulate massive duplicate questions on CQA websites. One of the primary reasons is that information seekers cannot retrieve answers they need and thus post another new question consequently. Therefore, it becomes more and more important to find semantically similar questions. The major challenge for CQA retrieval is the lexical gap (or lexical chasm) among the questions (Jeon et al., 2005b; Xue et al., 2008), 1http://answers.yahoo.com/ 2http://zhidao.baidu.com/ Query: Q: Why is my laptop screen blinking? Expected: Q1: How to troubleshoot a flashing screen on an LCD monitor? Not Expected: Q2: How to blinking text on screen with PowerPoint? Table 1: An example on question retrieval as shown in Table 1. Since question-answer pairs are usually short, the word mismatching problem is especially important. However, due to the lexical gap between questions and answers as well as spam typically existing in user-generated content, filtering and ranking answers is very challenging. The e</context>
<context position="5525" citStr="Jeon et al. (2005" startWordPosition="855" endWordPosition="858">ws: Section 3 introduces the concept of LSI. Section 4 presents our method. Section 5 describes the experimental analysis. Section 6 concludes the paper. 2 Related Works There are some related works on question retrieval in CQA. Various query expansion techniques have been studied to solve word mismatch problems between queries and documents. The early works on question retrieval can be traced back to finding similar questions in Frequently Asked Questions (FAQ) archives, such as the FAQ finder (Burke et al., 1997), which usually used statistical and semantic similarity measures to rank FAQs. Jeon et al. (2005a; 2005b) compared four different retrieval methods, i.e., the vector space model(Jijkoun and de Rijke, 2005), the Okapi BM25 model (Robertson et al., 1994), the language model, and the translation model, for question retrieval on CQA data, and the experimental results showed that the translation model outperforms the others. However, they focused only on similarity measures between queries (questions) and question titles. In subsequent work (Xue et al., 2008), a translation-based language model combining the translation model and the language model for question retrieval was proposed. The res</context>
<context position="13585" citStr="Jeon et al., 2005" startWordPosition="2281" endWordPosition="2284">tions that already have been given their best answers. The CQA triples are preprocessed with stopwords removal (Chinese sentences are segmented into words in advance by FudanNLP toolkit(Qiu et al., 2013)). In order to evaluate our retrieval system, we divide our dataset into two parts. The first part is used as training dataset; the rest is used as test dataset for evaluation. The datasets are shown in Table 2. A : B A : B = 437 DataSet training test data data size size Baidu Zhidao 423k 1000 Yahoo! Answers 300k 1000 Table 2: Statistics of Collected Datasets Methods MAP Okapi 0.359 LSI 0.387 (Jeon et al., 2005b) 0.372 (Xue et al., 2008) 0.381 LSTI 0.415 Table 3: Retrieval Performance on Dataset from Yahoo! Answers 5.2 Evaluation We compare our method with two baseline methods: Okapi BM25 and LSI and two stateof-the-art methods: (Jeon et al., 2005b)(Xue et al., 2008). In LSI, we regard each triple as a single document. Three annotators are involved in the evaluation process. Given a returned result, two annotators are asked to label it with “relevant” or “irrelevant”. If an annotator considers the returned result semantically equivalent to the queried question, he labels it as “relevant”; otherwise,</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>J. Jeon, W.B. Croft, and J.H. Lee. 2005b. Finding similar questions in large question and answer archives. Proceedings of the 14th ACM international conference on Information and knowledge management, pages 84–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M de Rijke</author>
</authors>
<title>Retrieving answers from frequently asked questions pages on the web.</title>
<date>2005</date>
<booktitle>Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>76--83</pages>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>V. Jijkoun and M. de Rijke. 2005. Retrieving answers from frequently asked questions pages on the web. Proceedings of the 14th ACM international conference on Information and knowledge management, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Kolda</author>
</authors>
<title>Orthogonal tensor decompositions.</title>
<date>2002</date>
<journal>SIAM Journal on Matrix Analysis and Applications,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="8928" citStr="Kolda, 2002" startWordPosition="1435" endWordPosition="1436">nitions of multilinear algebra. Scalars are denoted by lower case letters (a, b,... ), vectors by bold lower case letters (a, b,... ), matrices by bold upper-case letters (A, B,... ), and higher-order tensors by calligraphic upper-case letters (A, 13,... ). A tensor, also known as n-way array, is a higher order generalization of a vector (first order tensor) and a matrix (second order tensor). The order of tensor D E RI1xI2x···xI is N. An element of D is denoted as di1,..., . An Nth-order tensor can be flattened into a matrix by N ways. We denote the matrix D(n) as the mode-n flattening of D (Kolda, 2002). Similar with a matrix, an Nth-order tensor can be decomposed through “N-mode singular value decomposition (SVD)”, which is a an extension of SVD that expresses the tensor as the mode-n product of N-orthogonal spaces. D = i X1U1 X2U2 ···XnUn ···XN UN. (1) Tensor i, known as the core tensor, is analogous to the diagonal singular value matrix in conventional matrix SVD. i is in general a full tensor. The core tensor governs the interaction between the mode matrices Un, for n = 1, ... , N. Mode matrix Un contains the orthogonal left singular vectors of the mode-n flattened matrix D(n). The N-mod</context>
</contexts>
<marker>Kolda, 2002</marker>
<rawString>T.G. Kolda. 2002. Orthogonal tensor decompositions. SIAM Journal on Matrix Analysis and Applications, 23(1):243–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
</authors>
<title>Fudannlp: A toolkit for chinese natural language processing.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13171" citStr="Qiu et al., 2013" startWordPosition="2204" endWordPosition="2207">product. √ √ (8) A : A × B : B While given a new question both with title and content parts, MContent is not a zero matrix and could be also employed in the question retrieval process. A simple strategy is to sum up the scores of two parts. 5 Experiments 5.1 Datasets We collected the resolved CQA triples from the “computer” category of Yahoo! Answers and Baidu Zhidao websites. We just selected the resolved questions that already have been given their best answers. The CQA triples are preprocessed with stopwords removal (Chinese sentences are segmented into words in advance by FudanNLP toolkit(Qiu et al., 2013)). In order to evaluate our retrieval system, we divide our dataset into two parts. The first part is used as training dataset; the rest is used as test dataset for evaluation. The datasets are shown in Table 2. A : B A : B = 437 DataSet training test data data size size Baidu Zhidao 423k 1000 Yahoo! Answers 300k 1000 Table 2: Statistics of Collected Datasets Methods MAP Okapi 0.359 LSI 0.387 (Jeon et al., 2005b) 0.372 (Xue et al., 2008) 0.381 LSTI 0.415 Table 3: Retrieval Performance on Dataset from Yahoo! Answers 5.2 Evaluation We compare our method with two baseline methods: Okapi BM25 and </context>
</contexts>
<marker>Qiu, Zhang, Huang, 2013</marker>
<rawString>Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. Fudannlp: A toolkit for chinese natural language processing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>A Vasserman</author>
<author>I Tsochantaridis</author>
<author>V Mittal</author>
<author>Y Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2745" citStr="Riezler et al., 2007" startWordPosition="411" endWordPosition="414">mainly focus on generating redundant features, or finding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and se mantic gap. In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(Blooma and Kurian, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–439, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics pipeline methods: (1) modeling word association; (2) question retrieval combined with other models, such as vector space model (VSM), Okapi model (Robertson et al., 1994) or language model (LM). The pipeline met</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>S Jones</author>
<author>M M Hancock-Beaulieu</author>
<author>M Gatford</author>
</authors>
<title>Okapi at trec-3. In</title>
<date>1994</date>
<booktitle>TREC,</booktitle>
<pages>109--126</pages>
<contexts>
<context position="3304" citStr="Robertson et al., 1994" startWordPosition="494" endWordPosition="497">, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–439, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics pipeline methods: (1) modeling word association; (2) question retrieval combined with other models, such as vector space model (VSM), Okapi model (Robertson et al., 1994) or language model (LM). The pipeline methods often have many non-trivial experimental setting and result to be very hard to reproduce. In this paper, we propose a novel unified retrieval model for CQA, latent semantic tensor indexing (LSTI), which is an extension of the conventional latent semantic indexing (LSI) (Deerwester et al., 1990). Similar to LSI, LSTI can integrate the two detached parts (modeling word association and question retrieval) into a single model. In traditional document retrieval, LSI is an effective method to overcome two of the most severe constraints on Boolean keyword</context>
<context position="5681" citStr="Robertson et al., 1994" startWordPosition="878" endWordPosition="881">er. 2 Related Works There are some related works on question retrieval in CQA. Various query expansion techniques have been studied to solve word mismatch problems between queries and documents. The early works on question retrieval can be traced back to finding similar questions in Frequently Asked Questions (FAQ) archives, such as the FAQ finder (Burke et al., 1997), which usually used statistical and semantic similarity measures to rank FAQs. Jeon et al. (2005a; 2005b) compared four different retrieval methods, i.e., the vector space model(Jijkoun and de Rijke, 2005), the Okapi BM25 model (Robertson et al., 1994), the language model, and the translation model, for question retrieval on CQA data, and the experimental results showed that the translation model outperforms the others. However, they focused only on similarity measures between queries (questions) and question titles. In subsequent work (Xue et al., 2008), a translation-based language model combining the translation model and the language model for question retrieval was proposed. The results showed that translation models help question retrieval since they could effectively address the word mismatch problem of questions. Additionally, they </context>
</contexts>
<marker>Robertson, Walker, Jones, Hancock-Beaulieu, Gatford, 1994</marker>
<rawString>S.E. Robertson, S. Walker, S. Jones, M.M. Hancock-Beaulieu, and M. Gatford. 1994. Okapi at trec-3. In TREC, pages 109–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wang</author>
<author>Z Ming</author>
<author>T S Chua</author>
</authors>
<title>A syntactic tree matching approach to finding similar questions in community-based QA services.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>187--194</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2833" citStr="Wang et al., 2009" startWordPosition="426" endWordPosition="429">ning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and se mantic gap. In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(Blooma and Kurian, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–439, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics pipeline methods: (1) modeling word association; (2) question retrieval combined with other models, such as vector space model (VSM), Okapi model (Robertson et al., 1994) or language model (LM). The pipeline methods often have many non-trivial experimental setting and result to be very hard to repr</context>
<context position="6576" citStr="Wang et al. (2009)" startWordPosition="1014" endWordPosition="1017">sequent work (Xue et al., 2008), a translation-based language model combining the translation model and the language model for question retrieval was proposed. The results showed that translation models help question retrieval since they could effectively address the word mismatch problem of questions. Additionally, they also explored answers in question retrieval. Duan et al. (2008) proposed a solution that made use of question structures for retrieval by building a structure tree for questions in a category of Yahoo! Answers, which gave more weight to important phrases in question matching. Wang et al. (2009) employed a parser to build syntactic trees for questions, and questions were ranked based on the similarity between their syntactic trees and that of the query question. It is worth noting that our method is totally different to the work (Cai et al., 2006) of the same name. They regard documents as matrices, or the second order tensors to generate a low rank approximations of matrices (Ye, 2005). For example, they convert a 1, 000, 000-dimensional vector of word space into a 1000 × 1000 matrix. However in our model, a document is still represented by a vector. We just project a higher-dimensi</context>
</contexts>
<marker>Wang, Ming, Chua, 2009</marker>
<rawString>K. Wang, Z. Ming, and T.S. Chua. 2009. A syntactic tree matching approach to finding similar questions in community-based QA services. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 187–194. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Xue</author>
<author>J Jeon</author>
<author>W B Croft</author>
</authors>
<title>Retrieval models for question and answer archives.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>475--482</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1530" citStr="Xue et al., 2008" startWordPosition="226" endWordPosition="229">s. Unlike traditional question answering (QA), information seekers can post their questions on a CQA website which are later answered by other users. However, with the increase of the CQA archive, there accumulate massive duplicate questions on CQA websites. One of the primary reasons is that information seekers cannot retrieve answers they need and thus post another new question consequently. Therefore, it becomes more and more important to find semantically similar questions. The major challenge for CQA retrieval is the lexical gap (or lexical chasm) among the questions (Jeon et al., 2005b; Xue et al., 2008), 1http://answers.yahoo.com/ 2http://zhidao.baidu.com/ Query: Q: Why is my laptop screen blinking? Expected: Q1: How to troubleshoot a flashing screen on an LCD monitor? Not Expected: Q2: How to blinking text on screen with PowerPoint? Table 1: An example on question retrieval as shown in Table 1. Since question-answer pairs are usually short, the word mismatching problem is especially important. However, due to the lexical gap between questions and answers as well as spam typically existing in user-generated content, filtering and ranking answers is very challenging. The earlier studies mainl</context>
<context position="2763" citStr="Xue et al., 2008" startWordPosition="415" endWordPosition="418">ting redundant features, or finding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and se mantic gap. In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(Blooma and Kurian, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–439, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics pipeline methods: (1) modeling word association; (2) question retrieval combined with other models, such as vector space model (VSM), Okapi model (Robertson et al., 1994) or language model (LM). The pipeline methods often have ma</context>
<context position="4368" citStr="Xue et al., 2008" startWordPosition="662" endWordPosition="665">val) into a single model. In traditional document retrieval, LSI is an effective method to overcome two of the most severe constraints on Boolean keyword queries: synonymy, that is, multiple words with similar meanings, and polysemy, or words with more than one meanings. Usually in a CQA archive, each entry (or question) is in the following triple form:⟨question title, question content, answer⟩. Because the performance based solely on the content or the answer part is less than satisfactory, many works proposed that additional relevant information should be provided to help question retrieval(Xue et al., 2008). For example, if a question title contains the keyword “why”, the CQA triple, which contains “because” or “reason” in its answer part, is more likely to be what the user looks for. Since each triple in CQA has three parts, the natural representation of the CQA collection is a three-dimensional array, or 3rd-order tensor, rather than a matrix. Based on the tensor decomposition, we can model the word association simultaneously in the pairs: questionquestion, question-body and question-answer. The rest of the paper is organized as follows: Section 3 introduces the concept of LSI. Section 4 prese</context>
<context position="5989" citStr="Xue et al., 2008" startWordPosition="925" endWordPosition="928">Q) archives, such as the FAQ finder (Burke et al., 1997), which usually used statistical and semantic similarity measures to rank FAQs. Jeon et al. (2005a; 2005b) compared four different retrieval methods, i.e., the vector space model(Jijkoun and de Rijke, 2005), the Okapi BM25 model (Robertson et al., 1994), the language model, and the translation model, for question retrieval on CQA data, and the experimental results showed that the translation model outperforms the others. However, they focused only on similarity measures between queries (questions) and question titles. In subsequent work (Xue et al., 2008), a translation-based language model combining the translation model and the language model for question retrieval was proposed. The results showed that translation models help question retrieval since they could effectively address the word mismatch problem of questions. Additionally, they also explored answers in question retrieval. Duan et al. (2008) proposed a solution that made use of question structures for retrieval by building a structure tree for questions in a category of Yahoo! Answers, which gave more weight to important phrases in question matching. Wang et al. (2009) employed a p</context>
<context position="13612" citStr="Xue et al., 2008" startWordPosition="2286" endWordPosition="2289">n given their best answers. The CQA triples are preprocessed with stopwords removal (Chinese sentences are segmented into words in advance by FudanNLP toolkit(Qiu et al., 2013)). In order to evaluate our retrieval system, we divide our dataset into two parts. The first part is used as training dataset; the rest is used as test dataset for evaluation. The datasets are shown in Table 2. A : B A : B = 437 DataSet training test data data size size Baidu Zhidao 423k 1000 Yahoo! Answers 300k 1000 Table 2: Statistics of Collected Datasets Methods MAP Okapi 0.359 LSI 0.387 (Jeon et al., 2005b) 0.372 (Xue et al., 2008) 0.381 LSTI 0.415 Table 3: Retrieval Performance on Dataset from Yahoo! Answers 5.2 Evaluation We compare our method with two baseline methods: Okapi BM25 and LSI and two stateof-the-art methods: (Jeon et al., 2005b)(Xue et al., 2008). In LSI, we regard each triple as a single document. Three annotators are involved in the evaluation process. Given a returned result, two annotators are asked to label it with “relevant” or “irrelevant”. If an annotator considers the returned result semantically equivalent to the queried question, he labels it as “relevant”; otherwise, it is labeled as “irreleva</context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval models for question and answer archives. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 475–482. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ye</author>
</authors>
<title>Generalized low rank approximations of matrices.</title>
<date>2005</date>
<journal>Mach. Learn.,</journal>
<volume>61</volume>
<issue>1</issue>
<contexts>
<context position="6975" citStr="Ye, 2005" startWordPosition="1088" endWordPosition="1089">hat made use of question structures for retrieval by building a structure tree for questions in a category of Yahoo! Answers, which gave more weight to important phrases in question matching. Wang et al. (2009) employed a parser to build syntactic trees for questions, and questions were ranked based on the similarity between their syntactic trees and that of the query question. It is worth noting that our method is totally different to the work (Cai et al., 2006) of the same name. They regard documents as matrices, or the second order tensors to generate a low rank approximations of matrices (Ye, 2005). For example, they convert a 1, 000, 000-dimensional vector of word space into a 1000 × 1000 matrix. However in our model, a document is still represented by a vector. We just project a higher-dimensional vector to a lower-dimensional vector, but not a matrix in Cai’s model. A 3rd-order tensor is 435 also introduced in our model for better representation for CQA corpus. 3 Latent Semantic Indexing Latent Semantic Indexing (LSI) (Deerwester et al., 1990), also called Latent Semantic Analysis (LSA), is an approach to automatic indexing and information retrieval that attempts to overcome these pr</context>
</contexts>
<marker>Ye, 2005</marker>
<rawString>J.M. Ye. 2005. Generalized low rank approximations of matrices. Mach. Learn., 61(1):167–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>L Cai</author>
<author>J Zhao</author>
<author>K Liu</author>
</authors>
<title>Phrase-based translation model for question retrieval in community question answer archives.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>653--662</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2783" citStr="Zhou et al., 2011" startWordPosition="419" endWordPosition="422">tures, or finding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and se mantic gap. In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(Blooma and Kurian, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–439, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics pipeline methods: (1) modeling word association; (2) question retrieval combined with other models, such as vector space model (VSM), Okapi model (Robertson et al., 1994) or language model (LM). The pipeline methods often have many non-trivial exper</context>
</contexts>
<marker>Zhou, Cai, Zhao, Liu, 2011</marker>
<rawString>G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-based translation model for question retrieval in community question answer archives. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 653–662. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>