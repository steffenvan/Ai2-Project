<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998465">
Mining Search Engine Clickthrough Log for
Matching N-gram Features
</title>
<author confidence="0.992888">
Huihsin Tseng, Longbin Chen, Fan Li, Ziming Zhuang,
Lei Duan, Belle Tseng
</author>
<affiliation confidence="0.598525">
Yahoo! Inc., Santa Clara, CA 95054
</affiliation>
<email confidence="0.830424">
{huihui,longbin,fanli,ziming,leiduan,belle}@yahoo-inc.com
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994416681818182">
User clicks on a URL in response to a query are
extremely useful predictors of the URL’s rele-
vance to that query. Exact match click features
tend to suffer from severe data sparsity issues in
web ranking. Such sparsity is particularly pro-
nounced for new URLs or long queries where
each distinct query-url pair will rarely occur. To
remedy this, we present a set of straightforward
yet informative query-url n-gram features that al-
lows for generalization of limited user click data
to large amounts of unseen query-url pairs. The
method is motivated by techniques leveraged in
the NLP community for dealing with unseen
words. We find that there are interesting regulari-
ties across queries and their preferred destination
URLs; for example, queries containing “form”
tend to lead to clicks on URLs containing “pdf”.
We evaluate our set of new query-url features on
a web search ranking task and obtain improve-
ments that are statistically significant at a p-value
&lt; 0.0001 level over a strong baseline with exact
match clickthrough features.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921425925926">
Clickthrough logs record user click behaviors,
which are a critical source for improving search
relevance (Bilenko and White, 2008; Radlinski et
al., 2007; Agichtein and Zheng, 2006; Lu et al.
2006). Previous work (Agichtein et al., 2006)
demonstrated that clickthrough features (e.g.,
IsNextClicked and IsPreviousClicked) can lead
to substantial improvements in relevance. Such
features summarize query-specific user interac-
tions on a search engine. One commonly used
clickthrough feature is generated based on the
following observation: if a URL receives a large
number of first and last clicks across many user
sessions, then it indicates that this URL might be
a strongly preferred destination of a query. For
example, when a user searches for “yahoo”, they
tend to only click on the URL www.yahoo.com
rather than other alternatives. This results in
www.yahoo.com being the first and last clicked
URL for the query. We refer to such behavior as
being navigational clicks (NavClicks). Features
that use exact query and URL string matches
(e.g., NavClick, IsNextClicked and IsPrevious-
Clicked) are referred to as exact match features
(ExactM) for the remainder of this paper.
The coverage of ExactM features is sparse, espe-
cially for long queries and new URLs. Many
long queries are either unique or very low fre-
quency. Hence, the improvements from ExactM
features are limited to the more popular queries.
In addition, ExactM features tend to be weighted
heavily in the ranking of results when they are
available. This introduces a bias where the rank-
ing models tend to strongly favor older URLs
over new URLs even when the latter otherwise
appear to be more relevant.
By inspecting the clickthrough logs, we observed
that unseen query-url pairs are often composed of
informative previously observed subsequences.
Specifically, we saw that query n-grams can be
correlated with sequences of URL n-grams. For
example, we find that there are interesting regu-
larities across queries and URLs, such as queries
containing “form” tending to lead to clicks on
URLs containing “pdf”. This strongly motivates
the adoption of an approach similar to the Natu-
ral Language Processing (NLP) technique of us-
ing n-grams to deal with unseen words. For ex-
ample, part-of-speech tagging (Brants, 2000) and
parsing (Klein and Manning, 2003) both require
dealing with unknown words. By using n-gram
substrings, novel items can be dealt with using
any informative substrings they contain that were
actually observed in the training data.
</bodyText>
<page confidence="0.971457">
524
</page>
<note confidence="0.996852">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 524–533,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.964947">
The remainder of the paper is organized as fol-
lows. In Section 2, we introduce our overall me-
thodology. Section 2.1 presents a data mining
method for building a query-url n-gram diction-
ary, Section 2.2 describes the new ranking fea-
tures in detail. In section 3, we present our ex-
perimental results. Section 4 discusses related
work, and Section 5 summarizes the contribution
of this work.
</bodyText>
<sectionHeader confidence="0.997351" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.997116444444444">
This section describes the detailed methodology
used in generating the query-url n-gram features.
Our features require association scores to be pre-
viously calculated, and, hence, we first introduce
a data mining approach that is used to build an
association dictionary in Section 2.1. Then, we
present the procedure used to generate the query-
url n-gram features that use the dictionary in Sec-
tion 2.2.
</bodyText>
<figureCaption confidence="0.747663">
Figure 1: Steps to build a query-url n-gram dic-
</figureCaption>
<sectionHeader confidence="0.508346" genericHeader="method">
tionary
</sectionHeader>
<subsectionHeader confidence="0.898057">
2.1 Data Mining on a Query-URL
N-gram Dictionary
</subsectionHeader>
<bodyText confidence="0.999976083333333">
The steps involved in building the dictionary are
shown in Figure 1. We first collect seed query-
url pairs from clickthrough data based on Nav-
Clicks. The queries and URLs from the collected
pairs are tokenized and converted into a collec-
tion of paired query-url n-grams. For each pair,
we calculate the mutual information of the query
n-gram and its corresponding URL n-gram. For
our experiment, we collect a total of more than
15M seed pairs and 0.5B query-url n-gram pairs
using six months of query log data. The details
are described in the following sections.
</bodyText>
<subsectionHeader confidence="0.927532">
2.1.1 Seed List
</subsectionHeader>
<bodyText confidence="0.984881125">
We identify the seed list based on characteristic
user click behavior. Given a query, we select the
URL with the most NavClicks as compared to
other URLs returned. During data collection, the
rank positions of the top 5 URLs were shuffled to
avoid the position bias. We aggregate NavClicks
for a URL occurring in these positions in order to
both obtain more click data and to avoid the posi-
tion bias issue discussed in Dupret and Piwowar-
ski (2008) and Craswell et al. (2008).
For example, in Figure 1, the numbers of Nav-
Clicks for the top three URLs are shown. The
URL www.irs.gov/pub/irs-pdf/f1040.pdf receives
the largest number of NavClicks, and, therefore,
it is used to create the query-url pair:
[irs 1040 form, www.irs.gov/pub/irs-pdf/f1040.pdf]
</bodyText>
<subsectionHeader confidence="0.721338">
2.1.2 Query and URL Segmentation
</subsectionHeader>
<bodyText confidence="0.999647533333334">
We segment the seed pairs to n-gram pairs in
order to increase the coverage beyond that of
ExactM click features. Within NLP, n-grams are
typically extracted such that words that are adja-
cent in the original sequence are also adjacent in
the extracted n-grams. Furthermore, we attempt
to achieve additional generalization by using skip
n-grams (Lin and Och, 2004). This means we not
only extract n-grams for adjacent terms but also
for sequences that leave out intermediate terms.
This is motivated by the observation that the se-
mantics of user queries is often preserved even
when some intermediate terms are removed. The
details of the segmentation methods are de-
scribed below.
</bodyText>
<subsectionHeader confidence="0.712662">
2.1.2.1 Query Segmentation
</subsectionHeader>
<bodyText confidence="0.812112666666667">
Prior to query segmentation, we normalize raw
queries by replacing punctuations with spaces.
Queries are then segmented into a sequence of
</bodyText>
<page confidence="0.997207">
525
</page>
<bodyText confidence="0.996607">
space delimited tokens. From these, we extract
all possible query n-grams and skip n-grams for
n smaller than or equal to three (i.e., all unigrams,
bigrams, and trigrams). For example, given the
sequence “irs 1040 form” the adjacent bigrams
would be “irs 1040” and “1040 form”. With skip
n-grams we also extract “irs form” as shown in
</bodyText>
<tableCaption confidence="0.7299628">
Table 1. We do not use n-grams longer than 3 in
order to avoid problems with overfitting. We will
refer to this segmentation method as Affix Seg-
mentation.
Table 1: An Example of Affix Segmentation
</tableCaption>
<table confidence="0.998601">
N-gram Affix Segmentation
Unigram irs, 1040, form
Bigram irs 1040, 1040 form, irs form
Trigram irs 1040 form
</table>
<sectionHeader confidence="0.492136" genericHeader="method">
2.1.2.2 URL Segmentation
</sectionHeader>
<bodyText confidence="0.9971626">
As shown in Table 2, after the queries are seg-
mented, URLs are categorized into four groups:
domain, URL language, URL region and URL
path. In general, a URL is delimited by punctua-
tion characters such as “?”, “.”,” “/”, and “=”.
</bodyText>
<tableCaption confidence="0.995849">
Table 2: An Example of URL Segmentation
</tableCaption>
<table confidence="0.9762558">
URL Groups Example
Domain irs.gov
URL language en
URL region us
URL path pub, irs, pdf, f1040, pdf
</table>
<bodyText confidence="0.99958225">
The domain group includes one domain token,
for example, irs.gov. Although domains could be
divided into multiple n-grams, we treat them as a
single unit, with the exception of encoded lan-
guage and region information.
The language and region groups are based on the
language or region part of the URL n-grams such
as the suffixes “.en” and “.de”. The language and
region of a URL n-gram are identified by a table
look-up method. The table is created based on
the information available at en.wikipedia.org/
wiki/List of ISO 639-1 codes and en.wikipedia.
org/wiki/ISO 3166. When there is no clear lan-
guage or region URL n-gram, we use English (en)
as the default language and United States (us) as
the default region.
</bodyText>
<subsectionHeader confidence="0.99722">
2.1.3 Calculation of Mutual Information
</subsectionHeader>
<bodyText confidence="0.998515666666667">
After query and URL n-grams are extracted, we
calculate mutual information (Gale and Church,
1991) to determine the degree of association be-
tween the n-grams. The definition of query-url n-
gram mutual information (MI) is given in Equa-
tion 1.
</bodyText>
<equation confidence="0.996041666666667">
_ Freq(q,u) 1
q, u)
log Freq(q) Freq(u) ( )
</equation>
<bodyText confidence="0.999914111111111">
Here q corresponds to a query n-gram and u cor-
responds to a URL n-gram. Freq (q) is the count
of q in the seed list normalized by the total num-
ber of q. Freq (u) is the count of u normalized by
the total number of u. Freq (q, u) is the count of
q and u that co-occurred in a full query-url pair
normalized by the total number of q and u. A pair
will be assigned to a MI score of zero if the items
occur together no more than expected by chance,
under the assumption that the two items are sta-
tistically independent. When a pair occurs more
than is expected by chance, the MI score is posi-
tive. On the other hand, if a pair occurs together
less than is expected by chance, the mutual in-
formation score is negative. In order to increase
the confidence of the MI scores, we remove all
n-grams with less than 3 occurrences in the seed
list, and assign a zero MI score for any pairs in-
volving these n-grams. No smoothing is applied.
This scoring scheme fits well with the associa-
tion properties we would like to have for our
query-url n-gram click features. If a query n-
gram cues for a certain URL through one of its n-
grams, the feature will take on a positive value.
Similarly, if a query n-gram cues against a cer-
tain URL, the feature will take on a negative val-
ue.
</bodyText>
<subsectionHeader confidence="0.996783">
2.1.4 Analysis of Query-URL N-gram
Association
</subsectionHeader>
<bodyText confidence="0.999757333333333">
By examining our dictionary, we observed a
number of pairs that are interesting from a rele-
vance ranking perspective. To illustrate, we pre-
sent four examples of n-gram pairs and intui-
tively explore the nature of the n-gram associa-
tions in the dictionary.
</bodyText>
<tableCaption confidence="0.995686">
Table 3: Examples of MI Scores
</tableCaption>
<table confidence="0.764228">
Query n-gram URL n-gram MI score
“iphone” apple.com 8.7713
“iphone” amazon.com -0.1555
“iphone plan” att.com 11.5388
“iphone plan” apple.com 8.9676
</table>
<bodyText confidence="0.834500666666667">
First, let’s examine the association between
query n-grams and URL n-grams for the queries
MI(
</bodyText>
<page confidence="0.990846">
526
</page>
<bodyText confidence="0.999980769230769">
“iphone” and “iphone plan”. Notice that the
query unigram “iphone” is strongly associated
with apple.com, but negatively associated with
amazon.com. This can be explained by the fact
that “iphone” as a product is not only developed
by Apple but also strongly associated with the
Apple brand. In contrast, while Amazon.com
sells iphones, it also sells a large variety of other
products, thus is not regarded as a very authorita-
tive source of information about the “iphone”.
However, by adding additional context, the most
preferred URL according to MI can change. The
two examples in the bottom of Table 3 illustrate
the URL preferences for the query bigram
“iphone plan”. While apple.com is still a strongly
preferred destination, there is a much stronger
preference for att.com. This preference follows
since apple.com has more product information on
the “iphone” while the information provided by
att.com will be more targeted at visitors who
want to explore what rate plans are available.
Second, Table 4 shows the association between
“kimo”, “.tw” and “.us”. “Kimo” was a Taiwan-
ese start-up acquired by Yahoo!. The mutual in-
formation scores accurately reflect the associa-
tion between the query n-gram and region ids.
</bodyText>
<tableCaption confidence="0.975186">
Table 4: Example of MI Scores
</tableCaption>
<figureCaption confidence="0.411075">
Query n-gram URL n-gram MI score
“kimo” tw (taiwan) 12.8303
“kimo” us (united states) 0.7209
</figureCaption>
<bodyText confidence="0.99957875">
Third, Table 5 shows the association between
“kanji”, and URLs with Language identification
of “Japanese”, “Chinese” and “English”. “Kanji”
means “Chinese” in Japanese. Since queries con-
taining “Kanji” are typically from users inter-
ested in Japanese sites, the mutual information
shows higher correlation with Japanese than with
English or Chinese.
</bodyText>
<tableCaption confidence="0.991053">
Table 5: Example of MI Scores
</tableCaption>
<table confidence="0.899555">
Query n-gram URL n-gram MI score
“kanji” ja (japanese) 11.3862
“kanji” zh (chinese) 6.2567
“kanji” en (english) 4.2110
</table>
<tableCaption confidence="0.987765">
Table 6: Example of MI Score
</tableCaption>
<bodyText confidence="0.994238638888889">
Query n-gram URL n-gram MI score
“form” pdf 4.9067
“form” htm 1.0916
“video” watch 5.7192
“video” htm -1.9079
Fourth, Table 6 shows the association between
two query n-grams, “form” and “video”, that at
first glance may not actually look very informa-
tive for URL path selection. However, notice that
the unigram “form” has a strong preference for
pdf documents over more standard web pages
with an html extension. Similarly, queries that
include “video” convey a preference for URLs
containing “watch”, a characteristic URL n-gram
for many video sharing websites.
It is reasonable to anticipate that incorporating
such associations into a search engine’s ranking
function should help improve both search quality
and user experience. Take the example where,
there are two high ranking competing URLs for
the query “irs 1040 form”. Let’s also assume
both documents contain the same query relevant
keywords, but one is an introduction of the “irs
1040 form” as an htm webpage and the other one
is the real filing form given as a pdf document.
Since in our dictionary, “form” is more associ-
ated with pdf than htm, we predict that most us-
ers would prefer the real pdf form directly, so it
should be placed first in the list of query results.
While click data for the exact query-url pairs
confirms this preference, it is reassuring that we
could identify it without needing to rely on see-
ing the specific query string before. As described
in detail below, and motivated by this analysis,
we designed our query-url click features based
on the contents of the n-gram MI dictionary.
</bodyText>
<subsectionHeader confidence="0.983106">
2.2 Query-URL N-gram Features
</subsectionHeader>
<bodyText confidence="0.999348857142857">
For our feature set, we explored the use of differ-
ent query segmentation approaches (concept and
affix segmentation) in order to increase the di-
versity of n-grams. In the following section, we
use an unseen query “irs 1040 forms” and con-
trast it with the known query “irs 1040 form”
from the last section.
</bodyText>
<subsectionHeader confidence="0.618936">
2.2.1 Concept Segmentation Features
</subsectionHeader>
<bodyText confidence="0.999978818181818">
Query concept segmentation is a weighted query
segmentation approach. Each query is analyti-
cally interpreted as being a main concept and a
sub concept. We search for the unique segmenta-
tion of the query that maximizes its cumulative
mutual information score with the URL n-grams.
Main concepts and sub concepts are n-grams
from the query that have the strongest association
with URL n-grams and thus assist in identifying
relevant landing URL n-grams when the whole
query or the whole URL has not been seen.
</bodyText>
<page confidence="0.985512">
527
</page>
<table confidence="0.573524909090909">
Algorithm 1: Concept Segmentation
for U = domain, URL language, URL region,
URL path do
for j = 0... n-1 do
M ⇐ W0...j
S ⇐ Wj+1...n
for k = 0... m do
curr_mi_M ⇐ arg maxk=1...m MI (M, Uk)
curr_mi_S ⇐ arg maxk=1...m MI (S, Uk)
if curr_mi_M + curr_mi_S &gt; curr_best
then
</table>
<equation confidence="0.808970333333333">
curr_best = curr_mi_M + curr_mi_S
mi_M ⇐ curr_mi_M
mi_S ⇐ curr_mi_S
</equation>
<tableCaption confidence="0.575287833333333">
end if
end for
adding mi_M as a feature
adding mi_S as a feature
end for
end for
</tableCaption>
<bodyText confidence="0.999068344827586">
Pseudo-code for generating query-url n-gram
features based on the concept segmentation is
given in Algorithm 1. Each query (Q) is com-
posed of a number of words, w1, w2, w3...,wn.
Each URL is segmented and categorized to four
groups: domain, URL language, URL region and
URL path. Each URL group has m number of
URL n-grams. M is the main concept of Q and S
is the sub concept of Q.
One potential drawback of such concept segmen-
tation is data sparsity. When we look for the
maximum of cumulative mutual information, we
may obtain main concepts with very high mutual
information and sub concepts which do not exist
in the dictionary. In order to address this problem,
we implement a second query segmentation me-
thod, affix segmentation, that is discussed in sec-
tion 2.2.2.
Table 7 shows eight concept segmented features.
“Coverage” is the percentage of query-url pairs
that have valid feature values. Some of the sam-
ples do not have values because no clicks for the
pairs were seen in the sample of data used to
build the dictionary. When a pair does not have a
value, the default value of zero is assigned. This
default value is based on the assumption that
unless we have evidence otherwise, we assume
all query-url n-grams are statistically independ-
ent and thus provide no preference signal.
</bodyText>
<tableCaption confidence="0.9052315">
Table 7: Eight Features Generated based on
Concept Segmentation.
</tableCaption>
<table confidence="0.995421727272727">
Feature Query N- URL N- Coverage
gram gram (%)
MainDS M domain 54.09
SubDS S domain 30.46
MainLang M lang. 94.41
SubLang S lang. 72.40
MainReg M reg. 90.34
SubReg S reg. 68.19
MainPath M path 64.96
SubPath S path 58.76
Query-URL Domain Features are defined as
</table>
<bodyText confidence="0.9746986">
the mutual information of a query n-gram and the
domain level URL. There are two features in this
category, one for the query main concept and one
for the sub concept. They help to identify the
user preferred host given a query.
</bodyText>
<tableCaption confidence="0.8429625">
Table 8: Example of Selecting Query Segmenta-
tion
</tableCaption>
<table confidence="0.991930166666667">
MI(q,u) irs.gov
“irs” 11.6175 11.2174
“1040” 11.5550
“forms” 7.5049
Cumulative MI 19.1224 22.7724
Seg. 1 Seg.2
</table>
<bodyText confidence="0.9995904">
To illustrate the concept segmentation features,
let’s examine the query, “irs 1040 forms” in the
context of the domain irs.gov. The query “irs
1040 forms” can be segmented either as “irs
1040” and “forms” or as “irs” and “1040 forms”.
As shown in Table 8, taking the cumulative max-
imum, the second segmentation scores higher
than the first one. Therefore, the “irs” and “1040
forms” segmentation is preferred. The feature
value for the main concept is 11.5550, and the
sub concept is then assigned to be 11.2174.
Query-URL Language and Region Features
are the mutual information of a query n-gram and
URL language/region. They are used for provid-
ing language and region information.
</bodyText>
<subsectionHeader confidence="0.658902">
Query-URL Path Features are the mutual
</subsectionHeader>
<bodyText confidence="0.9997502">
information of a query n-gram and a URL path n-
gram. While there are typically many URL path
n-grams, only one URL path n-gram is selected
to be paired with each query n-gram. The se-
lected n-gram is the one that achieves the highest
</bodyText>
<page confidence="0.995232">
528
</page>
<bodyText confidence="0.999471333333333">
cumulative maximum MI score. They are used
for providing association between query n-grams
and url n-grams such as “forms” and “pdf”.
</bodyText>
<subsectionHeader confidence="0.706939">
2.2.2 Affix Segmentation Features
</subsectionHeader>
<bodyText confidence="0.9999263125">
As previously mentioned, affix segmentation
addresses sparsity issues associated with concept
segmentation. Here, we introduce the features
generated based on affix segmentation. Pseudo-
code for generating the features is given in Algo-
rithm 2. Two query unigrams (w0 and wn) and
one bigram (w0wn) is used. Each URL is seg-
mented and categorized to four groups: domain,
URL language, URL region and URL path. Each
URL group has m number of URL n-grams.
This approach is complementary to the concept
segmentation for long queries. The affix n-grams
are in smaller unit, and therefore, are less sparse.
In addition, the skip bigrams allow for generali-
zations using non-adjacent terms. Table 9 shows
the coverage of the twelve affix features.
</bodyText>
<figure confidence="0.68494625">
Algorithm 2: Affix Segmentation
for U = domain, URL language, URL region,
URL path do
for q = w0, wn, w0wn do
for k = 0... m do
curr_mi_q ⇐ arg maxk=1...m MI (q, Uk)
if curr_mi_q &gt; curr_best then
curr_best = curr_mi_q
</figure>
<tableCaption confidence="0.921377857142857">
end if
end for
adding curr_mi_q as a feature
end for
end for
Table 9: Twelve Features Generated based on
Affix Segmentation
</tableCaption>
<table confidence="0.9980355">
Feature Query N- URL N- Coverage
gram gram (%)
PreDS w0 domain 48.09
SufDS wn domain 47.72
PresufDS w0wn domain 23.57
PreLang w0 lang. 55.58
SufLang wn lang. 58.22
PresufLang w0wn lang. 24.91
PreReg w0 reg. 93.82
SufReg wn reg. 93.59
PresufReg w0wn reg. 69.29
PrePath w0 path 98.15
SufPath wn path 97.80
PresufPath w0wn path 75.81
</table>
<bodyText confidence="0.999575814814815">
Query-url domain affix features has three fea-
tures: MI(w0, domain), MI(wn, domain), and
MI(w0wn, domain). In the example of “irs 1040
forms” and “irs.gov”, the features are MI(irs,
irs.gov), MI(forms, irs.gov), and MI(irs forms,
irs.gov).
Query-url language and region affix features
has three features respectively: MI(w0, language),
MI(wn, language), MI(w0wn, language) MI(w0,
region), MI(wn, region), and MI(w0wn, region).
In the example of “irs 1040 forms”, “en” and
“us”, the features are MI (irs, en), MI (forms, en),
MI (irs forms, en), MI (irs, us), MI (forms, us),
and MI (irs forms,us).
Query-url path affix features has three fea-
tures: MI(w0, path), MI(wn, path), and MI(w0wn,
path). In the example of “irs 1040 forms” and
“www.irs.gov/pub/irs-pdf/f1040.pdf”, there are
four URL path n-grams, “pub”, “irs”, “pdf”, and
“f1040”. The URL path n-gram, irs, gets maxi-
mum MI score. Therefore, the query-url path af-
fix features are MI (irs, irs), MI (forms, irs), and
MI (irs forms, irs).
We demonstrated the procedure to generate 20
query-url n-gram features, and in Section 3, we
will present their effectiveness in relevance rank-
ing.
</bodyText>
<sectionHeader confidence="0.99911" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.99997175">
We evaluate the performance of query-url n-
grams features (8 concept and 12 affix features)
on a ranking application and analyze the results
from several different perspectives.
</bodyText>
<subsectionHeader confidence="0.989111">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999987538461539">
For all experiments, our training and test data are
query-url pairs annotated with human judgments.
In our data, we use five grades to evaluate rele-
vance of a query and URL pair.
The data includes 94K queries for training and
3.4K queries for evaluation, and each query is
associated with the top ranked URLs returned
from a search engine. Totally, there are 916K
query-url pairs for training and 42K pairs for
testing. The queries are general and uniformly
and randomly sampled with replacement, result-
ing in more frequent queries also appearing more
frequently in our training and test sets.
</bodyText>
<page confidence="0.995871">
529
</page>
<subsectionHeader confidence="0.999058">
3.2 Ranking Algorithm
</subsectionHeader>
<bodyText confidence="0.999392142857143">
GBRank is a supervised learning algorithm that
uses boosted decision trees and incorporates the
pair-wise information from the training data
(Zheng et al, 2007). It is able to deal with a large
amount of training data with hundreds of features.
We use an internal C++ implementation of
GBRank.
</bodyText>
<subsectionHeader confidence="0.997983">
3.3 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.9999628">
We use Discounted Cumulative Gain (Järvelin
and Kekäläinen, 2002) to evaluate our ranking
accuracy. Discounted Cumulative Gain (DCG)
has been widely used in evaluating the quality of
search engine rankings and is defined as:
</bodyText>
<equation confidence="0.985230666666667">
_I(2)k Gi
log2 (1)
i
</equation>
<bodyText confidence="0.999964454545455">
Gi represents the editorial judgment of the i-th
document. In this paper, we only report normal-
ized DCG5, which is an absolute DCG5 normal-
ized by a baseline, and relative DCG5 im-
provement, which is an improvement normal-
ized by the baseline. Note normalized DCG5 is
different than NDCG (Normalized Discounted
Cumulative Gain defined in Järvelin and
Kekäläinen, 2002). We use Wilcoxon signed test
(Wilcoxon, 1945) to evaluate the significance for
model comparison.
</bodyText>
<subsectionHeader confidence="0.978556">
3.4 Feature Sets
</subsectionHeader>
<bodyText confidence="0.61827">
Five feature sets are used in our experiments.
Details are listed in Table 10.
</bodyText>
<tableCaption confidence="0.988327">
Table 10: Five Feature Sets
</tableCaption>
<table confidence="0.99580125">
Tag Description
Base Feature Core Feature Set and ExactM
Set click features
Q-U N-gram Base Feature Set and Q-U N-
Feature Set (I) gram features
Core Feature query-based, document-based,
Set query-document based fea-
tures
NavClick Fea- Core Feature Set and Nav-
ture Set Click
Q-U N-gram Core Feature Set and Q-U N-
Feature Set (II) gram features
</table>
<bodyText confidence="0.998188285714286">
Base Feature Set is a strong baseline feature set
from a state-of-the-art commercial search engine.
This set includes NavClick features, and other
internal ExactM click features. It is used for
evaluating Query-URL N-gram Feature Set (I) in
order to know whether query-url n-gram features
can achieve gains when stacked on top of Ex-
actM features.
Core Feature Set is a weaker variant of the
baseline system that excludes ExactM click fea-
tures. This system is used for evaluating
NavClick Feature Set and Query-URL N-gram
Feature Set (II) independently in order to study
and contrast the effected queries.
</bodyText>
<subsectionHeader confidence="0.982873">
3.5 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9997798">
We compare the query-URL N-gram feature set
(I) with the base feature set in Section 3.5.1, and
contrast the NavClick features and the query-
URL N-gram features (II) using the Core Feature
Set in Section 3.5.2.
</bodyText>
<subsectionHeader confidence="0.94056">
3.5.1 Query-URL N-gram Feature Set (I)
versus Base Feature Set
</subsectionHeader>
<bodyText confidence="0.999804266666667">
As shown in Figure 2, Query-URL N-gram Fea-
ture Set (I) outperforms Base Feature Set. The
additional 20 query-url n-gram features achieve
statistically significant gains at a p-value &lt;
0.0001 level, suggesting that they are compli-
mentary to ExactM click features. Even though
the query-url n-gram features are generated from
the same data as the ExactM features, the gain is
additive and stackable. The DCG5 impact is
0.53% relative improvement when running
GBRank using 2500 trees. Every data point is
normalized by the DCG5 of the baseline feature
set using 2500 trees. This is represented in the
graph as the rightmost point of Base Feature Set
curve.
</bodyText>
<figureCaption confidence="0.998619666666667">
Figure 2: Comparison of the five feature sets on
the normalized DCG5 (Y-axis) against number of
trees (X-axis).
</figureCaption>
<figure confidence="0.998570230769231">
500 1000 1500 2000
Q-U N-gram Features (I)
Base Features
NavClick
Q-U N-gram Features (II)
Core Features
0.99
0.98
0.97
0.96
1.01
1
DCGk
</figure>
<page confidence="0.975681">
530
</page>
<bodyText confidence="0.957214823529412">
3.5.2 NavClick and Query-URL N-gram
Feature Set (II) versus Core Fea-
ture Set
We compare NavClick Feature Set and Query-
URL N-gram Feature Set (II) in the context of
Core Feature Set, in order to evaluate the two
independently. As shown in Figure 2, both
NavClick and Query-URL N-gram Feature Set
(II) outperform Core Feature Set. It is not sur-
prising that NavClick also outperforms Query-
URL N-gram Feature Set (II) since the n-gram
features are backoff of NavClick. However, their
gains are competitive suggesting the query-url n-
gram features are very good relevance indicators.
The impact of NavClick and Query-URL N-gram
Feature Set (II) is 0.72% and 0.62% relative
DCG5 improvement at Tree 2500 respectively.
</bodyText>
<subsectionHeader confidence="0.824232">
3.5.3 Feature Importance
</subsectionHeader>
<bodyText confidence="0.99961125">
Using the GBRank model, features are evaluated
and sequentially selected to build the boosted
decision trees. The split of each node increases
the DCG during training. We evaluate a feature’s
importance by aggregating the DCG impact of
the feature over all trees (Zheng et al., 2007).
Here, the feature importance is rescaled so that
the feature with largest DCG impact is assigned a
normalized score of 1. Figure 3 illustrates the
relative influence of each of query-url n-gram
feature. Of these, n-gram features associated with
a domain name (i.e., MainDS) rank highest.
</bodyText>
<figureCaption confidence="0.99512925">
Figure 3: Feature importance of query-url n-
gram features. The importance (Y axis) is nor-
malized so that the most important feature
(MainDS)’s importance is 1.
</figureCaption>
<subsectionHeader confidence="0.986986">
3.6 Analysis
</subsectionHeader>
<bodyText confidence="0.9997868">
We access system performance with respect to
both query length and frequency using the two
click features sets in combination with the Core
Feature Set in order to gain insight into the ef-
fected queries.
</bodyText>
<subsectionHeader confidence="0.903852">
3.6.1 Query Length
</subsectionHeader>
<bodyText confidence="0.999697529411765">
As shown in Table 11, NavClick (NavClick Fea-
ture Set) best improves relevance for two word
queries. In contrast, Query-url n-gram features in
isolation (Query-URL N-gram Feature II) are
able to show sizable improvements on longer
queries, while slightly degrading performance on
short 1-word queries. Using both feature sets to-
gether (Query-URL N-gram Feature I) results in
improvement for queries of all lengths.
These results suggest that the strong signal being
provided by NavClick for short queries helps to
compensate for any additional noisy introduced
by the n-gram features, while allowing the n-
gram features to handle longer queries that are
less well covered by NavClick. These longer
queries are exactly the type of queries our query-
url n-gram features were designed to help with.
</bodyText>
<tableCaption confidence="0.960656">
Table 11: Relative DCG5 Improvement of
</tableCaption>
<table confidence="0.9787761">
NavClick, Query-URL N-gram (II), and Query-
URL N-gram Features (I) vs Core Feature Set
Length NavClick QU N- QU N-
vs Core gram (II) gram (I)
(%) vs Core vs Core
(%) (%)
1 word 0.03 -0.04 0.62
2 words 1.04 1.06 1.58
3 words 1.00 1.44 2.12
4+ words 0.4 0.68 1.01
</table>
<subsectionHeader confidence="0.819073">
3.6.2 Query Frequency
</subsectionHeader>
<bodyText confidence="0.99863025">
We found that query-url n-gram features improve
tail queries. Head queries are considered as top
two million frequent queries in our traffic and
tail queries include anything outside of that range.
</bodyText>
<tableCaption confidence="0.970088">
Table 12: Relative DCG5 Improvement of
</tableCaption>
<table confidence="0.965022727272727">
NavClick, Query-URL N-gram Features (II) and
Query-URL N-gram Features (I) vs Core Feature
Set
NavClick vs QU N-gram QU N-
Core (%) (II) vs Core gram (I) vs
(%) Core (%)
Head 0.91 -0.15 1.11
Tail 0.59 1.11 1.40
As shown in Table 12, query-url n-gram features
(Query-URL Feature Set II) differ from
NavClick (NavClick Feature Set) in that they get
</table>
<page confidence="0.996569">
531
</page>
<bodyText confidence="0.946008333333333">
more gain from tail queries. Together, they
(Query-URL Feature Set I) improve both head
and tail queries.
</bodyText>
<subsectionHeader confidence="0.99779">
3.7 Case Study
</subsectionHeader>
<bodyText confidence="0.999987">
Below we examine queries from the test set and
analyze the effects of Query-URL N-gram Fea-
ture Set (II) versus Core Feature Set.
</bodyText>
<subsectionHeader confidence="0.700336">
3.7.1 Positive Cases
</subsectionHeader>
<bodyText confidence="0.993906111111111">
1) Animal shelter in va: this query targets a spe-
cific geographic location. Using the baseline fea-
ture set, the root url wvanimalshelter.org is incor-
rectly ranked higher than www.netpets.com/
cats/catresc/virginia.htm. Without any addition-
ally ranking information, general URLs (root)
tend to be ranked more highly than more specific
URLs (path), as the root pages tend to be more
popular. However, our new features express a
preference between “va” and “virginia”, and this
correctly flips the ranking order.
2) Myspace profile generator: www. myspacgens.
com/handler.php?gen=profile was incorrectly
ranked higher than www.profilemods.com/
myspace-generators. Our new features convey a
high user preference association between “profile
generator” and the domain profilemods.com,
which helps to correctly swap the order.
</bodyText>
<subsectionHeader confidence="0.554909">
3.7.2 Negative Cases
</subsectionHeader>
<bodyText confidence="0.999959833333333">
We determined that negative cases where the
baseline feature set outperforms the new features
are typically one word navigational queries such
as “craigslist”. However, after we combine the
query-url n-gram features with NavClick, one
word navigational queries are ranked correctly.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999978421052631">
Our work is mainly related to Gao et al. (2009)
and Bilenko and White (2008). Gao et al. (2009)
addressed the sparsity issue by propagating click
information among similar queries in the same
cluster. Their idea is based on an observation that
similar queries go to similar pages. When two
queries have similar clicked URLs, it is likely
that they share clicked URLs. In contrast, our
idea is to utilize NLP techniques to break down
long, infrequent queries into shorter, frequent
queries. The two approaches can be mutually
beneficial. Bilenko and White (2008) expanded
click data with a search engine by using post-
search user experience collected from toolbars.
Toolbars keep track of users’ click behavior both
when they are using the search engine directly
and beyond. Their relevance features are built
based on whole session clicks extracted from the
toolbar. In contrast, our n-gram features are built
on search engine clicks directly. We should be
able to expand our method to integrate the post-
search clicks with toolbar data.
Other related work can be found in the domain of
query rewriting. Our n-gram dictionary was orig-
inally designed for query rewriting. Query re-
writing (Xu and Croft, 1996; Salton and Voor-
hees, 1984) reformulates a query to its synonyms
or related terms automatically. However, the
coverage of query rewriting is normally small,
because an inappropriate rewrite can cause sig-
nificant decrease in precision. In contrast, our
approach can cover a larger number of queries
without decreasing precision, because it does not
need to make a binary decision whether a query
should be reformulated. The association scores
between queries and rewrites are used as ranking
features which are trained discriminatively to-
ward search quality.
</bodyText>
<sectionHeader confidence="0.999112" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999888769230769">
In this paper, we presented a set of straightfor-
ward yet informative query-url n-gram features.
They allow for generalization of limited user
click data to large amounts of unseen query-url
pairs. Our experiments showed such features
gave significant improvement over models with-
out using the features. In addition, we mined an
interesting dictionary which contains informa-
tive, but not necessarily obvious, query-url syno-
nym pairs such as “form” and “pdf”. We are cur-
rently extending our work to a variety of exact
match features and different sources of click-
through logs.
</bodyText>
<sectionHeader confidence="0.973587" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9992786">
Thanks to the anonymous reviewers for detailed
suggestion and our colleagues: Jon Degenhardt
and Narayanan Sadagopan for assistance on gen-
erating clickthrough data, Jiang Chen for devel-
oping the decision tree package, Xiangyu Jin for
a discussion on map/reduce, Benoît Dumoulin,
Fuchun Peng, Yumao Lu, and Xing Wei for pro-
ductizing the work, and Rosie Jones, Su-lin Wu,
Bo Long, Xin Li and Ruiqiang Zhang for com-
ments on an earlier draft.
</bodyText>
<page confidence="0.995238">
532
</page>
<sectionHeader confidence="0.995877" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999310868852459">
Agichtein, E., E. Brill, and S. Dumais. 2006. Im-
proving web search ranking by incorporating
user behavior information. In Proceedings of
the ACM SIGIR 29.
Agichtein, Eugene, Zijian Zheng. 2006. Identify-
ing &amp;quot;best bet&amp;quot; web search results by mining
past user behavior. In Proceedings of KDD.
Bilenko, Mikhail and Ryen W. White. 2008.
Mining the search trails of surfing crowds:
identifying relevant websites from user activ-
ity. In Proceedings of WWW.
Brants, T. 2000. Tnt: a statistical part-ofspeech
tagger. In Proceedings of ANLP 6.
Craswell, Nick and Martin Szummer. 2007. Ran-
dom walks on the click graph. In Proceedings
of SIGIR.
Craswell, Nick, Onno Zoeter, Michael Taylor,
Bill Ramsey. 2008. An experimental compari-
son of click position-bias models in WSDM.
Dupret, Georges, Benjamin Piwowarski. 2008. A
user browsing model to predict search engine
click data from past observations. In Proceed-
ings of SIGIR 31.
Gale, William A. and Kenneth W. Church. 1991.
Identifying word correspondence in parallel
texts. In Proceedings of HLT 91.
Gao, Jianfeng, Wei Yuan, Xiao Li, Kefeng Deng,
and Jian-Yun Nie. 2009. Smoothing Click-
through Data for Web Search Ranking. In Pro-
ceedings of SIGIR 32.
Järvelin, K. and J. Kekäläinen. 2002. Cumulated
gain-based evaluation of IR techniques, Jour-
nal ACM Transactions on Information Sys-
tems, 20: 422-446.
Klein, D. and C. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of ACL 41.
Lin, Chin-Yew and Franz Josef Och. 2004. Au-
tomatic evaluation of machine translation
quality using longest common subsequence
and skip-bigram. In In Proceedings of ACL 42.
Lu, Yumao, Fuchun Peng, Xin Li and Nawaaz
Ahmed, 2006, Coupling Feature Selection and
Machine Learning Methods for Navigational
Query Identification, In Proceeding of CIKM.
Radlinski, F., Kurup, M. and Joachims, T. 2007.
Active exploration for learning rankings from
clickthrough data. In SIGKDD.
Salton G. and E. Voorhees. 1984. Comparison of
two methods for Boolean query relevancy
feedback. Information Processing &amp; Manage-
ment, 20(5).
Wilcoxon, F. 1945. Individual Comparisons by
Ranking Methods. Biometrics, 1:80–83.
Xu Q. and W. Croft. 1996. Query expansion us-
ing local and global document analysis. In
Proceed of the 19th annual international ACM
SIGIR.
Zheng, Z., H. Zha, K. Chen, and G. Sun. 2007. A
regression framework for learning ranking
functions using relative relevance judgments.
In Proceedings of SIGIR 30.
</reference>
<page confidence="0.998843">
533
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635427">
<title confidence="0.929317333333333">Mining Search Engine Clickthrough Log for Features Huihsin Tseng, Longbin Chen, Fan Li, Ziming</title>
<author confidence="0.813623">Lei Duan</author>
<author confidence="0.813623">Belle Tseng</author>
<address confidence="0.97713">Yahoo! Inc., Santa Clara, CA 95054</address>
<email confidence="0.999634">huihui@yahoo-inc.com</email>
<email confidence="0.999634">longbin@yahoo-inc.com</email>
<email confidence="0.999634">fanli@yahoo-inc.com</email>
<email confidence="0.999634">ziming@yahoo-inc.com</email>
<email confidence="0.999634">leiduan@yahoo-inc.com</email>
<email confidence="0.999634">belle@yahoo-inc.com</email>
<abstract confidence="0.99949647826087">User clicks on a URL in response to a query are extremely useful predictors of the URL’s relevance to that query. Exact match click features tend to suffer from severe data sparsity issues in web ranking. Such sparsity is particularly pronounced for new URLs or long queries where each distinct query-url pair will rarely occur. To remedy this, we present a set of straightforward informative query-url features that allows for generalization of limited user click data to large amounts of unseen query-url pairs. The method is motivated by techniques leveraged in the NLP community for dealing with unseen words. We find that there are interesting regularities across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvethat are statistically significant at a &lt; 0.0001 level over a strong baseline with exact match clickthrough features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>E Brill</author>
<author>S Dumais</author>
</authors>
<title>Improving web search ranking by incorporating user behavior information.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACM SIGIR 29.</booktitle>
<contexts>
<context position="1535" citStr="Agichtein et al., 2006" startWordPosition="234" endWordPosition="237">ies across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value &lt; 0.0001 level over a strong baseline with exact match clickthrough features. 1 Introduction Clickthrough logs record user click behaviors, which are a critical source for improving search relevance (Bilenko and White, 2008; Radlinski et al., 2007; Agichtein and Zheng, 2006; Lu et al. 2006). Previous work (Agichtein et al., 2006) demonstrated that clickthrough features (e.g., IsNextClicked and IsPreviousClicked) can lead to substantial improvements in relevance. Such features summarize query-specific user interactions on a search engine. One commonly used clickthrough feature is generated based on the following observation: if a URL receives a large number of first and last clicks across many user sessions, then it indicates that this URL might be a strongly preferred destination of a query. For example, when a user searches for “yahoo”, they tend to only click on the URL www.yahoo.com rather than other alternatives. </context>
</contexts>
<marker>Agichtein, Brill, Dumais, 2006</marker>
<rawString>Agichtein, E., E. Brill, and S. Dumais. 2006. Improving web search ranking by incorporating user behavior information. In Proceedings of the ACM SIGIR 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Zijian Zheng</author>
</authors>
<title>Identifying &amp;quot;best bet&amp;quot; web search results by mining past user behavior.</title>
<date>2006</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="1478" citStr="Agichtein and Zheng, 2006" startWordPosition="224" endWordPosition="227"> unseen words. We find that there are interesting regularities across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value &lt; 0.0001 level over a strong baseline with exact match clickthrough features. 1 Introduction Clickthrough logs record user click behaviors, which are a critical source for improving search relevance (Bilenko and White, 2008; Radlinski et al., 2007; Agichtein and Zheng, 2006; Lu et al. 2006). Previous work (Agichtein et al., 2006) demonstrated that clickthrough features (e.g., IsNextClicked and IsPreviousClicked) can lead to substantial improvements in relevance. Such features summarize query-specific user interactions on a search engine. One commonly used clickthrough feature is generated based on the following observation: if a URL receives a large number of first and last clicks across many user sessions, then it indicates that this URL might be a strongly preferred destination of a query. For example, when a user searches for “yahoo”, they tend to only click </context>
</contexts>
<marker>Agichtein, Zheng, 2006</marker>
<rawString>Agichtein, Eugene, Zijian Zheng. 2006. Identifying &amp;quot;best bet&amp;quot; web search results by mining past user behavior. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bilenko</author>
<author>Ryen W White</author>
</authors>
<title>Mining the search trails of surfing crowds: identifying relevant websites from user activity.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="1427" citStr="Bilenko and White, 2008" startWordPosition="216" endWordPosition="219">s leveraged in the NLP community for dealing with unseen words. We find that there are interesting regularities across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value &lt; 0.0001 level over a strong baseline with exact match clickthrough features. 1 Introduction Clickthrough logs record user click behaviors, which are a critical source for improving search relevance (Bilenko and White, 2008; Radlinski et al., 2007; Agichtein and Zheng, 2006; Lu et al. 2006). Previous work (Agichtein et al., 2006) demonstrated that clickthrough features (e.g., IsNextClicked and IsPreviousClicked) can lead to substantial improvements in relevance. Such features summarize query-specific user interactions on a search engine. One commonly used clickthrough feature is generated based on the following observation: if a URL receives a large number of first and last clicks across many user sessions, then it indicates that this URL might be a strongly preferred destination of a query. For example, when a </context>
<context position="30456" citStr="Bilenko and White (2008)" startWordPosition="5026" endWordPosition="5029">orrectly ranked higher than www.profilemods.com/ myspace-generators. Our new features convey a high user preference association between “profile generator” and the domain profilemods.com, which helps to correctly swap the order. 3.7.2 Negative Cases We determined that negative cases where the baseline feature set outperforms the new features are typically one word navigational queries such as “craigslist”. However, after we combine the query-url n-gram features with NavClick, one word navigational queries are ranked correctly. 4 Related Work Our work is mainly related to Gao et al. (2009) and Bilenko and White (2008). Gao et al. (2009) addressed the sparsity issue by propagating click information among similar queries in the same cluster. Their idea is based on an observation that similar queries go to similar pages. When two queries have similar clicked URLs, it is likely that they share clicked URLs. In contrast, our idea is to utilize NLP techniques to break down long, infrequent queries into shorter, frequent queries. The two approaches can be mutually beneficial. Bilenko and White (2008) expanded click data with a search engine by using postsearch user experience collected from toolbars. Toolbars kee</context>
</contexts>
<marker>Bilenko, White, 2008</marker>
<rawString>Bilenko, Mikhail and Ryen W. White. 2008. Mining the search trails of surfing crowds: identifying relevant websites from user activity. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>Tnt: a statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP 6.</booktitle>
<contexts>
<context position="3578" citStr="Brants, 2000" startWordPosition="558" endWordPosition="559">. By inspecting the clickthrough logs, we observed that unseen query-url pairs are often composed of informative previously observed subsequences. Specifically, we saw that query n-grams can be correlated with sequences of URL n-grams. For example, we find that there are interesting regularities across queries and URLs, such as queries containing “form” tending to lead to clicks on URLs containing “pdf”. This strongly motivates the adoption of an approach similar to the Natural Language Processing (NLP) technique of using n-grams to deal with unseen words. For example, part-of-speech tagging (Brants, 2000) and parsing (Klein and Manning, 2003) both require dealing with unknown words. By using n-gram substrings, novel items can be dealt with using any informative substrings they contain that were actually observed in the training data. 524 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 524–533, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP The remainder of the paper is organized as follows. In Section 2, we introduce our overall methodology. Section 2.1 presents a data mining method for building a query-url n-gram dictionary, Section 2.2 describe</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Brants, T. 2000. Tnt: a statistical part-ofspeech tagger. In Proceedings of ANLP 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Craswell</author>
<author>Martin Szummer</author>
</authors>
<title>Random walks on the click graph.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<marker>Craswell, Szummer, 2007</marker>
<rawString>Craswell, Nick and Martin Szummer. 2007. Random walks on the click graph. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Craswell</author>
<author>Onno Zoeter</author>
<author>Michael Taylor</author>
<author>Bill Ramsey</author>
</authors>
<title>An experimental comparison of click position-bias models in WSDM.</title>
<date>2008</date>
<contexts>
<context position="5930" citStr="Craswell et al. (2008)" startWordPosition="948" endWordPosition="951">seed pairs and 0.5B query-url n-gram pairs using six months of query log data. The details are described in the following sections. 2.1.1 Seed List We identify the seed list based on characteristic user click behavior. Given a query, we select the URL with the most NavClicks as compared to other URLs returned. During data collection, the rank positions of the top 5 URLs were shuffled to avoid the position bias. We aggregate NavClicks for a URL occurring in these positions in order to both obtain more click data and to avoid the position bias issue discussed in Dupret and Piwowarski (2008) and Craswell et al. (2008). For example, in Figure 1, the numbers of NavClicks for the top three URLs are shown. The URL www.irs.gov/pub/irs-pdf/f1040.pdf receives the largest number of NavClicks, and, therefore, it is used to create the query-url pair: [irs 1040 form, www.irs.gov/pub/irs-pdf/f1040.pdf] 2.1.2 Query and URL Segmentation We segment the seed pairs to n-gram pairs in order to increase the coverage beyond that of ExactM click features. Within NLP, n-grams are typically extracted such that words that are adjacent in the original sequence are also adjacent in the extracted n-grams. Furthermore, we attempt to </context>
</contexts>
<marker>Craswell, Zoeter, Taylor, Ramsey, 2008</marker>
<rawString>Craswell, Nick, Onno Zoeter, Michael Taylor, Bill Ramsey. 2008. An experimental comparison of click position-bias models in WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georges Dupret</author>
<author>Benjamin Piwowarski</author>
</authors>
<title>A user browsing model to predict search engine click data from past observations.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR 31.</booktitle>
<contexts>
<context position="5903" citStr="Dupret and Piwowarski (2008)" startWordPosition="942" endWordPosition="946">collect a total of more than 15M seed pairs and 0.5B query-url n-gram pairs using six months of query log data. The details are described in the following sections. 2.1.1 Seed List We identify the seed list based on characteristic user click behavior. Given a query, we select the URL with the most NavClicks as compared to other URLs returned. During data collection, the rank positions of the top 5 URLs were shuffled to avoid the position bias. We aggregate NavClicks for a URL occurring in these positions in order to both obtain more click data and to avoid the position bias issue discussed in Dupret and Piwowarski (2008) and Craswell et al. (2008). For example, in Figure 1, the numbers of NavClicks for the top three URLs are shown. The URL www.irs.gov/pub/irs-pdf/f1040.pdf receives the largest number of NavClicks, and, therefore, it is used to create the query-url pair: [irs 1040 form, www.irs.gov/pub/irs-pdf/f1040.pdf] 2.1.2 Query and URL Segmentation We segment the seed pairs to n-gram pairs in order to increase the coverage beyond that of ExactM click features. Within NLP, n-grams are typically extracted such that words that are adjacent in the original sequence are also adjacent in the extracted n-grams. </context>
</contexts>
<marker>Dupret, Piwowarski, 2008</marker>
<rawString>Dupret, Georges, Benjamin Piwowarski. 2008. A user browsing model to predict search engine click data from past observations. In Proceedings of SIGIR 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>Identifying word correspondence in parallel texts.</title>
<date>1991</date>
<booktitle>In Proceedings of HLT 91.</booktitle>
<contexts>
<context position="8982" citStr="Gale and Church, 1991" startWordPosition="1450" endWordPosition="1453">nd region groups are based on the language or region part of the URL n-grams such as the suffixes “.en” and “.de”. The language and region of a URL n-gram are identified by a table look-up method. The table is created based on the information available at en.wikipedia.org/ wiki/List of ISO 639-1 codes and en.wikipedia. org/wiki/ISO 3166. When there is no clear language or region URL n-gram, we use English (en) as the default language and United States (us) as the default region. 2.1.3 Calculation of Mutual Information After query and URL n-grams are extracted, we calculate mutual information (Gale and Church, 1991) to determine the degree of association between the n-grams. The definition of query-url ngram mutual information (MI) is given in Equation 1. _ Freq(q,u) 1 q, u) log Freq(q) Freq(u) ( ) Here q corresponds to a query n-gram and u corresponds to a URL n-gram. Freq (q) is the count of q in the seed list normalized by the total number of q. Freq (u) is the count of u normalized by the total number of u. Freq (q, u) is the count of q and u that co-occurred in a full query-url pair normalized by the total number of q and u. A pair will be assigned to a MI score of zero if the items occur together n</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>Gale, William A. and Kenneth W. Church. 1991. Identifying word correspondence in parallel texts. In Proceedings of HLT 91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Wei Yuan</author>
<author>Xiao Li</author>
<author>Kefeng Deng</author>
<author>Jian-Yun Nie</author>
</authors>
<title>Smoothing Clickthrough Data for Web Search Ranking.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGIR 32.</booktitle>
<contexts>
<context position="30427" citStr="Gao et al. (2009)" startWordPosition="5021" endWordPosition="5024">hp?gen=profile was incorrectly ranked higher than www.profilemods.com/ myspace-generators. Our new features convey a high user preference association between “profile generator” and the domain profilemods.com, which helps to correctly swap the order. 3.7.2 Negative Cases We determined that negative cases where the baseline feature set outperforms the new features are typically one word navigational queries such as “craigslist”. However, after we combine the query-url n-gram features with NavClick, one word navigational queries are ranked correctly. 4 Related Work Our work is mainly related to Gao et al. (2009) and Bilenko and White (2008). Gao et al. (2009) addressed the sparsity issue by propagating click information among similar queries in the same cluster. Their idea is based on an observation that similar queries go to similar pages. When two queries have similar clicked URLs, it is likely that they share clicked URLs. In contrast, our idea is to utilize NLP techniques to break down long, infrequent queries into shorter, frequent queries. The two approaches can be mutually beneficial. Bilenko and White (2008) expanded click data with a search engine by using postsearch user experience collecte</context>
</contexts>
<marker>Gao, Yuan, Li, Deng, Nie, 2009</marker>
<rawString>Gao, Jianfeng, Wei Yuan, Xiao Li, Kefeng Deng, and Jian-Yun Nie. 2009. Smoothing Clickthrough Data for Web Search Ranking. In Proceedings of SIGIR 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Järvelin</author>
<author>J Kekäläinen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques,</title>
<date>2002</date>
<journal>Journal ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<pages>422--446</pages>
<contexts>
<context position="22648" citStr="Järvelin and Kekäläinen, 2002" startWordPosition="3763" endWordPosition="3766">rl pairs for training and 42K pairs for testing. The queries are general and uniformly and randomly sampled with replacement, resulting in more frequent queries also appearing more frequently in our training and test sets. 529 3.2 Ranking Algorithm GBRank is a supervised learning algorithm that uses boosted decision trees and incorporates the pair-wise information from the training data (Zheng et al, 2007). It is able to deal with a large amount of training data with hundreds of features. We use an internal C++ implementation of GBRank. 3.3 Evaluation Metric We use Discounted Cumulative Gain (Järvelin and Kekäläinen, 2002) to evaluate our ranking accuracy. Discounted Cumulative Gain (DCG) has been widely used in evaluating the quality of search engine rankings and is defined as: _I(2)k Gi log2 (1) i Gi represents the editorial judgment of the i-th document. In this paper, we only report normalized DCG5, which is an absolute DCG5 normalized by a baseline, and relative DCG5 improvement, which is an improvement normalized by the baseline. Note normalized DCG5 is different than NDCG (Normalized Discounted Cumulative Gain defined in Järvelin and Kekäläinen, 2002). We use Wilcoxon signed test (Wilcoxon, 1945) to eval</context>
</contexts>
<marker>Järvelin, Kekäläinen, 2002</marker>
<rawString>Järvelin, K. and J. Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques, Journal ACM Transactions on Information Systems, 20: 422-446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL 41.</booktitle>
<contexts>
<context position="3616" citStr="Klein and Manning, 2003" startWordPosition="562" endWordPosition="565">rough logs, we observed that unseen query-url pairs are often composed of informative previously observed subsequences. Specifically, we saw that query n-grams can be correlated with sequences of URL n-grams. For example, we find that there are interesting regularities across queries and URLs, such as queries containing “form” tending to lead to clicks on URLs containing “pdf”. This strongly motivates the adoption of an approach similar to the Natural Language Processing (NLP) technique of using n-grams to deal with unseen words. For example, part-of-speech tagging (Brants, 2000) and parsing (Klein and Manning, 2003) both require dealing with unknown words. By using n-gram substrings, novel items can be dealt with using any informative substrings they contain that were actually observed in the training data. 524 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 524–533, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP The remainder of the paper is organized as follows. In Section 2, we introduce our overall methodology. Section 2.1 presents a data mining method for building a query-url n-gram dictionary, Section 2.2 describes the new ranking features in detail. </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. and C. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram. In</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>42</pages>
<contexts>
<context position="6605" citStr="Lin and Och, 2004" startWordPosition="1052" endWordPosition="1055">the top three URLs are shown. The URL www.irs.gov/pub/irs-pdf/f1040.pdf receives the largest number of NavClicks, and, therefore, it is used to create the query-url pair: [irs 1040 form, www.irs.gov/pub/irs-pdf/f1040.pdf] 2.1.2 Query and URL Segmentation We segment the seed pairs to n-gram pairs in order to increase the coverage beyond that of ExactM click features. Within NLP, n-grams are typically extracted such that words that are adjacent in the original sequence are also adjacent in the extracted n-grams. Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). This means we not only extract n-grams for adjacent terms but also for sequences that leave out intermediate terms. This is motivated by the observation that the semantics of user queries is often preserved even when some intermediate terms are removed. The details of the segmentation methods are described below. 2.1.2.1 Query Segmentation Prior to query segmentation, we normalize raw queries by replacing punctuations with spaces. Queries are then segmented into a sequence of 525 space delimited tokens. From these, we extract all possible query n-grams and skip n-grams for n smaller than or </context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Lin, Chin-Yew and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram. In In Proceedings of ACL 42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yumao Lu</author>
<author>Fuchun Peng</author>
<author>Xin Li</author>
<author>Nawaaz Ahmed</author>
</authors>
<title>Coupling Feature Selection and Machine Learning Methods for Navigational Query Identification,</title>
<date>2006</date>
<booktitle>In Proceeding of CIKM.</booktitle>
<contexts>
<context position="1495" citStr="Lu et al. 2006" startWordPosition="228" endWordPosition="231"> there are interesting regularities across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value &lt; 0.0001 level over a strong baseline with exact match clickthrough features. 1 Introduction Clickthrough logs record user click behaviors, which are a critical source for improving search relevance (Bilenko and White, 2008; Radlinski et al., 2007; Agichtein and Zheng, 2006; Lu et al. 2006). Previous work (Agichtein et al., 2006) demonstrated that clickthrough features (e.g., IsNextClicked and IsPreviousClicked) can lead to substantial improvements in relevance. Such features summarize query-specific user interactions on a search engine. One commonly used clickthrough feature is generated based on the following observation: if a URL receives a large number of first and last clicks across many user sessions, then it indicates that this URL might be a strongly preferred destination of a query. For example, when a user searches for “yahoo”, they tend to only click on the URL www.ya</context>
</contexts>
<marker>Lu, Peng, Li, Ahmed, 2006</marker>
<rawString>Lu, Yumao, Fuchun Peng, Xin Li and Nawaaz Ahmed, 2006, Coupling Feature Selection and Machine Learning Methods for Navigational Query Identification, In Proceeding of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Radlinski</author>
<author>M Kurup</author>
<author>T Joachims</author>
</authors>
<title>Active exploration for learning rankings from clickthrough data.</title>
<date>2007</date>
<booktitle>In SIGKDD.</booktitle>
<contexts>
<context position="1451" citStr="Radlinski et al., 2007" startWordPosition="220" endWordPosition="223">mmunity for dealing with unseen words. We find that there are interesting regularities across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value &lt; 0.0001 level over a strong baseline with exact match clickthrough features. 1 Introduction Clickthrough logs record user click behaviors, which are a critical source for improving search relevance (Bilenko and White, 2008; Radlinski et al., 2007; Agichtein and Zheng, 2006; Lu et al. 2006). Previous work (Agichtein et al., 2006) demonstrated that clickthrough features (e.g., IsNextClicked and IsPreviousClicked) can lead to substantial improvements in relevance. Such features summarize query-specific user interactions on a search engine. One commonly used clickthrough feature is generated based on the following observation: if a URL receives a large number of first and last clicks across many user sessions, then it indicates that this URL might be a strongly preferred destination of a query. For example, when a user searches for “yahoo</context>
</contexts>
<marker>Radlinski, Kurup, Joachims, 2007</marker>
<rawString>Radlinski, F., Kurup, M. and Joachims, T. 2007. Active exploration for learning rankings from clickthrough data. In SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>E Voorhees</author>
</authors>
<title>Comparison of two methods for Boolean query relevancy feedback.</title>
<date>1984</date>
<journal>Information Processing &amp; Management,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="31612" citStr="Salton and Voorhees, 1984" startWordPosition="5214" endWordPosition="5218">using postsearch user experience collected from toolbars. Toolbars keep track of users’ click behavior both when they are using the search engine directly and beyond. Their relevance features are built based on whole session clicks extracted from the toolbar. In contrast, our n-gram features are built on search engine clicks directly. We should be able to expand our method to integrate the postsearch clicks with toolbar data. Other related work can be found in the domain of query rewriting. Our n-gram dictionary was originally designed for query rewriting. Query rewriting (Xu and Croft, 1996; Salton and Voorhees, 1984) reformulates a query to its synonyms or related terms automatically. However, the coverage of query rewriting is normally small, because an inappropriate rewrite can cause significant decrease in precision. In contrast, our approach can cover a larger number of queries without decreasing precision, because it does not need to make a binary decision whether a query should be reformulated. The association scores between queries and rewrites are used as ranking features which are trained discriminatively toward search quality. 5 Conclusion In this paper, we presented a set of straightforward yet</context>
</contexts>
<marker>Salton, Voorhees, 1984</marker>
<rawString>Salton G. and E. Voorhees. 1984. Comparison of two methods for Boolean query relevancy feedback. Information Processing &amp; Management, 20(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wilcoxon</author>
</authors>
<title>Individual Comparisons by Ranking Methods.</title>
<date>1945</date>
<journal>Biometrics,</journal>
<pages>1--80</pages>
<contexts>
<context position="23240" citStr="Wilcoxon, 1945" startWordPosition="3861" endWordPosition="3862">n and Kekäläinen, 2002) to evaluate our ranking accuracy. Discounted Cumulative Gain (DCG) has been widely used in evaluating the quality of search engine rankings and is defined as: _I(2)k Gi log2 (1) i Gi represents the editorial judgment of the i-th document. In this paper, we only report normalized DCG5, which is an absolute DCG5 normalized by a baseline, and relative DCG5 improvement, which is an improvement normalized by the baseline. Note normalized DCG5 is different than NDCG (Normalized Discounted Cumulative Gain defined in Järvelin and Kekäläinen, 2002). We use Wilcoxon signed test (Wilcoxon, 1945) to evaluate the significance for model comparison. 3.4 Feature Sets Five feature sets are used in our experiments. Details are listed in Table 10. Table 10: Five Feature Sets Tag Description Base Feature Core Feature Set and ExactM Set click features Q-U N-gram Base Feature Set and Q-U NFeature Set (I) gram features Core Feature query-based, document-based, Set query-document based features NavClick Fea- Core Feature Set and Navture Set Click Q-U N-gram Core Feature Set and Q-U NFeature Set (II) gram features Base Feature Set is a strong baseline feature set from a state-of-the-art commercial</context>
</contexts>
<marker>Wilcoxon, 1945</marker>
<rawString>Wilcoxon, F. 1945. Individual Comparisons by Ranking Methods. Biometrics, 1:80–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Xu</author>
<author>W Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In Proceed of the 19th annual international ACM SIGIR.</booktitle>
<contexts>
<context position="31584" citStr="Xu and Croft, 1996" startWordPosition="5210" endWordPosition="5213"> a search engine by using postsearch user experience collected from toolbars. Toolbars keep track of users’ click behavior both when they are using the search engine directly and beyond. Their relevance features are built based on whole session clicks extracted from the toolbar. In contrast, our n-gram features are built on search engine clicks directly. We should be able to expand our method to integrate the postsearch clicks with toolbar data. Other related work can be found in the domain of query rewriting. Our n-gram dictionary was originally designed for query rewriting. Query rewriting (Xu and Croft, 1996; Salton and Voorhees, 1984) reformulates a query to its synonyms or related terms automatically. However, the coverage of query rewriting is normally small, because an inappropriate rewrite can cause significant decrease in precision. In contrast, our approach can cover a larger number of queries without decreasing precision, because it does not need to make a binary decision whether a query should be reformulated. The association scores between queries and rewrites are used as ranking features which are trained discriminatively toward search quality. 5 Conclusion In this paper, we presented </context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>Xu Q. and W. Croft. 1996. Query expansion using local and global document analysis. In Proceed of the 19th annual international ACM SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zheng</author>
<author>H Zha</author>
<author>K Chen</author>
<author>G Sun</author>
</authors>
<title>A regression framework for learning ranking functions using relative relevance judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR 30.</booktitle>
<contexts>
<context position="22427" citStr="Zheng et al, 2007" startWordPosition="3727" endWordPosition="3730">ry and URL pair. The data includes 94K queries for training and 3.4K queries for evaluation, and each query is associated with the top ranked URLs returned from a search engine. Totally, there are 916K query-url pairs for training and 42K pairs for testing. The queries are general and uniformly and randomly sampled with replacement, resulting in more frequent queries also appearing more frequently in our training and test sets. 529 3.2 Ranking Algorithm GBRank is a supervised learning algorithm that uses boosted decision trees and incorporates the pair-wise information from the training data (Zheng et al, 2007). It is able to deal with a large amount of training data with hundreds of features. We use an internal C++ implementation of GBRank. 3.3 Evaluation Metric We use Discounted Cumulative Gain (Järvelin and Kekäläinen, 2002) to evaluate our ranking accuracy. Discounted Cumulative Gain (DCG) has been widely used in evaluating the quality of search engine rankings and is defined as: _I(2)k Gi log2 (1) i Gi represents the editorial judgment of the i-th document. In this paper, we only report normalized DCG5, which is an absolute DCG5 normalized by a baseline, and relative DCG5 improvement, which is </context>
<context position="26585" citStr="Zheng et al., 2007" startWordPosition="4406" endWordPosition="4409">ture Set (II) since the n-gram features are backoff of NavClick. However, their gains are competitive suggesting the query-url ngram features are very good relevance indicators. The impact of NavClick and Query-URL N-gram Feature Set (II) is 0.72% and 0.62% relative DCG5 improvement at Tree 2500 respectively. 3.5.3 Feature Importance Using the GBRank model, features are evaluated and sequentially selected to build the boosted decision trees. The split of each node increases the DCG during training. We evaluate a feature’s importance by aggregating the DCG impact of the feature over all trees (Zheng et al., 2007). Here, the feature importance is rescaled so that the feature with largest DCG impact is assigned a normalized score of 1. Figure 3 illustrates the relative influence of each of query-url n-gram feature. Of these, n-gram features associated with a domain name (i.e., MainDS) rank highest. Figure 3: Feature importance of query-url ngram features. The importance (Y axis) is normalized so that the most important feature (MainDS)’s importance is 1. 3.6 Analysis We access system performance with respect to both query length and frequency using the two click features sets in combination with the Cor</context>
</contexts>
<marker>Zheng, Zha, Chen, Sun, 2007</marker>
<rawString>Zheng, Z., H. Zha, K. Chen, and G. Sun. 2007. A regression framework for learning ranking functions using relative relevance judgments. In Proceedings of SIGIR 30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>