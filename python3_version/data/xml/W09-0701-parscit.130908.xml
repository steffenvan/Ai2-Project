<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.037452">
<title confidence="0.976104">
Collecting and evaluating speech recognition corpora
for nine Southern Bantu languages
</title>
<author confidence="0.725012">
Jaco Badenhorst, Charl van Heerden, Marelie Davel and Etienne Barnard
</author>
<affiliation confidence="0.695999">
HLT Research Group, Meraka Institute, CSIR, South Africa
</affiliation>
<email confidence="0.913294">
jbadenhorst@csir.co.za, mdavel@csir.co.za
cvheerden@csir.co.za, ebarnard@csir.co.za
</email>
<sectionHeader confidence="0.994647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942428571429">
We describe the Lwazi corpus for auto-
matic speech recognition (ASR), a new
telephone speech corpus which includes
data from nine Southern Bantu lan-
guages. Because of practical constraints,
the amount of speech per language is
relatively small compared to major cor-
pora in world languages, and we report
on our investigation of the stability of
the ASR models derived from the corpus.
We also report on phoneme distance mea-
sures across languages, and describe initial
phone recognisers that were developed us-
ing this data.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998999375">
There is a widespread belief that spoken dialog
systems (SDSs) will have a significant impact in
the developing countries of Africa (Tucker and
Shalonova, 2004), where the availability of alter-
native information sources is often low. Tradi-
tional computer infrastructure is scarce in Africa,
but telephone networks (especially cellular net-
works) are spreading rapidly. In addition, speech-
based access to information may empower illiter-
ate or semi-literate people, 98% of whom live in
the developing world.
Spoken dialog systems can play a useful role
in a wide range of applications. Of particular im-
portance in Africa are applications such as ed-
ucation, using speech-enabled learning software
or kiosks and information dissemination through
media such as telephone-based information sys-
tems. Significant benefits can be envisioned if
information is provided in domains such as agri-
culture (Nasfors, 2007), health care (Sherwani et
al., ; Sharma et al., 2009) and government ser-
vices (Barnard et al., 2003). In order to make
SDSs a reality in Africa, technology components
such as text-to-speech (TTS) systems and auto-
matic speech recognition (ASR) systems are re-
quired. The latter category of technologies is the
focus of the current contribution.
Speech recognition systems exist for only a
handful of African languages (Roux et al., ; Seid
and Gambck, 2005; Abdillahi et al., 2006), and
to our knowledge no service available to the gen-
eral public currently uses ASR in an indigenous
African language. A significant reason for this
state of affairs is the lack of sufficient linguistic
resources in the African languages. Most impor-
tantly, modern speech recognition systems use sta-
tistical models which are trained on corpora of
relevant speech (i.e. appropriate for the recogni-
tion task in terms of the language used, the pro-
file of the speakers, speaking style, etc.) This
speech generally needs to be curated and tran-
scribed prior to the development of ASR systems,
and for most applications speech from a large
number of speakers is required in order to achieve
acceptable system performance. On the African
continent, where infrastructure such as computer
networks is less developed than in countries such
as America, Japan and the European countries, the
development of such speech corpora is a signifi-
cant hurdle to the development of ASR systems.
The complexity of speech corpus development
is strongly correlated with the amount of data that
is required, since the number of speakers that need
to be canvassed and the amount of speech that
must be curated and transcribed are major fac-
tors in determining the feasibility of such devel-
opment. In order to minimise this complexity, it
is important to have tools and guidelines that can
be used to assist in designing the smallest cor-
pora that will be sufficient for typical applications
of ASR systems. As minimal corpora can be ex-
tended by sharing data across languages, tools are
also required to indicate when data sharing will be
beneficial and when detrimental.
</bodyText>
<note confidence="0.358763">
Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages – AfLaT 2009, pages 1–8,
Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.979241">
1
</page>
<bodyText confidence="0.9993525">
In this paper we describe and evaluate a new
speech corpus of South African languages cur-
rently under development (the Lwazi corpus) and
evaluate the extent in which computational anal-
ysis tools can provide further guidelines for ASR
corpus design in resource-scarce languages.
</bodyText>
<sectionHeader confidence="0.883646" genericHeader="method">
2 Project Lwazi
</sectionHeader>
<bodyText confidence="0.998765170731707">
The goal of Project Lwazi is to provide South
African citizens with information and informa-
tion services in their home language, over the
telephone, in an efficient and affordable manner.
Commissioned by the South African Department
of Arts and Culture, the activities of this three year
project (2006-2009) include the development of
core language technology resources and compo-
nents for all the official languages of South Africa,
where, for the majority of these, no prior language
technology components were available.
The core linguistic resources being developed
include phoneme sets, electronic pronunciation
dictionaries and the speech and text corpora re-
quired to develop automated speech recognition
(ASR) and text-to-speech (TTS) systems for all
eleven official languages of South Africa. The
usability of these resources will be demonstrated
during a national pilot planned for the third quar-
ter of 2009. All outputs from the project are be-
ing released as open source software and open
content(Meraka-Institute, 2009).
Resources are being developed for all nine
Southern Bantu languages that are recognised as
official languages in South Africa (SA). These lan-
guages are: (1) isiZulu (zul1) and isiXhosa (xho),
the two Nguni languages most widely spoken in
SA. Together these form the home language of
41% of the SA population. (2) The three Sotho
languages: Sepedi (nso), Setswana (tsn), Sesotho
(sot), together the home language of 26% of the
SA population. (3) The two Nguni languages less
widely spoken in SA: siSwati (ssw) and isiNde-
bele (nbl), together the home language of 4% of
the SA population. (4) Xitsonga (tso) and Tshiv-
enda (ven), the home languages of 4% and 2% of
the SA population, respectively (Lehohla, 2003).
(The other two official languages of South Africa
are Germanic languages, namely English (eng)
and Afrikaans (afr).)
For all these languages, new pronunciation dic-
</bodyText>
<footnote confidence="0.9938845">
1After each language name, the ISO 639-3:2007 language
code is provided in brackets.
</footnote>
<bodyText confidence="0.999942461538462">
tionaries, text and speech corpora are being de-
veloped. ASR speech corpora consist of ap-
proximately 200 speakers per language, produc-
ing read and elicited speech, recorded over a tele-
phone channel. Each speaker produced approxi-
mately 30 utterances, 16 of these were randomly
selected from a phonetically balanced corpus and
the remainder consist of short words and phrases:
answers to open questions, answers to yes/no
questions, spelt words, dates and numbers. The
speaker population was selected to provide a bal-
anced profile with regard to age, gender and type
of telephone (cellphone or landline).
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.999818714285714">
Below, we review earlier work relevant to the de-
velopment of speech recognisers for languages
with limited resources. This includes both ASR
system design (Sec. 3.1) and ASR corpus design
(Sec. 3.2). In Sec. 3.3, we also review the ana-
lytical tools that we utilise in order to investigate
corpus design systematically.
</bodyText>
<subsectionHeader confidence="0.970468">
3.1 ASR for resource-scarce languages
</subsectionHeader>
<bodyText confidence="0.999957296296296">
The main linguistic resources required when de-
veloping ASR systems for telephone based sys-
tems are electronic pronunciation dictionaries, an-
notated audio corpora (used to construct acous-
tic models) and recognition grammars. An ASR
audio corpus consists of recordings from multi-
ple speakers, with each utterance carefully tran-
scribed orthographically and markers used to indi-
cate non-speech and other events important from
an ASR perspective. Both the collection of ap-
propriate speech from multiple speakers and the
accurate annotation of this speech are resource-
intensive processes, and therefore corpora for
resource-scarce languages tend to be very small
(1 to 10 hours of audio) when compared to the
speech corpora used to build commercial systems
for world languages (hundreds to thousands of
hours per language).
Different approaches have been used to best
utilise limited audio resources when developing
ASR systems. Bootstrapping has been shown to
be a very efficient technique for the rapid devel-
opment of pronunciation dictionaries, even when
utilising linguistic assistants with limited phonetic
training (Davel and Barnard, 2004).
Small audio corpora can be used efficiently by
utilising techniques that share data across lan-
</bodyText>
<page confidence="0.962551">
2
</page>
<bodyText confidence="0.999978166666667">
guages, either by developing multilingual ASR
systems (a single system that simultaneously
recognises different languages), or by using addi-
tional source data to supplement the training data
that exists in the target language. Various data
sharing techniques for language-dependant acous-
tic modelling have been studied, including cross-
language transfer, data pooling, language adap-
tation and bootstrapping (Wheatley et al., 1994;
Schultz and Waibel, 2001; Byrne et al., 2000).
Both (Wheatley et al., 1994) and (Schultz and
Waibel, 2001) found that useful gains could be
obtained by sharing data across languages with
the size of the benefit dependent on the similar-
ity of the sound systems of the languages com-
bined. In the only cross-lingual adaptation study
using African languages (Niesler, 2007), similar
gains have not yet been observed.
</bodyText>
<subsectionHeader confidence="0.997646">
3.2 ASR corpus design
</subsectionHeader>
<bodyText confidence="0.999977480769231">
Corpus design techniques for ASR are generally
aimed at specifying or selecting the most appro-
priate subset of data from a larger domain in order
to optimise recognition accuracy, often while ex-
plicitly minimising the size of the selected corpus.
This is achieved through various techniques that
aim to include as much variability in the data as
possible, while simultaneously ensuring that the
corpus matches the intended operating environ-
ment as accurately as possible.
Three directions are primarily employed: (1)
explicit specification of phonotactic, speaker and
channel variability during corpus development, (2)
automated selection of informative subsets of data
from larger corpora, with the smaller subset yield-
ing comparable results, and (3) the use of active
learning to optimise existing speech recognition
systems. All three techniques provide a perspec-
tive on the sources of variation inherent in a speech
corpus, and the effect of this variation on speech
recognition accuracy.
In (Nagroski et al., 2003), Principle Component
Analysis (PCA) is used to cluster data acousti-
cally. These clusters then serve as a starting point
for selecting the optimal utterances from a train-
ing database. As a consequence of the clustering
technique, it is possible to characterise some of the
acoustic properties of the data being analysed, and
to obtain an understanding of the major sources of
variation, such as different speakers and genders
(Riccardi and Hakkani-Tur, 2003).
Active and unsupervised learning methods can
be combined to circumvent the need for tran-
scribing massive amounts of data (Riccardi and
Hakkani-Tur, 2003). The most informative untran-
scribed data is selected for a human to label, based
on acoustic evidence of a partially and iteratively
trained ASR system. From such work, it soon be-
comes evident that the optimisation of the amount
of variation inherent to training data is needed,
since randomly selected additional data does not
necessarily improve recognition accuracy. By fo-
cusing on the selection (based on existing tran-
scriptions) of a uniform distribution across differ-
ent speech units such as words and phonemes, im-
provements are obtained (Wu et al., 2007).
In our focus on resource-scarce languages, the
main aim is to understand the amount of data that
needs to be collected in order to achieve accept-
able accuracy. This is achieved through the use
of analytic measures of data variability, which we
describe next.
</bodyText>
<subsectionHeader confidence="0.997491">
3.3 Evaluating phoneme stability
</subsectionHeader>
<bodyText confidence="0.999889307692308">
In (Badenhorst and Davel, 2008) a technique is
developed that estimates how stable a specific
phoneme is, given a specific set of training data.
This statistical measure provides an indication of
the effect that additional training data will have on
recognition accuracy: the higher the stability, the
less the benefit of additional speech data.
The model stability measure utilises the Bhat-
tacharyya bound (Fukunaga, 1990), a widely-used
upper bound of the Bayes error. If Pi and pi(X)
denote the prior probability and class-conditional
density function for class i, respectively, the Bhat-
tacharyya bound 2 is calculated as:
</bodyText>
<equation confidence="0.990134">
2 = �lP1P2 f �p1(X)p2(X)dX (1)
</equation>
<bodyText confidence="0.997225333333333">
When both density functions are Gaussian with
mean µi and covariance matrix Σi, integration of
2 leads to a closed-form expression for 2:
</bodyText>
<equation confidence="0.954503">
�
2 = P1P2e�µ(1/2) (2)
</equation>
<bodyText confidence="0.804345">
where
</bodyText>
<equation confidence="0.966699833333333">
µ(1/2) = 8(µ2 − µ1)TrΣ1 2 Σ21� (µ2 − µ1)
1
|Σ1+Σ2
2 |
+ 2ln (3)
�|Σ1||Σ2|
</equation>
<bodyText confidence="0.877836">
is referred to as the Bhattacharyya distance.
</bodyText>
<page confidence="0.992479">
3
</page>
<bodyText confidence="0.999923833333333">
In order to estimate the stability of an acous-
tic model, the training data for that model is sep-
arated into a number of disjoint subsets. All sub-
sets are selected to be mutually exclusive with re-
spect to the speakers they contain. For each sub-
set, a separate acoustic model is trained, and the
Bhattacharyya bound between each pair of mod-
els calculated. By calculating both the mean of
this bound and the standard deviation of this mea-
sure across the various model pairs, a statistically
sound measure of model estimation stability is ob-
tained.
</bodyText>
<sectionHeader confidence="0.9901595" genericHeader="method">
4 Computational analysis of the Lwazi
corpus
</sectionHeader>
<bodyText confidence="0.9999065">
We now report on our analysis of the Lwazi
speech corpus, using the stability measure de-
scribed above. Here, we focus on four languages
(isiNdebele, siSwati, isiZulu and Tshivenda) for
reasons of space; later, we shall see that the other
languages behave quite similarly.
</bodyText>
<subsectionHeader confidence="0.975805">
4.1 Experimental design
</subsectionHeader>
<bodyText confidence="0.999987136363636">
For each phoneme in each of our target lan-
guages, we extract all the phoneme occurrences
from the 150 speakers with the most utterances per
phoneme. We utilise the technique described in
Sec. 3.3 to estimate the Bhattacharyya bound both
when evaluating phoneme variability and model
distance. In both cases we separate the data for
each phoneme into 5 disjoint subsets. We calcu-
late the mean of the 10 distances obtained between
the various intra-phoneme model pairs when mea-
suring phoneme stability, and the mean of the
25 distances obtained between the various inter-
phoneme model pairs when measuring phoneme
distance.
In order to be able to control the number of
phoneme observations used to train our acoustic
models, we first train a speech recognition system
and then use forced alignment to label all of the
utterances using the systems described in Sec. 5.
Mel-frequency cepstral coefficients (MFCCs) with
cepstral mean and variance normalisation are used
as features, as described in Sec. 5.
</bodyText>
<subsectionHeader confidence="0.999684">
4.2 Analysis of phoneme variability
</subsectionHeader>
<bodyText confidence="0.999989872727273">
In an earlier analysis of phoneme variability of
an English corpus (Badenhorst and Davel, 2008),
it was observed that similar trends are observed
when utilising different numbers of mixtures in
a Gaussian mixture model. For both context de-
pendent and context independent models similar
trends are also observed. (Asymptotes occur later,
but trends remain similar.) Because of the limited
size of the Lwazi corpus, we therefore only report
on single-mixture context-independent models in
the current section.
As we also observe similar trends for phonemes
within the same broad categories, we report on
one or two examples from several broad categories
which occur in most of our target languages. Us-
ing SAMPA notation, the following phonemes are
selected: /a/ (vowels), /m/ (nasals), /b/ and /g/
(voiced plosives) and /s/ (unvoiced fricatives), af-
ter verifying that these phonemes are indeed rep-
resentative of the larger groups.
Figures 1 and 2 demonstrate the effects of vari-
able numbers of phonemes and speakers, respec-
tively, on the value of the mean Bhattacharyya
bound. This value should approach 0.5 for a model
fully trained on a sufficiently representative set of
data. In Fig. 1 we see that the various broad cate-
gories of sounds approach the asymptotic bound in
different ways. The vowels and nasals require the
largest number of phoneme occurrences to reach
a given level, whereas the fricatives and plosives
converge quite rapidly (With 10 observations per
speaker, both the fricatives and plosives achieve
values of 0.48 or better for all languages, in con-
trast to the vowels and nasals which require 30 ob-
servations to reach similar stability). Note that we
employed 30 speakers per phoneme group, since
that is the largest number achievable with our pro-
tocol.
For the results in Fig. 2, we keep the number
of phoneme occurrences per speaker fixed at 20
(this ensures that we have sufficient data for all
phonemes, and corresponds with reasonable con-
vergence in Fig. 1). It is clear that additional
speakers would still improve the modelling ac-
curacy for especially the vowels and nasals. We
observe that the voiced plosives and fricatives
quickly achieve high values for the bound (close
to the ideal 0.5).
Figures 1 and 2 – as well as similar figures for
the other phoneme classes and languages we have
studied – suggest that all phoneme categories re-
quire at least 20 training speakers to achieve rea-
sonable levels of convergence (bound levels of
0.48 or better). The number of phoneme observa-
tions required per speaker is more variable, rang-
</bodyText>
<page confidence="0.996087">
4
</page>
<figureCaption confidence="0.996795">
Figure 1: Effect of number of phoneme utterances per speaker on mean of Bhattacharyya bound for
different phoneme groups using data from 30 speakers
</figureCaption>
<bodyText confidence="0.996689666666667">
ing from less than 10 for the voiceless fricatives
to 30 or more for vowels, liquids and nasals. We
return to these observations below.
</bodyText>
<subsectionHeader confidence="0.999066">
4.3 Distances between languages
</subsectionHeader>
<bodyText confidence="0.999993689655172">
In Sec. 3.1 it was pointed out that the simi-
larities between the same phonemes in different
languages are important predictors of the bene-
fit achievable from pooling the data from those
languages. Armed with the knowledge that sta-
ble models can be estimated with 30 speakers per
phoneme and between 10 and 30 phonemes oc-
currences per speaker, we now turn to the task of
measuring distances between phonemes in various
languages.
We again use the mean Bhattacharyya bound
to compare phonemes, and obtain values between
all possible combinations of phonemes. Results
are shown for the isiNdebele phonemes /n/ and /a/
in Fig. 3. As expected, similar phonemes from
the different languages are closer to one another
than different phonemes of the same language.
However, the details of the distances are quite re-
vealing: for /a/, siSwati is closest to the isiN-
debele model, as would be expected given their
close linguistic relationship, but for /n/, the Tshiv-
enda model is found to be closer than either of
the other Nguni languages. For comparative pur-
poses, we have included one non-Bantu language
(Afrikaans), and we see that its models are indeed
significantly more dissimilar from the isiNdebele
model than any of the Bantu languages. In fact,
the Afrikaans /n/ is about as distant from isiNde-
bele /n/ as isiNdebele and isiZulu /l/ are!
</bodyText>
<sectionHeader confidence="0.996017" genericHeader="method">
5 Initial ASR results
</sectionHeader>
<bodyText confidence="0.998575818181818">
In order to verify the usability of the Lwazi cor-
pus for speech recognition, we develop initial
ASR systems for all 11 official South African
languages. A summary of the data statistics for
the Bantu languages investigated is shown in Tab.
1, and recognition accuracies achieved are sum-
marised in Tab. 2. For these tests, data from 30
speakers per language were used as test data, with
the remaining data being used for training.
Although the Southern Bantu languages are
tone languages, our systems do not encode tonal
</bodyText>
<page confidence="0.99033">
5
</page>
<figureCaption confidence="0.9831875">
Figure 2: Effect of number of speakers on mean of Bhattacharyya bound for different phoneme groups
using 20 utterances per speaker
</figureCaption>
<table confidence="0.99928775">
Language total # # speech # distinct
minutes minutes phonemes
isiNdebele 564 465 46
isiXhosa 470 370 52
isiZulu 525 407 46
Tshivenda 354 286 38
Sepedi 394 301 45
Sesotho 387 313 44
Setswana 379 295 34
siSwati 603 479 39
Xitsonga 378 316 54
N-TIMIT 315 - 39
</table>
<tableCaption confidence="0.9875455">
Table 1: A summary of the Lwazi ASR corpus:
Bantu languages.
</tableCaption>
<bodyText confidence="0.999856290322581">
information, since tone is unlikely to be impor-
tant for small-to-medium vocabulary applications
(Zerbian and Barnard, 2008).
As the initial pronunciation dictionaries were
developed to provide good coverage of the lan-
guage in general, these dictionaries did not cover
the entire ASR corpus. Grapheme-to-phoneme
rules are therefore extracted from the general
dictionaries using the Default&amp;Refine algorithm
(Davel and Barnard, 2008) and used to generate
missing pronunciations.
We use HTK 3.4 to build a context-dependent
cross-word HMM-based phoneme recogniser with
triphone models. Each model had 3 emitting
states with 7 mixtures per state. 39 features are
used: 13 MFCCs together with their first and sec-
ond order derivatives. Cepstral Mean Normali-
sation (CMN) as well as Cepstral Variance Nor-
malisation (CMV) are used to perform speaker-
independent normalisation. A diagonal covariance
matrix is used; to partially compensate for this in-
correct assumption of feature independence semi-
tied transforms are applied. A flat phone-based
language model is employed throughout.
As a rough benchmark of acceptable phoneme-
recognition accuracy, recently reported results ob-
tained by (Morales et al., 2008) on a similar-sized
telephone corpus in American English (N-TIMIT)
are also shown in Tab. 2. We see that the Lwazi
results compare very well with this benchmark.
An important issue in ASR corpus design is
</bodyText>
<page confidence="0.996478">
6
</page>
<figureCaption confidence="0.998258">
Figure 3: Effective distances in terms of the mean of the Bhattacharyya bound between a single phoneme
(/n/-nbl top and /a/-nbl bottom) and each of its closest matches within the set ofphonemes investigated.
</figureCaption>
<table confidence="0.9982695">
Language % corr % acc avg # total #
phons speakers
isiNdebele 74.21 65.41 28.66 200
isiXhosa 69.25 57.24 17.79 210
isiZulu 71.18 60.95 23.42 201
Tshivenda 76.37 66.78 19.53 201
Sepedi 66.44 55.19 16.45 199
Sesotho 68.17 54.79 18.57 200
Setswana 69.00 56.19 20.85 207
siSwati 74.19 64.46 30.66 208
Xitsonga 70.32 59.41 14.35 199
N-TIMIT 64.07 55.73 - -
</table>
<tableCaption confidence="0.964918">
Table 2: Initial results for South African ASR sys-
</tableCaption>
<bodyText confidence="0.9643914">
tems. The column labelled “avg # phonemes” lists
the average number of phoneme occurrences for
each phoneme for each speaker.
the trade-off between the number of speakers and
the amount of data per speaker (Wheatley et al.,
1994). The figures in Sec. 4.2 are not conclusive
on this trade-off, so we have also investigated the
effect of reducing either the number of speakers
or the amount of data per speaker when training
the isiZulu and Tshivenda recognisers. As shown
in Fig. 4, the impact of both forms of reduction
is comparable across languages and different de-
grees of reduction, in agreement with the results
of Sec. 4.2.
These results indicate that we now have a firm
</bodyText>
<figureCaption confidence="0.9981385">
Figure 4: The influence of a reduction in training
corpus size on phone recognition accuracy.
</figureCaption>
<bodyText confidence="0.9984935">
baseline to investigate data-efficient training meth-
ods such as those described in Sec. 3.1.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9989626">
In this paper we have introduced a new tele-
phone speech corpus which contains data from
nine Southern Bantu languages. Our stability anal-
ysis shows that the speaker variety as well as
the amount of speech per speaker is sufficient to
achieve acceptable model stability, and this con-
clusion is confirmed by the successful training of
phone recognisers in all the languages. We con-
firm the observation in (Badenhorst and Davel,
2008) that different phone classes have different
</bodyText>
<page confidence="0.997699">
7
</page>
<bodyText confidence="0.9999263">
data requirements, but even for the more demand-
ing classes (vowels, nasals, liquids) our amount of
data seems sufficient. Our results suggest that sim-
ilar accuracies may be achievable by using more
speech from fewer speakers – a finding that may
be useful for the further development of speech
corpora in resource-scarce languages.
Based on the proven stability of our models, we
have performed some preliminary measurements
of the distances between the phones in the dif-
ferent languages; such distance measurements are
likely to be important for the sharing of data across
languages in order to further improve ASR accu-
racy. The development of real-world applications
using this data is currently an active topic of re-
search; for that purpose, we are continuing to in-
vestigate additional methods to improve recogni-
tion accuracy with such relatively small corpora,
including cross-language data sharing and effi-
cient adaptation methods.
</bodyText>
<sectionHeader confidence="0.997125" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999410583333334">
Nimaan Abdillahi, Pascal Nocera, and Jean-Franois
Bonastre. 2006. Automatic transcription of Somali
language. In Interspeech, pages 289–292, Pitts-
burgh, PA.
J.A.C. Badenhorst and M.H. Davel. 2008. Data re-
quirements for speaker independent acoustic mod-
els. In PRASA, pages 147–152.
E. Barnard, L. Cloete, and H. Patel. 2003. Language
and technology literacy barriers to accessing govern-
ment services. Lecture Notes in Computer Science,
2739:37–42.
W. Byrne, P. Beyerlein, J. M. Huerta, S. Khudanpur,
B. Marthi, J. Morgan, N. Peterek, J. Picone, D. Ver-
gyri1, and W. Wang. 2000. Towards language inde-
pendent acoustic modeling. In ICASSP, volume 2,
pages 1029–1032, Istanbul, Turkey.
M. Davel and E. Barnard. 2004. The efficient cre-
ation of pronunication dictionaries: human factors
in bootstrapping. In Interspeech, pages 2797–2800,
Jeju, Korea, Oct.
M. Davel and E. Barnard. 2008. Pronunciation predi-
cation with Default&amp;Refine. Computer Speech and
Language, 22:374–393, Oct.
K. Fukunaga. 1990. Introduction to Statistical Pattern
Recognition. Academic Press, Inc., 2nd edition.
Pali Lehohla. 2003. Census 2001: Census in brief.
Statistics South Africa.
Meraka-Institute. 2009. Lwazi ASR corpus. Online:
http://www.meraka.org.za/lwazi.
N. Morales, J. Tejedor, J. Garrido, J. Colas, and D.T.
Toledano. 2008. STC-TIMIT: Generation of a
single-channel telephone corpus. In LREC, pages
391–395, Marrakech, Morocco.
A. Nagroski, L. Boves, and H. Steeneken. 2003. In
search of optimal data selection for training of auto-
matic speech recognition systems. ASRU workshop,
pages 67–72, Nov.
P. Nasfors. 2007. Efficient voice information services
for developing countries. Master’s thesis, Depart-
ment of Information Technology, Uppsala Univer-
sity.
T. Niesler. 2007. Language-dependent state clustering
for multilingual acoustic modeling. Speech Commu-
nication, 49:453–463.
G. Riccardi and D. Hakkani-Tur. 2003. Active and
unsupervised learning for automatic speech recog-
nition. In Eurospeech, pages 1825–1828, Geneva,
Switzerland.
J.C. Roux, E.C. Botha, and J.A. du Preez. Develop-
ing a multilingual telephone based information sys-
tem in african languages. In LREC, pages 975–980,
Athens, Greece.
T. Schultz and A. Waibel. 2001. Language-
independent and language-adaptive acoustic model-
ing for speech recognition. Speech Communication,
35:31–51, Aug.
Hussien Seid and Bjrn Gambck. 2005. A speaker inde-
pendent continuous speech recognizer for Amharic.
In Interspeech, pages 3349–3352, Lisboa, Portugal,
Oct.
A. Sharma, M. Plauche, C. Kuun, and E. Barnard.
2009. HIV health information access using spoken
dialogue systems: Touchtone vs. speech. Accepted
at IEEE Int. Conf. on ICTD.
J. Sherwani, N. Ali, S. Mirza, A. Fatma, Y. Memon,
M. Karim, R. Tongia, and R. Rosenfeld. Healthline:
Speech-based access to health information by low-
literate users. In IEEE Int. Conf. on ICTD, pages
131–139.
R. Tucker and K. Shalonova. 2004. The Local Lan-
guage Speech Technology Initiative. In SCALLA
Conf., Nepal.
B. Wheatley, K. Kondo, W. Anderson, and
Y. Muthusumy. 1994. An evaluation of cross-
language adaptation for rapid HMM development
in a new language. In ICASSP, pages 237–240,
Adelaide.
Y. Wu, R. Zhang, and A. Rudnicky. 2007. Data selec-
tion for speech recognition. ASRU workshop, pages
562–565, Dec.
S. Zerbian and E. Barnard. 2008. Phonetics of into-
nation in South African Bantu languages. Southern
African Linguistics and Applied Language Studies,
26(2):235–254.
</reference>
<page confidence="0.998438">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.488773">
<title confidence="0.998681">Collecting and evaluating speech recognition for nine Southern Bantu languages</title>
<author confidence="0.99947">Jaco Badenhorst</author>
<author confidence="0.99947">Charl van_Heerden</author>
<author confidence="0.99947">Marelie Davel</author>
<author confidence="0.99947">Etienne Barnard</author>
<affiliation confidence="0.825647">HLT Research Group, Meraka Institute, CSIR, South Africa</affiliation>
<email confidence="0.7592825">jbadenhorst@csir.co.za,mdavel@csir.co.zacvheerden@csir.co.za,ebarnard@csir.co.za</email>
<abstract confidence="0.999436733333333">We describe the Lwazi corpus for automatic speech recognition (ASR), a new telephone speech corpus which includes data from nine Southern Bantu languages. Because of practical constraints, the amount of speech per language is relatively small compared to major corpora in world languages, and we report on our investigation of the stability of the ASR models derived from the corpus. We also report on phoneme distance measures across languages, and describe initial phone recognisers that were developed using this data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nimaan Abdillahi</author>
<author>Pascal Nocera</author>
<author>Jean-Franois Bonastre</author>
</authors>
<title>Automatic transcription of Somali language. In</title>
<date>2006</date>
<booktitle>Interspeech,</booktitle>
<pages>289--292</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2227" citStr="Abdillahi et al., 2006" startWordPosition="332" endWordPosition="335">hone-based information systems. Significant benefits can be envisioned if information is provided in domains such as agriculture (Nasfors, 2007), health care (Sherwani et al., ; Sharma et al., 2009) and government services (Barnard et al., 2003). In order to make SDSs a reality in Africa, technology components such as text-to-speech (TTS) systems and automatic speech recognition (ASR) systems are required. The latter category of technologies is the focus of the current contribution. Speech recognition systems exist for only a handful of African languages (Roux et al., ; Seid and Gambck, 2005; Abdillahi et al., 2006), and to our knowledge no service available to the general public currently uses ASR in an indigenous African language. A significant reason for this state of affairs is the lack of sufficient linguistic resources in the African languages. Most importantly, modern speech recognition systems use statistical models which are trained on corpora of relevant speech (i.e. appropriate for the recognition task in terms of the language used, the profile of the speakers, speaking style, etc.) This speech generally needs to be curated and transcribed prior to the development of ASR systems, and for most </context>
</contexts>
<marker>Abdillahi, Nocera, Bonastre, 2006</marker>
<rawString>Nimaan Abdillahi, Pascal Nocera, and Jean-Franois Bonastre. 2006. Automatic transcription of Somali language. In Interspeech, pages 289–292, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A C Badenhorst</author>
<author>M H Davel</author>
</authors>
<title>Data requirements for speaker independent acoustic models.</title>
<date>2008</date>
<booktitle>In PRASA,</booktitle>
<pages>147--152</pages>
<contexts>
<context position="11940" citStr="Badenhorst and Davel, 2008" startWordPosition="1855" endWordPosition="1858">ing data is needed, since randomly selected additional data does not necessarily improve recognition accuracy. By focusing on the selection (based on existing transcriptions) of a uniform distribution across different speech units such as words and phonemes, improvements are obtained (Wu et al., 2007). In our focus on resource-scarce languages, the main aim is to understand the amount of data that needs to be collected in order to achieve acceptable accuracy. This is achieved through the use of analytic measures of data variability, which we describe next. 3.3 Evaluating phoneme stability In (Badenhorst and Davel, 2008) a technique is developed that estimates how stable a specific phoneme is, given a specific set of training data. This statistical measure provides an indication of the effect that additional training data will have on recognition accuracy: the higher the stability, the less the benefit of additional speech data. The model stability measure utilises the Bhattacharyya bound (Fukunaga, 1990), a widely-used upper bound of the Bayes error. If Pi and pi(X) denote the prior probability and class-conditional density function for class i, respectively, the Bhattacharyya bound 2 is calculated as: 2 = �</context>
<context position="14873" citStr="Badenhorst and Davel, 2008" startWordPosition="2341" endWordPosition="2344">the mean of the 25 distances obtained between the various interphoneme model pairs when measuring phoneme distance. In order to be able to control the number of phoneme observations used to train our acoustic models, we first train a speech recognition system and then use forced alignment to label all of the utterances using the systems described in Sec. 5. Mel-frequency cepstral coefficients (MFCCs) with cepstral mean and variance normalisation are used as features, as described in Sec. 5. 4.2 Analysis of phoneme variability In an earlier analysis of phoneme variability of an English corpus (Badenhorst and Davel, 2008), it was observed that similar trends are observed when utilising different numbers of mixtures in a Gaussian mixture model. For both context dependent and context independent models similar trends are also observed. (Asymptotes occur later, but trends remain similar.) Because of the limited size of the Lwazi corpus, we therefore only report on single-mixture context-independent models in the current section. As we also observe similar trends for phonemes within the same broad categories, we report on one or two examples from several broad categories which occur in most of our target languages</context>
<context position="23310" citStr="Badenhorst and Davel, 2008" startWordPosition="3733" endWordPosition="3736">re 4: The influence of a reduction in training corpus size on phone recognition accuracy. baseline to investigate data-efficient training methods such as those described in Sec. 3.1. 6 Conclusion In this paper we have introduced a new telephone speech corpus which contains data from nine Southern Bantu languages. Our stability analysis shows that the speaker variety as well as the amount of speech per speaker is sufficient to achieve acceptable model stability, and this conclusion is confirmed by the successful training of phone recognisers in all the languages. We confirm the observation in (Badenhorst and Davel, 2008) that different phone classes have different 7 data requirements, but even for the more demanding classes (vowels, nasals, liquids) our amount of data seems sufficient. Our results suggest that similar accuracies may be achievable by using more speech from fewer speakers – a finding that may be useful for the further development of speech corpora in resource-scarce languages. Based on the proven stability of our models, we have performed some preliminary measurements of the distances between the phones in the different languages; such distance measurements are likely to be important for the sh</context>
</contexts>
<marker>Badenhorst, Davel, 2008</marker>
<rawString>J.A.C. Badenhorst and M.H. Davel. 2008. Data requirements for speaker independent acoustic models. In PRASA, pages 147–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Barnard</author>
<author>L Cloete</author>
<author>H Patel</author>
</authors>
<title>Language and technology literacy barriers to accessing government services.</title>
<date>2003</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>2739--37</pages>
<contexts>
<context position="1849" citStr="Barnard et al., 2003" startWordPosition="271" endWordPosition="274">echbased access to information may empower illiterate or semi-literate people, 98% of whom live in the developing world. Spoken dialog systems can play a useful role in a wide range of applications. Of particular importance in Africa are applications such as education, using speech-enabled learning software or kiosks and information dissemination through media such as telephone-based information systems. Significant benefits can be envisioned if information is provided in domains such as agriculture (Nasfors, 2007), health care (Sherwani et al., ; Sharma et al., 2009) and government services (Barnard et al., 2003). In order to make SDSs a reality in Africa, technology components such as text-to-speech (TTS) systems and automatic speech recognition (ASR) systems are required. The latter category of technologies is the focus of the current contribution. Speech recognition systems exist for only a handful of African languages (Roux et al., ; Seid and Gambck, 2005; Abdillahi et al., 2006), and to our knowledge no service available to the general public currently uses ASR in an indigenous African language. A significant reason for this state of affairs is the lack of sufficient linguistic resources in the A</context>
</contexts>
<marker>Barnard, Cloete, Patel, 2003</marker>
<rawString>E. Barnard, L. Cloete, and H. Patel. 2003. Language and technology literacy barriers to accessing government services. Lecture Notes in Computer Science, 2739:37–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Byrne</author>
<author>P Beyerlein</author>
<author>J M Huerta</author>
<author>S Khudanpur</author>
<author>B Marthi</author>
<author>J Morgan</author>
<author>N Peterek</author>
<author>J Picone</author>
<author>D Vergyri1</author>
<author>W Wang</author>
</authors>
<title>Towards language independent acoustic modeling.</title>
<date>2000</date>
<booktitle>In ICASSP,</booktitle>
<volume>2</volume>
<pages>1029--1032</pages>
<location>Istanbul, Turkey.</location>
<marker>Byrne, Beyerlein, Huerta, Khudanpur, Marthi, Morgan, Peterek, Picone, Vergyri1, Wang, 2000</marker>
<rawString>W. Byrne, P. Beyerlein, J. M. Huerta, S. Khudanpur, B. Marthi, J. Morgan, N. Peterek, J. Picone, D. Vergyri1, and W. Wang. 2000. Towards language independent acoustic modeling. In ICASSP, volume 2, pages 1029–1032, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Davel</author>
<author>E Barnard</author>
</authors>
<title>The efficient creation of pronunication dictionaries: human factors in bootstrapping.</title>
<date>2004</date>
<booktitle>In Interspeech,</booktitle>
<pages>2797--2800</pages>
<location>Jeju, Korea,</location>
<contexts>
<context position="8458" citStr="Davel and Barnard, 2004" startWordPosition="1313" endWordPosition="1316">ccurate annotation of this speech are resourceintensive processes, and therefore corpora for resource-scarce languages tend to be very small (1 to 10 hours of audio) when compared to the speech corpora used to build commercial systems for world languages (hundreds to thousands of hours per language). Different approaches have been used to best utilise limited audio resources when developing ASR systems. Bootstrapping has been shown to be a very efficient technique for the rapid development of pronunciation dictionaries, even when utilising linguistic assistants with limited phonetic training (Davel and Barnard, 2004). Small audio corpora can be used efficiently by utilising techniques that share data across lan2 guages, either by developing multilingual ASR systems (a single system that simultaneously recognises different languages), or by using additional source data to supplement the training data that exists in the target language. Various data sharing techniques for language-dependant acoustic modelling have been studied, including crosslanguage transfer, data pooling, language adaptation and bootstrapping (Wheatley et al., 1994; Schultz and Waibel, 2001; Byrne et al., 2000). Both (Wheatley et al., 19</context>
</contexts>
<marker>Davel, Barnard, 2004</marker>
<rawString>M. Davel and E. Barnard. 2004. The efficient creation of pronunication dictionaries: human factors in bootstrapping. In Interspeech, pages 2797–2800, Jeju, Korea, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Davel</author>
<author>E Barnard</author>
</authors>
<title>Pronunciation predication with Default&amp;Refine. Computer Speech and Language,</title>
<date>2008</date>
<pages>22--374</pages>
<contexts>
<context position="20418" citStr="Davel and Barnard, 2008" startWordPosition="3259" endWordPosition="3262">hivenda 354 286 38 Sepedi 394 301 45 Sesotho 387 313 44 Setswana 379 295 34 siSwati 603 479 39 Xitsonga 378 316 54 N-TIMIT 315 - 39 Table 1: A summary of the Lwazi ASR corpus: Bantu languages. information, since tone is unlikely to be important for small-to-medium vocabulary applications (Zerbian and Barnard, 2008). As the initial pronunciation dictionaries were developed to provide good coverage of the language in general, these dictionaries did not cover the entire ASR corpus. Grapheme-to-phoneme rules are therefore extracted from the general dictionaries using the Default&amp;Refine algorithm (Davel and Barnard, 2008) and used to generate missing pronunciations. We use HTK 3.4 to build a context-dependent cross-word HMM-based phoneme recogniser with triphone models. Each model had 3 emitting states with 7 mixtures per state. 39 features are used: 13 MFCCs together with their first and second order derivatives. Cepstral Mean Normalisation (CMN) as well as Cepstral Variance Normalisation (CMV) are used to perform speakerindependent normalisation. A diagonal covariance matrix is used; to partially compensate for this incorrect assumption of feature independence semitied transforms are applied. A flat phone-ba</context>
</contexts>
<marker>Davel, Barnard, 2008</marker>
<rawString>M. Davel and E. Barnard. 2008. Pronunciation predication with Default&amp;Refine. Computer Speech and Language, 22:374–393, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukunaga</author>
</authors>
<title>Introduction to Statistical Pattern Recognition.</title>
<date>1990</date>
<publisher>Academic Press, Inc.,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="12332" citStr="Fukunaga, 1990" startWordPosition="1917" endWordPosition="1918">ds to be collected in order to achieve acceptable accuracy. This is achieved through the use of analytic measures of data variability, which we describe next. 3.3 Evaluating phoneme stability In (Badenhorst and Davel, 2008) a technique is developed that estimates how stable a specific phoneme is, given a specific set of training data. This statistical measure provides an indication of the effect that additional training data will have on recognition accuracy: the higher the stability, the less the benefit of additional speech data. The model stability measure utilises the Bhattacharyya bound (Fukunaga, 1990), a widely-used upper bound of the Bayes error. If Pi and pi(X) denote the prior probability and class-conditional density function for class i, respectively, the Bhattacharyya bound 2 is calculated as: 2 = �lP1P2 f �p1(X)p2(X)dX (1) When both density functions are Gaussian with mean µi and covariance matrix Σi, integration of 2 leads to a closed-form expression for 2: � 2 = P1P2e�µ(1/2) (2) where µ(1/2) = 8(µ2 − µ1)TrΣ1 2 Σ21� (µ2 − µ1) 1 |Σ1+Σ2 2 | + 2ln (3) �|Σ1||Σ2| is referred to as the Bhattacharyya distance. 3 In order to estimate the stability of an acoustic model, the training data fo</context>
</contexts>
<marker>Fukunaga, 1990</marker>
<rawString>K. Fukunaga. 1990. Introduction to Statistical Pattern Recognition. Academic Press, Inc., 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pali Lehohla</author>
</authors>
<title>Census 2001: Census in brief. Statistics South Africa.</title>
<date>2003</date>
<contexts>
<context position="6094" citStr="Lehohla, 2003" startWordPosition="953" endWordPosition="954">icial languages in South Africa (SA). These languages are: (1) isiZulu (zul1) and isiXhosa (xho), the two Nguni languages most widely spoken in SA. Together these form the home language of 41% of the SA population. (2) The three Sotho languages: Sepedi (nso), Setswana (tsn), Sesotho (sot), together the home language of 26% of the SA population. (3) The two Nguni languages less widely spoken in SA: siSwati (ssw) and isiNdebele (nbl), together the home language of 4% of the SA population. (4) Xitsonga (tso) and Tshivenda (ven), the home languages of 4% and 2% of the SA population, respectively (Lehohla, 2003). (The other two official languages of South Africa are Germanic languages, namely English (eng) and Afrikaans (afr).) For all these languages, new pronunciation dic1After each language name, the ISO 639-3:2007 language code is provided in brackets. tionaries, text and speech corpora are being developed. ASR speech corpora consist of approximately 200 speakers per language, producing read and elicited speech, recorded over a telephone channel. Each speaker produced approximately 30 utterances, 16 of these were randomly selected from a phonetically balanced corpus and the remainder consist of s</context>
</contexts>
<marker>Lehohla, 2003</marker>
<rawString>Pali Lehohla. 2003. Census 2001: Census in brief. Statistics South Africa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meraka-Institute</author>
</authors>
<title>Lwazi ASR corpus.</title>
<date>2009</date>
<note>Online: http://www.meraka.org.za/lwazi.</note>
<contexts>
<context position="5384" citStr="Meraka-Institute, 2009" startWordPosition="834" endWordPosition="835">languages of South Africa, where, for the majority of these, no prior language technology components were available. The core linguistic resources being developed include phoneme sets, electronic pronunciation dictionaries and the speech and text corpora required to develop automated speech recognition (ASR) and text-to-speech (TTS) systems for all eleven official languages of South Africa. The usability of these resources will be demonstrated during a national pilot planned for the third quarter of 2009. All outputs from the project are being released as open source software and open content(Meraka-Institute, 2009). Resources are being developed for all nine Southern Bantu languages that are recognised as official languages in South Africa (SA). These languages are: (1) isiZulu (zul1) and isiXhosa (xho), the two Nguni languages most widely spoken in SA. Together these form the home language of 41% of the SA population. (2) The three Sotho languages: Sepedi (nso), Setswana (tsn), Sesotho (sot), together the home language of 26% of the SA population. (3) The two Nguni languages less widely spoken in SA: siSwati (ssw) and isiNdebele (nbl), together the home language of 4% of the SA population. (4) Xitsonga</context>
</contexts>
<marker>Meraka-Institute, 2009</marker>
<rawString>Meraka-Institute. 2009. Lwazi ASR corpus. Online: http://www.meraka.org.za/lwazi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Morales</author>
<author>J Tejedor</author>
<author>J Garrido</author>
<author>J Colas</author>
<author>D T Toledano</author>
</authors>
<title>STC-TIMIT: Generation of a single-channel telephone corpus.</title>
<date>2008</date>
<booktitle>In LREC,</booktitle>
<pages>391--395</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="21185" citStr="Morales et al., 2008" startWordPosition="3375" endWordPosition="3378">dels. Each model had 3 emitting states with 7 mixtures per state. 39 features are used: 13 MFCCs together with their first and second order derivatives. Cepstral Mean Normalisation (CMN) as well as Cepstral Variance Normalisation (CMV) are used to perform speakerindependent normalisation. A diagonal covariance matrix is used; to partially compensate for this incorrect assumption of feature independence semitied transforms are applied. A flat phone-based language model is employed throughout. As a rough benchmark of acceptable phonemerecognition accuracy, recently reported results obtained by (Morales et al., 2008) on a similar-sized telephone corpus in American English (N-TIMIT) are also shown in Tab. 2. We see that the Lwazi results compare very well with this benchmark. An important issue in ASR corpus design is 6 Figure 3: Effective distances in terms of the mean of the Bhattacharyya bound between a single phoneme (/n/-nbl top and /a/-nbl bottom) and each of its closest matches within the set ofphonemes investigated. Language % corr % acc avg # total # phons speakers isiNdebele 74.21 65.41 28.66 200 isiXhosa 69.25 57.24 17.79 210 isiZulu 71.18 60.95 23.42 201 Tshivenda 76.37 66.78 19.53 201 Sepedi 6</context>
</contexts>
<marker>Morales, Tejedor, Garrido, Colas, Toledano, 2008</marker>
<rawString>N. Morales, J. Tejedor, J. Garrido, J. Colas, and D.T. Toledano. 2008. STC-TIMIT: Generation of a single-channel telephone corpus. In LREC, pages 391–395, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nagroski</author>
<author>L Boves</author>
<author>H Steeneken</author>
</authors>
<title>In search of optimal data selection for training of automatic speech recognition systems. ASRU workshop,</title>
<date>2003</date>
<pages>67--72</pages>
<contexts>
<context position="10440" citStr="Nagroski et al., 2003" startWordPosition="1616" endWordPosition="1619">rpus matches the intended operating environment as accurately as possible. Three directions are primarily employed: (1) explicit specification of phonotactic, speaker and channel variability during corpus development, (2) automated selection of informative subsets of data from larger corpora, with the smaller subset yielding comparable results, and (3) the use of active learning to optimise existing speech recognition systems. All three techniques provide a perspective on the sources of variation inherent in a speech corpus, and the effect of this variation on speech recognition accuracy. In (Nagroski et al., 2003), Principle Component Analysis (PCA) is used to cluster data acoustically. These clusters then serve as a starting point for selecting the optimal utterances from a training database. As a consequence of the clustering technique, it is possible to characterise some of the acoustic properties of the data being analysed, and to obtain an understanding of the major sources of variation, such as different speakers and genders (Riccardi and Hakkani-Tur, 2003). Active and unsupervised learning methods can be combined to circumvent the need for transcribing massive amounts of data (Riccardi and Hakka</context>
</contexts>
<marker>Nagroski, Boves, Steeneken, 2003</marker>
<rawString>A. Nagroski, L. Boves, and H. Steeneken. 2003. In search of optimal data selection for training of automatic speech recognition systems. ASRU workshop, pages 67–72, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nasfors</author>
</authors>
<title>Efficient voice information services for developing countries.</title>
<date>2007</date>
<tech>Master’s thesis,</tech>
<institution>Department of Information Technology, Uppsala University.</institution>
<contexts>
<context position="1748" citStr="Nasfors, 2007" startWordPosition="255" endWordPosition="256"> but telephone networks (especially cellular networks) are spreading rapidly. In addition, speechbased access to information may empower illiterate or semi-literate people, 98% of whom live in the developing world. Spoken dialog systems can play a useful role in a wide range of applications. Of particular importance in Africa are applications such as education, using speech-enabled learning software or kiosks and information dissemination through media such as telephone-based information systems. Significant benefits can be envisioned if information is provided in domains such as agriculture (Nasfors, 2007), health care (Sherwani et al., ; Sharma et al., 2009) and government services (Barnard et al., 2003). In order to make SDSs a reality in Africa, technology components such as text-to-speech (TTS) systems and automatic speech recognition (ASR) systems are required. The latter category of technologies is the focus of the current contribution. Speech recognition systems exist for only a handful of African languages (Roux et al., ; Seid and Gambck, 2005; Abdillahi et al., 2006), and to our knowledge no service available to the general public currently uses ASR in an indigenous African language. A</context>
</contexts>
<marker>Nasfors, 2007</marker>
<rawString>P. Nasfors. 2007. Efficient voice information services for developing countries. Master’s thesis, Department of Information Technology, Uppsala University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Niesler</author>
</authors>
<title>Language-dependent state clustering for multilingual acoustic modeling.</title>
<date>2007</date>
<journal>Speech Communication,</journal>
<pages>49--453</pages>
<contexts>
<context position="9355" citStr="Niesler, 2007" startWordPosition="1452" endWordPosition="1453"> data that exists in the target language. Various data sharing techniques for language-dependant acoustic modelling have been studied, including crosslanguage transfer, data pooling, language adaptation and bootstrapping (Wheatley et al., 1994; Schultz and Waibel, 2001; Byrne et al., 2000). Both (Wheatley et al., 1994) and (Schultz and Waibel, 2001) found that useful gains could be obtained by sharing data across languages with the size of the benefit dependent on the similarity of the sound systems of the languages combined. In the only cross-lingual adaptation study using African languages (Niesler, 2007), similar gains have not yet been observed. 3.2 ASR corpus design Corpus design techniques for ASR are generally aimed at specifying or selecting the most appropriate subset of data from a larger domain in order to optimise recognition accuracy, often while explicitly minimising the size of the selected corpus. This is achieved through various techniques that aim to include as much variability in the data as possible, while simultaneously ensuring that the corpus matches the intended operating environment as accurately as possible. Three directions are primarily employed: (1) explicit specific</context>
</contexts>
<marker>Niesler, 2007</marker>
<rawString>T. Niesler. 2007. Language-dependent state clustering for multilingual acoustic modeling. Speech Communication, 49:453–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>D Hakkani-Tur</author>
</authors>
<title>Active and unsupervised learning for automatic speech recognition.</title>
<date>2003</date>
<booktitle>In Eurospeech,</booktitle>
<pages>1825--1828</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="10898" citStr="Riccardi and Hakkani-Tur, 2003" startWordPosition="1688" endWordPosition="1691">es provide a perspective on the sources of variation inherent in a speech corpus, and the effect of this variation on speech recognition accuracy. In (Nagroski et al., 2003), Principle Component Analysis (PCA) is used to cluster data acoustically. These clusters then serve as a starting point for selecting the optimal utterances from a training database. As a consequence of the clustering technique, it is possible to characterise some of the acoustic properties of the data being analysed, and to obtain an understanding of the major sources of variation, such as different speakers and genders (Riccardi and Hakkani-Tur, 2003). Active and unsupervised learning methods can be combined to circumvent the need for transcribing massive amounts of data (Riccardi and Hakkani-Tur, 2003). The most informative untranscribed data is selected for a human to label, based on acoustic evidence of a partially and iteratively trained ASR system. From such work, it soon becomes evident that the optimisation of the amount of variation inherent to training data is needed, since randomly selected additional data does not necessarily improve recognition accuracy. By focusing on the selection (based on existing transcriptions) of a unifo</context>
</contexts>
<marker>Riccardi, Hakkani-Tur, 2003</marker>
<rawString>G. Riccardi and D. Hakkani-Tur. 2003. Active and unsupervised learning for automatic speech recognition. In Eurospeech, pages 1825–1828, Geneva, Switzerland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J C Roux</author>
<author>E C Botha</author>
<author>J A du Preez</author>
</authors>
<title>Developing a multilingual telephone based information system in african languages. In</title>
<booktitle>LREC,</booktitle>
<pages>975--980</pages>
<location>Athens, Greece.</location>
<marker>Roux, Botha, Preez, </marker>
<rawString>J.C. Roux, E.C. Botha, and J.A. du Preez. Developing a multilingual telephone based information system in african languages. In LREC, pages 975–980, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Schultz</author>
<author>A Waibel</author>
</authors>
<title>Languageindependent and language-adaptive acoustic modeling for speech recognition.</title>
<date>2001</date>
<journal>Speech Communication,</journal>
<pages>35--31</pages>
<contexts>
<context position="9010" citStr="Schultz and Waibel, 2001" startWordPosition="1393" endWordPosition="1396">stic assistants with limited phonetic training (Davel and Barnard, 2004). Small audio corpora can be used efficiently by utilising techniques that share data across lan2 guages, either by developing multilingual ASR systems (a single system that simultaneously recognises different languages), or by using additional source data to supplement the training data that exists in the target language. Various data sharing techniques for language-dependant acoustic modelling have been studied, including crosslanguage transfer, data pooling, language adaptation and bootstrapping (Wheatley et al., 1994; Schultz and Waibel, 2001; Byrne et al., 2000). Both (Wheatley et al., 1994) and (Schultz and Waibel, 2001) found that useful gains could be obtained by sharing data across languages with the size of the benefit dependent on the similarity of the sound systems of the languages combined. In the only cross-lingual adaptation study using African languages (Niesler, 2007), similar gains have not yet been observed. 3.2 ASR corpus design Corpus design techniques for ASR are generally aimed at specifying or selecting the most appropriate subset of data from a larger domain in order to optimise recognition accuracy, often whi</context>
</contexts>
<marker>Schultz, Waibel, 2001</marker>
<rawString>T. Schultz and A. Waibel. 2001. Languageindependent and language-adaptive acoustic modeling for speech recognition. Speech Communication, 35:31–51, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hussien Seid</author>
<author>Bjrn Gambck</author>
</authors>
<title>A speaker independent continuous speech recognizer for Amharic. In Interspeech,</title>
<date>2005</date>
<pages>3349--3352</pages>
<location>Lisboa, Portugal,</location>
<contexts>
<context position="2202" citStr="Seid and Gambck, 2005" startWordPosition="328" endWordPosition="331">ugh media such as telephone-based information systems. Significant benefits can be envisioned if information is provided in domains such as agriculture (Nasfors, 2007), health care (Sherwani et al., ; Sharma et al., 2009) and government services (Barnard et al., 2003). In order to make SDSs a reality in Africa, technology components such as text-to-speech (TTS) systems and automatic speech recognition (ASR) systems are required. The latter category of technologies is the focus of the current contribution. Speech recognition systems exist for only a handful of African languages (Roux et al., ; Seid and Gambck, 2005; Abdillahi et al., 2006), and to our knowledge no service available to the general public currently uses ASR in an indigenous African language. A significant reason for this state of affairs is the lack of sufficient linguistic resources in the African languages. Most importantly, modern speech recognition systems use statistical models which are trained on corpora of relevant speech (i.e. appropriate for the recognition task in terms of the language used, the profile of the speakers, speaking style, etc.) This speech generally needs to be curated and transcribed prior to the development of A</context>
</contexts>
<marker>Seid, Gambck, 2005</marker>
<rawString>Hussien Seid and Bjrn Gambck. 2005. A speaker independent continuous speech recognizer for Amharic. In Interspeech, pages 3349–3352, Lisboa, Portugal, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sharma</author>
<author>M Plauche</author>
<author>C Kuun</author>
<author>E Barnard</author>
</authors>
<title>HIV health information access using spoken dialogue systems: Touchtone vs. speech.</title>
<date>2009</date>
<booktitle>Accepted at IEEE Int. Conf. on ICTD.</booktitle>
<contexts>
<context position="1802" citStr="Sharma et al., 2009" startWordPosition="263" endWordPosition="266">works) are spreading rapidly. In addition, speechbased access to information may empower illiterate or semi-literate people, 98% of whom live in the developing world. Spoken dialog systems can play a useful role in a wide range of applications. Of particular importance in Africa are applications such as education, using speech-enabled learning software or kiosks and information dissemination through media such as telephone-based information systems. Significant benefits can be envisioned if information is provided in domains such as agriculture (Nasfors, 2007), health care (Sherwani et al., ; Sharma et al., 2009) and government services (Barnard et al., 2003). In order to make SDSs a reality in Africa, technology components such as text-to-speech (TTS) systems and automatic speech recognition (ASR) systems are required. The latter category of technologies is the focus of the current contribution. Speech recognition systems exist for only a handful of African languages (Roux et al., ; Seid and Gambck, 2005; Abdillahi et al., 2006), and to our knowledge no service available to the general public currently uses ASR in an indigenous African language. A significant reason for this state of affairs is the l</context>
</contexts>
<marker>Sharma, Plauche, Kuun, Barnard, 2009</marker>
<rawString>A. Sharma, M. Plauche, C. Kuun, and E. Barnard. 2009. HIV health information access using spoken dialogue systems: Touchtone vs. speech. Accepted at IEEE Int. Conf. on ICTD.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Sherwani</author>
<author>N Ali</author>
<author>S Mirza</author>
<author>A Fatma</author>
<author>Y Memon</author>
<author>M Karim</author>
<author>R Tongia</author>
<author>R Rosenfeld</author>
</authors>
<title>Healthline: Speech-based access to health information by lowliterate users.</title>
<booktitle>In IEEE Int. Conf. on ICTD,</booktitle>
<pages>131--139</pages>
<marker>Sherwani, Ali, Mirza, Fatma, Memon, Karim, Tongia, Rosenfeld, </marker>
<rawString>J. Sherwani, N. Ali, S. Mirza, A. Fatma, Y. Memon, M. Karim, R. Tongia, and R. Rosenfeld. Healthline: Speech-based access to health information by lowliterate users. In IEEE Int. Conf. on ICTD, pages 131–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tucker</author>
<author>K Shalonova</author>
</authors>
<title>The Local Language Speech Technology Initiative.</title>
<date>2004</date>
<booktitle>In SCALLA Conf.,</booktitle>
<location>Nepal.</location>
<contexts>
<context position="1004" citStr="Tucker and Shalonova, 2004" startWordPosition="142" endWordPosition="145"> new telephone speech corpus which includes data from nine Southern Bantu languages. Because of practical constraints, the amount of speech per language is relatively small compared to major corpora in world languages, and we report on our investigation of the stability of the ASR models derived from the corpus. We also report on phoneme distance measures across languages, and describe initial phone recognisers that were developed using this data. 1 Introduction There is a widespread belief that spoken dialog systems (SDSs) will have a significant impact in the developing countries of Africa (Tucker and Shalonova, 2004), where the availability of alternative information sources is often low. Traditional computer infrastructure is scarce in Africa, but telephone networks (especially cellular networks) are spreading rapidly. In addition, speechbased access to information may empower illiterate or semi-literate people, 98% of whom live in the developing world. Spoken dialog systems can play a useful role in a wide range of applications. Of particular importance in Africa are applications such as education, using speech-enabled learning software or kiosks and information dissemination through media such as telep</context>
</contexts>
<marker>Tucker, Shalonova, 2004</marker>
<rawString>R. Tucker and K. Shalonova. 2004. The Local Language Speech Technology Initiative. In SCALLA Conf., Nepal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wheatley</author>
<author>K Kondo</author>
<author>W Anderson</author>
<author>Y Muthusumy</author>
</authors>
<title>An evaluation of crosslanguage adaptation for rapid HMM development in a new language. In</title>
<date>1994</date>
<booktitle>ICASSP,</booktitle>
<pages>237--240</pages>
<location>Adelaide.</location>
<contexts>
<context position="8984" citStr="Wheatley et al., 1994" startWordPosition="1389" endWordPosition="1392">n when utilising linguistic assistants with limited phonetic training (Davel and Barnard, 2004). Small audio corpora can be used efficiently by utilising techniques that share data across lan2 guages, either by developing multilingual ASR systems (a single system that simultaneously recognises different languages), or by using additional source data to supplement the training data that exists in the target language. Various data sharing techniques for language-dependant acoustic modelling have been studied, including crosslanguage transfer, data pooling, language adaptation and bootstrapping (Wheatley et al., 1994; Schultz and Waibel, 2001; Byrne et al., 2000). Both (Wheatley et al., 1994) and (Schultz and Waibel, 2001) found that useful gains could be obtained by sharing data across languages with the size of the benefit dependent on the similarity of the sound systems of the languages combined. In the only cross-lingual adaptation study using African languages (Niesler, 2007), similar gains have not yet been observed. 3.2 ASR corpus design Corpus design techniques for ASR are generally aimed at specifying or selecting the most appropriate subset of data from a larger domain in order to optimise recog</context>
<context position="22231" citStr="Wheatley et al., 1994" startWordPosition="3552" endWordPosition="3555">% corr % acc avg # total # phons speakers isiNdebele 74.21 65.41 28.66 200 isiXhosa 69.25 57.24 17.79 210 isiZulu 71.18 60.95 23.42 201 Tshivenda 76.37 66.78 19.53 201 Sepedi 66.44 55.19 16.45 199 Sesotho 68.17 54.79 18.57 200 Setswana 69.00 56.19 20.85 207 siSwati 74.19 64.46 30.66 208 Xitsonga 70.32 59.41 14.35 199 N-TIMIT 64.07 55.73 - - Table 2: Initial results for South African ASR systems. The column labelled “avg # phonemes” lists the average number of phoneme occurrences for each phoneme for each speaker. the trade-off between the number of speakers and the amount of data per speaker (Wheatley et al., 1994). The figures in Sec. 4.2 are not conclusive on this trade-off, so we have also investigated the effect of reducing either the number of speakers or the amount of data per speaker when training the isiZulu and Tshivenda recognisers. As shown in Fig. 4, the impact of both forms of reduction is comparable across languages and different degrees of reduction, in agreement with the results of Sec. 4.2. These results indicate that we now have a firm Figure 4: The influence of a reduction in training corpus size on phone recognition accuracy. baseline to investigate data-efficient training methods su</context>
</contexts>
<marker>Wheatley, Kondo, Anderson, Muthusumy, 1994</marker>
<rawString>B. Wheatley, K. Kondo, W. Anderson, and Y. Muthusumy. 1994. An evaluation of crosslanguage adaptation for rapid HMM development in a new language. In ICASSP, pages 237–240, Adelaide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wu</author>
<author>R Zhang</author>
<author>A Rudnicky</author>
</authors>
<title>Data selection for speech recognition. ASRU workshop,</title>
<date>2007</date>
<pages>562--565</pages>
<contexts>
<context position="11615" citStr="Wu et al., 2007" startWordPosition="1802" endWordPosition="1805">ssive amounts of data (Riccardi and Hakkani-Tur, 2003). The most informative untranscribed data is selected for a human to label, based on acoustic evidence of a partially and iteratively trained ASR system. From such work, it soon becomes evident that the optimisation of the amount of variation inherent to training data is needed, since randomly selected additional data does not necessarily improve recognition accuracy. By focusing on the selection (based on existing transcriptions) of a uniform distribution across different speech units such as words and phonemes, improvements are obtained (Wu et al., 2007). In our focus on resource-scarce languages, the main aim is to understand the amount of data that needs to be collected in order to achieve acceptable accuracy. This is achieved through the use of analytic measures of data variability, which we describe next. 3.3 Evaluating phoneme stability In (Badenhorst and Davel, 2008) a technique is developed that estimates how stable a specific phoneme is, given a specific set of training data. This statistical measure provides an indication of the effect that additional training data will have on recognition accuracy: the higher the stability, the less</context>
</contexts>
<marker>Wu, Zhang, Rudnicky, 2007</marker>
<rawString>Y. Wu, R. Zhang, and A. Rudnicky. 2007. Data selection for speech recognition. ASRU workshop, pages 562–565, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zerbian</author>
<author>E Barnard</author>
</authors>
<title>Phonetics of intonation in South African Bantu languages.</title>
<date>2008</date>
<journal>Southern African Linguistics and Applied Language Studies,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="20110" citStr="Zerbian and Barnard, 2008" startWordPosition="3216" endWordPosition="3219">are tone languages, our systems do not encode tonal 5 Figure 2: Effect of number of speakers on mean of Bhattacharyya bound for different phoneme groups using 20 utterances per speaker Language total # # speech # distinct minutes minutes phonemes isiNdebele 564 465 46 isiXhosa 470 370 52 isiZulu 525 407 46 Tshivenda 354 286 38 Sepedi 394 301 45 Sesotho 387 313 44 Setswana 379 295 34 siSwati 603 479 39 Xitsonga 378 316 54 N-TIMIT 315 - 39 Table 1: A summary of the Lwazi ASR corpus: Bantu languages. information, since tone is unlikely to be important for small-to-medium vocabulary applications (Zerbian and Barnard, 2008). As the initial pronunciation dictionaries were developed to provide good coverage of the language in general, these dictionaries did not cover the entire ASR corpus. Grapheme-to-phoneme rules are therefore extracted from the general dictionaries using the Default&amp;Refine algorithm (Davel and Barnard, 2008) and used to generate missing pronunciations. We use HTK 3.4 to build a context-dependent cross-word HMM-based phoneme recogniser with triphone models. Each model had 3 emitting states with 7 mixtures per state. 39 features are used: 13 MFCCs together with their first and second order deriva</context>
</contexts>
<marker>Zerbian, Barnard, 2008</marker>
<rawString>S. Zerbian and E. Barnard. 2008. Phonetics of intonation in South African Bantu languages. Southern African Linguistics and Applied Language Studies, 26(2):235–254.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>