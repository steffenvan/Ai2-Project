<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993623">
Real-Word Spelling Correction using Google Web 1T 3-grams
</title>
<author confidence="0.990187">
Aminul Islam
</author>
<affiliation confidence="0.997964">
Department of Computer Science
University of Ottawa
</affiliation>
<address confidence="0.941221">
Ottawa, ON, K1N 6N5, Canada
</address>
<email confidence="0.996624">
mdislam@site.uottawa.ca
</email>
<author confidence="0.983755">
Diana Inkpen
</author>
<affiliation confidence="0.99795">
Department of Computer Science
University of Ottawa
</affiliation>
<address confidence="0.940445">
Ottawa, ON, K1N 6N5, Canada
</address>
<email confidence="0.995998">
diana@site.uottawa.ca
</email>
<sectionHeader confidence="0.99471" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998726875">
We present a method for detecting and
correcting multiple real-word spelling er-
rors using the Google Web 1T 3-gram data
set and a normalized and modified ver-
sion of the Longest Common Subsequence
(LCS) string matching algorithm. Our
method is focused mainly on how to im-
prove the detection recall (the fraction of
errors correctly detected) and the correc-
tion recall (the fraction of errors correctly
amended), while keeping the respective
precisions (the fraction of detections or
amendments that are correct) as high as
possible. Evaluation results on a standard
data set show that our method outperforms
two other methods on the same task.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937068181818">
Real-word spelling errors are words in a text that
occur when a user mistakenly types a correctly
spelled word when another was intended. Errors
of this type may be caused by the writer’s igno-
rance of the correct spelling of the intended word
or by typing mistakes. Such errors generally go
unnoticed by most spellcheckers as they deal with
words in isolation, accepting them as correct if
they are found in the dictionary, and flagging them
as errors if they are not. This approach would
be sufficient to detect the non-word error myss in
“It doesn’t know what the myss is all about.” but
not the real-word error muss in “It doesn’t know
what the muss is all about.” To detect the latter,
the spell-checker needs to make use of the sur-
rounding context such as, in this case, to recog-
nise that fuss is more likely to occur than muss in
the context of all about. Ironically, errors of this
type may even be caused by spelling checkers in
the correction of non-word spelling errors when
the auto-correct feature in some word-processing
software sometimes silently change a non-word to
the wrong real word (Hirst and Budanitsky, 2005),
and sometimes when correcting a flagged error,
the user accidentally make a wrong selection from
the choices offered (Wilcox-O’Hearn et al., 2008).
An extensive review of real-word spelling cor-
rection is given in (Pedler, 2007; Hirst and Budan-
itsky, 2005) and the problem of spelling correction
more generally is reviewed in (Kukich, 1992).
The Google Web 1T data set (Brants and Franz,
2006), contributed by Google Inc., contains En-
glish word n-grams (from unigrams to 5-grams)
and their observed frequency counts calculated
over 1 trillion words from web page text col-
lected by Google in January 2006. The text was
tokenised following the Penn Treebank tokenisa-
tion, except that hyphenated words, dates, email
addresses and URLs are kept as single tokens.
The sentence boundaries are marked with two spe-
cial tokens &lt;S&gt; and &lt;/S&gt;. Words that occurred
fewer than 200 times were replaced with the spe-
cial token &lt;UNK&gt;. Table 1 shows the data sizes
of the Web 1T corpus. The n-grams themselves
</bodyText>
<tableCaption confidence="0.99046">
Table 1: Google Web 1T Data Sizes
</tableCaption>
<figure confidence="0.850874">
Number of Number Size on disk
(in KB)
Tokens 1,024,908,267,229 N/A
Sentences 95,119,665,584 N/A
Unigrams 13,588,391 185,569
Bigrams 314,843,401 5,213,440
Trigrams 977,069,902 19,978,540
4-grams 1,313,818,354 32,040,884
5-grams 1,176,470,663 33,678,504
</figure>
<bodyText confidence="0.528976333333333">
must appear at least 40 times to be included in the
Web 1T corpus1. It is expected that this data will
be useful for statistical language modeling, e.g.,
</bodyText>
<footnote confidence="0.990498">
1Details of the Google Web 1T data set can be found at
www.ldc.upenn.edu/Catalog/docs/LDC2006T13/readme.txt
</footnote>
<page confidence="0.831256">
1241
</page>
<note confidence="0.9966405">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1241–1249,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999389166666667">
for machine translation or speech recognition, as
well as for other uses.
In this paper, we present a method for detecting
and correcting multiple real-word spelling errors
using the Google Web 1T 3-gram data set, and a
normalized and modified version of the Longest
Common Subsequence (LCS) string matching al-
gorithm (details are in section 3.1). By multiple er-
rors, we mean that if we have n words in the input
sentence, then we try to detect and correct at most
n-1 errors. We do not try to detect and correct an
error, if any, in the first word as it is not compu-
tationally feasible to search in the Google Web 1T
3-grams while keeping the first word in the 3-gram
as a variable. Our intention is to focus on how to
improve the detection recall (the fraction of errors
correctly detected) or correction recall (the frac-
tion of errors correctly amended) while maintain-
ing the respective precisions (the fraction of de-
tections or amendments that are correct) as high as
possible. The reason behind this intention is that if
the recall for any method is around 0.5, this means
that the method fails to detect or correct around 50
percent of the errors. As a result, we can not com-
pletely rely on these type of methods, for that we
need some type of human interventions or sugges-
tions to detect or correct the rest of the undetected
or uncorrected errors. Thus, if we have a method
that can detect or correct almost 80 percent of the
errors, even generating some extra candidates that
are incorrect is more helpful to the human.
This paper is organized as follow: Section 2
presents a brief overview of the related work. Our
proposed method is described in Section 3. Eval-
uation and experimental results are discussed in
Section 4. We conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999925666666667">
Work on real-word spelling correction can roughly
be classified into two basic categories: methods
based on semantic information or human-made
lexical resources, and methods based on machine
learning or probability information. Our proposed
method falls into the latter category.
</bodyText>
<subsectionHeader confidence="0.996718">
2.1 Methods Based on Semantic Information
</subsectionHeader>
<bodyText confidence="0.999947727272727">
The ‘semantic information’ approach first pro-
posed by Hirst and St-Onge (1998) and later devel-
oped by Hirst and Budanitsky (2005) detected se-
mantic anomalies, but was not restricted to check-
ing words from predefined confusion sets. This
approach was based on the observation that the
words that a writer intends are generally seman-
tically related to their surrounding words, whereas
some types of real-word spelling errors are not,
such as (using Hirst and Budanitsky’s example),
“It is my sincere hole (hope) that you will recover
swiftly.” Such “malapropisms” cause “a pertur-
bation of the cohesion (and coherence) of a text.”
Hirst and Budanitsky (2005) use semantic distance
measures in WordNet (Miller et al., 1993) to de-
tect words that are potentially anomalous in con-
text - that is, semantically distant from nearby
words; if a variation in spelling results in a word
that was semantically closer to the context, it is
hypothesized that the original word is an error (a
“malapropism”) and the closer word is its correc-
tion.
</bodyText>
<subsectionHeader confidence="0.99913">
2.2 Methods Based on Machine Learning
</subsectionHeader>
<bodyText confidence="0.999882173913043">
Machine learning methods are regarded as lexical
disambiguation tasks and confusion sets are used
to model the ambiguity between words. Normally,
the machine learning and statistical approaches
rely on pre-defined confusion sets, which are sets
(usually pairs) of commonly confounded words,
such as {their, there, they’re} and {principle, prin-
cipal}. The methods learn the characteristics of
typical context for each member of the set and de-
tect situations in which one member occurs in con-
text that is more typical of another. Such meth-
ods, therefore, are inherently limited to a set of
common, predefined errors, but such errors can in-
clude both content and function words. Given an
occurrence of one of its confusion set members,
the spellchecker’s job is to predict which mem-
ber of that confusion set is the most appropriate in
the context. Golding and Roth (1999), an exam-
ple of a machine-learning method, combined the
Winnow algorithm with weighted-majority voting,
using nearby and adjacent words as features. An-
other example of a machine-learning method is
that of Carlson et al. (2001).
</bodyText>
<subsectionHeader confidence="0.9823935">
2.3 Methods Based on Probability
Information
</subsectionHeader>
<bodyText confidence="0.999101285714286">
Mays et al. (1991) proposed a statistical method
using word-trigram probabilities for detecting and
correcting real-word errors without requiring pre-
defined confusion sets. In this method, if the
trigram-derived probability of an observed sen-
tence is lower than that of a sentence obtained by
replacing one of the words with a spelling varia-
</bodyText>
<page confidence="0.988993">
1242
</page>
<bodyText confidence="0.999961193548387">
tion, then we hypothesize that the original is an
error and the variation is what the user intended.
Wilcox-O’Hearn et al. (2008) analyze the ad-
vantages and limitations of Mays et al. (1991)’s
method, and present a new evaluation of the al-
gorithm, designed so that the results can be com-
pared with those of other methods, and then con-
struct and evaluate some variations of the algo-
rithm that use fixed-length windows. They con-
sider a variation of the method that optimizes over
relatively short, fixed-length windows instead of
over a whole sentence (except in the special case
when the sentence is smaller than the window),
while respecting sentence boundaries as natural
breakpoints. To check the spelling of a span of
d words requires a window of length d+4 to ac-
commodate all the trigrams that overlap with the
words in the span. The smallest possible window
is therefore 5 words long, which uses 3 trigrams
to optimize only its middle word. They assume
that the sentence is bracketed by two BoS and two
EoS markers (to accommodate trigrams involving
the first two and last two words of the sentence).
The window starts with its left-hand edge at the
first BoS marker, and the Mays et al. (1991)’s
method is run on the words covered by the tri-
grams that it contains; the window then moves d
words to the right and the process repeats until all
the words in the sentence have been checked. As
Mays et al. (1991)’s algorithm is run separately in
each window, potentially changing a word in each,
Wilcox-O’Hearn et al. (2008)’s method as a side-
effect also permits multiple corrections in a single
sentence.
Wilcox-O’Hearn et al. (2008) show that
the trigram-based real-word spelling-correction
method of Mays et al. (1991) is superior in per-
formance to the WordNet-based method of Hirst
and Budanitsky (2005), even on content words
(“malapropisms”), especially when supplied with
a realistically large trigram model. Wilcox-
O’Hearn et al. (2008) state that their attempts to
improve the method with smaller windows and
with multiple corrections per sentence were not
successful, because of excessive false positives.
Verberne (2002) proposed a trigram-based
method for real-word errors without explicitly us-
ing probabilities or even localizing the possible er-
ror to a specific word. This method simply as-
sumes that any word trigram in the text that is
attested in the British National Corpus (Burnard,
2000) is correct, and any unattested trigram is a
likely error. When an unattested trigram is ob-
served, the method then tries the spelling varia-
tions of all words in the trigram to find attested
trigrams to present to the user as possible correc-
tions. The evaluation of this method was carried
out on only 7100 words of the Wall Street Journal
corpus, with 31 errors introduced (i.e., one error
in every approximately 200 words) obtaining a re-
call of 0.33 for correction, a precision of 0.05 and
a F-measure of 0.086.
</bodyText>
<sectionHeader confidence="0.983324" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.9999906">
The proposed method first tries to determine some
probable candidates and then finds the best one
among the candidates or sorts them based on some
weights. We consider a string similarity function
and a normalized frequency value function in our
method. The following sections present a detailed
description of each of these functions followed by
the procedure to determine some probable candi-
dates along with the procedure to sort the candi-
dates.
</bodyText>
<subsectionHeader confidence="0.999691">
3.1 Similarity between Two Strings
</subsectionHeader>
<bodyText confidence="0.985209192307692">
We use the longest common subsequence (LCS)
(Allison and Dix, 1986) measure with some nor-
malization and small modifications for our string
similarity measure. We use the same three differ-
ent modified versions of LCS that we (Islam and
Inkpen, 2008) used, along with another modified
version of LCS, and then take a weighted sum of
these2. Kondrak (2005) showed that edit distance
and the length of the longest common subsequence
are special cases of n-gram distance and similarity,
respectively. Melamed (1999) normalized LCS by
dividing the length of the longest common subse-
quence by the length of the longer string and called
it longest common subsequence ratio (LCSR). But
LCSR does not take into account the length of the
shorter string which sometimes has a significant
impact on the similarity score.
Islam and Inkpen (2008) normalized the longest
common subsequence so that it takes into account
the length of both the shorter and the longer string
and called it normalized longest common subse-
2We (Islam and Inkpen, 2008) use modified versions be-
cause in our experiments we obtained better results (precision
and recall) for schema matching on a sample of data than
when using the original LCS, or other string similarity mea-
sures.
</bodyText>
<equation confidence="0.7927652">
1243
quence (NLCS) which is:
len(LCS(si, sj))2
v1 = NLCS(si, sj) = (1)
len(si) x len(sj)
</equation>
<bodyText confidence="0.992764545454545">
While in classical LCS, the common subse-
quence needs not be consecutive, in spelling cor-
rection, a consecutive common subsequence is im-
portant for a high degree of matching. We (Is-
lam and Inkpen, 2008) used maximal consecutive
longest common subsequence starting at charac-
ter 1, MCLCS1 and maximal consecutive longest
common subsequence starting at any character n,
MCLCSn. MCLCS1 takes two strings as input
and returns the shorter string or maximal consec-
utive portions of the shorter string that consecu-
tively match with the longer string, where match-
ing must be from first character (character 1) for
both strings. MCLCSn takes two strings as in-
put and returns the shorter string or maximal con-
secutive portions of the shorter string that con-
secutively match with the longer string, where
matching may start from any character (char-
acter n) for both of the strings. We normal-
ized MCLCS1 and MCLCSn and called it nor-
malized MCLCS1 (NMCLCS1) and normalized
MCLCSn (NMCLCSn), respectively.
</bodyText>
<equation confidence="0.997159833333333">
len(MCLCS1(si, sj))2
v2 =NMCLCS1(si,sj) = len(si) x len(sj)
(2)
len(MCLCSn(si, sj))2
v3 =NMCLCSn(si, sj) = len(si) x len(sj)
(3)
</equation>
<bodyText confidence="0.999154066666667">
Islam and Inkpen (2008) did not consider consecu-
tive common subsequences ending at the last char-
acter, though MCLCSn sometimes covers this,
but not always. We argue that the consecutive
common subsequence ending at the last character
is as significant as the consecutive common sub-
sequence starting at the first character. So, we
introduce the maximal consecutive longest com-
mon subsequence ending at the last character,
MCLCSz (Algorithm 1). Algorithm 1, takes two
strings as input and returns the shorter string or the
maximal consecutive portions of the shorter string
that consecutively matches with the longer string,
where matching must end at the last character for
both strings. We normalize MCLCSz and call it
</bodyText>
<equation confidence="0.9061345">
normalized MCLCSz (NMCLCSz).
len(MCLCSz(si, sj))2
v4 =NMCLCSz(si, sj) = len(si) x len(sj)
(4)
</equation>
<bodyText confidence="0.9994076">
We take the weighted sum of these individual
values v1, v2, v3, and v4 to determine string simi-
larity score, where α1, α2, α3, α4 are weights and
α1 + α2 + α3 + α4 = 1. Therefore, the similarity
of the two strings, S E [0, 1] is:
</bodyText>
<equation confidence="0.83155">
S(si, sj) = α1v1 + α2v2 + α3v3 + α4v4 (5)
</equation>
<bodyText confidence="0.804545666666667">
We heuristically set equal weights for our ex-
periments3. Theoretically, v3 &gt; v2 and v3 &gt; v4.
To give an example, consider si = albastru and
</bodyText>
<equation confidence="0.980534833333333">
sj = alabasteru, then
LCS(si, sj) = albastru
MCLCS1(si, sj) = al
MCLCSn(si, sj) = bast
MCLCSz(si, sj) = ru
NLCS(si, sj) = 82/(8 x 10) = 0.8
NMCLCS1(si, sj) = 22/(8 x 10) = 0.05
NMCLCSn(si, sj) = 42/(8 x 10) = 0.2
NMCLCSz(si, sj) = 22/(8 x 10) = 0.05
The string similarity, S = α1v1+α2v2+α3v3+
α4v4 = 0.25 x 0.8 + 0.25 x 0.05 + 0.25 x 0.2 +
0.25 x 0.05 = 0.275
</equation>
<subsectionHeader confidence="0.996224">
3.2 Normalized Frequency Value
</subsectionHeader>
<bodyText confidence="0.992063923076923">
We determine the normalized frequency value of
each candidate word for a single position with re-
spect to all other candidates for the same position.
If we find n replacements of a word wi which are
{wi1, wi2, · · · , wij, · · · , win}, and their frequen-
cies {fi1, fi2, · · · , fij, · · · , fin}, where fij is the
frequency of a 3-gram (where any candidate word
wij is a member of the 3-gram), then we determine
the normalized frequency value of any candidate
word wij, represented as F(wij) E (0, 1], as the
frequency of the 3-gram having wij over the maxi-
mum frequency among all the candidate words for
that position:
</bodyText>
<equation confidence="0.799581">
fij
F (wij) = max(fi1, fi2, ··· ,fij, ··· ,fin)
(6)
</equation>
<subsectionHeader confidence="0.995293">
3.3 Determining Candidate Words
</subsectionHeader>
<bodyText confidence="0.994042">
Our task is to correct real-word spelling error
from an input text using Google Web 1T 3-gram
data set. Let us consider an input text W which
</bodyText>
<footnote confidence="0.994045666666667">
3We use equal weights in several places in this paper in
order to keep the system unsupervised. If development data
would be available, we could adjust the weights.
</footnote>
<page confidence="0.961956">
1244
</page>
<table confidence="0.3826977">
Algorithm 1: MCLCSz ( Maximal Consec-
utive LCS ending at the last character)
input : si, sj /* si and sj are input
strings where Isil &lt; Isj� */
output: str /* str is the Maximal
Consecutive LCS ending at
the last character */
1 str +—NULL
2 c +— 1
3 while Isil &gt; c do
</table>
<listItem confidence="0.649543166666667">
4 x +— SubStr(si, −c, 1) /* returns
cth character of si from the
end */
5 y +— SubStr(sj, −c, 1) /* returns
cth character of sj from the
end */
</listItem>
<figure confidence="0.937903857142857">
6 if x = y then
7 str +— SubStr(si, −c, c)
8 else
9 return str
10 end
11 increment c
12 end
</figure>
<bodyText confidence="0.994193810810811">
after tokenization4 has m words, i.e., W =
{w1, w2, ... , wm}. Our method aims to correct
m-1 spelling errors, for all m-1 word positions,
except for the first word position, as we do not try
to correct the first word. We use a slight differ-
ent way to correct the first word (i.e., w2) and the
last word (i.e., wm) among those m-1 words, than
for the rest of the words. First, we discuss how
we find the candidates for a word (say wi, where
2&lt;i&lt;m) which is not either w2 or wm. Then, we
discuss the procedure to find the candidates for ei-
ther w2 or wm. Our method could have worked
for the first word too. We did not do it here due
4We need to tokenize the input sentence to make the 3-
grams formed using the tokens returned after the tokeniza-
tion consistent with the Google 3-grams. The input sentence
is tokenized in a manner similar to the tokenization of the
Wall Street Journal portion of the Penn Treebank. Notable
exceptions include the following:
- Hyphenated word are usually separated, and hyphen-
ated numbers usually form one token.
- Sequences of numbers separated by slashes (e.g., in
dates) form one token.
- Sequences that look like urls or email addresses form
one token.
to efficiency reasons. Google 3-grams are sorted
based on the first word, then the second word, and
so on. Based on this sorting, all Google 3-grams
are stored in 97 different files. All the 97 Google
3-gram files could have been needed to access a
single word, instead of accessing just one 3-gram
file as we do for any other words. This is because
when the first word needs to be corrected, it might
be in any file among those 97 Google 3-gram files.
No error appears in the first position among 1402
inserted malapropisms. The errors start appearing
from the second position till the last position.
</bodyText>
<subsectionHeader confidence="0.474998">
3.3.1 Determining Candidate Words for wi
</subsectionHeader>
<equation confidence="0.440183">
(2 &lt; i &lt; m)
</equation>
<bodyText confidence="0.950672352941176">
We use the following steps:
1. We define the term cut offfrequency for word
wi or word wi+1 as the frequency of the 3-
gram wi−1 wi wi+1 in the Google Web 1T 3-
grams, if the said 3-gram exists. Otherwise,
we set the cut offfrequency of wi as 0. The
intuition behind using the cut off frequency
is the fact that, if the word is misspelled,
then the correct one should have a higher fre-
quency than the misspelled one. Thus, using
the cut offfrequency, we isolate a large num-
ber of candidates that we do not need to pro-
cess.
2. We find all the 3-grams (where only wi
is changed while wi−1 and wi+1 are un-
changed) having frequency greater than the
cut off frequency of wi (determined in
step 1). Let us consider that we find
n replacements of wi which are R1 =
{wi1, wi2, · · · , win} and their frequencies
F1 = {fi1, fi2, · · · , fin} where fij is the fre-
quency of the 3-gram wi−1 wij wi+1.
3. We determine the cut offfrequency for word
wi−1 or word wi as the frequency of the 3-
gram wi−2 wi−1 wi in the Google Web 1T 3-
grams, if the said 3-gram exists. Otherwise,
we set the cut offfrequency of wi as 0.
4. We find all the 3-grams (where only wi
is changed while wi−2 and wi−1 are un-
changed) having frequency greater than the
cut off frequency of wi (determined in
step 3). Let us consider that we find
n replacements of wi which are R2 =
{wi1, wi2, · · · , win} and their frequencies
</bodyText>
<page confidence="0.775488">
1245
</page>
<bodyText confidence="0.64132">
F2 = {fi1, fi2, · · · , fin} where fij is the fre-
quency of the 3-gram wi−2 wi−1 wij.
</bodyText>
<listItem confidence="0.9144824">
5. For each wij ∈ R1, we calculate the string
similarity between wij and wi using equation
(5) and then assign a weight using the follow-
ing equation (7) only to the words that return
the string similarity value greater than 0.5.
</listItem>
<equation confidence="0.991834">
weight= βS(wi, wij)+(1−β)F(wij) (7)
</equation>
<bodyText confidence="0.88699">
Equation (7) is used to ensure a balanced
weight between the string similarity function
and the normalized frequency value function
where β refers to how much importance we
give to the string similarity function with re-
spect to the normalized frequency value func-
tion5.
6. For each wij ∈ R2, we calculate the string
similarity between wij and wi using equa-
tion (5), and then assign a weight using the
equation (7) only to the words that return the
string similarity value greater than 0.5.
7. We sort the words found in step 5 and in step
6 that were given weights, if any, in descend-
ing order by the assigned weights and keep
only one word as candidate word6.
</bodyText>
<subsectionHeader confidence="0.662695">
3.3.2 Determining Candidate Words for w2
</subsectionHeader>
<bodyText confidence="0.977334">
We use the following steps:
</bodyText>
<listItem confidence="0.980156833333333">
1. We determine the cut offfrequency for word
w2 as the frequency of the 3-gram w1 w2 w3
in the Google Web 1T 3-grams, if the said
3-gram exists. Otherwise, we set the cut off
frequency of w2 as 0.
2. We find all the 3-grams (where only w2 is
</listItem>
<bodyText confidence="0.81251175">
changed while w1 and w3 are unchanged)
having frequency greater than the cut offfre-
quency of w2 (determined in step 1). Let us
consider that we find n replacements of w2
which are R1 = {w21, w22, · · · , w2n}, and
their frequencies F1 = {f21, f22, · · · , f2n},
5We give more importance to string similarity function
with respect to frequency value function throughout the sec-
tion of ‘determining candidate words’ to have more candidate
words so that the chance of including the target word into the
set of candidate words gets higher. For this reason, we heuris-
tically set ,Q=0.85 in equation (7) instead of setting ,Q=0.5.
6Sometimes the top candidate word might be either a plu-
ral form or a past participle form of the original word. Or
even it might be a high frequency function word (e.g., the).
We omit these type of words from the candidacy.
where f2j is the frequency of the 3-gram w1
w2j w3.
3. For each w2j ∈ R1, we calculate the string
similarity between w2j and w2 using equa-
tion (5), and then assign a weight using the
following equation only to the words that re-
turn the string similarity value greater than
0.5.
</bodyText>
<equation confidence="0.924498">
weight = βS(w2, w2j) + (1 − β)F(w2j)
</equation>
<bodyText confidence="0.981166">
4. We sort the words found in step 3 that were
given weights, if any, in descending order by
the assigned weights and keep only one word
as candidate word.
</bodyText>
<subsectionHeader confidence="0.689543">
3.3.3 Determining Candidate Words for wm
</subsectionHeader>
<bodyText confidence="0.98873">
We use the following steps:
</bodyText>
<listItem confidence="0.8832894375">
1. We determine the cut offfrequency for word
wm as the frequency of the 3-gram wm−2
wm−1 wm in the Google Web 1T 3-grams,
if the said 3-gram exists. Otherwise, we set
the cut offfrequency of wm as 0.
2. We find all the 3-grams (where only wm
is changed while wm−2 and wm−1 are un-
changed) having frequency greater than the
cut off frequency of wm (determined in
step 1). Let us consider that we find
n replacements of wm which are R2 =
{wm1, wm2, · · · , wmn} and their frequencies
F2 = {fm1, fm2,··· , fmn}, where fmj is
the frequency of the 3-gram wm−2 wm−1
wmj.
3. For each wmj ∈ R2, we calculate the string
</listItem>
<bodyText confidence="0.8551402">
similarity between wmj and wm using equa-
tion (5) and then assign a weight using the
following equation only to the words that re-
turn the string similarity value greater than
0.5.
</bodyText>
<equation confidence="0.725466">
weight = βS(wm, wmj) + (1 − β)F(wmj)
</equation>
<bodyText confidence="0.8694475">
4. We sort the words found in step 3 that were
given weights, if any, in descending order by
the assigned weights and keep only one word
as the candidate word.
</bodyText>
<page confidence="0.992453">
1246
</page>
<sectionHeader confidence="0.975142" genericHeader="method">
4 Evaluation and Experimental Results
</sectionHeader>
<bodyText confidence="0.980278298245614">
We used as test data the same data that Wilcox-
O’Hearn et al. (2008) used in their evaluation of
Mays et al. (1991) method, which in turn was a
replication of the data used by Hirst and St-Onge
(1998) and Hirst and Budanitsky (2005) to evalu-
ate their methods.
The data consisted of 500 articles (approxi-
mately 300,000 words) from the 1987−89 Wall
Street Journal corpus, with all headings, identi-
fiers, and so on removed; that is, just a long stream
of text. It is assumed that this data contains no er-
rors; that is, the Wall Street Journal contains no
malapropisms or other typos. In fact, a few typos
(both non-word and real-word) were noticed dur-
ing the evaluation, but they were small in number
compared to the size of the text.
Malapropisms were randomly induced into this
text at a frequency of approximately one word in
200. Specifically, any word whose base form was
listed as a noun in WordNet (but regardless of
whether it was used as a noun in the text; there was
no syntactic analysis) was potentially replaced by
any spelling variation found in the lexicon of the
ispell spelling checker7. A spelling variation was
defined as any word with an edit distance of 1 from
the original word; that is, any single-character in-
sertion, deletion, or substitution, or the transposi-
tion of two characters, that results in another real
word. Thus, none of the induced malapropisms
were derived from closed-class words, and none
were formed by the insertion or deletion of an
apostrophe or by splitting a word. The data con-
tained 1402 inserted malapropisms.
Because it had earlier been used for evaluat-
ing Mays et al. (1991)’s trigram method, which
operates at the sentence level, the data set had
been divided into three parts, without regard
for article boundaries or text coherence: sen-
tences into which no malapropism had been in-
duced; the original versions of the sentences
that received malapropisms; and the malapropized
sentences. In addition, all instances of num-
bers of various kinds had been replaced by tags
such as &lt;INTEGER&gt;, &lt;DOLLAR VALUE&gt;,
7Ispell is a fast screen-oriented spelling checker that
shows you your errors in the context of the original file, and
suggests possible corrections when it can figure them out.
The original was written in PDP-10 assembly in 1971, by
R. E. Gorin. The C version was written by Pace Willisson
of MIT. Geoff Kuenning added the international support and
created the current release.
and &lt;PERCENTAGE VALUE&gt;. Actual (ran-
dom) numbers or values were restored for these
tags. Some spacing anomalies around punctuation
marks were corrected. A detailed description of
this data can be found in (Hirst, 2008; Wilcox-
O’Hearn et al., 2008).
</bodyText>
<sectionHeader confidence="0.698115" genericHeader="method">
SUCCESSFUL CORRECTION:
</sectionHeader>
<bodyText confidence="0.999969222222222">
The Iran revelations were particularly disturbing
to the Europeans because they came on the heels
of the Reykjavik summit between President Rea-
gan and Soviet reader —* leader [leader] Mikhail
Gorbachev.
Even the now sainted Abraham Lincoln was of-
ten reviled while in officer —* office [office], some-
times painted by cartoonists and editorial writers
as that baboon in the White House.
</bodyText>
<listItem confidence="0.743346333333333">
FALSE POSITIVE:
· · · by such public displays of interest in Latinos
—* Latin [Latinos], many undocumented · · ·
</listItem>
<bodyText confidence="0.997918333333333">
The southeast Asian nation was one reported
contributor —* contribution [contributor] to the
Nicaraguans.
</bodyText>
<sectionHeader confidence="0.806094" genericHeader="method">
FALSE NEGATIVE:
</sectionHeader>
<bodyText confidence="0.950642615384616">
Kevin Mack, Geldermann president and chief
executive officer, didn’t return calls for comment
on the Clayton purchaser [purchase].
U.S. manufactures [manufacturers], in short,
again are confronting a ball game in which they
will be able to play.
TRUE POSITIVE DETECTION, FALSE POSI-
TIVE CORRECTION:
Hawkeye also is known to rear —* reader [fear]
that a bankruptcy-law filing by the parent com-
pany, which theoretically shouldn’t affect the op-
erations of its member banks, would spark runs on
the banks that could drag down the whole entity.
The London Daily News has quoted sources
saying as many as 23 British mercenaries were en-
listed by KMS to lid —* slide [aid] the Contras.
Table 2: Examples of successful and unsuccessful
corrections. Italics indicate observed word, arrow
indicates correction, square brackets indicate in-
tended word.
Some examples of successful and unsuccessful
corrections using our proposed method are shown
in Table 2.
Table 3 shows our method’s results on the de-
scribed data set compared with the results for the
trigram method of Wilcox-O’Hearn et al. (2008)
</bodyText>
<page confidence="0.978754">
1247
</page>
<table confidence="0.8829605">
Detection correction
R P F1 R P F1
Lexical cohesion
(Hirst and Budanitsky, 2005)
0.306 0.225 0.260 0.281 0.207 0.238
Trigrams
(Wilcox-O’Hearn et al., 2008)
0.544 0.528 0.536 0.491 0.503 0.497
Multiple 3-grams
0.890 0.445 0.593 0.763 0.381 0.508
</table>
<tableCaption confidence="0.996972">
Table 3: A comparison of recall, precision, and F1
</tableCaption>
<bodyText confidence="0.980117789473684">
score for three methods of malapropism detection
and correction on the same data set.
and the lexical cohesion method of Hirst and Bu-
danitsky (2005). The data shown here for tri-
gram method are not from (Wilcox-O’Hearn et al.,
2008), but rather are later results following some
corrections reported in (Hirst, 2008). We have not
tried optimizing our adjustable parameters: Q and
αs, because the whole data set was used as test-
ing set by the other methods we compare with. To
keep the comparison consistent, we did not use
any portion of the data set for training purpose.
Having optimized parameters could lead to a bet-
ter result. The performance is measured using Re-
call (R), Precision (P) and F1:
true positives
R = true positives + false negatives
true positives
P = true positives + false positives
</bodyText>
<equation confidence="0.9413915">
2PR
F1 = P + R
</equation>
<bodyText confidence="0.999989689655172">
The fraction of errors correctly detected is the de-
tection recall and the fraction of detections that
are correct is the detection precision. Again, the
fraction of errors correctly amended is the correc-
tion recall and the fraction of amendments that
are correct is the correction precision. To give
an example, consider a sentence from the data set:
“The Philippine president, in her commencement
address at the academy, complained that the U.S.
was living → giving [giving] advice instead of the
aid → said [aid] it pledged.”, where italics indi-
cate the observed word, arrow indicates the correc-
tion and the square brackets indicate the intended
word. The detection recall of this sentence is 1.0
and the precision is 0.5. The correction recall of
this sentence is 1.0 and the precision is 0.5. For
both cases, the F1 score is 0.667.
We loose some precision because our method
tries to detect and correct errors for all the words
(except the first word) in the input sentence, and,
as a result, it generates more false positives than
the other methods. Even so, we get better F1
scores than the other competing methods. Ac-
cepting 8.3 percents extra incorrect detections, we
get 34.6 percents extra correct detections of errors,
and similarly, accepting 12.2 percents extra incor-
rect amendments, we get 27.2 percents extra cor-
rect amendments of errors compared with the tri-
grams method (Wilcox-O’Hearn et al., 2008)8.
</bodyText>
<sectionHeader confidence="0.999504" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999997608695652">
The Google 3-grams proved to be very useful in
detecting real-word errors, and finding the correc-
tions. We did not use the 4-grams and 5-grams
because of data sparsity. When we tried with 5-
grams the results were lower than the ones pre-
sented in Section 4. Having sacrificed a bit the
precision score, our proposed method achieves a
very good detection recall (0.89) and correction
recall (0.76). Our attempts to improve the detec-
tion recall or correction recall, while maintaining
the respective precisions as high as possible are
helpful to the human correctors who post-edit the
output of the real-word spell checker. If there is
no postediting, at least more errors get corrected
automatically. Our method could also detect and
correct misspelled words, not only malapropisms,
without any modification. In future work, we plan
to extend our method to allow for deleted or in-
serted words, and to find the corrected strings in
the Google Web 1T n-grams. In this way we
will be able to correct grammar errors too. We
also plan more experiments using the 5-grams, but
backing off to 4-grams and 3-grams when needed.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999136">
This work is funded by the Natural Sciences and
Engineering Research Council of Canada. We
want to thank Professor Graeme Hirst from the
Department of Computer Science, University of
Toronto, for providing the evaluation data set.
</bodyText>
<footnote confidence="0.97095525">
8We can run our algorithm on subsets of data to check for
variance in the results. We cannot test statistical significance
compared to the related work (t-test), because we do not have
the system from related work to run it on subsets of the data.
</footnote>
<page confidence="0.9949">
1248
</page>
<sectionHeader confidence="0.993782" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946059701493">
L. Allison and T.I. Dix. 1986. A bit-string longest-
common-subsequence algorithm. Information Pro-
cessing Letters, 23:305–310.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram corpus version 1.1. Technical report, Google
Research.
Lou Burnard, 2000. Reference Guide for the
British National Corpus (World Edition), October.
www.natcorp.ox.ac.uk/docs/userManual/urg.pdf.
Andrew J. Carlson, Jeffrey Rosen, and Dan Roth.
2001. Scaling up context-sensitive text correction.
In Proceedings of the Thirteenth Conference on In-
novative Applications of Artificial Intelligence Con-
ference, pages 45–50. AAAI Press.
Andrew R. Golding and Dan Roth. 1999. A winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107–130.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lex-
ical cohesion. Natural Language Engineering,
11(1):87–111, March.
Graeme Hirst and David St-Onge, 1998. WordNet: An
electronic lexical database, chapter Lexical chains
as representations of context for the detection and
correction of malapropisms, pages 305–332. The
MIT Press, Cambridge, MA.
Graeme Hirst. 2008. An evaluation of the contextual
spelling checker of microsoft office word 2007, Jan-
uary. http://ftp.cs.toronto.edu/pub/gh/Hirst-2008-
Word.pdf.
Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2(2):1–25.
G. Kondrak. 2005. N-gram similarity and distance. In
Proceedings of the 12h International Conference on
String Processing and Information Retrieval, pages
115–126, Buenos Aires, Argentina.
Karen Kukich. 1992. Technique for automatically
correcting words in text. ACM Comput. Surv.,
24(4):377–439.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Informa-
tion Processing and Management, 27(5):517–522.
I. D. Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107–130.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K.J. Miller. 1993. Introduction to wordnet:
An on-line lexical database. Technical Report 43,
Cognitive Science Laboratory, Princeton University,
Princeton, NJ.
Jennifer Pedler. 2007. Computer Correction of Real-
word Spelling Errors in Dyslexic Text. Ph.D. thesis,
Birkbeck, London University.
Suzan Verberne. 2002. Context-sensitive spell check-
ing based on word trigram probabilities. Master’s
thesis, University of Nijmegen, February-August.
L. Amber Wilcox-O’Hearn, Graeme Hirst, and Alexan-
der Budanitsky. 2008. Real-word spelling correc-
tion with trigrams: A reconsideration of the mays,
damerau, and mercer model. In Alexander Gel-
bukh, editor, Proceedings, 9th International Con-
ference on Intelligent Text Processing and Compu-
tational Linguistics (CICLing-2008) (Lecture Notes
in Computer Science 4919, Springer-Verlag), pages
605–616, Haifa, February.
</reference>
<page confidence="0.995031">
1249
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493212">
<title confidence="0.780449">Real-Word Spelling Correction using Google Web 1T 3-grams</title>
<author confidence="0.725226">Aminul</author>
<affiliation confidence="0.999149">Department of Computer University of</affiliation>
<address confidence="0.801517">Ottawa, ON, K1N 6N5,</address>
<email confidence="0.982084">mdislam@site.uottawa.ca</email>
<author confidence="0.942087">Diana</author>
<affiliation confidence="0.9997705">Department of Computer University of</affiliation>
<address confidence="0.964315">Ottawa, ON, K1N 6N5,</address>
<email confidence="0.997145">diana@site.uottawa.ca</email>
<abstract confidence="0.999417764705882">We present a method for detecting and correcting multiple real-word spelling errors using the Google Web 1T 3-gram data set and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Our method is focused mainly on how to improve the detection recall (the fraction of errors correctly detected) and the correction recall (the fraction of errors correctly amended), while keeping the respective precisions (the fraction of detections or amendments that are correct) as high as possible. Evaluation results on a standard data set show that our method outperforms two other methods on the same task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Allison</author>
<author>T I Dix</author>
</authors>
<title>A bit-string longestcommon-subsequence algorithm.</title>
<date>1986</date>
<booktitle>Information Processing Letters,</booktitle>
<pages>23--305</pages>
<contexts>
<context position="11925" citStr="Allison and Dix, 1986" startWordPosition="1959" endWordPosition="1962">ection, a precision of 0.05 and a F-measure of 0.086. 3 Proposed Method The proposed method first tries to determine some probable candidates and then finds the best one among the candidates or sorts them based on some weights. We consider a string similarity function and a normalized frequency value function in our method. The following sections present a detailed description of each of these functions followed by the procedure to determine some probable candidates along with the procedure to sort the candidates. 3.1 Similarity between Two Strings We use the longest common subsequence (LCS) (Allison and Dix, 1986) measure with some normalization and small modifications for our string similarity measure. We use the same three different modified versions of LCS that we (Islam and Inkpen, 2008) used, along with another modified version of LCS, and then take a weighted sum of these2. Kondrak (2005) showed that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively. Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (L</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>L. Allison and T.I. Dix. 1986. A bit-string longestcommon-subsequence algorithm. Information Processing Letters, 23:305–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram corpus version 1.1.</title>
<date>2006</date>
<tech>Technical report, Google Research.</tech>
<contexts>
<context position="2482" citStr="Brants and Franz, 2006" startWordPosition="402" endWordPosition="405"> be caused by spelling checkers in the correction of non-word spelling errors when the auto-correct feature in some word-processing software sometimes silently change a non-word to the wrong real word (Hirst and Budanitsky, 2005), and sometimes when correcting a flagged error, the user accidentally make a wrong selection from the choices offered (Wilcox-O’Hearn et al., 2008). An extensive review of real-word spelling correction is given in (Pedler, 2007; Hirst and Budanitsky, 2005) and the problem of spelling correction more generally is reviewed in (Kukich, 1992). The Google Web 1T data set (Brants and Franz, 2006), contributed by Google Inc., contains English word n-grams (from unigrams to 5-grams) and their observed frequency counts calculated over 1 trillion words from web page text collected by Google in January 2006. The text was tokenised following the Penn Treebank tokenisation, except that hyphenated words, dates, email addresses and URLs are kept as single tokens. The sentence boundaries are marked with two special tokens &lt;S&gt; and &lt;/S&gt;. Words that occurred fewer than 200 times were replaced with the special token &lt;UNK&gt;. Table 1 shows the data sizes of the Web 1T corpus. The n-grams themselves Ta</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram corpus version 1.1. Technical report, Google Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>Reference Guide for the British National Corpus (World Edition),</title>
<date>2000</date>
<note>www.natcorp.ox.ac.uk/docs/userManual/urg.pdf.</note>
<contexts>
<context position="10844" citStr="Burnard, 2000" startWordPosition="1779" endWordPosition="1780">udanitsky (2005), even on content words (“malapropisms”), especially when supplied with a realistically large trigram model. WilcoxO’Hearn et al. (2008) state that their attempts to improve the method with smaller windows and with multiple corrections per sentence were not successful, because of excessive false positives. Verberne (2002) proposed a trigram-based method for real-word errors without explicitly using probabilities or even localizing the possible error to a specific word. This method simply assumes that any word trigram in the text that is attested in the British National Corpus (Burnard, 2000) is correct, and any unattested trigram is a likely error. When an unattested trigram is observed, the method then tries the spelling variations of all words in the trigram to find attested trigrams to present to the user as possible corrections. The evaluation of this method was carried out on only 7100 words of the Wall Street Journal corpus, with 31 errors introduced (i.e., one error in every approximately 200 words) obtaining a recall of 0.33 for correction, a precision of 0.05 and a F-measure of 0.086. 3 Proposed Method The proposed method first tries to determine some probable candidates</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard, 2000. Reference Guide for the British National Corpus (World Edition), October. www.natcorp.ox.ac.uk/docs/userManual/urg.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Carlson</author>
<author>Jeffrey Rosen</author>
<author>Dan Roth</author>
</authors>
<title>Scaling up context-sensitive text correction.</title>
<date>2001</date>
<booktitle>In Proceedings of the Thirteenth Conference on Innovative Applications of Artificial Intelligence Conference,</booktitle>
<pages>45--50</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="8044" citStr="Carlson et al. (2001)" startWordPosition="1315" endWordPosition="1318">ber occurs in context that is more typical of another. Such methods, therefore, are inherently limited to a set of common, predefined errors, but such errors can include both content and function words. Given an occurrence of one of its confusion set members, the spellchecker’s job is to predict which member of that confusion set is the most appropriate in the context. Golding and Roth (1999), an example of a machine-learning method, combined the Winnow algorithm with weighted-majority voting, using nearby and adjacent words as features. Another example of a machine-learning method is that of Carlson et al. (2001). 2.3 Methods Based on Probability Information Mays et al. (1991) proposed a statistical method using word-trigram probabilities for detecting and correcting real-word errors without requiring predefined confusion sets. In this method, if the trigram-derived probability of an observed sentence is lower than that of a sentence obtained by replacing one of the words with a spelling varia1242 tion, then we hypothesize that the original is an error and the variation is what the user intended. Wilcox-O’Hearn et al. (2008) analyze the advantages and limitations of Mays et al. (1991)’s method, and pr</context>
</contexts>
<marker>Carlson, Rosen, Roth, 2001</marker>
<rawString>Andrew J. Carlson, Jeffrey Rosen, and Dan Roth. 2001. Scaling up context-sensitive text correction. In Proceedings of the Thirteenth Conference on Innovative Applications of Artificial Intelligence Conference, pages 45–50. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Dan Roth</author>
</authors>
<title>A winnowbased approach to context-sensitive spelling correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="7818" citStr="Golding and Roth (1999)" startWordPosition="1280" endWordPosition="1283">(usually pairs) of commonly confounded words, such as {their, there, they’re} and {principle, principal}. The methods learn the characteristics of typical context for each member of the set and detect situations in which one member occurs in context that is more typical of another. Such methods, therefore, are inherently limited to a set of common, predefined errors, but such errors can include both content and function words. Given an occurrence of one of its confusion set members, the spellchecker’s job is to predict which member of that confusion set is the most appropriate in the context. Golding and Roth (1999), an example of a machine-learning method, combined the Winnow algorithm with weighted-majority voting, using nearby and adjacent words as features. Another example of a machine-learning method is that of Carlson et al. (2001). 2.3 Methods Based on Probability Information Mays et al. (1991) proposed a statistical method using word-trigram probabilities for detecting and correcting real-word errors without requiring predefined confusion sets. In this method, if the trigram-derived probability of an observed sentence is lower than that of a sentence obtained by replacing one of the words with a </context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>Andrew R. Golding and Dan Roth. 1999. A winnowbased approach to context-sensitive spelling correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Alexander Budanitsky</author>
</authors>
<title>Correcting real-word spelling errors by restoring lexical cohesion.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="2088" citStr="Hirst and Budanitsky, 2005" startWordPosition="339" endWordPosition="342">o detect the non-word error myss in “It doesn’t know what the myss is all about.” but not the real-word error muss in “It doesn’t know what the muss is all about.” To detect the latter, the spell-checker needs to make use of the surrounding context such as, in this case, to recognise that fuss is more likely to occur than muss in the context of all about. Ironically, errors of this type may even be caused by spelling checkers in the correction of non-word spelling errors when the auto-correct feature in some word-processing software sometimes silently change a non-word to the wrong real word (Hirst and Budanitsky, 2005), and sometimes when correcting a flagged error, the user accidentally make a wrong selection from the choices offered (Wilcox-O’Hearn et al., 2008). An extensive review of real-word spelling correction is given in (Pedler, 2007; Hirst and Budanitsky, 2005) and the problem of spelling correction more generally is reviewed in (Kukich, 1992). The Google Web 1T data set (Brants and Franz, 2006), contributed by Google Inc., contains English word n-grams (from unigrams to 5-grams) and their observed frequency counts calculated over 1 trillion words from web page text collected by Google in January </context>
<context position="6011" citStr="Hirst and Budanitsky (2005)" startWordPosition="984" endWordPosition="987">ew of the related work. Our proposed method is described in Section 3. Evaluation and experimental results are discussed in Section 4. We conclude in Section 5. 2 Related Work Work on real-word spelling correction can roughly be classified into two basic categories: methods based on semantic information or human-made lexical resources, and methods based on machine learning or probability information. Our proposed method falls into the latter category. 2.1 Methods Based on Semantic Information The ‘semantic information’ approach first proposed by Hirst and St-Onge (1998) and later developed by Hirst and Budanitsky (2005) detected semantic anomalies, but was not restricted to checking words from predefined confusion sets. This approach was based on the observation that the words that a writer intends are generally semantically related to their surrounding words, whereas some types of real-word spelling errors are not, such as (using Hirst and Budanitsky’s example), “It is my sincere hole (hope) that you will recover swiftly.” Such “malapropisms” cause “a perturbation of the cohesion (and coherence) of a text.” Hirst and Budanitsky (2005) use semantic distance measures in WordNet (Miller et al., 1993) to detect</context>
<context position="10246" citStr="Hirst and Budanitsky (2005)" startWordPosition="1686" endWordPosition="1689">ays et al. (1991)’s method is run on the words covered by the trigrams that it contains; the window then moves d words to the right and the process repeats until all the words in the sentence have been checked. As Mays et al. (1991)’s algorithm is run separately in each window, potentially changing a word in each, Wilcox-O’Hearn et al. (2008)’s method as a sideeffect also permits multiple corrections in a single sentence. Wilcox-O’Hearn et al. (2008) show that the trigram-based real-word spelling-correction method of Mays et al. (1991) is superior in performance to the WordNet-based method of Hirst and Budanitsky (2005), even on content words (“malapropisms”), especially when supplied with a realistically large trigram model. WilcoxO’Hearn et al. (2008) state that their attempts to improve the method with smaller windows and with multiple corrections per sentence were not successful, because of excessive false positives. Verberne (2002) proposed a trigram-based method for real-word errors without explicitly using probabilities or even localizing the possible error to a specific word. This method simply assumes that any word trigram in the text that is attested in the British National Corpus (Burnard, 2000) i</context>
<context position="24648" citStr="Hirst and Budanitsky (2005)" startWordPosition="4314" endWordPosition="4317">and wm using equation (5) and then assign a weight using the following equation only to the words that return the string similarity value greater than 0.5. weight = βS(wm, wmj) + (1 − β)F(wmj) 4. We sort the words found in step 3 that were given weights, if any, in descending order by the assigned weights and keep only one word as the candidate word. 1246 4 Evaluation and Experimental Results We used as test data the same data that WilcoxO’Hearn et al. (2008) used in their evaluation of Mays et al. (1991) method, which in turn was a replication of the data used by Hirst and St-Onge (1998) and Hirst and Budanitsky (2005) to evaluate their methods. The data consisted of 500 articles (approximately 300,000 words) from the 1987−89 Wall Street Journal corpus, with all headings, identifiers, and so on removed; that is, just a long stream of text. It is assumed that this data contains no errors; that is, the Wall Street Journal contains no malapropisms or other typos. In fact, a few typos (both non-word and real-word) were noticed during the evaluation, but they were small in number compared to the size of the text. Malapropisms were randomly induced into this text at a frequency of approximately one word in 200. S</context>
<context position="28909" citStr="Hirst and Budanitsky, 2005" startWordPosition="5014" endWordPosition="5017">ondon Daily News has quoted sources saying as many as 23 British mercenaries were enlisted by KMS to lid —* slide [aid] the Contras. Table 2: Examples of successful and unsuccessful corrections. Italics indicate observed word, arrow indicates correction, square brackets indicate intended word. Some examples of successful and unsuccessful corrections using our proposed method are shown in Table 2. Table 3 shows our method’s results on the described data set compared with the results for the trigram method of Wilcox-O’Hearn et al. (2008) 1247 Detection correction R P F1 R P F1 Lexical cohesion (Hirst and Budanitsky, 2005) 0.306 0.225 0.260 0.281 0.207 0.238 Trigrams (Wilcox-O’Hearn et al., 2008) 0.544 0.528 0.536 0.491 0.503 0.497 Multiple 3-grams 0.890 0.445 0.593 0.763 0.381 0.508 Table 3: A comparison of recall, precision, and F1 score for three methods of malapropism detection and correction on the same data set. and the lexical cohesion method of Hirst and Budanitsky (2005). The data shown here for trigram method are not from (Wilcox-O’Hearn et al., 2008), but rather are later results following some corrections reported in (Hirst, 2008). We have not tried optimizing our adjustable parameters: Q and αs, be</context>
</contexts>
<marker>Hirst, Budanitsky, 2005</marker>
<rawString>Graeme Hirst and Alexander Budanitsky. 2005. Correcting real-word spelling errors by restoring lexical cohesion. Natural Language Engineering, 11(1):87–111, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>WordNet: An electronic lexical database, chapter Lexical chains as representations of context for the detection and correction of malapropisms,</title>
<date>1998</date>
<pages>305--332</pages>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5960" citStr="Hirst and St-Onge (1998)" startWordPosition="975" endWordPosition="978">zed as follow: Section 2 presents a brief overview of the related work. Our proposed method is described in Section 3. Evaluation and experimental results are discussed in Section 4. We conclude in Section 5. 2 Related Work Work on real-word spelling correction can roughly be classified into two basic categories: methods based on semantic information or human-made lexical resources, and methods based on machine learning or probability information. Our proposed method falls into the latter category. 2.1 Methods Based on Semantic Information The ‘semantic information’ approach first proposed by Hirst and St-Onge (1998) and later developed by Hirst and Budanitsky (2005) detected semantic anomalies, but was not restricted to checking words from predefined confusion sets. This approach was based on the observation that the words that a writer intends are generally semantically related to their surrounding words, whereas some types of real-word spelling errors are not, such as (using Hirst and Budanitsky’s example), “It is my sincere hole (hope) that you will recover swiftly.” Such “malapropisms” cause “a perturbation of the cohesion (and coherence) of a text.” Hirst and Budanitsky (2005) use semantic distance </context>
<context position="24616" citStr="Hirst and St-Onge (1998)" startWordPosition="4309" endWordPosition="4312">tring similarity between wmj and wm using equation (5) and then assign a weight using the following equation only to the words that return the string similarity value greater than 0.5. weight = βS(wm, wmj) + (1 − β)F(wmj) 4. We sort the words found in step 3 that were given weights, if any, in descending order by the assigned weights and keep only one word as the candidate word. 1246 4 Evaluation and Experimental Results We used as test data the same data that WilcoxO’Hearn et al. (2008) used in their evaluation of Mays et al. (1991) method, which in turn was a replication of the data used by Hirst and St-Onge (1998) and Hirst and Budanitsky (2005) to evaluate their methods. The data consisted of 500 articles (approximately 300,000 words) from the 1987−89 Wall Street Journal corpus, with all headings, identifiers, and so on removed; that is, just a long stream of text. It is assumed that this data contains no errors; that is, the Wall Street Journal contains no malapropisms or other typos. In fact, a few typos (both non-word and real-word) were noticed during the evaluation, but they were small in number compared to the size of the text. Malapropisms were randomly induced into this text at a frequency of </context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge, 1998. WordNet: An electronic lexical database, chapter Lexical chains as representations of context for the detection and correction of malapropisms, pages 305–332. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>An evaluation of the contextual spelling checker of microsoft office word</title>
<date>2008</date>
<note>http://ftp.cs.toronto.edu/pub/gh/Hirst-2008-Word.pdf.</note>
<contexts>
<context position="27064" citStr="Hirst, 2008" startWordPosition="4721" endWordPosition="4722">&lt;DOLLAR VALUE&gt;, 7Ispell is a fast screen-oriented spelling checker that shows you your errors in the context of the original file, and suggests possible corrections when it can figure them out. The original was written in PDP-10 assembly in 1971, by R. E. Gorin. The C version was written by Pace Willisson of MIT. Geoff Kuenning added the international support and created the current release. and &lt;PERCENTAGE VALUE&gt;. Actual (random) numbers or values were restored for these tags. Some spacing anomalies around punctuation marks were corrected. A detailed description of this data can be found in (Hirst, 2008; WilcoxO’Hearn et al., 2008). SUCCESSFUL CORRECTION: The Iran revelations were particularly disturbing to the Europeans because they came on the heels of the Reykjavik summit between President Reagan and Soviet reader —* leader [leader] Mikhail Gorbachev. Even the now sainted Abraham Lincoln was often reviled while in officer —* office [office], sometimes painted by cartoonists and editorial writers as that baboon in the White House. FALSE POSITIVE: · · · by such public displays of interest in Latinos —* Latin [Latinos], many undocumented · · · The southeast Asian nation was one reported cont</context>
<context position="29439" citStr="Hirst, 2008" startWordPosition="5102" endWordPosition="5103">7 Detection correction R P F1 R P F1 Lexical cohesion (Hirst and Budanitsky, 2005) 0.306 0.225 0.260 0.281 0.207 0.238 Trigrams (Wilcox-O’Hearn et al., 2008) 0.544 0.528 0.536 0.491 0.503 0.497 Multiple 3-grams 0.890 0.445 0.593 0.763 0.381 0.508 Table 3: A comparison of recall, precision, and F1 score for three methods of malapropism detection and correction on the same data set. and the lexical cohesion method of Hirst and Budanitsky (2005). The data shown here for trigram method are not from (Wilcox-O’Hearn et al., 2008), but rather are later results following some corrections reported in (Hirst, 2008). We have not tried optimizing our adjustable parameters: Q and αs, because the whole data set was used as testing set by the other methods we compare with. To keep the comparison consistent, we did not use any portion of the data set for training purpose. Having optimized parameters could lead to a better result. The performance is measured using Recall (R), Precision (P) and F1: true positives R = true positives + false negatives true positives P = true positives + false positives 2PR F1 = P + R The fraction of errors correctly detected is the detection recall and the fraction of detections </context>
</contexts>
<marker>Hirst, 2008</marker>
<rawString>Graeme Hirst. 2008. An evaluation of the contextual spelling checker of microsoft office word 2007, January. http://ftp.cs.toronto.edu/pub/gh/Hirst-2008-Word.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="12106" citStr="Islam and Inkpen, 2008" startWordPosition="1989" endWordPosition="1992">candidates or sorts them based on some weights. We consider a string similarity function and a normalized frequency value function in our method. The following sections present a detailed description of each of these functions followed by the procedure to determine some probable candidates along with the procedure to sort the candidates. 3.1 Similarity between Two Strings We use the longest common subsequence (LCS) (Allison and Dix, 1986) measure with some normalization and small modifications for our string similarity measure. We use the same three different modified versions of LCS that we (Islam and Inkpen, 2008) used, along with another modified version of LCS, and then take a weighted sum of these2. Kondrak (2005) showed that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively. Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR). But LCSR does not take into account the length of the shorter string which sometimes has a significant impact on the similarity score. Islam and Inkpen (2008) normalized the l</context>
<context position="13389" citStr="Islam and Inkpen, 2008" startWordPosition="2202" endWordPosition="2206">length of both the shorter and the longer string and called it normalized longest common subse2We (Islam and Inkpen, 2008) use modified versions because in our experiments we obtained better results (precision and recall) for schema matching on a sample of data than when using the original LCS, or other string similarity measures. 1243 quence (NLCS) which is: len(LCS(si, sj))2 v1 = NLCS(si, sj) = (1) len(si) x len(sj) While in classical LCS, the common subsequence needs not be consecutive, in spelling correction, a consecutive common subsequence is important for a high degree of matching. We (Islam and Inkpen, 2008) used maximal consecutive longest common subsequence starting at character 1, MCLCS1 and maximal consecutive longest common subsequence starting at any character n, MCLCSn. MCLCS1 takes two strings as input and returns the shorter string or maximal consecutive portions of the shorter string that consecutively match with the longer string, where matching must be from first character (character 1) for both strings. MCLCSn takes two strings as input and returns the shorter string or maximal consecutive portions of the shorter string that consecutively match with the longer string, where matching </context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Transactions on Knowledge Discovery from Data, 2(2):1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kondrak</author>
</authors>
<title>N-gram similarity and distance.</title>
<date>2005</date>
<booktitle>In Proceedings of the 12h International Conference on String Processing and Information Retrieval,</booktitle>
<pages>115--126</pages>
<location>Buenos Aires, Argentina.</location>
<contexts>
<context position="12211" citStr="Kondrak (2005)" startWordPosition="2009" endWordPosition="2010">y value function in our method. The following sections present a detailed description of each of these functions followed by the procedure to determine some probable candidates along with the procedure to sort the candidates. 3.1 Similarity between Two Strings We use the longest common subsequence (LCS) (Allison and Dix, 1986) measure with some normalization and small modifications for our string similarity measure. We use the same three different modified versions of LCS that we (Islam and Inkpen, 2008) used, along with another modified version of LCS, and then take a weighted sum of these2. Kondrak (2005) showed that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively. Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR). But LCSR does not take into account the length of the shorter string which sometimes has a significant impact on the similarity score. Islam and Inkpen (2008) normalized the longest common subsequence so that it takes into account the length of both the shorter and the longer str</context>
</contexts>
<marker>Kondrak, 2005</marker>
<rawString>G. Kondrak. 2005. N-gram similarity and distance. In Proceedings of the 12h International Conference on String Processing and Information Retrieval, pages 115–126, Buenos Aires, Argentina.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Technique for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Comput. Surv.,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2429" citStr="Kukich, 1992" startWordPosition="394" endWordPosition="395">t. Ironically, errors of this type may even be caused by spelling checkers in the correction of non-word spelling errors when the auto-correct feature in some word-processing software sometimes silently change a non-word to the wrong real word (Hirst and Budanitsky, 2005), and sometimes when correcting a flagged error, the user accidentally make a wrong selection from the choices offered (Wilcox-O’Hearn et al., 2008). An extensive review of real-word spelling correction is given in (Pedler, 2007; Hirst and Budanitsky, 2005) and the problem of spelling correction more generally is reviewed in (Kukich, 1992). The Google Web 1T data set (Brants and Franz, 2006), contributed by Google Inc., contains English word n-grams (from unigrams to 5-grams) and their observed frequency counts calculated over 1 trillion words from web page text collected by Google in January 2006. The text was tokenised following the Penn Treebank tokenisation, except that hyphenated words, dates, email addresses and URLs are kept as single tokens. The sentence boundaries are marked with two special tokens &lt;S&gt; and &lt;/S&gt;. Words that occurred fewer than 200 times were replaced with the special token &lt;UNK&gt;. Table 1 shows the data </context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Technique for automatically correcting words in text. ACM Comput. Surv., 24(4):377–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Mays</author>
<author>Fred J Damerau</author>
<author>Robert L Mercer</author>
</authors>
<title>Context based spelling correction.</title>
<date>1991</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>27--5</pages>
<contexts>
<context position="8109" citStr="Mays et al. (1991)" startWordPosition="1325" endWordPosition="1328"> therefore, are inherently limited to a set of common, predefined errors, but such errors can include both content and function words. Given an occurrence of one of its confusion set members, the spellchecker’s job is to predict which member of that confusion set is the most appropriate in the context. Golding and Roth (1999), an example of a machine-learning method, combined the Winnow algorithm with weighted-majority voting, using nearby and adjacent words as features. Another example of a machine-learning method is that of Carlson et al. (2001). 2.3 Methods Based on Probability Information Mays et al. (1991) proposed a statistical method using word-trigram probabilities for detecting and correcting real-word errors without requiring predefined confusion sets. In this method, if the trigram-derived probability of an observed sentence is lower than that of a sentence obtained by replacing one of the words with a spelling varia1242 tion, then we hypothesize that the original is an error and the variation is what the user intended. Wilcox-O’Hearn et al. (2008) analyze the advantages and limitations of Mays et al. (1991)’s method, and present a new evaluation of the algorithm, designed so that the res</context>
<context position="9636" citStr="Mays et al. (1991)" startWordPosition="1584" endWordPosition="1587">n the sentence is smaller than the window), while respecting sentence boundaries as natural breakpoints. To check the spelling of a span of d words requires a window of length d+4 to accommodate all the trigrams that overlap with the words in the span. The smallest possible window is therefore 5 words long, which uses 3 trigrams to optimize only its middle word. They assume that the sentence is bracketed by two BoS and two EoS markers (to accommodate trigrams involving the first two and last two words of the sentence). The window starts with its left-hand edge at the first BoS marker, and the Mays et al. (1991)’s method is run on the words covered by the trigrams that it contains; the window then moves d words to the right and the process repeats until all the words in the sentence have been checked. As Mays et al. (1991)’s algorithm is run separately in each window, potentially changing a word in each, Wilcox-O’Hearn et al. (2008)’s method as a sideeffect also permits multiple corrections in a single sentence. Wilcox-O’Hearn et al. (2008) show that the trigram-based real-word spelling-correction method of Mays et al. (1991) is superior in performance to the WordNet-based method of Hirst and Budanit</context>
<context position="24531" citStr="Mays et al. (1991)" startWordPosition="4293" endWordPosition="4296">frequency of the 3-gram wm−2 wm−1 wmj. 3. For each wmj ∈ R2, we calculate the string similarity between wmj and wm using equation (5) and then assign a weight using the following equation only to the words that return the string similarity value greater than 0.5. weight = βS(wm, wmj) + (1 − β)F(wmj) 4. We sort the words found in step 3 that were given weights, if any, in descending order by the assigned weights and keep only one word as the candidate word. 1246 4 Evaluation and Experimental Results We used as test data the same data that WilcoxO’Hearn et al. (2008) used in their evaluation of Mays et al. (1991) method, which in turn was a replication of the data used by Hirst and St-Onge (1998) and Hirst and Budanitsky (2005) to evaluate their methods. The data consisted of 500 articles (approximately 300,000 words) from the 1987−89 Wall Street Journal corpus, with all headings, identifiers, and so on removed; that is, just a long stream of text. It is assumed that this data contains no errors; that is, the Wall Street Journal contains no malapropisms or other typos. In fact, a few typos (both non-word and real-word) were noticed during the evaluation, but they were small in number compared to the s</context>
<context position="26038" citStr="Mays et al. (1991)" startWordPosition="4554" endWordPosition="4557">entially replaced by any spelling variation found in the lexicon of the ispell spelling checker7. A spelling variation was defined as any word with an edit distance of 1 from the original word; that is, any single-character insertion, deletion, or substitution, or the transposition of two characters, that results in another real word. Thus, none of the induced malapropisms were derived from closed-class words, and none were formed by the insertion or deletion of an apostrophe or by splitting a word. The data contained 1402 inserted malapropisms. Because it had earlier been used for evaluating Mays et al. (1991)’s trigram method, which operates at the sentence level, the data set had been divided into three parts, without regard for article boundaries or text coherence: sentences into which no malapropism had been induced; the original versions of the sentences that received malapropisms; and the malapropized sentences. In addition, all instances of numbers of various kinds had been replaced by tags such as &lt;INTEGER&gt;, &lt;DOLLAR VALUE&gt;, 7Ispell is a fast screen-oriented spelling checker that shows you your errors in the context of the original file, and suggests possible corrections when it can figure t</context>
</contexts>
<marker>Mays, Damerau, Mercer, 1991</marker>
<rawString>Eric Mays, Fred J. Damerau, and Robert L. Mercer. 1991. Context based spelling correction. Information Processing and Management, 27(5):517–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="12368" citStr="Melamed (1999)" startWordPosition="2032" endWordPosition="2033">probable candidates along with the procedure to sort the candidates. 3.1 Similarity between Two Strings We use the longest common subsequence (LCS) (Allison and Dix, 1986) measure with some normalization and small modifications for our string similarity measure. We use the same three different modified versions of LCS that we (Islam and Inkpen, 2008) used, along with another modified version of LCS, and then take a weighted sum of these2. Kondrak (2005) showed that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively. Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR). But LCSR does not take into account the length of the shorter string which sometimes has a significant impact on the similarity score. Islam and Inkpen (2008) normalized the longest common subsequence so that it takes into account the length of both the shorter and the longer string and called it normalized longest common subse2We (Islam and Inkpen, 2008) use modified versions because in our experiments we obtained better results (pr</context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>I. D. Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, 25(1):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>Introduction to wordnet: An on-line lexical database.</title>
<date>1993</date>
<tech>Technical Report 43,</tech>
<institution>Cognitive Science Laboratory, Princeton University,</institution>
<location>Princeton, NJ.</location>
<contexts>
<context position="6601" citStr="Miller et al., 1993" startWordPosition="1078" endWordPosition="1081">by Hirst and Budanitsky (2005) detected semantic anomalies, but was not restricted to checking words from predefined confusion sets. This approach was based on the observation that the words that a writer intends are generally semantically related to their surrounding words, whereas some types of real-word spelling errors are not, such as (using Hirst and Budanitsky’s example), “It is my sincere hole (hope) that you will recover swiftly.” Such “malapropisms” cause “a perturbation of the cohesion (and coherence) of a text.” Hirst and Budanitsky (2005) use semantic distance measures in WordNet (Miller et al., 1993) to detect words that are potentially anomalous in context - that is, semantically distant from nearby words; if a variation in spelling results in a word that was semantically closer to the context, it is hypothesized that the original word is an error (a “malapropism”) and the closer word is its correction. 2.2 Methods Based on Machine Learning Machine learning methods are regarded as lexical disambiguation tasks and confusion sets are used to model the ambiguity between words. Normally, the machine learning and statistical approaches rely on pre-defined confusion sets, which are sets (usual</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K.J. Miller. 1993. Introduction to wordnet: An on-line lexical database. Technical Report 43, Cognitive Science Laboratory, Princeton University, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Pedler</author>
</authors>
<title>Computer Correction of Realword Spelling Errors in Dyslexic Text.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Birkbeck, London University.</institution>
<contexts>
<context position="2316" citStr="Pedler, 2007" startWordPosition="376" endWordPosition="377">ntext such as, in this case, to recognise that fuss is more likely to occur than muss in the context of all about. Ironically, errors of this type may even be caused by spelling checkers in the correction of non-word spelling errors when the auto-correct feature in some word-processing software sometimes silently change a non-word to the wrong real word (Hirst and Budanitsky, 2005), and sometimes when correcting a flagged error, the user accidentally make a wrong selection from the choices offered (Wilcox-O’Hearn et al., 2008). An extensive review of real-word spelling correction is given in (Pedler, 2007; Hirst and Budanitsky, 2005) and the problem of spelling correction more generally is reviewed in (Kukich, 1992). The Google Web 1T data set (Brants and Franz, 2006), contributed by Google Inc., contains English word n-grams (from unigrams to 5-grams) and their observed frequency counts calculated over 1 trillion words from web page text collected by Google in January 2006. The text was tokenised following the Penn Treebank tokenisation, except that hyphenated words, dates, email addresses and URLs are kept as single tokens. The sentence boundaries are marked with two special tokens &lt;S&gt; and &lt;</context>
</contexts>
<marker>Pedler, 2007</marker>
<rawString>Jennifer Pedler. 2007. Computer Correction of Realword Spelling Errors in Dyslexic Text. Ph.D. thesis, Birkbeck, London University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzan Verberne</author>
</authors>
<title>Context-sensitive spell checking based on word trigram probabilities. Master’s thesis,</title>
<date>2002</date>
<institution>University of Nijmegen, February-August.</institution>
<contexts>
<context position="10569" citStr="Verberne (2002)" startWordPosition="1734" endWordPosition="1735">l. (2008)’s method as a sideeffect also permits multiple corrections in a single sentence. Wilcox-O’Hearn et al. (2008) show that the trigram-based real-word spelling-correction method of Mays et al. (1991) is superior in performance to the WordNet-based method of Hirst and Budanitsky (2005), even on content words (“malapropisms”), especially when supplied with a realistically large trigram model. WilcoxO’Hearn et al. (2008) state that their attempts to improve the method with smaller windows and with multiple corrections per sentence were not successful, because of excessive false positives. Verberne (2002) proposed a trigram-based method for real-word errors without explicitly using probabilities or even localizing the possible error to a specific word. This method simply assumes that any word trigram in the text that is attested in the British National Corpus (Burnard, 2000) is correct, and any unattested trigram is a likely error. When an unattested trigram is observed, the method then tries the spelling variations of all words in the trigram to find attested trigrams to present to the user as possible corrections. The evaluation of this method was carried out on only 7100 words of the Wall S</context>
</contexts>
<marker>Verberne, 2002</marker>
<rawString>Suzan Verberne. 2002. Context-sensitive spell checking based on word trigram probabilities. Master’s thesis, University of Nijmegen, February-August.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Amber Wilcox-O’Hearn</author>
<author>Graeme Hirst</author>
<author>Alexander Budanitsky</author>
</authors>
<title>Real-word spelling correction with trigrams: A reconsideration of the mays, damerau, and mercer model.</title>
<date>2008</date>
<booktitle>Proceedings, 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2008) (Lecture Notes in Computer Science 4919, Springer-Verlag),</booktitle>
<pages>605--616</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<location>Haifa,</location>
<marker>Wilcox-O’Hearn, Hirst, Budanitsky, 2008</marker>
<rawString>L. Amber Wilcox-O’Hearn, Graeme Hirst, and Alexander Budanitsky. 2008. Real-word spelling correction with trigrams: A reconsideration of the mays, damerau, and mercer model. In Alexander Gelbukh, editor, Proceedings, 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2008) (Lecture Notes in Computer Science 4919, Springer-Verlag), pages 605–616, Haifa, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>