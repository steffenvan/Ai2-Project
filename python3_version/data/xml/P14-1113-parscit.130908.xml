<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985022">
Learning Semantic Hierarchies via Word Embeddings
</title>
<author confidence="0.970682">
Ruiji Fu†, Jiang Guo†, Bing Qin†, Wanxiang Che†, Haifeng Wang$, Ting Liu†*
</author>
<affiliation confidence="0.820667666666667">
†Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
$Baidu Inc., Beijing, China
</affiliation>
<email confidence="0.880182">
{rjfu, jguo, bqin, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
</email>
<sectionHeader confidence="0.98316" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948809523809">
Semantic hierarchy construction aims to
build structures of concepts linked by
hypernym–hyponym (“is-a”) relations. A
major challenge for this task is the
automatic discovery of such relations.
This paper proposes a novel and effec-
tive method for the construction of se-
mantic hierarchies based on word em-
beddings, which can be used to mea-
sure the semantic relationship between
words. We identify whether a candidate
word pair has hypernym–hyponym rela-
tion by using the word-embedding-based
semantic projections between words and
their hypernyms. Our result, an F-score
of 73.74%, outperforms the state-of-the-
art methods on a manually labeled test
dataset. Moreover, combining our method
with a previous manually-built hierarchy
extension method can further improve F-
score to 80.29%.
</bodyText>
<sectionHeader confidence="0.992534" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921333333333">
Semantic hierarchies are natural ways to orga-
nize knowledge. They are the main components
of ontologies or semantic thesauri (Miller, 1995;
Suchanek et al., 2008). In the WordNet hierar-
chy, senses are organized according to the “is-a”
relations. For example, “dog” and “canine” are
connected by a directed edge. Here, “canine” is
called a hypernym of “dog.” Conversely, “dog”
is a hyponym of “canine.” As key sources
of knowledge, semantic thesauri and ontologies
can support many natural language processing
applications. However, these semantic resources
are limited in its scope and domain, and their
manual construction is knowledge intensive and
time consuming. Therefore, many researchers
</bodyText>
<note confidence="0.527065">
∗Email correspondence.
</note>
<figureCaption confidence="0.9227065">
Figure 1: An example of semantic hierarchy con-
struction.
</figureCaption>
<bodyText confidence="0.988752142857143">
have attempted to automatically extract semantic
relations or to construct taxonomies.
A major challenge for this task is the auto-
matic discovery of hypernym-hyponym relations.
Fu et al. (2013) propose a distant supervision
method to extract hypernyms for entities from
multiple sources. The output of their model is
a list of hypernyms for a given enity (left pan-
el, Figure 1). However, there usually also exists
hypernym–hyponym relations among these hy-
pernyms. For instance, “植 物 (plant)” and
“毛茛科 (Ranunculaceae)” are both hyper-
nyms of the entity “乌头 (aconit),” and “植
物 (plant)” is also a hypernym of “毛 茛
科
(Ranunculaceae).” Given a list of hypernyms
of an entity, our goal in the present work is to
construct a semantic hierarchy of these hypernyms
(right panel, Figure 1).1
Some previous works extend and refine
manually-built semantic hierarchies by using other
resources (e.g., Wikipedia) (Suchanek et al.,
2008). However, the coverage is limited by the
scope of the resources. Several other works relied
heavily on lexical patterns, which would suffer
from deficiency because such patterns can only
cover a small proportion of complex linguistic cir-
cumstances (Hearst, 1992; Snow et al., 2005).
</bodyText>
<note confidence="0.422959666666667">
1In this study, we focus on Chinese semantic hierarchy
construction. The proposed method can be easily adapted to
other languages.
</note>
<figure confidence="0.99349975">
生物
organism
药品
medicine
植物
plant
毛茛科
Ranunculaceae
植物药
medicinal plant
乌头属
Aconitum
乌头
aconite
植物
plant
药品
medicine
毛茛科
Ranunculaceae
植物药
medicinal plant
乌头属
Aconitum
生物
organism
乌头
aconite
</figure>
<page confidence="0.468085">
1199
</page>
<note confidence="0.9522285">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999861333333333">
Besides, distributional similarity methods (Kotler-
man et al., 2010; Lenci and Benotto, 2012) are
based on the assumption that a term can only be
used in contexts where its hypernyms can be used
and that a term might be used in any contexts
where its hyponyms are used. However, it is not
always rational. Our previous method based on
web mining (Fu et al., 2013) works well for hy-
pernym extraction of entity names, but it is unsuit-
able for semantic hierarchy construction which in-
volves many words with broad semantics. More-
over, all of these methods do not use the word
semantics effectively.
This paper proposes a novel approach for se-
mantic hierarchy construction based on word em-
beddings. Word embeddings, also known as dis-
tributed word representations, typically represent
words with dense, low-dimensional and real-
valued vectors. Word embeddings have been
empirically shown to preserve linguistic regular-
ities, such as the semantic relationship between
words (Mikolov et al., 2013b). For example,
v(king) − v(queen) ≈ v(man) − v(woman),
where v(w) is the embedding of the word w. We
observe that a similar property also applies to the
hypernym–hyponym relationship (Section 3.3),
which is the main inspiration of the present study.
However, we further observe that hypernym–
hyponym relations are more complicated than a
single offset can represent. To address this chal-
lenge, we propose a more sophisticated and gen-
eral method — learning a linear projection which
maps words to their hypernyms (Section 3.3.1).
Furthermore, we propose a piecewise linear pro-
jection method based on relation clustering to
better model hypernym–hyponym relations (Sec-
tion 3.3.2). Subsequently, we identify whether an
unknown word pair is a hypernym–hyponym re-
lation using the projections (Section 3.4). To the
best of our knowledge, we are the first to apply
word embeddings to this task.
For evaluation, we manually annotate a dataset
containing 418 Chinese entities and their hyper-
nym hierarchies, which is the first dataset for this
task as far as we know. The experimental results
show that our method achieves an F-score of
73.74% which significantly outperforms the pre-
vious state-of-the-art methods. Moreover, com-
bining our method with the manually-built hier-
archy extension method proposed by Suchanek et
al. (2008) can further improve F-score to 80.29%.
</bodyText>
<sectionHeader confidence="0.957829" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.990050461538462">
As main components of ontologies, semantic hi-
erarchies have been studied by many researchers.
Some have established concept hierarchies based
on manually-built semantic resources such as
WordNet (Miller, 1995). Such hierarchies have
good structures and high accuracy, but their cov-
erage is limited to fine-grained concepts (e.g.,
“Ranunculaceae” is not included in Word-
Net.). We have made similar obsevation that about
a half of hypernym–hyponym relations are absent
in a Chinese semantic thesaurus. Therefore, a
broader range of resources is needed to supple-
ment the manually built resources. In the construc-
tion of the famous ontology YAGO, Suchanek et
al. (2008) link the categories in Wikipedia onto
WordNet. However, the coverage is still limited
by the scope of Wikipedia.
Several other methods are based on lexical
patterns. They use manually or automatically
constructed lexical patterns to mine hypernym–
hyponym relations from text corpora. A hierarchy
can then be built based on these pairwise relations.
The pioneer work by Hearst (1992) has found
out that linking two noun phrases (NPs) via cer-
tain lexical constructions often implies hypernym
relations. For example, NP1 is a hypernym of NP2
in the lexical pattern “such NP1 as NP2.” Snow et
al. (2005) propose to automatically extract large
numbers of lexico-syntactic patterns and subse-
quently detect hypernym relations from a large
newswire corpus. Their method relies on accurate
syntactic parsers, and the quality of the automat-
ically extracted patterns is difficult to guarantee.
Generally speaking, these pattern-based methods
often suffer from low recall or precision because
of the coverage or the quality of the patterns.
The distributional methods assume that the con-
texts of hypernyms are broader than the ones of
their hyponyms. For distributional similarity com-
puting, each word is represented as a semantic
vector composed of the pointwise mutual infor-
mation (PMI) with its contexts. Kotlerman et al.
(2010) design a directional distributional measure
to infer hypernym–hyponym relations based on
the standard IR Average Precision evaluation mea-
sure. Lenci and Benotto (2012) propose anoth-
er measure focusing on the contexts that hyper-
nyms do not share with their hyponyms. However,
broader semantics may not always infer broader
contexts. For example, for terms “Obama’ and
1200
“American people”, it is hard to say whose
contexts are broader.
Our previous work (Fu et al., 2013) applies a
web mining method to discover the hypernyms of
Chinese entities from multiple sources. We as-
sume that the hypernyms of an entity co-occur
with it frequently. It works well for named enti-
ties. But for class names (e.g., singers in Hong
Kong, tropical fruits) with wider range of mean-
ings, this assumption may fail.
In this paper, we aim to identify hypernym–
hyponym relations using word embeddings, which
have been shown to preserve good properties for
capturing semantic relationship between words.
</bodyText>
<sectionHeader confidence="0.968688" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9999368">
In this section, we first define the task formally.
Then we elaborate on our proposed method com-
posed of three major steps, namely, word embed-
ding training, projection learning, and hypernym–
hyponym relation identification.
</bodyText>
<subsectionHeader confidence="0.998775">
3.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.999947285714286">
Given a list of hypernyms of an entity, our goal is
to construct a semantic hierarchy on it (Figure 1).
We represent the hierarchy as a directed graph
G, in which the nodes denote the words, and the
edges denote the hypernym–hyponym relations.
Hypernym-hyponym relations are asymmetric and
transitive when words are unambiguous:
</bodyText>
<listItem confidence="0.999908">
• Vx, y E L: xH−→y ==�. _,(yH−→x)
• Vx, y, z E L : (xH−→z ∧ zH−→y) ==�. x H −→y
</listItem>
<bodyText confidence="0.999858">
Here, L denotes the list of hypernyms. x, y and
z denote the hypernyms in L. We use H −→ to
represent a hypernym–hyponym relation in this
paper. Actually, x, y and z are unambiguous as
the hypernyms of a certain entity. Therefore, G
should be a directed acyclic graph (DAG).
</bodyText>
<subsectionHeader confidence="0.99968">
3.2 Word Embedding Training
</subsectionHeader>
<bodyText confidence="0.9998986">
Various models for learning word embeddings
have been proposed, including neural net lan-
guage models (Bengio et al., 2003; Mnih and
Hinton, 2008; Mikolov et al., 2013b) and spec-
tral models (Dhillon et al., 2011). More recent-
ly, Mikolov et al. (2013a) propose two log-linear
models, namely the Skip-gram and CBOW model,
to efficiently induce word embeddings. These two
models can be trained very efficiently on a large-
scale corpus because of their low time complexity.
</bodyText>
<equation confidence="0.849827">
No. Examples
1 v(虾) − v(对虾) ≈ v(鱼) − v(金鱼)
v(shrimp) − v(prawn) ≈ v(fish) − v(gold fish)
2 v(工人) − v(木 FE) ≈ v(演员) − v(小丑)
v(laborer) − v(carpenter) ≈ v(actor) − v(clown)
3 v(工人) − v(木 FE) ≈6 v(鱼) − v(金鱼)
v(laborer) − v(carpenter) ≈6 v(fish) − v(gold fish)
</equation>
<tableCaption confidence="0.958059">
Table 1: Embedding offsets on a sample of
hypernym–hyponym word pairs.
</tableCaption>
<bodyText confidence="0.9998902">
Additionally, their experiment results have shown
that the Skip-gram model performs best in identi-
fying semantic relationship among words. There-
fore, we employ the Skip-gram model for estimat-
ing word embeddings in this study.
The Skip-gram model adopts log-linear classi-
fiers to predict context words given the current
word w(t) as input. First, w(t) is projected to its
embedding. Then, log-linear classifiers are em-
ployed, taking the embedding as input and pre-
dict w(t)’s context words within a certain range,
e.g. k words in the left and k words in the
right. After maximizing the log-likelihood over
the entire dataset using stochastic gradient descent
(SGD), the embeddings are learned.
</bodyText>
<subsectionHeader confidence="0.999803">
3.3 Projection Learning
</subsectionHeader>
<bodyText confidence="0.999802913043478">
Mikolov et al. (2013b) observe that word em-
beddings preserve interesting linguistic regulari-
ties, capturing a considerable amount of syntac-
tic/semantic relations. Looking at the well-known
example: v(king) − v(queen) Pz� v(man) −
v(woman), it indicates that the embedding offsets
indeed represent the shared semantic relation be-
tween the two word pairs.
We observe that the same property also ap-
plies to some hypernym–hyponym relations. As
a preliminary experiment, we compute the em-
bedding offsets between some randomly sampled
hypernym–hyponym word pairs and measure their
similarities. The results are shown in Table 1.
The first two examples imply that a word can
also be mapped to its hypernym by utilizing word
embedding offsets. However, the offset from
“carpenter” to “laborer” is distant from
the one from “gold fish” to “fish,” indicat-
ing that hypernym–hyponym relations should be
more complicated than a single vector offset can
represent. To verify this hypothesis, we com-
pute the embedding offsets over all hypernym–
</bodyText>
<figure confidence="0.998023816326531">
1201
鱼-鲨鱼 鱼-金鱼
fish - sharkfish - gold fish
鱼-热带鱼
fish - tropical fish
蟹-海蟹
crab - sea crab
鸡-公鸡
chicken - cock
驴-野驴
donkey - wild ass 羊-小尾寒羊
sheep - small-tail Han sheep
羊-公羊
sheep - ram
虾-对虾
shrimp - prawn
海豚-白鳍豚
dolphin - white-flag dolphin
狗-警犬
dog - police dog
兔-长毛兔
rabbit - wool rabbit
马-斑马
equus - zebra
演员-歌手
actor - singer 演员-主角 演员-小丑 actor - protagonist
actor - clown 演员-斗牛士
actor - matador
运动员-足球球员
sportsman - footballer
工人-木匠
laborer - carpenter
工人-丁
园
工人-临时工
laborer - gardener
laborer - temporary worker
海员-领航员
seaman - navigator
职员-售货员
职员-售票员 staff - salesclerk
staff - conductor 职员-空姐
职员-公务员
staff - airline hostess
staff - civil servant
职位-校长
position - headmaster
职位-总领事
position – consul general
</figure>
<figureCaption confidence="0.993782">
Figure 2: Clusters of the vector offsets in training data. The figure shows that the vector offsets distribute
</figureCaption>
<bodyText confidence="0.978698153846154">
in some clusters. The left cluster shows some hypernym–hyponym relations about animals. The right
one shows some relations about people’s occupations.
hyponym word pairs in our training data and vi-
sualize them.2 Figure 2 shows that the relations
are adequately distributed in the clusters, which
implies that hypernym–hyponym relations in-
deed can be decomposed into more fine-grained
relations. Moreover, the relations about animals
are spatially close, but separate from the relations
about people’s occupations.
To address this challenge, we propose to learn
the hypernym–hyponym relations using projection
matrices.
</bodyText>
<subsectionHeader confidence="0.995301">
3.3.1 A Uniform Linear Projection
</subsectionHeader>
<bodyText confidence="0.998434">
Intuitively, we assume that all words can be pro-
jected to their hypernyms based on a uniform tran-
sition matrix. That is, given a word x and its hy-
pernym y, there exists a matrix Φ so that y = Φx.
For simplicity, we use the same symbols as the
words to represent the embedding vectors. Ob-
taining a consistent exact Φ for the projection of
all hypernym–hyponym pairs is difficult. Instead,
we can learn an approximate Φ using Equation 1
on the training data, which minimizes the mean-
squared error:
</bodyText>
<equation confidence="0.88121">
k Φx − y k2 (1)
</equation>
<bodyText confidence="0.9989732">
where N is the number of (x, y) word pairs in
the training data. This is a typical linear regres-
sion problem. The only difference is that our pre-
dictions are multi-dimensional vectors instead of
scalar values. We use SGD for optimization.
</bodyText>
<listItem confidence="0.317994">
2Principal Component Analysis (PCA) is applied for di-
mensionality reduction.
</listItem>
<subsectionHeader confidence="0.956942">
3.3.2 Piecewise Linear Projections
</subsectionHeader>
<bodyText confidence="0.999986181818182">
A uniform linear projection may still be under-
representative for fitting all of the hypernym–
hyponym word pairs, because the relations are
rather diverse, as shown in Figure 2. To better
model the various kinds of hypernym–hyponym
relations, we apply the idea of piecewise linear re-
gression (Ritzema, 1994) in this study.
Specifically, the input space is first segmented
into several regions. That is, all word pairs (x, y)
in the training data are first clustered into sever-
al groups, where word pairs in each group are
expected to exhibit similar hypernym–hyponym
relations. Each word pair (x, y) is represented
with their vector offsets: y − x for clustering.
The reasons are twofold: (1) Mikolov’s work has
shown that the vector offsets imply a certain lev-
el of semantic relationship. (2) The vector off-
sets distribute in clusters well, and the word pairs
which are close indeed represent similar relations,
as shown in Figure 2.
Then we learn a separate projection for each
cluster, respectively (Equation 2).
</bodyText>
<equation confidence="0.843366">
k Φkx − y k2 (2)
</equation>
<bodyText confidence="0.99984775">
where Nk is the amount of word pairs in the kth
cluster Ck.
We use the k-means algorithm for clustering,
where k is tuned on a development dataset.
</bodyText>
<subsectionHeader confidence="0.941538">
3.3.3 Training Data
</subsectionHeader>
<bodyText confidence="0.986630666666667">
To learn the projection matrices, we extract train-
ing data from a Chinese semantic thesaurus,
Tongyi Cilin (Extended) (CilinE for short) which
</bodyText>
<figure confidence="0.9433654">
Φ∗ = arg min 1 N
Φ (x,y)
Φ∗k = arg min 1
Φk Nk (x,y)∈Ck
1202
</figure>
<figureCaption confidence="0.9829645">
Figure 3: Hierarchy of CilinE and an Example of
Training Data Generation
</figureCaption>
<bodyText confidence="0.999823727272727">
contains 100,093 words (Che et al., 2010).3 CilinE
is organized as a hierarchy of five levels, in which
the words are linked by hypernym–hyponym
relations (right panel, Figure 3). Each word in
CilinE has one or more sense codes (some words
are polysemous) that indicate its position in the hi-
erarchy.
The senses of words in the first level, such as
“物 (object)” and “时间 (time),” are very gen-
eral. The fourth level only has sense codes without
real words. Therefore, we extract words in the sec-
ond, third and fifth levels to constitute hypernym–
hyponym pairs (left panel, Figure 3).
Note that mapping one hyponym to multi-
ple hypernyms with the same projection (Φx is
unique) is difficult. Therefore, the pairs with the
same hyponym but different hypernyms are ex-
pected to be clustered into separate groups. Fig-
ure 3 shows that the word “dragonfly” in the
fifth level has two hypernyms: “insect” in the
third level and “animal” in the second level.
Hence the relations dragonfly H−→ insect and
dragonfly H−→ animal should fall into differ-
ent clusters.
In our implementation, we apply this constraint
by simply dividing the training data into two cat-
egories, namely, direct and indirect. Hypernym-
hyponym word pair (x, y) is classified into the di-
rect category, only if there doesn’t exist another
word z in the training data, which is a hypernym of
x and a hyponym of y. Otherwise, (x, y) is classi-
fied into the indirect category. Then, data in these
two categories are clustered separately.
</bodyText>
<footnote confidence="0.430758">
3www.ltp-cloud.com/download/
</footnote>
<figureCaption confidence="0.54265975">
Figure 4: In this example, Φkx is located in the
circle with center y and radius δ. So y is consid-
ered as a hypernym of x. Conversely, y is not a
hypernym of x&apos;.
</figureCaption>
<equation confidence="0.77094975">
y
z
x
(a)(b) zx yFi gu re 5: (a ) I f d(Φjy, x) &gt; d( Φkx
</equation>
<bodyText confidence="0.934970375">
,y), wer e-mo veth e pa th fro my to x; (b ) i f
d(Φjy, x) &gt;d( Φkx, z) an d d(Φjy, x) &gt; d( Φiz,y),
wer ever seth e path
fro mytox.3.4Hyperny m-hypony
mRelationIden
tific ationUpon obt ainingth ec lusterso ftra ini
ngd ataandthecorr espondingpro je cti ons,weca
nidenti fyw hethe rtwo w ordshaveahyperny m–h
yponymr e-lat ion . Give n two wo rd sxa ndy, wefi
nd clust erCkwh os ecenter is clo sest to t h eo ffs
etyCx, and obtain thecorr espondingp roj ect i on
Φk . For ytobec o nsidered ah yp ern ym ofx ,on
eofthetwoc ondit ions below
musthold. Co ndi tion 1:Thep ro ject ion Φkput
sΦkx cl os e enought o y ( Figure4). For mally,the
euclidea ndistan ce b etw ee n Φkx an d y :d(Φ kx
</bodyText>
<equation confidence="0.72887075">
,y) m ustb e lessthana th
resho l dδ.
d(Φkx, y) =n Φkx
C yn2&lt;δ(3) wordzsat- Condition2: Thereexistsanother
</equation>
<table confidence="0.827313">
,weusethe isfyingH x −→z andzH−→y. In thisc ase
elations . transitivityofhypernym–hyponymr
ul dbe aDAG Besides,thefinal hierarchy sho
r ,the pro- as discussed in Section3.1.Ho weve
ore tical- jectionmethodcannot guaranteethatthe
from pair- ly,becausetheprojectionsarelearned
hout thew- wisehypernym–hyponym relationsw it
hypernym– hole hierarchy structure. Allpairwise
hodswould hyponym relationidentificationmet
saninter- sufferfrom thisproblem actually.Iti
estingproblem how to construct aopti-
hypernym- CilinE
hyponym pairs
</table>
<equation confidence="0.8236495">
蜻蜓 : 动物
(dragonfly : animal)
蜻蜓 : 昆虫
(dragonfly : insect)
昆虫 : 动物
(insect : animal)
</equation>
<figure confidence="0.840507088235294">
物 object
Sense Code: B
动物 animal
蜻蜓 dragonfly
Sense Code: Bi
昆虫 insect
Sense Code: Bi18
Sense Code: Bi18A
Sense Code: Bi18A06@
--
18
A
06@
B
i
...
...
...
...
...
Root
Level 1
Level 2
Level 3
Level 4
Level 5
Φl
Φk
x
x
δ
y
x
y
</figure>
<bodyText confidence="0.99865275">
mal semantic hierarchy conforming to the form
of a DAG. But this is not the focus of this paper.
So if some conflicts occur, that is, a relation cir-
cle exists, we remove or reverse the weakest path
heuristically (Figure 5). If a circle has only two
nodes, we remove the weakest path. If a circle has
more than two nodes, we reverse the weakest path
to form an indirect hypernym–hyponym relation.
</bodyText>
<sectionHeader confidence="0.997193" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.964635">
4.1 Experimental Data
</subsectionHeader>
<bodyText confidence="0.999979607142857">
In this work, we learn word embeddings from a
Chinese encyclopedia corpus named Baidubaike4,
which contains about 30 million sentences (about
780 million words). The Chinese segmentation
is provided by the open-source Chinese language
processing platform LTP5 (Che et al., 2010).
Then, we employ the Skip-gram method (Section
3.2) to train word embeddings. Finally we obtain
the embedding vectors of 0.56 million words.
The training data for projection learning is
collected from CilinE (Section 3.3.3). We ob-
tain 15,247 word pairs of hypernym–hyponym
relations (9,288 for direct relations and 5,959 for
indirect relations).
For evaluation, we collect the hypernyms for
418 entities, which are selected randomly from
Baidubaike, following Fu et al. (2013). We then
ask two annotators to manually label the seman-
tic hierarchies of the correct hypernyms. The final
data set contains 655 unique hypernyms and 1,391
hypernym–hyponym relations among them. We
randomly split the labeled data into 1/5 for de-
velopment and 4/5 for testing (Table 2). The hi-
erarchies are represented as relations of pairwise
words. We measure the inter-annotator agreement
using the kappa coefficient (Siegel and Castel-
lan Jr, 1988). The kappa value is 0.96, which indi-
cates a good strength of agreement.
</bodyText>
<subsectionHeader confidence="0.97441">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999476">
We use precision, recall, and F-score as our met-
rics to evaluate the performances of the methods.
Since hypernym–hyponym relations and its re-
verse (hyponym–hypernym) have one-to-one cor-
respondence, their performances are equal. For
</bodyText>
<footnote confidence="0.60842125">
4Baidubaike (bailee.baidu.com) is one of the largest
Chinese encyclopedias containing more than 7.05 million en-
tries as of September, 2013.
5www.ltp-cloud.com/demo/
</footnote>
<table confidence="0.995968666666667">
Relation # of word pairs
Dev. Test
hypernym–hyponym 312 1,079
hyponym–hypernym* 312 1,079
unrelated 1,044 3,250
Total 1,668 5,408
</table>
<tableCaption confidence="0.984993">
Table 2: The evaluation data. *Since hypernym–
</tableCaption>
<bodyText confidence="0.517328">
hyponym relations and hyponym–hypernym
relations have one-to-one correspondence, their
numbers are the same.
</bodyText>
<figureCaption confidence="0.817678">
Figure 6: Performance on development data w.r.t.
cluster size.
</figureCaption>
<bodyText confidence="0.9976675">
simplicity, we only report the performance of the
former in the experiments.
</bodyText>
<sectionHeader confidence="0.998771" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.999525">
5.1 Varying the Amount of Clusters
</subsectionHeader>
<bodyText confidence="0.99999775">
We first evaluate the effect of different number of
clusters based on the development data. We vary
the numbers of the clusters both for the direct and
indirect training word pairs.
As shown in Figure 6, the performance of clus-
tering is better than non-clustering (when the clus-
ter number is 1), thus providing evidences that
learning piecewise projections based on clustering
is reasonable. We finally set the numbers of the
clusters of direct and indirect to 20 and 5, respec-
tively, where the best performances are achieved
on the development data.
</bodyText>
<subsectionHeader confidence="0.999832">
5.2 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.999322">
In this section, we compare the proposed method
with previous methods, including manually-built
hierarchy extension, pairwise relation extraction
</bodyText>
<figure confidence="0.963191619047619">
40
Direct 30
0.8
0.75
0.7
0.55
0.5
0.45
1
10
20
20
15
10
5 Indirect
60 1
50
F1-Score
0.65
0.6
1204
</figure>
<table confidence="0.9841206">
P(%) R(%) F(%)
MWiki+CilinE 92.41 60.61 73.20
MPattern 97.47 21.41 35.11
MSnow 60.88 25.67 36.11
MbalApinc 54.96 53.38 54.16
MinvCL 49.63 62.84 55.46
MFu 87.40 48.19 62.13
MEmb 80.54 67.99 73.74
MEmb+CilinE 80.59 72.42 76.29
MEmb+Wiki+CilinE 79.78 80.81 80.29
</table>
<tableCaption confidence="0.93243875">
Table 3: Comparison of the proposed method with
existing methods in the test set.
Table 4: Chinese Hearst-style lexical patterns. The
contents in square brackets are omissible.
</tableCaption>
<bodyText confidence="0.871466">
based on patterns, word distributions, and web
mining (Section 2). Results are shown in Table 3.
</bodyText>
<subsubsectionHeader confidence="0.535126">
5.2.1 Overall Comparison
</subsubsectionHeader>
<bodyText confidence="0.999966909090909">
MWiki+CilinE refers to the manually-built hierar-
chy extension method of Suchanek et al. (2008).
In our experiment, we use the category taxonomy
of Chinese Wikipedia6 to extend CilinE. Table 3
shows that this method achieves a high precision
but also a low recall, mainly because of the limit-
ed scope of Wikipedia.
MPattern refers to the pattern-based method of
Hearst (1992). We extract hypernym–hyponym
relations in the Baidubaike corpus, which is al-
so used to train word embeddings (Section 4.1).
We use the Chinese Hearst-style patterns (Table
4) proposed by Fu et al. (2013), in which w rep-
resents a word, and h represents one of its hy-
pernyms. The result shows that only a small part
of the hypernyms can be extracted based on these
patterns because only a few hypernym relations
are expressed in these fixed patterns, and many are
expressed in highly flexible manners.
In the same corpus, we apply the method
MSnow originally proposed by Snow et al. (2005).
The same training data for projections learn-
</bodyText>
<equation confidence="0.571786">
6dumps.wikimedia.org/zhwiki/20131205/
</equation>
<bodyText confidence="0.999858058823529">
ing from CilinE (Section 3.3.3) is used as
seed hypernym–hyponym pairs. Lexico-syntactic
patterns are extracted from the Baidubaike corpus
by using the seeds. We then develop a logistic re-
gression classifier based on the patterns to recog-
nize hypernym–hyponym relations. This method
relies on an accurate syntactic parser, and the qual-
ity of the automatically extracted patterns is diffi-
cult to guarantee.
We re-implement two previous distribution-
al methods MbalApinc (Kotlerman et al., 2010)
and MinvCL (Lenci and Benotto, 2012) in the
Baidubaike corpus. Each word is represented as a
feature vector in which each dimension is the PMI
value of the word and its context words. We com-
pute a score for each word pair and apply a thresh-
old to identify whether it is a hypernym–hyponym
relation.
MFu refers to our previous web mining
method (Fu et al., 2013). This method mines hy-
pernyms of a given word w from multiple sources
and returns a ranked list of the hypernyms. We
select the hypernyms with scores over a threshold
of each word in the test set for evaluation. This
method assumes that frequent co-occurrence of a
noun or noun phrase n in multiple sources with w
indicate possibility of n being a hypernym of w.
The results presented in Fu et al. (2013) show that
the method works well when w is an entity, but
not when w is a word with a common semantic
concept. The main reason may be that there are
relatively more introductory pages about entities
than about common words in the Web.
MEmb is the proposed method based on word
embeddings. Table 3 shows that the proposed
method achieves a better recall and F-score than
all of the previous methods do. It can significantly
(p &lt; 0.01) improve the F-score over the state-of-
the-art method MWiki+CilinE.
MEmb and MCilinE can also be combined. The
combination strategy is to simply merge all pos-
itive results from the two methods together, and
then to infer new relations based on the transitiv-
ity of hypernym–hyponym relations. The F-score
is further improved from 73.74% to 76.29%. Note
that, the combined method achieves a 4.43% re-
call improvement over MEmb, but the precision is
almost unchanged. The reason is that the infer-
ence based on the relations identified automatical-
ly may lead to error propagation. For example, the
relation xH−→y is incorrectly identified by MEmb.
</bodyText>
<table confidence="0.483786823529412">
Pattern
Translation
w 是[一个|一种] h
w [、]等 h
h [,] 叫[做] w
h [,] [像]如 w
h [,] 特别是 w
w is a [a kind of] h
w[,] and other h
h[,] called w
h[,] such as w
h[,] especially w
1205
P(%) R(%) F(%)
MWiki+CilinE 80.39 19.29 31.12
MEmb+CilinE 71.16 52.80 60.62
MEmb+Wiki+CilinE 69.13 61.65 65.17
</table>
<tableCaption confidence="0.932867">
Table 5: Performance on the out-of-CilinE data in
the test set.
</tableCaption>
<figure confidence="0.535561">
Recall
</figure>
<figureCaption confidence="0.972095">
Figure 7: Precision-Recall curves on the out-of-
CilinE data in the test set.
</figureCaption>
<bodyText confidence="0.992281571428571">
When the relation yH---+z from MCilinE is added, it
will cause a new incorrect relation x-+z.
Combining MEmb with MWiki+CilinE achieves
a 7% F-score improvement over the best baseline
MWiki+CilinE. Therefore, the proposed method
is complementary to the manually-built hierarchy
extension method (Suchanek et al., 2008).
</bodyText>
<subsubsectionHeader confidence="0.72644">
5.2.2 Comparison on the Out-of-CilinE Data
</subsubsectionHeader>
<bodyText confidence="0.993702909090909">
We are greatly interested in the practical perfor-
mance of the proposed method on the hypernym–
hyponym relations outside of CilinE. We say a
word pair is outside of CilinE, as long as there
is one word in the pair not existing in CilinE. In
our test data, about 62% word pairs are outside
of CilinE. Table 5 shows the performances of the
best baseline method and our method on the out-
of-CilinE data. The method exploiting the tax-
onomy in Wikipedia, MWiki+CilinE, achieves the
highest precision but has a low recall. By con-
trast, our method can discover more hypernym–
hyponym relations with some loss of precision,
thereby achieving a more than 29% F-score im-
provement. The combination of these two meth-
ods achieves a further 4.5% F-score improvement
over MEmb+CilinE. Generally speaking, the pro-
posed method greatly improves the recall but dam-
ages the precision.
Actually, we can get different precisions and re-
Figure 8: An example for error analysis. The
red paths refer to the relations between the named
entity and its hypernyms extracted using the web
mining method (Fu et al., 2013). The black paths
with hollow arrows denote the relations identified
by the different methods. The boxes with dotted
borders refer to the concepts which are not linked
to correct positions.
calls by adjusting the threshold δ (Equation 3).
Figure 7 shows that MEmb+CilinE achieves a high-
er precision than MWiki+CilinE when their recalls
are the same. When they achieve the same preci-
sion, the recall of MEmb+CilinE is higher.
</bodyText>
<subsectionHeader confidence="0.998212">
5.3 Error Analysis and Discussion
</subsectionHeader>
<bodyText confidence="0.9998525">
We analyze error cases after experiments. Some
cases are shown in Figure 8. We can see that
there is only one general relation “植物 (plant)”
�� “生物 (organism)” existing in CilinE. Some
</bodyText>
<equation confidence="0.460248">
H
</equation>
<bodyText confidence="0.999672230769231">
fine-grained relations exist in Wikipedia, but the
coverage is limited. Our method based on
word embeddings can discover more hypernym–
hyponym relations than the previous methods can.
When we combine the methods together, we get
the correct hierarchy.
Figure 8 shows that our method loses the
relation “乌 头 属 (Aconitum)” � “毛 茛 科
(Ranunculaceae).” It is because they are
very semantically similar (their cosine similarity
is 0.9038). Their representations are so close to
each other in the embedding space that we have
not find projections suitable for these pairs. The
</bodyText>
<figure confidence="0.99660587012987">
0.0 0.2 0.4 0.6 0.8 1.0
Precision
0.0 0.2 0.4 0.6 0.8 1.0
MEmb+Wiki+CilinE
● MEmb+CilinE
MWiki+CilinE
● ●
●
●●
●
●
●
●
●
●
●●
生物
organism
生物
organism
乌头
aconite
乌头
aconite
(a) CilinE
(b) Wikipedia+CilinE
生物
organism
乌头
aconite
乌头
aconite
(c) Embedding
(d) Embedding+Wikipedia+CilinE
药品
medicine
植物药
medicinal plant
植物药
medicinal plant
药品
medicine
毛茛科
Ranunculaceae
植物
plant
乌头属
Aconitum
植物药
medicinal plant
药品
medicine
毛茛科
Ranunculaceae
植物
plant
乌头属
Aconitum
植物
plant
毛茛科
Ranunculaceae
药品
medicine
乌头属
Aconitum
植物药
medicinal plant
植物
plant
毛茛科
Ranunculaceae
乌头属
Aconitum
生物
organism
1206
</figure>
<bodyText confidence="0.999930666666667">
error statistics show that when the cosine similari-
ties of word pairs are greater than 0.8, the recall is
only 9.5%. This kind of error accounted for about
10.9% among all the errors in our test set. One
possible solution may be adding more data of this
kind to the training set.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999972654545455">
In addition to the works mentioned in Section 2,
we introduce another set of related studies in this
section.
Evans (2004), Ortega-Mendoza et al. (2007),
and Sang (2007) consider web data as a large cor-
pus and use search engines to identify hypernyms
based on the lexical patterns of Hearst (1992).
However, the low quality of the sentences in the
search results negatively influence the precision of
hypernym extraction.
Following the method for discovering patterns
automatically (Snow et al., 2005), McNamee et
al. (2008) apply the same method to extract hy-
pernyms of entities in order to improve the perfor-
mance of a question answering system. Ritter et al.
(2009) propose a method based on patterns to find
hypernyms on arbitrary noun phrases. They use
a support vector machine classifier to identify the
correct hypernyms from the candidates that match
the patterns. As our experiments show, pattern-
based methods suffer from low recall because of
the low coverage of patterns.
Besides Kotlerman et al. (2010) and Lenci and
Benotto (2012), other researchers also propose di-
rectional distributional similarity methods (Weeds
et al., 2004; Geffet and Dagan, 2005; Bhagat et al.,
2007; Szpektor et al., 2007; Clarke, 2009). How-
ever, their basic assumption that a hyponym can
only be used in contexts where its hypernyms can
be used and that a hypernym might be used in all
of the contexts where its hyponyms are used may
not always rational.
Snow et al. (2006) provides a global optimiza-
tion scheme for extending WordNet, which is d-
ifferent from the above-mentioned pairwise rela-
tionships identification methods.
Word embeddings have been successfully ap-
plied in many applications, such as in sentiment
analysis (Socher et al., 2011b), paraphrase detec-
tion (Socher et al., 2011a), chunking, and named
entity recognition (Turian et al., 2010; Collobert
et al., 2011). These applications mainly utilize
the representing power of word embeddings to al-
leviate the problem of data sparsity. Mikolov et
al. (2013a) and Mikolov et al. (2013b) further ob-
serve that the semantic relationship of words can
be induced by performing simple algebraic oper-
ations with word vectors. Their work indicates
that word embeddings preserve some interesting
linguistic regularities, which might provide sup-
port for many applications. In this paper, we
improve on their work by learning multiple lin-
ear projections in the embedding space, to model
hypernym–hyponym relationships within different
clusters.
</bodyText>
<sectionHeader confidence="0.96425" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999990576923077">
This paper proposes a novel method for seman-
tic hierarchy construction based on word em-
beddings, which are trained using a large-scale
corpus. Using the word embeddings, we learn
the hypernym–hyponym relationship by estimat-
ing projection matrices which map words to their
hypernyms. Further improvements are made us-
ing a cluster-based approach in order to model
the more fine-grained relations. Then we propose
a few simple criteria to identity whether a new
word pair is a hypernym–hyponym relation. Based
on the pairwise hypernym–hyponym relations, we
build semantic hierarchies automatically.
In our experiments, the proposed method signif-
icantly outperforms state-of-the-art methods and
achieves the best F1-score of 73.74% on a manual-
ly labeled test dataset. Further experiments show
that our method is complementary to the previous
manually-built hierarchy extension methods.
For future work, we aim to improve word
embedding learning under the guidance of
hypernym–hyponym relations. By including the
hypernym–hyponym relation constraints while
training word embeddings, we expect to improve
the embeddings such that they become more suit-
able for this task.
</bodyText>
<sectionHeader confidence="0.995468" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.918054333333333">
This work was supported by National Natu-
ral Science Foundation of China (NSFC) via
grant 61133012, 61273321 and the National 863
Leading Technology Research Project via grant
2012AA011102. Special thanks to Shiqi Zhao,
Zhenghua Li, Wei Song and the anonymous re-
viewers for insightful comments and suggestions.
We also thank Xinwei Geng and Hongbo Cai for
their help in the experiments.
1207
Paul McNamee, Rion Snow, Patrick Schone, and James
Mayfield. 2008. Learning named entity hyponyms
for question answering. In Proceedings of the
Third International Joint Conference on Natural
Language Processing, pages 799–804.
</bodyText>
<sectionHeader confidence="0.906233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986510525925927">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorith-
m for learning directionality of inference rules. In
EMNLP-CoNLL, pages 161–170.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Coling
2010: Demonstrations, pages 13–16, Beijing, Chi-
na, August.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, pages 112–119. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems, pages 199–207.
Richard Evans. 2004. A framework for named entity
recognition in the open domain. Recent Advances in
Natural Language Processing III: Selected Papers
from RANLP 2003, 260:267–274.
Ruiji Fu, Bing Qin, and Ting Liu. 2013. Exploiting
multiple sources for open-domain hypernym discov-
ery. In EMNLP, pages 1224–1234.
Maayan Geffet and Ido Dagan. 2005. The distribution-
al inclusion hypotheses and lexical entailment. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 107–114.
Association for Computational Linguistics.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539–545. Association for Compu-
tational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribution-
al similarity for lexical inference. Natural Language
Engineering, 16(4):359–389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 75–79. Association for
Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746–751.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081–1088.
Rosa M Ortega-Mendoza, Luis Villase˜nor-Pineda, and
Manuel Montes-y G´omez. 2007. Using lexical
patterns for extracting hyponyms from the web. In
MICAI 2007: Advances in Artificial Intelligence,
pages 904–911. Springer.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of the 2009 AAAI Spring
Symposium on Learning by Reading and Learning
to Read, pages 88–93.
HP Ritzema. 1994. Drainage principles and
applications.
Erik Tjong Kim Sang. 2007. Extracting hypernym
pairs from the web. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 165–168. Associa-
tion for Computational Linguistics.
Sidney Siegel and N John Castellan Jr. 1988. Non-
parametric statistics for the behavioral sciences.
McGraw-Hill, New York.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L´eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297–1304. MIT
Press, Cambridge, MA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computation-
al Linguistics, pages 801–808, Sydney, Australia,
July. Association for Computational Linguistics.
1208
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Ng. 2011a. Dynam-
ic pooling and unfolding recursive autoencoders for
paraphrase detection. In Advances in Neural Infor-
mation Processing Systems, pages 801–809.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203–217.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
456–463, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th internation-
al conference on Computational Linguistics, page
1015. Association for Computational Linguistics.
</reference>
<page confidence="0.836551">
1209
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.447583">
<title confidence="0.999962">Learning Semantic Hierarchies via Word Embeddings</title>
<author confidence="0.992001">Jiang Bing Wanxiang Haifeng Ting</author>
<affiliation confidence="0.9996535">Center for Social Computing and Information Harbin Institute of Technology,</affiliation>
<address confidence="0.978396">Inc., Beijing,</address>
<email confidence="0.932461">jguo,bqin,car,wanghaifeng@baidu.com</email>
<abstract confidence="0.999459476190476">Semantic hierarchy construction aims to build structures of concepts linked by hypernym–hyponym (“is-a”) relations. A major challenge for this task is the automatic discovery of such relations. This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings, which can be used to measure the semantic relationship between words. We identify whether a candidate word pair has hypernym–hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74%, outperforms the state-of-theart methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve F-</abstract>
<note confidence="0.539894">score to 80.29%.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="10054" citStr="Bengio et al., 2003" startWordPosition="1568" endWordPosition="1571">e hypernym–hyponym relations. Hypernym-hyponym relations are asymmetric and transitive when words are unambiguous: • Vx, y E L: xH−→y ==�. _,(yH−→x) • Vx, y, z E L : (xH−→z ∧ zH−→y) ==�. x H −→y Here, L denotes the list of hypernyms. x, y and z denote the hypernyms in L. We use H −→ to represent a hypernym–hyponym relation in this paper. Actually, x, y and z are unambiguous as the hypernyms of a certain entity. Therefore, G should be a directed acyclic graph (DAG). 3.2 Word Embedding Training Various models for learning word embeddings have been proposed, including neural net language models (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b) and spectral models (Dhillon et al., 2011). More recently, Mikolov et al. (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. These two models can be trained very efficiently on a largescale corpus because of their low time complexity. No. Examples 1 v(虾) − v(对虾) ≈ v(鱼) − v(金鱼) v(shrimp) − v(prawn) ≈ v(fish) − v(gold fish) 2 v(工人) − v(木 FE) ≈ v(演员) − v(小丑) v(laborer) − v(carpenter) ≈ v(actor) − v(clown) 3 v(工人) − v(木 FE) ≈6 v(鱼) − v(金鱼) v(laborer) − v(carpenter) ≈6 v(fish) − v(gold fi</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Patrick Pantel</author>
<author>Eduard H Hovy</author>
<author>Marina Rey</author>
</authors>
<title>Ledir: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>161--170</pages>
<contexts>
<context position="32217" citStr="Bhagat et al., 2007" startWordPosition="5258" endWordPosition="5261"> of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chu</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, Rey, 2007</marker>
<rawString>Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and Marina Rey. 2007. Ledir: An unsupervised algorithm for learning directionality of inference rules. In EMNLP-CoNLL, pages 161–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
</authors>
<title>Ltp: A chinese language technology platform.</title>
<date>2010</date>
<booktitle>In Coling 2010: Demonstrations,</booktitle>
<pages>13--16</pages>
<location>Beijing, China,</location>
<contexts>
<context position="16338" citStr="Che et al., 2010" startWordPosition="2610" endWordPosition="2613">relations, as shown in Figure 2. Then we learn a separate projection for each cluster, respectively (Equation 2). k Φkx − y k2 (2) where Nk is the amount of word pairs in the kth cluster Ck. We use the k-means algorithm for clustering, where k is tuned on a development dataset. 3.3.3 Training Data To learn the projection matrices, we extract training data from a Chinese semantic thesaurus, Tongyi Cilin (Extended) (CilinE for short) which Φ∗ = arg min 1 N Φ (x,y) Φ∗k = arg min 1 Φk Nk (x,y)∈Ck 1202 Figure 3: Hierarchy of CilinE and an Example of Training Data Generation contains 100,093 words (Che et al., 2010).3 CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym–hyponym relations (right panel, Figure 3). Each word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy. The senses of words in the first level, such as “物 (object)” and “时间 (time),” are very general. The fourth level only has sense codes without real words. Therefore, we extract words in the second, third and fifth levels to constitute hypernym– hyponym pairs (left panel, Figure 3). Note that mapping one hyponym to multiple hypernyms with t</context>
<context position="20380" citStr="Che et al., 2010" startWordPosition="3319" endWordPosition="3322">if some conflicts occur, that is, a relation circle exists, we remove or reverse the weakest path heuristically (Figure 5). If a circle has only two nodes, we remove the weakest path. If a circle has more than two nodes, we reverse the weakest path to form an indirect hypernym–hyponym relation. 4 Experimental Setup 4.1 Experimental Data In this work, we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike4, which contains about 30 million sentences (about 780 million words). The Chinese segmentation is provided by the open-source Chinese language processing platform LTP5 (Che et al., 2010). Then, we employ the Skip-gram method (Section 3.2) to train word embeddings. Finally we obtain the embedding vectors of 0.56 million words. The training data for projection learning is collected from CilinE (Section 3.3.3). We obtain 15,247 word pairs of hypernym–hyponym relations (9,288 for direct relations and 5,959 for indirect relations). For evaluation, we collect the hypernyms for 418 entities, which are selected randomly from Baidubaike, following Fu et al. (2013). We then ask two annotators to manually label the semantic hierarchies of the correct hypernyms. The final data set contai</context>
</contexts>
<marker>Che, Li, Liu, 2010</marker>
<rawString>Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technology platform. In Coling 2010: Demonstrations, pages 13–16, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: an overview.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>112--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32255" citStr="Clarke, 2009" startWordPosition="5266" endWordPosition="5267">ance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (T</context>
</contexts>
<marker>Clarke, 2009</marker>
<rawString>Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 112–119. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="32898" citStr="Collobert et al., 2011" startWordPosition="5369" endWordPosition="5372">ic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym–hyponym relationships within di</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>199--207</pages>
<contexts>
<context position="10144" citStr="Dhillon et al., 2011" startWordPosition="1584" endWordPosition="1587">hen words are unambiguous: • Vx, y E L: xH−→y ==�. _,(yH−→x) • Vx, y, z E L : (xH−→z ∧ zH−→y) ==�. x H −→y Here, L denotes the list of hypernyms. x, y and z denote the hypernyms in L. We use H −→ to represent a hypernym–hyponym relation in this paper. Actually, x, y and z are unambiguous as the hypernyms of a certain entity. Therefore, G should be a directed acyclic graph (DAG). 3.2 Word Embedding Training Various models for learning word embeddings have been proposed, including neural net language models (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b) and spectral models (Dhillon et al., 2011). More recently, Mikolov et al. (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. These two models can be trained very efficiently on a largescale corpus because of their low time complexity. No. Examples 1 v(虾) − v(对虾) ≈ v(鱼) − v(金鱼) v(shrimp) − v(prawn) ≈ v(fish) − v(gold fish) 2 v(工人) − v(木 FE) ≈ v(演员) − v(小丑) v(laborer) − v(carpenter) ≈ v(actor) − v(clown) 3 v(工人) − v(木 FE) ≈6 v(鱼) − v(金鱼) v(laborer) − v(carpenter) ≈6 v(fish) − v(gold fish) Table 1: Embedding offsets on a sample of hypernym–hyponym word pairs. Additionally, t</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer Dhillon, Dean P Foster, and Lyle H Ungar. 2011. Multi-view learning of word embeddings via cca. In Advances in Neural Information Processing Systems, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Evans</author>
</authors>
<title>A framework for named entity recognition in the open domain.</title>
<date>2004</date>
<booktitle>Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003,</booktitle>
<pages>260--267</pages>
<contexts>
<context position="31153" citStr="Evans (2004)" startWordPosition="5090" endWordPosition="5091">dicinal plant 药品 medicine 毛茛科 Ranunculaceae 植物 plant 乌头属 Aconitum 植物 plant 毛茛科 Ranunculaceae 药品 medicine 乌头属 Aconitum 植物药 medicinal plant 植物 plant 毛茛科 Ranunculaceae 乌头属 Aconitum 生物 organism 1206 error statistics show that when the cosine similarities of word pairs are greater than 0.8, the recall is only 9.5%. This kind of error accounted for about 10.9% among all the errors in our test set. One possible solution may be adding more data of this kind to the training set. 6 Related Work In addition to the works mentioned in Section 2, we introduce another set of related studies in this section. Evans (2004), Ortega-Mendoza et al. (2007), and Sang (2007) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst (1992). However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms </context>
</contexts>
<marker>Evans, 2004</marker>
<rawString>Richard Evans. 2004. A framework for named entity recognition in the open domain. Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003, 260:267–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiji Fu</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
</authors>
<title>Exploiting multiple sources for open-domain hypernym discovery.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1224--1234</pages>
<contexts>
<context position="2096" citStr="Fu et al. (2013)" startWordPosition="301" endWordPosition="304">dog.” Conversely, “dog” is a hyponym of “canine.” As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications. However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming. Therefore, many researchers ∗Email correspondence. Figure 1: An example of semantic hierarchy construction. have attempted to automatically extract semantic relations or to construct taxonomies. A major challenge for this task is the automatic discovery of hypernym-hyponym relations. Fu et al. (2013) propose a distant supervision method to extract hypernyms for entities from multiple sources. The output of their model is a list of hypernyms for a given enity (left panel, Figure 1). However, there usually also exists hypernym–hyponym relations among these hypernyms. For instance, “植 物 (plant)” and “毛茛科 (Ranunculaceae)” are both hypernyms of the entity “乌头 (aconit),” and “植 物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some previo</context>
<context position="3995" citStr="Fu et al., 2013" startWordPosition="605" endWordPosition="608">edicinal plant 乌头属 Aconitum 生物 organism 乌头 aconite 1199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Besides, distributional similarity methods (Kotlerman et al., 2010; Lenci and Benotto, 2012) are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used. However, it is not always rational. Our previous method based on web mining (Fu et al., 2013) works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics. Moreover, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for semantic hierarchy construction based on word embeddings. Word embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and realvalued vectors. Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between wor</context>
<context position="8456" citStr="Fu et al., 2013" startWordPosition="1297" endWordPosition="1300">ng, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts. Kotlerman et al. (2010) design a directional distributional measure to infer hypernym–hyponym relations based on the standard IR Average Precision evaluation measure. Lenci and Benotto (2012) propose another measure focusing on the contexts that hypernyms do not share with their hyponyms. However, broader semantics may not always infer broader contexts. For example, for terms “Obama’ and 1200 “American people”, it is hard to say whose contexts are broader. Our previous work (Fu et al., 2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources. We assume that the hypernyms of an entity co-occur with it frequently. It works well for named entities. But for class names (e.g., singers in Hong Kong, tropical fruits) with wider range of meanings, this assumption may fail. In this paper, we aim to identify hypernym– hyponym relations using word embeddings, which have been shown to preserve good properties for capturing semantic relationship between words. 3 Method In this section, we first define the task formally. Then we elaborate on our pro</context>
<context position="20857" citStr="Fu et al. (2013)" startWordPosition="3391" endWordPosition="3394">ut 780 million words). The Chinese segmentation is provided by the open-source Chinese language processing platform LTP5 (Che et al., 2010). Then, we employ the Skip-gram method (Section 3.2) to train word embeddings. Finally we obtain the embedding vectors of 0.56 million words. The training data for projection learning is collected from CilinE (Section 3.3.3). We obtain 15,247 word pairs of hypernym–hyponym relations (9,288 for direct relations and 5,959 for indirect relations). For evaluation, we collect the hypernyms for 418 entities, which are selected randomly from Baidubaike, following Fu et al. (2013). We then ask two annotators to manually label the semantic hierarchies of the correct hypernyms. The final data set contains 655 unique hypernyms and 1,391 hypernym–hyponym relations among them. We randomly split the labeled data into 1/5 for development and 4/5 for testing (Table 2). The hierarchies are represented as relations of pairwise words. We measure the inter-annotator agreement using the kappa coefficient (Siegel and Castellan Jr, 1988). The kappa value is 0.96, which indicates a good strength of agreement. 4.2 Evaluation Metrics We use precision, recall, and F-score as our metrics </context>
<context position="24252" citStr="Fu et al. (2013)" startWordPosition="3922" endWordPosition="3925"> in Table 3. 5.2.1 Overall Comparison MWiki+CilinE refers to the manually-built hierarchy extension method of Suchanek et al. (2008). In our experiment, we use the category taxonomy of Chinese Wikipedia6 to extend CilinE. Table 3 shows that this method achieves a high precision but also a low recall, mainly because of the limited scope of Wikipedia. MPattern refers to the pattern-based method of Hearst (1992). We extract hypernym–hyponym relations in the Baidubaike corpus, which is also used to train word embeddings (Section 4.1). We use the Chinese Hearst-style patterns (Table 4) proposed by Fu et al. (2013), in which w represents a word, and h represents one of its hypernyms. The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners. In the same corpus, we apply the method MSnow originally proposed by Snow et al. (2005). The same training data for projections learn6dumps.wikimedia.org/zhwiki/20131205/ ing from CilinE (Section 3.3.3) is used as seed hypernym–hyponym pairs. Lexico-syntactic patterns are extracted from the Baidubaike </context>
<context position="25574" citStr="Fu et al., 2013" startWordPosition="4140" endWordPosition="4143">nize hypernym–hyponym relations. This method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee. We re-implement two previous distributional methods MbalApinc (Kotlerman et al., 2010) and MinvCL (Lenci and Benotto, 2012) in the Baidubaike corpus. Each word is represented as a feature vector in which each dimension is the PMI value of the word and its context words. We compute a score for each word pair and apply a threshold to identify whether it is a hypernym–hyponym relation. MFu refers to our previous web mining method (Fu et al., 2013). This method mines hypernyms of a given word w from multiple sources and returns a ranked list of the hypernyms. We select the hypernyms with scores over a threshold of each word in the test set for evaluation. This method assumes that frequent co-occurrence of a noun or noun phrase n in multiple sources with w indicate possibility of n being a hypernym of w. The results presented in Fu et al. (2013) show that the method works well when w is an entity, but not when w is a word with a common semantic concept. The main reason may be that there are relatively more introductory pages about entiti</context>
<context position="28932" citStr="Fu et al., 2013" startWordPosition="4716" endWordPosition="4719">the highest precision but has a low recall. By contrast, our method can discover more hypernym– hyponym relations with some loss of precision, thereby achieving a more than 29% F-score improvement. The combination of these two methods achieves a further 4.5% F-score improvement over MEmb+CilinE. Generally speaking, the proposed method greatly improves the recall but damages the precision. Actually, we can get different precisions and reFigure 8: An example for error analysis. The red paths refer to the relations between the named entity and its hypernyms extracted using the web mining method (Fu et al., 2013). The black paths with hollow arrows denote the relations identified by the different methods. The boxes with dotted borders refer to the concepts which are not linked to correct positions. calls by adjusting the threshold δ (Equation 3). Figure 7 shows that MEmb+CilinE achieves a higher precision than MWiki+CilinE when their recalls are the same. When they achieve the same precision, the recall of MEmb+CilinE is higher. 5.3 Error Analysis and Discussion We analyze error cases after experiments. Some cases are shown in Figure 8. We can see that there is only one general relation “植物 (plant)” �</context>
</contexts>
<marker>Fu, Qin, Liu, 2013</marker>
<rawString>Ruiji Fu, Bing Qin, and Ting Liu. 2013. Exploiting multiple sources for open-domain hypernym discovery. In EMNLP, pages 1224–1234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>107--114</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32196" citStr="Geffet and Dagan, 2005" startWordPosition="5254" endWordPosition="5257">hod to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Soche</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Maayan Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 107–114. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguisticsVolume 2,</booktitle>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3087" citStr="Hearst, 1992" startWordPosition="462" endWordPosition="463"> 物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some previous works extend and refine manually-built semantic hierarchies by using other resources (e.g., Wikipedia) (Suchanek et al., 2008). However, the coverage is limited by the scope of the resources. Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances (Hearst, 1992; Snow et al., 2005). 1In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages. 生物 organism 药品 medicine 植物 plant 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 乌头 aconite 植物 plant 药品 medicine 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 生物 organism 乌头 aconite 1199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Besides, distributional similarity methods (Kotlerman </context>
<context position="7046" citStr="Hearst (1992)" startWordPosition="1079" endWordPosition="1080">m–hyponym relations are absent in a Chinese semantic thesaurus. Therefore, a broader range of resources is needed to supplement the manually built resources. In the construction of the famous ontology YAGO, Suchanek et al. (2008) link the categories in Wikipedia onto WordNet. However, the coverage is still limited by the scope of Wikipedia. Several other methods are based on lexical patterns. They use manually or automatically constructed lexical patterns to mine hypernym– hyponym relations from text corpora. A hierarchy can then be built based on these pairwise relations. The pioneer work by Hearst (1992) has found out that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2.” Snow et al. (2005) propose to automatically extract large numbers of lexico-syntactic patterns and subsequently detect hypernym relations from a large newswire corpus. Their method relies on accurate syntactic parsers, and the quality of the automatically extracted patterns is difficult to guarantee. Generally speaking, these pattern-based methods often suffer from low recall or precision because o</context>
<context position="24048" citStr="Hearst (1992)" startWordPosition="3891" endWordPosition="3892">hods in the test set. Table 4: Chinese Hearst-style lexical patterns. The contents in square brackets are omissible. based on patterns, word distributions, and web mining (Section 2). Results are shown in Table 3. 5.2.1 Overall Comparison MWiki+CilinE refers to the manually-built hierarchy extension method of Suchanek et al. (2008). In our experiment, we use the category taxonomy of Chinese Wikipedia6 to extend CilinE. Table 3 shows that this method achieves a high precision but also a low recall, mainly because of the limited scope of Wikipedia. MPattern refers to the pattern-based method of Hearst (1992). We extract hypernym–hyponym relations in the Baidubaike corpus, which is also used to train word embeddings (Section 4.1). We use the Chinese Hearst-style patterns (Table 4) proposed by Fu et al. (2013), in which w represents a word, and h represents one of its hypernyms. The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners. In the same corpus, we apply the method MSnow originally proposed by Snow et al. (2005). The same t</context>
<context position="31328" citStr="Hearst (1992)" startWordPosition="5120" endWordPosition="5121">tum 生物 organism 1206 error statistics show that when the cosine similarities of word pairs are greater than 0.8, the recall is only 9.5%. This kind of error accounted for about 10.9% among all the errors in our test set. One possible solution may be adding more data of this kind to the training set. 6 Related Work In addition to the works mentioned in Section 2, we introduce another set of related studies in this section. Evans (2004), Ortega-Mendoza et al. (2007), and Sang (2007) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst (1992). However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show,</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguisticsVolume 2, pages 539–545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="3699" citStr="Kotlerman et al., 2010" startWordPosition="548" endWordPosition="552">arst, 1992; Snow et al., 2005). 1In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages. 生物 organism 药品 medicine 植物 plant 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 乌头 aconite 植物 plant 药品 medicine 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 生物 organism 乌头 aconite 1199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Besides, distributional similarity methods (Kotlerman et al., 2010; Lenci and Benotto, 2012) are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used. However, it is not always rational. Our previous method based on web mining (Fu et al., 2013) works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics. Moreover, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for semantic hierarchy construction</context>
<context position="7983" citStr="Kotlerman et al. (2010)" startWordPosition="1223" endWordPosition="1226">ect hypernym relations from a large newswire corpus. Their method relies on accurate syntactic parsers, and the quality of the automatically extracted patterns is difficult to guarantee. Generally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns. The distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms. For distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts. Kotlerman et al. (2010) design a directional distributional measure to infer hypernym–hyponym relations based on the standard IR Average Precision evaluation measure. Lenci and Benotto (2012) propose another measure focusing on the contexts that hypernyms do not share with their hyponyms. However, broader semantics may not always infer broader contexts. For example, for terms “Obama’ and 1200 “American people”, it is hard to say whose contexts are broader. Our previous work (Fu et al., 2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources. We assume that the hypernyms </context>
<context position="25212" citStr="Kotlerman et al., 2010" startWordPosition="4073" endWordPosition="4076">MSnow originally proposed by Snow et al. (2005). The same training data for projections learn6dumps.wikimedia.org/zhwiki/20131205/ ing from CilinE (Section 3.3.3) is used as seed hypernym–hyponym pairs. Lexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds. We then develop a logistic regression classifier based on the patterns to recognize hypernym–hyponym relations. This method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee. We re-implement two previous distributional methods MbalApinc (Kotlerman et al., 2010) and MinvCL (Lenci and Benotto, 2012) in the Baidubaike corpus. Each word is represented as a feature vector in which each dimension is the PMI value of the word and its context words. We compute a score for each word pair and apply a threshold to identify whether it is a hypernym–hyponym relation. MFu refers to our previous web mining method (Fu et al., 2013). This method mines hypernyms of a given word w from multiple sources and returns a ranked list of the hypernyms. We select the hypernyms with scores over a threshold of each word in the test set for evaluation. This method assumes that f</context>
<context position="32045" citStr="Kotlerman et al. (2010)" startWordPosition="5232" endWordPosition="5235">cision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods.</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Giulia Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>75--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3725" citStr="Lenci and Benotto, 2012" startWordPosition="553" endWordPosition="556"> 2005). 1In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages. 生物 organism 药品 medicine 植物 plant 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 乌头 aconite 植物 plant 药品 medicine 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 生物 organism 乌头 aconite 1199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Besides, distributional similarity methods (Kotlerman et al., 2010; Lenci and Benotto, 2012) are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used. However, it is not always rational. Our previous method based on web mining (Fu et al., 2013) works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics. Moreover, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for semantic hierarchy construction based on word embeddings.</context>
<context position="8151" citStr="Lenci and Benotto (2012)" startWordPosition="1246" endWordPosition="1249">icult to guarantee. Generally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns. The distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms. For distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts. Kotlerman et al. (2010) design a directional distributional measure to infer hypernym–hyponym relations based on the standard IR Average Precision evaluation measure. Lenci and Benotto (2012) propose another measure focusing on the contexts that hypernyms do not share with their hyponyms. However, broader semantics may not always infer broader contexts. For example, for terms “Obama’ and 1200 “American people”, it is hard to say whose contexts are broader. Our previous work (Fu et al., 2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources. We assume that the hypernyms of an entity co-occur with it frequently. It works well for named entities. But for class names (e.g., singers in Hong Kong, tropical fruits) with wider range of meanin</context>
<context position="25249" citStr="Lenci and Benotto, 2012" startWordPosition="4079" endWordPosition="4082"> al. (2005). The same training data for projections learn6dumps.wikimedia.org/zhwiki/20131205/ ing from CilinE (Section 3.3.3) is used as seed hypernym–hyponym pairs. Lexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds. We then develop a logistic regression classifier based on the patterns to recognize hypernym–hyponym relations. This method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee. We re-implement two previous distributional methods MbalApinc (Kotlerman et al., 2010) and MinvCL (Lenci and Benotto, 2012) in the Baidubaike corpus. Each word is represented as a feature vector in which each dimension is the PMI value of the word and its context words. We compute a score for each word pair and apply a threshold to identify whether it is a hypernym–hyponym relation. MFu refers to our previous web mining method (Fu et al., 2013). This method mines hypernyms of a given word w from multiple sources and returns a ranked list of the hypernyms. We select the hypernyms with scores over a threshold of each word in the test set for evaluation. This method assumes that frequent co-occurrence of a noun or no</context>
<context position="32074" citStr="Lenci and Benotto (2012)" startWordPosition="5237" endWordPosition="5240">n. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been su</context>
</contexts>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 75–79. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="4619" citStr="Mikolov et al., 2013" startWordPosition="701" endWordPosition="704">ks well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics. Moreover, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for semantic hierarchy construction based on word embeddings. Word embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and realvalued vectors. Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al., 2013b). For example, v(king) − v(queen) ≈ v(man) − v(woman), where v(w) is the embedding of the word w. We observe that a similar property also applies to the hypernym–hyponym relationship (Section 3.3), which is the main inspiration of the present study. However, we further observe that hypernym– hyponym relations are more complicated than a single offset can represent. To address this challenge, we propose a more sophisticated and general method — learning a linear projection which maps words to their hypernyms (Section 3.3.1). Furthermore, we propose a piecewise linear projection method based o</context>
<context position="10099" citStr="Mikolov et al., 2013" startWordPosition="1576" endWordPosition="1579">ym relations are asymmetric and transitive when words are unambiguous: • Vx, y E L: xH−→y ==�. _,(yH−→x) • Vx, y, z E L : (xH−→z ∧ zH−→y) ==�. x H −→y Here, L denotes the list of hypernyms. x, y and z denote the hypernyms in L. We use H −→ to represent a hypernym–hyponym relation in this paper. Actually, x, y and z are unambiguous as the hypernyms of a certain entity. Therefore, G should be a directed acyclic graph (DAG). 3.2 Word Embedding Training Various models for learning word embeddings have been proposed, including neural net language models (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b) and spectral models (Dhillon et al., 2011). More recently, Mikolov et al. (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. These two models can be trained very efficiently on a largescale corpus because of their low time complexity. No. Examples 1 v(虾) − v(对虾) ≈ v(鱼) − v(金鱼) v(shrimp) − v(prawn) ≈ v(fish) − v(gold fish) 2 v(工人) − v(木 FE) ≈ v(演员) − v(小丑) v(laborer) − v(carpenter) ≈ v(actor) − v(clown) 3 v(工人) − v(木 FE) ≈6 v(鱼) − v(金鱼) v(laborer) − v(carpenter) ≈6 v(fish) − v(gold fish) Table 1: Embedding offsets on a sample of</context>
<context position="11465" citStr="Mikolov et al. (2013" startWordPosition="1803" endWordPosition="1806">ationship among words. Therefore, we employ the Skip-gram model for estimating word embeddings in this study. The Skip-gram model adopts log-linear classifiers to predict context words given the current word w(t) as input. First, w(t) is projected to its embedding. Then, log-linear classifiers are employed, taking the embedding as input and predict w(t)’s context words within a certain range, e.g. k words in the left and k words in the right. After maximizing the log-likelihood over the entire dataset using stochastic gradient descent (SGD), the embeddings are learned. 3.3 Projection Learning Mikolov et al. (2013b) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations. Looking at the well-known example: v(king) − v(queen) Pz� v(man) − v(woman), it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs. We observe that the same property also applies to some hypernym–hyponym relations. As a preliminary experiment, we compute the embedding offsets between some randomly sampled hypernym–hyponym word pairs and measure their similarities. The results are shown in Table </context>
<context position="33039" citStr="Mikolov et al. (2013" startWordPosition="5391" endWordPosition="5394">ts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym–hyponym relationships within different clusters. 7 Conclusion and Future Work This paper proposes a novel method for semantic hierarchy construction based on word embedding</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACLHLT,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="4619" citStr="Mikolov et al., 2013" startWordPosition="701" endWordPosition="704">ks well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics. Moreover, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for semantic hierarchy construction based on word embeddings. Word embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and realvalued vectors. Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al., 2013b). For example, v(king) − v(queen) ≈ v(man) − v(woman), where v(w) is the embedding of the word w. We observe that a similar property also applies to the hypernym–hyponym relationship (Section 3.3), which is the main inspiration of the present study. However, we further observe that hypernym– hyponym relations are more complicated than a single offset can represent. To address this challenge, we propose a more sophisticated and general method — learning a linear projection which maps words to their hypernyms (Section 3.3.1). Furthermore, we propose a piecewise linear projection method based o</context>
<context position="10099" citStr="Mikolov et al., 2013" startWordPosition="1576" endWordPosition="1579">ym relations are asymmetric and transitive when words are unambiguous: • Vx, y E L: xH−→y ==�. _,(yH−→x) • Vx, y, z E L : (xH−→z ∧ zH−→y) ==�. x H −→y Here, L denotes the list of hypernyms. x, y and z denote the hypernyms in L. We use H −→ to represent a hypernym–hyponym relation in this paper. Actually, x, y and z are unambiguous as the hypernyms of a certain entity. Therefore, G should be a directed acyclic graph (DAG). 3.2 Word Embedding Training Various models for learning word embeddings have been proposed, including neural net language models (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b) and spectral models (Dhillon et al., 2011). More recently, Mikolov et al. (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. These two models can be trained very efficiently on a largescale corpus because of their low time complexity. No. Examples 1 v(虾) − v(对虾) ≈ v(鱼) − v(金鱼) v(shrimp) − v(prawn) ≈ v(fish) − v(gold fish) 2 v(工人) − v(木 FE) ≈ v(演员) − v(小丑) v(laborer) − v(carpenter) ≈ v(actor) − v(clown) 3 v(工人) − v(木 FE) ≈6 v(鱼) − v(金鱼) v(laborer) − v(carpenter) ≈6 v(fish) − v(gold fish) Table 1: Embedding offsets on a sample of</context>
<context position="11465" citStr="Mikolov et al. (2013" startWordPosition="1803" endWordPosition="1806">ationship among words. Therefore, we employ the Skip-gram model for estimating word embeddings in this study. The Skip-gram model adopts log-linear classifiers to predict context words given the current word w(t) as input. First, w(t) is projected to its embedding. Then, log-linear classifiers are employed, taking the embedding as input and predict w(t)’s context words within a certain range, e.g. k words in the left and k words in the right. After maximizing the log-likelihood over the entire dataset using stochastic gradient descent (SGD), the embeddings are learned. 3.3 Projection Learning Mikolov et al. (2013b) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations. Looking at the well-known example: v(king) − v(queen) Pz� v(man) − v(woman), it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs. We observe that the same property also applies to some hypernym–hyponym relations. As a preliminary experiment, we compute the embedding offsets between some randomly sampled hypernym–hyponym word pairs and measure their similarities. The results are shown in Table </context>
<context position="33039" citStr="Mikolov et al. (2013" startWordPosition="5391" endWordPosition="5394">ts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym–hyponym relationships within different clusters. 7 Conclusion and Future Work This paper proposes a novel method for semantic hierarchy construction based on word embedding</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Proceedings of NAACLHLT, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1266" citStr="Miller, 1995" startWordPosition="180" endWordPosition="181">be used to measure the semantic relationship between words. We identify whether a candidate word pair has hypernym–hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74%, outperforms the state-of-theart methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve Fscore to 80.29%. 1 Introduction Semantic hierarchies are natural ways to organize knowledge. They are the main components of ontologies or semantic thesauri (Miller, 1995; Suchanek et al., 2008). In the WordNet hierarchy, senses are organized according to the “is-a” relations. For example, “dog” and “canine” are connected by a directed edge. Here, “canine” is called a hypernym of “dog.” Conversely, “dog” is a hyponym of “canine.” As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications. However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming. Therefore, many researchers ∗Email correspondence. Figure 1: An example</context>
<context position="6206" citStr="Miller, 1995" startWordPosition="948" endWordPosition="949">their hypernym hierarchies, which is the first dataset for this task as far as we know. The experimental results show that our method achieves an F-score of 73.74% which significantly outperforms the previous state-of-the-art methods. Moreover, combining our method with the manually-built hierarchy extension method proposed by Suchanek et al. (2008) can further improve F-score to 80.29%. 2 Background As main components of ontologies, semantic hierarchies have been studied by many researchers. Some have established concept hierarchies based on manually-built semantic resources such as WordNet (Miller, 1995). Such hierarchies have good structures and high accuracy, but their coverage is limited to fine-grained concepts (e.g., “Ranunculaceae” is not included in WordNet.). We have made similar obsevation that about a half of hypernym–hyponym relations are absent in a Chinese semantic thesaurus. Therefore, a broader range of resources is needed to supplement the manually built resources. In the construction of the famous ontology YAGO, Suchanek et al. (2008) link the categories in Wikipedia onto WordNet. However, the coverage is still limited by the scope of Wikipedia. Several other methods are base</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="10077" citStr="Mnih and Hinton, 2008" startWordPosition="1572" endWordPosition="1575">lations. Hypernym-hyponym relations are asymmetric and transitive when words are unambiguous: • Vx, y E L: xH−→y ==�. _,(yH−→x) • Vx, y, z E L : (xH−→z ∧ zH−→y) ==�. x H −→y Here, L denotes the list of hypernyms. x, y and z denote the hypernyms in L. We use H −→ to represent a hypernym–hyponym relation in this paper. Actually, x, y and z are unambiguous as the hypernyms of a certain entity. Therefore, G should be a directed acyclic graph (DAG). 3.2 Word Embedding Training Various models for learning word embeddings have been proposed, including neural net language models (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b) and spectral models (Dhillon et al., 2011). More recently, Mikolov et al. (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. These two models can be trained very efficiently on a largescale corpus because of their low time complexity. No. Examples 1 v(虾) − v(对虾) ≈ v(鱼) − v(金鱼) v(shrimp) − v(prawn) ≈ v(fish) − v(gold fish) 2 v(工人) − v(木 FE) ≈ v(演员) − v(小丑) v(laborer) − v(carpenter) ≈ v(actor) − v(clown) 3 v(工人) − v(木 FE) ≈6 v(鱼) − v(金鱼) v(laborer) − v(carpenter) ≈6 v(fish) − v(gold fish) Table 1: Embedding </context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosa M Ortega-Mendoza</author>
<author>Luis Villase˜nor-Pineda</author>
<author>Manuel Montes-y G´omez</author>
</authors>
<title>Using lexical patterns for extracting hyponyms from the web.</title>
<date>2007</date>
<booktitle>In MICAI 2007: Advances in Artificial Intelligence,</booktitle>
<pages>904--911</pages>
<publisher>Springer.</publisher>
<marker>Ortega-Mendoza, Villase˜nor-Pineda, G´omez, 2007</marker>
<rawString>Rosa M Ortega-Mendoza, Luis Villase˜nor-Pineda, and Manuel Montes-y G´omez. 2007. Using lexical patterns for extracting hyponyms from the web. In MICAI 2007: Advances in Artificial Intelligence, pages 904–911. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>What is this, anyway: Automatic hypernym discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read,</booktitle>
<pages>88--93</pages>
<contexts>
<context position="31699" citStr="Ritter et al. (2009)" startWordPosition="5177" endWordPosition="5180"> we introduce another set of related studies in this section. Evans (2004), Ortega-Mendoza et al. (2007), and Sang (2007) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst (1992). However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyp</context>
</contexts>
<marker>Ritter, Soderland, Etzioni, 2009</marker>
<rawString>Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009. What is this, anyway: Automatic hypernym discovery. In Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read, pages 88–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>HP Ritzema</author>
</authors>
<title>Drainage principles and applications.</title>
<date>1994</date>
<contexts>
<context position="15126" citStr="Ritzema, 1994" startWordPosition="2399" endWordPosition="2400">n the training data. This is a typical linear regression problem. The only difference is that our predictions are multi-dimensional vectors instead of scalar values. We use SGD for optimization. 2Principal Component Analysis (PCA) is applied for dimensionality reduction. 3.3.2 Piecewise Linear Projections A uniform linear projection may still be underrepresentative for fitting all of the hypernym– hyponym word pairs, because the relations are rather diverse, as shown in Figure 2. To better model the various kinds of hypernym–hyponym relations, we apply the idea of piecewise linear regression (Ritzema, 1994) in this study. Specifically, the input space is first segmented into several regions. That is, all word pairs (x, y) in the training data are first clustered into several groups, where word pairs in each group are expected to exhibit similar hypernym–hyponym relations. Each word pair (x, y) is represented with their vector offsets: y − x for clustering. The reasons are twofold: (1) Mikolov’s work has shown that the vector offsets imply a certain level of semantic relationship. (2) The vector offsets distribute in clusters well, and the word pairs which are close indeed represent similar relat</context>
</contexts>
<marker>Ritzema, 1994</marker>
<rawString>HP Ritzema. 1994. Drainage principles and applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
</authors>
<title>Extracting hypernym pairs from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>165--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31200" citStr="Sang (2007)" startWordPosition="5097" endWordPosition="5098">lant 乌头属 Aconitum 植物 plant 毛茛科 Ranunculaceae 药品 medicine 乌头属 Aconitum 植物药 medicinal plant 植物 plant 毛茛科 Ranunculaceae 乌头属 Aconitum 生物 organism 1206 error statistics show that when the cosine similarities of word pairs are greater than 0.8, the recall is only 9.5%. This kind of error accounted for about 10.9% among all the errors in our test set. One possible solution may be adding more data of this kind to the training set. 6 Related Work In addition to the works mentioned in Section 2, we introduce another set of related studies in this section. Evans (2004), Ortega-Mendoza et al. (2007), and Sang (2007) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst (1992). However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support v</context>
</contexts>
<marker>Sang, 2007</marker>
<rawString>Erik Tjong Kim Sang. 2007. Extracting hypernym pairs from the web. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 165–168. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N John Castellan Jr</author>
</authors>
<title>Nonparametric statistics for the behavioral sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<marker>Siegel, Jr, 1988</marker>
<rawString>Sidney Siegel and N John Castellan Jr. 1988. Nonparametric statistics for the behavioral sciences. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery. In</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 17,</booktitle>
<pages>1297--1304</pages>
<editor>Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3107" citStr="Snow et al., 2005" startWordPosition="464" endWordPosition="467"> also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some previous works extend and refine manually-built semantic hierarchies by using other resources (e.g., Wikipedia) (Suchanek et al., 2008). However, the coverage is limited by the scope of the resources. Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances (Hearst, 1992; Snow et al., 2005). 1In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages. 生物 organism 药品 medicine 植物 plant 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 乌头 aconite 植物 plant 药品 medicine 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 生物 organism 乌头 aconite 1199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Besides, distributional similarity methods (Kotlerman et al., 2010; Lenci </context>
<context position="7263" citStr="Snow et al. (2005)" startWordPosition="1115" endWordPosition="1118">nek et al. (2008) link the categories in Wikipedia onto WordNet. However, the coverage is still limited by the scope of Wikipedia. Several other methods are based on lexical patterns. They use manually or automatically constructed lexical patterns to mine hypernym– hyponym relations from text corpora. A hierarchy can then be built based on these pairwise relations. The pioneer work by Hearst (1992) has found out that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2.” Snow et al. (2005) propose to automatically extract large numbers of lexico-syntactic patterns and subsequently detect hypernym relations from a large newswire corpus. Their method relies on accurate syntactic parsers, and the quality of the automatically extracted patterns is difficult to guarantee. Generally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns. The distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms. For distributional similarity computing, each word is repres</context>
<context position="24636" citStr="Snow et al. (2005)" startWordPosition="3991" endWordPosition="3994">-based method of Hearst (1992). We extract hypernym–hyponym relations in the Baidubaike corpus, which is also used to train word embeddings (Section 4.1). We use the Chinese Hearst-style patterns (Table 4) proposed by Fu et al. (2013), in which w represents a word, and h represents one of its hypernyms. The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners. In the same corpus, we apply the method MSnow originally proposed by Snow et al. (2005). The same training data for projections learn6dumps.wikimedia.org/zhwiki/20131205/ ing from CilinE (Section 3.3.3) is used as seed hypernym–hyponym pairs. Lexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds. We then develop a logistic regression classifier based on the patterns to recognize hypernym–hyponym relations. This method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee. We re-implement two previous distributional methods MbalApinc (Kotlerman et al., 2010) and MinvCL (Lenci and B</context>
<context position="31532" citStr="Snow et al., 2005" startWordPosition="5147" endWordPosition="5150">rors in our test set. One possible solution may be adding more data of this kind to the training set. 6 Related Work In addition to the works mentioned in Section 2, we introduce another set of related studies in this section. Evans (2004), Ortega-Mendoza et al. (2007), and Sang (2007) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst (1992). However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributiona</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1297–1304. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>801--808</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="32493" citStr="Snow et al. (2006)" startWordPosition="5309" endWordPosition="5312">tes that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 801–808, Sydney, Australia, July. Association for Computational Linguistics. 1208</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="32765" citStr="Socher et al., 2011" startWordPosition="5349" endWordPosition="5352">ethods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we im</context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Ng. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32765" citStr="Socher et al., 2011" startWordPosition="5349" endWordPosition="5352">ethods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we im</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2008</date>
<pages>6--3</pages>
<contexts>
<context position="1290" citStr="Suchanek et al., 2008" startWordPosition="182" endWordPosition="185">sure the semantic relationship between words. We identify whether a candidate word pair has hypernym–hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74%, outperforms the state-of-theart methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve Fscore to 80.29%. 1 Introduction Semantic hierarchies are natural ways to organize knowledge. They are the main components of ontologies or semantic thesauri (Miller, 1995; Suchanek et al., 2008). In the WordNet hierarchy, senses are organized according to the “is-a” relations. For example, “dog” and “canine” are connected by a directed edge. Here, “canine” is called a hypernym of “dog.” Conversely, “dog” is a hyponym of “canine.” As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications. However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming. Therefore, many researchers ∗Email correspondence. Figure 1: An example of semantic hierarchy c</context>
<context position="2825" citStr="Suchanek et al., 2008" startWordPosition="420" endWordPosition="423">f their model is a list of hypernyms for a given enity (left panel, Figure 1). However, there usually also exists hypernym–hyponym relations among these hypernyms. For instance, “植 物 (plant)” and “毛茛科 (Ranunculaceae)” are both hypernyms of the entity “乌头 (aconit),” and “植 物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some previous works extend and refine manually-built semantic hierarchies by using other resources (e.g., Wikipedia) (Suchanek et al., 2008). However, the coverage is limited by the scope of the resources. Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances (Hearst, 1992; Snow et al., 2005). 1In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages. 生物 organism 药品 medicine 植物 plant 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 乌头 aconite 植物 plant 药品 medicine 毛茛科 Ranunculaceae 植物药 medicinal plant 乌头属 Aconitum 生物 organism 乌头 aco</context>
<context position="5944" citStr="Suchanek et al. (2008)" startWordPosition="909" endWordPosition="912">tify whether an unknown word pair is a hypernym–hyponym relation using the projections (Section 3.4). To the best of our knowledge, we are the first to apply word embeddings to this task. For evaluation, we manually annotate a dataset containing 418 Chinese entities and their hypernym hierarchies, which is the first dataset for this task as far as we know. The experimental results show that our method achieves an F-score of 73.74% which significantly outperforms the previous state-of-the-art methods. Moreover, combining our method with the manually-built hierarchy extension method proposed by Suchanek et al. (2008) can further improve F-score to 80.29%. 2 Background As main components of ontologies, semantic hierarchies have been studied by many researchers. Some have established concept hierarchies based on manually-built semantic resources such as WordNet (Miller, 1995). Such hierarchies have good structures and high accuracy, but their coverage is limited to fine-grained concepts (e.g., “Ranunculaceae” is not included in WordNet.). We have made similar obsevation that about a half of hypernym–hyponym relations are absent in a Chinese semantic thesaurus. Therefore, a broader range of resources is need</context>
<context position="23768" citStr="Suchanek et al. (2008)" startWordPosition="3842" endWordPosition="3845">41 60.61 73.20 MPattern 97.47 21.41 35.11 MSnow 60.88 25.67 36.11 MbalApinc 54.96 53.38 54.16 MinvCL 49.63 62.84 55.46 MFu 87.40 48.19 62.13 MEmb 80.54 67.99 73.74 MEmb+CilinE 80.59 72.42 76.29 MEmb+Wiki+CilinE 79.78 80.81 80.29 Table 3: Comparison of the proposed method with existing methods in the test set. Table 4: Chinese Hearst-style lexical patterns. The contents in square brackets are omissible. based on patterns, word distributions, and web mining (Section 2). Results are shown in Table 3. 5.2.1 Overall Comparison MWiki+CilinE refers to the manually-built hierarchy extension method of Suchanek et al. (2008). In our experiment, we use the category taxonomy of Chinese Wikipedia6 to extend CilinE. Table 3 shows that this method achieves a high precision but also a low recall, mainly because of the limited scope of Wikipedia. MPattern refers to the pattern-based method of Hearst (1992). We extract hypernym–hyponym relations in the Baidubaike corpus, which is also used to train word embeddings (Section 4.1). We use the Chinese Hearst-style patterns (Table 4) proposed by Fu et al. (2013), in which w represents a word, and h represents one of its hypernyms. The result shows that only a small part of th</context>
<context position="27799" citStr="Suchanek et al., 2008" startWordPosition="4524" endWordPosition="4527">alled w h[,] such as w h[,] especially w 1205 P(%) R(%) F(%) MWiki+CilinE 80.39 19.29 31.12 MEmb+CilinE 71.16 52.80 60.62 MEmb+Wiki+CilinE 69.13 61.65 65.17 Table 5: Performance on the out-of-CilinE data in the test set. Recall Figure 7: Precision-Recall curves on the out-ofCilinE data in the test set. When the relation yH---+z from MCilinE is added, it will cause a new incorrect relation x-+z. Combining MEmb with MWiki+CilinE achieves a 7% F-score improvement over the best baseline MWiki+CilinE. Therefore, the proposed method is complementary to the manually-built hierarchy extension method (Suchanek et al., 2008). 5.2.2 Comparison on the Out-of-CilinE Data We are greatly interested in the practical performance of the proposed method on the hypernym– hyponym relations outside of CilinE. We say a word pair is outside of CilinE, as long as there is one word in the pair not existing in CilinE. In our test data, about 62% word pairs are outside of CilinE. Table 5 shows the performances of the best baseline method and our method on the outof-CilinE data. The method exploiting the taxonomy in Wikipedia, MWiki+CilinE, achieves the highest precision but has a low recall. By contrast, our method can discover mo</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
</authors>
<title>Instance-based evaluation of entailment rule acquisition.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>456--463</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="32240" citStr="Szpektor et al., 2007" startWordPosition="5262" endWordPosition="5265"> to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity</context>
</contexts>
<marker>Szpektor, Shnarch, Dagan, 2007</marker>
<rawString>Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456–463, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32873" citStr="Turian et al., 2010" startWordPosition="5365" endWordPosition="5368">). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym–hypony</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1015</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="32172" citStr="Weeds et al., 2004" startWordPosition="5250" endWordPosition="5253">) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), par</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of the 20th international conference on Computational Linguistics, page 1015. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>