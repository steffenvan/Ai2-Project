<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010784">
<title confidence="0.991643">
Sequential Model Selection for Word Sense Disambiguation *
</title>
<author confidence="0.996434">
Ted Pedersent and Rebecca Brucet and Janyce Wiebef
</author>
<affiliation confidence="0.977264">
Department of Computer Science and Engineering
Southern Methodist University, Dallas, TX 75275
IDepartment of Computer Science
New Mexico State University, Las Cruces, NM 88003
</affiliation>
<email confidence="0.784984">
pedersenOseas.smu.edu, rbruceOseas.smu.edu, wiebeOcs.nmsu.edu
</email>
<sectionHeader confidence="0.997803" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998715625">
Statistical models of word—sense disam-
biguation are often based on a small num-
ber of contextual features or on a model
that is assumed to characterize the inter-
actions among a set of features. Model
selection is presented as an alternative to
these approaches, where a sequential search
of possible models is conducted in order to
find the model that best characterizes the
interactions among features. This paper
expands existing model selection method-
ology and presents the first comparative
study of model selection search strategies
and evaluation criteria when applied to the
problem of building probabilistic classifiers
for word—sense disambiguation.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998434952380952">
In this paper word—sense disambiguation is cast as
a problem in supervised learning, where a classifier
is induced from a corpus of sense—tagged text. Sup-
pose there is a training sample where each sense—
tagged sentence is represented by the feature vari-
ables (F1, , Fn_1, S). Selected contextual proper-
ties of the sentence are represented by (F1, , F, —1)
and the sense of the ambiguous word is represented
by S. Our task is to induce a classifier that will
predict the value of S given an untagged sentence
represented by the contextual feature variables.
We adopt a statistical approach whereby a prob-
abilistic model is selected that describes the inter-
actions among the feature variables. Such a model
can form the basis of a probabilistic classifier since
it specifies the probability of observing any and all
combinations of the values of the feature variables.
Suppose our training sample has N sense—tagged
sentences. There are q possible combinations of val-
ues for the n feature variables, where each such com-
bination is represented by a feature vector. Let
</bodyText>
<footnote confidence="0.980568">
*This research was supported by the Office of Naval
Research under grant number N00014-95-1-0776.
</footnote>
<bodyText confidence="0.999779">
fi and 0, be the frequency and probability of ob-
serving the feature vector, respectively. Then
(1&apos;, , fq) has a multinomial distribution with pa-
rameters (N,91, . ,0q). The 0 parameters, i.e., the
joint parameters, define the joint probability distri-
bution of the feature variables. These are the pa-
rameters of the fully saturated model, the model in
which the value of each variable directly affects the
values of all the other variables. These parameters
can be estimated as maximum likelihood estimates
(MLEs), such that the estimate of Oi, Oi, is
For these estimates to be reliable, each of the q
possible combinations of feature values must occur
in the training sample. This is unlikely for NLP data
samples, which are often sparse and highly skewed
(c.f., e.g. (Pedersen et al., 1996) and (Zipf, 1935)).
However, if the data sample can be adequately
characterized by a less complex model, i.e., a model
in which there are fewer interactions between vari-
ables, then more reliable parameter estimates can be
obtained: In the case of decomposable models (Dar-
roch et al., 1980; see below), the parameters of a less
complex model are parameters of marginal distribu-
tions, so the MLEs involve frequencies of combina-
tions of values of only subsets of the variables in the
model. How well a model characterizes the train-
ing sample is determined by measuring the fit of the
model to the sample, i.e., how well the distribution
defined by the model matches the distribution ob-
served in the training sample.
A good strategy for developing probabilistic clas-
sifiers is to perform an explicit model search to se-
lect the model to use in classification. This pa-
per presents the results of a comparative study of
search strategies and evaluation criteria for measur-
ing model fit. We restrict the selection process to the
class of decomposable models (Darroch et al., 1980),
since restricting model search to this class has many
computational advantages.
We begin with a short description of decompos-
able models (in section 2). Search strategies (in sec-
tion 3) and model evaluation (in section 4) are de-
scribed next, followed by the results of an extensive
disambiguation experiment involving 12 ambiguous
</bodyText>
<page confidence="0.997562">
388
</page>
<bodyText confidence="0.9998865">
words (in sections 5 and 6). We discuss related work
(in section 7) and close with recommendations for
search strategy and evaluation criterion when select-
ing models for word—sense disambiguation.
</bodyText>
<sectionHeader confidence="0.964376" genericHeader="method">
2 Decomposable Models
</sectionHeader>
<bodyText confidence="0.999798571428571">
Decomposable models are a subset of the class
of graphical models (Whittaker, 1990) which are
in turn a subset of the class of log-linear models
(Bishop et al., 1975). Familiar examples of decom-
posable models are Naive Bayes and n-gram models.
They are characterized by the following properties
(Bruce and Wiebe, 1994b):
</bodyText>
<listItem confidence="0.934343">
1. In a graphical model, variables are either inter-
dependent or conditionally independent of one
another.&apos; All graphical models have a graphi-
cal representation such that each variable in the
model is mapped to a node in the graph, and
there is an undirected edge between each pair
of nodes corresponding to interdependent vari-
ables. The sets of completely connected nodes
(i.e., cliques) correspond to sets of interdepen-
dent variables. Any two nodes that are not di-
rectly connected by an edge are conditionally
independent given the values of the nodes on
the path that connects them.
2. Decomposable models are those graphical mod-
els that express the joint distribution as the
product of the marginal distributions of the
variables in the maximal cliques of the graphical
representation, scaled by the marginal distribu-
tions of variables common to two or more of
these maximal sets. Because their joint distri-
butions have such closed-form expressions, the
parameters can be estimated directly from the
training data without the need for an iterative
fitting procedure (as is required, for example, to
estimate the parameters of maximum entropy
models; (Berger et al., 1996)).
3. Although there are far fewer decomposable
models than log-linear models for a given set of
feature variables, it has been shown that they
have substantially the same expressive power
(Whittaker, 1990).
</listItem>
<bodyText confidence="0.9997426">
The joint parameter estimate is the
probability that the feature vector (11, f, f s,) will
be observed in a training sample where each ob-
servation is represented by the feature variables
(F1, F2, F3, ,5). Suppose that the graphical represen-
tation of a decomposable model is defined by the two
cliques (i.e., marginals) (F1, S) and (F2, F3, S). The
frequencies of these marginals, f (Fi = f, S = si)
and f(F2 = 12, F3 = 13, S = Si), are sufficient
statistics in that they provide enough information
</bodyText>
<equation confidence="0.9911485">
1F2 and F5 are conditionally independent given S if
P(F2IF5,S) = p(F2IS).
</equation>
<bodyText confidence="0.9994562">
to calculate maximum likelihood estimates of the
model parameters. MLEs of the model parameters
are simply the marginal frequencies normalized by
the sample size N. The joint parameter estimate is
formulated as a normalized product:
</bodyText>
<equation confidence="0.9992632">
f(Fi=fi,S=s,) f(F2=f3,F3=f3,S=s,)
01&amp;quot;F2F3S = X
--
f1of2,13,3.
(1)
</equation>
<bodyText confidence="0.99986725">
Rather than having to observe the complete fea-
ture vector (fi, f2, f3, si) in the training sample to
estimate the joint parameter, it is only necessary to
observe the marginals (fi , si) and (12,13,81).
</bodyText>
<sectionHeader confidence="0.986708" genericHeader="method">
3 Model Search Strategies
</sectionHeader>
<bodyText confidence="0.996537325581395">
The search strategies presented in this paper are
backward sequential search (BSS) and forward se-
quential search (FSS). Sequential searches evaluate
models of increasing (FSS) or decreasing (BSS) lev-
els of complexity, where complexity is defined by the
number of interactions among the feature variables
(i.e., the number of edges in the graphical represen-
tation of the model).
A backward sequential search (BSS) begins by
designating the saturated model as the current
model. A saturated model has complexity level
= n(n2-1),
where n is the number of feature vari-
ables. At each stage in BSS we generate the set of
decomposable models of complexity level i — 1 that
can be created by removing an edge from the cur-
rent model of complexity level i. Each member of
this set is a hypothesized model and is judged by
the evaluation criterion to determine which model
results in the least degradation in fit from the cur-
rent model—that model becomes the current model
and the search continues. The search stops when ei-
ther (1) every hypothesized model results in an un-
acceptably high degradation in fit or (2) the current
model has a complexity level of zero.
A forward sequential search (FSS) begins by des-
ignating the model of independence as the current
model. The model of independence has complexity
level i = 0 since there are no interactions among the
feature variables. At each stage in FSS we generate
the set of decomposable models of complexity level
i + 1 that can be created by adding an edge to the
current model of complexity level i. Each member of
this set is a hypothesized model and is judged by the
evaluation criterion to determine which model re-
sults in the greatest improvement in fit from the cur-
rent model—that model becomes the current model
and the search continues. The search stops when
either (1) every hypothesized model results in an
unacceptably small increase in fit or (2) the current
model is saturated.
For sparse samples FSS is a natural choice since
early in the search the models are of low complexity.
</bodyText>
<equation confidence="0.712887">
/(S=s,)
</equation>
<page confidence="0.987224">
389
</page>
<bodyText confidence="0.999681888888889">
The number of model parameters is small and they
have more reliable estimated values. On the other
hand, BSS begins with a saturated model whose pa-
rameter estimates are known to be unreliable.
During both BSS and FSS, model selection also
performs feature selection. If a model is selected
where there is no edge connecting a feature variable
to the classification variable then that feature is not
relevant to the classification being performed.
</bodyText>
<sectionHeader confidence="0.969613" genericHeader="method">
4 Model Evaluation Criteria
</sectionHeader>
<bodyText confidence="0.99935425">
Evaluation criteria fall into two broad classes, signifi-
cance tests and information criteria. This paper con-
siders two significance tests, the exact conditional
test (Kreiner, 1987) and the Log—likelihood ratio
statistic G2 (Bishop et al., 1975), and two informa-
tion criteria, Akaike&apos;s Information Criterion (AIC)
(Akaike, 1974) and the Bayesian Information Crite-
rion (BIC) (Schwarz, 1978).
</bodyText>
<subsectionHeader confidence="0.998264">
4.1 Significance tests
</subsectionHeader>
<bodyText confidence="0.937745">
The Log-likelihood ratio statistic G2 is defined as:
</bodyText>
<subsectionHeader confidence="0.990383">
4.2 Information criteria
</subsectionHeader>
<bodyText confidence="0.999855">
The family of model evaluation criteria known as
information criteria have the following expression:
</bodyText>
<equation confidence="0.950913">
/CK, = G2 — x dof (3)
</equation>
<bodyText confidence="0.999012571428571">
where G2 and dof are defined above. Members of
this family are distinguished by their different values
of K. AIC corresponds to K = 2. BIC corresponds
to K = log(N), where N is the sample size.
The various information criteria are an alterna-
tive to using a pre-defined significance level (a) to
judge the acceptability of a model. AIC and BIC re-
ward good model fit and penalize models with large
numbers of parameters. The parameter penalty is
expressed as K x dof, where the size of the penalty
is the adjusted degrees of freedom, and the weight
of the penalty is controlled by K.
During BSS the hypothesized model with the
largest negative IC, value is selected as the cur-
rent model of complexity level i — 1, while during
FSS the hypothesized model with the largest pos-
itive ./C„ value is selected as the current model of
complexity level i 1. The search stops when the
IC„ values for all hypothesized models are greater
than zero in the case of BSS, or less than zero in the
case of FSS.
</bodyText>
<equation confidence="0.919034">
G2 E x log Tel (2) 5 Experimental Data
</equation>
<bodyText confidence="0.991610375">
where fi and ei are the observed and expected counts
of the Oh feature vector, respectively. The observed
count fi is simply the frequency in the training sam-
ple. The expected count ei is calculated from the
frequencies in the training data assuming that the
hypothesized model, i.e., the model generated in the
search, adequately fits the sample. The smaller the
value of G2 the better the fit of the hypothesized
model.
The distribution of G2 is asymptotically approx-
imated by the x2 distribution (G2 )(2) with ad-
justed degrees of freedom (dof) equal to the number
of model parameters that have non-zero estimates
given the training sample. The significance of a
model is equal to the probability of observing its
reference G2 in the x2 distribution with appropriate
dof. A hypothesized model is accepted if the signif-
icance (i.e., probability) of its reference G2 value is
greater than, in the case of FSS, or less than, in the
case of BSS, some pre—determined cutoff, a.
An alternative to using a x2 approximation is to
define the exact conditional distribution of G2. The
exact conditional distribution of G2 is the distribu-
tion of G2 values that would be observed for com-
parable data samples randomly generated from the
model being tested. The significance of G2 based on
the exact conditional distribution does not rely on an
asymptotic approximation and is accurate for sparse
and skewed data samples (Pedersen et al.. 1996)
The sense—tagged text and feature set used in
these experiments are the same as in (Bruce et al.,
1996). The text consists of every sentence from the
ACL/DCI Wall Street Journal corpus that contains
any of the nouns interest, bill, concern, and drug,
any of the verbs close, help, agree, and include, or
any of the adjectives chief, public, last, and common.
The extracted sentences have been hand—tagged
with senses defined in the Longman Dictionary of
Contemporary English (LDOCE). There are be-
tween 800 and 3,000 sense—tagged sentences for each
of the 12 words. This data was randomly divided
into training and test samples at a 10:1 ratio.
A sentence with an ambiguous word is represented
by a feature set with three types of contextual fea-
ture variables:2 (1) The morphological feature (E)
indicates if an ambiguous noun is plural or not. For
verbs it indicates the tense of the verb. This feature
is not used for adjectives. (2) The POS features
have one of 25 possible POS tags, derived from the
first letter of the tags in the ACL/DCI WSJ cor-
pus. There are four POS feature variables repre-
senting the POS of the two words immediately pre-
ceding (Li, L2) and following (Ri , R2) the ambigu-
ous word. (3) The three binary collocation-specific
features (C1, C2, C3) indicate if a particular word oc-
curs in a sentence with an ambiguous word.
</bodyText>
<footnote confidence="0.989031">
2An alternative feature set for this data is utilized
with an exemplar-based learning algorithm in (Ng and
Lee, 1996).
</footnote>
<page confidence="0.995459">
390
</page>
<bodyText confidence="0.999876333333333">
The sparse nature of our data can be illustrated
by interest. There are 6 possible values for the sense
variable. Combined with the other feature variables
this results in 37,500,000 possible feature vectors (or
joint parameters). However, we have a training sam-
ple of only 2,100 instances.
</bodyText>
<sectionHeader confidence="0.998118" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999740833333334">
In total, eight different decomposable models were
selected via a model search for each of the 12 words.
Each of the eight models is due to a different com-
bination of search strategy and evaluation criterion.
Two additional classifiers were evaluated to serve as
benchmarks. The default classifier assigns every in-
stance of an ambiguous word with its most frequent
sense in the training sample. The Naive Bayes clas-
sifier uses a model that assumes that each contex-
tual feature variable is conditionally independent of
all other contextual variables given the value of the
sense variable.
</bodyText>
<subsectionHeader confidence="0.988192">
6.1 Accuracy comparison
</subsectionHeader>
<bodyText confidence="0.982611172413793">
The accuracy3 of each of these classifiers for each
of the 12 words is shown in Figure 1. The highest
accuracy for each word is in bold type while any ac-
curacies less than the default classifier are italicized.
The complexity of the model selected is shown in
parenthesis. For convenience, we refer to model se-
lection using, for example, a search strategy of FSS
and the evaluation criterion AIC as FSS AIC.
Overall AIC selects the most accurate models dur-
ing both BSS and FSS. BSS AIC finds the most ac-
curate model for 6 of 12 words while FSS AIC finds
the most accurate for 4 of 12 words. BSS BIC and
the Naive Bayes find the most accurate model for 3
of 12 words. Each of the other combinations finds
the most most accurate model for 2 of 12 words ex-
cept for FSS exact conditional which never finds the
most accurate model.
Neither AIC nor BIC ever selects a model that
results in accuracy less than the default classifier.
However, FSS exact conditional has accuracy less
than the default for 6 of 12 words and BSS exact
conditional has accuracy less than the default for 3
of 12 words. BSS G2 X2 and FSS G2 X2 have
less than default accuracy for 2 of 12 and 1 of 12
words, respectively.
The accuracy of the significance tests vary greatly
depending on the choice of a. Of the various a values
that were tested, .01, .05, .001, and .0001, the value
of .0001 was found to produce the most accurate
models. Other values of a will certainly led to other
results. The information criteria do not require the
setting of any such cut-off values.
A low complexity model that results in high accu-
racy disambiguation is the ultimate goal. Figure 1
&apos;The percentage of ambiguous words in a held out
test sample that are disambiguated correctly.
shows that BIC and G2 — X2 select lower complexity
models than either AIC or the exact conditional test.
However, both appear to sacrifice accuracy when
compared to AIC. BIC assesses a greater parame-
ter penalty (ic = log(N)) than does AIC (K = 2),
causing BSS BIC to remove more interactions than
BSS AIC. Likewise, FSS BIC adds fewer interactions
than FSS AIC. In both cases BIC selects models
whose complexity is too low and adversely affects
accuracy when compared to AIC.
The Naive Bayes classifier achieves a high level
of accuracy using a model of low complexity. In
fact, while the Naive Bayes classifier is most accu-
rate for only 3 of the 12 words, the average accu-
racy of the Naive Bayes classifiers for all 12 words
is higher than the average classification accuracy re-
sulting from any combination of the search strategies
and evaluation criteria. The average complexity of
the Naive Bayes models is also lower than the av-
erage complexity of the models resulting from any
combination of the search strategies and evaluation
criteria except BSS BIC and FSS BIC.
</bodyText>
<subsectionHeader confidence="0.999792">
6.2 Search strategy and accuracy
</subsectionHeader>
<bodyText confidence="0.9999149">
An evaluation criterion that finds models of simi-
lar accuracy using either BSS or FSS is to be pre-
ferred over one that does not. Overall the infor-
mation criteria are not greatly affected by a change
in the search strategy, as illustrated in Figure 3.
Each point on this plot represents the accuracy of
the models selected for a word by the same evalua-
tion criterion using BSS and FSS. If this point falls
close to the line BSS = FSS then there is little
or no difference between the accuracy of the models
selected during FSS and BSS.
AIC exhibits only minor deviation from BSS =
FSS. This is also illustrated by the fact that the
average accuracy between BSS AIC and FSS AIC
only differs by .0013. The significance tests, espe-
cially the exact conditional, are more affected by
the search strategy. It is clear that BSS exact condi-
tional is much more accurate than FSS exact condi-
tional. FSS G2 X2 is slightly more accurate than
BSS G2 — x2.
</bodyText>
<subsectionHeader confidence="0.982436">
6.3 Feature selection: interest
</subsectionHeader>
<bodyText confidence="0.994334230769231">
Figure 2 shows the models selected by the various
combinations of search strategy and evaluation cri-
terion for interest.
During BSS, AIC removed feature L2 from the
model, BIC removed L1, L2, R1 and R2, G2 X2
removed no features, and the exact conditional test
removed C2. During FSS, AIC never added R2, BIC
never added C1, C3, L1, L2 and R2, and G2 ^- X2 and
the exact conditional test added all the features.
G2 — X2 is the most consistent of the evaluation
criteria in feature selection. During both BSS and
FSS it found that all the features were relevant to
classification.
</bodyText>
<page confidence="0.996311">
391
</page>
<table confidence="0.9966435">
Default Naive Search G2 -- X2 exact AIC BIC
Bayes a = .0001 a = .0001
agree .7660 .9362 (8) BSS .8936 (8) .9149 (10) .9220 (15) .9433 (9)
FSS .9291 (12) .9007 (15) .9362 (13) .9433 (7)
bill .7090 .8657 (8) BSS .6567 (22) .6194 (25) .8507 (26) .8806 (7)
FSS .7985 (20) .6866 (28) .8582 (20) .8433 (11)
chief .8750 .9643 (7) BSS .9464 (6) .9196 (17) .9643 (14) .9554 (6)
FSS .9464 (6) .9196 (18) .9643 (14) .9643 (7)
close .6815 .8344 (8) BSS .7580 (12) .7516 (13) .8408 (13) .7580 (3)
FSS .7898 (13) .7006 (19) .8408 (10) .7580 (3)
common .8696 .9130 (7) BSS .9217 (4) .8696 (10) .8957 (7) .8783 (2)
FSS .9217 (4) .7391 (16) .8957 (7) .8783 (2)
concern .6510 .8725 (8) BSS .8255 (5) .7651 (15) .8389 (16) .7181 (6)
FSS .8255 (17) .7047 (24) .8255 (13) .8389 (9)
drug .6721 .8279 (8) BSS .8115 (10) .8443 (7) .8443 (14) .7787 (9)
FSS .8115 (10) .5164 (19) .8115 (12) .7787 (9)
help .7266 .7698 (8) BSS .7410 (7) .7698 (6) .7914 (6) .7554 (4)
FSS .7554 (3) .7770 (9) .7914 (4) .7554 (4)
include .9325 .9448 (8) BSS .9571 (6) .9571 (3) .9387 (16) .9387 (8)
FSS .9571 (6) .7423 (22) .9448 (9) .9325 (9)
interest .5205 .7336 (8) BSS .6885 (24) .4959 (24) .7418 (21) .6311 (6)
FSS .7172 (22) .4590 (32) .7336 (15) .6926 (4)
last .9387 .9264 (7) BSS .9080 (8) .8865 (9) .9417 .9417 (9)
FSS .8804 .8466 (18) .9417 (14) .9387 (2)
public .5056 .5843 (7) BSS .5393 (7) .5393 (9) .5169 (8) .5506 (3)
FSS .5281 (6) .5506 (11) .5281 (6) .5506 (3)
average .7373 .8477 (8) BSS .8039 (10) .7778 (12) .8406 (14) .8108 (6)
FSS .8217 (11) .7119 (19) .8393 (11) .8229 (6)
</table>
<figureCaption confidence="0.862993">
Figure 1: Accuracy comparison
</figureCaption>
<table confidence="0.9995334">
Criterion Search Model
G2 &apos;&apos;&apos; X2 BSS (Ci ELiL2S)(Ci C2C3Li L2S)(Ci C2C3RiS)
FSS R1R2SK2 C3L1L2S)(C3R1R2S)
rEL1L2sr
Exact BSS C1EL 1 L2S Ci Li L2R1R2S)(C3Li L2Ri R2S)
FSS (Ci EL1L2R1R2S)(C3L 1 L2Ri R2S)(C2EL 1 L2R1 R2S)
(C1C2C3EL1S)(CiC3R1S)(C1C3R2S)
(EL1L2S)(C2EL2S)(Ci Ri S)(C3L 1 S)(C3RIS)
(C2ES)(C1C3S)
(C2ES)(R1 S)
AIC BSS
FSS
BIC BSS
FSS
Naive Bayes none (C1S)(C2S)(C3S)(ES)(Li S)(L2S)(RIS)(R2S)
</table>
<figureCaption confidence="0.976724">
Figure 2: Models selected: interest
</figureCaption>
<page confidence="0.921847">
392
</page>
<figure confidence="0.999266444444445">
1
0.9
0.8
FSS 0.7
0.6
0.5
0.4
0.4 0.5 0.6 0.7 0.8 0.9 1
BSS
</figure>
<figureCaption confidence="0.999985">
Figure 3: Effect of Search Strategy
</figureCaption>
<bodyText confidence="0.9999725">
AIC found seven features to be relevant in both
BSS and FSS. When using AIC, the only difference
in the feature set selected during FSS as compared
to that selected during BSS is the part of speech
feature that is found to be irrelevant: during BSS L2
is removed and during FSS R2 is never added. All
other criteria exhibit more variation between FSS
and BSS in feature set selection.
</bodyText>
<subsectionHeader confidence="0.999492">
6.4 Model selection: interest
</subsectionHeader>
<bodyText confidence="0.999897894736842">
Here we consider the results of each stage of the
sequential model selection for inierest. Figures 4
through 7 show the accuracy and recall&apos; for the
best fitting model at each level of complexity in the
search. The rightmost point on each plot for each
evaluation criterion is the measure associated with
the model ultimately selected.
These plots illustrate that BSS BIC selects mod-
els of too low complexity. In Figure 4 BSS BIC has
&amp;quot;gone past&amp;quot; much more accurate models than the
one it selected. We observe the related problem for
FSS BIC. In Figure 6 FSS BIC adds too few in-
teractions and does not select as accurate a model
as FSS AIC. The exact conditional test suffers from
the reverse problem of BIC. BSS exact conditional
removes only a few interactions while FSS exact con-
ditional adds many interactions, and in both cases
the resulting models have poor accuracy.
The difference between BSS and FSS is clearly il-
</bodyText>
<footnote confidence="0.9334504">
4The percentage of ambiguous words in a held out test
sample that are disambiguated, correctly or not. A word
is not disambiguated if the model parameters needed to
assign a sense tag cannot be estimated from the training
sample.
</footnote>
<bodyText confidence="0.999217533333334">
lustrated by these plots. AIC and BIC eliminate in-
teractions that have high dof&apos;s (and thus have large
numbers of parameters) much earlier in BSS than
the significance tests. This rapid reduction in the
number of parameters results in a rapid increases
in accuracy (Figure 4) and recall for AIC and BIC
(Figure 5) relative to the significance tests as they
produce models with smaller numbers of parameters
that can be estimated more reliably.
However, during the early stages of FSS the num-
ber of parameters in the models is very small and the
differences between the information criteria and the
significance tests are minimized. The major differ-
ence among the criteria in Figures 6 and 7 is that the
exact conditional test adds many more interactions.
</bodyText>
<sectionHeader confidence="0.999915" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999733947368421">
Statistical analysis of NLP data has often been lim-
ited to the application of standard models, such
as n-gram (Markov chain) models and the Naive
Bayes model. While n-grams perform well in part—
of—speech tagging and speech processing, they re-
quire a fixed interdependency structure that is inap-
propriate for the broad class of contextual features
used in word—sense disambiguation. However, the
Naive Bayes classifier has been found to perform
well for word—sense disambiguation both here and
in a variety of other works (e.g., (Bruce and Wiebe,
1994a), (Gale et al., 1992), (Leacock et al., 1993),
and (Mooney, 1996)).
In order to utilize models with more complicated
interactions among feature variables, (Bruce and
Wiebe, 1994b) introduce the use of sequential model
selection and decomposable models for word—sense
disambiguation.&apos;
Alternative probabilistic approaches have involved
using a single contextual feature to perform disam-
biguation (e.g., (Brown et al., 1991), (Dagan et al.,
1991), and (Yarowsky, 1993) present techniques for
identifying the optimal feature to use in disambigua-
tion). Maximum Entropy models have been used to
express the interactions among multiple feature vari-
ables (e.g., (Berger et al., 1996)), but within this
framework no systematic study of interactions has
been proposed. Decision tree induction has been
applied to word-sense disambiguation (e.g. (Black,
1988) and (Mooney, 1996)) but, while it is a type of
model selection, the models are not parametric.
&apos;They recommended a model selection procedure us-
ing BSS and the exact conditional test in combination
with a test for model predictive power. In their proce-
dure, the exact conditional test was used to guide the
generation of new models and the test of model predic-
tive power was used to select the final model from among
those generated during the search.
</bodyText>
<figure confidence="0.96971062962963">
&apos;exact&apos; 0 I
Gz x2 +
_ AICD
BIC x
BSS=FSS • • • - x
393
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0 5 10 15 20 25 30 35
# of interactions in model
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
35 30 25 20 15 10 5
# of interactions in model
</figure>
<figureCaption confidence="0.995202">
Figure 4: BSS accuracy: interest Figure 6: FSS accuracy: interest
</figureCaption>
<figure confidence="0.997285375">
&apos;46k4t•Afkis4,e,
AIC -A-
BIC 4-
- Exact a = .0001
G2 „ x2 a = .0001 •0&amp;quot;
I I I I I I I
35 30 25 20 15 10 5 0
# of interactions in model
</figure>
<figureCaption confidence="0.990757">
Figure 5: BSS recall: interest
</figureCaption>
<figure confidence="0.986839333333333">
0.3
0 5 10 15 20 25 30 35
# of interactions in model
</figure>
<figureCaption confidence="0.998051">
Figure 7: FSS recall: interest
</figureCaption>
<figure confidence="0.9990312">
1
0.9
0.8
0.7
0.6
0.5 AIC -A-
BIC 4-
- Exact a = .0001 -
G2 „ x2
= .000i .0 •
0.3 i
0.4
tte,
1
0.9
0.8
0.7
0.6
0.5
0.4
</figure>
<page confidence="0.996086">
394
</page>
<sectionHeader confidence="0.998479" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9941298">
Sequential model selection is a viable means of
choosing a probabilistic model to perform word-
sense disambiguation. We recommend AIC as the
evaluation criterion during model selection due to
the following:
</bodyText>
<listItem confidence="0.974856666666667">
1. It is difficult to set an appropriate cutoff value
(a) for a significance test.
2. The information criteria AIC and BIC are more
robust to changes in search strategy.
3. BIC removes too many interactions and results
in models of too low complexity.
</listItem>
<bodyText confidence="0.999342470588235">
The choice of search strategy when using AIC is
less critical than when using significance tests. How-
ever, we recommend FSS for sparse data (NLP data
is typically sparse) since it reduces the impact of very
high degrees of freedom and the resultant unreliable
parameter estimates on model selection.
The Naive Bayes classifier is based on a low com-
plexity model that is shown to lead to high accuracy.
If feature selection is not in doubt (i.e., it is fairly
certain that all of the features are somehow relevant
to classification) then this is a reasonable approach.
However, if some features are of questionable value
the Naive Bayes model will continue to utilize them
while sequential model selection will disregard them.
All of the search strategies and evaluation crite-
ria discussed are implemented in the public domain
program CoCo (Badsberg, 1995).
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998960671052631">
H. Akaike. 1974. A new look at the statistical model
identification. IEEE Transactions on Automatic
Control, AC-19(6):716-723.
J. Badsberg. 1995. An Environment for Graphical
Models. Ph.D. thesis, Aalborg University.
A. Berger, S. Della Pietra, and V. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39-71.
Y. Bishop, S. Fienberg, and P. Holland. 1975.
Discrete Multivariate Analysis. The MIT Press,
Cambridge, MA.
E. Black. 1988. An experiment in computational
discrimination of English word senses. IBM Jour-
nal of Research and Development, 32(2):185-194.
P. Brown, S. Della Pietra, and R. Mercer. 1991.
Word sense disambiguation using statistical meth-
ods. In Proceedings of the 29th Annual Meeting
of the Association for Computational Linguistics,
pages 264-304.
It. Bruce and J. Wiebe. 1994a. A new approach
to word sense disambiguation. In Proceedings of
the ARPA Workshop on Human Language Tech-
nology, pages 244-249.
R. Bruce and J. Wiebe. 1994b. Word-sense dis-
ambiguation using decomposable models. In Pro-
ceedings of the 32nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 139-
146.
. Bruce, J. Wiebe, and T. Pedersen. 1996. The
measure of a model. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 101-112.
Dagan, A. Itai, and U. Schwall. 1991. Two lan-
guages are more informative than one. In Proceed-
ings of the 29th Annual Meeting of the Association
for Computational Linguistics, pages 130-137.
Darroch, S. Lauritzen, and T. Speed. 1980.
Markov fields and log-linear interaction models
for contingency tables. The Annals of Statistics,
8(3):522-539.
W. Gale, K. Church, and D. Yarowsky. 1992. A
method for disambiguating word senses in a large
corpus. Computers and the Humanities, 26:415-
439.
S. Kreiner. 1987. Analysis of multidimensional con-
tingency tables by exact conditional tests: Tech-
niques and strategies. Scandinavian Journal of
Statistics, 14:97-112.
C. Leacock, G. Towell, and E. Voorhees. 1993.
Corpus-based statistical sense resolution. In Pro-
ceedings of the ARPA Workshop on Human Lan-
guage Technology.
R. Mooney. 1996. Comparative experiments on dis-
ambiguating word senses: An illustration of the
role of bias in machine learning. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
H.T. Ng and H.B. Lee. 1996. Integrating multi-
ple knowledge sources to disambiguate word sense:
An exemplar-based approach. In Proceedings of
the nth Annual Meeting of the Society for Com-
putational Linguistics, pages 40-47.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Sig-
nificant lexical relationships. In Proceedings of
the 13th National Conference on Artificial Intelli-
gence, pages 455-460.
G. Schwarz. 1978. Estimating the dimension of a
model. The Annals of Statistics, 6(2):461-464.
J. Whittaker. 1990. Graphical Models in Applied
Multivariate Statistics. John Wiley, New York.
D. Yarowsky. 1993. One sense per collocation. In
Proceedings of the ARPA Workshop on Human
Language Technology, pages 266-271.
G. Zipf. 1935. The Psycho-Biology of Language.
Houghton Mifflin, Boston, MA.
</reference>
<page confidence="0.999114">
395
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520380">
<title confidence="0.994056">Sequential Model Selection for Word Sense Disambiguation *</title>
<author confidence="0.992052">Pedersent Brucet Wiebef</author>
<affiliation confidence="0.996329">Department of Computer Science and Engineering</affiliation>
<address confidence="0.553888">Methodist University, 75275</address>
<affiliation confidence="0.980724">IDepartment of Computer Science</affiliation>
<address confidence="0.995534">New Mexico State University, Las Cruces, NM 88003</address>
<email confidence="0.999826">pedersenOseas.smu.edu,rbruceOseas.smu.edu,wiebeOcs.nmsu.edu</email>
<abstract confidence="0.999308882352941">Statistical models of word—sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word—sense disambiguation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Akaike</author>
</authors>
<title>A new look at the statistical model identification.</title>
<date>1974</date>
<journal>IEEE Transactions on Automatic Control,</journal>
<pages>19--6</pages>
<contexts>
<context position="10269" citStr="Akaike, 1974" startWordPosition="1673" endWordPosition="1674">e. During both BSS and FSS, model selection also performs feature selection. If a model is selected where there is no edge connecting a feature variable to the classification variable then that feature is not relevant to the classification being performed. 4 Model Evaluation Criteria Evaluation criteria fall into two broad classes, significance tests and information criteria. This paper considers two significance tests, the exact conditional test (Kreiner, 1987) and the Log—likelihood ratio statistic G2 (Bishop et al., 1975), and two information criteria, Akaike&apos;s Information Criterion (AIC) (Akaike, 1974) and the Bayesian Information Criterion (BIC) (Schwarz, 1978). 4.1 Significance tests The Log-likelihood ratio statistic G2 is defined as: 4.2 Information criteria The family of model evaluation criteria known as information criteria have the following expression: /CK, = G2 — x dof (3) where G2 and dof are defined above. Members of this family are distinguished by their different values of K. AIC corresponds to K = 2. BIC corresponds to K = log(N), where N is the sample size. The various information criteria are an alternative to using a pre-defined significance level (a) to judge the acceptab</context>
</contexts>
<marker>Akaike, 1974</marker>
<rawString>H. Akaike. 1974. A new look at the statistical model identification. IEEE Transactions on Automatic Control, AC-19(6):716-723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Badsberg</author>
</authors>
<title>An Environment for Graphical Models.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Aalborg University.</institution>
<marker>Badsberg, 1995</marker>
<rawString>J. Badsberg. 1995. An Environment for Graphical Models. Ph.D. thesis, Aalborg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="6109" citStr="Berger et al., 1996" startWordPosition="976" endWordPosition="979">n the path that connects them. 2. Decomposable models are those graphical models that express the joint distribution as the product of the marginal distributions of the variables in the maximal cliques of the graphical representation, scaled by the marginal distributions of variables common to two or more of these maximal sets. Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; (Berger et al., 1996)). 3. Although there are far fewer decomposable models than log-linear models for a given set of feature variables, it has been shown that they have substantially the same expressive power (Whittaker, 1990). The joint parameter estimate is the probability that the feature vector (11, f, f s,) will be observed in a training sample where each observation is represented by the feature variables (F1, F2, F3, ,5). Suppose that the graphical representation of a decomposable model is defined by the two cliques (i.e., marginals) (F1, S) and (F2, F3, S). The frequencies of these marginals, f (Fi = f, S</context>
<context position="25495" citStr="Berger et al., 1996" startWordPosition="4320" endWordPosition="4323">ey, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word—sense disambiguation.&apos; Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. (Black, 1988) and (Mooney, 1996)) but, while it is a type of model selection, the models are not parametric. &apos;They recommended a model selection procedure using BSS and the exact conditional test in combination with a test for model predictive power. In their procedure, the exact conditional test was used to guide the generation of new models and the test of model predictive power was used to select the final model from among those gene</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bishop</author>
<author>S Fienberg</author>
<author>P Holland</author>
</authors>
<title>Discrete Multivariate Analysis.</title>
<date>1975</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4780" citStr="Bishop et al., 1975" startWordPosition="764" endWordPosition="767">t description of decomposable models (in section 2). Search strategies (in section 3) and model evaluation (in section 4) are described next, followed by the results of an extensive disambiguation experiment involving 12 ambiguous 388 words (in sections 5 and 6). We discuss related work (in section 7) and close with recommendations for search strategy and evaluation criterion when selecting models for word—sense disambiguation. 2 Decomposable Models Decomposable models are a subset of the class of graphical models (Whittaker, 1990) which are in turn a subset of the class of log-linear models (Bishop et al., 1975). Familiar examples of decomposable models are Naive Bayes and n-gram models. They are characterized by the following properties (Bruce and Wiebe, 1994b): 1. In a graphical model, variables are either interdependent or conditionally independent of one another.&apos; All graphical models have a graphical representation such that each variable in the model is mapped to a node in the graph, and there is an undirected edge between each pair of nodes corresponding to interdependent variables. The sets of completely connected nodes (i.e., cliques) correspond to sets of interdependent variables. Any two n</context>
<context position="10186" citStr="Bishop et al., 1975" startWordPosition="1660" endWordPosition="1663">and, BSS begins with a saturated model whose parameter estimates are known to be unreliable. During both BSS and FSS, model selection also performs feature selection. If a model is selected where there is no edge connecting a feature variable to the classification variable then that feature is not relevant to the classification being performed. 4 Model Evaluation Criteria Evaluation criteria fall into two broad classes, significance tests and information criteria. This paper considers two significance tests, the exact conditional test (Kreiner, 1987) and the Log—likelihood ratio statistic G2 (Bishop et al., 1975), and two information criteria, Akaike&apos;s Information Criterion (AIC) (Akaike, 1974) and the Bayesian Information Criterion (BIC) (Schwarz, 1978). 4.1 Significance tests The Log-likelihood ratio statistic G2 is defined as: 4.2 Information criteria The family of model evaluation criteria known as information criteria have the following expression: /CK, = G2 — x dof (3) where G2 and dof are defined above. Members of this family are distinguished by their different values of K. AIC corresponds to K = 2. BIC corresponds to K = log(N), where N is the sample size. The various information criteria are</context>
</contexts>
<marker>Bishop, Fienberg, Holland, 1975</marker>
<rawString>Y. Bishop, S. Fienberg, and P. Holland. 1975. Discrete Multivariate Analysis. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
</authors>
<title>An experiment in computational discrimination of English word senses.</title>
<date>1988</date>
<journal>IBM Journal of Research and Development,</journal>
<pages>32--2</pages>
<contexts>
<context position="25668" citStr="Black, 1988" startWordPosition="4346" endWordPosition="4347">posable models for word—sense disambiguation.&apos; Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. (Black, 1988) and (Mooney, 1996)) but, while it is a type of model selection, the models are not parametric. &apos;They recommended a model selection procedure using BSS and the exact conditional test in combination with a test for model predictive power. In their procedure, the exact conditional test was used to guide the generation of new models and the test of model predictive power was used to select the final model from among those generated during the search. &apos;exact&apos; 0 I Gz x2 + _ AICD BIC x BSS=FSS • • • - x 393 0 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0 5 10 15 20 25 30 35 # of interactions in model 1 0.9 0.8 0.</context>
</contexts>
<marker>Black, 1988</marker>
<rawString>E. Black. 1988. An experiment in computational discrimination of English word senses. IBM Journal of Research and Development, 32(2):185-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>Word sense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>264--304</pages>
<contexts>
<context position="25241" citStr="Brown et al., 1991" startWordPosition="4281" endWordPosition="4284">word—sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word—sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word—sense disambiguation.&apos; Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. (Black, 1988) and (Mooney, 1996)) but, while it is a type of model selection, the models are not parametric. &apos;They recommended a model selection procedure using BSS and the exact conditi</context>
</contexts>
<marker>Brown, Pietra, Mercer, 1991</marker>
<rawString>P. Brown, S. Della Pietra, and R. Mercer. 1991. Word sense disambiguation using statistical methods. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 264-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce</author>
<author>J Wiebe</author>
</authors>
<title>A new approach to word sense disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>244--249</pages>
<contexts>
<context position="4931" citStr="Bruce and Wiebe, 1994" startWordPosition="787" endWordPosition="790"> by the results of an extensive disambiguation experiment involving 12 ambiguous 388 words (in sections 5 and 6). We discuss related work (in section 7) and close with recommendations for search strategy and evaluation criterion when selecting models for word—sense disambiguation. 2 Decomposable Models Decomposable models are a subset of the class of graphical models (Whittaker, 1990) which are in turn a subset of the class of log-linear models (Bishop et al., 1975). Familiar examples of decomposable models are Naive Bayes and n-gram models. They are characterized by the following properties (Bruce and Wiebe, 1994b): 1. In a graphical model, variables are either interdependent or conditionally independent of one another.&apos; All graphical models have a graphical representation such that each variable in the model is mapped to a node in the graph, and there is an undirected edge between each pair of nodes corresponding to interdependent variables. The sets of completely connected nodes (i.e., cliques) correspond to sets of interdependent variables. Any two nodes that are not directly connected by an edge are conditionally independent given the values of the nodes on the path that connects them. 2. Decompos</context>
<context position="24817" citStr="Bruce and Wiebe, 1994" startWordPosition="4222" endWordPosition="4225">he exact conditional test adds many more interactions. 7 Related Work Statistical analysis of NLP data has often been limited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in part— of—speech tagging and speech processing, they require a fixed interdependency structure that is inappropriate for the broad class of contextual features used in word—sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word—sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word—sense disambiguation.&apos; Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express </context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>It. Bruce and J. Wiebe. 1994a. A new approach to word sense disambiguation. In Proceedings of the ARPA Workshop on Human Language Technology, pages 244-249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Word-sense disambiguation using decomposable models.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="4931" citStr="Bruce and Wiebe, 1994" startWordPosition="787" endWordPosition="790"> by the results of an extensive disambiguation experiment involving 12 ambiguous 388 words (in sections 5 and 6). We discuss related work (in section 7) and close with recommendations for search strategy and evaluation criterion when selecting models for word—sense disambiguation. 2 Decomposable Models Decomposable models are a subset of the class of graphical models (Whittaker, 1990) which are in turn a subset of the class of log-linear models (Bishop et al., 1975). Familiar examples of decomposable models are Naive Bayes and n-gram models. They are characterized by the following properties (Bruce and Wiebe, 1994b): 1. In a graphical model, variables are either interdependent or conditionally independent of one another.&apos; All graphical models have a graphical representation such that each variable in the model is mapped to a node in the graph, and there is an undirected edge between each pair of nodes corresponding to interdependent variables. The sets of completely connected nodes (i.e., cliques) correspond to sets of interdependent variables. Any two nodes that are not directly connected by an edge are conditionally independent given the values of the nodes on the path that connects them. 2. Decompos</context>
<context position="24817" citStr="Bruce and Wiebe, 1994" startWordPosition="4222" endWordPosition="4225">he exact conditional test adds many more interactions. 7 Related Work Statistical analysis of NLP data has often been limited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in part— of—speech tagging and speech processing, they require a fixed interdependency structure that is inappropriate for the broad class of contextual features used in word—sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word—sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word—sense disambiguation.&apos; Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express </context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>R. Bruce and J. Wiebe. 1994b. Word-sense disambiguation using decomposable models. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 139-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe Bruce</author>
<author>T Pedersen</author>
</authors>
<title>The measure of a model.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>101--112</pages>
<marker>Bruce, Pedersen, 1996</marker>
<rawString>. Bruce, J. Wiebe, and T. Pedersen. 1996. The measure of a model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 101-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Itai Dagan</author>
<author>U Schwall</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>130--137</pages>
<marker>Dagan, Schwall, 1991</marker>
<rawString>Dagan, A. Itai, and U. Schwall. 1991. Two languages are more informative than one. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lauritzen Darroch</author>
<author>T Speed</author>
</authors>
<title>Markov fields and log-linear interaction models for contingency tables. The Annals of Statistics,</title>
<date>1980</date>
<pages>8--3</pages>
<marker>Darroch, Speed, 1980</marker>
<rawString>Darroch, S. Lauritzen, and T. Speed. 1980. Markov fields and log-linear interaction models for contingency tables. The Annals of Statistics, 8(3):522-539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--415</pages>
<contexts>
<context position="24840" citStr="Gale et al., 1992" startWordPosition="4226" endWordPosition="4229">adds many more interactions. 7 Related Work Statistical analysis of NLP data has often been limited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in part— of—speech tagging and speech processing, they require a fixed interdependency structure that is inappropriate for the broad class of contextual features used in word—sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word—sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word—sense disambiguation.&apos; Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among </context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kreiner</author>
</authors>
<title>Analysis of multidimensional contingency tables by exact conditional tests: Techniques and strategies.</title>
<date>1987</date>
<journal>Scandinavian Journal of Statistics,</journal>
<pages>14--97</pages>
<contexts>
<context position="10122" citStr="Kreiner, 1987" startWordPosition="1652" endWordPosition="1653">d they have more reliable estimated values. On the other hand, BSS begins with a saturated model whose parameter estimates are known to be unreliable. During both BSS and FSS, model selection also performs feature selection. If a model is selected where there is no edge connecting a feature variable to the classification variable then that feature is not relevant to the classification being performed. 4 Model Evaluation Criteria Evaluation criteria fall into two broad classes, significance tests and information criteria. This paper considers two significance tests, the exact conditional test (Kreiner, 1987) and the Log—likelihood ratio statistic G2 (Bishop et al., 1975), and two information criteria, Akaike&apos;s Information Criterion (AIC) (Akaike, 1974) and the Bayesian Information Criterion (BIC) (Schwarz, 1978). 4.1 Significance tests The Log-likelihood ratio statistic G2 is defined as: 4.2 Information criteria The family of model evaluation criteria known as information criteria have the following expression: /CK, = G2 — x dof (3) where G2 and dof are defined above. Members of this family are distinguished by their different values of K. AIC corresponds to K = 2. BIC corresponds to K = log(N), </context>
</contexts>
<marker>Kreiner, 1987</marker>
<rawString>S. Kreiner. 1987. Analysis of multidimensional contingency tables by exact conditional tests: Techniques and strategies. Scandinavian Journal of Statistics, 14:97-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>G Towell</author>
<author>E Voorhees</author>
</authors>
<title>Corpus-based statistical sense resolution.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="24864" citStr="Leacock et al., 1993" startWordPosition="4230" endWordPosition="4233">ctions. 7 Related Work Statistical analysis of NLP data has often been limited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in part— of—speech tagging and speech processing, they require a fixed interdependency structure that is inappropriate for the broad class of contextual features used in word—sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word—sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word—sense disambiguation.&apos; Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variabl</context>
</contexts>
<marker>Leacock, Towell, Voorhees, 1993</marker>
<rawString>C. Leacock, G. Towell, and E. Voorhees. 1993. Corpus-based statistical sense resolution. In Proceedings of the ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="24884" citStr="Mooney, 1996" startWordPosition="4235" endWordPosition="4236">stical analysis of NLP data has often been limited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in part— of—speech tagging and speech processing, they require a fixed interdependency structure that is inappropriate for the broad class of contextual features used in word—sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word—sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word—sense disambiguation.&apos; Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>R. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the nth Annual Meeting of the Society for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="14481" citStr="Ng and Lee, 1996" startWordPosition="2401" endWordPosition="2404">For verbs it indicates the tense of the verb. This feature is not used for adjectives. (2) The POS features have one of 25 possible POS tags, derived from the first letter of the tags in the ACL/DCI WSJ corpus. There are four POS feature variables representing the POS of the two words immediately preceding (Li, L2) and following (Ri , R2) the ambiguous word. (3) The three binary collocation-specific features (C1, C2, C3) indicate if a particular word occurs in a sentence with an ambiguous word. 2An alternative feature set for this data is utilized with an exemplar-based learning algorithm in (Ng and Lee, 1996). 390 The sparse nature of our data can be illustrated by interest. There are 6 possible values for the sense variable. Combined with the other feature variables this results in 37,500,000 possible feature vectors (or joint parameters). However, we have a training sample of only 2,100 instances. 6 Experimental Results In total, eight different decomposable models were selected via a model search for each of the 12 words. Each of the eight models is due to a different combination of search strategy and evaluation criterion. Two additional classifiers were evaluated to serve as benchmarks. The d</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>H.T. Ng and H.B. Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the nth Annual Meeting of the Society for Computational Linguistics, pages 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>M Kayaalp</author>
<author>R Bruce</author>
</authors>
<title>Significant lexical relationships.</title>
<date>1996</date>
<booktitle>In Proceedings of the 13th National Conference on Artificial Intelligence,</booktitle>
<pages>455--460</pages>
<contexts>
<context position="2986" citStr="Pedersen et al., 1996" startWordPosition="468" endWordPosition="471">The 0 parameters, i.e., the joint parameters, define the joint probability distribution of the feature variables. These are the parameters of the fully saturated model, the model in which the value of each variable directly affects the values of all the other variables. These parameters can be estimated as maximum likelihood estimates (MLEs), such that the estimate of Oi, Oi, is For these estimates to be reliable, each of the q possible combinations of feature values must occur in the training sample. This is unlikely for NLP data samples, which are often sparse and highly skewed (c.f., e.g. (Pedersen et al., 1996) and (Zipf, 1935)). However, if the data sample can be adequately characterized by a less complex model, i.e., a model in which there are fewer interactions between variables, then more reliable parameter estimates can be obtained: In the case of decomposable models (Darroch et al., 1980; see below), the parameters of a less complex model are parameters of marginal distributions, so the MLEs involve frequencies of combinations of values of only subsets of the variables in the model. How well a model characterizes the training sample is determined by measuring the fit of the model to the sample</context>
</contexts>
<marker>Pedersen, Kayaalp, Bruce, 1996</marker>
<rawString>T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Significant lexical relationships. In Proceedings of the 13th National Conference on Artificial Intelligence, pages 455-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Schwarz</author>
</authors>
<title>Estimating the dimension of a model. The Annals of Statistics,</title>
<date>1978</date>
<pages>6--2</pages>
<contexts>
<context position="10330" citStr="Schwarz, 1978" startWordPosition="1682" endWordPosition="1683">ature selection. If a model is selected where there is no edge connecting a feature variable to the classification variable then that feature is not relevant to the classification being performed. 4 Model Evaluation Criteria Evaluation criteria fall into two broad classes, significance tests and information criteria. This paper considers two significance tests, the exact conditional test (Kreiner, 1987) and the Log—likelihood ratio statistic G2 (Bishop et al., 1975), and two information criteria, Akaike&apos;s Information Criterion (AIC) (Akaike, 1974) and the Bayesian Information Criterion (BIC) (Schwarz, 1978). 4.1 Significance tests The Log-likelihood ratio statistic G2 is defined as: 4.2 Information criteria The family of model evaluation criteria known as information criteria have the following expression: /CK, = G2 — x dof (3) where G2 and dof are defined above. Members of this family are distinguished by their different values of K. AIC corresponds to K = 2. BIC corresponds to K = log(N), where N is the sample size. The various information criteria are an alternative to using a pre-defined significance level (a) to judge the acceptability of a model. AIC and BIC reward good model fit and penal</context>
</contexts>
<marker>Schwarz, 1978</marker>
<rawString>G. Schwarz. 1978. Estimating the dimension of a model. The Annals of Statistics, 6(2):461-464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Whittaker</author>
</authors>
<title>Graphical Models in Applied Multivariate Statistics.</title>
<date>1990</date>
<publisher>John Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="4697" citStr="Whittaker, 1990" startWordPosition="750" endWordPosition="751">el search to this class has many computational advantages. We begin with a short description of decomposable models (in section 2). Search strategies (in section 3) and model evaluation (in section 4) are described next, followed by the results of an extensive disambiguation experiment involving 12 ambiguous 388 words (in sections 5 and 6). We discuss related work (in section 7) and close with recommendations for search strategy and evaluation criterion when selecting models for word—sense disambiguation. 2 Decomposable Models Decomposable models are a subset of the class of graphical models (Whittaker, 1990) which are in turn a subset of the class of log-linear models (Bishop et al., 1975). Familiar examples of decomposable models are Naive Bayes and n-gram models. They are characterized by the following properties (Bruce and Wiebe, 1994b): 1. In a graphical model, variables are either interdependent or conditionally independent of one another.&apos; All graphical models have a graphical representation such that each variable in the model is mapped to a node in the graph, and there is an undirected edge between each pair of nodes corresponding to interdependent variables. The sets of completely connec</context>
<context position="6315" citStr="Whittaker, 1990" startWordPosition="1010" endWordPosition="1011">raphical representation, scaled by the marginal distributions of variables common to two or more of these maximal sets. Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; (Berger et al., 1996)). 3. Although there are far fewer decomposable models than log-linear models for a given set of feature variables, it has been shown that they have substantially the same expressive power (Whittaker, 1990). The joint parameter estimate is the probability that the feature vector (11, f, f s,) will be observed in a training sample where each observation is represented by the feature variables (F1, F2, F3, ,5). Suppose that the graphical representation of a decomposable model is defined by the two cliques (i.e., marginals) (F1, S) and (F2, F3, S). The frequencies of these marginals, f (Fi = f, S = si) and f(F2 = 12, F3 = 13, S = Si), are sufficient statistics in that they provide enough information 1F2 and F5 are conditionally independent given S if P(F2IF5,S) = p(F2IS). to calculate maximum likel</context>
</contexts>
<marker>Whittaker, 1990</marker>
<rawString>J. Whittaker. 1990. Graphical Models in Applied Multivariate Statistics. John Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>266--271</pages>
<contexts>
<context position="25285" citStr="Yarowsky, 1993" startWordPosition="4290" endWordPosition="4291">yes classifier has been found to perform well for word—sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word—sense disambiguation.&apos; Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. (Black, 1988) and (Mooney, 1996)) but, while it is a type of model selection, the models are not parametric. &apos;They recommended a model selection procedure using BSS and the exact conditional test in combination with a test for mod</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>D. Yarowsky. 1993. One sense per collocation. In Proceedings of the ARPA Workshop on Human Language Technology, pages 266-271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zipf</author>
</authors>
<title>The Psycho-Biology of Language.</title>
<date>1935</date>
<publisher>Houghton Mifflin,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="3003" citStr="Zipf, 1935" startWordPosition="473" endWordPosition="474">joint parameters, define the joint probability distribution of the feature variables. These are the parameters of the fully saturated model, the model in which the value of each variable directly affects the values of all the other variables. These parameters can be estimated as maximum likelihood estimates (MLEs), such that the estimate of Oi, Oi, is For these estimates to be reliable, each of the q possible combinations of feature values must occur in the training sample. This is unlikely for NLP data samples, which are often sparse and highly skewed (c.f., e.g. (Pedersen et al., 1996) and (Zipf, 1935)). However, if the data sample can be adequately characterized by a less complex model, i.e., a model in which there are fewer interactions between variables, then more reliable parameter estimates can be obtained: In the case of decomposable models (Darroch et al., 1980; see below), the parameters of a less complex model are parameters of marginal distributions, so the MLEs involve frequencies of combinations of values of only subsets of the variables in the model. How well a model characterizes the training sample is determined by measuring the fit of the model to the sample, i.e., how well </context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>G. Zipf. 1935. The Psycho-Biology of Language. Houghton Mifflin, Boston, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>