<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000605">
<title confidence="0.997751">
Tense and Aspect Error Correction for ESL Learners
Using Global Context
</title>
<author confidence="0.991003">
Toshikazu Tajiri Mamoru Komachi Yuji Matsumoto
</author>
<affiliation confidence="0.996562">
Graduate School of Information Science
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.714766">
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
</address>
<email confidence="0.988887">
{toshikazu-t, komachi, matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.982913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831952380952">
As the number of learners of English is con-
stantly growing, automatic error correction of
ESL learners’ writing is an increasingly ac-
tive area of research. However, most research
has mainly focused on errors concerning arti-
cles and prepositions even though tense/aspect
errors are also important. One of the main
reasons why tense/aspect error correction is
difficult is that the choice of tense/aspect is
highly dependent on global context. Previous
research on grammatical error correction typ-
ically uses pointwise prediction that performs
classification on each word independently, and
thus fails to capture the information of neigh-
boring labels. In order to take global infor-
mation into account, we regard the task as se-
quence labeling: each verb phrase in a doc-
ument is labeled with tense/aspect depending
on surrounding labels. Our experiments show
that the global context makes a moderate con-
tribution to tense/aspect error correction.
</bodyText>
<sectionHeader confidence="0.995159" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987261733333333">
Because of the growing number of learners of En-
glish, there is an increasing demand to help learn-
ers of English. It is highly effective for learners to
receive feedback on their essays from a human tu-
tor (Nagata and Nakatani, 2010). However, man-
ual feedback needs a lot of work and time, and it
also requires much grammatical knowledge. Thus,
a variety of automatic methods for helping English
learning and education have been proposed.
The mainstream of English error detection and
correction has focused on article errors (Knight and
Chander, 1994; Brockett et al., 2006) and preposi-
tion errors (Chodorow et al., 2007; Rozovskaya and
Roth, 2011), that commonly occur in essays by ESL
learners. On the other hand, tense and aspect errors
have been little studied, even though they are also
commonly found in learners’ essays (Lee and Sen-
eff, 2006; Bitchener et al., 2005). For instance, Lee
(2008) corrects English verb inflection errors, but
they do not deal with tense/aspect errors because the
choice of tense and aspect highly depends on global
context, which makes correction difficult. Consider
the following sentences taken from a corpus of a
Japanese learner of English.
(1) I had a good time this Summer Vacation.
First, I *go to KAIYUKAN 1 with my friends.
In this example, go in the second sentence should
be written as went. It is difficult to correct this type
of error because there are two choices for correc-
tion, namely went and will go. In this case, we
can exploit global context to determine which cor-
rection is appropriate: the first sentence describes a
past event, and the second sentence refers the first
sentence. Thus, the verb should be changed to past
tense. This deduction is easy for humans, but is dif-
ficult for machines.
One way to incorporate such global context into
tense/aspect error correction is to use a machine
learning-based sequence labeling approach. There-
fore, we regard the task as sequence labeling:
each verb phrase in the document is labeled with
tense/aspect depending on surrounding labels. This
model naturally takes global context into account.
Our experiments show that global context makes a
moderate contribution to tense/aspect correction.
</bodyText>
<footnote confidence="0.712672">
1Kaiyukan is an aquarium in Osaka, Japan.
198
</footnote>
<note confidence="0.9830895">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 198–202,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.853759" genericHeader="method">
2 Tense/Aspect Error Corpus
</sectionHeader>
<bodyText confidence="0.99993255">
Developing a high-quality tense and aspect error
correction system requires a large corpus annotated
with tense/aspect errors. However, existing anno-
tated corpora are limited in size,2 which precludes
the possibility of machine learning-based approach.
Therefore, we constructed a large-scale tense/aspect
corpus from Lang-8,3a social networking service
for learners of foreign languages. ESL learners post
their writing to be collaboratively corrected by na-
tive speakers. We leverage these corrections in creat-
ing our tense/aspect annotation. Lang-8 has 300,000
users from 180 countries worldwide, with more than
580,000 entries, approximately 170,000 of them
in English.4 After cleaning the data, the corpus
consists of approximately 120,000 English entries
containing 2,000,000 verb phrases with 750,000
verb phrases having corrections.5 The annotated
tense/aspect labels include 12 combinations of tense
(past, present, future) and aspect (nothing, perfect,
progressive, perfect progressive).
</bodyText>
<sectionHeader confidence="0.992695" genericHeader="method">
3 Error Correction Using Global Context
</sectionHeader>
<bodyText confidence="0.999998545454545">
As we described in Section 1, using only local in-
formation about the target verb phrase may lead to
inaccurate correction of tense/aspect errors. Thus,
we take into account global context: the relation be-
tween target and preceding/following verb phrases.
In this paper, we formulate the task as sequence la-
beling, and use Conditional Random Fields (Laf-
ferty, 2001), which provides state-of-the-art perfor-
mance in sequence labeling while allowing flexible
feature design for combining local and global fea-
ture sets.
</bodyText>
<subsectionHeader confidence="0.999375">
3.1 Local Features
</subsectionHeader>
<bodyText confidence="0.877767571428571">
Table 1 shows the local features used to train the er-
ror correction model.
2Konan-JIEM Learner Corpus Second Edition (http://
gsk.or.jp/catalog/GSK2011-B/catalog.html)
contains 170 essays, and Cambridge English First Certificate in
English (http://www.cambridgeesol.org/exams/
fce/index.html) contains 1244 essays.
</bodyText>
<equation confidence="0.235311">
3http://lang-8.com/
</equation>
<bodyText confidence="0.4523775">
4As of January, 2012. More details about the Lang-8 corpus
can be found in (Mizumoto et al., 2011).
5Note that not all the 750,000 verb phrases were corrected
due to the misuse of tense/aspect.
</bodyText>
<tableCaption confidence="0.98302">
Table 1: Local features for a verb phrase
</tableCaption>
<bodyText confidence="0.9996988">
name description
t-learn tense/aspect written by the learner
(surface tense/aspect)
bare the verb lemma
L the word to the left
R the word to the right
nsubj nominal subject
dobj direct object
aux auxiliary verb
pobj object of a preposition
p-tmod temporal adverb
norm-p-tmod normalized temporal adverb
advmod other adverb
conj subordinating conjunction
main-clause true if the target VP is in main clause
sub-clause true if the target VP is in subordinate clause
We use dependency relations such as nsubj, dobj,
aux, pobj, and advmod for syntactic features. If a
sentence including a target verb phrase is a complex
sentence, we use the conj feature and add either the
main-clause or the sub-clause feature depending on
whether the target verb is in the main clause or in a
subordinate clause. For example, the following two
sentences have the same features although they have
different structures.
</bodyText>
<listItem confidence="0.9966665">
(2) It pours when it rains.
(3) When it rains it pours.
</listItem>
<bodyText confidence="0.99939425">
In both sentences, we use the feature main-clause
for the verb phrase pours, and sub-clause for the
verb phrase rains along with the feature conj:when
for both verb phrases.
Regarding p-tmod, we extract a noun phrase in-
cluding a word labeled tmod (temporal adverb). For
instance, consider the following sentence containing
a temporal adverb:
</bodyText>
<listItem confidence="0.509198">
(4) I had a good time last night.
</listItem>
<bodyText confidence="0.999720571428571">
In (4), the word night is the head of the noun phrase
last night and is a temporal noun,6 so we add the
feature p-tmod:last night for the verb phrase had.
Additionally, norm-p-tmod is a normalized form
of p-tmod. Table 2 shows the value of the fea-
ture norm-p-tmod and the corresponding tempo-
ral keywords. We use norm-p-tmod when p-tmod
</bodyText>
<footnote confidence="0.829838">
6We made our own temporal noun list.
</footnote>
<page confidence="0.83006">
199
</page>
<tableCaption confidence="0.8516685">
Table 2: The value of the feature norm-p-tmod and cor-
responding temporal keywords
</tableCaption>
<table confidence="0.9943774">
temporal keywords value
yesterday or last past
now present
tomorrow or next future
today or this this
</table>
<tableCaption confidence="0.850219">
Table 3: Feature templates
</tableCaption>
<table confidence="0.906173666666666">
Local Feature Templates
&lt;head&gt; &lt;head, t-learn&gt; &lt;head, L, R&gt; &lt;L&gt; &lt;L, head&gt;
&lt;L, t-learn&gt; &lt;R&gt; &lt;R, head&gt; &lt;R, t-learn&gt; &lt;nsubj&gt;
&lt;nsubj, t-learn&gt; &lt;aux&gt; &lt;aux, head&gt; &lt;aux, t-learn&gt;
&lt;pobj&gt; &lt;pobj, t-learn&gt; &lt;norm-p-tmod&gt;
&lt;norm-p-tmod, t-learn&gt; &lt;advmod&gt; &lt;advmod, t-learn&gt;
&lt;tmod&gt; &lt;tmod, t-learn&gt; &lt;conj&gt; &lt;conj, t-learn&gt;
&lt;main-clause&gt; &lt;main-clause, t-learn&gt;
&lt;sub-clause&gt; &lt;sub-clause, t-learn&gt;
&lt;conj, main-clause&gt; &lt;conj, sub-clause&gt;
Global Context Feature Templates
&lt;p-tmod′&gt; &lt;p-tmod′, t-learn&gt; &lt;p-tmod′, t-learn′&gt;
&lt;p-tmod′, t-learn′, t-learn&gt; &lt;norm-p-tmod′&gt;
&lt;norm-p-tmod′, t-learn&gt; &lt;norm-p-tmod′, t-learn′&gt;
&lt;norm-p-tmod′, t-learn′, t-learn&gt;
</table>
<bodyText confidence="0.99941275">
includes any temporal keywords. For instance, in
the sentence (4), we identify last night as temporal
adverb representing past, and thus create a feature
time:past for the verb phrase had.
</bodyText>
<subsectionHeader confidence="0.999369">
3.2 Feature Template
</subsectionHeader>
<bodyText confidence="0.998200142857143">
Table 3 shows feature templates. &lt;a&gt; represents a
singleton feature and &lt;a, b&gt; represents a combina-
tion of features a and b. Also, a′ means the feature
a of the preceding verb phrase. A local feature tem-
plate is a feature function combining features in the
target verb phrase, and a global context feature tem-
plate is a feature function including features from a
non-target verb phrase. Suppose we have following
learner’s sentences:
(5) I went to Kyoto yesterday.
I *eat yatsuhashi7 and drank green tea.
In (5), the verb before eat is went, and p-
tmod:yesterday and norm-p-tmod:past are added
to the feature set of verb went. Accordingly,
</bodyText>
<footnote confidence="0.840873">
7Yatsuhashi is a Japanese snack.
</footnote>
<tableCaption confidence="0.978834">
Table 4: Example of global context feature functions gen-
erated by feature templates
</tableCaption>
<figure confidence="0.881031">
&lt;p-tmod′:yesterday&gt;
&lt;p-tmod′:yesterday, t-learn′:simple past&gt;
&lt;p-tmod′:yesterday, t-learn:simple present&gt;
&lt;p-tmod′:yesterday, t-learn′:simple past, t-learn:simple past&gt;
&lt;norm-p-tmod′:past&gt;
&lt;norm-p-tmod′:past, t-learn′:simple past&gt;
&lt;norm-p-tmod′:past, t-learn:simple present&gt;
&lt;norm-p-tmod′:past, t-learn′:simple past, t-learn:simple present&gt;
</figure>
<bodyText confidence="0.99682575">
the global context features p-tmod′:yesterday and
norm-p-tmod′:past are added to the verb eat.
Table 4 lists all the global context features for the
verb eat generated by the feature templates.
</bodyText>
<subsectionHeader confidence="0.997449">
3.3 Trade-off between Precision and Recall
</subsectionHeader>
<bodyText confidence="0.9999773">
Use of surface tense/aspect forms of target verbs im-
proves precision but harms recall. This is because
in most cases the surface tense/aspect and the cor-
rect tense/aspect form of a verb are the same. It is,
of course, desirable to achieve high precision, but
very low recall leads to the system making no cor-
rections. In order to control the trade-off between
precision and recall, we re-estimate the best output
label y� based on the originally estimated label y as
follows:
</bodyText>
<equation confidence="0.9961834">
y� = arg max s(y)
Y
{
αc(y), if y is the same as learner’s tense/aspect
s(y) =c(y) otherwise.
</equation>
<bodyText confidence="0.999930555555556">
where c(y) is the confidence value of y estimated
by the originally trained model (explained in 4.3),
and α (0 &lt; α &lt; 1) is the weight of the surface
tense/aspect.
We first calculate c(y) of all the labels, and dis-
count only the label that is the same as learner’s
tense/aspect, and finally we choose the best output
label. This process leads to an increase of recall. We
call this method T-correction.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999157">
4.1 Data and Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999915666666667">
We used the Lang-8 tense/aspect corpus described
in Section 2. We randomly selected 100,000 entries
for training and 1,000 entries for testing. The test
</bodyText>
<figure confidence="0.996462689655172">
200
0
1
R
0.8
0.6
0.4
0.2
P
0.2 0.4 0.6
0
1
P
R
0.8
0.6
0.4
0.2
P
R
0 0.2 0.4 0.6
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6
(a) tense (b) aspect (c) tense/aspect
</figure>
<figureCaption confidence="0.996844">
Figure 1: Precision-Recall curve for error detection
</figureCaption>
<figure confidence="0.9978665">
1 R 1 R 1 R
0.8 0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.6
0.4
0.2
0 P 0 0 P
P
0 0.2 0.4 0.6 0.2 0.4 0.6 0 0.2 0.4 0.6
(a) tense (b) aspect (c) tense/aspect
</figure>
<figureCaption confidence="0.99724">
Figure 2: Precision-Recall curve for error correction
</figureCaption>
<sectionHeader confidence="0.788289" genericHeader="method">
SVM MAXENT CRF
</sectionHeader>
<bodyText confidence="0.9999225">
data includes 16,308 verb phrases, of which 1,072
(6.6%) contain tense/aspect errors. We used Stan-
ford Parser 1.6.9 8 for generating syntactic features
and tense/aspect tagging.
</bodyText>
<subsectionHeader confidence="0.945142">
4.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.999776642857143">
Because we want to know the effect of using global
context information with CRF, we trained a one-
versus-rest multiclass SVM and a maximum entropy
classifier (MAXENT) as baselines.
We built a SVM model with LIBLINEAR 1.89
and a CRF and a MAXENT model with CRF++
0.54.10 We use the default parameters for each
toolkit.
In every method, we use the same features and
feature described in Section 3, and use T-correction
for choosing the final output. The confidence mea-
sure of the SVM is the distance to the separating hy-
perplane, and that of the MAXENT and the CRF is
the marginal probability of the estimated label.
</bodyText>
<figure confidence="0.8331046">
8http://nlp.stanford.edu/software/
lex-parser.shtml
9http://www.csie.ntu.edu.tw/~cjlin/
liblinear/
10http://crfpp.sourceforge.net/
</figure>
<sectionHeader confidence="0.994688" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999847833333333">
Figures 1 and 2 show the Precision-Recall curves
of the error detection and correction performance of
each model. The figures are grouped by error types:
tense, aspect, and both tense and aspect. All figures
indicate that the CRF model achieves better perfor-
mance than SVM and MAXENT.
</bodyText>
<sectionHeader confidence="0.99443" genericHeader="conclusions">
6 Analysis
</sectionHeader>
<bodyText confidence="0.999971846153846">
We analysed the results of experiments with the α
parameter of the CRF model set to 0.1. The most
frequent type of error in the corpus is using simple
present tense instread of simple past, with 211 in-
stances. Of these our system detected 61 and suc-
cessfully corrected 52 instances. However, of the
second most frequent error type (using simple past
instead of simple present), with 94 instances in the
corpus, our system only detected 9 instances. One
reason why the proposed method achieves high per-
formance in the first type of errors is that tense errors
with action verbs written as simple present are rela-
tively easy to detect.
</bodyText>
<page confidence="0.86018">
201
</page>
<sectionHeader confidence="0.995" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997995">
John Bitchener, Stuart Young, and Denise Cameron.
2005. The Effect of Different Types of Corrective
Feedback on ESL Student Writing. Journal of Second
Language Writing, 14(3):191–205.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting ESL Errors Using Phrasal SMT
Techniques. In Proceedings of COLING-ACL, pages
249–256.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of Grammatical Errors Involving
Prepositions. In Proceedings of ACL-SIGSEM, pages
25–30.
Kevin Knight and Ishwar Chander. 1994. Automated
Postediting of Documents. In Proceedings of the
AAAI’94, pages 779–784.
John Lafferty. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of ICML, pages 282–289.
John Lee and Stephanie Seneff. 2006. Automatic Gram-
mar Correction for Second-Language Learners. In
Proceedings of the 9th ICSLP, pages 1978–1981.
John Lee and Stephanie Seneff. 2008. Correcting Misuse
of Verb Forms. In Proceedings of the 46th ACL:HLT,
pages 174–182.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata,
and Yuji Matsumoto. 2011. Mining Revision Log of
Language Learning SNS for Automated Japanese Er-
ror Correction of Second Language Learners. In Pro-
ceedings of 5th IJCNLP, pages 147–155.
Ryo Nagata and Kazuhide Nakatani. 2010. Evaluating
Performance of Grammatical Error Detection to Max-
imize Learning Effect. In Proceedings of COLING,
pages 894–900.
Alla Rozovskaya and Dan Roth. 2011. Algorithm Selec-
tion and Model Adaptation for ESL Correction Tasks.
In Proceedings of the 49th ACL:HLT, pages 924–933.
</reference>
<page confidence="0.90902">
202
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.827032">
<title confidence="0.9976445">Tense and Aspect Error Correction for ESL Using Global Context</title>
<author confidence="0.994428">Toshikazu Tajiri Mamoru Komachi Yuji Matsumoto</author>
<affiliation confidence="0.9997145">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.991282">8916-5 Takayama, Ikoma, Nara, 630-0192,</address>
<email confidence="0.989764">toshikazu-t@is.naist.jp</email>
<email confidence="0.989764">komachi@is.naist.jp</email>
<email confidence="0.989764">matsu@is.naist.jp</email>
<abstract confidence="0.993211863636364">As the number of learners of English is constantly growing, automatic error correction of ESL learners’ writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate contribution to tense/aspect error correction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Bitchener</author>
<author>Stuart Young</author>
<author>Denise Cameron</author>
</authors>
<title>The Effect of Different Types of Corrective Feedback on ESL Student Writing.</title>
<date>2005</date>
<journal>Journal of Second Language Writing,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="2126" citStr="Bitchener et al., 2005" startWordPosition="329" endWordPosition="332">feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and Roth, 2011), that commonly occur in essays by ESL learners. On the other hand, tense and aspect errors have been little studied, even though they are also commonly found in learners’ essays (Lee and Seneff, 2006; Bitchener et al., 2005). For instance, Lee (2008) corrects English verb inflection errors, but they do not deal with tense/aspect errors because the choice of tense and aspect highly depends on global context, which makes correction difficult. Consider the following sentences taken from a corpus of a Japanese learner of English. (1) I had a good time this Summer Vacation. First, I *go to KAIYUKAN 1 with my friends. In this example, go in the second sentence should be written as went. It is difficult to correct this type of error because there are two choices for correction, namely went and will go. In this case, we </context>
</contexts>
<marker>Bitchener, Young, Cameron, 2005</marker>
<rawString>John Bitchener, Stuart Young, and Denise Cameron. 2005. The Effect of Different Types of Corrective Feedback on ESL Student Writing. Journal of Second Language Writing, 14(3):191–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Michael Gamon</author>
</authors>
<title>Correcting ESL Errors Using Phrasal SMT Techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="1827" citStr="Brockett et al., 2006" startWordPosition="278" endWordPosition="281">to tense/aspect error correction. 1 Introduction Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and Roth, 2011), that commonly occur in essays by ESL learners. On the other hand, tense and aspect errors have been little studied, even though they are also commonly found in learners’ essays (Lee and Seneff, 2006; Bitchener et al., 2005). For instance, Lee (2008) corrects English verb inflection errors, but they do not deal with tense/aspect errors because the choice of tense and aspect highly depends on global context, which makes correction difficult. Consider the following sentences taken from a corpus of a Japanese learner of En</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting ESL Errors Using Phrasal SMT Techniques. In Proceedings of COLING-ACL, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Joel R Tetreault</author>
<author>Na-Rae Han</author>
</authors>
<title>Detection of Grammatical Errors Involving Prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-SIGSEM,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="1873" citStr="Chodorow et al., 2007" startWordPosition="286" endWordPosition="289">n Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and Roth, 2011), that commonly occur in essays by ESL learners. On the other hand, tense and aspect errors have been little studied, even though they are also commonly found in learners’ essays (Lee and Seneff, 2006; Bitchener et al., 2005). For instance, Lee (2008) corrects English verb inflection errors, but they do not deal with tense/aspect errors because the choice of tense and aspect highly depends on global context, which makes correction difficult. Consider the following sentences taken from a corpus of a Japanese learner of English. (1) I had a good time this Summer Vacat</context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>Martin Chodorow, Joel R. Tetreault, and Na-Rae Han. 2007. Detection of Grammatical Errors Involving Prepositions. In Proceedings of ACL-SIGSEM, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automated Postediting of Documents.</title>
<date>1994</date>
<booktitle>In Proceedings of the AAAI’94,</booktitle>
<pages>779--784</pages>
<contexts>
<context position="1803" citStr="Knight and Chander, 1994" startWordPosition="274" endWordPosition="277">s a moderate contribution to tense/aspect error correction. 1 Introduction Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and Roth, 2011), that commonly occur in essays by ESL learners. On the other hand, tense and aspect errors have been little studied, even though they are also commonly found in learners’ essays (Lee and Seneff, 2006; Bitchener et al., 2005). For instance, Lee (2008) corrects English verb inflection errors, but they do not deal with tense/aspect errors because the choice of tense and aspect highly depends on global context, which makes correction difficult. Consider the following sentences taken from a corpus of </context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Kevin Knight and Ishwar Chander. 1994. Automated Postediting of Documents. In Proceedings of the AAAI’94, pages 779–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="5119" citStr="Lafferty, 2001" startWordPosition="782" endWordPosition="784">phrases with 750,000 verb phrases having corrections.5 The annotated tense/aspect labels include 12 combinations of tense (past, present, future) and aspect (nothing, perfect, progressive, perfect progressive). 3 Error Correction Using Global Context As we described in Section 1, using only local information about the target verb phrase may lead to inaccurate correction of tense/aspect errors. Thus, we take into account global context: the relation between target and preceding/following verb phrases. In this paper, we formulate the task as sequence labeling, and use Conditional Random Fields (Lafferty, 2001), which provides state-of-the-art performance in sequence labeling while allowing flexible feature design for combining local and global feature sets. 3.1 Local Features Table 1 shows the local features used to train the error correction model. 2Konan-JIEM Learner Corpus Second Edition (http:// gsk.or.jp/catalog/GSK2011-B/catalog.html) contains 170 essays, and Cambridge English First Certificate in English (http://www.cambridgeesol.org/exams/ fce/index.html) contains 1244 essays. 3http://lang-8.com/ 4As of January, 2012. More details about the Lang-8 corpus can be found in (Mizumoto et al., 20</context>
</contexts>
<marker>Lafferty, 2001</marker>
<rawString>John Lafferty. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>Automatic Grammar Correction for Second-Language Learners.</title>
<date>2006</date>
<booktitle>In Proceedings of the 9th ICSLP,</booktitle>
<pages>1978--1981</pages>
<contexts>
<context position="2101" citStr="Lee and Seneff, 2006" startWordPosition="324" endWordPosition="328">010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and Roth, 2011), that commonly occur in essays by ESL learners. On the other hand, tense and aspect errors have been little studied, even though they are also commonly found in learners’ essays (Lee and Seneff, 2006; Bitchener et al., 2005). For instance, Lee (2008) corrects English verb inflection errors, but they do not deal with tense/aspect errors because the choice of tense and aspect highly depends on global context, which makes correction difficult. Consider the following sentences taken from a corpus of a Japanese learner of English. (1) I had a good time this Summer Vacation. First, I *go to KAIYUKAN 1 with my friends. In this example, go in the second sentence should be written as went. It is difficult to correct this type of error because there are two choices for correction, namely went and w</context>
</contexts>
<marker>Lee, Seneff, 2006</marker>
<rawString>John Lee and Stephanie Seneff. 2006. Automatic Grammar Correction for Second-Language Learners. In Proceedings of the 9th ICSLP, pages 1978–1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>Correcting Misuse of Verb Forms.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th ACL:HLT,</booktitle>
<pages>174--182</pages>
<marker>Lee, Seneff, 2008</marker>
<rawString>John Lee and Stephanie Seneff. 2008. Correcting Misuse of Verb Forms. In Proceedings of the 46th ACL:HLT, pages 174–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoya Mizumoto</author>
<author>Mamoru Komachi</author>
<author>Masaaki Nagata</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th IJCNLP,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="5722" citStr="Mizumoto et al., 2011" startWordPosition="860" endWordPosition="863">ds (Lafferty, 2001), which provides state-of-the-art performance in sequence labeling while allowing flexible feature design for combining local and global feature sets. 3.1 Local Features Table 1 shows the local features used to train the error correction model. 2Konan-JIEM Learner Corpus Second Edition (http:// gsk.or.jp/catalog/GSK2011-B/catalog.html) contains 170 essays, and Cambridge English First Certificate in English (http://www.cambridgeesol.org/exams/ fce/index.html) contains 1244 essays. 3http://lang-8.com/ 4As of January, 2012. More details about the Lang-8 corpus can be found in (Mizumoto et al., 2011). 5Note that not all the 750,000 verb phrases were corrected due to the misuse of tense/aspect. Table 1: Local features for a verb phrase name description t-learn tense/aspect written by the learner (surface tense/aspect) bare the verb lemma L the word to the left R the word to the right nsubj nominal subject dobj direct object aux auxiliary verb pobj object of a preposition p-tmod temporal adverb norm-p-tmod normalized temporal adverb advmod other adverb conj subordinating conjunction main-clause true if the target VP is in main clause sub-clause true if the target VP is in subordinate clause</context>
</contexts>
<marker>Mizumoto, Komachi, Nagata, Matsumoto, 2011</marker>
<rawString>Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata, and Yuji Matsumoto. 2011. Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners. In Proceedings of 5th IJCNLP, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Nagata</author>
<author>Kazuhide Nakatani</author>
</authors>
<title>Evaluating Performance of Grammatical Error Detection to Maximize Learning Effect.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>894--900</pages>
<contexts>
<context position="1485" citStr="Nagata and Nakatani, 2010" startWordPosition="224" endWordPosition="227">h word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate contribution to tense/aspect error correction. 1 Introduction Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and Roth, 2011), that commonly occur in essays by ESL learners. On the other hand, tense and aspect errors have been little studied, even though they are also commonly found in learners’ essays (Lee </context>
</contexts>
<marker>Nagata, Nakatani, 2010</marker>
<rawString>Ryo Nagata and Kazuhide Nakatani. 2010. Evaluating Performance of Grammatical Error Detection to Maximize Learning Effect. In Proceedings of COLING, pages 894–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Algorithm Selection and Model Adaptation for ESL Correction Tasks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th ACL:HLT,</booktitle>
<pages>924--933</pages>
<contexts>
<context position="1901" citStr="Rozovskaya and Roth, 2011" startWordPosition="290" endWordPosition="293">g number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and Roth, 2011), that commonly occur in essays by ESL learners. On the other hand, tense and aspect errors have been little studied, even though they are also commonly found in learners’ essays (Lee and Seneff, 2006; Bitchener et al., 2005). For instance, Lee (2008) corrects English verb inflection errors, but they do not deal with tense/aspect errors because the choice of tense and aspect highly depends on global context, which makes correction difficult. Consider the following sentences taken from a corpus of a Japanese learner of English. (1) I had a good time this Summer Vacation. First, I *go to KAIYUKA</context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2011. Algorithm Selection and Model Adaptation for ESL Correction Tasks. In Proceedings of the 49th ACL:HLT, pages 924–933.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>