<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.959124">
Semi-supervised learning of morphological paradigms and lexicons
</title>
<author confidence="0.895765">
Malin Ahlberg
</author>
<affiliation confidence="0.873706">
Spr˚akbanken
University of Gothenburg
</affiliation>
<email confidence="0.961897">
malin.ahlberg@gu.se
</email>
<author confidence="0.835286">
Markus Forsberg
</author>
<affiliation confidence="0.8413975">
Spr˚akbanken
University of Gothenburg
</affiliation>
<email confidence="0.965286">
markus.forsberg@gu.se
</email>
<author confidence="0.968592">
Mans Hulden
</author>
<affiliation confidence="0.994126">
University of Helsinki
</affiliation>
<email confidence="0.984656">
mans.hulden@helsinki.fi
</email>
<sectionHeader confidence="0.997247" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908935483871">
We present a semi-supervised approach
to the problem of paradigm induction
from inflection tables. Our system ex-
tracts generalizations from inflection ta-
bles, representing the resulting paradigms
in an abstract form. The process is in-
tended to be language-independent, and
to provide human-readable generalizations
of paradigms. The tools we provide can
be used by linguists for the rapid cre-
ation of lexical resources. We evaluate the
system through an inflection table recon-
struction task using Wiktionary data for
German, Spanish, and Finnish. With no
additional corpus information available,
the evaluation yields per word form ac-
curacy scores on inflecting unseen base
forms in different languages ranging from
87.81% (German nouns) to 99.52% (Span-
ish verbs); with additional unlabeled text
corpora available for training the scores
range from 91.81% (German nouns) to
99.58% (Spanish verbs). We separately
evaluate the system in a simulated task of
Swedish lexicon creation, and show that
on the basis of a small number of inflection
tables, the system can accurately collect
from a list of noun forms a lexicon with in-
flection information ranging from 100.0%
correct (collect 100 words), to 96.4% cor-
rect (collect 1000 words).
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999258862068965">
Large scale morphologically accurate lexicon con-
struction for natural language is a very time-
consuming task, if done manually. Usually, the
construction of large-scale lexical resources pre-
supposes a linguist who constructs a detailed mor-
phological grammar that models inflection, com-
pounding, and other morphological and phonolog-
ical phenomena, and additionally performs a man-
ual classification of lemmas in the language ac-
cording to their paradigmatic behavior.
In this paper we address the problem of lexicon
construction by constructing a semi-supervised
system that accepts concrete inflection tables as in-
put, generalizes inflection paradigms from the ta-
bles provided, and subsequently allows the use of
unannotated corpora to expand the inflection ta-
bles and the automatically generated paradigms.1
In contrast to many machine learning ap-
proaches that address the problem of paradigm ex-
traction, the current method is intended to produce
human-readable output of its generalizations. That
is, the paradigms provided by the system can be
inspected for errors by a linguist, and if neces-
sary, corrected and improved. Decisions made by
the extraction algorithms are intended to be trans-
parent, permitting morphological system develop-
ment in tandem with linguist-provided knowledge.
Some of the practical tasks tackled by the sys-
tem include the following:
</bodyText>
<listItem confidence="0.946124666666667">
• Given a small number of known inflection ta-
bles, extract from a corpus a lexicon of those
lemmas that behave like the examples pro-
vided by the linguist.
• Given a large number of inflection tables—
such as those provided by the crowdsourced
lexical resource, Wiktionary—generalize the
tables into a smaller number of abstract
paradigms.
</listItem>
<sectionHeader confidence="0.997091" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.99975">
Automatic learning of morphology has long been a
prominent research goal in computational linguis-
tics. Recent studies have focused on unsupervised
methods in particular—learning morphology from
</bodyText>
<footnote confidence="0.9992725">
1Our programs and the datasets used, including the
evaluation procedure for this paper, are freely avail-
able at https://svn.spraakbanken.gu.se/clt/
eacl/2014/extract
</footnote>
<page confidence="0.927728">
569
</page>
<note confidence="0.9725448">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569–578,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
unlabeled data (Goldsmith, 2001; Schone and Ju-
rafsky, 2001; Chan, 2006; Creutz and Lagus,
2007; Monson et al., 2008). Hammarstr¨om and
</note>
<bodyText confidence="0.993819810810811">
Borin (2011) provides a current overview of unsu-
pervised learning.
Previous work with similar semi-supervised
goals as the ones in this paper include Yarowsky
and Wicentowski (2000), Neuvel and Fulop
(2002), Cl´ement et al. (2004). Recent machine
learning oriented work includes Dreyer and Eis-
ner (2011) and Durrett and DeNero (2013), which
documents a method to learn orthographic trans-
formation rules to capture patterns across inflec-
tion tables. Part of our evaluation uses the same
dataset as Durrett and DeNero (2013). Eskander
et al. (2013) shares many of the goals in this paper,
but is more supervised in that it focuses on learn-
ing inflectional classes from richer annotation.
A major departure from much previous work
is that we do not attempt to encode variation
as string-changing operations, say by string edits
(Dreyer and Eisner, 2011) or transformation rules
(Lind´en, 2008; Durrett and DeNero, 2013) that
perform mappings between forms. Rather, our
goal is to encode all variation within paradigms
by presenting them in a sufficiently generic fash-
ion so as to allow affixation processes, phonolog-
ical alternations as well as orthographic changes
to naturally fall out of the paradigm specification
itself. Also, we perform no explicit alignment of
the various forms in an inflection table, as in e.g.
Tchoukalov et al. (2010). Rather, we base our al-
gorithm on extracting the longest common subse-
quence (LCS) shared by all forms in an inflection
table, from which alignment of segments falls out
naturally. Although our paradigm representation
is similar to and inspired by that of Forsberg et al.
(2006) and D´etrez and Ranta (2012), our method
of generalizing from inflection tables to paradigms
is novel.
</bodyText>
<sectionHeader confidence="0.973773" genericHeader="method">
3 Paradigm learning
</sectionHeader>
<bodyText confidence="0.999848333333333">
In what follows, we adopt the view that words
and their inflection patterns can be organized
into paradigms (Hockett, 1954; Robins, 1959;
Matthews, 1972; Stump, 2001). We essentially
treat a paradigm as an ordered set of functions
(f1, ... , fn), where fz: x1, ... , xn H E∗, that is,
where each entry in a paradigm is a function from
variables to strings, and each function in a partic-
ular paradigm shares the same variables.
</bodyText>
<subsectionHeader confidence="0.9996">
3.1 Paradigm representation
</subsectionHeader>
<bodyText confidence="0.990221041666667">
We represent the functions in what we call ab-
stract paradigm. In our representation, an ab-
stract paradigm is an ordered collection of strings,
where each string may additionally contain in-
terspersed variables denoted x1, x2,. . . , xn. The
strings represent fixed, obligatory parts of a
paradigm, while the variables represent mutable
parts. These variables, when instantiated, must
contain at least one segment, but may otherwise
vary from word to word. A complete abstract
paradigm captures some generalization where the
mutable parts represented by variables are instan-
tiated the same way for all forms in one particu-
lar inflection table. For example, the fairly simple
paradigm
x1 x1+s x1+ed x1+ing
could represent a set of English verb forms, where
x1 in this case would coincide with the infinitive
form of the verb—walk, climb, look, etc.
For more complex patterns, several variable
parts may be invoked, some of them discontinu-
ous. For example, part of an inflection paradigm
for German verbs of the type schreiben (to write)
verbs may be described as:
</bodyText>
<equation confidence="0.9136475">
x1+e+x2+x3+en INFINITIVE
x1+e+x2+x3+end PRESENT PARTICIPLE
ge+x1+x2+e+x3+en PAST PARTICIPLE
x1+e+x2+x3+e PRESENT 1P SG
x1+e+x2+x3+st PRESENT 2P SG
x1+e+x2+x3+t PRESENT 3P SG
</equation>
<bodyText confidence="0.999617315789474">
If the variables are instantiated as x1=schr,
x2=i, and x3=b, the paradigm corresponds to
the forms (schreiben, schreibend, geschrieben,
schreibe, schreibst, schreibt). If, on the other
hand, x1=l, x2=i, and x3=h, the same paradigm re-
flects the conjugation of leihen (to lend/borrow)—
(leihen, leihend, geliehen, leihe, leihst, leiht).
It is worth noting that in this representation, no
particular form is privileged in the sense that all
other forms can only be generated from some spe-
cial form, say the infinitive. Rather, in the cur-
rent representation, all forms can be derived from
knowing the variable instantiations. Also, given
only a particular word form and a hypothetical
paradigm to fit it in, the variable instantiations can
often be logically deduced unambiguously. For
example, let us say we have a hypothetical form
steigend and need to fit it in the above paradigm,
without knowing which slot it should occupy. We
</bodyText>
<page confidence="0.990596">
570
</page>
<bodyText confidence="0.999913">
may deduce that it must represent the present par-
ticiple, and that x1=st, x2=i, and x3=g. From this
knowledge, all other forms can subsequently be
derived.
Although we have provided grammatical in-
formation in the above table for illustrative pur-
poses, our primary concern in the current work is
the generalization from inflection tables—which
for our purposes are simply an ordered set of
word forms—to paradigms of the format dis-
cussed above.
</bodyText>
<subsectionHeader confidence="0.999602">
3.2 Paradigm induction from inflection tables
</subsectionHeader>
<bodyText confidence="0.999984333333333">
The core component of our method consists of
finding, given an inflection table, the maximally
general paradigm that reflects the information in
that table. To this end, we make the assumption
that string subsequences that are shared by dif-
ferent forms in an inflection table are incidental
and can be generalized over. For example, given
the English verb swim, and a simple inflection ta-
ble swim#swam#swum,2 we make the assump-
tion that the common sequences sw and m are ir-
relevant to the inflection, and that by disregarding
these strings, we can focus on the segments that
vary within the table—in this case the variation
i—a—u. In other words, we can assume sw and
m to be variables that vary from word to word
and describe the table swim#swam#swum as
x1+i+x2#x1+a+x2#x1+u+x2, where x1=sw and
x2=m in the specific table.
</bodyText>
<subsectionHeader confidence="0.81232">
3.2.1 Maximally general paradigms
</subsectionHeader>
<bodyText confidence="0.978750666666667">
In order to generalize as much as possible from an
inflection table, we extract from it what we call the
maximally general paradigm by:
</bodyText>
<listItem confidence="0.8627964">
1. Finding the longest common subsequence
(LCS) to all the entries in the inflection table.
2. Finding the segmentation into variables of
the LCS(s) (there may be several) in the in-
flection table that results in
</listItem>
<bodyText confidence="0.722984333333333">
(a) The smallest number of variables. Two
segments xy in the LCS must be part of
the same variable if they always occur
together in every form in the inflection
table, otherwise they must be assigned
separate variables.
</bodyText>
<footnote confidence="0.839556">
2To save space, we will henceforth use the #-symbol as a
delimiter between entries in an inflection table or paradigm.
</footnote>
<figure confidence="0.435193333333333">
Input: ① Extract ② Fit LCS ③ Generalize ④ Collapse
inflection LCS to table to paradigms paradigms
tables
</figure>
<figureCaption confidence="0.986127">
Figure 1: Illustration of our paradigm generaliza-
</figureCaption>
<bodyText confidence="0.970752571428571">
tion algorithm. In step ➀ we extract the LCS sep-
arately for each inflection table, attempt to find
a consistent fit between the LCS and the forms
present in the table (step ➁), and assign the seg-
ments that participate in the LCS variables (step
➂). Finally, resulting paradigms that turn out to be
identical may be collapsed (step ➃) (section 3.3).
</bodyText>
<listItem confidence="0.853064">
(b) The smallest total number of infixed
non-variable segments in the inflection
table (segments that occur between vari-
ables).
3. Replacing the discontinuous sequences that
are part of the LCS with variables (every
form in a paradigm will contain the same
number of variables).
</listItem>
<bodyText confidence="0.99997347826087">
These steps are illustrated in figure 1. The
first step, extracting the LCS from a collection of
strings, is the well-known multiple longest com-
mon subsequence problem (MLCS). It is known
to be NP-hard (Maier, 1978). Although the num-
ber of strings to find the LCS from may be rather
large in real-world data, we find that a few sensible
heuristic techniques allow us to solve this problem
efficiently for practical linguistic material, i.e., in-
flection tables. We calculate the LCS by calculat-
ing intersections of finite-state machines that en-
code all subsequences of all words, using the foma
finite-state toolkit (Hulden, 2009).3
While for most tables there is only one way
to segment the LCS in the various forms, some
ambiguous corner cases need to be resolved by
imposing additional criteria for the segmentation,
given in steps 2(a) and 2(b). As an example,
consider a snippet of a small conjugation table
for the Spanish verb comprar (to buy), com-
prar#compra#compro. Obviously the LCS is
compr—however, this can be distributed in two
different ways across the strings, as seen below.
</bodyText>
<footnote confidence="0.7002805">
3Steps 2 and 3 are implemented using more involved
finite-state techniques that we plan to describe elsewhere.
</footnote>
<figure confidence="0.998587555555555">
[r]i[ng]
[r]a[ng]
[r]u[ng]
[sw]i[m]
[sw]a[m]
[sw]u[m]
x1+i+x2
x1+a+x2
x1+u+x2
x1+i+x2
x1+a+x2
x1+u+x2
}
}
x1+i+x2
x1+a+x2
x1+u+x2
}
}
rng
ring
rang
rung
swm
swim
swam
swum
</figure>
<page confidence="0.777945">
571
</page>
<subsectionHeader confidence="0.389668">
Form Input Generalization
</subsectionHeader>
<bodyText confidence="0.308871333333333">
comprar
compra
compro
</bodyText>
<equation confidence="0.667179">
x1 x2
</equation>
<bodyText confidence="0.999856571428571">
The obvious difference here is that in the first
assignment, we only need to declare one vari-
able x1=compr, while in the second, we need
two, x1=comp, x2=r. Such cases are resolved by
choosing the segmentation with the smallest num-
ber of variables by step 2(a).
Remaining ambiguities are resolved by mini-
mizing the total number of infixed segments. As
an illustration of where this is necessary, consider
a small extract from the Swedish noun table segel
(sail): segel#seglen#seglet. Here, the LCS, of
which there are two of equal length (sege/segl)
must be assigned to two variables where either
x1=seg and x2=e, or x1=seg and x2=l:
</bodyText>
<figure confidence="0.7942628">
(a) (b)
x1 x2
segel
seglen
seglet
</figure>
<bodyText confidence="0.996869333333333">
However, in case (a), the number of infixed
segments—the l’s in the second and third form—
total one more than in the distribution in (b), where
only one e needs to be infixed in one form. Hence,
the representation in (b) is chosen in step 2(b).
The need for this type of disambiguation strat-
egy surfaces very rarely and the choice to mini-
mize infix length is largely arbitrary—although it
may be argued that some linguistic plausibility is
encoded in the minimization of infixes. However,
choosing a consistent strategy is important for the
subsequent collapsing of paradigms.
</bodyText>
<subsectionHeader confidence="0.999812">
3.3 Collapsing paradigms
</subsectionHeader>
<bodyText confidence="0.999941083333333">
If several tables are given as input, and we extract
the maximally general paradigm from each, we
may collapse resulting paradigms that are identi-
cal. This is also illustrated in figure 1.
As paradigms are collapsed, we record the in-
formation about how the various variables were
interpreted prior to collapsing. That is, for the
example in figure 1, we not only store the result-
ing single paradigm, but also the information that
x1=r, x2=ng in one table and that x1=sw, x2=m
in another. This allows us to potentially recon-
struct all the inflection tables seen during learn-
</bodyText>
<figure confidence="0.908151545454545">
[Inf] kaufen x1+en
[PresPart] kaufend x1+end
[PastPart] gekauft ge+x1+t
[Pres1pSg] kaufe x1+e
[Pres1pPl] kaufen x1+en
[Pres2pSg] kaufst x1+st
[Pres2pPl] kauft x1+t
[Pres3pSg] kauft x1+t
[Pres3pPl] kaufen x1+en
. . . . . . . . .
x1 = kauf
</figure>
<tableCaption confidence="0.900466">
Table 1: Generalization from a German example
verb kaufen (to buy) exemplifying typical render-
ing of paradigms.
</tableCaption>
<bodyText confidence="0.95925125">
ing. Storing this information is also crucial for
paradigm table collection from text, fitting unseen
word forms into paradigms, and reasoning about
unseen paradigms, as will be discussed below.
</bodyText>
<sectionHeader confidence="0.4817035" genericHeader="method">
3.4 MLCS as a language-independent
generalization strategy
</sectionHeader>
<bodyText confidence="0.999972916666667">
There is very little language-specific information
encoded in the strategy of paradigm generaliza-
tion that focuses on the LCS in an inflection
table. That is, we do not explicitly prioritize
processes like prefixation, suffixation, or left-to-
right writing systems. The resulting algorithm
thus generalizes tables that reflect concatenative
and non-concatenative morphological processes
equally well. Tables 1 and 2 show the outputs of
the method for German and Arabic verb conjuga-
tion reflecting the generalization of concatenative
and non-concatenative patterns.
</bodyText>
<subsectionHeader confidence="0.963774">
3.5 Instantiating paradigms
</subsectionHeader>
<bodyText confidence="0.999977533333333">
As mentioned above, given that the variable in-
stantiations of a paradigm are known, we may gen-
erate the full inflection table. The variable instan-
tiations are retrieved by matching a word form to
one of the patterns in the paradigms. For example,
the German word form b¨ucken (to bend down)
may be matched to three patterns in the paradigm
exemplified in table 1, and all three matches yield
the same variable instantiation, i.e., x1=b¨uck.
Paradigms with more than one variable may
be sensitive to the matching strategy of the vari-
ables. To see this, consider the pattern x1+a+x2
and the word banana. Here, two matches are pos-
sible x1=b and x2=nana and x1=ban and x2 =na.
In other words, there are three possible matching
</bodyText>
<figure confidence="0.930586078947368">
comprar
compra
compro
{
x1
{
{
x1 x2
{ {
segel
seglen
seglet
{
{
(a) (b)
x1
x1 x2
{
{
{
572
Form Input Generalization
[Past1SG] katabtu ( ��I��.��J�») x1+a+x2+a+x3+tu
[Past2SGM] katabta ( ��I��.��J�») x1+a+x2+a+x3+ta
[Past2SGF] katabti ( �I���.��J�») x1+a+x2+a+x3+ti
[Past3SGM] kataba ( ~I. ~~J~») x1+a+x2+a+x3+a
[Past3SGF] katabat ( ��I��.��J�») x1+a+x2+a+x3+at
. . . . . . . . .
[Pres1SG] aktubu (�I.�J»@) a+x1+x2+u+x3+u
[Pres2SGM] taktubu ( �I. J��º��K) ta+x1+x2+u+x3+u
� � �
�
[Pres2SGF] taktubina ( LJ.�&apos;�J&apos;º��K) ta+x1+x2+u+x3+¯ına
[Pres3SGM] yaktubu ( �I. J���º�K�) ya+x1+x2+u+x3+u
��
[Pres3SGF] taktubu ( ~I. Jº��K) ta+x1+x2+u+x3+u
. . . . . . . . .
x1 = k (1/4), x2 = t ( u), x3 = b (H. )
</figure>
<tableCaption confidence="0.780385">
Table 2: Generalization from an Arabic con-
</tableCaption>
<bodyText confidence="0.95456">
jugation table involving the root /k-t-b/ from
which the stems katab (to write/past) and ktub
(present/non-past) are formed, conjugated in Form
I, past and present tenses. Extracting the longest
common subsequence yields a paradigm where
variables correspond to root radicals.
strategies:4
</bodyText>
<listItem confidence="0.998838">
1. shortest match (x1 =b and x2 =nana)
2. longest match (x1 =ban and x2 =na)
3. try all matching combinations
</listItem>
<bodyText confidence="0.999976615384616">
The matching strategy that tends to be success-
ful is somewhat language-dependent: for a lan-
guage with a preference for suffixation, longest
match is typically preferred, while for others
shortest match or trying all combinations may be
the best choice. All languages evaluated in this
article have a preference for suffixation, so in our
experiments we have opted for using the longest
match for the sake of convenience. Our imple-
mentation allows for exploring all matches, how-
ever. Even though all matches were to be tried,
‘bad’ matches will likely result in implausible in-
flections that can be discarded using other cues.
</bodyText>
<sectionHeader confidence="0.964253" genericHeader="method">
4 Assigning paradigms automatically
</sectionHeader>
<bodyText confidence="0.9948305">
The next problem we consider is assigning the cor-
rect paradigms to candidate words automatically.
</bodyText>
<footnote confidence="0.919222666666667">
4The number of matches may increase quickly for longer
words and many variables in the worst case: e.g. caravan
matches x1+a+x2 in three different ways.
</footnote>
<bodyText confidence="0.999870769230769">
As a first step, we match the current word to a pat-
tern. In the general case, all patterns are tried for a
given candidate word. However, we usually have
access to additional information about the candi-
date words—e.g., that they are in the base form of
a certain part of speech—which we use to improve
the results by only matching the relevant patterns.
From a candidate word, all possible inflection
tables are generated. Following this, a decision
procedure is applied that calculates a confidence
score to determine which paradigm is the most
probable. The score is a weighted combination of
the following calculations:
</bodyText>
<listItem confidence="0.9969642">
1. Compute the longest common suffix for the
generated base form (which may be the input
form) with previously seen base forms. If of
equal length, select the paradigm where the
suffix occurs with higher frequency.
2. Compute frequency spread over the set of
unique word forms according to the follow-
ing formula: Ew∈set(ly) log(freq(w) + 1)
3. Use the most frequent paradigm as a tie-
breaker.
</listItem>
<bodyText confidence="0.999858375">
Step 1 is a simple memory-based approach,
much in the same spirit as van den Bosch and
Daelemans (1999), where we compare the current
base form with what we have seen before.
For step 2, let us elaborate further why the
frequency spread is computed on unique word
forms. We do this to avoid favoring paradigms
that have the same word forms for many or all
inflected forms. For example, the German noun
Ananas (pineapple) has a syncretic inflection with
one repeated word form across all slots, Ananas.
When trying to assign a paradigm to an unknown
word form that matches x1, it will surely fit the
paradigm that Ananas has generated perfectly
since we have encountered every word form in that
paradigm, of which there is only one, namely x1.
Hence, we want to penalize low variation of word
forms when assigning paradigms.
The confidence score calculated is not only ap-
plicable for selecting the most probable paradigm
for a given word-form; it may also be used to rank
a list of words so that the highest ranked paradigm
is the most likely to be correct. Examples of such
rankings are found in section 5.3.
</bodyText>
<page confidence="0.997013">
573
</page>
<figure confidence="0.68160475">
Inflection table coverage
DE-VERBS
DE-NOUNS
ES-VERBS
FI-VERBS
FI-NOUNS
-ADJS
0 50 100 150 200
</figure>
<figureCaption confidence="0.832767">
Figure 2: Degree of coverage with varying num-
bers of paradigms.
</figureCaption>
<table confidence="0.985009625">
Input: Output:
Data inflection abstract
tables paradigms
DE-VERBS 1827 140
DE-NOUNS 2564 70
ES-VERBS 3855 97
FI-VERBS 7049 282
FI-NOUNS-ADJS 6200 258
</table>
<tableCaption confidence="0.968747">
Table 3: Generalization of paradigms. The num-
ber of paradigms produced from Wiktionary in-
flection tables by generalization and collapsing of
abstract paradigms.
</tableCaption>
<figure confidence="0.997981125">
1.00
0.95
0.90
0.85
0.80
0.75
0.70
Paradigms
</figure>
<sectionHeader confidence="0.932851" genericHeader="conclusions">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999845">
To evaluate the method, we have conducted three
experiments. First we repeat an experiment pre-
sented in Durrett and DeNero (2013) using the
same data and experiment setup, but with our
generalization method. In this experiment, we
are given a number of complete inflection tables
scraped from Wiktionary. The task is to recon-
struct complete inflection tables from 200 held-out
base forms. For this task, we evaluate per form
accuracy as well as per table accuracy for recon-
struction. The second experiment is the same as
the first, but with additional access to an unlabeled
text dump for the language from Wikipedia.
In the last experiment we try to mimic the situa-
tion of a linguist starting out to describe a new lan-
guage. The experiment uses a large-scale Swedish
morphology as reference and evaluates how reli-
ably a lexicon can be gathered from a word list us-
ing only a few manually specified inflection tables
generalized into abstract paradigms by our system.
</bodyText>
<subsectionHeader confidence="0.99678">
5.1 Experiment 1: Wiktionary
</subsectionHeader>
<bodyText confidence="0.999914771428571">
In our first experiment we start from the inflec-
tion tables in the development and test set from
Durrett and DeNero (2013), henceforth D&amp;DN13.
Table 3 shows the number of input tables as well
as the number of paradigms that they result in af-
ter generalization and collapsing. For all cases,
the number of output paradigms are below 10%
of the number of input inflection tables. Figure
2 shows the generalization rate achieved with the
paradigms. For instance, the 20 most common re-
sulting German noun paradigms are sufficient to
model almost 95% of the 2,564 separate inflection
tables given as input.
As described earlier, in the reconstruction task,
the input base forms are compared to the abstract
paradigms by measuring the longest common suf-
fix length for each input base form compared to
the ones seen during training. This approach is
memory-based: it simply measures the similarity
of a given lemma to the lemmas encountered dur-
ing the learning phase. Table 4 presents our results
juxtaposed with the ones reported by D&amp;DN13.
While scoring slightly below D&amp;DN13 for the
majority of the languages when measuring form
accuracy, our method shows an advantage when
measuring the accuracy of complete tables. In-
terestingly, the only case where we improve upon
the form accuracy of D&amp;DN13 is German verbs,
where we get our lowest table accuracy.
Table 4 further shows an oracle score, giv-
ing an upper bound for our method that would
be achieved if we were always able to pick the
best fitting paradigm available. This upper bound
ranges from 99% (Finnish verbs) to 100% (three
out of five tests).
</bodyText>
<subsectionHeader confidence="0.9683565">
5.2 Experiment 2: Wiktionary and
Wikipedia
</subsectionHeader>
<bodyText confidence="0.999389">
In our second experiment, we extend the previous
experiment by adding access to a corpus. Apart
from measuring the longest common suffix length,
we now also compute the frequency of the hy-
pothetical candidate forms in every generated ta-
ble and use this to favor paradigms that generate
a large number of attested forms. For this, we
use a Wikipedia dump, from which we have ex-
tracted word-form frequencies.5 In total, the num-
ber of word types in the Wikipedia corpus was
8.9M (German), 3.4M (Spanish), 0.7M (Finnish),
and 2.7M (Swedish). Table 5 presents the results,
</bodyText>
<footnote confidence="0.997548">
5The corpora were downloaded and extracted as de-
scribed at http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
</footnote>
<page confidence="0.987931">
574
</page>
<table confidence="0.999889142857143">
Data Per D&amp;DN13 Per D&amp;DN13 Oracle accuracy
table form per form (per table)
DE-VERBS 68.0 85.0 97.04 96.19 99.70 (198/200)
DE-NOUNS 76.5 79.5 87.81 88.94 100.00 (200/200)
ES-VERBS 96.0 95.0 99.52 99.67 100.00 (200/200)
FI-VERBS 92.5 87.5 96.36 96.43 99.00 (195/200)
FI-NOUNS-ADJS 85.0 83.5 91.91 93.41 100.00 (200/200)
</table>
<tableCaption confidence="0.853271666666667">
Table 4: Experiment 1: Accuracy of reconstructing 200 inflection tables given only base forms from
held-out data when paradigms are learned from the Wiktionary dataset. For comparison, figures from
Durrett and DeNero (2013) are included (shown as D&amp;DN13).
</tableCaption>
<table confidence="0.98323">
Data Per Per Oracle acc. Top-1000 rank Correct/Incorrect
table form per form (table)
TOP 10% 100/0 (100.0%)
DE-VERBS 76.50 97.87 99.70 (198/200) TOP 50% 489/11 (97.8%)
DE-NOUNS 82.00 91.81 100.00 (200/200) TOP 100% 964/36 (96.4%)
ES-VERBS 98.00 99.58 100.00 (200/200)
FI-VERBS 92.50 96.63 99.00 (195/200)
FI-NOUNS-ADJS 88.00 93.82 100.00 (200/200) Table 6: Top-1000 rank for all nouns in SALDO
</table>
<tableCaption confidence="0.995003">
Table 5: Experiment 2: Reconstructing 200 held-
</tableCaption>
<bodyText confidence="0.987274125">
out inflection tables with paradigms induced from
Wiktionary and further access to raw text from
Wikipedia.
where an increased accuracy is noted for all lan-
guages, as is to be expected since we have added
more knowledge to the system. The bold numbers
mark the cases where we outperform the result in
Durrett and DeNero (2013), which is now the case
in four out of five tests for table accuracy, scoring
between 76.50% for German verbs and 98.00% for
Spanish verbs.
Measuring form accuracy, we achieve scores
between 91.81% and 99.58%. The smallest im-
provement is noted for Finnish verbs, which has
the largest number of paradigms, but also the
smallest corpus.
</bodyText>
<subsectionHeader confidence="0.988176">
5.3 Experiment 3: Ranking candidates
</subsectionHeader>
<bodyText confidence="0.999989904761905">
In this experiment we consider a task where we
only have a small number of inflection tables,
mimicking the situation where a linguist has man-
ually entered a few inflection tables, allowed the
system to generalize these into paradigms, and
now faces the task of culling from a corpus—in
this case labeled with basic POS information—the
candidate words/lemmas that best fit the induced
paradigms. This would be a typical task during
lexicon creation.
We selected the 20 most frequent noun
paradigms (from a total of 346), with one in-
flection table each, from our gold standard, the
Swedish lexical resource SALDO (Borin et al.,
2013). From this set, we discarded paradigms
that lack plural forms.6 We also removed from
the paradigms special compounding forms that
Swedish nouns have, since compound informa-
tion is not taken into account in this experiment.
The compounding forms are part of the original
paradigm specification, and after a collapsing pro-
cedure after compound-form removal, we were
left with a total of 11 paradigms.
In the next step we ranked all nouns in SALDO
(79.6k lemmas) according to our confidence score,
which indicates how well a noun fits a given
paradigm. We then evaluated the paradigm assign-
ment for the top-1000 lemmas. Among these top-
1000 words, we found 44 that were outside the
20 most frequent noun paradigms. These words
were not necessarily incorrectly assigned, since
they may only differ in their compound forms; as
a heuristic, we considered them correct if they had
the same declension and gender as the paradigm,
and incorrect otherwise.
Table 6 displays the results, including a total ac-
curacy of 96.4%.
Next, we investigated the top-1000 distribution
for individual paradigms. This corresponds to the
situation where a linguist has just entered a new
inflection table and is looking for words that fit the
resulting paradigm. The result is presented in two
</bodyText>
<footnote confidence="0.99867325">
6The paradigms that lack plural forms are subsets of other
paradigms. In other words: when no plural forms are attested,
we would need a procedure to decide if plural forms are even
possible, which is currently beyond the scope of our method.
</footnote>
<page confidence="0.99608">
575
</page>
<figure confidence="0.996637">
Error rate (%)
Top ranked H
10 20 30 40 50 60 70
10 20 30 40 50 60 70
Top ranked (%)
10
8
6
4
p_kikare
2
p_flicka
p_mening
</figure>
<sectionHeader confidence="0.930582" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999772820754717">
Lars Borin, Markus Forsberg, and Lennart L¨onngren.
2013. SALDO: a touch of yin to WordNet’s yang.
Language Resources and Evaluation, May. Online
first publication; DOI 10.1007/s10579-013-9233-4.
Erwin Chan. 2006. Learning probabilistic paradigms
for morphology in a latent class model. In Proceed-
ings of the Eighth Meeting of the ACL Special Inter-
est Group on Computational Phonology and Mor-
phology, pages 69–78. Association for Computa-
tional Linguistics.
Lionel Cl´ement, Bernard Lang, Benoit Sagot, et al.
2004. Morphology based automatic acquisition of
large-coverage lexica. In LREC 04, pages 1841–
1844.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.
Gr´egoire D´etrez and Aarne Ranta. 2012. Smart
paradigms and the predictability and complexity of
inflectional morphology. In Proceedings of the 13th
EACL, pages 645–653. Association for Computa-
tional Linguistics.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 616–627. Association
for Computational Linguistics.
Greg Durrett and John DeNero. 2013. Supervised
learning of complete morphological paradigms. In
Proceedings of NAACL-HLT, pages 1185–1195.
Ramy Eskander, Nizar Habash, and Owen Rambow.
2013. Automatic extraction of morphological lex-
icons from morphologically annotated corpora. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1032–1043. Association for Computational Linguis-
tics.
Markus Forsberg, Harald Hammarstr¨om, and Aarne
Ranta. 2006. Morphological lexicon extraction
from raw text data. In Advances in Natural Lan-
guage Processing, pages 488–499. Springer.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
linguistics, 27(2):153–198.
Harald Hammarstr¨om and Lars Borin. 2011. Unsuper-
vised learning of morphology. Computational Lin-
guistics, 37(2):309–350.
Charles F Hockett. 1954. Two models of grammati-
cal description. Morphology: Critical Concepts in
Linguistics, 1:110–138.
Mans Hulden. 2009. Foma: a finite-state compiler and
library. In Proceedings of the 12th Conference of the
European Chapter of the European Chapter of the
Association for Computational Linguistics: Demon-
strations Session, pages 29–32, Athens, Greece. As-
sociation for Computational Linguistics.
Kimmo Koskenniemi. 1991. A discovery procedure
for two-level phonology. Computational Lexicol-
ogy and Lexicography: A Special Issue Dedicated
to Bernard Quemada, 1:451–46.
Kimmo Koskenniemi. 2013. An informal discovery
procedure for two-level rules. Journal of Language
Modelling, 1(1):155–188.
Krister Lind´en. 2008. A probabilistic model for guess-
ing base forms of new words by analogy. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 106–116. Springer.
David Maier. 1978. The complexity of some problems
on subsequences and supersequences. Journal of the
ACM (JACM), 25(2):322–336.
Peter H. Matthews. 1972. Inflectional morphology:
A theoretical study based on aspects of Latin verb
conjugation. Cambridge University Press.
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. Paramor: finding paradigms
across morphology. In Advances in Multilingual
and Multimodal Information Retrieval, pages 900–
907. Springer.
Sylvain Neuvel and Sean A Fulop. 2002. Unsuper-
vised learning of morphology without morphemes.
In Proceedings of the ACL-02 workshop on Morpho-
logical and phonological learning-Volume 6, pages
31–40. Association for Computational Linguistics.
Robert H Robins. 1959. In defence of WP. Transac-
tions of the Philological Society, 58(1):116–144.
Patrick Schone and Daniel Jurafsky. 2001.
Knowledge-free induction of inflectional mor-
phologies. In Proceedings of the second meeting
of the North American Chapter of the Association
for Computational Linguistics on Language tech-
nologies, pages 1–9. Association for Computational
Linguistics.
Gregory T. Stump. 2001. A theory of paradigm struc-
ture. Cambridge University Press.
Tzvetan Tchoukalov, Christian Monson, and Brian
Roark. 2010. Morphological analysis by mul-
tiple sequence alignment. In Multilingual Infor-
mation Access Evaluation I. Text Retrieval Experi-
ments, pages 666–673. Springer.
Pieter Theron and Ian Cloete. 1997. Automatic acqui-
sition of two-level morphological rules. In Proceed-
ings of the fifth conference on Applied natural lan-
guage processing, pages 103–110. Association for
Computational Linguistics.
</reference>
<page confidence="0.97124">
577
</page>
<reference confidence="0.999164545454545">
Antal van den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In Proceed-
ings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 285–292. As-
sociation for Computational Linguistics.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pages 207–216. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.9967">
578
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.315986">
<title confidence="0.996188">Semi-supervised learning of morphological paradigms and lexicons</title>
<author confidence="0.994361">Malin</author>
<affiliation confidence="0.998296">University of Gothenburg</affiliation>
<email confidence="0.802293">malin.ahlberg@gu.se</email>
<author confidence="0.923036">Markus</author>
<affiliation confidence="0.995685">University of Gothenburg</affiliation>
<email confidence="0.671597">markus.forsberg@gu.se</email>
<author confidence="0.72094">Mans</author>
<affiliation confidence="0.997722">University of Helsinki</affiliation>
<email confidence="0.976442">mans.hulden@helsinki.fi</email>
<abstract confidence="0.99363278125">We present a semi-supervised approach to the problem of paradigm induction from inflection tables. Our system extracts generalizations from inflection tables, representing the resulting paradigms in an abstract form. The process is intended to be language-independent, and to provide human-readable generalizations of paradigms. The tools we provide can be used by linguists for the rapid creation of lexical resources. We evaluate the system through an inflection table reconstruction task using Wiktionary data for German, Spanish, and Finnish. With no additional corpus information available, the evaluation yields per word form accuracy scores on inflecting unseen base forms in different languages ranging from 87.81% (German nouns) to 99.52% (Spanish verbs); with additional unlabeled text corpora available for training the scores range from 91.81% (German nouns) to 99.58% (Spanish verbs). We separately evaluate the system in a simulated task of Swedish lexicon creation, and show that on the basis of a small number of inflection tables, the system can accurately collect from a list of noun forms a lexicon with inflection information ranging from 100.0% correct (collect 100 words), to 96.4% correct (collect 1000 words).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lars Borin</author>
<author>Markus Forsberg</author>
<author>Lennart L¨onngren</author>
</authors>
<title>SALDO: a touch of yin to WordNet’s yang. Language Resources and Evaluation, May. Online first publication;</title>
<date>2013</date>
<journal>DOI</journal>
<pages>10--1007</pages>
<marker>Borin, Forsberg, L¨onngren, 2013</marker>
<rawString>Lars Borin, Markus Forsberg, and Lennart L¨onngren. 2013. SALDO: a touch of yin to WordNet’s yang. Language Resources and Evaluation, May. Online first publication; DOI 10.1007/s10579-013-9233-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Chan</author>
</authors>
<title>Learning probabilistic paradigms for morphology in a latent class model.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology and Morphology,</booktitle>
<pages>69--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3889" citStr="Chan, 2006" startWordPosition="566" endWordPosition="567">s long been a prominent research goal in computational linguistics. Recent studies have focused on unsupervised methods in particular—learning morphology from 1Our programs and the datasets used, including the evaluation procedure for this paper, are freely available at https://svn.spraakbanken.gu.se/clt/ eacl/2014/extract 569 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569–578, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics unlabeled data (Goldsmith, 2001; Schone and Jurafsky, 2001; Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as Durrett and DeNero (2013). Eskander et</context>
</contexts>
<marker>Chan, 2006</marker>
<rawString>Erwin Chan. 2006. Learning probabilistic paradigms for morphology in a latent class model. In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology and Morphology, pages 69–78. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lionel Cl´ement</author>
<author>Bernard Lang</author>
<author>Benoit Sagot</author>
</authors>
<title>Morphology based automatic acquisition of large-coverage lexica.</title>
<date>2004</date>
<booktitle>In LREC 04,</booktitle>
<pages>1841--1844</pages>
<marker>Cl´ement, Lang, Sagot, 2004</marker>
<rawString>Lionel Cl´ement, Bernard Lang, Benoit Sagot, et al. 2004. Morphology based automatic acquisition of large-coverage lexica. In LREC 04, pages 1841– 1844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing (TSLP),</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="3913" citStr="Creutz and Lagus, 2007" startWordPosition="568" endWordPosition="571">a prominent research goal in computational linguistics. Recent studies have focused on unsupervised methods in particular—learning morphology from 1Our programs and the datasets used, including the evaluation procedure for this paper, are freely available at https://svn.spraakbanken.gu.se/clt/ eacl/2014/extract 569 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569–578, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics unlabeled data (Goldsmith, 2001; Schone and Jurafsky, 2001; Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as Durrett and DeNero (2013). Eskander et al. (2013) shares many </context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing (TSLP), 4(1):3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gr´egoire D´etrez</author>
<author>Aarne Ranta</author>
</authors>
<title>Smart paradigms and the predictability and complexity of inflectional morphology.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th EACL,</booktitle>
<pages>645--653</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>D´etrez, Ranta, 2012</marker>
<rawString>Gr´egoire D´etrez and Aarne Ranta. 2012. Smart paradigms and the predictability and complexity of inflectional morphology. In Proceedings of the 13th EACL, pages 645–653. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Discovering morphological paradigms from plain text using a Dirichlet process mixture model.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>616--627</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4257" citStr="Dreyer and Eisner (2011)" startWordPosition="619" endWordPosition="623">onference of the European Chapter of the Association for Computational Linguistics, pages 569–578, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics unlabeled data (Goldsmith, 2001; Schone and Jurafsky, 2001; Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as Durrett and DeNero (2013). Eskander et al. (2013) shares many of the goals in this paper, but is more supervised in that it focuses on learning inflectional classes from richer annotation. A major departure from much previous work is that we do not attempt to encode variation as string-changing operations, say by string edits (Dreyer and Eisner, 2011) or transformation rules (Lind´en, 2008; Durrett and </context>
</contexts>
<marker>Dreyer, Eisner, 2011</marker>
<rawString>Markus Dreyer and Jason Eisner. 2011. Discovering morphological paradigms from plain text using a Dirichlet process mixture model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 616–627. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>John DeNero</author>
</authors>
<title>Supervised learning of complete morphological paradigms.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1185--1195</pages>
<contexts>
<context position="4287" citStr="Durrett and DeNero (2013)" startWordPosition="625" endWordPosition="628">pter of the Association for Computational Linguistics, pages 569–578, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics unlabeled data (Goldsmith, 2001; Schone and Jurafsky, 2001; Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as Durrett and DeNero (2013). Eskander et al. (2013) shares many of the goals in this paper, but is more supervised in that it focuses on learning inflectional classes from richer annotation. A major departure from much previous work is that we do not attempt to encode variation as string-changing operations, say by string edits (Dreyer and Eisner, 2011) or transformation rules (Lind´en, 2008; Durrett and DeNero, 2013) that perform map</context>
<context position="21169" citStr="Durrett and DeNero (2013)" startWordPosition="3388" endWordPosition="3391">RBS DE-NOUNS ES-VERBS FI-VERBS FI-NOUNS -ADJS 0 50 100 150 200 Figure 2: Degree of coverage with varying numbers of paradigms. Input: Output: Data inflection abstract tables paradigms DE-VERBS 1827 140 DE-NOUNS 2564 70 ES-VERBS 3855 97 FI-VERBS 7049 282 FI-NOUNS-ADJS 6200 258 Table 3: Generalization of paradigms. The number of paradigms produced from Wiktionary inflection tables by generalization and collapsing of abstract paradigms. 1.00 0.95 0.90 0.85 0.80 0.75 0.70 Paradigms 5 Evaluation To evaluate the method, we have conducted three experiments. First we repeat an experiment presented in Durrett and DeNero (2013) using the same data and experiment setup, but with our generalization method. In this experiment, we are given a number of complete inflection tables scraped from Wiktionary. The task is to reconstruct complete inflection tables from 200 held-out base forms. For this task, we evaluate per form accuracy as well as per table accuracy for reconstruction. The second experiment is the same as the first, but with additional access to an unlabeled text dump for the language from Wikipedia. In the last experiment we try to mimic the situation of a linguist starting out to describe a new language. The</context>
<context position="24905" citStr="Durrett and DeNero (2013)" startWordPosition="4000" endWordPosition="4003">extracted as described at http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor 574 Data Per D&amp;DN13 Per D&amp;DN13 Oracle accuracy table form per form (per table) DE-VERBS 68.0 85.0 97.04 96.19 99.70 (198/200) DE-NOUNS 76.5 79.5 87.81 88.94 100.00 (200/200) ES-VERBS 96.0 95.0 99.52 99.67 100.00 (200/200) FI-VERBS 92.5 87.5 96.36 96.43 99.00 (195/200) FI-NOUNS-ADJS 85.0 83.5 91.91 93.41 100.00 (200/200) Table 4: Experiment 1: Accuracy of reconstructing 200 inflection tables given only base forms from held-out data when paradigms are learned from the Wiktionary dataset. For comparison, figures from Durrett and DeNero (2013) are included (shown as D&amp;DN13). Data Per Per Oracle acc. Top-1000 rank Correct/Incorrect table form per form (table) TOP 10% 100/0 (100.0%) DE-VERBS 76.50 97.87 99.70 (198/200) TOP 50% 489/11 (97.8%) DE-NOUNS 82.00 91.81 100.00 (200/200) TOP 100% 964/36 (96.4%) ES-VERBS 98.00 99.58 100.00 (200/200) FI-VERBS 92.50 96.63 99.00 (195/200) FI-NOUNS-ADJS 88.00 93.82 100.00 (200/200) Table 6: Top-1000 rank for all nouns in SALDO Table 5: Experiment 2: Reconstructing 200 heldout inflection tables with paradigms induced from Wiktionary and further access to raw text from Wikipedia. where an increased </context>
</contexts>
<marker>Durrett, DeNero, 2013</marker>
<rawString>Greg Durrett and John DeNero. 2013. Supervised learning of complete morphological paradigms. In Proceedings of NAACL-HLT, pages 1185–1195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramy Eskander</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Automatic extraction of morphological lexicons from morphologically annotated corpora.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1032--1043</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4500" citStr="Eskander et al. (2013)" startWordPosition="659" endWordPosition="662"> Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as Durrett and DeNero (2013). Eskander et al. (2013) shares many of the goals in this paper, but is more supervised in that it focuses on learning inflectional classes from richer annotation. A major departure from much previous work is that we do not attempt to encode variation as string-changing operations, say by string edits (Dreyer and Eisner, 2011) or transformation rules (Lind´en, 2008; Durrett and DeNero, 2013) that perform mappings between forms. Rather, our goal is to encode all variation within paradigms by presenting them in a sufficiently generic fashion so as to allow affixation processes, phonological alternations as well as orth</context>
</contexts>
<marker>Eskander, Habash, Rambow, 2013</marker>
<rawString>Ramy Eskander, Nizar Habash, and Owen Rambow. 2013. Automatic extraction of morphological lexicons from morphologically annotated corpora. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Forsberg</author>
<author>Harald Hammarstr¨om</author>
<author>Aarne Ranta</author>
</authors>
<title>Morphological lexicon extraction from raw text data.</title>
<date>2006</date>
<booktitle>In Advances in Natural Language Processing,</booktitle>
<pages>488--499</pages>
<publisher>Springer.</publisher>
<marker>Forsberg, Hammarstr¨om, Ranta, 2006</marker>
<rawString>Markus Forsberg, Harald Hammarstr¨om, and Aarne Ranta. 2006. Morphological lexicon extraction from raw text data. In Advances in Natural Language Processing, pages 488–499. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational linguistics,</journal>
<pages>27--2</pages>
<contexts>
<context position="3850" citStr="Goldsmith, 2001" startWordPosition="559" endWordPosition="560">ous work Automatic learning of morphology has long been a prominent research goal in computational linguistics. Recent studies have focused on unsupervised methods in particular—learning morphology from 1Our programs and the datasets used, including the evaluation procedure for this paper, are freely available at https://svn.spraakbanken.gu.se/clt/ eacl/2014/extract 569 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569–578, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics unlabeled data (Goldsmith, 2001; Schone and Jurafsky, 2001; Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Hammarstr¨om</author>
<author>Lars Borin</author>
</authors>
<title>Unsupervised learning of morphology.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<marker>Hammarstr¨om, Borin, 2011</marker>
<rawString>Harald Hammarstr¨om and Lars Borin. 2011. Unsupervised learning of morphology. Computational Linguistics, 37(2):309–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles F Hockett</author>
</authors>
<title>Two models of grammatical description. Morphology: Critical Concepts in Linguistics,</title>
<date>1954</date>
<pages>1--110</pages>
<contexts>
<context position="5820" citStr="Hockett, 1954" startWordPosition="874" endWordPosition="875">lignment of the various forms in an inflection table, as in e.g. Tchoukalov et al. (2010). Rather, we base our algorithm on extracting the longest common subsequence (LCS) shared by all forms in an inflection table, from which alignment of segments falls out naturally. Although our paradigm representation is similar to and inspired by that of Forsberg et al. (2006) and D´etrez and Ranta (2012), our method of generalizing from inflection tables to paradigms is novel. 3 Paradigm learning In what follows, we adopt the view that words and their inflection patterns can be organized into paradigms (Hockett, 1954; Robins, 1959; Matthews, 1972; Stump, 2001). We essentially treat a paradigm as an ordered set of functions (f1, ... , fn), where fz: x1, ... , xn H E∗, that is, where each entry in a paradigm is a function from variables to strings, and each function in a particular paradigm shares the same variables. 3.1 Paradigm representation We represent the functions in what we call abstract paradigm. In our representation, an abstract paradigm is an ordered collection of strings, where each string may additionally contain interspersed variables denoted x1, x2,. . . , xn. The strings represent fixed, ob</context>
</contexts>
<marker>Hockett, 1954</marker>
<rawString>Charles F Hockett. 1954. Two models of grammatical description. Morphology: Critical Concepts in Linguistics, 1:110–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mans Hulden</author>
</authors>
<title>Foma: a finite-state compiler and library.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the European Chapter of the Association for Computational Linguistics: Demonstrations Session,</booktitle>
<pages>29--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<contexts>
<context position="11768" citStr="Hulden, 2009" startWordPosition="1848" endWordPosition="1849">illustrated in figure 1. The first step, extracting the LCS from a collection of strings, is the well-known multiple longest common subsequence problem (MLCS). It is known to be NP-hard (Maier, 1978). Although the number of strings to find the LCS from may be rather large in real-world data, we find that a few sensible heuristic techniques allow us to solve this problem efficiently for practical linguistic material, i.e., inflection tables. We calculate the LCS by calculating intersections of finite-state machines that encode all subsequences of all words, using the foma finite-state toolkit (Hulden, 2009).3 While for most tables there is only one way to segment the LCS in the various forms, some ambiguous corner cases need to be resolved by imposing additional criteria for the segmentation, given in steps 2(a) and 2(b). As an example, consider a snippet of a small conjugation table for the Spanish verb comprar (to buy), comprar#compra#compro. Obviously the LCS is compr—however, this can be distributed in two different ways across the strings, as seen below. 3Steps 2 and 3 are implemented using more involved finite-state techniques that we plan to describe elsewhere. [r]i[ng] [r]a[ng] [r]u[ng] </context>
</contexts>
<marker>Hulden, 2009</marker>
<rawString>Mans Hulden. 2009. Foma: a finite-state compiler and library. In Proceedings of the 12th Conference of the European Chapter of the European Chapter of the Association for Computational Linguistics: Demonstrations Session, pages 29–32, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>A discovery procedure for two-level phonology. Computational Lexicology and Lexicography: A Special Issue Dedicated to Bernard Quemada,</title>
<date>1991</date>
<pages>1--451</pages>
<marker>Koskenniemi, 1991</marker>
<rawString>Kimmo Koskenniemi. 1991. A discovery procedure for two-level phonology. Computational Lexicology and Lexicography: A Special Issue Dedicated to Bernard Quemada, 1:451–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>An informal discovery procedure for two-level rules.</title>
<date>2013</date>
<journal>Journal of Language Modelling,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Koskenniemi, 2013</marker>
<rawString>Kimmo Koskenniemi. 2013. An informal discovery procedure for two-level rules. Journal of Language Modelling, 1(1):155–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krister Lind´en</author>
</authors>
<title>A probabilistic model for guessing base forms of new words by analogy.</title>
<date>2008</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>106--116</pages>
<publisher>Springer.</publisher>
<marker>Lind´en, 2008</marker>
<rawString>Krister Lind´en. 2008. A probabilistic model for guessing base forms of new words by analogy. In Computational Linguistics and Intelligent Text Processing, pages 106–116. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Maier</author>
</authors>
<title>The complexity of some problems on subsequences and supersequences.</title>
<date>1978</date>
<journal>Journal of the ACM (JACM),</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="11354" citStr="Maier, 1978" startWordPosition="1781" endWordPosition="1782">riables (step ). Finally, resulting paradigms that turn out to be identical may be collapsed (step ) (section 3.3). (b) The smallest total number of infixed non-variable segments in the inflection table (segments that occur between variables). 3. Replacing the discontinuous sequences that are part of the LCS with variables (every form in a paradigm will contain the same number of variables). These steps are illustrated in figure 1. The first step, extracting the LCS from a collection of strings, is the well-known multiple longest common subsequence problem (MLCS). It is known to be NP-hard (Maier, 1978). Although the number of strings to find the LCS from may be rather large in real-world data, we find that a few sensible heuristic techniques allow us to solve this problem efficiently for practical linguistic material, i.e., inflection tables. We calculate the LCS by calculating intersections of finite-state machines that encode all subsequences of all words, using the foma finite-state toolkit (Hulden, 2009).3 While for most tables there is only one way to segment the LCS in the various forms, some ambiguous corner cases need to be resolved by imposing additional criteria for the segmentati</context>
</contexts>
<marker>Maier, 1978</marker>
<rawString>David Maier. 1978. The complexity of some problems on subsequences and supersequences. Journal of the ACM (JACM), 25(2):322–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter H Matthews</author>
</authors>
<title>Inflectional morphology: A theoretical study based on aspects of Latin verb conjugation.</title>
<date>1972</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5850" citStr="Matthews, 1972" startWordPosition="878" endWordPosition="879"> in an inflection table, as in e.g. Tchoukalov et al. (2010). Rather, we base our algorithm on extracting the longest common subsequence (LCS) shared by all forms in an inflection table, from which alignment of segments falls out naturally. Although our paradigm representation is similar to and inspired by that of Forsberg et al. (2006) and D´etrez and Ranta (2012), our method of generalizing from inflection tables to paradigms is novel. 3 Paradigm learning In what follows, we adopt the view that words and their inflection patterns can be organized into paradigms (Hockett, 1954; Robins, 1959; Matthews, 1972; Stump, 2001). We essentially treat a paradigm as an ordered set of functions (f1, ... , fn), where fz: x1, ... , xn H E∗, that is, where each entry in a paradigm is a function from variables to strings, and each function in a particular paradigm shares the same variables. 3.1 Paradigm representation We represent the functions in what we call abstract paradigm. In our representation, an abstract paradigm is an ordered collection of strings, where each string may additionally contain interspersed variables denoted x1, x2,. . . , xn. The strings represent fixed, obligatory parts of a paradigm, </context>
</contexts>
<marker>Matthews, 1972</marker>
<rawString>Peter H. Matthews. 1972. Inflectional morphology: A theoretical study based on aspects of Latin verb conjugation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Jaime Carbonell</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
</authors>
<title>Paramor: finding paradigms across morphology.</title>
<date>2008</date>
<booktitle>In Advances in Multilingual and Multimodal Information Retrieval,</booktitle>
<pages>900--907</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3935" citStr="Monson et al., 2008" startWordPosition="572" endWordPosition="575">l in computational linguistics. Recent studies have focused on unsupervised methods in particular—learning morphology from 1Our programs and the datasets used, including the evaluation procedure for this paper, are freely available at https://svn.spraakbanken.gu.se/clt/ eacl/2014/extract 569 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569–578, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics unlabeled data (Goldsmith, 2001; Schone and Jurafsky, 2001; Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as Durrett and DeNero (2013). Eskander et al. (2013) shares many of the goals in this p</context>
</contexts>
<marker>Monson, Carbonell, Lavie, Levin, 2008</marker>
<rawString>Christian Monson, Jaime Carbonell, Alon Lavie, and Lori Levin. 2008. Paramor: finding paradigms across morphology. In Advances in Multilingual and Multimodal Information Retrieval, pages 900– 907. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Neuvel</author>
<author>Sean A Fulop</author>
</authors>
<title>Unsupervised learning of morphology without morphemes.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6,</booktitle>
<pages>31--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4160" citStr="Neuvel and Fulop (2002)" startWordPosition="605" endWordPosition="608">available at https://svn.spraakbanken.gu.se/clt/ eacl/2014/extract 569 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569–578, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics unlabeled data (Goldsmith, 2001; Schone and Jurafsky, 2001; Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as Durrett and DeNero (2013). Eskander et al. (2013) shares many of the goals in this paper, but is more supervised in that it focuses on learning inflectional classes from richer annotation. A major departure from much previous work is that we do not attempt to encode variation as string-changing operations, s</context>
</contexts>
<marker>Neuvel, Fulop, 2002</marker>
<rawString>Sylvain Neuvel and Sean A Fulop. 2002. Unsupervised learning of morphology without morphemes. In Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6, pages 31–40. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert H Robins</author>
</authors>
<date>1959</date>
<booktitle>In defence of WP. Transactions of the Philological Society,</booktitle>
<volume>58</volume>
<issue>1</issue>
<contexts>
<context position="5834" citStr="Robins, 1959" startWordPosition="876" endWordPosition="877"> various forms in an inflection table, as in e.g. Tchoukalov et al. (2010). Rather, we base our algorithm on extracting the longest common subsequence (LCS) shared by all forms in an inflection table, from which alignment of segments falls out naturally. Although our paradigm representation is similar to and inspired by that of Forsberg et al. (2006) and D´etrez and Ranta (2012), our method of generalizing from inflection tables to paradigms is novel. 3 Paradigm learning In what follows, we adopt the view that words and their inflection patterns can be organized into paradigms (Hockett, 1954; Robins, 1959; Matthews, 1972; Stump, 2001). We essentially treat a paradigm as an ordered set of functions (f1, ... , fn), where fz: x1, ... , xn H E∗, that is, where each entry in a paradigm is a function from variables to strings, and each function in a particular paradigm shares the same variables. 3.1 Paradigm representation We represent the functions in what we call abstract paradigm. In our representation, an abstract paradigm is an ordered collection of strings, where each string may additionally contain interspersed variables denoted x1, x2,. . . , xn. The strings represent fixed, obligatory parts</context>
</contexts>
<marker>Robins, 1959</marker>
<rawString>Robert H Robins. 1959. In defence of WP. Transactions of the Philological Society, 58(1):116–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Knowledge-free induction of inflectional morphologies.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3877" citStr="Schone and Jurafsky, 2001" startWordPosition="561" endWordPosition="565">c learning of morphology has long been a prominent research goal in computational linguistics. Recent studies have focused on unsupervised methods in particular—learning morphology from 1Our programs and the datasets used, including the evaluation procedure for this paper, are freely available at https://svn.spraakbanken.gu.se/clt/ eacl/2014/extract 569 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569–578, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics unlabeled data (Goldsmith, 2001; Schone and Jurafsky, 2001; Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as Durrett and DeNero (2013).</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2001. Knowledge-free induction of inflectional morphologies. In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory T Stump</author>
</authors>
<title>A theory of paradigm structure.</title>
<date>2001</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5864" citStr="Stump, 2001" startWordPosition="880" endWordPosition="881">n table, as in e.g. Tchoukalov et al. (2010). Rather, we base our algorithm on extracting the longest common subsequence (LCS) shared by all forms in an inflection table, from which alignment of segments falls out naturally. Although our paradigm representation is similar to and inspired by that of Forsberg et al. (2006) and D´etrez and Ranta (2012), our method of generalizing from inflection tables to paradigms is novel. 3 Paradigm learning In what follows, we adopt the view that words and their inflection patterns can be organized into paradigms (Hockett, 1954; Robins, 1959; Matthews, 1972; Stump, 2001). We essentially treat a paradigm as an ordered set of functions (f1, ... , fn), where fz: x1, ... , xn H E∗, that is, where each entry in a paradigm is a function from variables to strings, and each function in a particular paradigm shares the same variables. 3.1 Paradigm representation We represent the functions in what we call abstract paradigm. In our representation, an abstract paradigm is an ordered collection of strings, where each string may additionally contain interspersed variables denoted x1, x2,. . . , xn. The strings represent fixed, obligatory parts of a paradigm, while the vari</context>
</contexts>
<marker>Stump, 2001</marker>
<rawString>Gregory T. Stump. 2001. A theory of paradigm structure. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tzvetan Tchoukalov</author>
<author>Christian Monson</author>
<author>Brian Roark</author>
</authors>
<title>Morphological analysis by multiple sequence alignment. In Multilingual Information Access Evaluation I. Text Retrieval Experiments,</title>
<date>2010</date>
<pages>666--673</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5296" citStr="Tchoukalov et al. (2010)" startWordPosition="787" endWordPosition="790">ious work is that we do not attempt to encode variation as string-changing operations, say by string edits (Dreyer and Eisner, 2011) or transformation rules (Lind´en, 2008; Durrett and DeNero, 2013) that perform mappings between forms. Rather, our goal is to encode all variation within paradigms by presenting them in a sufficiently generic fashion so as to allow affixation processes, phonological alternations as well as orthographic changes to naturally fall out of the paradigm specification itself. Also, we perform no explicit alignment of the various forms in an inflection table, as in e.g. Tchoukalov et al. (2010). Rather, we base our algorithm on extracting the longest common subsequence (LCS) shared by all forms in an inflection table, from which alignment of segments falls out naturally. Although our paradigm representation is similar to and inspired by that of Forsberg et al. (2006) and D´etrez and Ranta (2012), our method of generalizing from inflection tables to paradigms is novel. 3 Paradigm learning In what follows, we adopt the view that words and their inflection patterns can be organized into paradigms (Hockett, 1954; Robins, 1959; Matthews, 1972; Stump, 2001). We essentially treat a paradig</context>
</contexts>
<marker>Tchoukalov, Monson, Roark, 2010</marker>
<rawString>Tzvetan Tchoukalov, Christian Monson, and Brian Roark. 2010. Morphological analysis by multiple sequence alignment. In Multilingual Information Access Evaluation I. Text Retrieval Experiments, pages 666–673. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pieter Theron</author>
<author>Ian Cloete</author>
</authors>
<title>Automatic acquisition of two-level morphological rules.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied natural language processing,</booktitle>
<pages>103--110</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Theron, Cloete, 1997</marker>
<rawString>Pieter Theron and Ian Cloete. 1997. Automatic acquisition of two-level morphological rules. In Proceedings of the fifth conference on Applied natural language processing, pages 103–110. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based morphological analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>285--292</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>van den Bosch, Daelemans, 1999</marker>
<rawString>Antal van den Bosch and Walter Daelemans. 1999. Memory-based morphological analysis. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 285–292. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Richard Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>207--216</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4135" citStr="Yarowsky and Wicentowski (2000)" startWordPosition="601" endWordPosition="604">edure for this paper, are freely available at https://svn.spraakbanken.gu.se/clt/ eacl/2014/extract 569 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569–578, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics unlabeled data (Goldsmith, 2001; Schone and Jurafsky, 2001; Chan, 2006; Creutz and Lagus, 2007; Monson et al., 2008). Hammarstr¨om and Borin (2011) provides a current overview of unsupervised learning. Previous work with similar semi-supervised goals as the ones in this paper include Yarowsky and Wicentowski (2000), Neuvel and Fulop (2002), Cl´ement et al. (2004). Recent machine learning oriented work includes Dreyer and Eisner (2011) and Durrett and DeNero (2013), which documents a method to learn orthographic transformation rules to capture patterns across inflection tables. Part of our evaluation uses the same dataset as Durrett and DeNero (2013). Eskander et al. (2013) shares many of the goals in this paper, but is more supervised in that it focuses on learning inflectional classes from richer annotation. A major departure from much previous work is that we do not attempt to encode variation as stri</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>David Yarowsky and Richard Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 207–216. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>