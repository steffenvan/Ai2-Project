<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002057">
<title confidence="0.999526">
A Hassle-Free Unsupervised Domain Adaptation Method
Using Instance Similarity Features
</title>
<author confidence="0.998004">
Jianfei Yu
</author>
<affiliation confidence="0.895809">
School of Information Systems
Singapore Management University
</affiliation>
<email confidence="0.994936">
jfyu.2014@phdis.smu.edu.sg
</email>
<sectionHeader confidence="0.993786" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999728125">
We present a simple yet effective unsu-
pervised domain adaptation method that
can be generally applied for different NLP
tasks. Our method uses unlabeled tar-
get domain instances to induce a set of
instance similarity features. These fea-
tures are then combined with the origi-
nal features to represent labeled source do-
main instances. Using three NLP tasks,
we show that our method consistently out-
performs a few baselines, including SCL,
an existing general unsupervised domain
adaptation method widely used in NLP.
More importantly, our method is very easy
to implement and incurs much less com-
putational cost than SCL.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9987435">
Domain adaptation aims to use labeled data from
a source domain to help build a system for a target
domain, possibly with a small amount of labeled
data from the target domain. The problem arises
when the target domain has a different data distri-
bution from the source domain, which is often the
case. In NLP, domain adaptation has been well
studied in recent years. Existing work has pro-
posed both techniques designed for specific NLP
tasks (Chan and Ng, 2007; Daume III and Ja-
garlamudi, 2011; Yang et al., 2012; Plank and
Moschitti, 2013; Hu et al., 2014; Nguyen et al.,
2014; Nguyen and Grishman, 2014) and general
approaches applicable to different tasks (Blitzer
et al., 2006; Daum´e III, 2007; Jiang and Zhai,
2007; Dredze and Crammer, 2008; Titov, 2011).
With the recent trend of applying deep learn-
ing in NLP, deep learning-based domain adap-
tation methods (Glorot et al., 2011; Chen et
al., 2012; Yang and Eisenstein, 2014) have also
been adopted for NLP tasks (Yang and Eisenstein,
2015).
</bodyText>
<author confidence="0.839097">
Jing Jiang
</author>
<affiliation confidence="0.849691">
School of Information Systems
Singapore Management University
</affiliation>
<email confidence="0.976264">
jingjiang@smu.edu.sg
</email>
<bodyText confidence="0.999827268292683">
There are generally two settings of domain
adaptation. We use supervised domain adaptation
to refer to the setting when a small amount of la-
beled target data is available, and when no such
data is available during training we call it unsu-
pervised domain adaptation.
Although many domain adaptation methods
have been proposed, for practitioners who wish
to avoid implementing or tuning sophisticated or
computationally expensive methods due to either
lack of enough machine learning background or
limited resources, simple approaches are often
more attractive. A notable example is the frus-
tratingly easy domain adaptation method proposed
by Daum´e III (2007), which simply augments
the feature space by duplicating features in a
clever way. However, this method is only suit-
able for supervised domain adaptation. A later
semi-supervised version of this easy adaptation
method uses unlabeled data from the target do-
main (Daum´e III et al., 2010), but it still requires
some labeled data from the target domain. In this
paper, we propose a general unsupervised domain
adaptation method that is almost equally hassle-
free but does not use any labeled target data.
Our method uses a set of unlabeled target in-
stances to induce a new feature space, which is
then combined with the original feature space. We
explain analytically why the new feature space
may help domain adaptation. Using a few dif-
ferent NLP tasks, we then empirically show that
our method can indeed learn a better classifier for
the target domain than a few baselines. In partic-
ular, our method performs consistently better than
or competitively with Structural Correspondence
Learning (SCL) (Blitzer et al., 2006), a well-
known unsupervised domain adaptation method in
NLP. Furthermore, compared with SCL and other
advanced methods such as the marginalized struc-
tured dropout method (Yang and Eisenstein, 2014)
and a recent feature embedding method (Yang and
</bodyText>
<page confidence="0.862503">
168
</page>
<bodyText confidence="0.821437375">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 168–173,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Eisenstein, 2015), our method is much easier to
implement.
In summary, our main contribution is a simple,
effective and theoretically justifiable unsupervised
domain adaptation method for NLP problems.
</bodyText>
<sectionHeader confidence="0.597026" genericHeader="method">
2 Adaptation with Similarity Features
</sectionHeader>
<bodyText confidence="0.999980791666667">
We first introduce the necessary notation needed
for presenting our method. Without loss of gen-
erality, we assume a binary classification problem
where each input is represented as a feature vec-
tor x from an input vector space X and the out-
put is a label y E {0, 1}. This assumption is
general because many NLP tasks such as text cat-
egorization, NER and relation extraction can be
cast into classification problems and our discus-
sion below can be easily extended to multi-class
settings. We further assume that we have a set of
labeled instances from a source domain, denoted
by Ds = {(xsi, ysi)}Ni=1. We also have a set of un-
labeled instances from a target domain, denoted
by Dt = {xtj}Mj=1. We assume a general setting
of learning a linear classifier, which is essentially
a weight vector w such that x is labeled as 1 if
wTx ≥ 0.1
A naive method is to simply learn a classifier
from Ds. The goal of unsupervised domain adap-
tation is to make use of both Ds and Dt to learn a
good w for the target domain. It has to be assumed
that the source and the target domains are similar
enough such that adaptation is possible.
</bodyText>
<subsectionHeader confidence="0.986314">
2.1 The Method
</subsectionHeader>
<bodyText confidence="0.993526428571428">
Our method works as follows. We first randomly
select a subset of target instances from Dt and
normalize them. We refer to the resulting vectors
as exemplar vectors, denoted by E = {e(k)}Kk=1.
Next, we transform each source instance x into
a new feature vector by computing its similarity
with each e(k), as defined below:
</bodyText>
<equation confidence="0.997297">
g(x) = [s(x, e(1)),... , s(x, e(K))]T, (1)
</equation>
<bodyText confidence="0.999935666666667">
where T indicates transpose and s(x, x&apos;) is a sim-
ilarity function between x and x&apos;. In our work
we use dot product as s.2 Once each labeled
</bodyText>
<footnote confidence="0.951598285714286">
1A bias feature that is always set to be 1 can be added to
allow a non-zero threshold.
2We find that normalizing the exemplar vectors results in
better performance empirically. On the other hand, if we nor-
malize both the exemplar vectors and each instance x, i.e. if
we use cosine similarity as s, the performance is similar to
not normalizing x.
</footnote>
<bodyText confidence="0.999972526315789">
source domain instance is transformed into a K-
dimensional vector by Equation 1, we can ap-
pend this vector to the original feature vector of
the source instance and use the combined feature
vectors of all labeled source instances to train a
classifier. To apply this classifier to the target do-
main, each target instance also needs to add this
K-dimensional induced feature vector.
It is worth noting that the exemplar vectors
are randomly chosen from the available target in-
stances and no special trick is needed. Overall,
the method is fairly easy to implement, and yet
as we will see in Section 3, it performs surpris-
ingly well. We also want to point out that our in-
stance similarity features bear strong similarity to
what was proposed by Sun and Lam (2013), but
their work addresses a completely different prob-
lem and we developed our method independently
of their work.
</bodyText>
<subsectionHeader confidence="0.994397">
2.2 Justification
</subsectionHeader>
<bodyText confidence="0.999987">
In this section, we provide some intuitive justifica-
tion for our method without any theoretical proof.
</bodyText>
<subsectionHeader confidence="0.673336">
Learning in the Target Subspace
</subsectionHeader>
<bodyText confidence="0.995539851851852">
Blitzer et al. (2011) pointed out that the hope
of unsupervised domain adaptation is to “couple”
the learning of weights for target-specific features
with that of common features. We show our in-
duced feature representation is exactly doing this.
First, we review the claim by Blitzer et al.
(2011). We note that although the input vector
space X is typically high-dimensional for NLP
tasks, the actual space where input vectors lie can
have a lower dimension because of the strong fea-
ture dependence we observe with NLP tasks. For
example, binary features defined from the same
feature template such as the previous word are
mutually exclusive. Furthermore, the actual low-
dimensional spaces for the source and the target
domains are usually different because of domain-
specific features and distributional difference be-
tween the domains. Borrowing the notation used
by Blitzer et al. (2011), define subspace Xs to be
the (lowest dimensional) subspace of X spanned
by all source domain input vectors. Similarly,
a subspace Xt can be defined. Define Xs,t =
Xs n Xt, the shared subspace between the two do-
mains. Define Xs,1 to be the subspace that is or-
thogonal to Xs,t but together with Xs,t spans Xs,
that is, Xs,1 + Xs,t = Xs. Similarly we can define
X1,t. Essentially Xs,t, Xs,1 and X1,t are the shared
</bodyText>
<page confidence="0.994323">
169
</page>
<bodyText confidence="0.99933025">
subspace and the domain-specific subspaces, and
they are mutually orthogonal.
We can project any input vector x into the three
subspaces defined above as follows:
</bodyText>
<equation confidence="0.936378">
x = xs,t + xs,⊥ + x⊥,t.
</equation>
<bodyText confidence="0.970307965517242">
Similarly, any linear classifier w can be decom-
posed into ws,t, ws,⊥ and w⊥,t, and
w&gt;x = w&gt; s,txs,t + w&gt; s,⊥xs,⊥ + w&gt; ⊥,tx⊥,t.
For a naive method that simply learns w from Ds,
the learned component w⊥,t will be 0, because the
component x⊥,t of any source instance is 0, and
therefore the training error would not be reduced
by any non-zero w⊥,t. Moreover, any non-zero
ws,⊥ learned from Ds would not be useful for the
target domain because for all target instances we
have xs,⊥ = 0. So for a w learned from Ds, only
its component ws,t is useful for domain transfer.
Blitzer et al. (2011) argues that with unlabeled
target instances, we can hope to “couple” the
learning of w⊥,t with that of ws,t. We show that
if we use only our induced feature representation
without appending it to the original feature vec-
tor, we can achieve this. We first define a ma-
trix ME whose column vectors are the exemplar
vectors from £. Then g(x) can be rewritten as
M&gt;E x. Let w0 denote a linear classifier learned
from the transformed labeled data. w0 makes pre-
diction based on w0&gt;M&gt;E x, which is the same as
(MEw0)&gt;x. This shows that the learned classifier
w0 for the induced features is equivalent to a linear
classifier w = MEw0 for the original features.
It is not hard to see that MEw0 is essentially
Pk w0ke(k), i.e. a linear combination of vectors
in £. Because e(k) comes from Xt, we can write
</bodyText>
<equation confidence="0.9504222">
e(k) = e(k)
s,t + e(k)
⊥,t. Therefore we have
Xw =
k
</equation>
<bodyText confidence="0.999862818181818">
There are two things to note from the formula
above. (1) The learned classifier w does not have
any component in the subspace Xs,⊥, which is
good because such a component would not be use-
ful for the target domain. (2) The learned w⊥,t will
unlikely be zero because its learning is “coupled”
with the learning of ws,t through w0. In effect, we
pick up target specific features that correlate with
useful common features.
In practice, however, we need to append the in-
duced features to the original features to achieve
good adaptation results. One may find this
counter-intuitive because this results in an ex-
panded instead of restricted hypothesis space. Our
explanation is that because of the typical L2 regu-
larizer used during training, there is an incentive to
shift the weight mass to the additional induced fea-
tures. The need to combine the induced features
with original features was also reported in previ-
ous domain adaptation work such as SCL (Blitzer
et al., 2006) and marginalized denoising autoen-
coders (Chen et al., 2012).
</bodyText>
<subsectionHeader confidence="0.823335">
Reduction of Domain Divergence
</subsectionHeader>
<bodyText confidence="0.999947571428571">
Another theory on domain adaptation developed
by Ben-David et al. (2010) essentially states that
we should use a hypothesis space that can achieve
low error on the source domain while at the same
time making it hard to separate source and tar-
get instances. If we use only our induced fea-
tures, then Xs,⊥ is excluded from the hypothesis
space. This is likely to make it harder to distin-
guish source and target instances. To verify this,
in Table 1 we show the following errors based
on three feature representations: (1) The training
error on the source domain (ˆεs). (2) The classi-
fication error when we train a classifier to sepa-
rate source and target instances. (3) The error on
the target domain using the classifier trained from
the source domain (ˆεt). ISF- means only our in-
duced instance similarity features are used while
ISF uses combined feature vectors. The results
show that ISF achieves relatively low ˆεs and in-
creases the domain separation error. These two
factors lead to a reduction in ˆεt.
</bodyText>
<table confidence="0.9826745">
features ˆεs domain separation error ˆεt
Original 0.000 0.011 0.283
ISF- 0.120 0.129 0.315
ISF 0.006 0.062 0.254
</table>
<tableCaption confidence="0.997372">
Table 1: Three errors of different feature representations on
a spam filtering task. K is 200 for ISF- and ISF. We expect a
low ˆεt when ˆεs is low and domain separation error is high.
</tableCaption>
<sectionHeader confidence="0.383284" genericHeader="method">
Difference from EA++
</sectionHeader>
<bodyText confidence="0.998889">
The easy domain adaptation method EA proposed
by Daum´e III (2007) has later been extended to a
semi-supervised version EA++ (Daum´e III et al.,
2010), where unlabeled data from the target do-
main is also used. Theoretical justifications for
both EA and EA++ are given by Kumar et al.
</bodyText>
<equation confidence="0.9971615">
 |{z }
ws,t
|{z }
w⊥,t
0 (k)
wk es t
X
+
k
0 (k)
wke⊥ t
.
</equation>
<page confidence="0.956508">
170
</page>
<bodyText confidence="0.993216">
(2010). Here we briefly discuss how our method
is different from EA++ in terms of using unla-
beled data. In both EA and EA++, since labeled
target data is available, the algorithms still learn
two classifiers, one for each domain. In our al-
gorithm, we only learn a single classifier using
labeled data from the source domain. In EA++,
unlabeled target data is used to construct a reg-
ularizer that brings the two classifiers of the two
domains closer. Specifically, the regularizer de-
fines a penalty if the source classifier and the tar-
get classifier make different predictions on an un-
labeled target instance. However, with this regu-
larizer, EA++ does not strictly restrict either the
source classifier or the target classifier to lie in
the target subspace Xt. In contrast, as we have
pointed out above, when only the induced features
are used, our method leverages the unlabeled tar-
get instances to force the learned classifier to lie in
Xt.
</bodyText>
<sectionHeader confidence="0.999712" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999881">
3.1 Tasks and Data Sets
</subsectionHeader>
<bodyText confidence="0.99934728">
We consider the following NLP tasks.
Personalized Spam Filtering (Spam): The data
set comes from ECML/PKDD 2006 discovery
challenge. The goal is to adapt a spam filter trained
on a common pool of 4000 labeled emails to three
individual users’ personal inboxes, each contain-
ing 2500 emails. We use bag-of-word features for
this task, and we report classification accuracy.
Gene Name Recognition (NER): The data set
comes from BioCreAtIvE Task 1B (Hirschman et
al., 2005). It contains three sets of Medline ab-
stracts with labeled gene names. Each set corre-
sponds to a single species (fly, mouse or yeast).
We consider domain adaptation from one species
to another. We use standard NER features includ-
ing words, POS tags, prefixes/suffixes and contex-
tual features. We report F1 scores for this task.
Relation Extraction (Relation): We use the
ACE2005 data where the annotated documents are
from several different sources such as broadcast
news and conversational telephone speech. We re-
port the F1 scores of identifying the 7 major rela-
tion types. We use standard features including en-
tity types, entity head words, contextual words and
other syntactic features derived from parse trees.
</bodyText>
<subsectionHeader confidence="0.898163">
3.2 Methods for Comparison
</subsectionHeader>
<bodyText confidence="0.987047791666667">
Naive uses the original features.
Common uses only features commonly seen in
both domains.
SCL is our implementation of Structural Corre-
spondence Learning (Blitzer et al., 2006). We set
the number of induced features to 50 based on pre-
liminary experiments. For pivot features, we fol-
low the setting used by Blitzer et al. (2006) and se-
lect the features with a term frequency more than
50 in both domains.
PCA uses principal component analysis on Dt to
obtain K-dimensional induced feature vectors and
then appends them to the original feature vectors.
ISF is our method using instance similarity fea-
tures. We first transform each training instance to
a K-dimensional vector according to Equation 1
and then append the vector to the original vector.
For all the three NLP tasks and the methods
above that we compare, we employ the logistic re-
gression (a.k.a. maximum entropy) classification
algorithm with L2 regularization to train a clas-
sifier, which means the loss function is the cross
entropy error. We use the L-BFGS optimization
algorithm to optimize our objective function.
</bodyText>
<subsectionHeader confidence="0.916667">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.998283391304348">
In Table 2, we show the comparison between our
method and Naive, Common and SCL. For ISF,
the parameter K is set to 100 for Spam, 50 for
NER and 500 for Relation after tuning. As we
can see from the table, Common, which removes
source domain specific features during training,
can sometimes improve the classification perfor-
mance, but this is not consistent and the improve-
ment is small. SCL can improve the performance
in most settings for all three tasks, which confirms
the general effectiveness of this method. For our
method ISF, we can see that on average it outper-
forms both Naive and SCL significantly. When
we zoom into the different source-target domain
pairs of the three tasks, we can see that ISF out-
performs SCL in most of the cases. This shows
that our method is competitive despite its simplic-
ity. It is also worth pointing out that SCL incurs
much more computational cost than ISF.
We next compare ISF with PCA. Because PCA
is also expensive, we only managed to run it on
the Spam task. Table 3 shows that ISF also out-
performs PCA significantly.
</bodyText>
<page confidence="0.995695">
171
</page>
<table confidence="0.9995065">
Method u00 Spam average f→y f→m m→y NER y→f y→m average Relation
u01 u02 m→f average
Naive 0.678 0.710 0.816 0.735 0.396 0.379 0.526 0.222 0.050 0.339 0.319 0.398
Common 0.697 0.732 0.781 0.737 0.409 0.388 0.559 0.208 0.059 0.344 0.328 0.401
SCL 0.699 0.717 0.824 0.747 0.405 0.380 0.525 0.239 0.063 0.35 0.327 0.403
ISF 0.720 0.769 0.884 0.791** 0.415 0.395 0.566 0.212 0.079 0.360 0.338** 0.416**
Method bc→bn bc→cts bc→nw bc→un Relation bn→cts bn→nw bn→un bn→wl
bc→wl bn→bc
Naive 0.455 0.400 0.445 0.376 0.397 0.528 0.430 0.482 0.469 0.454
Common 0.484 0.408 0.446 0.373 0.400 0.536 0.452 0.478 0.465 0.444
SCL 0.467 0.395 0.453 0.391 0.415 0.531 0.434 0.484 0.461 0.461
ISF 0.474 0.434 0.455 0.446 0.405 0.537 0.454 0.484 0.504 0.460
cts→bc cts→bn cts→nw cts→un cts→wl nw2bc nw→bn nw→cts nw→un nw→wl
Naive 0.358 0.355 0.307 0.446 0.358 0.476 0.433 0.360 0.394 0.420
Common 0.345 0.336 0.292 0.432 0.339 0.475 0.441 0.363 0.399 0.429
SCL 0.361 0.359 0.314 0.448 0.357 0.480 0.439 0.354 0.405 0.426
ISF 0.387 0.377 0.333 0.449 0.361 0.488 0.439 0.342 0.401 0.431
un→bc un→bn un→cts un→nw un→wl wl→bc wl→bn wl→cts wl→nw wl→un
Naive 0.373 0.394 0.423 0.357 0.375 0.355 0.338 0.282 0.373 0.316
Common 0.399 0.409 0.416 0.370 0.370 0.351 0.364 0.298 0.379 0.335
SCL 0.379 0.399 0.423 0.356 0.377 0.361 0.355 0.288 0.381 0.330
ISF 0.442 0.404 0.436 0.381 0.380 0.389 0.368 0.298 0.395 0.329
</table>
<tableCaption confidence="0.9210125">
Table 2: Comparison of performance on three NLP tasks. For each source-target pair of each task, the performance shown
is the average of 5-fold cross validation. We also report the overall average performance for each task. We tested statistical
significance only for the overall average performance and found that ISF was significantly better than both Naive and SCL with
P &lt; 0.05 (indicated by **) based on the Wilcoxon signed-rank test.
</tableCaption>
<table confidence="0.9998334">
Method u00 Spam average
u01 u02
Naive 0.678 0.710 0.816 0.735
PCA 0.700 0.718 0.818 0.745
ISF 0.720 0.769 0.884 0.791**
</table>
<tableCaption confidence="0.99995">
Table 3: Comparison between ISF and PCA.
</tableCaption>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999957125">
We presented a hassle-free unsupervised domain
adaptation method. The method is simple to im-
plement, fast to run and yet effective for a few
NLP tasks, outperforming SCL, a widely-used un-
supervised domain adaptation method. We believe
the proposed method can benefit a large number of
practitioners who prefer simple methods than so-
phisticated domain adaptation methods.
</bodyText>
<sectionHeader confidence="0.943522" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999772">
We would like to thank the reviewers for their
valuable comments.
</bodyText>
<sectionHeader confidence="0.999322" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998772815789474">
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from differ-
ent domains. Machine Learning, 79(1-2):151–175.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120–128. Association for Com-
putational Linguistics.
John Blitzer, Sham Kakade, and Dean P. Foster. 2011.
Domain adaptation with coupled subspaces. In Pro-
ceedings of the Fourteenth International Conference
on Artificial Intelligence and Statistics, pages 173–
181.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 49–56.
Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Wein-
berger, and Fei Sha. 2012. Marginalized denoising
autoencoders for domain adaptation. In Proceed-
ings of the 29th International Conference on Ma-
chine Learning.
Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by min-
ing unseen words. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
407–412.
Hal Daum´e III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Pro-
cessing, pages 53–59.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
</reference>
<page confidence="0.989867">
172
</page>
<reference confidence="0.9986993">
the Association of Computational Linguistics, pages
256–263.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 689–
697.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In In Pro-
ceedings of the Twenty-eight International Confer-
ence on Machine Learning.
Lynette Hirschman, Marc Colosimo, Alexander Mor-
gan, and Alexander Yeh. 2005. Overview of
BioCreAtIvE task 1B: normailized gene lists. BMC
Bioinformatics, 6(Suppl 1):S11.
Yuening Hu, Ke Zhai, Vladimir Eidelman, and Jordan
Boyd-Graber. 2014. Polylingual tree-based topic
models for translation domain adaptation. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1166–
1176.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 264–271.
Abhishek Kumar, Avishek Saha, and Hal Daume.
2010. Co-regularization based semi-supervised do-
main adaptation. In Advances in neural information
processing systems, pages 478–486.
Thien Huu Nguyen and Ralph Grishman. 2014. Em-
ploying word representations and regularization for
domain adaptation of relation extraction. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 68–74.
Minh Luan Nguyen, Ivor W. Tsang, Kian Ming A.
Chai, and Hai Leong Chieu. 2014. Robust domain
adaptation for relation extraction via clustering con-
sistency. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 807–817.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 1498–1507.
Chensheng Sun and Kin-Man Lam. 2013. Multiple-
kernel, multiple-instance similarity features for effi-
cient visual object detection. IEEE Transactions on
Image Processing, 22(8):3050–3061.
Ivan Titov. 2011. Domain adaptation by constraining
inter-domain variability of latent feature representa-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 62–71.
Yi Yang and Jacob Eisenstein. 2014. Fast easy unsu-
pervised domain adaptation with marginalized struc-
tured dropout. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 538–544.
Yi Yang and Jacob Eisenstein. 2015. Unsupervised
multi-domain adaptation with feature embeddings.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics, pages
672–682.
Pei Yang, Wei Gao, Qi Tan, and Kam-Fai Wong.
2012. Information-theoretic multi-view domain
adaptation. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 270–274.
</reference>
<page confidence="0.999105">
173
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.663248">
<title confidence="0.999768">A Hassle-Free Unsupervised Domain Adaptation Using Instance Similarity Features</title>
<author confidence="0.979775">Jianfei</author>
<affiliation confidence="0.9638845">School of Information Singapore Management</affiliation>
<email confidence="0.824255">jfyu.2014@phdis.smu.edu.sg</email>
<abstract confidence="0.991487294117647">We present a simple yet effective unsupervised domain adaptation method that can be generally applied for different NLP tasks. Our method uses unlabeled target domain instances to induce a set of instance similarity features. These features are then combined with the original features to represent labeled source domain instances. Using three NLP tasks, we show that our method consistently outperforms a few baselines, including SCL, an existing general unsupervised domain adaptation method widely used in NLP. More importantly, our method is very easy to implement and incurs much less computational cost than SCL.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--1</pages>
<contexts>
<context position="11447" citStr="Ben-David et al. (2010)" startWordPosition="1924" endWordPosition="1927">ve good adaptation results. One may find this counter-intuitive because this results in an expanded instead of restricted hypothesis space. Our explanation is that because of the typical L2 regularizer used during training, there is an incentive to shift the weight mass to the additional induced features. The need to combine the induced features with original features was also reported in previous domain adaptation work such as SCL (Blitzer et al., 2006) and marginalized denoising autoencoders (Chen et al., 2012). Reduction of Domain Divergence Another theory on domain adaptation developed by Ben-David et al. (2010) essentially states that we should use a hypothesis space that can achieve low error on the source domain while at the same time making it hard to separate source and target instances. If we use only our induced features, then Xs,⊥ is excluded from the hypothesis space. This is likely to make it harder to distinguish source and target instances. To verify this, in Table 1 we show the following errors based on three feature representations: (1) The training error on the source domain (ˆεs). (2) The classification error when we train a classifier to separate source and target instances. (3) The </context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2010</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from different domains. Machine Learning, 79(1-2):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1510" citStr="Blitzer et al., 2006" startWordPosition="237" endWordPosition="240">ain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the setting when a small amount of labeled target data is available, and when no such</context>
<context position="3599" citStr="Blitzer et al., 2006" startWordPosition="568" endWordPosition="571">ed domain adaptation method that is almost equally hasslefree but does not use any labeled target data. Our method uses a set of unlabeled target instances to induce a new feature space, which is then combined with the original feature space. We explain analytically why the new feature space may help domain adaptation. Using a few different NLP tasks, we then empirically show that our method can indeed learn a better classifier for the target domain than a few baselines. In particular, our method performs consistently better than or competitively with Structural Correspondence Learning (SCL) (Blitzer et al., 2006), a wellknown unsupervised domain adaptation method in NLP. Furthermore, compared with SCL and other advanced methods such as the marginalized structured dropout method (Yang and Eisenstein, 2014) and a recent feature embedding method (Yang and 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 168–173, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Eisenstein, 2015), our method is much easier to implement. In summary, our </context>
<context position="11282" citStr="Blitzer et al., 2006" startWordPosition="1900" endWordPosition="1903">target specific features that correlate with useful common features. In practice, however, we need to append the induced features to the original features to achieve good adaptation results. One may find this counter-intuitive because this results in an expanded instead of restricted hypothesis space. Our explanation is that because of the typical L2 regularizer used during training, there is an incentive to shift the weight mass to the additional induced features. The need to combine the induced features with original features was also reported in previous domain adaptation work such as SCL (Blitzer et al., 2006) and marginalized denoising autoencoders (Chen et al., 2012). Reduction of Domain Divergence Another theory on domain adaptation developed by Ben-David et al. (2010) essentially states that we should use a hypothesis space that can achieve low error on the source domain while at the same time making it hard to separate source and target instances. If we use only our induced features, then Xs,⊥ is excluded from the hypothesis space. This is likely to make it harder to distinguish source and target instances. To verify this, in Table 1 we show the following errors based on three feature represen</context>
<context position="15417" citStr="Blitzer et al., 2006" startWordPosition="2601" endWordPosition="2604">ort F1 scores for this task. Relation Extraction (Relation): We use the ACE2005 data where the annotated documents are from several different sources such as broadcast news and conversational telephone speech. We report the F1 scores of identifying the 7 major relation types. We use standard features including entity types, entity head words, contextual words and other syntactic features derived from parse trees. 3.2 Methods for Comparison Naive uses the original features. Common uses only features commonly seen in both domains. SCL is our implementation of Structural Correspondence Learning (Blitzer et al., 2006). We set the number of induced features to 50 based on preliminary experiments. For pivot features, we follow the setting used by Blitzer et al. (2006) and select the features with a term frequency more than 50 in both domains. PCA uses principal component analysis on Dt to obtain K-dimensional induced feature vectors and then appends them to the original feature vectors. ISF is our method using instance similarity features. We first transform each training instance to a K-dimensional vector according to Equation 1 and then append the vector to the original vector. For all the three NLP tasks </context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Sham Kakade</author>
<author>Dean P Foster</author>
</authors>
<title>Domain adaptation with coupled subspaces.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>173--181</pages>
<contexts>
<context position="7401" citStr="Blitzer et al. (2011)" startWordPosition="1217" endWordPosition="1220">e randomly chosen from the available target instances and no special trick is needed. Overall, the method is fairly easy to implement, and yet as we will see in Section 3, it performs surprisingly well. We also want to point out that our instance similarity features bear strong similarity to what was proposed by Sun and Lam (2013), but their work addresses a completely different problem and we developed our method independently of their work. 2.2 Justification In this section, we provide some intuitive justification for our method without any theoretical proof. Learning in the Target Subspace Blitzer et al. (2011) pointed out that the hope of unsupervised domain adaptation is to “couple” the learning of weights for target-specific features with that of common features. We show our induced feature representation is exactly doing this. First, we review the claim by Blitzer et al. (2011). We note that although the input vector space X is typically high-dimensional for NLP tasks, the actual space where input vectors lie can have a lower dimension because of the strong feature dependence we observe with NLP tasks. For example, binary features defined from the same feature template such as the previous word </context>
<context position="9461" citStr="Blitzer et al. (2011)" startWordPosition="1576" endWordPosition="1579"> follows: x = xs,t + xs,⊥ + x⊥,t. Similarly, any linear classifier w can be decomposed into ws,t, ws,⊥ and w⊥,t, and w&gt;x = w&gt; s,txs,t + w&gt; s,⊥xs,⊥ + w&gt; ⊥,tx⊥,t. For a naive method that simply learns w from Ds, the learned component w⊥,t will be 0, because the component x⊥,t of any source instance is 0, and therefore the training error would not be reduced by any non-zero w⊥,t. Moreover, any non-zero ws,⊥ learned from Ds would not be useful for the target domain because for all target instances we have xs,⊥ = 0. So for a w learned from Ds, only its component ws,t is useful for domain transfer. Blitzer et al. (2011) argues that with unlabeled target instances, we can hope to “couple” the learning of w⊥,t with that of ws,t. We show that if we use only our induced feature representation without appending it to the original feature vector, we can achieve this. We first define a matrix ME whose column vectors are the exemplar vectors from £. Then g(x) can be rewritten as M&gt;E x. Let w0 denote a linear classifier learned from the transformed labeled data. w0 makes prediction based on w0&gt;M&gt;E x, which is the same as (MEw0)&gt;x. This shows that the learned classifier w0 for the induced features is equivalent to a l</context>
</contexts>
<marker>Blitzer, Kakade, Foster, 2011</marker>
<rawString>John Blitzer, Sham Kakade, and Dean P. Foster. 2011. Domain adaptation with coupled subspaces. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 173– 181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="1290" citStr="Chan and Ng, 2007" startWordPosition="200" endWordPosition="203">daptation method widely used in NLP. More importantly, our method is very easy to implement and incurs much less computational cost than SCL. 1 Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management </context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Zhixiang Eddie Xu</author>
<author>Kilian Q Weinberger</author>
<author>Fei Sha</author>
</authors>
<title>Marginalized denoising autoencoders for domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1733" citStr="Chen et al., 2012" startWordPosition="275" endWordPosition="278">often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the setting when a small amount of labeled target data is available, and when no such data is available during training we call it unsupervised domain adaptation. Although many domain adaptation methods have been proposed, for practitioners who wish to avoid implementing or tuning sophisticated or computati</context>
<context position="11342" citStr="Chen et al., 2012" startWordPosition="1909" endWordPosition="1912">ures. In practice, however, we need to append the induced features to the original features to achieve good adaptation results. One may find this counter-intuitive because this results in an expanded instead of restricted hypothesis space. Our explanation is that because of the typical L2 regularizer used during training, there is an incentive to shift the weight mass to the additional induced features. The need to combine the induced features with original features was also reported in previous domain adaptation work such as SCL (Blitzer et al., 2006) and marginalized denoising autoencoders (Chen et al., 2012). Reduction of Domain Divergence Another theory on domain adaptation developed by Ben-David et al. (2010) essentially states that we should use a hypothesis space that can achieve low error on the source domain while at the same time making it hard to separate source and target instances. If we use only our induced features, then Xs,⊥ is excluded from the hypothesis space. This is likely to make it harder to distinguish source and target instances. To verify this, in Table 1 we show the following errors based on three feature representations: (1) The training error on the source domain (ˆεs). </context>
</contexts>
<marker>Chen, Xu, Weinberger, Sha, 2012</marker>
<rawString>Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Weinberger, and Fei Sha. 2012. Marginalized denoising autoencoders for domain adaptation. In Proceedings of the 29th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>407--412</pages>
<marker>Daume, Jagarlamudi, 2011</marker>
<rawString>Hal Daume III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 407–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e Abhishek Kumar</author>
<author>Avishek Saha</author>
</authors>
<title>Frustratingly easy semi-supervised domain adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,</booktitle>
<pages>53--59</pages>
<marker>Kumar, Saha, 2010</marker>
<rawString>Hal Daum´e III, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 53–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
</authors>
<title>Online methods for multi-domain learning and adaptation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>689--697</pages>
<contexts>
<context position="1576" citStr="Dredze and Crammer, 2008" startWordPosition="248" endWordPosition="251">a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the setting when a small amount of labeled target data is available, and when no such data is available during training we call it unsupervised domain </context>
</contexts>
<marker>Dredze, Crammer, 2008</marker>
<rawString>Mark Dredze and Koby Crammer. 2008. Online methods for multi-domain learning and adaptation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 689– 697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach. In</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-eight International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1714" citStr="Glorot et al., 2011" startWordPosition="271" endWordPosition="274">rce domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the setting when a small amount of labeled target data is available, and when no such data is available during training we call it unsupervised domain adaptation. Although many domain adaptation methods have been proposed, for practitioners who wish to avoid implementing or tuning sophist</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In In Proceedings of the Twenty-eight International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Marc Colosimo</author>
<author>Alexander Morgan</author>
<author>Alexander Yeh</author>
</authors>
<title>Overview of BioCreAtIvE task 1B: normailized gene lists.</title>
<date>2005</date>
<journal>BMC Bioinformatics,</journal>
<volume>6</volume>
<pages>1--11</pages>
<contexts>
<context position="14497" citStr="Hirschman et al., 2005" startWordPosition="2455" endWordPosition="2458">eatures are used, our method leverages the unlabeled target instances to force the learned classifier to lie in Xt. 3 Experiments 3.1 Tasks and Data Sets We consider the following NLP tasks. Personalized Spam Filtering (Spam): The data set comes from ECML/PKDD 2006 discovery challenge. The goal is to adapt a spam filter trained on a common pool of 4000 labeled emails to three individual users’ personal inboxes, each containing 2500 emails. We use bag-of-word features for this task, and we report classification accuracy. Gene Name Recognition (NER): The data set comes from BioCreAtIvE Task 1B (Hirschman et al., 2005). It contains three sets of Medline abstracts with labeled gene names. Each set corresponds to a single species (fly, mouse or yeast). We consider domain adaptation from one species to another. We use standard NER features including words, POS tags, prefixes/suffixes and contextual features. We report F1 scores for this task. Relation Extraction (Relation): We use the ACE2005 data where the annotated documents are from several different sources such as broadcast news and conversational telephone speech. We report the F1 scores of identifying the 7 major relation types. We use standard features</context>
</contexts>
<marker>Hirschman, Colosimo, Morgan, Yeh, 2005</marker>
<rawString>Lynette Hirschman, Marc Colosimo, Alexander Morgan, and Alexander Yeh. 2005. Overview of BioCreAtIvE task 1B: normailized gene lists. BMC Bioinformatics, 6(Suppl 1):S11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Ke Zhai</author>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
</authors>
<title>Polylingual tree-based topic models for translation domain adaptation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1166--1176</pages>
<contexts>
<context position="1386" citStr="Hu et al., 2014" startWordPosition="218" endWordPosition="221">curs much less computational cost than SCL. 1 Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use su</context>
</contexts>
<marker>Hu, Zhai, Eidelman, Boyd-Graber, 2014</marker>
<rawString>Yuening Hu, Ke Zhai, Vladimir Eidelman, and Jordan Boyd-Graber. 2014. Polylingual tree-based topic models for translation domain adaptation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1166– 1176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>264--271</pages>
<contexts>
<context position="1550" citStr="Jiang and Zhai, 2007" startWordPosition="244" endWordPosition="247">domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the setting when a small amount of labeled target data is available, and when no such data is available during training we ca</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhishek Kumar</author>
<author>Avishek Saha</author>
<author>Hal Daume</author>
</authors>
<title>Co-regularization based semi-supervised domain adaptation.</title>
<date>2010</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>478--486</pages>
<marker>Kumar, Saha, Daume, 2010</marker>
<rawString>Abhishek Kumar, Avishek Saha, and Hal Daume. 2010. Co-regularization based semi-supervised domain adaptation. In Advances in neural information processing systems, pages 478–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thien Huu Nguyen</author>
<author>Ralph Grishman</author>
</authors>
<title>Employing word representations and regularization for domain adaptation of relation extraction.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>68--74</pages>
<contexts>
<context position="1435" citStr="Nguyen and Grishman, 2014" startWordPosition="226" endWordPosition="229"> SCL. 1 Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the settin</context>
</contexts>
<marker>Nguyen, Grishman, 2014</marker>
<rawString>Thien Huu Nguyen and Ralph Grishman. 2014. Employing word representations and regularization for domain adaptation of relation extraction. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 68–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh Luan Nguyen</author>
<author>Ivor W Tsang</author>
<author>Kian Ming A Chai</author>
<author>Hai Leong Chieu</author>
</authors>
<title>Robust domain adaptation for relation extraction via clustering consistency.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>807--817</pages>
<contexts>
<context position="1407" citStr="Nguyen et al., 2014" startWordPosition="222" endWordPosition="225">mputational cost than SCL. 1 Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adapt</context>
</contexts>
<marker>Nguyen, Tsang, Chai, Chieu, 2014</marker>
<rawString>Minh Luan Nguyen, Ivor W. Tsang, Kian Ming A. Chai, and Hai Leong Chieu. 2014. Robust domain adaptation for relation extraction via clustering consistency. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 807–817.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding semantic similarity in tree kernels for domain adaptation of relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1498--1507</pages>
<contexts>
<context position="1369" citStr="Plank and Moschitti, 2013" startWordPosition="214" endWordPosition="217">ry easy to implement and incurs much less computational cost than SCL. 1 Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adap</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498–1507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chensheng Sun</author>
<author>Kin-Man Lam</author>
</authors>
<title>Multiplekernel, multiple-instance similarity features for efficient visual object detection.</title>
<date>2013</date>
<journal>IEEE Transactions on Image Processing,</journal>
<volume>22</volume>
<issue>8</issue>
<contexts>
<context position="7112" citStr="Sun and Lam (2013)" startWordPosition="1172" endWordPosition="1175">e source instance and use the combined feature vectors of all labeled source instances to train a classifier. To apply this classifier to the target domain, each target instance also needs to add this K-dimensional induced feature vector. It is worth noting that the exemplar vectors are randomly chosen from the available target instances and no special trick is needed. Overall, the method is fairly easy to implement, and yet as we will see in Section 3, it performs surprisingly well. We also want to point out that our instance similarity features bear strong similarity to what was proposed by Sun and Lam (2013), but their work addresses a completely different problem and we developed our method independently of their work. 2.2 Justification In this section, we provide some intuitive justification for our method without any theoretical proof. Learning in the Target Subspace Blitzer et al. (2011) pointed out that the hope of unsupervised domain adaptation is to “couple” the learning of weights for target-specific features with that of common features. We show our induced feature representation is exactly doing this. First, we review the claim by Blitzer et al. (2011). We note that although the input v</context>
</contexts>
<marker>Sun, Lam, 2013</marker>
<rawString>Chensheng Sun and Kin-Man Lam. 2013. Multiplekernel, multiple-instance similarity features for efficient visual object detection. IEEE Transactions on Image Processing, 22(8):3050–3061.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
</authors>
<title>Domain adaptation by constraining inter-domain variability of latent feature representation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>62--71</pages>
<contexts>
<context position="1590" citStr="Titov, 2011" startWordPosition="252" endWordPosition="253">data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the setting when a small amount of labeled target data is available, and when no such data is available during training we call it unsupervised domain adaptation. Al</context>
</contexts>
<marker>Titov, 2011</marker>
<rawString>Ivan Titov. 2011. Domain adaptation by constraining inter-domain variability of latent feature representation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 62–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Fast easy unsupervised domain adaptation with marginalized structured dropout.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>538--544</pages>
<contexts>
<context position="1761" citStr="Yang and Eisenstein, 2014" startWordPosition="279" endWordPosition="282">NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the setting when a small amount of labeled target data is available, and when no such data is available during training we call it unsupervised domain adaptation. Although many domain adaptation methods have been proposed, for practitioners who wish to avoid implementing or tuning sophisticated or computationally expensive methods due</context>
<context position="3795" citStr="Yang and Eisenstein, 2014" startWordPosition="597" endWordPosition="600">h is then combined with the original feature space. We explain analytically why the new feature space may help domain adaptation. Using a few different NLP tasks, we then empirically show that our method can indeed learn a better classifier for the target domain than a few baselines. In particular, our method performs consistently better than or competitively with Structural Correspondence Learning (SCL) (Blitzer et al., 2006), a wellknown unsupervised domain adaptation method in NLP. Furthermore, compared with SCL and other advanced methods such as the marginalized structured dropout method (Yang and Eisenstein, 2014) and a recent feature embedding method (Yang and 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 168–173, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Eisenstein, 2015), our method is much easier to implement. In summary, our main contribution is a simple, effective and theoretically justifiable unsupervised domain adaptation method for NLP problems. 2 Adaptation with Similarity Features We first introduce the necessar</context>
</contexts>
<marker>Yang, Eisenstein, 2014</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2014. Fast easy unsupervised domain adaptation with marginalized structured dropout. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 538–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Unsupervised multi-domain adaptation with feature embeddings.</title>
<date>2015</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>672--682</pages>
<contexts>
<context position="1826" citStr="Yang and Eisenstein, 2015" startWordPosition="290" endWordPosition="293">sting work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the setting when a small amount of labeled target data is available, and when no such data is available during training we call it unsupervised domain adaptation. Although many domain adaptation methods have been proposed, for practitioners who wish to avoid implementing or tuning sophisticated or computationally expensive methods due to either lack of enough machine learning background or limited </context>
</contexts>
<marker>Yang, Eisenstein, 2015</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2015. Unsupervised multi-domain adaptation with feature embeddings. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 672–682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pei Yang</author>
<author>Wei Gao</author>
<author>Qi Tan</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Information-theoretic multi-view domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="1342" citStr="Yang et al., 2012" startWordPosition="210" endWordPosition="213">y, our method is very easy to implement and incurs much less computational cost than SCL. 1 Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg There are generally </context>
</contexts>
<marker>Yang, Gao, Tan, Wong, 2012</marker>
<rawString>Pei Yang, Wei Gao, Qi Tan, and Kam-Fai Wong. 2012. Information-theoretic multi-view domain adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270–274.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>