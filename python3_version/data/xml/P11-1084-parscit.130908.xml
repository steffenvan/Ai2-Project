<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.964463">
Binarized Forest to String Translation
</title>
<author confidence="0.963758">
Hao Zhang
</author>
<affiliation confidence="0.868002">
Google Research
</affiliation>
<email confidence="0.956471">
haozhang@google.com
</email>
<author confidence="0.980233">
Peng Xu
</author>
<affiliation confidence="0.869603">
Google Research
</affiliation>
<email confidence="0.995591">
xp@google.com
</email>
<sectionHeader confidence="0.998576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994599038461538">
Tree-to-string translation is syntax-aware and
efficient but sensitive to parsing errors. Forest-
to-string translation approaches mitigate the
risk of propagating parser errors into transla-
tion errors by considering a forest of alterna-
tive trees, as generated by a source language
parser. We propose an alternative approach to
generating forests that is based on combining
sub-trees within the first best parse through
binarization. Provably, our binarization for-
est can cover any non-consitituent phrases in
a sentence but maintains the desirable prop-
erty that for each span there is at most one
nonterminal so that the grammar constant for
decoding is relatively small. For the purpose
of reducing search errors, we apply the syn-
chronous binarization technique to forest-to-
string decoding. Combining the two tech-
niques, we show that using a fast shift-reduce
parser we can achieve significant quality gains
in NIST 2008 English-to-Chinese track (1.3
BLEU points over a phrase-based system, 0.8
BLEU points over a hierarchical phrase-based
system). Consistent and significant gains are
also shown in WMT 2010 in the English to
German, French, Spanish and Czech tracks.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999295857142857">
In recent years, researchers have explored a wide
spectrum of approaches to incorporate syntax and
structure into machine translation models. The uni-
fying framework for these models is synchronous
grammars (Chiang, 2005) or tree transducers
(Graehl and Knight, 2004). Depending on whether
or not monolingual parsing is carried out on the
</bodyText>
<author confidence="0.723428">
Licheng Fang
</author>
<affiliation confidence="0.986825">
Computer Science Department
University of Rochester
</affiliation>
<email confidence="0.958577">
lfang@cs.rochester.edu
</email>
<author confidence="0.651266">
Xiaoyun Wu
</author>
<affiliation confidence="0.359074">
Google Research
</affiliation>
<email confidence="0.88501">
xiaoyunwu@google.com
</email>
<bodyText confidence="0.988866">
source side or the target side for inference, there are
four general categories within the framework:
</bodyText>
<listItem confidence="0.9998325">
• string-to-string (Chiang, 2005; Zollmann and
Venugopal, 2006)
• string-to-tree (Galley et al., 2006; Shen et al.,
2008)
• tree-to-string (Lin, 2004; Quirk et al., 2005;
Liu et al., 2006; Huang et al., 2006; Mi et al.,
2008)
• tree-to-tree (Eisner, 2003; Zhang et al., 2008)
</listItem>
<bodyText confidence="0.999960909090909">
In terms of search, the string-to-x models explore all
possible source parses and map them to the target
side, while the tree-to-x models search over the sub-
space of structures of the source side constrained
by an input tree or trees. Hence, tree-to-x mod-
els are more constrained but more efficient. Mod-
els such as Huang et al. (2006) can match multi-
level tree fragments on the source side which means
larger contexts are taken into account for transla-
tion (Poutsma, 2000), which is a modeling advan-
tage. To balance efficiency and accuracy, forest-to-
string models (Mi et al., 2008; Mi and Huang, 2008)
use a compact representation of exponentially many
trees to improve tree-to-string models. Tradition-
ally, such forests are obtained through hyper-edge
pruning in the k-best search space of a monolin-
gual parser (Huang, 2008). The pruning parameters
that control the size of forests are normally hand-
tuned. Such forests encode both syntactic variants
and structural variants. By syntactic variants, we re-
fer to the fact that a parser can parse a substring into
either a noun phrase or verb phrase in certain cases.
</bodyText>
<page confidence="0.981164">
835
</page>
<note confidence="0.9795135">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 835–845,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999204">
We believe that structural variants which allow more
source spans to be explored during translation are
more important (DeNeefe et al., 2007), while syn-
tactic variants might improve word sense disam-
biguation but also introduce more spurious ambi-
guities (Chiang, 2005) during decoding. To focus
on structural variants, we propose a family of bina-
rization algorithms to expand one single constituent
tree into a packed forest of binary trees containing
combinations of adjacent tree nodes. We control the
freedom of tree node binary combination by restrict-
ing the distance to the lowest common ancestor of
two tree nodes. We show that the best results are
achieved when the distance is two, i.e., when com-
bining tree nodes sharing a common grand-parent.
In contrast to conventional parser-produced-forest-
to-string models, in our model:
coding algorithm with intergrated LM intersection
using the cube pruning technique (Chiang, 2005).
The rest of the paper is organized as follows. In
Section 2, we give an overview of the forest-to-
string models. In Section 2.1, we introduce a more
efficient and flexible algorithm for extracting com-
posed GHKM rules based on the same principle as
cube pruning (Chiang, 2007). In Section 3, we in-
troduce our source tree binarization algorithm for
producing binarized forests. In Section 4, we ex-
plain how to do synchronous rule factorization in a
forest-to-string decoder. Experimental results are in
Section 5.
</bodyText>
<sectionHeader confidence="0.899955" genericHeader="method">
2 Forest-to-string Translation
</sectionHeader>
<bodyText confidence="0.827832">
Forest-to-string models can be described as
</bodyText>
<listItem confidence="0.993245142857143">
• Forests are not generated by a parser but by e = Y( arg max P(d|T) ) (1)
combining sub-structures using a tree binarizer. d∈D(T), T∈F(f)
• Instead of using arbitary pruning parameters,
we control forest size by an integer number that
defines the degree of tree structure violation.
• There is at most one nonterminal per span so
that the grammar constant is small.
</listItem>
<bodyText confidence="0.999806346938776">
Since GHKM rules (Galley et al., 2004) can cover
multi-level tree fragments, a synchronous grammar
extracted using the GHKM algorithm can have syn-
chronous translation rules with more than two non-
terminals regardless of the branching factor of the
source trees. For the first time, we show that simi-
lar to string-to-tree decoding, synchronous binariza-
tion significantly reduces search errors and improves
translation quality for forest-to-string decoding.
To summarize, the whole pipeline is as follows.
First, a parser produces the highest-scored tree for
an input sentence. Second, the parse tree is re-
structured using our binarization algorithm, result-
ing in a binary packed forest. Third, we apply the
forest-based variant of the GHKM algorithm (Mi
and Huang, 2008) on the new forest for rule extrac-
tion. Fourth, on the translation forest generated by
all applicable translation rules, which is not neces-
sarily binary, we apply the synchronous binarization
algorithm (Zhang et al., 2006) to generate a binary
translation forest. Finally, we use a bottom-up de-
where f stands for a source string, e stands for a tar-
get string, F stands for a forest, D stands for a set
of synchronous derivations on a given tree T, and
Y stands for the target side yield of a derivation.
The search problem is finding the derivation with
the highest probability in the space of all deriva-
tions for all parse trees for an input sentence. The
log probability of a derivation is normally a lin-
ear combination of local features which enables dy-
namic programming to find the optimal combination
efficiently. In this paper, we focus on the models
based on the Synchronous Tree Substitution Gram-
mars (STSG) defined by Galley et al. (2004). In con-
trast to a tree-to-string model, the introduction of F
augments the search space systematically. When the
first-best parse is wrong or no good translation rules
are applicable to the first-best parse, the model can
recover good translations from alternative parses.
In STSG, local features are defined on tree-to-
string rules, which are synchronous grammar rules
defining how a sequence of terminals and nontermi-
nals on the source side translates to a sequence of
target terminals and nonterminals. One-to-one map-
ping of nonterminals is assumed. But terminals do
not necessarily need to be aligned. Figure 1 shows a
typical English-Chinese tree-to-string rule with a re-
ordering pattern consisting of two nonterminals and
different numbers of terminals on the two sides.
</bodyText>
<page confidence="0.996648">
836
</page>
<figure confidence="0.810118">
by
</figure>
<figureCaption confidence="0.999694">
Figure 1: An example tree-to-string rule.
</figureCaption>
<bodyText confidence="0.999871444444444">
Forest-to-string translation has two stages. The
first stage is rule extraction on word-aligned parallel
texts with source forests. The second stage is rule
enumeration and DP decoding on forests of input
strings. In both stages, at each tree node, the task on
the source side is to generate a list of tree fragments
by composing the tree fragments of its children. We
propose a cube-pruning style algorithm that is suit-
able for both rule extraction during training and rule
enumeration during decoding.
At the highest level, our algorithm involves three
steps. In the first step, we label each node in the in-
put forest by a boolean variable indicating whether it
is a site of interest for tree fragment generation. If it
is marked true, it is an admissible node. In the case
of rule extraction, a node is admissible if and only if
it corresponds to a phrase pair according to the un-
derlying word alignment. In the case of decoding,
every node is admissible for the sake of complete-
ness of search. An initial one-node tree fragment is
placed at each admissible node for seeding the tree
fragment generation process. In the second step,
we do cube-pruning style bottom-up combinations
to enumerate a pruned list of tree fragments at each
tree node. In the third step, we extract or enumerate-
and-match tree-to-string rules for the tree fragments
at the admissible nodes.
</bodyText>
<subsectionHeader confidence="0.995228">
2.1 A Cube-pruning-inspired Algorithm for
Tree Fragment Composition
</subsectionHeader>
<bodyText confidence="0.999974692307692">
Galley et al. (2004) defined minimal tree-to-string
rules. Galley et al. (2006) showed that tree-to-string
rules made by composing smaller ones are impor-
tant to translation. It can be understood by the anal-
ogy of going from word-based models to phrase-
based models. We relate composed rule extraction
to cube-pruning (Chiang, 2007). In cube-pruning,
the process is to keep track of the k-best sorted lan-
guage model states at each node and combine them
bottom-up with the help of a priority queue. We
can imagine substituting k-best LM states with k
composed rules at each node and composing them
bottom-up. We can also borrow the cube pruning
trick to compose multiple lists of rules using a pri-
ority queue to lazily explore the space of combina-
tions starting from the top-most element in the cube
formed by the lists.
We need to define a ranking function for com-
posed rules. To simulate the breadth-first expansion
heuristics of Galley et al. (2006), we define the fig-
ure of merit of a tree-to-string rule as a tuple m =
(h, s, t), where h is the height of a tree fragment,
s is the number of frontier nodes, i.e., bottom-level
nodes including both terminals and non-terminals,
and t is the number of terminals in the set of frontier
nodes. We define an additive operator +:
</bodyText>
<equation confidence="0.947334833333333">
m1 + m2
= ( max{h1, h2} + 1, s1 + s2, t1 + t2 )
and a min operator based on the order &lt;:
m1 &lt; m2 �� { h1 &lt; h2 V
h1 = h2 ∧ s1 &lt; s2 V
h1 = h2 ∧ s1 = s2 ∧ t1 &lt; t2
</equation>
<bodyText confidence="0.999761888888889">
The + operator corresponds to rule compositions.
The &lt; operator corresponds to ranking rules by their
sizes. A concrete example is shown in Figure 2,
in which case the monotonicity property of (+, &lt;)
holds: if ma &lt; mb, ma +mc &lt; mb +mc. However,
this is not true in general for the operators in our def-
inition, which implies that our algorithm is indeed
like cube-pruning: an approximate k-shortest-path
algorithm.
</bodyText>
<sectionHeader confidence="0.945169" genericHeader="method">
3 Source Tree Binarization
</sectionHeader>
<bodyText confidence="0.998178">
The motivation of tree binarization is to factorize
large and rare structures into smaller but frequent
ones to improve generalization. For example, Penn
Treebank annotations are often flat at the phrase
level. Translation rules involving flat phrases are un-
likely to generalize. If long sequences are binarized,
</bodyText>
<figure confidence="0.968658847457627">
VP
VBD VP-C
was .x1:VBN PP
P
.x2:NP-C
�
bei
x2 x1
被
837
(1,1, 0) (2, 2, 0) (3,3,1)
VP
(2, 2, 0)
VBD VP-C
VP
(3, 2,1)
VBD
VP-C
VP
(3,3,1)
VBD
VP-C
was
VPB PP
VBD (1,1,0) �
VBD (2,1,1) ������������������������������� �
i was i x
��������������������������������
�
��������������������������������
������������������������������� �
VP-C (1,1,0)
(2,2,0)
VP-C
VP-C
(3,3,1)
P NP-C
VPB PP
VPB PP
was
VP
VBD VP-C
VPB PP
P NP-C
VP
VBD
was
(1,1,0)
VP-C
VPB PP
P NP-C
=
(2,1,1)
(4, 4,1)
(4,4,2)
VP
VBD VP-C
VPB PP
(3,3,0)
</figure>
<figureCaption confidence="0.754271">
Figure 2: Tree-to-string rule composition as cube-pruning. The left shows two lists of composed rules sorted by their
geometric measures (height, # frontiers, # frontier terminals), under the gluing rule of VP → VBD VP−C.
The right part shows a cube view of the combination space. We explore the space from the top-left corner to the
neighbors.
</figureCaption>
<bodyText confidence="0.999972903225807">
the commonality of subsequences can be discov-
ered. For example, the simplest binarization meth-
ods left-to-right, right-to-left, and head-out explore
sharing of prefixes or suffixes. Among exponentially
many binarization choices, these algorithms pick a
single bracketing structure for a sequence of sibling
nodes. To explore all possible binarizations, we use
a CYK algorithm to produce a packed forest of bi-
nary trees for a given sibling sequence.
With CYK binarization, we can explore any span
that is nested within the original tree structure, but
still miss all cross-bracket spans. For example,
translating from English to Chinese, The phrase
“There is” should often be translated into one verb
in Chinese. In a correct English parse tree, however,
the subject-verb boundary is between “There” and
“is”. As a result, tree-to-string translation based on
constituent phrases misses the good translation rule.
The CYK-n binarization algorithm shown in Al-
gorithm 1 is a parameterization of the basic CYK
binarization algorithm we just outlined. The idea is
that binarization can go beyond the scope of parent
nodes to more distant ancestors. The CYK-n algo-
rithm first annotates each node with its n nearest
ancestors in the source tree, then generates a bina-
rization forest that allows combining any two nodes
with common ancestors. The ancestor chain labeled
at each node licenses the node to only combine with
nodes having common ancestors in the past n gener-
ations.
The algorithm creates new tree nodes on the fly.
New tree nodes need to have their own states in-
dicated by a node label representing what is cov-
ered internally by the node and an ancestor chain
representing which nodes the node attaches to ex-
ternally. Line 22 and Line 23 of Algorithm 1 up-
date the label and ancestor annotations of new tree
nodes. Using the parsing semiring notations (Good-
man, 1999), the ancestor computation can be sum-
marized by the (n, U) pair. n produces the ances-
tor chain of a hyper-edge. U produces the ancestor
chain of a hyper-node. The node label computation
can be summarized by the (concatenate, min) pair.
concatenate produces a concatenation of node la-
bels. min yields the label with the shortest length.
A tree-sequence (Liu et al., 2007) is a sequence of
sub-trees covering adjacent spans. It can be proved
that the final label of each new node in the forest
corresponds to the tree sequence which has the min-
imum length among all sequences covered by the
node span. The ancestor chain of a new node is the
common ancestors of the nodes in its minimum tree
sequence.
For clarity, we do full CYK loops over all O(|w|2)
spans and O(|w|3) potential hyper-edges, where |w|
is the length of a source string. In reality, only de-
scendants under a shared ancestor can combine. If
we assume trees have a bounded branching factor
b, the number of descendants after n generations is
still bounded by a constant c = b�. The algorithm is
O(c3 · |w|), which is still linear to the size of input
sentence when the parameter n is a constant.
</bodyText>
<page confidence="0.97592">
838
</page>
<figure confidence="0.994239709677419">
RB+JJ
x0:RB JJ
responsible
PP
IN
NP-C
(a) (b)
ADJP
VP VP
VBD+VBN PP VBD VP-C
P
VBN+P
VBD
NP-C
was
NP-C
VBN
was by VBN P
by
x2:PP
x1:NN
DT
for
NPB
fuze de
y x0 负责 x2 的 x1
the
VBD
VBN
by
was
VBN P
was
by
de
的 x0
� x1
ADJP
RB+JJ
x0:RB JJ
responsible
PP
x1:PP
fuze
负责 x1
� x0
(c) (d)
VBD+VBN+P
NP-C
VP
VBD+VBN+P
VBD+VBN
P
NP-C
VP
VBD
VBN+P
1 2 3 4
0 VBD VBD+VBN VBD+VBN+P VP
1 VBN VBN+P VP-C
2 P PP
3 NP-C
</figure>
<figureCaption confidence="0.926421333333333">
Figure 3: Alternative binary parses created for the origi-
nal tree fragment in Figure 1 through CYK-2 binarization
(a and b) and CYK-3 binarization (c and d). In the chart
representation at the bottom, cells with labels containing
the concatenation symbol + hold nodes created through
binarization.
Figure 3 shows some examples of alternative trees
generated by the CYK-n algorithm. In this example,
standard CYK binarization will not create any new
trees since the input is already binary. The CYK-2
and CYK-3 algorithms discover new trees with an
increasing degree of freedom.
</figureCaption>
<sectionHeader confidence="0.907093" genericHeader="method">
4 Synchronous Binarization for
Forest-to-string Decoding
</sectionHeader>
<bodyText confidence="0.9917447">
In this section, we deal with binarization of transla-
tion forests, also known as translation hypergraphs
(Mi et al., 2008). A translation forest is a packed
forest representation of all synchronous derivations
composed of tree-to-string rules that match the
source forest. Tree-to-string decoding algorithms
work on a translation forest, rather than a source for-
est. A binary source forest does not necessarily al-
ways result in a binary translation forest. In the tree-
to-string rule in Figure 4, the source tree is already
</bodyText>
<figure confidence="0.614932">
the
</figure>
<figureCaption confidence="0.9908105">
Figure 4: Synchronous binarization for a tree-to-string
rule. The top rule can be binarized into two smaller rules.
</figureCaption>
<bodyText confidence="0.99922295">
binary with the help of source tree binarization, but
the translation rule involves three variables in the set
of frontier nodes. If we apply synchronous binariza-
tion (Zhang et al., 2006), we can factorize it into
two smaller translation rules each having two vari-
ables. Obviously, the second rule, which is a com-
mon pattern, is likely to be shared by many transla-
tion rules in the derivation forest. When beams are
fixed, search goes deeper in a factorized translation
forest.
The challenge of synchronous binarization for a
forest-to-string system is that we need to first match
large tree fragments in the input forest as the first
step of decoding. Our solution is to do the matching
using the original rules and then run synchronous
binarization to break matching rules down to factor
rules which can be shared in the derivation forest.
This is different from the offline binarization scheme
described in (Zhang et al., 2006), although the core
algorithm stays the same.
</bodyText>
<sectionHeader confidence="0.999754" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9793615">
We ran experiments on public data sets for English
to Chinese, Czech, French, German, and Spanish
</bodyText>
<figure confidence="0.962105666666667">
IN NP-C
x1:PP
x0:NN
DT
for
NPB
</figure>
<page confidence="0.787092">
839
</page>
<construct confidence="0.409603">
Algorithm 1 The CYK-n Binarization Algorithm
</construct>
<listItem confidence="0.949869333333333">
1: function CYKBINARIZER(T,n)
2: for each tree node ∈ T in bottom-up topological order do
3: Make a copy of node in the forest output F
4: Ancestors[node] = the nearest n ancestors of node
5: Label[node] = the label of node in T
6: L ← the length of the yield of T
7: for k = 2...L do
8: for i = 0,..., L − k do
9: for j = i + 1, ..., i + k − 1 do
</listItem>
<figure confidence="0.9242954">
10: lnode ← Node[i, j]; rnode ← Node[j, i + k]
11: if Ancestors[lnode] ∩ Ancestors[rnode] =6 ∅ then
12: pnode ← GETNODE(i, i + k)
13: ADDEDGE(pnode, lnode, rnode)
return F
14: function GETNODE(begin, end)
15: if Node[begin, end] ∈� F then
16: Create a new node for the span (begin, end)
17: Ancestors[node] = ∅
18: Label[node] = the sequence of terminals in the span (begin, end) in T
19: return Node[begin, end]
20: function ADDEDGE(pnode, lnode, rnode)
21: Add a hyper-edge from lnode and rnode to pnode
22: Ancestors[pnode] = Ancestors[pnode] ∪ (Ancestors[lnode] ∩ Ancestors[rnode])
23: Label[pnode] = min{Label[pnode], CONCATENATE(Label[lnode], Label[rnode])}
</figure>
<figureCaption confidence="0.320602">
translation to evaluate our methods.
</figureCaption>
<subsectionHeader confidence="0.96661">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999542111111111">
For English-to-Chinese translation, we used all the
allowed training sets in the NIST 2008 constrained
track. For English to the European languages, we
used the training data sets for WMT 2010 (Callison-
Burch et al., 2010). For NIST, we filtered out sen-
tences exceeding 80 words in the parallel texts. For
WMT, the filtering limit is 60. There is no filtering
on the test data set. Table 1 shows the corpus statis-
tics of our bilingual training data sets.
</bodyText>
<tableCaption confidence="0.992997">
Table 1: The Sizes of Parallel Texts.
</tableCaption>
<bodyText confidence="0.999918793103448">
At the word alignment step, we did 6 iterations
of IBM Model-1 and 6 iterations of HMM. For
English-Chinese, we ran 2 iterations of IBM Model-
4 in addition to Model-1 and HMM. The word align-
ments are symmetrized using the “union” heuris-
tics. Then, the standard phrase extraction heuristics
(Koehn et al., 2003) were applied to extract phrase
pairs with a length limit of 6. We ran the hierar-
chical phrase extraction algorithm with the standard
heuristics of Chiang (2005). The phrase-length limit
is interpreted as the maximum number of symbols
on either the source side or the target side of a given
rule. On the same aligned data sets, we also ran the
tree-to-string rule extraction algorithm described in
Section 2.1 with a limit of 16 rules per tree node.
The default parser in the experiments is a shift-
reduce dependency parser (Nivre and Scholz, 2004).
It achieves 87.8% labelled attachment score and
88.8% unlabeled attachment score on the standard
Penn Treebank test set. We convert dependency
parses to constituent trees by propagating the part-
of-speech tags of the head words to the correspond-
ing phrase structures.
We compare three systems: a phrase-based sys-
tem (Och and Ney, 2004), a hierarchical phrase-
based system (Chiang, 2005), and our forest-to-
string system with different binarization schemes. In
the phrase-based decoder, jump width is set to 8. In
the hierarchical decoder, only the glue rule is applied
</bodyText>
<figure confidence="0.998092764705882">
Source Words
Target Words
English-Chinese
English-Czech
English-French
English-German
English-Spanish
287M
66M
857M
45M
216M
254M
57M
996M
43M
238M
</figure>
<page confidence="0.989981">
840
</page>
<bodyText confidence="0.9997789">
to spans longer than 10. For the forest-to-string sys-
tem, we do not have such length-based reordering
constraints.
We trained two 5-gram language models with
Kneser-Ney smoothing for each of the target lan-
guages. One is trained on the target side of the par-
allel text, the other is on a corpus provided by the
evaluation: the Gigaword corpus for Chinese and
news corpora for the others. Besides standard fea-
tures (Och and Ney, 2004), the phrase-based decoder
also uses a Maximum Entropy phrasal reordering
model (Zens and Ney, 2006). Both the hierarchi-
cal decoder and the forest-to-string decoder only use
the standard features. For feature weight tuning, we
do Minimum Error Rate Training (Och, 2003). To
explore a larger n-best list more efficiently in train-
ing, we adopt the hypergraph-based MERT (Kumar
et al., 2009).
To evaluate the translation results, we use BLEU
(Papineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.99965">
5.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.999977428571429">
Table 2 shows the scores of our system with the
best binarization scheme compared to the phrase-
based system and the hierarchical phrase-based sys-
tem. Our system is consistently better than the other
two systems in all data sets. On the English-Chinese
data set, the improvement over the phrase-based sys-
tem is 1.3 BLEU points, and 0.8 over the hierarchi-
cal phrase-based system. In the tasks of translat-
ing to European languages, the improvements over
the phrase-based baseline are in the range of 0.5 to
1.0 BLEU points, and 0.3 to 0.5 over the hierar-
chical phrase-based system. All improvements ex-
cept the bf2s and hier difference in English-Czech
are significant with confidence level above 99% us-
ing the bootstrap method (Koehn, 2004). To demon-
strate the strength of our systems including the two
baseline systems, we also show the reported best re-
sults on these data sets from the 2010 WMT work-
shop. Our forest-to-string system (bf2s) outperforms
or ties with the best ones in three out of four lan-
guage pairs.
</bodyText>
<subsectionHeader confidence="0.997059">
5.3 Different Binarization Methods
</subsectionHeader>
<bodyText confidence="0.999337">
The translation results for the bf2s system in Ta-
ble 2 are based on the cyk binarization algorithm
with bracket violation degree 2. In this section, we
</bodyText>
<table confidence="0.999535428571429">
BLEU
dev test
English-Chinese pb 29.7 39.4
hier 31.7 38.9
bf2s 31.9 40.7**
English-Czech wmt best - 15.4
pb 14.3 15.5
hier 14.7 16.0
bf2s 14.8 16.3*
English-French wmt best - 27.6
pb 24.1 26.1
hier 23.9 26.1
bf2s 24.5 26.6**
English-German wmt best - 16.3
pb 14.5 15.5
hier 14.9 15.9
bf2s 15.2 16.3**
English-Spanish wmt best - 28.4
pb 24.1 27.9
hier 24.2 28.4
bf2s 24.9 28.9**
</table>
<tableCaption confidence="0.994424">
Table 2: Translation results comparing bf2s, the
</tableCaption>
<bodyText confidence="0.990498333333333">
binarized-forest-to-string system, pb, the phrase-based
system, and hier, the hierarchical phrase-based system.
For comparison, the best scores from WMT 2010 are also
shown. ** indicates the result is significantly better than
both pb and hier. * indicates the result is significantly
better than pb only.
vary the degree to generate forests that are incremen-
tally augmented from a single tree. Table 3 shows
the scores of different tree binarization methods for
the English-Chinese task.
It is clear from reading the table that cyk-2 is the
optimal binarization parameter. We have verified
this is true for other language pairs on non-standard
data sets. We can explain it from two angles. At
degree 2, we allow phrases crossing at most one
bracket in the original tree. If the parser is reason-
ably good, crossing just one bracket is likely to cover
most interesting phrases that can be translation units.
From another point of view, enlarging the forests
entails more parameters in the resulting translation
model, making over-fitting likely to happen.
</bodyText>
<subsectionHeader confidence="0.994976">
5.4 Binarizer or Parser?
</subsectionHeader>
<bodyText confidence="0.999452">
A natural question is how the binarizer-generated
forests compare with parser-generated forests in
translation. To answer this question, we need a
</bodyText>
<page confidence="0.996068">
841
</page>
<table confidence="0.99455275">
BLEU
rules dev test
no binarization 378M 28.0 36.3
head-out 408M 30.0 38.2
cyk-1 527M 31.6 40.5
cyk-2 803M 31.9 40.7
cyk-3 1053M 32.0 40.6
cyk-oo 1441M 32.0 40.3
</table>
<tableCaption confidence="0.992076">
Table 3: Comparing different source tree binarization
schemes for English-Chinese translation, showing both
BLEU scores and model sizes. The rule counts include
normal phrases which are used at the leaf level during
decoding.
</tableCaption>
<bodyText confidence="0.999204235294118">
parser that can generate a packed forest. Our fast
deterministic dependency parser does not generate
a packed forest. Instead, we use a CRF constituent
parser (Finkel et al., 2008) with state-of-the-art ac-
curacy. On the standard Penn Treebank test set, it
achieves an F-score of 89.5%. It uses a CYK algo-
rithm to do full dynamic programming inference, so
is much slower. We modified the parser to do hyper-
edge pruning based on posterior probabilities. The
parser preprocesses the Penn Treebank training data
through binarization. So the packed forest it pro-
duces is also a binarized forest. We compare two
systems: one is using the cyk-2 binarizer to generate
forests; the other is using the CRF parser with prun-
ing threshold e−T&apos;, where p = 2 to generate forests.1
Although the parser outputs binary trees, we found
cross-bracket cyk-2 binarization is still helpful.
</bodyText>
<table confidence="0.89129425">
BLEU
dev test
cyk-2 14.9 16.0
parser 14.7 15.7
</table>
<tableCaption confidence="0.989869">
Table 4: Binarized forests versus parser-generated forests
for forest-to-string English-German translation.
</tableCaption>
<bodyText confidence="0.995926444444444">
Table 4 shows the comparison of binarization for-
est and parser forest on English-German translation.
The results show that cyk-2 forest performs slightly
1All hyper-edges with negative log posterior probability
larger than p are pruned. In Mi and Huang (2008), the thresh-
old is p = 10. The difference is that they do the forest pruning
on a forest generated by a k-best algorithm, while we do the
forest-pruning on the full CYK chart. As a result, we need more
aggressive pruning to control forest size.
better than the parser forest. We have not done full
exploration of forest pruning parameters to fine-tune
the parser-forest. The speed of the constituent parser
is the efficiency bottleneck. This actually demon-
strates the advantage of the binarizer plus forest-to-
string scheme. It is flexible, and works with any
parser that generates projective parses. It does not
require hand-tuning of forest pruning parameters for
training.
</bodyText>
<subsectionHeader confidence="0.970465">
5.5 Synchronous Binarization
</subsectionHeader>
<bodyText confidence="0.999509461538462">
In this section, we demonstrate the effect of syn-
chronous binarization for both tree-to-string and
forest-to-string translation. The experiments are on
the English-Chinese data set. The baseline systems
use k-way cube pruning, where k is the branching
factor, i.e., the maximum number of nonterminals on
the right-hand side of any synchronous translation
rule in an input grammar. The competing system
does online synchronous binarization as described in
Section 4 to transform the grammar intersected with
the input sentence to the minimum branching factor
k′ (k′ &lt; k), and then applies k′-way cube pruning.
Typically, k′ is 2.
</bodyText>
<table confidence="0.526852166666667">
BLEU
dev test
29.2 37.0
30.0 38.2
31.7 40.5
31.9 40.7
</table>
<tableCaption confidence="0.8454445">
Table 5: The effect of synchronous binarization for tree-
to-string and forest-to-string systems, on the English-
Chinese task.
Table 5 shows that synchronous binarization does
help reduce search errors and find better translations
consistently in all settings.
</tableCaption>
<sectionHeader confidence="0.999973" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999899">
The idea of concatenating adjacent syntactic cate-
gories has been explored in various syntax-based
models. Zollmann and Venugopal (2006) aug-
mented hierarchial phrase based systems with joint
syntactic categories. Liu et al. (2007) proposed tree-
sequence-to-string translation rules but did not pro-
vide a good solution to place joint subtrees into con-
nection with the rest of the tree structure. Zhang et
</bodyText>
<table confidence="0.939628">
head-out cube pruning
+ synch. binarization
cyk-2 cube pruning
+ synch. binarization
</table>
<page confidence="0.984144">
842
</page>
<bodyText confidence="0.997237977777778">
al. (2009) is the closest to our work. But their goal References
was to augment a k-best forest. They did not bina- Chris Callison-Burch, Philipp Koehn, Christof Monz,
rize the tree sequences. They also did not put con- Kay Peterson, Mark Przybocki, and Omar Zaidan.
straint on the tree-sequence nodes according to how 2010. Findings of the 2010 joint workshop on statisti-
many brackets are crossed. cal machine translation and metrics for machine trans-
Wang et al. (2007) used target tree binarization to lation. In Proceedings of the Joint Fifth Workshop on
improve rule extraction for their string-to-tree sys- Statistical Machine Translation and Metrics(MATR),
tem. Their binarization forest is equivalent to our pages 17–53, Uppsala, Sweden, July. Association for
cyk-1 forest. In contrast to theirs, our binarization Computational Linguistics. Revised August 2010.
scheme affects decoding directly because we match David Chiang. 2005. A hierarchical phrase-based model
tree-to-string rules on a binarized forest. for statistical machine translation. In Proceedings of
Different methods of translation rule binarization the 43rd Annual Conference of the Association for
have been discussed in Huang (2007). Their argu- Computational Linguistics (ACL-05), pages 263–270,
ment is that for tree-to-string decoding target side Ann Arbor, MI.
binarization is simpler than synchronous binariza- David Chiang. 2007. Hierarchical phrase-based transla-
tion and works well because creating discontinous tion. Computational Linguistics, 33(2):201–228.
source spans does not explode the state space. The Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
forest-to-string senario is more similar to string-to- Marcu. 2007. What can syntax-based MT learn from
tree decoding in which state-sharing is important. phrase-based MT? In Proceedings of the 2007 Joint
Our experiments show that synchronous binariza- Conference on Empirical Methods in Natural Lan-
tion helps significantly in the forest-to-string case. guage Processing and Computational Natural Lan-
7 Conclusion guage Learning (EMNLP-CoNLL), pages 755–763,
We have presented a new approach to tree-to-string Prague, Czech Republic, June. Association for Com-
translation. It involves a source tree binarization putational Linguistics.
step and a standard forest-to-string translation step. Jason Eisner. 2003. Learning non-isomorphic tree map-
The method renders it unnecessary to have a k-best pings for machine translation. In Proceedings of the
parser to generate a packed forest. We have demon- 41st Meeting of the Association for Computational
strated state-of-the-art results using a fast parser and Linguistics, companion volume, pages 205–208, Sap-
a simple tree binarizer that allows crossing at most poro, Japan.
one bracket in each binarized node. We have also Jenny Rose Finkel, Alex Kleeman, and Christopher D.
shown that reducing search errors is important for Manning. 2008. Efficient, feature-based, conditional
forest-to-string translation. We adapted the syn- random field parsing. In Proceedings of ACL-08:
chronous binarization technqiue to improve search HLT, pages 959–967, Columbus, Ohio, June. Associa-
and have shown significant gains. In addition, we tion for Computational Linguistics.
also presented a new cube-pruning-style algorithm Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
for rule extraction. In the new algorithm, it is easy to Marcu. 2004. What’s in a translation rule? In Pro-
adjust the figure-of-merit of rules for extraction. In ceedings of the 2004 Meeting of the North American
the future, we plan to improve the learning of trans- chapter of the Association for Computational Linguis-
lation rules with binarized forests. tics (NAACL-04), pages 273–280.
Acknowledgments Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
We would like to thank the members of the MT team Marcu, Steve DeNeefe, Wei Wang, and Ignacio
at Google, especially Ashish Venugopal, Zhifei Li, Thayer. 2006. Scalable inference and training of
John DeNero, and Franz Och, for their help and dis- context-rich syntactic translation models. In Proceed-
cussions. We would also like to thank Daniel Gildea ings of the International Conference on Computational
for his suggestions on improving the paper. Linguistics/Association for Computational Linguistics
</bodyText>
<table confidence="0.455589">
843 (COLING/ACL-06), pages 961–968, July.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573–605.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (NAACL-04).
</table>
<reference confidence="0.9970155">
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proceedings
of the NAACL/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation (SSST), pages 33–40,
Rochester, NY.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
46th Annual Conference of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08:HLT), Columbus, OH. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 388–395, Barcelona, Spain, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163–171, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), pages 625–630, Geneva, Switzerland.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the International Conference
on Computational Linguistics/Association for Compu-
tational Linguistics (COLING/ACL-06), Sydney, Aus-
tralia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Conference of the Associ-
ation for Computational Linguistics (ACL-07), Prague.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206–214, Honolulu, Hawaii, Octo-
ber. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th An-
nual Conference of the Association for Computational
Linguistics: Human Language Technologies (ACL-
08:HLT), pages 192–199.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
Coling 2004, pages 64–70, Geneva, Switzerland, Aug
23–Aug 27. COLING.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02).
Arjen Poutsma. 2000. Data-oriented translation. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING-00).
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Con-
ference of the Association for Computational Linguis-
tics (ACL-05), pages 271–279, Ann Arbor, Michigan.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Conference of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), Columbus, OH.
ACL.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 746–
754, Prague, Czech Republic, June. Association for
Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55–63, New York City, June.
Association for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
</reference>
<page confidence="0.986828">
844
</page>
<reference confidence="0.9990835">
North American chapter of the Association for Compu-
tational Linguistics (NAACL-06), pages 256–263, New
York, NY.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings ofACL-08: HLT, pages 559–567, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 172–180,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138–141, New York City, June. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.998893">
845
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423569">
<title confidence="0.99959">Binarized Forest to String Translation</title>
<author confidence="0.99881">Hao Zhang</author>
<affiliation confidence="0.98967">Google Research</affiliation>
<email confidence="0.997336">haozhang@google.com</email>
<author confidence="0.999312">Peng Xu</author>
<affiliation confidence="0.98857">Google Research</affiliation>
<email confidence="0.998742">xp@google.com</email>
<abstract confidence="0.978398148148148">Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2099" citStr="Huang et al., 2006" startWordPosition="307" endWordPosition="310">for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the Licheng Fang Computer Science Department University of Rochester lfang@cs.rochester.edu Xiaoyun Wu Google Research xiaoyunwu@google.com source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and </context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Binarization, synchronous binarization, and target-side binarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the NAACL/AMTA Workshop on Syntax and Structure in Statistical Translation (SSST),</booktitle>
<pages>33--40</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="30202" citStr="Huang (2007)" startWordPosition="4986" endWordPosition="4987">ng-to-tree sys- Statistical Machine Translation and Metrics(MATR), tem. Their binarization forest is equivalent to our pages 17–53, Uppsala, Sweden, July. Association for cyk-1 forest. In contrast to theirs, our binarization Computational Linguistics. Revised August 2010. scheme affects decoding directly because we match David Chiang. 2005. A hierarchical phrase-based model tree-to-string rules on a binarized forest. for statistical machine translation. In Proceedings of Different methods of translation rule binarization the 43rd Annual Conference of the Association for have been discussed in Huang (2007). Their argu- Computational Linguistics (ACL-05), pages 263–270, ment is that for tree-to-string decoding target side Ann Arbor, MI. binarization is simpler than synchronous binariza- David Chiang. 2007. Hierarchical phrase-based translation and works well because creating discontinous tion. Computational Linguistics, 33(2):201–228. source spans does not explode the state space. The Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel forest-to-string senario is more similar to string-to- Marcu. 2007. What can syntax-based MT learn from tree decoding in which state-sharing is important. phrase-ba</context>
</contexts>
<marker>Huang, 2007</marker>
<rawString>Liang Huang. 2007. Binarization, synchronous binarization, and target-side binarization. In Proceedings of the NAACL/AMTA Workshop on Syntax and Structure in Statistical Translation (SSST), pages 33–40, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT),</booktitle>
<publisher>ACL.</publisher>
<location>Columbus, OH.</location>
<contexts>
<context position="2769" citStr="Huang, 2008" startWordPosition="425" endWordPosition="426">al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Traditionally, such forests are obtained through hyper-edge pruning in the k-best search space of a monolingual parser (Huang, 2008). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a noun phrase or verb phrase in certain cases. 835 Proceedings of the 49th Annual Meeting of the Association for Computational Lingui</context>
<context position="6130" citStr="Huang, 2008" startWordPosition="953" endWordPosition="954">ranslation rules with more than two nonterminals regardless of the branching factor of the source trees. For the first time, we show that similar to string-to-tree decoding, synchronous binarization significantly reduces search errors and improves translation quality for forest-to-string decoding. To summarize, the whole pipeline is as follows. First, a parser produces the highest-scored tree for an input sentence. Second, the parse tree is restructured using our binarization algorithm, resulting in a binary packed forest. Third, we apply the forest-based variant of the GHKM algorithm (Mi and Huang, 2008) on the new forest for rule extraction. Fourth, on the translation forest generated by all applicable translation rules, which is not necessarily binary, we apply the synchronous binarization algorithm (Zhang et al., 2006) to generate a binary translation forest. Finally, we use a bottom-up dewhere f stands for a source string, e stands for a target string, F stands for a forest, D stands for a set of synchronous derivations on a given tree T, and Y stands for the target side yield of a derivation. The search problem is finding the derivation with the highest probability in the space of all de</context>
<context position="26843" citStr="Huang (2008)" startWordPosition="4470" endWordPosition="4471">he other is using the CRF parser with pruning threshold e−T&apos;, where p = 2 to generate forests.1 Although the parser outputs binary trees, we found cross-bracket cyk-2 binarization is still helpful. BLEU dev test cyk-2 14.9 16.0 parser 14.7 15.7 Table 4: Binarized forests versus parser-generated forests for forest-to-string English-German translation. Table 4 shows the comparison of binarization forest and parser forest on English-German translation. The results show that cyk-2 forest performs slightly 1All hyper-edges with negative log posterior probability larger than p are pruned. In Mi and Huang (2008), the threshold is p = 10. The difference is that they do the forest pruning on a forest generated by a k-best algorithm, while we do the forest-pruning on the full CYK chart. As a result, we need more aggressive pruning to control forest size. better than the parser forest. We have not done full exploration of forest pruning parameters to fine-tune the parser-forest. The speed of the constituent parser is the efficiency bottleneck. This actually demonstrates the advantage of the binarizer plus forest-tostring scheme. It is flexible, and works with any parser that generates projective parses. </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), Columbus, OH. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03),</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="20126" citStr="Koehn et al., 2003" startWordPosition="3375" endWordPosition="3378">r WMT 2010 (CallisonBurch et al., 2010). For NIST, we filtered out sentences exceeding 80 words in the parallel texts. For WMT, the filtering limit is 60. There is no filtering on the test data set. Table 1 shows the corpus statistics of our bilingual training data sets. Table 1: The Sizes of Parallel Texts. At the word alignment step, we did 6 iterations of IBM Model-1 and 6 iterations of HMM. For English-Chinese, we ran 2 iterations of IBM Model4 in addition to Model-1 and HMM. The word alignments are symmetrized using the “union” heuristics. Then, the standard phrase extraction heuristics (Koehn et al., 2003) were applied to extract phrase pairs with a length limit of 6. We ran the hierarchical phrase extraction algorithm with the standard heuristics of Chiang (2005). The phrase-length limit is interpreted as the maximum number of symbols on either the source side or the target side of a given rule. On the same aligned data sets, we also ran the tree-to-string rule extraction algorithm described in Section 2.1 with a limit of 16 rules per tree node. The default parser in the experiments is a shiftreduce dependency parser (Nivre and Scholz, 2004). It achieves 87.8% labelled attachment score and 88.</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="23050" citStr="Koehn, 2004" startWordPosition="3854" endWordPosition="3855">cal phrase-based system. Our system is consistently better than the other two systems in all data sets. On the English-Chinese data set, the improvement over the phrase-based system is 1.3 BLEU points, and 0.8 over the hierarchical phrase-based system. In the tasks of translating to European languages, the improvements over the phrase-based baseline are in the range of 0.5 to 1.0 BLEU points, and 0.3 to 0.5 over the hierarchical phrase-based system. All improvements except the bf2s and hier difference in English-Czech are significant with confidence level above 99% using the bootstrap method (Koehn, 2004). To demonstrate the strength of our systems including the two baseline systems, we also show the reported best results on these data sets from the 2010 WMT workshop. Our forest-to-string system (bf2s) outperforms or ties with the best ones in three out of four language pairs. 5.3 Different Binarization Methods The translation results for the bf2s system in Table 2 are based on the cyk binarization algorithm with bracket violation degree 2. In this section, we BLEU dev test English-Chinese pb 29.7 39.4 hier 31.7 38.9 bf2s 31.9 40.7** English-Czech wmt best - 15.4 pb 14.3 15.5 hier 14.7 16.0 bf</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>163--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="22213" citStr="Kumar et al., 2009" startWordPosition="3714" endWordPosition="3717"> languages. One is trained on the target side of the parallel text, the other is on a corpus provided by the evaluation: the Gigaword corpus for Chinese and news corpora for the others. Besides standard features (Och and Ney, 2004), the phrase-based decoder also uses a Maximum Entropy phrasal reordering model (Zens and Ney, 2006). Both the hierarchical decoder and the forest-to-string decoder only use the standard features. For feature weight tuning, we do Minimum Error Rate Training (Och, 2003). To explore a larger n-best list more efficiently in training, we adopt the hypergraph-based MERT (Kumar et al., 2009). To evaluate the translation results, we use BLEU (Papineni et al., 2002). 5.2 Translation Results Table 2 shows the scores of our system with the best binarization scheme compared to the phrasebased system and the hierarchical phrase-based system. Our system is consistently better than the other two systems in all data sets. On the English-Chinese data set, the improvement over the phrase-based system is 1.3 BLEU points, and 0.8 over the hierarchical phrase-based system. In the tasks of translating to European languages, the improvements over the phrase-based baseline are in the range of 0.5</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 163–171, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A path-based transfer model for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04),</booktitle>
<pages>625--630</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2041" citStr="Lin, 2004" startWordPosition="297" endWordPosition="298">chine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the Licheng Fang Computer Science Department University of Rochester lfang@cs.rochester.edu Xiaoyun Wu Google Research xiaoyunwu@google.com source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000),</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04), pages 625–630, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="2079" citStr="Liu et al., 2006" startWordPosition="303" endWordPosition="306">nifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the Licheng Fang Computer Science Department University of Rochester lfang@cs.rochester.edu Xiaoyun Wu Google Research xiaoyunwu@google.com source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To bal</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06), Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-string statistical translation rules.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (ACL-07),</booktitle>
<location>Prague.</location>
<contexts>
<context position="14625" citStr="Liu et al., 2007" startWordPosition="2403" endWordPosition="2406">rnally by the node and an ancestor chain representing which nodes the node attaches to externally. Line 22 and Line 23 of Algorithm 1 update the label and ancestor annotations of new tree nodes. Using the parsing semiring notations (Goodman, 1999), the ancestor computation can be summarized by the (n, U) pair. n produces the ancestor chain of a hyper-edge. U produces the ancestor chain of a hyper-node. The node label computation can be summarized by the (concatenate, min) pair. concatenate produces a concatenation of node labels. min yields the label with the shortest length. A tree-sequence (Liu et al., 2007) is a sequence of sub-trees covering adjacent spans. It can be proved that the final label of each new node in the forest corresponds to the tree sequence which has the minimum length among all sequences covered by the node span. The ancestor chain of a new node is the common ancestors of the nodes in its minimum tree sequence. For clarity, we do full CYK loops over all O(|w|2) spans and O(|w|3) potential hyper-edges, where |w| is the length of a source string. In reality, only descendants under a shared ancestor can combine. If we assume trees have a bounded branching factor b, the number of </context>
<context position="28732" citStr="Liu et al. (2007)" startWordPosition="4763" endWordPosition="4766">), and then applies k′-way cube pruning. Typically, k′ is 2. BLEU dev test 29.2 37.0 30.0 38.2 31.7 40.5 31.9 40.7 Table 5: The effect of synchronous binarization for treeto-string and forest-to-string systems, on the EnglishChinese task. Table 5 shows that synchronous binarization does help reduce search errors and find better translations consistently in all settings. 6 Related Work The idea of concatenating adjacent syntactic categories has been explored in various syntax-based models. Zollmann and Venugopal (2006) augmented hierarchial phrase based systems with joint syntactic categories. Liu et al. (2007) proposed treesequence-to-string translation rules but did not provide a good solution to place joint subtrees into connection with the rest of the tree structure. Zhang et head-out cube pruning + synch. binarization cyk-2 cube pruning + synch. binarization 842 al. (2009) is the closest to our work. But their goal References was to augment a k-best forest. They did not bina- Chris Callison-Burch, Philipp Koehn, Christof Monz, rize the tree sequences. They also did not put con- Kay Peterson, Mark Przybocki, and Omar Zaidan. straint on the tree-sequence nodes according to how 2010. Findings of t</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (ACL-07), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>206--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2769" citStr="Mi and Huang, 2008" startWordPosition="423" endWordPosition="426">ang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Traditionally, such forests are obtained through hyper-edge pruning in the k-best search space of a monolingual parser (Huang, 2008). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a noun phrase or verb phrase in certain cases. 835 Proceedings of the 49th Annual Meeting of the Association for Computational Lingui</context>
<context position="6130" citStr="Mi and Huang, 2008" startWordPosition="951" endWordPosition="954">onous translation rules with more than two nonterminals regardless of the branching factor of the source trees. For the first time, we show that similar to string-to-tree decoding, synchronous binarization significantly reduces search errors and improves translation quality for forest-to-string decoding. To summarize, the whole pipeline is as follows. First, a parser produces the highest-scored tree for an input sentence. Second, the parse tree is restructured using our binarization algorithm, resulting in a binary packed forest. Third, we apply the forest-based variant of the GHKM algorithm (Mi and Huang, 2008) on the new forest for rule extraction. Fourth, on the translation forest generated by all applicable translation rules, which is not necessarily binary, we apply the synchronous binarization algorithm (Zhang et al., 2006) to generate a binary translation forest. Finally, we use a bottom-up dewhere f stands for a source string, e stands for a target string, F stands for a forest, D stands for a set of synchronous derivations on a given tree T, and Y stands for the target side yield of a derivation. The search problem is finding the derivation with the highest probability in the space of all de</context>
<context position="26843" citStr="Mi and Huang (2008)" startWordPosition="4468" endWordPosition="4471">ests; the other is using the CRF parser with pruning threshold e−T&apos;, where p = 2 to generate forests.1 Although the parser outputs binary trees, we found cross-bracket cyk-2 binarization is still helpful. BLEU dev test cyk-2 14.9 16.0 parser 14.7 15.7 Table 4: Binarized forests versus parser-generated forests for forest-to-string English-German translation. Table 4 shows the comparison of binarization forest and parser forest on English-German translation. The results show that cyk-2 forest performs slightly 1All hyper-edges with negative log posterior probability larger than p are pruned. In Mi and Huang (2008), the threshold is p = 10. The difference is that they do the forest pruning on a forest generated by a k-best algorithm, while we do the forest-pruning on the full CYK chart. As a result, we need more aggressive pruning to control forest size. better than the parser forest. We have not done full exploration of forest pruning parameters to fine-tune the parser-forest. The speed of the constituent parser is the efficiency bottleneck. This actually demonstrates the advantage of the binarizer plus forest-tostring scheme. It is flexible, and works with any parser that generates projective parses. </context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL08:HLT),</booktitle>
<pages>192--199</pages>
<contexts>
<context position="2117" citStr="Mi et al., 2008" startWordPosition="311" endWordPosition="314">synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the Licheng Fang Computer Science Department University of Rochester lfang@cs.rochester.edu Xiaoyun Wu Google Research xiaoyunwu@google.com source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-t</context>
<context position="16577" citStr="Mi et al., 2008" startWordPosition="2764" endWordPosition="2767">ation (c and d). In the chart representation at the bottom, cells with labels containing the concatenation symbol + hold nodes created through binarization. Figure 3 shows some examples of alternative trees generated by the CYK-n algorithm. In this example, standard CYK binarization will not create any new trees since the input is already binary. The CYK-2 and CYK-3 algorithms discover new trees with an increasing degree of freedom. 4 Synchronous Binarization for Forest-to-string Decoding In this section, we deal with binarization of translation forests, also known as translation hypergraphs (Mi et al., 2008). A translation forest is a packed forest representation of all synchronous derivations composed of tree-to-string rules that match the source forest. Tree-to-string decoding algorithms work on a translation forest, rather than a source forest. A binary source forest does not necessarily always result in a binary translation forest. In the treeto-string rule in Figure 4, the source tree is already the Figure 4: Synchronous binarization for a tree-to-string rule. The top rule can be binarized into two smaller rules. binary with the help of source tree binarization, but the translation rule invo</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL08:HLT), pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>64--70</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="20673" citStr="Nivre and Scholz, 2004" startWordPosition="3469" endWordPosition="3472">istics. Then, the standard phrase extraction heuristics (Koehn et al., 2003) were applied to extract phrase pairs with a length limit of 6. We ran the hierarchical phrase extraction algorithm with the standard heuristics of Chiang (2005). The phrase-length limit is interpreted as the maximum number of symbols on either the source side or the target side of a given rule. On the same aligned data sets, we also ran the tree-to-string rule extraction algorithm described in Section 2.1 with a limit of 16 rules per tree node. The default parser in the experiments is a shiftreduce dependency parser (Nivre and Scholz, 2004). It achieves 87.8% labelled attachment score and 88.8% unlabeled attachment score on the standard Penn Treebank test set. We convert dependency parses to constituent trees by propagating the partof-speech tags of the head words to the corresponding phrase structures. We compare three systems: a phrase-based system (Och and Ney, 2004), a hierarchical phrasebased system (Chiang, 2005), and our forest-tostring system with different binarization schemes. In the phrase-based decoder, jump width is set to 8. In the hierarchical decoder, only the glue rule is applied Source Words Target Words Englis</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of Coling 2004, pages 64–70, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="21009" citStr="Och and Ney, 2004" startWordPosition="3522" endWordPosition="3525">he target side of a given rule. On the same aligned data sets, we also ran the tree-to-string rule extraction algorithm described in Section 2.1 with a limit of 16 rules per tree node. The default parser in the experiments is a shiftreduce dependency parser (Nivre and Scholz, 2004). It achieves 87.8% labelled attachment score and 88.8% unlabeled attachment score on the standard Penn Treebank test set. We convert dependency parses to constituent trees by propagating the partof-speech tags of the head words to the corresponding phrase structures. We compare three systems: a phrase-based system (Och and Ney, 2004), a hierarchical phrasebased system (Chiang, 2005), and our forest-tostring system with different binarization schemes. In the phrase-based decoder, jump width is set to 8. In the hierarchical decoder, only the glue rule is applied Source Words Target Words English-Chinese English-Czech English-French English-German English-Spanish 287M 66M 857M 45M 216M 254M 57M 996M 43M 238M 840 to spans longer than 10. For the forest-to-string system, we do not have such length-based reordering constraints. We trained two 5-gram language models with Kneser-Ney smoothing for each of the target languages. One</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03).</booktitle>
<contexts>
<context position="22094" citStr="Och, 2003" startWordPosition="3696" endWordPosition="3697">reordering constraints. We trained two 5-gram language models with Kneser-Ney smoothing for each of the target languages. One is trained on the target side of the parallel text, the other is on a corpus provided by the evaluation: the Gigaword corpus for Chinese and news corpora for the others. Besides standard features (Och and Ney, 2004), the phrase-based decoder also uses a Maximum Entropy phrasal reordering model (Zens and Ney, 2006). Both the hierarchical decoder and the forest-to-string decoder only use the standard features. For feature weight tuning, we do Minimum Error Rate Training (Och, 2003). To explore a larger n-best list more efficiently in training, we adopt the hypergraph-based MERT (Kumar et al., 2009). To evaluate the translation results, we use BLEU (Papineni et al., 2002). 5.2 Translation Results Table 2 shows the scores of our system with the best binarization scheme compared to the phrasebased system and the hierarchical phrase-based system. Our system is consistently better than the other two systems in all data sets. On the English-Chinese data set, the improvement over the phrase-based system is 1.3 BLEU points, and 0.8 over the hierarchical phrase-based system. In </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02).</booktitle>
<contexts>
<context position="22287" citStr="Papineni et al., 2002" startWordPosition="3726" endWordPosition="3729"> other is on a corpus provided by the evaluation: the Gigaword corpus for Chinese and news corpora for the others. Besides standard features (Och and Ney, 2004), the phrase-based decoder also uses a Maximum Entropy phrasal reordering model (Zens and Ney, 2006). Both the hierarchical decoder and the forest-to-string decoder only use the standard features. For feature weight tuning, we do Minimum Error Rate Training (Och, 2003). To explore a larger n-best list more efficiently in training, we adopt the hypergraph-based MERT (Kumar et al., 2009). To evaluate the translation results, we use BLEU (Papineni et al., 2002). 5.2 Translation Results Table 2 shows the scores of our system with the best binarization scheme compared to the phrasebased system and the hierarchical phrase-based system. Our system is consistently better than the other two systems in all data sets. On the English-Chinese data set, the improvement over the phrase-based system is 1.3 BLEU points, and 0.8 over the hierarchical phrase-based system. In the tasks of translating to European languages, the improvements over the phrase-based baseline are in the range of 0.5 to 1.0 BLEU points, and 0.3 to 0.5 over the hierarchical phrase-based sys</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjen Poutsma</author>
</authors>
<title>Data-oriented translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING-00).</booktitle>
<contexts>
<context position="2640" citStr="Poutsma, 2000" startWordPosition="403" endWordPosition="404">ring (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Traditionally, such forests are obtained through hyper-edge pruning in the k-best search space of a monolingual parser (Huang, 2008). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a no</context>
</contexts>
<marker>Poutsma, 2000</marker>
<rawString>Arjen Poutsma. 2000. Data-oriented translation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING-00).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2061" citStr="Quirk et al., 2005" startWordPosition="299" endWordPosition="302">lation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the Licheng Fang Computer Science Department University of Rochester lfang@cs.rochester.edu Xiaoyun Wu Google Research xiaoyunwu@google.com source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05), pages 271–279, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT),</booktitle>
<publisher>ACL.</publisher>
<location>Columbus, OH.</location>
<contexts>
<context position="2013" citStr="Shen et al., 2008" startWordPosition="291" endWordPosition="294">orporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the Licheng Fang Computer Science Department University of Rochester lfang@cs.rochester.edu Xiaoyun Wu Google Research xiaoyunwu@google.com source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for </context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), Columbus, OH. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>746--754</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="29464" citStr="Wang et al. (2007)" startWordPosition="4882" endWordPosition="4886">o connection with the rest of the tree structure. Zhang et head-out cube pruning + synch. binarization cyk-2 cube pruning + synch. binarization 842 al. (2009) is the closest to our work. But their goal References was to augment a k-best forest. They did not bina- Chris Callison-Burch, Philipp Koehn, Christof Monz, rize the tree sequences. They also did not put con- Kay Peterson, Mark Przybocki, and Omar Zaidan. straint on the tree-sequence nodes according to how 2010. Findings of the 2010 joint workshop on statistimany brackets are crossed. cal machine translation and metrics for machine transWang et al. (2007) used target tree binarization to lation. In Proceedings of the Joint Fifth Workshop on improve rule extraction for their string-to-tree sys- Statistical Machine Translation and Metrics(MATR), tem. Their binarization forest is equivalent to our pages 17–53, Uppsala, Sweden, July. Association for cyk-1 forest. In contrast to theirs, our binarization Computational Linguistics. Revised August 2010. scheme affects decoding directly because we match David Chiang. 2005. A hierarchical phrase-based model tree-to-string rules on a binarized forest. for statistical machine translation. In Proceedings o</context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 746– 754, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative reordering models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="21925" citStr="Zens and Ney, 2006" startWordPosition="3668" endWordPosition="3671">rench English-German English-Spanish 287M 66M 857M 45M 216M 254M 57M 996M 43M 238M 840 to spans longer than 10. For the forest-to-string system, we do not have such length-based reordering constraints. We trained two 5-gram language models with Kneser-Ney smoothing for each of the target languages. One is trained on the target side of the parallel text, the other is on a corpus provided by the evaluation: the Gigaword corpus for Chinese and news corpora for the others. Besides standard features (Och and Ney, 2004), the phrase-based decoder also uses a Maximum Entropy phrasal reordering model (Zens and Ney, 2006). Both the hierarchical decoder and the forest-to-string decoder only use the standard features. For feature weight tuning, we do Minimum Error Rate Training (Och, 2003). To explore a larger n-best list more efficiently in training, we adopt the hypergraph-based MERT (Kumar et al., 2009). To evaluate the translation results, we use BLEU (Papineni et al., 2002). 5.2 Translation Results Table 2 shows the scores of our system with the best binarization scheme compared to the phrasebased system and the hierarchical phrase-based system. Our system is consistently better than the other two systems i</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative reordering models for statistical machine translation. In Proceedings on the Workshop on Statistical Machine Translation, pages 55–63, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Meeting of the</booktitle>
<contexts>
<context position="6352" citStr="Zhang et al., 2006" startWordPosition="986" endWordPosition="989">duces search errors and improves translation quality for forest-to-string decoding. To summarize, the whole pipeline is as follows. First, a parser produces the highest-scored tree for an input sentence. Second, the parse tree is restructured using our binarization algorithm, resulting in a binary packed forest. Third, we apply the forest-based variant of the GHKM algorithm (Mi and Huang, 2008) on the new forest for rule extraction. Fourth, on the translation forest generated by all applicable translation rules, which is not necessarily binary, we apply the synchronous binarization algorithm (Zhang et al., 2006) to generate a binary translation forest. Finally, we use a bottom-up dewhere f stands for a source string, e stands for a target string, F stands for a forest, D stands for a set of synchronous derivations on a given tree T, and Y stands for the target side yield of a derivation. The search problem is finding the derivation with the highest probability in the space of all derivations for all parse trees for an input sentence. The log probability of a derivation is normally a linear combination of local features which enables dynamic programming to find the optimal combination efficiently. In </context>
<context position="17285" citStr="Zhang et al., 2006" startWordPosition="2877" endWordPosition="2880">omposed of tree-to-string rules that match the source forest. Tree-to-string decoding algorithms work on a translation forest, rather than a source forest. A binary source forest does not necessarily always result in a binary translation forest. In the treeto-string rule in Figure 4, the source tree is already the Figure 4: Synchronous binarization for a tree-to-string rule. The top rule can be binarized into two smaller rules. binary with the help of source tree binarization, but the translation rule involves three variables in the set of frontier nodes. If we apply synchronous binarization (Zhang et al., 2006), we can factorize it into two smaller translation rules each having two variables. Obviously, the second rule, which is a common pattern, is likely to be shared by many translation rules in the derivation forest. When beams are fixed, search goes deeper in a factorized translation forest. The challenge of synchronous binarization for a forest-to-string system is that we need to first match large tree fragments in the input forest as the first step of decoding. Our solution is to do the matching using the original rules and then run synchronous binarization to break matching rules down to fact</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the 2006 Meeting of the</rawString>
</citation>
<citation valid="true">
<authors>
<author>North</author>
</authors>
<title>American chapter of the Association for Computational Linguistics</title>
<date></date>
<pages>256--263</pages>
<location>New York, NY.</location>
<marker>North, </marker>
<rawString>North American chapter of the Association for Computational Linguistics (NAACL-06), pages 256–263, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>559--567</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2167" citStr="Zhang et al., 2008" startWordPosition="319" endWordPosition="322">nsducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the Licheng Fang Computer Science Department University of Rochester lfang@cs.rochester.edu Xiaoyun Wu Google Research xiaoyunwu@google.com source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 200</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proceedings ofACL-08: HLT, pages 559–567, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
<author>Aiti Aw</author>
<author>Chew Lim Tan</author>
</authors>
<title>Forest-based tree sequence to string translation model.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>172--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<marker>Zhang, Zhang, Li, Aw, Tan, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and Chew Lim Tan. 2009. Forest-based tree sequence to string translation model. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 172–180, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="1955" citStr="Zollmann and Venugopal, 2006" startWordPosition="281" endWordPosition="284">years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the Licheng Fang Computer Science Department University of Rochester lfang@cs.rochester.edu Xiaoyun Wu Google Research xiaoyunwu@google.com source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source si</context>
<context position="28638" citStr="Zollmann and Venugopal (2006)" startWordPosition="4749" endWordPosition="4752"> 4 to transform the grammar intersected with the input sentence to the minimum branching factor k′ (k′ &lt; k), and then applies k′-way cube pruning. Typically, k′ is 2. BLEU dev test 29.2 37.0 30.0 38.2 31.7 40.5 31.9 40.7 Table 5: The effect of synchronous binarization for treeto-string and forest-to-string systems, on the EnglishChinese task. Table 5 shows that synchronous binarization does help reduce search errors and find better translations consistently in all settings. 6 Related Work The idea of concatenating adjacent syntactic categories has been explored in various syntax-based models. Zollmann and Venugopal (2006) augmented hierarchial phrase based systems with joint syntactic categories. Liu et al. (2007) proposed treesequence-to-string translation rules but did not provide a good solution to place joint subtrees into connection with the rest of the tree structure. Zhang et head-out cube pruning + synch. binarization cyk-2 cube pruning + synch. binarization 842 al. (2009) is the closest to our work. But their goal References was to augment a k-best forest. They did not bina- Chris Callison-Burch, Philipp Koehn, Christof Monz, rize the tree sequences. They also did not put con- Kay Peterson, Mark Przyb</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation, pages 138–141, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>