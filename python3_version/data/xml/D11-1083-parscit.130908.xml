<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000968">
<title confidence="0.995013">
Heuristic Search for Non-Bottom-Up Tree Structure Prediction
</title>
<author confidence="0.997152">
Andrea Gesmundo
</author>
<affiliation confidence="0.9995635">
Department of Computer Science
University of Geneva
</affiliation>
<email confidence="0.974048">
andrea.gesmundo@unige.ch
</email>
<author confidence="0.981945">
James Henderson
</author>
<affiliation confidence="0.9995335">
Department of Computer Science
University of Geneva
</affiliation>
<email confidence="0.989752">
james.henderson@unige.ch
</email>
<sectionHeader confidence="0.99856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999609842105263">
State of the art Tree Structures Prediction
techniques rely on bottom-up decoding. These
approaches allow the use of context-free fea-
tures and bottom-up features. We discuss
the limitations of mainstream techniques in
solving common Natural Language Process-
ing tasks. Then we devise a new framework
that goes beyond Bottom-up Decoding, and
that allows a better integration of contextual
features. Furthermore we design a system that
addresses these issues and we test it on Hierar-
chical Machine Translation, a well known tree
structure prediction problem. The structure
of the proposed system allows the incorpora-
tion of non-bottom-up features and relies on
a more sophisticated decoding approach. We
show that the proposed approach can find bet-
ter translations using a smaller portion of the
search space.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993172">
Tree Structure Prediction (TSP) techniques have
become relevant in many Natural Language Pro-
cessing (NLP) applications, such as Syntactic Pars-
ing, Semantic Role Labeling and Hierarchical Ma-
chine Translation (HMT) (Chiang, 2007). HMT
approaches have a higher complexity than Phrase-
Based Machine Translation techniques, but exploit
a more sophisticated reordering model, and can
produce translations with higher Syntactic-Semantic
quality.
TSP requires as inputs: a weighted grammar, G,
and a sequence of symbols or a set of sequences en-
coded as a Lattice (Chappelier et al., 1999). The
input sequence is often a sentence for NLP applica-
tions. Tree structures generating the input sequence
can be composed using rules, r, from the weighted
grammar, G. TSP techniques return as output a tree
structure or a set of trees (forest) that generate the
input string or lattice. The output forest can be rep-
resented compactly as a weighted hypergraph (Klein
and Manning, 2001). TSP tasks require finding the
tree, t, with the highest score, or the best-k such
trees. Mainstream TSP relies on Bottom-up Decod-
ing (BD) techniques.
With this paper we propose a new framework
as a generalization of the CKY-like Bottom-up ap-
proach. We also design and test an instantiation of
this framework, empirically showing that wider con-
textual information leads to higher accuracy for TSP
tasks that rely on non-local features, like HMT.
</bodyText>
<sectionHeader confidence="0.978663" genericHeader="method">
2 Beyond Bottom-up Decoding
</sectionHeader>
<bodyText confidence="0.99977925">
TSP decoding requires scoring candidate trees,
cost(t). Some TSP tasks require only local features.
For these cases cost(t) depends only on the local
score of the rules that compose t :
</bodyText>
<equation confidence="0.8994095">
cost(t) = � cost(ri) (1)
ri∈t
</equation>
<bodyText confidence="0.99996875">
This is the case for Context Free Grammars. More
complex tasks need non-local features. Those fea-
tures can be represented by a non-local factor,
nonLocal(t), into the overall t score:
</bodyText>
<equation confidence="0.734757">
cost(t) = � cost(ri) + nonLocal(t) (2)
ri∈t
</equation>
<page confidence="0.987521">
899
</page>
<note confidence="0.9586395">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 899–908,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9988200625">
For example, in HMT the Language Model (LM) is
a non-local fundamental feature that approximates
the adequacy of the translation with the sum of log-
probabilities of composing n-grams.
CKY-like BD approaches build candidate trees in
a bottom-up fashion, allowing the use of Dynamic
Programming techniques to simplify the search
space by mering sub-trees with the same state, and
also easing application of pruning techniques (such
as Cube Pruning, e.g. Chiang (2007), Gesmundo
(2010)). For clarity of presentation and follow-
ing HMT practice, we will henceforth restrict our
focus to binary grammars. Standard CKY works
by building objects known as items (Hopkins and
Langmead, 2009). Each item, t, corresponds to a
candidate sub-tree. Items are built linking a rule
instantiation, r, to two sub-items that represents
left context, t1, and right context, t2; formally:
t - ( t1 &gt; r &lt; t2 ). An item is a triple that
contains a span, a postcondition and a carry. The
span contains the indexes of the starting and end-
ing input words delimiting the continuous sequence
covered by the sub-tree represented by the item. The
postcondition is a string that represents r’s head non-
terminal label, telling us which rules may be applied.
The carry, K, stores extra information required to
correctly score the non-local interactions of the item
when it will be linked in a broader context (for HMT
with LM the carry consists of boundary words that
will form new n-grams).
Items, t - (t1 &gt; r &lt; t2), are scored according to
the following formula:
</bodyText>
<equation confidence="0.998762">
cost(t) = cost(r) + cost(t1) + cost(t2) (3)
+ interaction(r, K1, K2)
</equation>
<bodyText confidence="0.992265473684211">
Where: cost(r) is the cost associated to the weighted
rule r; cost(t1) and cost(t2) are the costs of the two
sub-items computed recursively using formula (3);
interaction(r, K1, K2) is the interaction cost between
the rule instantiation and the two sub-items. In HMT
the interaction cost includes the LM score of new n-
grams generated by connecting the childrens’ sub-
spans with terminals of r. Notice that formula (3) is
equal to formula (2) for items that cover the whole
input sequence.
In many TSP applications, the search space is
too large to allow an exhaustive search and there-
fore pruning techniques must be used. Pruning deci-
sions are based on the score of partial derivations.
It is not always possible to compute exactly non-
local features while computing the score of partial
derivations, since partial derivations miss part of the
context. Formula (3) accounts for the interaction be-
tween r and sub-items t1 and t2, but it does not in-
tegrate the cost relative to the interaction between
the item and the surrounding context. Therefore the
item score computed in a bottom-up fashion is an
approximation of the score the item has in a broader
context. For example, in HMT the LM score for n-
grams that partially overlap the item’s span cannot
be computed exactly since the surrounding words
are not known.
Basing pruning decisions on approximated scores
can introduce search errors. It is possible to reduce
search errors using heuristics based on future cost
estimation. In general the estimation of the interac-
tion between t and the surrounding context is func-
tion of the carry, K. In HMT it is possible to estimate
the cost of n-grams that partially overlap t’s span
considering the boundary words. We can obtain the
heuristic cost for an item, t, adding to formula (3)
the factor, est(K), for the estimation of interaction
with missing context:
</bodyText>
<equation confidence="0.999674">
heuristicCost(t) = cost(t) + est(K) (4)
</equation>
<bodyText confidence="0.99975365">
And use heuristicCost(t) to guide BD pruning de-
cisions. Anyway, even if a good interaction estima-
tion is available, in practice it is not possible to avoid
search errors while pruning.
More sophisticated parsing models allow the use
of non-bottom-up features within a BD framework.
Caraballo and Charniak (1998) present best-first
parsing with Figures of Merit that allows condition-
ing of the heuristic function on statistics of the input
string. Corazza et al. (1994), and Klein and Man-
ning (2003) propose an A* parsing algorithm that
estimates the upper bound of the parse completion
scores using contextual summary features. These
models achieve time efficiency and state-of-the-art
accuracy for PCFG parsing, but still use a BD frame-
work that doesn’t allow the application of a broader
class of non-bottom-up contextual features.
In HMT, knowing the sentence-wide context in
which a sub-phrase is translated is extremely impor-
tant. It is obviously important for word choice: as
</bodyText>
<page confidence="0.991573">
900
</page>
<bodyText confidence="0.999293272727273">
a simple example consider the translation of the fre-
quent English word “get” into Chinese. The choice
of the correct set of ideograms to translate “get” of-
ten requires being aware of the presence of particles
that can be at any distance within the sentence. In a
common English to Chinese dictionary we found 93
different sets of ideograms that could be translations
of “get”. Sentence-wide context is also important
in the choice of word re-ordering: as an example
consider the following translations from English to
German:
</bodyText>
<listItem confidence="0.882668">
1. EN : Igo home.
DE : Ich gehe nach Hause.
2. EN : I say, that Igo home.
DE : Ich sage, dass ich nach Hause gehe.
3. EN : On Sunday Igo home.
DE : Am Sonntag gehe ich nach Hause.
</listItem>
<bodyText confidence="0.9998439">
The English phrase “I go home” is translated in Ger-
man using the same set of words but with different
orderings. It is not possible to choose the correct
ordering of the phrase without being aware of the
context. Thus a bottom-up decoder without context
needs to build all translations for “I go home”, intro-
ducing the possibility of pruning errors.
Having shown the importance of contextual fea-
tures, we define a framework that overcomes the
limitations of bottom-up feature approximation.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="method">
3 Undirected-CKY Framework
</sectionHeader>
<bodyText confidence="0.995655783783784">
Our aim is to propose a new Framework that over-
comes BD limitations allowing a better integration
of contextual features. The presented framework can
be regarded as a generalization of CKY.
To introduce the new framework let us focus on a
detail of CKY BD. The items are created and scored
in topological order. The ordering constraint can be
formally stated as: an item covering the span [i, j]
must be processed after items covering sub spans
[h, k]|h ≥ i, k ≤ j. This ordering constraint im-
plies that full yield information is available when
an item is processed, but information about ances-
tors and siblings is missing. Therefore non-bottom-
up context cannot be used because of the ordering
constraint. Now let us investigate how the decoding
algorithm could change if we remove the ordering
constraint.
Removing the ordering constraint would lead to
the occurrence of cases in which an item is pro-
cessed before all child items have been processed.
For example, we could imagine to create and score
an item, ι, with postcondition X and span [i, j], link-
ing the rule instantiation r : X →AB with only
the left sub-item, ιA, while information for the right
sub-item, ιB is still missing. In this case, we can
rely on local and partial contextual features to score
ι. Afterwards, it is possible to process ιB using the
parent item, ι, as a source of additional informa-
tion about the parent context and sibling ιA yield.
This approach can avoid search errors in cases where
pruning at the parent level can be correctly done us-
ing only local and partial yield context, while prun-
ing at the child level needs extra non-bottom-up con-
text to make a better pruning decision. For exam-
ple, consider the translation of the English sentence
“I run” into French using the following synchronous
grammar:
</bodyText>
<equation confidence="0.951933">
r1 : S → X1 X2  |X1 X2
r2 : X → I  |Je
r3 : X → run  |course
r4 : X → run  |courir
r5 : X → run  |cours
r6 : X → run  |courons
...
</equation>
<bodyText confidence="0.981244176470588">
Where: r1 is a Glue rule and boxed indexes de-
scribe the alignment; r2 translates “I” in the cor-
responding French pronoun; r3 translates “run” as
a noun; remaining rules translate “run” as one of
the possible conjugations of the verb “courir”. Us-
ing only bottom-up features it is not possible to re-
solve the ambiguity of the word “run”. If the beam
is not big enough the correct translation could be
pruned. Anyway a CKY decoder would give the
highest score to the most frequent translation. In-
stead, if we follow a non bottom-up approach, as
described in Figure 1, we can: 1) first translate “I”;
2) Then create an item using r1 with missing right
child; 3) finally choose the correct translation for
“run” using r1 to access a wider context. Notice
that with this undirected approach it is possible to
reach the correct translation using only beam size of
</bodyText>
<page confidence="0.995726">
901
</page>
<figureCaption confidence="0.9975445">
Figure 1: Example of undirected decoding for HMT. The arrows point to the direction in which information is propa-
gated. Notice that the parent link at step 3 is fundamental to correctly disambiguate the translation for “run”.
</figureCaption>
<bodyText confidence="0.995636684210527">
1 and the LM feature.
To formalize Undirected-CKY, we define a gen-
eralized item called undirected-item. Undirected-
items, 1, are built linking rule instantiations with
elements in L - {left child, right child, parent};
for example: 1 - (11 ⋗ r ∨˙ 1p), is built linking
r with left child, 11, and parent, 1p. We denote
with L+ι˚ the set of links for which the undirected-
item, 1, has a connection, and with L−ι˚ the set
of missing links. An undirected-item is a triple
that contains a span, a carry and an undirected-
postcondition. The undirected-postcondition is a
set of strings, one string for each ofi’s missing links,
l E L−ι˚ . Each string represents the non-terminal re-
lated to one of the missing links available for expan-
sion. Bottom-up items can be considered specific
cases of undirected-items having L+ = { left child,
right child} and L− = {parent}. We can formally
describe the steps of the example depicted in Figure
</bodyText>
<equation confidence="0.987890142857143">
1 with:
1) r2 : X → I|Je , terminal : [0, 1]
11 : [0, 1, {Xp }, K1]
r1 : S → X1X2 |X1 X2 , 11 : [0, 1, {Xp },K1]
2
3)r5:X → run|cours,12:[· · · ] , terminal : [1, 2]
13 : [0, 2, {}, K3]
</equation>
<bodyText confidence="0.9951535">
The scoring function for undirected-items can be ob-
tained generalizing formula (3):
</bodyText>
<equation confidence="0.99315275">
cost(!) = cost(r)
+ � cost(LI) (5)
lEG+
+ interaction(r, L+)
</equation>
<bodyText confidence="0.999939523809524">
In CKY, each span is processed separately in
topological order, and the best-k items for each span
are selected in sequence according to scoring func-
tion (4). In the proposed framework, the selec-
tion of undirected-items can be done in any order,
for example: in a first step selecting an undirected-
item for span s1, then selecting an undirected-item
for span s2, and in a third step selecting a second
undirected-item for s1, and so on. As in agenda
based parsing (Klein and Manning, 2001), all candi-
date undirected-items can be handled with an unique
queue. Allowing the system to decide decoding or-
der based on the candidates’ scores, so that candi-
dates with higher confidence can be selected earlier
and used as context for candidates with lower confi-
dence.
Having all candidates in the same queue intro-
duces comparability issues. In CKY, candidates are
comparable since each span is processed separately
and each candidate is scored with the estimation of
the yield score. Instead, in the proposed framework,
</bodyText>
<equation confidence="0.751222">
12 : [0, 1, {X2 }, K2)]
</equation>
<page confidence="0.973012">
902
</page>
<bodyText confidence="0.999178">
the unique queue contains candidates relative to dif-
ferent nodes and with different context scope. To
ensure comparability, we can associate to candidate
undirected-items a heuristic score of the full deriva-
tion:
</bodyText>
<equation confidence="0.999769">
heuristicCost(l) = cost(!) + est(l) (6)
</equation>
<bodyText confidence="0.99994556">
Where est(l) estimates the cost of the missing
branches of the derivation as a function of s par-
tial structure and carry.
In this framework, the queue can be initialized
with a candidate for each rule instantiation. These
initializing candidates have no context information
and can be scored using only local features. A
generic decoding algorithm can loop selecting the
candidate undirected-item with the highest score,1,
and then propagating its information to neighboring
candidates, which can update using1 as context. In
this general framework the link to the parent node is
not treated differently from links to children. While
in CKY the information is always passed from chil-
dren to parent, in Undirected-CKY the information
can be propagated in any direction, and any decod-
ing order is allowed.
We can summarize the steps done to generalize
CKY into the proposed framework: 1) remove the
node ordering constraint; 2) define the scoring of
candidates with missing children or parent; 3) use
a single candidate queue; 4) handle comparability of
candidates from different nodes and/or with differ-
ent context scope; 5) allow information propagation
in any direction.
</bodyText>
<sectionHeader confidence="0.996883" genericHeader="method">
4 Undirected Decoding
</sectionHeader>
<bodyText confidence="0.968697357142857">
In this section we propose Undirected Decod-
ing (UD), an instantiation of the Undirected-CKY
framework presented above. The generic framework
introduces many new degrees of freedom that could
lead to a higher complexity of the decoder. In our
actual instantiation we apply constraints on the ini-
tialization step, on the propagation policy, and fix a
search beam of k. These constraints allow the sys-
tem to converge to a solution in practical time, al-
low the use of dynamic programming techniques to
merge items with equivalent states, and gives us the
possibility of using non-bottom-up features and test-
ing their relevance.
Algorithm 1 Undirected Decoding
</bodyText>
<listItem confidence="0.923179555555556">
1: function decoder (k) : out-forest
2: Q +- LeafRules();
3: while JQJ &gt; 0 do
4: 1 +- PopBest (Q);
5: if CanPop(l) then
6: out-forest.Add(l);
7: if i.HasChildrenLinks() then
8: for all r E HeadRules(L) do
9: C +- NewUndirectedItems(r,1);
10: for all c E C do
11: if CanPop(c) then
12: Q.Insert(c);
13: end if
14: end for
15: end for
16: end if
17: end if
18: end while
</listItem>
<bodyText confidence="0.999749666666667">
Algorithm 1 summarizes the UD approach. The
beam size, k, is given as input. At line 2 the queue
of undirected-item candidates, Q, is initialized with
only leafs rules. At line 3 the loop starts, it will
terminate when Q is empty. At line 4 the candi-
date with highest score,1, is popped from Q. line 5
checks if1 is within the beam width: if1 has a span
for which k candidates were already popped, then1
is dropped and a new iteration is begun. Otherwise
1 is added to the out-forest at line 6. From line 7
to line 10 the algorithm deals with the generation of
new candidate undirected-items. line 7 checks if1
has both children links, if not a new decoding iter-
ation is begun. line 8 loops over the rule instantia-
tions, r, that can use1 as child. At line 9, the set of
new candidates, C, is built linking r with1 and any
context already available in the out-forest. Finally,
between line 10 and line 12, each element c in C
is inserted in Q after checking that c is within the
beam width: if c has a span for which k candidates
were already popped it doesn’t make sense to insert
it in Q since it will be surely discarded at line 5.
In more detail, the function
NewUndirectedItems(r,1) at line 9 creates new
undirected-items linking r using: 1)1 as child; 2)
(optionally) as other child any other undirected-item
that has already been inserted in the out-forest and
</bodyText>
<page confidence="0.997627">
903
</page>
<bodyText confidence="0.999960375">
doesn’t have a missing child and matches missing
span coverage; 3) and using as parent context the
best undirected-item with missing child link that
has been incorporated in the out-forest and can
expand the missing child link using r. In our current
method, only the best possible parent context is
used because it only provides context for ranking
candidates, as discussed at the end of this section.
In contrast, a different candidate is generated for
each possible other child in 2), as well as for
the case where no other child is included in the
undirected-item.
We can make some general observations on the
Undirected Decoding Algorithm. Notice that, the
if statement at line 7 and the way new undirected-
items are created at line 9, enforce that each
undirected-item covers a contiguous span. An
undirected-item that is missing a child link cannot
be used as child context but can be used as parent
context since it is added to the out-forest at line 6
before the if statement at line 7. Furthermore, the
if statements at line 5 and line 11 check that no
more than k candidates are selected for each span,
but the algorithm does not require the the selection
of exactly k candidates per span as in CKY.
The queue of candidates, Q, is ordered according
to the heuristic cost of formula (6). The score of the
candidate partial structure is accounted for with fac-
tor cost(˚�) computed according to formula (5). The
factor est(˚�) accounts for the estimation of the miss-
ing part of the derivation. We compute this factor
with the following formula:
</bodyText>
<equation confidence="0.902602">
( )
localCost(˚�, l) + contextEst(˚�, l)
(7)
</equation>
<bodyText confidence="0.99437937254902">
For each missing link, l E L−ι˚ , we estimate the cost
of the corresponding derivation branch with two fac-
tors: localCost(˚�, l) that computes the context-free
score of the branch with highest score that could
be attached to l; and contextEst(˚�, l) that estimates
the contextual score of the branch and its interac-
tion with˚�. Because our model is implemented in
the Forest Rescoring framework (e.g. Huang and
Chiang (2007), Dyer et al. (2010), Li et al. (2009)),
localCost(˚�, l) can be efficiently computed exactly.
In HMT it is possible to exhaustively represent and
search the context-free-forest (ignoring the LM),
which is done in the Forest Rescoring framework be-
fore our task of decoding with the LM. We exploit
this context-free-forest to compute localCost(˚�, l):
for missing child links the localCost(·) is the In-
side score computed using the (max, +) semiring
(also known as the Viterbi score), and for missing
parent links the localCost(·) is the corresponding
Outside score. The factor contextEst(·) estimates
the LM score of the words generated by the missing
branch and their interaction with the span covered
by˚�. To compute the expected interaction cost we
use the boundary words information contained in˚�’s
carry as done in BD. To estimate the LM cost of the
missing branch we use an estimation function, con-
ditioned on the missing span length, whose parame-
ters are tuned on held-out data with gradient descent,
using the search score as objective function.
To show that UD leads to better results than BD,
the two algorithms are compared in the same search
space. Therefore we ensure that candidates em-
bedded in the UD out-forest would have the same
score if they were scored from BD. We don’t need
to worry about differences derived from the missing
context estimation factor, est(·), since this factor is
only considered while sorting the queue, Q, accord-
ing to the heuristicCost(·). Also, we don’t have to
worry about candidates that are scored with no miss-
ing child and no parent link, because in that case
scoring function (3) for BD is equivalent to scoring
function (5) for UD. Instead, for candidates that are
scored with parent link, we remove the parent link
factor from the cost(·) function when inserting the
candidate into the out forest. And for the candi-
dates that are scored with a missing child, we ad-
just the score once the link to the missing child is
created in the out-forest. In this way UD and BD
score the same derivation with the same score and
can be regarded as two ways to explore the same
search space.
</bodyText>
<sectionHeader confidence="0.999761" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999964166666667">
In this section we test the algorithm presented, and
empirically show that it produces better translations
searching a smaller portion of the search space.
We implemented UD on top of a widely-used
HMT open-source system, cdec (Dyer et al., 2010).
We compare with cdec Cube Pruning BD. The ex-
</bodyText>
<equation confidence="0.9919925">
�est(˚�) =
l∈L−ι˚
</equation>
<page confidence="0.996111">
904
</page>
<figure confidence="0.980845">
Search Score
2 4 6 8 10 12 14 16
Beam Size
</figure>
<figureCaption confidence="0.994817">
Figure 2: Comparison of the quality of the translations.
</figureCaption>
<figure confidence="0.9677405">
2 4 6 8 10 12 14 16
Beam Size
</figure>
<figureCaption confidence="0.98412">
Figure 3: Search score evolution for BD and UD.
</figureCaption>
<figure confidence="0.978292772727273">
UD best score
BD best score
-123
-123.5
-124
-124.5
-125
-125.5
-126
-126.5
Bottom-up Decoding
Guided Decoding
Test Set 900
800
700
600
500
400
300
200
100
0
</figure>
<bodyText confidence="0.99935121875">
periments are executed on the NIST MT03 Chinese-
English parallel corpus. The training corpus con-
tains 239k sentence pairs with 6.9M Chinese words
and 8.9M English words. We use a hierarchical
phrase-based translation grammar extracted using a
suffix array rule extractor (Lopez, 2007). The NIST-
03 test set is used for decoding, it has 919 sentence
pairs. The experiments can be reproduced on an
average desktop computer. Since we compare two
different decoding strategies that rely on the same
training technique, the evaluation is primarily based
on search errors rather than on BLEU. We compare
the two systems on a variety of beam sizes between
1 and 16.
Figure 2 reports a comparison of the translation
quality for the two systems in relation to the beam
size. The blue area represents the portion of sen-
tences for which UD found a better translation. The
white area represents the portion of sentences for
which the two systems found a translation with the
same search score. With beam 1 the two systems ob-
viously have a similar behavior, since both the sys-
tems stop investigating the candidates for a node af-
ter having selected the best candidate immediately
available. For beams 2-4, UD has a clear advan-
tage. In this range UD finds a better translation for
two thirds of the sentences. With beam 4, we ob-
serve that UD is able to find a better translation for
63.76% of the sentences, instead BD is able to find a
better translation for only 21.54% of the sentences.
For searches that employ a beam bigger than 8, we
notice that the UD advantage slightly decreases, and
the number of sentences with equivalent translation
slowly increases. We can understand this behavior
considering that as the beam increases the two sys-
tems get closer to exhaustive search. Anyway with
this experiment UD shows a consistent accuracy ad-
vantage over BD.
Figure 3 plots the search score variation for dif-
ferent beam sizes. We can see that UD search leads
to an average search score that is consistently bet-
ter than the one computed for BD. Undirected De-
coding improves the average search score by 0.411
for beam 16. The search score is the logarithm of
a probability. This variation corresponds to a rel-
ative gain of 50.83% in terms of probability. For
beams greater than 8 we see that the two curves keep
a monotonic ascendant behavior while converging to
exhaustive search.
Figure 4 shows the BLEU score variation. Again
we can see the consistent improvement of UD over
BD. In the graph we report also the performance ob-
tained using BD with beam 32. BD reaches BLEU
score of 32.07 with beam 32 while UD reaches
32.38 with beam 16: UD reaches a clearly higher
BLEU score using half the beam size. The differ-
ence is even more impressive if we consider that UD
reaches a BLEU of 32.19 with beam 4.
In Figure 5 we plot the percentage reduction of the
size of the hypergraphs generated by UD compared
to those generated by BD. The size reduction grows
quickly for both nodes and edges. This is due to the
fact that BD, using Cube Pruning, must select k can-
didates for each node. Instead, UD is not obliged to
</bodyText>
<page confidence="0.994634">
905
</page>
<figure confidence="0.974015">
2 4 6 8 10 12 14 16
Beam Size
</figure>
<figureCaption confidence="0.994585">
Figure 4: BLEU score evolution for BD and UD.
</figureCaption>
<figure confidence="0.9322325">
2 4 6 8 10 12 14 16
Beam Size
</figure>
<figureCaption confidence="0.975566">
Figure 5: Percentage of reduction of the size of the hy-
pergraph produced by UD.
</figureCaption>
<bodyText confidence="0.9494385625">
select k candidates per f-node. As we can see from
Algorithm 1, the decoding loop terminates when the
queue of candidates is empty, and the statements at
line 5 and line 11 ensure that no more than k can-
didates are selected per f-node, but nothing requires
the selection of k elements, and some bad candidates
may not be generated due to the sophisticated prop-
agation strategy. The number of derivations that a
hypergraph represents is exponential in the number
of nodes and edges composing the structure. With
beam 16, the hypergraphs produced by UD contain
on average 4.6k fewer translations. Therefore UD
is able to find better translations even if exploring a
smaller portion of the search space.
Figure 6 reports the time comparison between
BD and UD with respect to sentence length. The
</bodyText>
<figureCaption confidence="0.998089">
Figure 6: Time comparison between BD and UD.
</figureCaption>
<bodyText confidence="0.999899807692308">
sentence length is measured with the number of
ideogram groups appearing in the source Chinese
sentences. We compare BD with beam of 16 and
UD with beam of 8, so that we compare two sys-
tems with comparable search score. We can notice
that for short sentences UD is faster, while for longer
sentences UD becomes slower. To understand this
result consider that for simple sentences UD can
rely on the advantage of exploring a smaller search
space. While, for longer sentences, the amount of
candidates considered during decoding grows ex-
ponentially with the size of the sentence, and UD
needs to maintain an unique queue whose size is not
bounded by the beam size k, as for the queues used
in BD’s Cube Pruning. It may be possible to address
this issue with more efficient handling of the queue.
In conclusion we can assert that, even if explor-
ing a smaller portion of the search space, UD finds
often a translation that is better than the one found
with standard BD. UD’s higher accuracy is due to
its sophisticated search strategy that allows a more
efficient integration of contextual features. This set
of experiments show the validity of the UD approach
and empirically confirm our intuition about the BD’s
inadequacy in solving tasks that rely on fundamental
contextual features.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.99920625">
In the proposed framework the link to the parent
node is not treated differently from links to child
nodes, the information in the hypergraph can be
propagated in any direction. Then the Derivation
</bodyText>
<figure confidence="0.991210575757576">
Bottom-up Decoding
Undirected Decoding
BD beam 30
Reduction (%)
40
35
30
25
20
15
10
5
Nodes Reduction
Edges Reduction
1200
1000
800
600
400
200
Bottom-up Decoding, beam = 16
Undirected Decoding, beam = 8
0
10 15 20 25 30 35 40 45 50
Input Sentence Size
Time (ms)
BLEU Score 32.4
32.2
32
31.8
31.6
31.4
31.2
</figure>
<page confidence="0.996133">
906
</page>
<bodyText confidence="0.999774307692308">
Hypergraph can be regarded as a non-directed graph.
In this setting we could imagine applying mes-
sage passing algorithms from graphical model the-
ory (Koller and Friedman, 2010).
Furthermore, considering that the proposed
framework lets the system decide the decoding or-
der, we could design a system that explicitly learns
to infer the decoding order at training time. Sim-
ilar ideas have been successfully tried: Shen et al.
(2010) and Gesmundo (2011) investigate the Guided
Learning framework, that dynamically incorporates
the tasks of learning the order of inference and train-
ing the local classifier.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999635">
With this paper we investigate the limitations of
Bottom-up parsing techniques, widely used in Tree
Structures Prediction, focusing on Hierarchical Ma-
chine Translation. We devise a framework that al-
lows a better integration of non-bottom-up features.
Compared to a state of the art HMT decoder the pre-
sented system produces higher quality translations
searching a smaller portion of the search space, em-
pirically showing that the bottom-up approximation
of contextual features is a limitation for NLP tasks
like HMT.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992722333333333">
This work was partly funded by Swiss NSF grant
CRSI22 127510 and European Community FP7
grant 216594 (CLASSiC, www.classic-project.org).
</bodyText>
<sectionHeader confidence="0.999312" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898376811594">
Sharon A. Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing, Computational Linguistics, 24:275-298.
J. C. Chappelier and M. Rajman and R. Arages and A.
Rozenknop. 1999. Lattice Parsing for Speech Recog-
nition. In Proceedings of TALN 1999, Cargse, France.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201-228,
2007.
Anna Corazza, Renato De Mori, Roberto Gretter and
Giorgio Satta. 1994. Optimal Probabilistic Evalu-
ation Functions for Search Controlled by Stochastic
Context-Free Grammars. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 16(10):1018-
1027.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning framework for
finite-state and context-free translation models. In
Proceedings of the Conference of the Association of
Computational Linguistics 2010, Uppsala, Sweden.
Andrea Gesmundo and James Henderson 2010. Faster
Cube Pruning. Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), Paris, France.
Andrea Gesmundo 2011. Bidirectional Sequence Classi-
fication for Tagging Tasks with Guided Learning. Pro-
ceedings of TALN 2011, Montpellier, France.
Mark Hopkins and Greg Langmead 2009. Cube prun-
ing as heuristic search. Proceedings of the Conference
on Empirical Methods in Natural Language Processing
2009, Singapore.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the Conference of the Association of
Computational Linguistics 2007, Prague, Czech Re-
public.
Dan Klein and Christopher D. Manning. 2001 Pars-
ing and Hypergraphs, In Proceedings of the Interna-
tional Workshop on Parsing Technologies 2001, Bei-
jing, China.
Dan Klein and Christopher D. Manning. 2003 A* Pars-
ing: Fast Exact Viterbi Parse Selection, In Proceed-
ings of the Conference of the North American Associ-
ation for Computational Linguistics 2003, Edmonton,
Canada.
Daphne Koller and Nir Friedman. 2010. Probabilistic
Graphical Models: Principles and Techniques. The
MIT Press, Cambridge, Massachusetts.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient Minimum Error Rate
Training and Minimum Bayes-Risk decoding for
translation hypergraphs and lattices, In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, Suntec,
Singapore.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An Open Source Toolkit for
Parsing-based Machine Translation. In Proceedings of
the Workshop on Statistical Machine Translation 2009,
Athens, Greece.
Adam Lopez. 2007. Hierarchical Phrase-Based Transla-
tion with Suffix Arrays. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing 2007, Prague, Czech Republic.
</reference>
<page confidence="0.978888">
907
</page>
<reference confidence="0.999156235294118">
Haitao Mi, Liang Huang and Qun Liu. 2008. Forest-
Based Translation. In Proceedings of the Conference
of the Association of Computational Linguistics 2008,
Columbus, OH.
Libin Shen, Giorgio Satta and Aravind Joshi. 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of the Conference of the As-
sociation of Computational Linguistics 2007, Prague,
Czech Republic.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
2002, Denver, CO.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing,
Proceedings of the Workshop on Statistical Machine
Translation, New York City, New York.
</reference>
<page confidence="0.997469">
908
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.410569">
<title confidence="0.999654">Heuristic Search for Non-Bottom-Up Tree Structure Prediction</title>
<author confidence="0.99918">Andrea</author>
<affiliation confidence="0.99959">Department of Computer University of</affiliation>
<title confidence="0.497648">andrea.gesmundo@unige.ch</title>
<author confidence="0.968751">James</author>
<affiliation confidence="0.999506">Department of Computer University of</affiliation>
<email confidence="0.838442">james.henderson@unige.ch</email>
<abstract confidence="0.99950485">State of the art Tree Structures Prediction techniques rely on bottom-up decoding. These approaches allow the use of context-free features and bottom-up features. We discuss the limitations of mainstream techniques in solving common Natural Language Processing tasks. Then we devise a new framework that goes beyond Bottom-up Decoding, and that allows a better integration of contextual features. Furthermore we design a system that addresses these issues and we test it on Hierarchical Machine Translation, a well known tree structure prediction problem. The structure of the proposed system allows the incorporation of non-bottom-up features and relies on a more sophisticated decoding approach. We show that the proposed approach can find better translations using a smaller portion of the search space.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
<author>Eugene Charniak</author>
</authors>
<title>New figures of merit for best-first probabilistic chart parsing,</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--275</pages>
<contexts>
<context position="6970" citStr="Caraballo and Charniak (1998)" startWordPosition="1116" endWordPosition="1119">he carry, K. In HMT it is possible to estimate the cost of n-grams that partially overlap t’s span considering the boundary words. We can obtain the heuristic cost for an item, t, adding to formula (3) the factor, est(K), for the estimation of interaction with missing context: heuristicCost(t) = cost(t) + est(K) (4) And use heuristicCost(t) to guide BD pruning decisions. Anyway, even if a good interaction estimation is available, in practice it is not possible to avoid search errors while pruning. More sophisticated parsing models allow the use of non-bottom-up features within a BD framework. Caraballo and Charniak (1998) present best-first parsing with Figures of Merit that allows conditioning of the heuristic function on statistics of the input string. Corazza et al. (1994), and Klein and Manning (2003) propose an A* parsing algorithm that estimates the upper bound of the parse completion scores using contextual summary features. These models achieve time efficiency and state-of-the-art accuracy for PCFG parsing, but still use a BD framework that doesn’t allow the application of a broader class of non-bottom-up contextual features. In HMT, knowing the sentence-wide context in which a sub-phrase is translated</context>
</contexts>
<marker>Caraballo, Charniak, 1998</marker>
<rawString>Sharon A. Caraballo and Eugene Charniak. 1998. New figures of merit for best-first probabilistic chart parsing, Computational Linguistics, 24:275-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Chappelier</author>
<author>M Rajman</author>
<author>R Arages</author>
<author>A Rozenknop</author>
</authors>
<title>Lattice Parsing for Speech Recognition.</title>
<date>1999</date>
<booktitle>In Proceedings of TALN</booktitle>
<location>Cargse, France.</location>
<contexts>
<context position="1657" citStr="Chappelier et al., 1999" startWordPosition="236" endWordPosition="239"> portion of the search space. 1 Introduction Tree Structure Prediction (TSP) techniques have become relevant in many Natural Language Processing (NLP) applications, such as Syntactic Parsing, Semantic Role Labeling and Hierarchical Machine Translation (HMT) (Chiang, 2007). HMT approaches have a higher complexity than PhraseBased Machine Translation techniques, but exploit a more sophisticated reordering model, and can produce translations with higher Syntactic-Semantic quality. TSP requires as inputs: a weighted grammar, G, and a sequence of symbols or a set of sequences encoded as a Lattice (Chappelier et al., 1999). The input sequence is often a sentence for NLP applications. Tree structures generating the input sequence can be composed using rules, r, from the weighted grammar, G. TSP techniques return as output a tree structure or a set of trees (forest) that generate the input string or lattice. The output forest can be represented compactly as a weighted hypergraph (Klein and Manning, 2001). TSP tasks require finding the tree, t, with the highest score, or the best-k such trees. Mainstream TSP relies on Bottom-up Decoding (BD) techniques. With this paper we propose a new framework as a generalizatio</context>
</contexts>
<marker>Chappelier, Rajman, Arages, Rozenknop, 1999</marker>
<rawString>J. C. Chappelier and M. Rajman and R. Arages and A. Rozenknop. 1999. Lattice Parsing for Speech Recognition. In Proceedings of TALN 1999, Cargse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--2</pages>
<contexts>
<context position="1305" citStr="Chiang, 2007" startWordPosition="184" endWordPosition="185">resses these issues and we test it on Hierarchical Machine Translation, a well known tree structure prediction problem. The structure of the proposed system allows the incorporation of non-bottom-up features and relies on a more sophisticated decoding approach. We show that the proposed approach can find better translations using a smaller portion of the search space. 1 Introduction Tree Structure Prediction (TSP) techniques have become relevant in many Natural Language Processing (NLP) applications, such as Syntactic Parsing, Semantic Role Labeling and Hierarchical Machine Translation (HMT) (Chiang, 2007). HMT approaches have a higher complexity than PhraseBased Machine Translation techniques, but exploit a more sophisticated reordering model, and can produce translations with higher Syntactic-Semantic quality. TSP requires as inputs: a weighted grammar, G, and a sequence of symbols or a set of sequences encoded as a Lattice (Chappelier et al., 1999). The input sequence is often a sentence for NLP applications. Tree structures generating the input sequence can be composed using rules, r, from the weighted grammar, G. TSP techniques return as output a tree structure or a set of trees (forest) t</context>
<context position="3627" citStr="Chiang (2007)" startWordPosition="553" endWordPosition="554"> Methods in Natural Language Processing, pages 899–908, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics For example, in HMT the Language Model (LM) is a non-local fundamental feature that approximates the adequacy of the translation with the sum of logprobabilities of composing n-grams. CKY-like BD approaches build candidate trees in a bottom-up fashion, allowing the use of Dynamic Programming techniques to simplify the search space by mering sub-trees with the same state, and also easing application of pruning techniques (such as Cube Pruning, e.g. Chiang (2007), Gesmundo (2010)). For clarity of presentation and following HMT practice, we will henceforth restrict our focus to binary grammars. Standard CKY works by building objects known as items (Hopkins and Langmead, 2009). Each item, t, corresponds to a candidate sub-tree. Items are built linking a rule instantiation, r, to two sub-items that represents left context, t1, and right context, t2; formally: t - ( t1 &gt; r &lt; t2 ). An item is a triple that contains a span, a postcondition and a carry. The span contains the indexes of the starting and ending input words delimiting the continuous sequence co</context>
<context position="20036" citStr="Chiang (2007)" startWordPosition="3398" endWordPosition="3399">a (5). The factor est(˚�) accounts for the estimation of the missing part of the derivation. We compute this factor with the following formula: ( ) localCost(˚�, l) + contextEst(˚�, l) (7) For each missing link, l E L−ι˚ , we estimate the cost of the corresponding derivation branch with two factors: localCost(˚�, l) that computes the context-free score of the branch with highest score that could be attached to l; and contextEst(˚�, l) that estimates the contextual score of the branch and its interaction with˚�. Because our model is implemented in the Forest Rescoring framework (e.g. Huang and Chiang (2007), Dyer et al. (2010), Li et al. (2009)), localCost(˚�, l) can be efficiently computed exactly. In HMT it is possible to exhaustively represent and search the context-free-forest (ignoring the LM), which is done in the Forest Rescoring framework before our task of decoding with the LM. We exploit this context-free-forest to compute localCost(˚�, l): for missing child links the localCost(·) is the Inside score computed using the (max, +) semiring (also known as the Viterbi score), and for missing parent links the localCost(·) is the corresponding Outside score. The factor contextEst(·) estimates</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201-228, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Corazza</author>
<author>Renato De Mori</author>
<author>Roberto Gretter</author>
<author>Giorgio Satta</author>
</authors>
<title>Optimal Probabilistic Evaluation Functions for Search Controlled by Stochastic Context-Free Grammars.</title>
<date>1994</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>16--10</pages>
<marker>Corazza, De Mori, Gretter, Satta, 1994</marker>
<rawString>Anna Corazza, Renato De Mori, Roberto Gretter and Giorgio Satta. 1994. Optimal Probabilistic Evaluation Functions for Search Controlled by Stochastic Context-Free Grammars. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(10):1018-1027.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A Decoder, Alignment, and Learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference of the Association of Computational Linguistics</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="20056" citStr="Dyer et al. (2010)" startWordPosition="3400" endWordPosition="3403">or est(˚�) accounts for the estimation of the missing part of the derivation. We compute this factor with the following formula: ( ) localCost(˚�, l) + contextEst(˚�, l) (7) For each missing link, l E L−ι˚ , we estimate the cost of the corresponding derivation branch with two factors: localCost(˚�, l) that computes the context-free score of the branch with highest score that could be attached to l; and contextEst(˚�, l) that estimates the contextual score of the branch and its interaction with˚�. Because our model is implemented in the Forest Rescoring framework (e.g. Huang and Chiang (2007), Dyer et al. (2010), Li et al. (2009)), localCost(˚�, l) can be efficiently computed exactly. In HMT it is possible to exhaustively represent and search the context-free-forest (ignoring the LM), which is done in the Forest Rescoring framework before our task of decoding with the LM. We exploit this context-free-forest to compute localCost(˚�, l): for missing child links the localCost(·) is the Inside score computed using the (max, +) semiring (also known as the Viterbi score), and for missing parent links the localCost(·) is the corresponding Outside score. The factor contextEst(·) estimates the LM score of the</context>
<context position="22427" citStr="Dyer et al., 2010" startWordPosition="3802" endWordPosition="3805">r from the cost(·) function when inserting the candidate into the out forest. And for the candidates that are scored with a missing child, we adjust the score once the link to the missing child is created in the out-forest. In this way UD and BD score the same derivation with the same score and can be regarded as two ways to explore the same search space. 5 Experiments In this section we test the algorithm presented, and empirically show that it produces better translations searching a smaller portion of the search space. We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al., 2010). We compare with cdec Cube Pruning BD. The ex�est(˚�) = l∈L−ι˚ 904 Search Score 2 4 6 8 10 12 14 16 Beam Size Figure 2: Comparison of the quality of the translations. 2 4 6 8 10 12 14 16 Beam Size Figure 3: Search score evolution for BD and UD. UD best score BD best score -123 -123.5 -124 -124.5 -125 -125.5 -126 -126.5 Bottom-up Decoding Guided Decoding Test Set 900 800 700 600 500 400 300 200 100 0 periments are executed on the NIST MT03 ChineseEnglish parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. We use a hierarchical phras</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A Decoder, Alignment, and Learning framework for finite-state and context-free translation models. In Proceedings of the Conference of the Association of Computational Linguistics 2010, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
</authors>
<title>Faster Cube Pruning.</title>
<date>2010</date>
<booktitle>Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>Paris, France.</location>
<marker>Gesmundo, Henderson, 2010</marker>
<rawString>Andrea Gesmundo and James Henderson 2010. Faster Cube Pruning. Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
</authors>
<title>Bidirectional Sequence Classification for Tagging Tasks with Guided Learning.</title>
<date>2011</date>
<booktitle>Proceedings of TALN 2011,</booktitle>
<location>Montpellier, France.</location>
<contexts>
<context position="29212" citStr="Gesmundo (2011)" startWordPosition="5011" endWordPosition="5012">ttom-up Decoding, beam = 16 Undirected Decoding, beam = 8 0 10 15 20 25 30 35 40 45 50 Input Sentence Size Time (ms) BLEU Score 32.4 32.2 32 31.8 31.6 31.4 31.2 906 Hypergraph can be regarded as a non-directed graph. In this setting we could imagine applying message passing algorithms from graphical model theory (Koller and Friedman, 2010). Furthermore, considering that the proposed framework lets the system decide the decoding order, we could design a system that explicitly learns to infer the decoding order at training time. Similar ideas have been successfully tried: Shen et al. (2010) and Gesmundo (2011) investigate the Guided Learning framework, that dynamically incorporates the tasks of learning the order of inference and training the local classifier. 7 Conclusion With this paper we investigate the limitations of Bottom-up parsing techniques, widely used in Tree Structures Prediction, focusing on Hierarchical Machine Translation. We devise a framework that allows a better integration of non-bottom-up features. Compared to a state of the art HMT decoder the presented system produces higher quality translations searching a smaller portion of the search space, empirically showing that the bot</context>
</contexts>
<marker>Gesmundo, 2011</marker>
<rawString>Andrea Gesmundo 2011. Bidirectional Sequence Classification for Tagging Tasks with Guided Learning. Proceedings of TALN 2011, Montpellier, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Greg Langmead</author>
</authors>
<title>Cube pruning as heuristic search.</title>
<date>2009</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing</booktitle>
<contexts>
<context position="3843" citStr="Hopkins and Langmead, 2009" startWordPosition="584" endWordPosition="587">ocal fundamental feature that approximates the adequacy of the translation with the sum of logprobabilities of composing n-grams. CKY-like BD approaches build candidate trees in a bottom-up fashion, allowing the use of Dynamic Programming techniques to simplify the search space by mering sub-trees with the same state, and also easing application of pruning techniques (such as Cube Pruning, e.g. Chiang (2007), Gesmundo (2010)). For clarity of presentation and following HMT practice, we will henceforth restrict our focus to binary grammars. Standard CKY works by building objects known as items (Hopkins and Langmead, 2009). Each item, t, corresponds to a candidate sub-tree. Items are built linking a rule instantiation, r, to two sub-items that represents left context, t1, and right context, t2; formally: t - ( t1 &gt; r &lt; t2 ). An item is a triple that contains a span, a postcondition and a carry. The span contains the indexes of the starting and ending input words delimiting the continuous sequence covered by the sub-tree represented by the item. The postcondition is a string that represents r’s head nonterminal label, telling us which rules may be applied. The carry, K, stores extra information required to corre</context>
</contexts>
<marker>Hopkins, Langmead, 2009</marker>
<rawString>Mark Hopkins and Greg Langmead 2009. Cube pruning as heuristic search. Proceedings of the Conference on Empirical Methods in Natural Language Processing 2009, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the Association of Computational Linguistics 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="20036" citStr="Huang and Chiang (2007)" startWordPosition="3396" endWordPosition="3399"> to formula (5). The factor est(˚�) accounts for the estimation of the missing part of the derivation. We compute this factor with the following formula: ( ) localCost(˚�, l) + contextEst(˚�, l) (7) For each missing link, l E L−ι˚ , we estimate the cost of the corresponding derivation branch with two factors: localCost(˚�, l) that computes the context-free score of the branch with highest score that could be attached to l; and contextEst(˚�, l) that estimates the contextual score of the branch and its interaction with˚�. Because our model is implemented in the Forest Rescoring framework (e.g. Huang and Chiang (2007), Dyer et al. (2010), Li et al. (2009)), localCost(˚�, l) can be efficiently computed exactly. In HMT it is possible to exhaustively represent and search the context-free-forest (ignoring the LM), which is done in the Forest Rescoring framework before our task of decoding with the LM. We exploit this context-free-forest to compute localCost(˚�, l): for missing child links the localCost(·) is the Inside score computed using the (max, +) semiring (also known as the Viterbi score), and for missing parent links the localCost(·) is the corresponding Outside score. The factor contextEst(·) estimates</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the Conference of the Association of Computational Linguistics 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and Hypergraphs,</title>
<date>2001</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2044" citStr="Klein and Manning, 2001" startWordPosition="301" endWordPosition="304">sticated reordering model, and can produce translations with higher Syntactic-Semantic quality. TSP requires as inputs: a weighted grammar, G, and a sequence of symbols or a set of sequences encoded as a Lattice (Chappelier et al., 1999). The input sequence is often a sentence for NLP applications. Tree structures generating the input sequence can be composed using rules, r, from the weighted grammar, G. TSP techniques return as output a tree structure or a set of trees (forest) that generate the input string or lattice. The output forest can be represented compactly as a weighted hypergraph (Klein and Manning, 2001). TSP tasks require finding the tree, t, with the highest score, or the best-k such trees. Mainstream TSP relies on Bottom-up Decoding (BD) techniques. With this paper we propose a new framework as a generalization of the CKY-like Bottom-up approach. We also design and test an instantiation of this framework, empirically showing that wider contextual information leads to higher accuracy for TSP tasks that rely on non-local features, like HMT. 2 Beyond Bottom-up Decoding TSP decoding requires scoring candidate trees, cost(t). Some TSP tasks require only local features. For these cases cost(t) d</context>
<context position="13620" citStr="Klein and Manning, 2001" startWordPosition="2298" endWordPosition="2301">g function for undirected-items can be obtained generalizing formula (3): cost(!) = cost(r) + � cost(LI) (5) lEG+ + interaction(r, L+) In CKY, each span is processed separately in topological order, and the best-k items for each span are selected in sequence according to scoring function (4). In the proposed framework, the selection of undirected-items can be done in any order, for example: in a first step selecting an undirecteditem for span s1, then selecting an undirected-item for span s2, and in a third step selecting a second undirected-item for s1, and so on. As in agenda based parsing (Klein and Manning, 2001), all candidate undirected-items can be handled with an unique queue. Allowing the system to decide decoding order based on the candidates’ scores, so that candidates with higher confidence can be selected earlier and used as context for candidates with lower confidence. Having all candidates in the same queue introduces comparability issues. In CKY, candidates are comparable since each span is processed separately and each candidate is scored with the estimation of the yield score. Instead, in the proposed framework, 12 : [0, 1, {X2 }, K2)] 902 the unique queue contains candidates relative to</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001 Parsing and Hypergraphs, In Proceedings of the International Workshop on Parsing Technologies 2001, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* Parsing: Fast Exact Viterbi Parse Selection,</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the North American Association for Computational Linguistics</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="7157" citStr="Klein and Manning (2003)" startWordPosition="1146" endWordPosition="1150">rmula (3) the factor, est(K), for the estimation of interaction with missing context: heuristicCost(t) = cost(t) + est(K) (4) And use heuristicCost(t) to guide BD pruning decisions. Anyway, even if a good interaction estimation is available, in practice it is not possible to avoid search errors while pruning. More sophisticated parsing models allow the use of non-bottom-up features within a BD framework. Caraballo and Charniak (1998) present best-first parsing with Figures of Merit that allows conditioning of the heuristic function on statistics of the input string. Corazza et al. (1994), and Klein and Manning (2003) propose an A* parsing algorithm that estimates the upper bound of the parse completion scores using contextual summary features. These models achieve time efficiency and state-of-the-art accuracy for PCFG parsing, but still use a BD framework that doesn’t allow the application of a broader class of non-bottom-up contextual features. In HMT, knowing the sentence-wide context in which a sub-phrase is translated is extremely important. It is obviously important for word choice: as 900 a simple example consider the translation of the frequent English word “get” into Chinese. The choice of the cor</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003 A* Parsing: Fast Exact Viterbi Parse Selection, In Proceedings of the Conference of the North American Association for Computational Linguistics 2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>Nir Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2010</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="28938" citStr="Koller and Friedman, 2010" startWordPosition="4965" endWordPosition="4968"> treated differently from links to child nodes, the information in the hypergraph can be propagated in any direction. Then the Derivation Bottom-up Decoding Undirected Decoding BD beam 30 Reduction (%) 40 35 30 25 20 15 10 5 Nodes Reduction Edges Reduction 1200 1000 800 600 400 200 Bottom-up Decoding, beam = 16 Undirected Decoding, beam = 8 0 10 15 20 25 30 35 40 45 50 Input Sentence Size Time (ms) BLEU Score 32.4 32.2 32 31.8 31.6 31.4 31.2 906 Hypergraph can be regarded as a non-directed graph. In this setting we could imagine applying message passing algorithms from graphical model theory (Koller and Friedman, 2010). Furthermore, considering that the proposed framework lets the system decide the decoding order, we could design a system that explicitly learns to infer the decoding order at training time. Similar ideas have been successfully tried: Shen et al. (2010) and Gesmundo (2011) investigate the Guided Learning framework, that dynamically incorporates the tasks of learning the order of inference and training the local classifier. 7 Conclusion With this paper we investigate the limitations of Bottom-up parsing techniques, widely used in Tree Structures Prediction, focusing on Hierarchical Machine Tra</context>
</contexts>
<marker>Koller, Friedman, 2010</marker>
<rawString>Daphne Koller and Nir Friedman. 2010. Probabilistic Graphical Models: Principles and Techniques. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient Minimum Error Rate Training and Minimum Bayes-Risk decoding for translation hypergraphs and lattices,</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<location>Suntec, Singapore.</location>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient Minimum Error Rate Training and Minimum Bayes-Risk decoding for translation hypergraphs and lattices, In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: An Open Source Toolkit for Parsing-based Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="20074" citStr="Li et al. (2009)" startWordPosition="3404" endWordPosition="3407">for the estimation of the missing part of the derivation. We compute this factor with the following formula: ( ) localCost(˚�, l) + contextEst(˚�, l) (7) For each missing link, l E L−ι˚ , we estimate the cost of the corresponding derivation branch with two factors: localCost(˚�, l) that computes the context-free score of the branch with highest score that could be attached to l; and contextEst(˚�, l) that estimates the contextual score of the branch and its interaction with˚�. Because our model is implemented in the Forest Rescoring framework (e.g. Huang and Chiang (2007), Dyer et al. (2010), Li et al. (2009)), localCost(˚�, l) can be efficiently computed exactly. In HMT it is possible to exhaustively represent and search the context-free-forest (ignoring the LM), which is done in the Forest Rescoring framework before our task of decoding with the LM. We exploit this context-free-forest to compute localCost(˚�, l): for missing child links the localCost(·) is the Inside score computed using the (max, +) semiring (also known as the Viterbi score), and for missing parent links the localCost(·) is the corresponding Outside score. The factor contextEst(·) estimates the LM score of the words generated b</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: An Open Source Toolkit for Parsing-based Machine Translation. In Proceedings of the Workshop on Statistical Machine Translation 2009, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical Phrase-Based Translation with Suffix Arrays.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="23114" citStr="Lopez, 2007" startWordPosition="3930" endWordPosition="3931">Score 2 4 6 8 10 12 14 16 Beam Size Figure 2: Comparison of the quality of the translations. 2 4 6 8 10 12 14 16 Beam Size Figure 3: Search score evolution for BD and UD. UD best score BD best score -123 -123.5 -124 -124.5 -125 -125.5 -126 -126.5 Bottom-up Decoding Guided Decoding Test Set 900 800 700 600 500 400 300 200 100 0 periments are executed on the NIST MT03 ChineseEnglish parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. We use a hierarchical phrase-based translation grammar extracted using a suffix array rule extractor (Lopez, 2007). The NIST03 test set is used for decoding, it has 919 sentence pairs. The experiments can be reproduced on an average desktop computer. Since we compare two different decoding strategies that rely on the same training technique, the evaluation is primarily based on search errors rather than on BLEU. We compare the two systems on a variety of beam sizes between 1 and 16. Figure 2 reports a comparison of the translation quality for the two systems in relation to the beam size. The blue area represents the portion of sentences for which UD found a better translation. The white area represents th</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical Phrase-Based Translation with Suffix Arrays. In Proceedings of the Conference on Empirical Methods in Natural Language Processing 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>ForestBased Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference of the Association of Computational Linguistics</booktitle>
<location>Columbus, OH.</location>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang and Qun Liu. 2008. ForestBased Translation. In Proceedings of the Conference of the Association of Computational Linguistics 2008, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind Joshi</author>
</authors>
<title>Guided Learning for Bidirectional Sequence Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the Association of Computational Linguistics 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta and Aravind Joshi. 2007. Guided Learning for Bidirectional Sequence Classification. In Proceedings of the Conference of the Association of Computational Linguistics 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing</booktitle>
<location>Denver, CO.</location>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing 2002, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing,</title>
<date>2006</date>
<booktitle>Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<location>New York City, New York.</location>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing, Proceedings of the Workshop on Statistical Machine Translation, New York City, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>