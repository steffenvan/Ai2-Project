<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000206">
<title confidence="0.992845">
Combining Collocations, Lexical and Encyclopedic Knowledge
for Metonymy Resolution
</title>
<author confidence="0.915868">
Vivi Nastase and Michael Strube
</author>
<affiliation confidence="0.739566">
EML Research gGmbH
</affiliation>
<address confidence="0.753589">
Heidelberg, Germany
</address>
<email confidence="0.945935">
http://www.eml-research.de/nlp
</email>
<sectionHeader confidence="0.996838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999780571428572">
This paper presents a supervised method
for resolving metonymies. We enhance
a commonly used feature set with fea-
tures extracted based on collocation in-
formation from corpora, generalized us-
ing lexical and encyclopedic knowledge
to determine the preferred sense of the
potentially metonymic word using meth-
ods from unsupervised word sense disam-
biguation. The methodology developed
addresses one issue related to metonymy
resolution – the influence of local context.
The method developed is applied to the
metonymy resolution task from SemEval
2007. The results obtained, higher for the
countries subtask, on a par for the compa-
nies subtask – compared to participating
systems – confirm that lexical, encyclo-
pedic and collocation information can be
successfully combined for metonymy res-
olution.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972771929825">
Metonymies are a pervasive phenomenon in
language. They occur because in communicating,
we use words as pointers to a larger body of
knowledge, that encompasses various facets of the
concept evoked by a given word.
A listener need not understand the cello to
be moved by its playing, just as it is unnecessary
for a rider to understand technical jargon; all
that matters is sensation, and here the Kawasaki
excels. The cockpit is sensibly designed, with a
narrow front seat portion ...
Kawasaki is a company, it has an organization, fa-
cilities, employees, it makes specific products. In
the context above, the company name stands in
for its products – motorcycles. Motorcycles have
parts, the cockpit and front seat are some of them,
and this provides the discourse links between the
two sentences. Constraints on the interpretation
of a word w in context comes both from the local
and global context, and are applied to the infor-
mation/knowledge evoked by w. The local con-
straints come from the words with which w is
(grammatically) related to. The global constraints
come from the domain/topic of the text, discourse
relations that span across sentences.
Metonymic words have a rather small num-
ber of possible interpretations (also called read-
ings) which occur frequently (Markert and Nissim,
2002). Idiosyncratic interpretations are also pos-
sible, but very rare. One can view the possible
interpretations of a potentially metonymic word
(PMW) as corresponding to the word’s possible
senses (Nissim and Markert, 2003), bringing the
task close to word sense disambiguation.
The approach to metonymy resolution pre-
sented here is supervised, with unsupervised fea-
ture enrichment. We apply techniques inspired by
unsupervised word sense disambiguation, which
allow us to go beyond the annotated data provided
in training, and quantify the restrictions imposed
on the interpretation of a PMW by its grammat-
ically related neighbours through collocation in-
formation extracted from corpora. The only anno-
tation required for the corpora are automatically
induced part-of-speech tags from which we ob-
tain grammatical relations through regular expres-
sion matching over sequences of parts-of-speech.
Collocation information is combined with lexical
resources – WordNet – and encyclopedic knowl-
edge extracted from Wikipedia to help us gener-
alize the collocations found to determine higher
level constraints on a word’s grammatical collo-
cates. In the example above, Kawasaki is gram-
matically related to the verb excel – it is its sub-
ject. To determine the most likely interpretation
of Kawasaki given that it is in the subject relation
with excel we look at all the nouns in the corpora
</bodyText>
<page confidence="0.954783">
910
</page>
<note confidence="0.996598">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 910–918,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99989608">
that appear as this verb’s subjects, and estimate
from this the preferences excel has for its subjects.
Let us say the corpus contains the following col-
locations in subject position (with frequency in-
formation in parentheses): player (4), musician
(50), car (30), computer (12), camera (40), driver
(55), bike (20) .... The knowledge resources –
WordNet, isa relations extracted from Wikipedia
– will help generalize these collocations: player,
musician, driver to person and car, computer,
camera, bike to artifact. This together with
frequency of occurrence are used to estimate the
probability that the verb excel takes a person or
artifact-type subject. These are excel’s se-
lectional preferences towards certain collocates,
and will help determine which possible interpre-
tation for the PMW Kawasaki is appropriate in
this context – organization-for-people
ororganization-for-product.
The paper continues with related work in Sec-
tion 2 and the description of the data in Section 3.
The representation used is introduced in Section
4. The results and the discussion are presented in
Section 5. The paper wraps up with conclusions
and future work.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999843151898734">
Analysis of metonymies as a linguistic phe-
nomenon dates back at least to the 1930s (Stern,
1931), and are increasingly recognized as an im-
portant phenomenon to tackle in the interest of
higher level language processing tasks, such as
anaphora resolution (Harabagiu, 1998; Markert
and Hahn, 2002), question answering (Stallard,
1993) or machine translation (Kamei and Wakao,
1992).
Until the early 90s, the main view about
metonymies was that they violate semantic con-
straints in their immediate context. To resolve
metonymies then amounts to detecting violated
constraints, usually from those imposed by the
verbs on their arguments (Pustejovsky, 1991;
Hobbs et al., 1993; Fass, 1991). Markert and
Hahn (2002) showed that this approach misses
metonymies which do not violate selectional re-
strictions. In this case referential cohesion re-
lations may indicate that the literal reading is
not appropriate and give clues about the intended
metonymic interpretation.
Markert and Nissim (2003) have combined
observations from the linguistic analysis of
metonymies with results of corpus studies. Lin-
guistic research has postulated that (i) conven-
tional metonymic readings are very systematic;
(ii) unconventional metonymies can be created on
the fly and their interpretation is context depen-
dent; (iii) metonymies are frequent. The fact
that most metonymic interpretations are system-
atic and correspond to a small set of possible read-
ings allow the metonymy resolution to be mod-
elled as a classifier learning task. Markert and Nis-
sim (2002) and Nissim and Markert (2003) have
shown that conventional metonymies can be effec-
tively resolved using a supervised machine learn-
ing approach. Moreover, grammatically related
words are crucial in determining the interpretation
of a PMW. The shortcoming is that manually an-
notated data is in short supply, and the approach
suffers from data sparseness. To address this prob-
lem, Nissim and Markert (2003) proposed a word
similarity-based method. They use Lin’s thesaurus
(Lin, 1998) to determine how close two lexical
heads are, and use this instead of the more re-
strictive identity constraint when comparing two
instances. This technique is complex, requiring
smoothing, multiple iterations over the thesaurus
and hybrid methods to allow a back-off to gram-
matical roles.
The supervised approach to resolving
metonymies was encouraged by the metonymy
resolution task at the semantic evaluation exercise
SemEval 2007 (Markert and Nissim, 2007). The
participating systems in this task were varied.
Most of them (four out of five) have used super-
vised machine learning techniques. The systems
that beat the baseline used either the grammatical
annotations provided by the organizers (Farkas
et al., 2007; Nicolae et al., 2007), or a robust
and deep (not freely available) parser (Brun et
al., 2007). These systems represented instances
in a manner similar to (Nissim and Markert,
2005). They used additional manually built
resources – WordNet, FrameNet, Levin’s verb
classes, manually built lists of “trigger” words
– to generalize the existing features. Brun et
al. (2007) also used the British National Corpus
(BNC) for computing the distance between words
based on their syntactic distribution.
While lexical resources and corpora are used
to estimate word similarity, all these systems rely
exclusively on the data provided by the organiz-
ers – instance representation captures only infor-
mation that can be derived from or between the
data points provided. The approach presented here
goes beyond the given data, and induces from cor-
pora measures that allow the system to determine
</bodyText>
<page confidence="0.996726">
911
</page>
<bodyText confidence="0.999962933333334">
what are the preferences of the words surround-
ing a PMW towards each of PMW’s possible read-
ings. The technique employed is adapted from
unsupervised word sense disambiguation (WSD).
In short, we use the local grammatical context
as it is commonly used in WSD approaches, to
guide the system in choosing the reading that fits
best. The benefits of using grammatical informa-
tion for automatic WSD were first explored by
Yarowsky (1995) and Resnik (1996) in unsuper-
vised approaches to disambiguating single words
in context. The method described here uses au-
tomatically induced selectional preferences, com-
puted from sense-untagged data, similar to Nas-
tase (2008).
</bodyText>
<sectionHeader confidence="0.998519" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.99976978125">
We work with the data from the metonymy reso-
lution task at SemEval 2007 (Markert and Nissim,
2007), generated based on a scheme developed by
Markert and Nissim (2003).
The metonymy resolution task at SemEval 2007
consisted of two subtasks – one for resolving
country names, the other for companies. For each
subtask there is a training and a test portion. Fig-
ure 1 shows the text fragment for one sample,
and Table 1 the data statistics. The reading col-
umn shows the possible interpretations of a PMW
for countries and companies respectively. For ex-
ample, org-for-product would be the inter-
pretation of the PMW Kawasaki in the example
shown in the introduction.
Occurrences of country and company names
were annotated with a small number of possi-
ble readings, as shown in Table 1. This reflects
previous analyses of the metonymy phenomenon,
which showed that there is a rather small number
of possible interpretations that appear more fre-
quently (Markert and Nissim, 2002). Special in-
terpretations are very rarely encountered.
Within the framework of the SemEval task,
metonymy resolution is evaluated on the
given test data, on three levels of granular-
ity: coarse – distinguish between literal and
non-literal readings; medium – distinguish
between literal, mixed and non-literal
readings; fine – identify the specific reading of the
target word/words (potentially metonymic word -
PMW).
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="method">
4 Representation
</sectionHeader>
<bodyText confidence="0.993927608695652">
The method presented in this paper is a supervised
learning method, along the same general lines as
reading train test
locations
literal
mixed
othermet
obj-for-name
obj-for-representation
place-for-people
place-for-event
place-for-product
organizations
literal
mixed
othermet
obj-for-name
obj-for-representation
org-for-members
org-for-event
org-for-product
org-for-facility
org-for-index
</bodyText>
<tableCaption confidence="0.990349">
Table 1: Reading distributions
</tableCaption>
<bodyText confidence="0.999990117647059">
the systems which participated in the SemEval
competition. As such, it represents each PMW in
the data through features that describe its context
and some semantic characteristics. The minimum
set of necessary features is taken to be that pre-
sented by Nissim and Markert (2005), and proved
to be effective in solving metonymies. These
are the M&amp;N features (Markert and Nissim fea-
tures). We expand on these features and estimate
preferences from words in a PMW’s context to-
wards specific PMW interpretations. These con-
stitute the selectional preference features. Finally,
Wikipedia is a source of facts which can be used
to derive information that can bias the decision to-
wards certain interpretations for a PMW. Each of
these features are described in more detail in the
following subsections.
</bodyText>
<subsectionHeader confidence="0.94687">
4.1 M&amp;N features
</subsectionHeader>
<bodyText confidence="0.991675">
The features used by Nissim and Markert (2005)
are:
</bodyText>
<listItem confidence="0.9981886">
• grammatical role of PMW (subj, obj, ...);
• lemmatized head/modifier of PMW (an-
nounce, say, ...);
• determiner of PMW (def, indef, bare,
demonst, other, ...);
</listItem>
<figure confidence="0.981239407407407">
925 908
737 721
15 20
9 11
0 4
0 0
161 141
3 10
0 1
1090 842
690 520
59 60
14 8
8 6
1 0
220 161
2 1
74 67
15 16
7 3
912
XML tagged text
&lt;sample id=”samp114”&gt;
&lt;bnc:title&gt; Computergram international
&lt;/bnc:title&gt;
&lt;par&gt;
LITTLE FEAR OF MICHELANGELO
</figure>
<bodyText confidence="0.9653107">
The computer industry equivalent of “Small
earthquake in Chile” ...
The Michelangelo computer virus that received
worldwide attention last year is expected to cause
even fewer problems this Saturday than it did
when it struck last year, a team of &lt;annot&gt;&lt;org
reading=”literal”&gt; IBM &lt;/org&gt;&lt;/annot&gt; re-
searchers said.
&lt;/par&gt;
&lt;/sample&gt;
</bodyText>
<subsectionHeader confidence="0.724176">
Grammatical annotations
</subsectionHeader>
<bodyText confidence="0.910237857142857">
SampleID|Lemma|PMW|GrRole|Reading
samp114|researcher|IBM|premod|literal
samp4|be|Williams Holdings|subj|literal
samp5|parent|Fujitsu Ltd|app|mixed
samp5|have|Fujitsu Ltd|subj|mixed
samp5|keep|Fujitsu Ltd|subj|mixed
samp8|against|IBM|pp|literal
</bodyText>
<subsectionHeader confidence="0.429078">
POS tags
</subsectionHeader>
<bodyText confidence="0.911571454545455">
&lt;bnc:s id=”samp114-bncCNJ-s341”&gt; ...
&lt;bnc:w id=”samp114-bncCNJ-s343-w29”
bnc:type=”NN0”&gt; team &lt;/bnc:w&gt; &lt;bnc:w
id=”samp114-bncCNJ-s343-w30”
bnc:type=”PRF”&gt; of &lt;/bnc:w&gt; &lt;annot&gt; &lt;org
reading=”literal”&gt; &lt;bnc:w possmeto=”yes”
id=”samp114-bncCNJ-s343-w31”
bnc:type=”NP0”&gt; IBM &lt;/bnc:w&gt; &lt;/org&gt;
&lt;/annot&gt; &lt;bnc:w
id=”samp114-bncCNJ-s343-w32”
bnc:type=”NN2”&gt; researchers &lt;/bnc:w&gt; ...
</bodyText>
<figureCaption confidence="0.977018">
Figure 1: Sample annotation
</figureCaption>
<listItem confidence="0.901495571428571">
• grammatical number of PMW (sg, pl);
• number of grammatical roles in which the
PMW appears in its current context;
• number of words in PMW;
All these features can be extracted from the
grammatically annotated and POS tagged data
provided by the organizers.
</listItem>
<subsectionHeader confidence="0.993552">
4.2 Selectional preference features
</subsectionHeader>
<bodyText confidence="0.986119604166667">
The grammatical relations and the connected
words are important to describe the local context
of the target PMW. Because of the limited amount
of annotated data (a few thousand instances), lem-
mas of PMW’s grammatically related words will
make for very sparse data that a machine learn-
ing system would not be able to generalize over.
Nissim and Markert (2003) and the teams partici-
pating in the metonymy resolution task have then
supplemented their systems with Lin’s thesaurus,
WordNet, Beth Levin’s verb groups, FrameNet in-
formation, or manually designed lists of words to
generalize the grammatically related words and
thus find shared characteristics across instances of
metonymies in text.
The notion of selectional restrictions used in
metonymy resolution – meaning the restrictions
imposed on the interpretation of a PMW by its
context – is similar to the notion of selectional
preferences from word sense disambiguation –
meaning the preferences of a word for the senses
of the words in its context. We import this no-
tion, and compute selectional preferences for the
words in a PMW’s (grammatical) neighbourhood,
and allow them to influence the chosen reading for
the PMW. Applying methods from unsupervised
WSD allow us to estimate such preferences from
(sense/metonymy) untagged corpora.
A potentially metonymic word (or phrase) has
a small number of possible readings. These can
be viewed as possible senses, and the task is to
choose the one that fits best in the given context.
The preference for each possible sense can be
determined based on the PMW’s grammatically
related words. To estimate these sense preferences
we use grammatical collocations extracted from
the British National Corpus (BNC), detected
using regular expression matching over sequences
of POS using the Word Sketch Engine (Kilgarriff
et al., 2004). The scores are computed following
a technique similar to Nastase (2008), which is
illustrated using the following example:
The Kawasaki drives well, steers brilliantly
both under power and in tight corners ...
The PMW Kawasaki is involved in the follow-
ing grammatical relations in the previous sentence:
(drive,subject,Kawasaki)
(steer,subject,Kawasaki)
</bodyText>
<page confidence="0.988933">
913
</page>
<table confidence="0.951681333333333">
SampleID Lemma PMW GrRole Reading act animal artifact ... person ...
samp190 say Sun subj org-for-members 0.00056 0.01171 0.01958 ... 0.61422 ...
samp190 claim Sun subj org-for-members 0.00198 0.00099 0.00893 ... 0.50211 ...
</table>
<tableCaption confidence="0.996366">
Table 2: Grammatical annotation file enhanced with selectional preference estimates
</tableCaption>
<bodyText confidence="0.881635672727273">
The BNC provides the collocations
(drive,subject,l) and (steer,subject,Y), to de-
termine what kind of subject drive and steer
prefer, in “word-POS:frequency” format:
drive subject chauffeur-n:12, engine-
n:30, car-n:62, taxi-n:13,
motorist-n:10, disk-n:15,
truck-n:11, man-n:75, ...
steer subject power-n:6, car-n:3, sport-
n:2, firm-n:2, boy-n:2,
government-n:2, man-n:2,
people-n:2 ...
The target whose interpretation must be deter-
mined is Kawasaki. If for a potentially metonymic
word representing a company name, there are the
following possible interpretations: company,
member/person, product/artifact,
facility, name, we compute the preference
for each of these interpretations based on the
extracted collocations. For the verb drive for
example, the collocations engine, car, taxi, truck
are all artifacts (according to WordNet), and
thus vote for the product/artifact reading,
while chauffeur, motorist, man are all person,
and vote for the member/person reading.
Preferences from different grammatical relation
for the same PMW are summed.
Formally, we choose the PMWs’ “senses” –
a set of words which are close to the possible
readings of metonymic words in the data. In
this work, these senses are the WordNet 3.0
supersenses:
S = { act, animal, artifact,
attribute, body, cognition,
communication, event, feeling,
food, group, location, motive,
object, person, phenomenon,
plant, possession, process,
quantity, relation, shape, state,
substance, time 1.
Because none of these can be seen as a sense
for “company”, the list is supplemented with
company and organization. Granted, there
is no 1:1 mapping from these supersenses to PMW
readings, but find such a strict correspondence is
not necessary because the context preferences for
each of these senses are used as features, and the
mapping to PMW readings is found through a su-
pervised learned model.
To compute the preference of a word w in
the grammatical context of a PMW t (the target)
towards each of t’s possible senses, we consider
each relation (w, R, t), where R is the grammati-
cal relation. The set C of word collocations are
extracted from the BNC
</bodyText>
<equation confidence="0.530026">
C = {(w, R, wj : fj)I(w, R, wj) E BNC,
fjis the frequency of occurrence1
</equation>
<bodyText confidence="0.9855575">
and used to compute a preference score Psi
for each sense si E S:
</bodyText>
<equation confidence="0.994716">
Psi =
E(w,R,wi,j :fi,j)∈Csi fi,j
E(w,R,wj:fj)∈C fj
</equation>
<bodyText confidence="0.646942">
where
</bodyText>
<equation confidence="0.9688365">
Csi = {(w, R, wj : fj)I(w, R, wj : fj) E C;
supersense(wj, si) II isa(wj, si)1.
</equation>
<bodyText confidence="0.998901705882353">
supersense(wj, si) is true if si is a super-
sense of one of wj’s senses;
isa(wj, si) is true if si is a hypernym of one
of wj’s senses in WordNet, or is a fact extracted
from Wikipedia.
To determine the supersense and isa relation we
use WordNet 3.0, and a set of 7,578,112 isa rela-
tions extracted by processing the page and cate-
gory network of Wikipedia1 (Nastase and Strube,
2008). The collocations extracted from BNC con-
tain numerous named entities, most of which are
not part of WordNet. If an isa relation be-
tween a collocate from the corpus wj and a pos-
sible sense of a PMW si cannot be established us-
ing supersense information (for the supersenses)
or through transitive closure in the hypernym-
hyponym hierarchy in WordNet (for company
</bodyText>
<footnote confidence="0.990836">
1http://www/eml-research.de/nlp/
download/wikirelations.php
</footnote>
<page confidence="0.997544">
914
</page>
<bodyText confidence="0.9997745">
and organization) for any sense of wj, it is
tried against the Wikipedia-based links.
This process transforms the grammatical anno-
tation file and enhances it with the collocation es-
timates, as shown in Table 2 (compare this with a
sample of the original file presented in Figure 1).
</bodyText>
<subsectionHeader confidence="0.990571">
4.3 Product and event features
</subsectionHeader>
<bodyText confidence="0.999781071428571">
Farkas et al. (2007) observed that using the PMWs
themselves as features leads to improvement on
determining the reading for organization names,
and postulate that this is because some company
names are more likely to be used in a metonymic
way. This is often the case with companies that
make products which are commonly used (cars,
for example).
Brun et al. (2007) note that certain locations,
such as Vietnam, are more likely to be used with
an event reading than others locations. Generally,
locations strongly associated with events tend to
be used to refer to the event, and more often have
a place-for-event interpretation rather than
a literal one.
These two observations have lead us to mine for
these pieces of information in the Wikipedia rela-
tions, and to add two more features for a target
PMW:
has-product will take a value of 1 if any of the
PMW’s hypernyms (according to the isa re-
lations extracted from Wikipedia) contains
the string manufacturer, will have the value
0 otherwise;
has-event will have the value 1 if any of the
PMW’s hypernyms refers to an event (move-
ments/operations/riots), and value 0 other-
wise.
</bodyText>
<subsectionHeader confidence="0.997764">
4.4 Data representation
</subsectionHeader>
<bodyText confidence="0.983150285714286">
As mentioned before, the representation built can
be seen as consisting of roughly three subsets of
features:
• the M&amp;N features proposed by Nissim and
Markert (2005). To combine the grammati-
cal information from all relations, we trans-
form the grammatical relations into features
(as opposed to values). For a relation subject
for example, we generate a binary subject
feature that indicates whether for a given
target this grammatical relation is filled or
not, and a subject lemma feature, whose
value is the lemma of the grammatically re-
lated word.
</bodyText>
<listItem confidence="0.637209333333333">
• the selectional preference scores. Each of
these features corresponds to one of the ele-
ments of S, presented above. These features
combine the selectional preferences of all the
grammatical relations for one target PMW.
• product and event information from
</listItem>
<bodyText confidence="0.984625705882353">
Wikipedia – has-product and has-event.
The grammatical annotation file consists of one
entry for each grammatical relation in which a
PMW appears. For the final representation, in-
formation about all relations of a given PMW is
compressed into one instance. Because the ba-
sic features were binarized, and instead of having
one grammatical role feature now each possible
grammatical relation has its own feature, combin-
ing several entries for one PMW is easy, as it only
implies setting the correct value for the grammati-
cal relations that are valid in the PMWs context.
The final representation consists of 63 features
+ class feature for the subset for company PMWs,
59 features + class feature for the subset contain-
ing countries PMWs. The sample ID and the
PMW itself were not part of this representation.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999989653846154">
The models for determining a PMW’s correct in-
terpretation are learned on the training data pro-
vided, and evaluated on the test portion, using
the answer keys and evaluation script provided
with the data. For learning the models we use
Weka (Witten and Frank, 2005), and select the
final learning algorithms based on 10-fold cross-
validation on the training data. We have settled on
support vector machines (SMO in Weka), and we
use the learner’s default settings.
Tables 3 and Table 4 show the results obtained,
and the baseline and the best results from the Sem-
Eval task for comparison (Markert and Nissim,
2007). The baseline in Table 3 corresponds to
classifying everything as the most frequent class
– literal interpretation. The M&amp;N feat. and
M&amp;N feat.bin. correspond to datasets that con-
tain only the M&amp;N features and the binarized
versions of these features, respectively. SemEval
best gives the best results obtained on each task
in the SemEval 2007 task (Markert and Nissim,
2007). SMOwiki are the results obtained with the
complete feature set described in Section 4, and
SMOSP are the results obtained when only the
new features are used – only selectional prefer-
ence, has-product and has-event features (none of
</bodyText>
<page confidence="0.995226">
915
</page>
<table confidence="0.999000285714286">
task 1 method -* baseline SemEval best SMOwiki SMOSP M&amp;Nfeat. M&amp;Nfeat.bin.
LOCATION-COARSE 79.4 85.2 86.1 82.8 79.4 83.4
LOCATION-MEDIUM 79.4 84.8 85.9 82.6 79.4 82.3
LOCATION-FINE 79.4 84.1 85.0 82.0 79.4 81.3
ORGANIZATION-COARSE 61.8 76.7 74.9 66.6 73.8 74.0
ORGANIZATION-MEDIUM 61.8 73.3 72.4 65.0 69.8 69.4
ORGANIZATION-FINE 61.8 72.8 71.0 64.7 68.4 68.5
</table>
<tableCaption confidence="0.991096">
Table 3: Accuracy scores
</tableCaption>
<table confidence="0.933488142857143">
task 1 method-* base max SMOwiki SMO
LOCATION-COARSE
literal 79.4 91.2 91.6 91.6
non-literal 20.6 57.6 59.1 58.8
LOCATION-MEDIUM
literal 79.4 91.2 91.6 91.6
metonymic 18.4 58.0 61.5 61.5
mixed 2.2 8.3 16 8.7
LOCATION-FINE
ORGANIZATION-COARSE
literal 61.8 82.5 81.4 81.2
non-literal 38.2 65.2 61.6 60.7
ORGANIZATION-MEDIUM
ORGANIZATION-FINE
</table>
<tableCaption confidence="0.997211">
Table 4: Detailed F-scores
</tableCaption>
<bodyText confidence="0.998793046511628">
the M&amp;N features). The baseline for detailed read-
ing results in Table 4 reflects the distribution of
the classes in the test file. The max column shows
the best performance for each task in the SemEval
2007 competition (Markert and Nissim, 2007).
The SMO column shows the results of learning
when Wikipedia information is not used to com-
pute the values of the collocation, has-product and
has-event features.
Nissim and Markert (2003) have shown that
grammatical roles are very strong features. Exper-
iments on the data represented exclusively through
grammatical role features confirm this observa-
tion, as the results obtained using only the syn-
tactic features (no lexical head information) give
the same results as the M&amp;Nfeat.bin. which does
include lexical information.
On the location metonymies, the current ap-
proach performs better on all evaluation types
(coarse, medium, fine) by 0.9, 1.1 and 0.9% points
respectively. The improvement comes from rec-
ognizing better the metonymic readings, as it is
apparent from the detailed F-score results in Ta-
ble 4. For the coarse readings, the F-score for
the non-literal reading is 1.5% points higher
than the best performance at SemEval, and 2.5%
and 7.7% points respectively for the metonymic
and mixed readings for the medium and fine
coarseness. It is interesting that the learning is
quite successful even when only selectional pref-
erence and Wikipedia-based has-product and has-
event features are used – the SMOSP column in
Table 3. The grammatical role and the related
lemma were used to derive these collocation fea-
tures, but they do not appear as such in the repre-
sentation used for this batch of experiments.
For company metonymies the current approach
does not perform better than the state-of-the-art.
For these metonymies the syntactic information is
not as useful. This is evidenced by the lower per-
formance of the classifier that uses only syntactic
information (column M&amp;N feat.bin. in Table 3),
despite the fact that the training dataset for com-
</bodyText>
<figure confidence="0.997276866666667">
literal
place-for-people
place-for-event
place-for-product
obj-for-name
obj-for-rep
othermet
mixed
91.2 91.6 91.6
58.9 61.7 61.7
16.7 0 0
0 0 0
66.7 0 0
0 0 0
0 0 0
8.3 16 8.7
79.4
15.5
1.1
1.1
0.4
0
1.2
2.2
literal 61.8
metonymic 31.0
mixed 7.2
82.5 81.4 81.2
60.4 58.7 58.1
30.8 26.8 28.9
literal
org-for-members
org-for-event
org-for-product
org-for-facility
org-for-index
org-for-name
org-for-rep
othermet
mixed
82.6 81.4 81.2
63.0 59.7 59.2
0 0 0
50.0 44.4 44
22.2 36.3 38.1
0 0 0
80.0 58.8 58.8
0 0 0
0 0 0
34.3 27.1 29.3
61.8
19.1
0.1
8.0
2.0
0.3
0.7
0
1.0
7.2
</figure>
<page confidence="0.996003">
916
</page>
<bodyText confidence="0.999951540983607">
panies is larger than the one for countries. This
observation is further supported by the low results
when using only selectional preference features.
It indicates that for company metonymies the lo-
cal context does not provide as strong clues as it
does for locations. For such PMWs we should
explore the larger context. We have made a start
with the Wikipedia-based features built following
the observation about companies and their prod-
ucts made by Farkas et al. (2007) and Brun et
al. (2007). In future work we plan to analyse
this matter further, and find a method to derive
more such features, and without manually pro-
vided clues (such as manufacturer or riots).
Wikipedia derived information does not con-
tribute very much, but as expected it is helpful
to identify other classes than the literal one.
It is helpful to detect the mixed class – 16%
F-score when using Wikipedia information com-
pared to 8.7% for the countries data when we esti-
mate preferences using only WordNet. It also in-
creases the performance on the non-literal,
metonymic and org-for-members classes
in coarse, medium and fine classification re-
spectively for both countries and companies.
There is a small improvement for recognizing the
org-for-product reading for organizations
when using Wikipedia-based features. It is an in-
dication that the has-product feature is useful. We
cannot draw conclusions about the has-event fea-
ture, as there are only 3 training instances for the
place-for-event reading. The results are en-
couraging, as we have just scraped the surface of
the information that Wikipedia can provide.
The corpus derived selectional preferences per-
form very well, especially for determining the
reading of locations. Analysis of the data and
the features gives some indication as to why this
happens: in the grammatical annotations provided,
when the PMW is a prepositional complement or
has a prepositional complement, the grammati-
cally related word is a preposition. We extract only
grammatical collocations for open-class words, re-
stricted by the grammatical relation of interest,
so we do not extract collocations for preposi-
tions. Location prepositions (in, at, from) are
less ambiguous than others (e.g. for), which are
more common for the organization data. We have
attempted to bypass this problem by generating
parses using the dependency output of the Stan-
ford Parser (de Marneffe et al., 2006), and bypass-
ing the preposition – incorporate it in the gram-
matical role (pp in, for example), and using as
lemma the head of the prepositional complement
or the constituent which dominates the preposi-
tional phrase, depending on the position of the
PMW. Now we can use the grammatical relation
and the associated open-class word to look for col-
locations. This approach did not lead to good re-
sults, because the quality of the automatic parses
is far from the manually provided information.
</bodyText>
<sectionHeader confidence="0.999529" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999982470588235">
We have explored the use of selectional preference
scores derived from a sense untagged corpus as lo-
cal constrains for determining the interpretation of
potentially metonymic words. Such methods were
previously successfully used for word sense dis-
ambiguation, and transfer nicely to the metonymy
resolution task. Adding encyclopedic knowledge
to the mix improved the results further, by filling
in gaps for WordNet, and extracting information
particular to PMW. We plan to expand on this, and
find methods to extract more such features auto-
matically, without manually provided clues.
For a more comprehensive treatment of
metonymies one must take into consideration not
only local context but also discourse relations.
A possible avenue of research is to build upon
coreference resolution systems, and use the
mentions they detect and link to each other in a
manner similar to using grammatical information
and grammatically related words to determine
constraints from a larger context. Determining
the link between two mentions in a text can take
advantage of encyclopedic knowledge, and the
system’s ability to infer the connection between
the mentions.
There is much work on unsupervised word
sense disambiguation. Working with untagged
data gives a system access to a much larger in-
formation base. Since selectional preferences ac-
quired from sense-untagged corpora have worked
well for the metonymy resolution task, we plan to
push further towards unsupervised metonymy res-
olution, putting to use the lessons learned from un-
supervised WSD.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998014">
We thank the Klaus Tschira Foundation, Heidel-
berg, for financial support, and the organizers of
the SemEval-2007 Task #8 Metonymy Resolution
– Katja Markert and Malvina Nissim – for making
the annotated data freely available.
</bodyText>
<page confidence="0.995364">
917
</page>
<sectionHeader confidence="0.998335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999713045871559">
Caroline Brun, Maud Ehrmann, and Guillaume
Jacquet. 2007. XRCE-M: A hybrid system for
named entity metonymy resolution. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-1), Prague, Czech Republic, 23–
24 June 2007, pages 488–491.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, Genoa, Italy,
22–28 May 2006, pages 449–454.
Richard Farkas, Eszter Simon, Gyorgy Szarvas, and
Daniel Varga. 2007. GYDER: Maxent metonymy
resolution. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-1),
Prague, Czech Republic, 23–24 June 2007, pages
161–164.
Dan C. Fass. 1991. met∗: A method for discriminating
metonomy and metaphor by computer. Computa-
tional Linguistics, 17(1):49–90.
Sanda M. Harabagiu. 1998. Deriving metonymic coer-
cions from WordNet. In In Workshop on the Usage
of WordNet in Natural Language Processing Sys-
tems, Montreal, Canada, August 16, 1998, pages
142–148.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1-2):69–142.
Shin-ichiro Kamei and Takahiro Wakao. 1992.
Metonymy: Reassessment, survey of acceptability,
and its treatment in a machine translation system.
In Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics, Newark,
Del., 28 June – 2 July 1992, pages 309–311.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The Sketch Engine. In Proceedings
of the 11th International Congress of the European
Association for Lexicography, Lorient, France, 6–10
July 2004, pages 105–116.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning, Madi-
son, Wisc., 24–27 July 1998, pages 296–304.
Katja Markert and Udo Hahn. 2002. Metonymies in
discourse. Artificial Intelligence, 135(1/2):145–198.
Katja Markert and Malvina Nissim. 2002. Metonymy
resolution as classification task. In Proceedings of
the 2002 Conference on Empirical Methods in Nat-
ural Language Processing, Philadelphia, Penn., 6–7
July 2002, pages 204–213.
Katja Markert and Malvina Nissim. 2003. Corpus-
based metonymy analysis. Metaphor and Symbol,
18(3):175–188.
Katja Markert and Malvina Nissim. 2007. SemEval-
2007 Task 08: Metonymy Resolution at SemEval-
2007. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23–24 June 2007, pages 36–41.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia category names for knowledge acquisi-
tion. In Proceedings of the 23rd Conference on the
Advancement ofArtificial Intelligence, Chicago, Ill.,
13–17 July 2008, pages 1219–1224.
Vivi Nastase. 2008. Unsupervised all-words word
sense disambiguation with grammatical dependen-
cies. In Proceedings of the 3rd International Joint
Conference on Natural Language Processing, Hy-
derabad, India, 7–12 January 2008, pages 757–762.
Cristina Nicolae, Gabriel Nicolae, and Sanda
Harabagiu. 2007. UTD-HLT-CG: Semantic
architecture for metonymy resolution and classifica-
tion of nominal relations. In Proceedings of the 4th
International Workshop on Semantic Evaluations
(SemEval-1), Prague, Czech Republic, 23–24 June
2007, pages 454–459.
Malvina Nissim and Katja Markert. 2003. Syn-
tactic features and word similarity for supervised
metonymy resolution. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, Sapporo, Japan, 7–12 July 2003,
pages 56–63.
Malvina Nissim and Katja Markert. 2005. Learning
to buy a Renault and talk to BMW: A supervised
approach to conventional metonymy. In Proceed-
ings of the 6th International Workshop on Computa-
tional Semantics, Tilburg, Netherlands, January 12-
14, 2005.
James Pustejovsky. 1991. The generative lexicon.
Computational Linguistics, 17(4):209–241.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, (61):127–159.
David Stallard. 1993. Two kinds of metonymy. In
Proceedings of the 31st Annual Meeting of the As-
sociation for Computational Linguistics, Columbus,
Ohio, 22–26 June 1993, pages 87–94.
Gustaf Stern. 1931. Meaning and Changes of Mean-
ing. Indiana University Press, Bloomington, Indi-
ana. (1968; first published in Sweden 1931).
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, Cal., 2nd edi-
tion.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivalling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, Cambridge,
Mass., 26–30 June 1995, pages 189–196.
</reference>
<page confidence="0.995735">
918
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.556719">
<title confidence="0.9985485">Combining Collocations, Lexical and Encyclopedic for Metonymy Resolution</title>
<author confidence="0.766571">Nastase</author>
<affiliation confidence="0.994996">EML Research</affiliation>
<address confidence="0.951278">Heidelberg, Germany</address>
<web confidence="0.908124">http://www.eml-research.de/nlp</web>
<abstract confidence="0.992878909090909">This paper presents a supervised method for resolving metonymies. We enhance a commonly used feature set with features extracted based on collocation information from corpora, generalized using lexical and encyclopedic knowledge to determine the preferred sense of the potentially metonymic word using methods from unsupervised word sense disambiguation. The methodology developed addresses one issue related to metonymy resolution – the influence of local context. The method developed is applied to the metonymy resolution task from SemEval 2007. The results obtained, higher for the countries subtask, on a par for the companies subtask – compared to participating systems – confirm that lexical, encyclopedic and collocation information can be successfully combined for metonymy resolution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Caroline Brun</author>
<author>Maud Ehrmann</author>
<author>Guillaume Jacquet</author>
</authors>
<title>XRCE-M: A hybrid system for named entity metonymy resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-1),</booktitle>
<volume>23</volume>
<pages>488--491</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7828" citStr="Brun et al., 2007" startWordPosition="1202" endWordPosition="1205">ple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusively on the data provided by the organizers – instance representation captures only information </context>
<context position="20070" citStr="Brun et al. (2007)" startWordPosition="3081" endWordPosition="3084"> Wikipedia-based links. This process transforms the grammatical annotation file and enhances it with the collocation estimates, as shown in Table 2 (compare this with a sample of the original file presented in Figure 1). 4.3 Product and event features Farkas et al. (2007) observed that using the PMWs themselves as features leads to improvement on determining the reading for organization names, and postulate that this is because some company names are more likely to be used in a metonymic way. This is often the case with companies that make products which are commonly used (cars, for example). Brun et al. (2007) note that certain locations, such as Vietnam, are more likely to be used with an event reading than others locations. Generally, locations strongly associated with events tend to be used to refer to the event, and more often have a place-for-event interpretation rather than a literal one. These two observations have lead us to mine for these pieces of information in the Wikipedia relations, and to add two more features for a target PMW: has-product will take a value of 1 if any of the PMW’s hypernyms (according to the isa relations extracted from Wikipedia) contains the string manufacturer, w</context>
<context position="27541" citStr="Brun et al. (2007)" startWordPosition="4306" endWordPosition="4309">0 0 50.0 44.4 44 22.2 36.3 38.1 0 0 0 80.0 58.8 58.8 0 0 0 0 0 0 34.3 27.1 29.3 61.8 19.1 0.1 8.0 2.0 0.3 0.7 0 1.0 7.2 916 panies is larger than the one for countries. This observation is further supported by the low results when using only selectional preference features. It indicates that for company metonymies the local context does not provide as strong clues as it does for locations. For such PMWs we should explore the larger context. We have made a start with the Wikipedia-based features built following the observation about companies and their products made by Farkas et al. (2007) and Brun et al. (2007). In future work we plan to analyse this matter further, and find a method to derive more such features, and without manually provided clues (such as manufacturer or riots). Wikipedia derived information does not contribute very much, but as expected it is helpful to identify other classes than the literal one. It is helpful to detect the mixed class – 16% F-score when using Wikipedia information compared to 8.7% for the countries data when we estimate preferences using only WordNet. It also increases the performance on the non-literal, metonymic and org-for-members classes in coarse, medium a</context>
</contexts>
<marker>Brun, Ehrmann, Jacquet, 2007</marker>
<rawString>Caroline Brun, Maud Ehrmann, and Guillaume Jacquet. 2007. XRCE-M: A hybrid system for named entity metonymy resolution. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-1), Prague, Czech Republic, 23– 24 June 2007, pages 488–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy,</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation, Genoa, Italy, 22–28 May 2006, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Farkas</author>
<author>Eszter Simon</author>
<author>Gyorgy Szarvas</author>
<author>Daniel Varga</author>
</authors>
<title>GYDER: Maxent metonymy resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-1),</booktitle>
<pages>161--164</pages>
<location>Prague, Czech Republic, 23–24</location>
<contexts>
<context position="7733" citStr="Farkas et al., 2007" startWordPosition="1185" endWordPosition="1188">y constraint when comparing two instances. This technique is complex, requiring smoothing, multiple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusiv</context>
<context position="19724" citStr="Farkas et al. (2007)" startWordPosition="3023" endWordPosition="3026">collocate from the corpus wj and a possible sense of a PMW si cannot be established using supersense information (for the supersenses) or through transitive closure in the hypernymhyponym hierarchy in WordNet (for company 1http://www/eml-research.de/nlp/ download/wikirelations.php 914 and organization) for any sense of wj, it is tried against the Wikipedia-based links. This process transforms the grammatical annotation file and enhances it with the collocation estimates, as shown in Table 2 (compare this with a sample of the original file presented in Figure 1). 4.3 Product and event features Farkas et al. (2007) observed that using the PMWs themselves as features leads to improvement on determining the reading for organization names, and postulate that this is because some company names are more likely to be used in a metonymic way. This is often the case with companies that make products which are commonly used (cars, for example). Brun et al. (2007) note that certain locations, such as Vietnam, are more likely to be used with an event reading than others locations. Generally, locations strongly associated with events tend to be used to refer to the event, and more often have a place-for-event inter</context>
<context position="27518" citStr="Farkas et al. (2007)" startWordPosition="4301" endWordPosition="4304">.4 81.2 63.0 59.7 59.2 0 0 0 50.0 44.4 44 22.2 36.3 38.1 0 0 0 80.0 58.8 58.8 0 0 0 0 0 0 34.3 27.1 29.3 61.8 19.1 0.1 8.0 2.0 0.3 0.7 0 1.0 7.2 916 panies is larger than the one for countries. This observation is further supported by the low results when using only selectional preference features. It indicates that for company metonymies the local context does not provide as strong clues as it does for locations. For such PMWs we should explore the larger context. We have made a start with the Wikipedia-based features built following the observation about companies and their products made by Farkas et al. (2007) and Brun et al. (2007). In future work we plan to analyse this matter further, and find a method to derive more such features, and without manually provided clues (such as manufacturer or riots). Wikipedia derived information does not contribute very much, but as expected it is helpful to identify other classes than the literal one. It is helpful to detect the mixed class – 16% F-score when using Wikipedia information compared to 8.7% for the countries data when we estimate preferences using only WordNet. It also increases the performance on the non-literal, metonymic and org-for-members clas</context>
</contexts>
<marker>Farkas, Simon, Szarvas, Varga, 2007</marker>
<rawString>Richard Farkas, Eszter Simon, Gyorgy Szarvas, and Daniel Varga. 2007. GYDER: Maxent metonymy resolution. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-1), Prague, Czech Republic, 23–24 June 2007, pages 161–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan C Fass</author>
</authors>
<title>met∗: A method for discriminating metonomy and metaphor by computer.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="5683" citStr="Fass, 1991" startWordPosition="873" endWordPosition="874">the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conventional metonymic readings are very systematic; (ii) unconventional metonymies can be created on the fly and their interpretation is context depe</context>
</contexts>
<marker>Fass, 1991</marker>
<rawString>Dan C. Fass. 1991. met∗: A method for discriminating metonomy and metaphor by computer. Computational Linguistics, 17(1):49–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
</authors>
<title>Deriving metonymic coercions from WordNet. In</title>
<date>1998</date>
<booktitle>In Workshop on the Usage of WordNet in Natural Language Processing Systems,</booktitle>
<pages>142--148</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="5269" citStr="Harabagiu, 1998" startWordPosition="811" endWordPosition="812">n this context – organization-for-people ororganization-for-product. The paper continues with related work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the lit</context>
</contexts>
<marker>Harabagiu, 1998</marker>
<rawString>Sanda M. Harabagiu. 1998. Deriving metonymic coercions from WordNet. In In Workshop on the Usage of WordNet in Natural Language Processing Systems, Montreal, Canada, August 16, 1998, pages 142–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
<author>Mark Stickel</author>
<author>Douglas Appelt</author>
<author>Paul Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="5670" citStr="Hobbs et al., 1993" startWordPosition="869" endWordPosition="872">es back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conventional metonymic readings are very systematic; (ii) unconventional metonymies can be created on the fly and their interpretation is</context>
</contexts>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63(1-2):69–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shin-ichiro Kamei</author>
<author>Takahiro Wakao</author>
</authors>
<title>Metonymy: Reassessment, survey of acceptability, and its treatment in a machine translation system.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>309--311</pages>
<location>Newark, Del.,</location>
<contexts>
<context position="5378" citStr="Kamei and Wakao, 1992" startWordPosition="824" endWordPosition="827">work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissi</context>
</contexts>
<marker>Kamei, Wakao, 1992</marker>
<rawString>Shin-ichiro Kamei and Takahiro Wakao. 1992. Metonymy: Reassessment, survey of acceptability, and its treatment in a machine translation system. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, Newark, Del., 28 June – 2 July 1992, pages 309–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Pavel Rychly</author>
<author>Pavel Smrz</author>
<author>David Tugwell</author>
</authors>
<title>The Sketch Engine.</title>
<date>2004</date>
<booktitle>In Proceedings of the 11th International Congress of the European Association for Lexicography,</booktitle>
<pages>105--116</pages>
<location>Lorient,</location>
<contexts>
<context position="15519" citStr="Kilgarriff et al., 2004" startWordPosition="2360" endWordPosition="2363">rvised WSD allow us to estimate such preferences from (sense/metonymy) untagged corpora. A potentially metonymic word (or phrase) has a small number of possible readings. These can be viewed as possible senses, and the task is to choose the one that fits best in the given context. The preference for each possible sense can be determined based on the PMW’s grammatically related words. To estimate these sense preferences we use grammatical collocations extracted from the British National Corpus (BNC), detected using regular expression matching over sequences of POS using the Word Sketch Engine (Kilgarriff et al., 2004). The scores are computed following a technique similar to Nastase (2008), which is illustrated using the following example: The Kawasaki drives well, steers brilliantly both under power and in tight corners ... The PMW Kawasaki is involved in the following grammatical relations in the previous sentence: (drive,subject,Kawasaki) (steer,subject,Kawasaki) 913 SampleID Lemma PMW GrRole Reading act animal artifact ... person ... samp190 say Sun subj org-for-members 0.00056 0.01171 0.01958 ... 0.61422 ... samp190 claim Sun subj org-for-members 0.00198 0.00099 0.00893 ... 0.50211 ... Table 2: Gramma</context>
</contexts>
<marker>Kilgarriff, Rychly, Smrz, Tugwell, 2004</marker>
<rawString>Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004. The Sketch Engine. In Proceedings of the 11th International Congress of the European Association for Lexicography, Lorient, France, 6–10 July 2004, pages 105–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<location>Madison, Wisc.,</location>
<contexts>
<context position="7015" citStr="Lin, 1998" startWordPosition="1077" endWordPosition="1078">l set of possible readings allow the metonymy resolution to be modelled as a classifier learning task. Markert and Nissim (2002) and Nissim and Markert (2003) have shown that conventional metonymies can be effectively resolved using a supervised machine learning approach. Moreover, grammatically related words are crucial in determining the interpretation of a PMW. The shortcoming is that manually annotated data is in short supply, and the approach suffers from data sparseness. To address this problem, Nissim and Markert (2003) proposed a word similarity-based method. They use Lin’s thesaurus (Lin, 1998) to determine how close two lexical heads are, and use this instead of the more restrictive identity constraint when comparing two instances. This technique is complex, requiring smoothing, multiple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, Madison, Wisc., 24–27 July 1998, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Udo Hahn</author>
</authors>
<title>Metonymies in discourse.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<pages>135--1</pages>
<contexts>
<context position="5294" citStr="Markert and Hahn, 2002" startWordPosition="813" endWordPosition="816">organization-for-people ororganization-for-product. The paper continues with related work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appro</context>
</contexts>
<marker>Markert, Hahn, 2002</marker>
<rawString>Katja Markert and Udo Hahn. 2002. Metonymies in discourse. Artificial Intelligence, 135(1/2):145–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Metonymy resolution as classification task.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>204--213</pages>
<location>Philadelphia, Penn.,</location>
<contexts>
<context position="2304" citStr="Markert and Nissim, 2002" startWordPosition="353" endWordPosition="356">s have parts, the cockpit and front seat are some of them, and this provides the discourse links between the two sentences. Constraints on the interpretation of a word w in context comes both from the local and global context, and are applied to the information/knowledge evoked by w. The local constraints come from the words with which w is (grammatically) related to. The global constraints come from the domain/topic of the text, discourse relations that span across sentences. Metonymic words have a rather small number of possible interpretations (also called readings) which occur frequently (Markert and Nissim, 2002). Idiosyncratic interpretations are also possible, but very rare. One can view the possible interpretations of a potentially metonymic word (PMW) as corresponding to the word’s possible senses (Nissim and Markert, 2003), bringing the task close to word sense disambiguation. The approach to metonymy resolution presented here is supervised, with unsupervised feature enrichment. We apply techniques inspired by unsupervised word sense disambiguation, which allow us to go beyond the annotated data provided in training, and quantify the restrictions imposed on the interpretation of a PMW by its gram</context>
<context position="6533" citStr="Markert and Nissim (2002)" startWordPosition="1000" endWordPosition="1004">e clues about the intended metonymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conventional metonymic readings are very systematic; (ii) unconventional metonymies can be created on the fly and their interpretation is context dependent; (iii) metonymies are frequent. The fact that most metonymic interpretations are systematic and correspond to a small set of possible readings allow the metonymy resolution to be modelled as a classifier learning task. Markert and Nissim (2002) and Nissim and Markert (2003) have shown that conventional metonymies can be effectively resolved using a supervised machine learning approach. Moreover, grammatically related words are crucial in determining the interpretation of a PMW. The shortcoming is that manually annotated data is in short supply, and the approach suffers from data sparseness. To address this problem, Nissim and Markert (2003) proposed a word similarity-based method. They use Lin’s thesaurus (Lin, 1998) to determine how close two lexical heads are, and use this instead of the more restrictive identity constraint when c</context>
<context position="10260" citStr="Markert and Nissim, 2002" startWordPosition="1595" endWordPosition="1598">portion. Figure 1 shows the text fragment for one sample, and Table 1 the data statistics. The reading column shows the possible interpretations of a PMW for countries and companies respectively. For example, org-for-product would be the interpretation of the PMW Kawasaki in the example shown in the introduction. Occurrences of country and company names were annotated with a small number of possible readings, as shown in Table 1. This reflects previous analyses of the metonymy phenomenon, which showed that there is a rather small number of possible interpretations that appear more frequently (Markert and Nissim, 2002). Special interpretations are very rarely encountered. Within the framework of the SemEval task, metonymy resolution is evaluated on the given test data, on three levels of granularity: coarse – distinguish between literal and non-literal readings; medium – distinguish between literal, mixed and non-literal readings; fine – identify the specific reading of the target word/words (potentially metonymic word - PMW). 4 Representation The method presented in this paper is a supervised learning method, along the same general lines as reading train test locations literal mixed othermet obj-for-name o</context>
</contexts>
<marker>Markert, Nissim, 2002</marker>
<rawString>Katja Markert and Malvina Nissim. 2002. Metonymy resolution as classification task. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, Philadelphia, Penn., 6–7 July 2002, pages 204–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Corpusbased metonymy analysis.</title>
<date>2003</date>
<journal>Metaphor and Symbol,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="5986" citStr="Markert and Nissim (2003)" startWordPosition="916" endWordPosition="919"> and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conventional metonymic readings are very systematic; (ii) unconventional metonymies can be created on the fly and their interpretation is context dependent; (iii) metonymies are frequent. The fact that most metonymic interpretations are systematic and correspond to a small set of possible readings allow the metonymy resolution to be modelled as a classifier learning task. Markert and Nissim (2002) and Nissim and Markert (2003) have shown that conven</context>
<context position="9454" citStr="Markert and Nissim (2003)" startWordPosition="1461" endWordPosition="1464">rammatical context as it is commonly used in WSD approaches, to guide the system in choosing the reading that fits best. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996) in unsupervised approaches to disambiguating single words in context. The method described here uses automatically induced selectional preferences, computed from sense-untagged data, similar to Nastase (2008). 3 Data We work with the data from the metonymy resolution task at SemEval 2007 (Markert and Nissim, 2007), generated based on a scheme developed by Markert and Nissim (2003). The metonymy resolution task at SemEval 2007 consisted of two subtasks – one for resolving country names, the other for companies. For each subtask there is a training and a test portion. Figure 1 shows the text fragment for one sample, and Table 1 the data statistics. The reading column shows the possible interpretations of a PMW for countries and companies respectively. For example, org-for-product would be the interpretation of the PMW Kawasaki in the example shown in the introduction. Occurrences of country and company names were annotated with a small number of possible readings, as sho</context>
</contexts>
<marker>Markert, Nissim, 2003</marker>
<rawString>Katja Markert and Malvina Nissim. 2003. Corpusbased metonymy analysis. Metaphor and Symbol, 18(3):175–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>SemEval2007 Task 08: Metonymy Resolution at SemEval2007.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-1),</booktitle>
<pages>36--41</pages>
<location>Prague, Czech Republic, 23–24</location>
<contexts>
<context position="7475" citStr="Markert and Nissim, 2007" startWordPosition="1144" endWordPosition="1147">proach suffers from data sparseness. To address this problem, Nissim and Markert (2003) proposed a word similarity-based method. They use Lin’s thesaurus (Lin, 1998) to determine how close two lexical heads are, and use this instead of the more restrictive identity constraint when comparing two instances. This technique is complex, requiring smoothing, multiple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing </context>
<context position="9386" citStr="Markert and Nissim, 2007" startWordPosition="1450" endWordPosition="1453">rvised word sense disambiguation (WSD). In short, we use the local grammatical context as it is commonly used in WSD approaches, to guide the system in choosing the reading that fits best. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996) in unsupervised approaches to disambiguating single words in context. The method described here uses automatically induced selectional preferences, computed from sense-untagged data, similar to Nastase (2008). 3 Data We work with the data from the metonymy resolution task at SemEval 2007 (Markert and Nissim, 2007), generated based on a scheme developed by Markert and Nissim (2003). The metonymy resolution task at SemEval 2007 consisted of two subtasks – one for resolving country names, the other for companies. For each subtask there is a training and a test portion. Figure 1 shows the text fragment for one sample, and Table 1 the data statistics. The reading column shows the possible interpretations of a PMW for countries and companies respectively. For example, org-for-product would be the interpretation of the PMW Kawasaki in the example shown in the introduction. Occurrences of country and company n</context>
<context position="23101" citStr="Markert and Nissim, 2007" startWordPosition="3585" endWordPosition="3588">n. 5 Results The models for determining a PMW’s correct interpretation are learned on the training data provided, and evaluated on the test portion, using the answer keys and evaluation script provided with the data. For learning the models we use Weka (Witten and Frank, 2005), and select the final learning algorithms based on 10-fold crossvalidation on the training data. We have settled on support vector machines (SMO in Weka), and we use the learner’s default settings. Tables 3 and Table 4 show the results obtained, and the baseline and the best results from the SemEval task for comparison (Markert and Nissim, 2007). The baseline in Table 3 corresponds to classifying everything as the most frequent class – literal interpretation. The M&amp;N feat. and M&amp;N feat.bin. correspond to datasets that contain only the M&amp;N features and the binarized versions of these features, respectively. SemEval best gives the best results obtained on each task in the SemEval 2007 task (Markert and Nissim, 2007). SMOwiki are the results obtained with the complete feature set described in Section 4, and SMOSP are the results obtained when only the new features are used – only selectional preference, has-product and has-event feature</context>
<context position="24712" citStr="Markert and Nissim, 2007" startWordPosition="3835" endWordPosition="3838">uracy scores task 1 method-* base max SMOwiki SMO LOCATION-COARSE literal 79.4 91.2 91.6 91.6 non-literal 20.6 57.6 59.1 58.8 LOCATION-MEDIUM literal 79.4 91.2 91.6 91.6 metonymic 18.4 58.0 61.5 61.5 mixed 2.2 8.3 16 8.7 LOCATION-FINE ORGANIZATION-COARSE literal 61.8 82.5 81.4 81.2 non-literal 38.2 65.2 61.6 60.7 ORGANIZATION-MEDIUM ORGANIZATION-FINE Table 4: Detailed F-scores the M&amp;N features). The baseline for detailed reading results in Table 4 reflects the distribution of the classes in the test file. The max column shows the best performance for each task in the SemEval 2007 competition (Markert and Nissim, 2007). The SMO column shows the results of learning when Wikipedia information is not used to compute the values of the collocation, has-product and has-event features. Nissim and Markert (2003) have shown that grammatical roles are very strong features. Experiments on the data represented exclusively through grammatical role features confirm this observation, as the results obtained using only the syntactic features (no lexical head information) give the same results as the M&amp;Nfeat.bin. which does include lexical information. On the location metonymies, the current approach performs better on all </context>
</contexts>
<marker>Markert, Nissim, 2007</marker>
<rawString>Katja Markert and Malvina Nissim. 2007. SemEval2007 Task 08: Metonymy Resolution at SemEval2007. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-1), Prague, Czech Republic, 23–24 June 2007, pages 36–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
</authors>
<title>Decoding Wikipedia category names for knowledge acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd Conference on the Advancement ofArtificial Intelligence,</booktitle>
<pages>1219--1224</pages>
<location>Chicago, Ill.,</location>
<contexts>
<context position="18965" citStr="Nastase and Strube, 2008" startWordPosition="2901" endWordPosition="2904">E BNC, fjis the frequency of occurrence1 and used to compute a preference score Psi for each sense si E S: Psi = E(w,R,wi,j :fi,j)∈Csi fi,j E(w,R,wj:fj)∈C fj where Csi = {(w, R, wj : fj)I(w, R, wj : fj) E C; supersense(wj, si) II isa(wj, si)1. supersense(wj, si) is true if si is a supersense of one of wj’s senses; isa(wj, si) is true if si is a hypernym of one of wj’s senses in WordNet, or is a fact extracted from Wikipedia. To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa relations extracted by processing the page and category network of Wikipedia1 (Nastase and Strube, 2008). The collocations extracted from BNC contain numerous named entities, most of which are not part of WordNet. If an isa relation between a collocate from the corpus wj and a possible sense of a PMW si cannot be established using supersense information (for the supersenses) or through transitive closure in the hypernymhyponym hierarchy in WordNet (for company 1http://www/eml-research.de/nlp/ download/wikirelations.php 914 and organization) for any sense of wj, it is tried against the Wikipedia-based links. This process transforms the grammatical annotation file and enhances it with the collocat</context>
</contexts>
<marker>Nastase, Strube, 2008</marker>
<rawString>Vivi Nastase and Michael Strube. 2008. Decoding Wikipedia category names for knowledge acquisition. In Proceedings of the 23rd Conference on the Advancement ofArtificial Intelligence, Chicago, Ill., 13–17 July 2008, pages 1219–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
</authors>
<title>Unsupervised all-words word sense disambiguation with grammatical dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>757--762</pages>
<location>Hyderabad, India, 7–12</location>
<contexts>
<context position="9279" citStr="Nastase (2008)" startWordPosition="1431" endWordPosition="1433">ing a PMW towards each of PMW’s possible readings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). In short, we use the local grammatical context as it is commonly used in WSD approaches, to guide the system in choosing the reading that fits best. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996) in unsupervised approaches to disambiguating single words in context. The method described here uses automatically induced selectional preferences, computed from sense-untagged data, similar to Nastase (2008). 3 Data We work with the data from the metonymy resolution task at SemEval 2007 (Markert and Nissim, 2007), generated based on a scheme developed by Markert and Nissim (2003). The metonymy resolution task at SemEval 2007 consisted of two subtasks – one for resolving country names, the other for companies. For each subtask there is a training and a test portion. Figure 1 shows the text fragment for one sample, and Table 1 the data statistics. The reading column shows the possible interpretations of a PMW for countries and companies respectively. For example, org-for-product would be the interp</context>
<context position="15592" citStr="Nastase (2008)" startWordPosition="2373" endWordPosition="2374">pora. A potentially metonymic word (or phrase) has a small number of possible readings. These can be viewed as possible senses, and the task is to choose the one that fits best in the given context. The preference for each possible sense can be determined based on the PMW’s grammatically related words. To estimate these sense preferences we use grammatical collocations extracted from the British National Corpus (BNC), detected using regular expression matching over sequences of POS using the Word Sketch Engine (Kilgarriff et al., 2004). The scores are computed following a technique similar to Nastase (2008), which is illustrated using the following example: The Kawasaki drives well, steers brilliantly both under power and in tight corners ... The PMW Kawasaki is involved in the following grammatical relations in the previous sentence: (drive,subject,Kawasaki) (steer,subject,Kawasaki) 913 SampleID Lemma PMW GrRole Reading act animal artifact ... person ... samp190 say Sun subj org-for-members 0.00056 0.01171 0.01958 ... 0.61422 ... samp190 claim Sun subj org-for-members 0.00198 0.00099 0.00893 ... 0.50211 ... Table 2: Grammatical annotation file enhanced with selectional preference estimates The </context>
</contexts>
<marker>Nastase, 2008</marker>
<rawString>Vivi Nastase. 2008. Unsupervised all-words word sense disambiguation with grammatical dependencies. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, Hyderabad, India, 7–12 January 2008, pages 757–762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Nicolae</author>
<author>Gabriel Nicolae</author>
<author>Sanda Harabagiu</author>
</authors>
<title>UTD-HLT-CG: Semantic architecture for metonymy resolution and classification of nominal relations.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-1),</booktitle>
<pages>454--459</pages>
<location>Prague, Czech Republic, 23–24</location>
<contexts>
<context position="7756" citStr="Nicolae et al., 2007" startWordPosition="1189" endWordPosition="1192">paring two instances. This technique is complex, requiring smoothing, multiple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusively on the data provide</context>
</contexts>
<marker>Nicolae, Nicolae, Harabagiu, 2007</marker>
<rawString>Cristina Nicolae, Gabriel Nicolae, and Sanda Harabagiu. 2007. UTD-HLT-CG: Semantic architecture for metonymy resolution and classification of nominal relations. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-1), Prague, Czech Republic, 23–24 June 2007, pages 454–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malvina Nissim</author>
<author>Katja Markert</author>
</authors>
<title>Syntactic features and word similarity for supervised metonymy resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>56--63</pages>
<location>Sapporo,</location>
<contexts>
<context position="2523" citStr="Nissim and Markert, 2003" startWordPosition="385" endWordPosition="388">ontext, and are applied to the information/knowledge evoked by w. The local constraints come from the words with which w is (grammatically) related to. The global constraints come from the domain/topic of the text, discourse relations that span across sentences. Metonymic words have a rather small number of possible interpretations (also called readings) which occur frequently (Markert and Nissim, 2002). Idiosyncratic interpretations are also possible, but very rare. One can view the possible interpretations of a potentially metonymic word (PMW) as corresponding to the word’s possible senses (Nissim and Markert, 2003), bringing the task close to word sense disambiguation. The approach to metonymy resolution presented here is supervised, with unsupervised feature enrichment. We apply techniques inspired by unsupervised word sense disambiguation, which allow us to go beyond the annotated data provided in training, and quantify the restrictions imposed on the interpretation of a PMW by its grammatically related neighbours through collocation information extracted from corpora. The only annotation required for the corpora are automatically induced part-of-speech tags from which we obtain grammatical relations </context>
<context position="6563" citStr="Nissim and Markert (2003)" startWordPosition="1006" endWordPosition="1009">onymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conventional metonymic readings are very systematic; (ii) unconventional metonymies can be created on the fly and their interpretation is context dependent; (iii) metonymies are frequent. The fact that most metonymic interpretations are systematic and correspond to a small set of possible readings allow the metonymy resolution to be modelled as a classifier learning task. Markert and Nissim (2002) and Nissim and Markert (2003) have shown that conventional metonymies can be effectively resolved using a supervised machine learning approach. Moreover, grammatically related words are crucial in determining the interpretation of a PMW. The shortcoming is that manually annotated data is in short supply, and the approach suffers from data sparseness. To address this problem, Nissim and Markert (2003) proposed a word similarity-based method. They use Lin’s thesaurus (Lin, 1998) to determine how close two lexical heads are, and use this instead of the more restrictive identity constraint when comparing two instances. This t</context>
<context position="14045" citStr="Nissim and Markert (2003)" startWordPosition="2131" endWordPosition="2134">sg, pl); • number of grammatical roles in which the PMW appears in its current context; • number of words in PMW; All these features can be extracted from the grammatically annotated and POS tagged data provided by the organizers. 4.2 Selectional preference features The grammatical relations and the connected words are important to describe the local context of the target PMW. Because of the limited amount of annotated data (a few thousand instances), lemmas of PMW’s grammatically related words will make for very sparse data that a machine learning system would not be able to generalize over. Nissim and Markert (2003) and the teams participating in the metonymy resolution task have then supplemented their systems with Lin’s thesaurus, WordNet, Beth Levin’s verb groups, FrameNet information, or manually designed lists of words to generalize the grammatically related words and thus find shared characteristics across instances of metonymies in text. The notion of selectional restrictions used in metonymy resolution – meaning the restrictions imposed on the interpretation of a PMW by its context – is similar to the notion of selectional preferences from word sense disambiguation – meaning the preferences of a </context>
<context position="24901" citStr="Nissim and Markert (2003)" startWordPosition="3865" endWordPosition="3868"> 61.5 61.5 mixed 2.2 8.3 16 8.7 LOCATION-FINE ORGANIZATION-COARSE literal 61.8 82.5 81.4 81.2 non-literal 38.2 65.2 61.6 60.7 ORGANIZATION-MEDIUM ORGANIZATION-FINE Table 4: Detailed F-scores the M&amp;N features). The baseline for detailed reading results in Table 4 reflects the distribution of the classes in the test file. The max column shows the best performance for each task in the SemEval 2007 competition (Markert and Nissim, 2007). The SMO column shows the results of learning when Wikipedia information is not used to compute the values of the collocation, has-product and has-event features. Nissim and Markert (2003) have shown that grammatical roles are very strong features. Experiments on the data represented exclusively through grammatical role features confirm this observation, as the results obtained using only the syntactic features (no lexical head information) give the same results as the M&amp;Nfeat.bin. which does include lexical information. On the location metonymies, the current approach performs better on all evaluation types (coarse, medium, fine) by 0.9, 1.1 and 0.9% points respectively. The improvement comes from recognizing better the metonymic readings, as it is apparent from the detailed F</context>
</contexts>
<marker>Nissim, Markert, 2003</marker>
<rawString>Malvina Nissim and Katja Markert. 2003. Syntactic features and word similarity for supervised metonymy resolution. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan, 7–12 July 2003, pages 56–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malvina Nissim</author>
<author>Katja Markert</author>
</authors>
<title>Learning to buy a Renault and talk to BMW: A supervised approach to conventional metonymy.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Workshop on Computational Semantics,</booktitle>
<location>Tilburg, Netherlands,</location>
<contexts>
<context position="7915" citStr="Nissim and Markert, 2005" startWordPosition="1215" endWordPosition="1218">matical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusively on the data provided by the organizers – instance representation captures only information that can be derived from or between the data points provided. The approach presented he</context>
<context position="11391" citStr="Nissim and Markert (2005)" startWordPosition="1747" endWordPosition="1750">g the same general lines as reading train test locations literal mixed othermet obj-for-name obj-for-representation place-for-people place-for-event place-for-product organizations literal mixed othermet obj-for-name obj-for-representation org-for-members org-for-event org-for-product org-for-facility org-for-index Table 1: Reading distributions the systems which participated in the SemEval competition. As such, it represents each PMW in the data through features that describe its context and some semantic characteristics. The minimum set of necessary features is taken to be that presented by Nissim and Markert (2005), and proved to be effective in solving metonymies. These are the M&amp;N features (Markert and Nissim features). We expand on these features and estimate preferences from words in a PMW’s context towards specific PMW interpretations. These constitute the selectional preference features. Finally, Wikipedia is a source of facts which can be used to derive information that can bias the decision towards certain interpretations for a PMW. Each of these features are described in more detail in the following subsections. 4.1 M&amp;N features The features used by Nissim and Markert (2005) are: • grammatical </context>
<context position="21026" citStr="Nissim and Markert (2005)" startWordPosition="3241" endWordPosition="3244">ine for these pieces of information in the Wikipedia relations, and to add two more features for a target PMW: has-product will take a value of 1 if any of the PMW’s hypernyms (according to the isa relations extracted from Wikipedia) contains the string manufacturer, will have the value 0 otherwise; has-event will have the value 1 if any of the PMW’s hypernyms refers to an event (movements/operations/riots), and value 0 otherwise. 4.4 Data representation As mentioned before, the representation built can be seen as consisting of roughly three subsets of features: • the M&amp;N features proposed by Nissim and Markert (2005). To combine the grammatical information from all relations, we transform the grammatical relations into features (as opposed to values). For a relation subject for example, we generate a binary subject feature that indicates whether for a given target this grammatical relation is filled or not, and a subject lemma feature, whose value is the lemma of the grammatically related word. • the selectional preference scores. Each of these features corresponds to one of the elements of S, presented above. These features combine the selectional preferences of all the grammatical relations for one targ</context>
</contexts>
<marker>Nissim, Markert, 2005</marker>
<rawString>Malvina Nissim and Katja Markert. 2005. Learning to buy a Renault and talk to BMW: A supervised approach to conventional metonymy. In Proceedings of the 6th International Workshop on Computational Semantics, Tilburg, Netherlands, January 12-14, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The generative lexicon.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="5650" citStr="Pustejovsky, 1991" startWordPosition="867" endWordPosition="868">stic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conventional metonymic readings are very systematic; (ii) unconventional metonymies can be created on the fly and the</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>James Pustejovsky. 1991. The generative lexicon. Computational Linguistics, 17(4):209–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional constraints: An information-theoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="9070" citStr="Resnik (1996)" startWordPosition="1402" endWordPosition="1403">etween the data points provided. The approach presented here goes beyond the given data, and induces from corpora measures that allow the system to determine 911 what are the preferences of the words surrounding a PMW towards each of PMW’s possible readings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). In short, we use the local grammatical context as it is commonly used in WSD approaches, to guide the system in choosing the reading that fits best. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996) in unsupervised approaches to disambiguating single words in context. The method described here uses automatically induced selectional preferences, computed from sense-untagged data, similar to Nastase (2008). 3 Data We work with the data from the metonymy resolution task at SemEval 2007 (Markert and Nissim, 2007), generated based on a scheme developed by Markert and Nissim (2003). The metonymy resolution task at SemEval 2007 consisted of two subtasks – one for resolving country names, the other for companies. For each subtask there is a training and a test portion. Figure 1 shows the text fr</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Philip Resnik. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, (61):127–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Stallard</author>
</authors>
<title>Two kinds of metonymy.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>87--94</pages>
<location>Columbus, Ohio, 22–26</location>
<contexts>
<context position="5331" citStr="Stallard, 1993" startWordPosition="819" endWordPosition="820">oduct. The paper continues with related work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the inten</context>
</contexts>
<marker>Stallard, 1993</marker>
<rawString>David Stallard. 1993. Two kinds of metonymy. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio, 22–26 June 1993, pages 87–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gustaf Stern</author>
</authors>
<title>Meaning and Changes of Meaning.</title>
<date>1931</date>
<publisher>Indiana University Press,</publisher>
<location>Bloomington, Indiana.</location>
<note>first published in</note>
<contexts>
<context position="5095" citStr="Stern, 1931" startWordPosition="785" endWordPosition="786"> subject. These are excel’s selectional preferences towards certain collocates, and will help determine which possible interpretation for the PMW Kawasaki is appropriate in this context – organization-for-people ororganization-for-product. The paper continues with related work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert an</context>
</contexts>
<marker>Stern, 1931</marker>
<rawString>Gustaf Stern. 1931. Meaning and Changes of Meaning. Indiana University Press, Bloomington, Indiana. (1968; first published in Sweden 1931).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>edition.</title>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, Cal.,</location>
<contexts>
<context position="22753" citStr="Witten and Frank, 2005" startWordPosition="3526" endWordPosition="3529">s it only implies setting the correct value for the grammatical relations that are valid in the PMWs context. The final representation consists of 63 features + class feature for the subset for company PMWs, 59 features + class feature for the subset containing countries PMWs. The sample ID and the PMW itself were not part of this representation. 5 Results The models for determining a PMW’s correct interpretation are learned on the training data provided, and evaluated on the test portion, using the answer keys and evaluation script provided with the data. For learning the models we use Weka (Witten and Frank, 2005), and select the final learning algorithms based on 10-fold crossvalidation on the training data. We have settled on support vector machines (SMO in Weka), and we use the learner’s default settings. Tables 3 and Table 4 show the results obtained, and the baseline and the best results from the SemEval task for comparison (Markert and Nissim, 2007). The baseline in Table 3 corresponds to classifying everything as the most frequent class – literal interpretation. The M&amp;N feat. and M&amp;N feat.bin. correspond to datasets that contain only the M&amp;N features and the binarized versions of these features,</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, San Francisco, Cal., 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivalling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, Mass.,</location>
<contexts>
<context position="9052" citStr="Yarowsky (1995)" startWordPosition="1399" endWordPosition="1400">be derived from or between the data points provided. The approach presented here goes beyond the given data, and induces from corpora measures that allow the system to determine 911 what are the preferences of the words surrounding a PMW towards each of PMW’s possible readings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). In short, we use the local grammatical context as it is commonly used in WSD approaches, to guide the system in choosing the reading that fits best. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996) in unsupervised approaches to disambiguating single words in context. The method described here uses automatically induced selectional preferences, computed from sense-untagged data, similar to Nastase (2008). 3 Data We work with the data from the metonymy resolution task at SemEval 2007 (Markert and Nissim, 2007), generated based on a scheme developed by Markert and Nissim (2003). The metonymy resolution task at SemEval 2007 consisted of two subtasks – one for resolving country names, the other for companies. For each subtask there is a training and a test portion. Figure 1</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivalling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Cambridge, Mass., 26–30 June 1995, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>