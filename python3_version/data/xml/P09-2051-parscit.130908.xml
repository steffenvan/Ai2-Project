<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009487">
<title confidence="0.997003">
Mining Association Language Patterns for
Negative Life Event Classification
</title>
<author confidence="0.999258">
Liang-Chih Yu1, Chien-Lung Chan1, Chung-Hsien Wu2 and Chao-Cheng Lin3
</author>
<affiliation confidence="0.997235666666667">
1Department of Information Management, Yuan Ze University, Taiwan, R.O.C.
2Department of CSIE, National Cheng Kung University, Taiwan, R.O.C.
3Department of Psychiatry, National Taiwan University Hospital, Taiwan, R.O.C.
</affiliation>
<email confidence="0.97672">
{lcyu, clchan}@saturn.yzu.edu.tw, chwu@csie.ncku.edu.tw, linchri@gmail.com
</email>
<sectionHeader confidence="0.995592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999753818181818">
Negative life events, such as death of a family
member, argument with a spouse and loss of a
job, play an important role in triggering de-
pressive episodes. Therefore, it is worth to de-
velop psychiatric services that can automati-
cally identify such events. In this paper, we
propose the use of association language pat-
terns, i.e., meaningful combinations of words
(e.g., &lt;loss, job&gt;), as features to classify sen-
tences with negative life events into prede-
fined categories (e.g., Family, Love, Work).
The language patterns are discovered using a
data mining algorithm, called association pat-
tern mining, by incrementally associating fre-
quently co-occurred words in the sentences
annotated with negative life events. The dis-
covered patterns are then combined with sin-
gle words to train classifiers. Experimental re-
sults show that association language patterns
are significant features, thus yielding better
performance than the baseline system using
single words alone.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966740740741">
With the increased incidence of depressive dis-
orders, many psychiatric websites have devel-
oped community-based services such as message
boards, web forums and blogs for public access.
Through these services, individuals can describe
their stressful or negative life events such as
death of a family member, argument with a
spouse and loss of a job, along with depressive
symptoms, such as depressive mood, suicidal
tendencies and anxiety. Such psychiatric texts
(e.g., forum posts) contain large amounts of natu-
ral language expressions related to negative life
events, making them useful resources for build-
ing more effective psychiatric services. For in-
stance, a psychiatric retrieval service can retrieve
relevant forum or blog posts according to the
negative life events experienced by users so that
they can be aware that they are not alone because
many people have suffered from the same or
similar problems. The users can then create a
community discussion to share their experiences
with each other. Additionally, a dialog system
can generate supportive responses like “Don’t
worry”, “That’s really sad” and “Cheer up” if it
can understand the negative life events embed-
ded in the example sentences shown in Table 1.
Therefore, this study proposes a framework for
negative life event classification. We formulate
this problem as a sentence classification task;
that is, classify sentences according to the type of
negative life events within them. The class labels
used herein are presented in Table 1, which are
derived from Brostedt and Pedersen (2003).
Traditional approaches to sentence classifica-
tion (Khoo et al., 2006; Naughton et al., 2008) or
text categorization (Sebastiani 2002) usually
adopt bag-of-words as baseline features to train
classifiers. Since the bag-of-words approach
treats each word independently without consider-
ing the relationships of words in sentences, some
researchers have investigated the use of n-grams
to capture sequential relations between words to
boost classification performance (Chitturi and
Hansen, 2008; Li and Zong, 2008). The use of n-
grams is effective in capturing local dependen-
cies of words, but tends to suffer from data
sparseness problem in capturing long-distance
dependencies since higher-order n-grams require
large training data to obtain reliable estimation.
For our task, the expressions of negative life
events can be characterized by association lan-
guage patterns, i.e., meaningful combinations of
words, such as &lt;worry, children, health&gt;, &lt;break
up, boyfriend&gt;, &lt;argue, friend&gt;, &lt;loss, job&gt;, and
</bodyText>
<page confidence="0.976468">
201
</page>
<note confidence="0.806282">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201–204,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<table confidence="0.991049181818182">
Label Description Example Sentence
Family Serious illness of a family member; I am very worried about my children’s health.
Son or daughter leaving home I broke up with my dear but cruel boyfriend
Love Spouse/mate engaged in infidelity; recently.
Broke up with a boyfriend or girlfriend I hate to go to school because my teacher al-
School Examination failed or grade dropped; ways blames me.
Unable to enter/stay in school I lost my job in this economic recession a few
Work Laid off or fired from a job; months ago.
Demotion and salary reduction I argued with my best friend and was upset.
Social Substantial conflicts with a friend;
Difficulties in social activities
</table>
<tableCaption confidence="0.999895">
Table 1. Classification of negative life events.
</tableCaption>
<bodyText confidence="0.999905238095238">
&lt;school, teacher, blame&gt; in the example sen-
tences in Table 1. Such language patterns are not
necessarily composed of continuous words. In-
stead, they are usually composed of the words
with long-distance dependencies, which cannot
be easily captured by n-grams.
Therefore, the aim of this study is two-fold: (1)
to automatically discover association language
patterns from the sentences annotated with nega-
tive life events; and (2) to classify sentences with
negative life events using the discovered patterns.
To discover association language patterns, we
incorporate the measure mutual information (MI)
into a data mining algorithm, called association
pattern mining, to incrementally derive fre-
quently co-occurred words in sentences (Section
2). The discovered patterns are then combined
with single words as features to train classifiers
for negative life event classification (Section 3).
Experimental results are presented in Section 4.
Conclusions are finally drawn in Section 5.
</bodyText>
<sectionHeader confidence="0.735328" genericHeader="method">
2 Association Language Pattern Mining
</sectionHeader>
<bodyText confidence="0.999987380952381">
The problem of language pattern acquisition can
be converted into the problem of association pat-
tern mining, where each sales transaction in a
database can be considered as a sentence in the
corpora, and each item in a transaction denotes a
word in a sentence. An association language pat-
tern is defined herein as a combination of multi-
ple associated words, denoted by &lt; w1,...,wk &gt; .
Thus, the task of association pattern mining is to
mine the language patterns of frequently associ-
ated words from the training sentences. For this
purpose, we adopt the Apriori algorithm
(Agrawal and Srikant, 1994) and modified it
slightly to fit our application. Its basic concept is
to identify frequent word sets recursively, and
then generate association language patterns from
the frequent word sets. For simplicity, only the
combinations of nouns and verbs are considered,
and the length is restricted to at most 4 words,
i.e., 2-word, 3-word and 4-word combinations.
The detailed procedure is described as follows.
</bodyText>
<subsectionHeader confidence="0.941475">
2.1 Find frequent word sets
</subsectionHeader>
<bodyText confidence="0.995451">
A word set is frequent if it possesses a minimum
support. The support of a word set is defined as
the number of training sentences containing the
word set. For instance, the support of a two-word
set { wi, wj } denotes the number of training sen-
tences containing the word pair ( wi , wj ). The
frequent k-word sets are discovered from (k-1)-
word sets. First, the support of each word, i.e.,
word frequency, in the training corpus is counted.
The set of frequent one-word sets, denoted as L1,
is then generated by choosing the words with a
minimum support level. To calculate Lk, the fol-
lowing two-step process is performed iteratively
until no more frequent k-word sets are found.
</bodyText>
<listItem confidence="0.7733215">
• Join step: A set of candidate k-word sets,
denoted as Ck , is first generated by merg-
</listItem>
<bodyText confidence="0.577990333333333">
ing frequent word sets of Lk−1 , in which
only the word sets whose first (k-2) words
are identical can be merged.
</bodyText>
<listItem confidence="0.984147">
• Prune step: The support of each candidate
word set in Ck is then counted to determine
</listItem>
<bodyText confidence="0.998935">
which candidate word sets are frequent. Fi-
nally, the candidate word sets with a sup-
port count greater than or equal to the
minimum support are considered to form
Lk. The candidate word sets with a subset
that is not frequent are eliminated. Figure 1
shows an example of generating Lk.
</bodyText>
<page confidence="0.99462">
202
</page>
<figureCaption confidence="0.99988">
Figure 1. Example of generating association language patterns.
</figureCaption>
<figure confidence="0.973416722222222">
Find Frequent Word Sets Generate Association Language Patterns
Prune Step
(min. support)
Join
Step
Prune Step
(min. support)
&lt;Boyfriend, Conflict&gt;
&lt;Boyfriend, Break up&gt;
&lt;Boss, Conflict&gt;
&lt;Conflict, Break up&gt;
&lt;Boyfriend, Conflict, Break up&gt;
Join Step
Sorting
and
Thresholding
Prune Step
(min. support)
</figure>
<subsectionHeader confidence="0.2827035">
2.2 Generate association patterns from fre-
quent word sets
</subsectionHeader>
<bodyText confidence="0.99945">
Association language patterns can be generated
via a confidence measure once the frequent word
sets have been identified. The confidence of an
association language pattern of k words is de-
fined as the mutual information of the k words,
as shown below.
</bodyText>
<equation confidence="0.989797">
(&lt; w1, ... wk &gt;) = Ww1,... wk)
= P(w1, ... wk)log
P ( wi)
</equation>
<bodyText confidence="0.999912">
where P(w1,...wk) denotes the probability of the
k words co-occurring in a sentence in the training
set, and P(wi ) denotes the probability of a sin-
gle word occurring in the training set. Accord-
ingly, each frequent word set in Lk is assigned a
mutual information score. In order to generate a
set of association language patterns, all frequent
word sets are sorted in the descending order of
the mutual information scores. The minimum
confidence (a threshold at percentage) is then
applied to select top N percent frequent word sets
as the resulting language patterns. This threshold
is determined empirically by maximizing classi-
fication performance (Section 4). Figure 1 (right-
hand side) shows an example of generating the
association language patterns from Lk.
</bodyText>
<sectionHeader confidence="0.994143" genericHeader="method">
3 Sentence Classification
</sectionHeader>
<bodyText confidence="0.998999">
The classifiers used in this study include Support
Vector Machine (SVM), C4.5, and Naïve Bayes
(NB) classifier, which is provided by Weka
Package (Witten and Frank, 2005). The feature
set includes:
Bag-of-Words (BOW): Each single word in
sentences.
Association language patterns (ALP): The top
N percent association language patterns acquired
in the previous section.
Ontology expansion (Onto): The top N percent
association language patterns are expanded by
mapping the constituent words into their syno-
nyms. For example, the pattern &lt;boss, conflict&gt;
can be expanded as &lt;chief, conflict&gt; since the
words boss and chief are synonyms. Here we use
the HowNet (http://www.keenage.com), a Chi-
nese lexical ontology, for pattern expansion.
</bodyText>
<sectionHeader confidence="0.997417" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.990521631578947">
Data set: A total of 2,856 sentences were col-
lected from the Internet-based Self-assessment
Program for Depression (ISP-D) database of the
PsychPark (http://www.psychpark.org), a virtual
psychiatric clinic, maintained by a group of vol-
unteer professionals of Taiwan Association of
Mental Health Informatics (Bai et al., 2001).
Each sentence was then annotated by trained an-
notators with one of the five types of negative
life events. Table 2 shows the break-down of the
distribution of sentence types.
The data set was randomly split into a training
set, a development set, and a test set with an
8:1:1 ratio. The training set was used for lan-
guage pattern generation. The development set
was used to optimize the threshold (Section 2.2)
for the classifiers (SVM, C4.5 and NB). Each
classifier was implemented using three different
levels of features, namely BOW, BOW+ALP,
</bodyText>
<table confidence="0.998558333333333">
Sentence Type % in Corpus
Family 28.8
Love 22.8
School 13.3
Work 14.3
Social 20.8
</table>
<tableCaption confidence="0.999267">
Table 2. Distribution of sentence types.
</tableCaption>
<equation confidence="0.53763375">
Conf
P(w1, ... wk)
k
(1)
∏
i
=
1
</equation>
<page confidence="0.993499">
203
</page>
<table confidence="0.9920695">
NB C4.5 SVM
BOW 0.717 0.741 0.787
BOW+ALP 0.745 0.755 0.804
BOW+ALP+Onto 0.759 0.766 0.815
</table>
<tableCaption confidence="0.935998">
Table 3. Accuracy of classifiers on testing data.
</tableCaption>
<figure confidence="0.996123461538461">
Accuracy
0.76
0.74
0.72
0.70
0.68
0.66
0.64
0.62
BOW+ALP
BOW+ALP+Onto
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold
</figure>
<figureCaption confidence="0.999996">
Figure 2. Threshold selection.
</figureCaption>
<bodyText confidence="0.9998836">
and BOW+ALP+Onto, to examine the effective-
ness of association language patterns. The classi-
fication performance is measured by accuracy,
i.e., the number of correctly classified sentences
divided by the total number of test sentences.
</bodyText>
<subsectionHeader confidence="0.975617">
4.1 Evaluation on threshold selection
</subsectionHeader>
<bodyText confidence="0.999508185185185">
Since not all discovered association language
patterns contribute to the classification task, the
threshold described in Section 2.2 is used to se-
lect top N percent patterns for classification. This
experiment is to determine an optimal threshold
for each involved classifier by maximizing its
classification accuracy on the development set.
Figure 2 shows the classification accuracy of NB
against different threshold values.
When using association language patterns as
features (BOW+ALP), the accuracy increased
with increasing the threshold value up to 0.6,
indicating that the top 60% discovered patterns
contained more useful patterns for classification.
By contrast, the accuracy decreased when the
threshold value was above 0.6, indicating that the
remaining 40% contained more noisy patterns
that may increase the ambiguity in classification.
When using the ontology expansion approach
(BOW+ALP+Onto), both the number and diver-
sity of discovered patterns are increased. There-
fore, the accuracy was improved and the optimal
accuracy was achieved at 0.5. However, the ac-
curacy dropped significantly when the threshold
value was above 0.5. This finding indicates that
expansion on noisy patterns may produce more
noisy patterns and thus decrease performance.
</bodyText>
<subsectionHeader confidence="0.960243">
4.2 Results of classification performance
</subsectionHeader>
<bodyText confidence="0.999965230769231">
The results of each classifier were obtained from
the test set using its own threshold optimized in
the previous section. Table 3 shows the compara-
tive results of different classifiers with different
levels of features. The incorporation of associa-
tion language patterns improved the accuracy of
NB, C4.5, and SVM by 3.9%, 1.9%, and 2.2%,
respectively, and achieved an average improve-
ment of 2.7%. Additionally, the use of ontology
expansion can further improve the performance
by 1.6% in average. This finding indicates that
association language patterns are significant fea-
tures for negative life event classification.
</bodyText>
<sectionHeader confidence="0.999448" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999994818181818">
This work has presented a framework that uses a
data mining algorithm and ontology expansion
method to acquire association language patterns
for negative life event classification. The asso-
ciation language patterns can capture word rela-
tionships in sentences, thus yielding higher per-
formance than the baseline system using single
words alone. Future work will focus on devising
a semi-supervised or unsupervised method for
language pattern acquisition from web resources
so as to reduce reliance on annotated corpora.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902074074074">
R. Agrawal and R. Srikant. 1994. Fast Algorithms for Min-
ing Association Rules. In Proc. Int’l Conf. Very Large
Data Bases (VLDB), pages 487-499.
Y. M. Bai, C. C. Lin, J. Y. Chen, and W. C. Liu. 2001. Vir-
tual Psychiatric Clinics. American Journal of Psychiatry,
vol. 158, no. 7, pp. 1160-1161.
E. M. Brostedt and N. L. Pedersen. 2003. Stressful Life
Events and Affective Illness. Acta Psychiatrica Scandi-
navica, vol. 107, pp. 208-215.
R. Chitturi and J. H.L. Hansen. 2008. Dialect Classification
for online podcasts fusing Acoustic and Language based
Structural and Semantic Information. In Proc. of ACL-08,
pages 21-24.
A. Khoo, Y. Marom and D. Albrecht. 2006. Experiments
with Sentence Classification. In Proc. of Australasian
Language Technology Workshop, pages 18-25.
S. Li and C. Zong. 2008. Multi-domain Sentiment Classifi-
cation. In Proc. of ACL-08, pages 257-260.
M. Naughton, N. Stokes, and J. Carthy. 2008. Investigating
Statistical Techniques for Sentence-Level Event Classi-
fication. In Proc. of COLING-08, pages 617-624.
F. Sebastiani. 2002. Machine Learning in Automated Text
Categorization. ACM Computing Surveys, vol. 34, no. 1,
pp. 1-47.
I. H. Witten and E. Frank. 2005. Data Mining: Practical
Machine Learning Tools and Techniques, 2nd Edition,
Morgan Kaufmann, San Francisco.
</reference>
<page confidence="0.998894">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.387319">
<title confidence="0.999604">Mining Association Language Patterns for Negative Life Event Classification</title>
<author confidence="0.470549">Chien-Lung Chung-Hsien</author>
<author confidence="0.470549">Chao-Cheng</author>
<note confidence="0.915381333333333">of Information Management, Yuan Ze University, Taiwan, R.O.C. of CSIE, National Cheng Kung University, Taiwan, R.O.C. of Psychiatry, National Taiwan University Hospital, Taiwan, R.O.C.</note>
<email confidence="0.977028">lcyu@saturn.yzu.edu.tw,chwu@csie.ncku.edu.tw,linchri@gmail.com</email>
<email confidence="0.977028">clchan@saturn.yzu.edu.tw,chwu@csie.ncku.edu.tw,linchri@gmail.com</email>
<abstract confidence="0.999608">Negative life events, such as death of a family member, argument with a spouse and loss of a job, play an important role in triggering depressive episodes. Therefore, it is worth to develop psychiatric services that can automatically identify such events. In this paper, we propose the use of association language patterns, i.e., meaningful combinations of words (e.g., &lt;loss, job&gt;), as features to classify sentences with negative life events into predefined categories (e.g., Family, Love, Work). The language patterns are discovered using a data mining algorithm, called association pattern mining, by incrementally associating frequently co-occurred words in the sentences annotated with negative life events. The discovered patterns are then combined with single words to train classifiers. Experimental results show that association language patterns are significant features, thus yielding better performance than the baseline system using single words alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>R Srikant</author>
</authors>
<title>Fast Algorithms for Mining Association Rules.</title>
<date>1994</date>
<booktitle>In Proc. Int’l Conf. Very Large Data Bases (VLDB),</booktitle>
<pages>487--499</pages>
<contexts>
<context position="6470" citStr="Agrawal and Srikant, 1994" startWordPosition="974" endWordPosition="977">on Language Pattern Mining The problem of language pattern acquisition can be converted into the problem of association pattern mining, where each sales transaction in a database can be considered as a sentence in the corpora, and each item in a transaction denotes a word in a sentence. An association language pattern is defined herein as a combination of multiple associated words, denoted by &lt; w1,...,wk &gt; . Thus, the task of association pattern mining is to mine the language patterns of frequently associated words from the training sentences. For this purpose, we adopt the Apriori algorithm (Agrawal and Srikant, 1994) and modified it slightly to fit our application. Its basic concept is to identify frequent word sets recursively, and then generate association language patterns from the frequent word sets. For simplicity, only the combinations of nouns and verbs are considered, and the length is restricted to at most 4 words, i.e., 2-word, 3-word and 4-word combinations. The detailed procedure is described as follows. 2.1 Find frequent word sets A word set is frequent if it possesses a minimum support. The support of a word set is defined as the number of training sentences containing the word set. For inst</context>
</contexts>
<marker>Agrawal, Srikant, 1994</marker>
<rawString>R. Agrawal and R. Srikant. 1994. Fast Algorithms for Mining Association Rules. In Proc. Int’l Conf. Very Large Data Bases (VLDB), pages 487-499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y M Bai</author>
<author>C C Lin</author>
<author>J Y Chen</author>
<author>W C Liu</author>
</authors>
<title>Virtual Psychiatric Clinics.</title>
<date>2001</date>
<journal>American Journal of Psychiatry,</journal>
<volume>158</volume>
<pages>1160--1161</pages>
<contexts>
<context position="10769" citStr="Bai et al., 2001" startWordPosition="1672" endWordPosition="1675">g the constituent words into their synonyms. For example, the pattern &lt;boss, conflict&gt; can be expanded as &lt;chief, conflict&gt; since the words boss and chief are synonyms. Here we use the HowNet (http://www.keenage.com), a Chinese lexical ontology, for pattern expansion. 4 Experimental Results Data set: A total of 2,856 sentences were collected from the Internet-based Self-assessment Program for Depression (ISP-D) database of the PsychPark (http://www.psychpark.org), a virtual psychiatric clinic, maintained by a group of volunteer professionals of Taiwan Association of Mental Health Informatics (Bai et al., 2001). Each sentence was then annotated by trained annotators with one of the five types of negative life events. Table 2 shows the break-down of the distribution of sentence types. The data set was randomly split into a training set, a development set, and a test set with an 8:1:1 ratio. The training set was used for language pattern generation. The development set was used to optimize the threshold (Section 2.2) for the classifiers (SVM, C4.5 and NB). Each classifier was implemented using three different levels of features, namely BOW, BOW+ALP, Sentence Type % in Corpus Family 28.8 Love 22.8 Scho</context>
</contexts>
<marker>Bai, Lin, Chen, Liu, 2001</marker>
<rawString>Y. M. Bai, C. C. Lin, J. Y. Chen, and W. C. Liu. 2001. Virtual Psychiatric Clinics. American Journal of Psychiatry, vol. 158, no. 7, pp. 1160-1161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Brostedt</author>
<author>N L Pedersen</author>
</authors>
<title>Stressful Life Events and Affective Illness.</title>
<date>2003</date>
<journal>Acta Psychiatrica Scandinavica,</journal>
<volume>107</volume>
<pages>208--215</pages>
<contexts>
<context position="2994" citStr="Brostedt and Pedersen (2003)" startWordPosition="441" endWordPosition="444">create a community discussion to share their experiences with each other. Additionally, a dialog system can generate supportive responses like “Don’t worry”, “That’s really sad” and “Cheer up” if it can understand the negative life events embedded in the example sentences shown in Table 1. Therefore, this study proposes a framework for negative life event classification. We formulate this problem as a sentence classification task; that is, classify sentences according to the type of negative life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to </context>
</contexts>
<marker>Brostedt, Pedersen, 2003</marker>
<rawString>E. M. Brostedt and N. L. Pedersen. 2003. Stressful Life Events and Affective Illness. Acta Psychiatrica Scandinavica, vol. 107, pp. 208-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chitturi</author>
<author>J H L Hansen</author>
</authors>
<title>Dialect Classification for online podcasts fusing Acoustic and Language based Structural and Semantic Information.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08,</booktitle>
<pages>21--24</pages>
<contexts>
<context position="3486" citStr="Chitturi and Hansen, 2008" startWordPosition="508" endWordPosition="511">e life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order n-grams require large training data to obtain reliable estimation. For our task, the expressions of negative life events can be characterized by association language patterns, i.e., meaningful combinations of words, such as &lt;worry, children, health&gt;, &lt;break up, boyfriend&gt;, &lt;argue, friend&gt;, &lt;loss, job&gt;, and 201 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201–204, Suntec, Si</context>
</contexts>
<marker>Chitturi, Hansen, 2008</marker>
<rawString>R. Chitturi and J. H.L. Hansen. 2008. Dialect Classification for online podcasts fusing Acoustic and Language based Structural and Semantic Information. In Proc. of ACL-08, pages 21-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Khoo</author>
<author>Y Marom</author>
<author>D Albrecht</author>
</authors>
<title>Experiments with Sentence Classification.</title>
<date>2006</date>
<booktitle>In Proc. of Australasian Language Technology Workshop,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="3064" citStr="Khoo et al., 2006" startWordPosition="451" endWordPosition="454">nally, a dialog system can generate supportive responses like “Don’t worry”, “That’s really sad” and “Cheer up” if it can understand the negative life events embedded in the example sentences shown in Table 1. Therefore, this study proposes a framework for negative life event classification. We formulate this problem as a sentence classification task; that is, classify sentences according to the type of negative life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance depende</context>
</contexts>
<marker>Khoo, Marom, Albrecht, 2006</marker>
<rawString>A. Khoo, Y. Marom and D. Albrecht. 2006. Experiments with Sentence Classification. In Proc. of Australasian Language Technology Workshop, pages 18-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>C Zong</author>
</authors>
<title>Multi-domain Sentiment Classification.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08,</booktitle>
<pages>257--260</pages>
<contexts>
<context position="3506" citStr="Li and Zong, 2008" startWordPosition="512" endWordPosition="515">The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order n-grams require large training data to obtain reliable estimation. For our task, the expressions of negative life events can be characterized by association language patterns, i.e., meaningful combinations of words, such as &lt;worry, children, health&gt;, &lt;break up, boyfriend&gt;, &lt;argue, friend&gt;, &lt;loss, job&gt;, and 201 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201–204, Suntec, Singapore, 4 August 20</context>
</contexts>
<marker>Li, Zong, 2008</marker>
<rawString>S. Li and C. Zong. 2008. Multi-domain Sentiment Classification. In Proc. of ACL-08, pages 257-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Naughton</author>
<author>N Stokes</author>
<author>J Carthy</author>
</authors>
<title>Investigating Statistical Techniques for Sentence-Level Event Classification.</title>
<date>2008</date>
<booktitle>In Proc. of COLING-08,</booktitle>
<pages>617--624</pages>
<contexts>
<context position="3088" citStr="Naughton et al., 2008" startWordPosition="455" endWordPosition="458">tem can generate supportive responses like “Don’t worry”, “That’s really sad” and “Cheer up” if it can understand the negative life events embedded in the example sentences shown in Table 1. Therefore, this study proposes a framework for negative life event classification. We formulate this problem as a sentence classification task; that is, classify sentences according to the type of negative life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order</context>
</contexts>
<marker>Naughton, Stokes, Carthy, 2008</marker>
<rawString>M. Naughton, N. Stokes, and J. Carthy. 2008. Investigating Statistical Techniques for Sentence-Level Event Classification. In Proc. of COLING-08, pages 617-624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine Learning in Automated Text Categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<pages>1--47</pages>
<contexts>
<context position="3129" citStr="Sebastiani 2002" startWordPosition="462" endWordPosition="463">’t worry”, “That’s really sad” and “Cheer up” if it can understand the negative life events embedded in the example sentences shown in Table 1. Therefore, this study proposes a framework for negative life event classification. We formulate this problem as a sentence classification task; that is, classify sentences according to the type of negative life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order n-grams require large training data to o</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine Learning in Automated Text Categorization. ACM Computing Surveys, vol. 34, no. 1, pp. 1-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques, 2nd Edition,</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="9857" citStr="Witten and Frank, 2005" startWordPosition="1541" endWordPosition="1544">equent word sets are sorted in the descending order of the mutual information scores. The minimum confidence (a threshold at percentage) is then applied to select top N percent frequent word sets as the resulting language patterns. This threshold is determined empirically by maximizing classification performance (Section 4). Figure 1 (righthand side) shows an example of generating the association language patterns from Lk. 3 Sentence Classification The classifiers used in this study include Support Vector Machine (SVM), C4.5, and Naïve Bayes (NB) classifier, which is provided by Weka Package (Witten and Frank, 2005). The feature set includes: Bag-of-Words (BOW): Each single word in sentences. Association language patterns (ALP): The top N percent association language patterns acquired in the previous section. Ontology expansion (Onto): The top N percent association language patterns are expanded by mapping the constituent words into their synonyms. For example, the pattern &lt;boss, conflict&gt; can be expanded as &lt;chief, conflict&gt; since the words boss and chief are synonyms. Here we use the HowNet (http://www.keenage.com), a Chinese lexical ontology, for pattern expansion. 4 Experimental Results Data set: A t</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. H. Witten and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, 2nd Edition, Morgan Kaufmann, San Francisco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>