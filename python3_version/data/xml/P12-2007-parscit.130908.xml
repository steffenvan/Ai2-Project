<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010330">
<title confidence="0.970848">
Head-Driven Hierarchical Phrase-based Translation
</title>
<author confidence="0.99598">
Junhui Li Zhaopeng Tut Guodong Zhout Josef van Genabith
</author>
<affiliation confidence="0.972180333333333">
Centre for Next Generation Localisation
School of Computing, Dublin City University
t Key Lab. of Intelligent Info. Processing
Institute of Computing Technology, Chinese Academy of Sciences
tSchool of Computer Science and Technology
Soochow University, China
</affiliation>
<email confidence="0.9852395">
{jli,josef}@computing.dcu.ie
tuzhaopeng@ict.ac.cn gdzhou@suda.edu.cn
</email>
<sectionHeader confidence="0.995447" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999756230769231">
This paper presents an extension of Chi-
ang’s hierarchical phrase-based (HPB) model,
called Head-Driven HPB (HD-HPB), which
incorporates head information in translation
rules to better capture syntax-driven infor-
mation, as well as improved reordering be-
tween any two neighboring non-terminals at
any stage of a derivation to explore a larger
reordering search space. Experiments on
Chinese-English translation on four NIST MT
test sets show that the HD-HPB model signifi-
cantly outperforms Chiang’s model with aver-
age gains of 1.91 points absolute in BLEU.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999484125">
Chiang’s hierarchical phrase-based (HPB) transla-
tion model utilizes synchronous context free gram-
mar (SCFG) for translation derivation (Chiang,
2005; Chiang, 2007) and has been widely adopted
in statistical machine translation (SMT). Typically,
such models define two types of translation rules:
hierarchical (translation) rules which consist of both
terminals and non-terminals, and glue (grammar)
rules which combine translated phrases in a mono-
tone fashion. Due to lack of linguistic knowledge,
Chiang’s HPB model contains only one type of non-
terminal symbol X, often making it difficult to se-
lect the most appropriate translation rules.1 What
is more, Chiang’s HPB model suffers from limited
phrase reordering combining translated phrases in a
monotonic way with glue rules. In addition, once a
</bodyText>
<footnote confidence="0.442981">
1Another non-terminal symbol S is used in glue rules.
</footnote>
<bodyText confidence="0.998907529411765">
glue rule is adopted, it requires all rules above it to
be glue rules.
One important research question is therefore how
to refine the non-terminal category X using linguis-
tically motivated information: Zollmann and Venu-
gopal (2006) (SAMT) e.g. use (partial) syntactic
categories derived from CFG trees while Zollmann
and Vogel (2011) use word tags, generated by ei-
ther POS analysis or unsupervised word class in-
duction. Almaghout et al. (2011) employ CCG-
based supertags. Mylonakis and Sima’an (2011) use
linguistic information of various granularities such
as Phrase-Pair, Constituent, Concatenation of Con-
stituents, and Partial Constituents, where applica-
ble. Inspired by previous work in parsing (Char-
niak, 2000; Collins, 2003), our Head-Driven HPB
(HD-HPB) model is based on the intuition that lin-
guistic heads provide important information about a
constituent or distributionally defined fragment, as
in HPB. We identify heads using linguistically mo-
tivated dependency parsing, and use their POS to
refine X. In addition HD-HPB provides flexible re-
ordering rules freely mixing translation and reorder-
ing (including swap) at any stage in a derivation.
Different from the soft constraint modeling
adopted in (Chan et al., 2007; Marton and Resnik,
2008; Shen et al., 2009; He et al., 2010; Huang et
al., 2010; Gao et al., 2011), our approach encodes
syntactic information in translation rules. However,
the two approaches are not mutually exclusive, as
we could also include a set of syntax-driven features
into our translation model. Our approach maintains
the advantages of Chiang’s HPB model while at the
same time incorporating head information and flex-
</bodyText>
<page confidence="0.9937">
33
</page>
<note confidence="0.8096855">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.898433">
root
Eight European countries jointly support America’s stand
</figure>
<figureCaption confidence="0.9942215">
Figure 1: An example word alignment for a Chinese-
English sentence pair with the dependency parse tree for
the Chinese sentence. Here, each Chinese word is at-
tached with its POS tag and Pinyin.
</figureCaption>
<bodyText confidence="0.964937">
ible reordering in a derivation in a natural way. Ex-
periments on Chinese-English translation using four
NIST MT test sets show that our HD-HPB model
significantly outperforms Chiang’s HPB as well as a
SAMT-style refined version of HPB.
</bodyText>
<sectionHeader confidence="0.984076" genericHeader="method">
2 Head-Driven HPB Translation Model
</sectionHeader>
<bodyText confidence="0.973985714285714">
Like Chiang (2005) and Chiang (2007), our HD-
HPB translation model adopts a synchronous con-
text free grammar, a rewriting system which gen-
erates source and target side string pairs simulta-
neously using a context-free grammar. Instead of
collapsing all non-terminals in the source language
into a single symbol X as in Chiang (2007), given a
word sequence fi jfrom position i to position j, we
first find heads and then concatenate the POS tags
of these heads as fij’s non-terminal symbol. Specif-
ically, we adopt unlabeled dependency structure to
derive heads, which are defined as:
Definition 1. For word sequence fij, word
fk (i G k G j) is regarded as a head if it is domi-
nated by a word outside of this sequence.
Note that this definition (i) allows for a word se-
quence to have one or more heads (largely due to
the fact that a word sequence is not necessarily lin-
guistically constrained) and (ii) ensures that heads
are always the highest heads in the sequence from a
dependency structure perspective. For example, the
word sequence ouzhou baguo lianming in Figure 1
has two heads (i.e., baguo and lianming, ouzhou is
not a head of this sequence since its headword baguo
falls within this sequence) and the non-terminal cor-
responding to the sequence is thus labeled as NN-
AD. It is worth noting that in this paper we only
refine non-terminal X on the source side to head-
informed ones, while still using X on the target side.
According to the occurrence of terminals in
translation rules, we group rules in the HD-HPB
model into two categories: head-driven hierarchical
rules (HD-HRs) and non-terminal reordering rules
(NRRs), where the former have at least one terminal
on both source and target sides and the later have no
terminals. For rule extraction, we first identify ini-
tial phrase pairs on word-aligned sentence pairs by
using the same criterion as most phrase-based trans-
lation models (Och and Ney, 2004) and Chiang’s
HPB model (Chiang, 2005; Chiang, 2007). We
extract HD-HRs and NRRs based on initial phrase
pairs, respectively.
</bodyText>
<subsectionHeader confidence="0.991005">
2.1 HD-HRs: Head-Driven Hierarchical Rules
</subsectionHeader>
<bodyText confidence="0.999927333333333">
As mentioned, a HD-HR has at least one terminal
on both source and target sides. This is the same
as the hierarchical rules defined in Chiang’s HPB
model (Chiang, 2007), except that we use head POS-
informed non-terminal symbols in the source lan-
guage. We look for initial phrase pairs that contain
other phrases and then replace sub-phrases with POS
tags corresponding to their heads. Given the word
alignment in Figure 1, Table 1 demonstrates the dif-
ference between hierarchical rules in Chiang (2007)
and HD-HRs defined here.
Similar to Chiang’s HPB model, our HD-HPB
model will result in a large number of rules causing
problems in decoding. To alleviate these problems,
we filter our HD-HRs according to the same con-
straints as described in Chiang (2007). Moreover,
we discard rules that have non-terminals with more
than four heads.
</bodyText>
<subsectionHeader confidence="0.993773">
2.2 NRRs: Non-terminal Reordering Rules
</subsectionHeader>
<bodyText confidence="0.999920166666667">
NRRs are translation rules without terminals. Given
an initial phrase pair on the source side, there are
four possible positional relationships for their target
side translations (we use Y as a variable for non-
terminals on the source side while all non-terminals
on the target side are labeled as X):
</bodyText>
<listItem confidence="0.9820494">
• Monotone (Y — Y1Y2, X — X1X2);
• Discontinuous monotone
(Y — Y1Y2, X — X1 ... X2);
• Swap (Y — Y1Y2, X — X2X1);
• Discontinuous swap
</listItem>
<equation confidence="0.697623">
(Y — Y1Y2, X — X2 ... X1).
</equation>
<figure confidence="0.968609083333333">
KtM/NR
Ouzhou
AM/NN
baguo
A*/NN
lichang
JK-VAD
lianming
�cWVV
zhichi
�M/NR
meiguo
</figure>
<page confidence="0.973688">
34
</page>
<table confidence="0.96242">
phrase pairs hierarchical rule head-driven hierarchical rule
NN→lichang,
lichang, stand X→lichang, stand X→stand
NN→meiguo NN1,
meiguo lichang1, America’s stand1 X→meiguo X1, America’s X1 X→America’s X1
VV-NR→zhichi meiguo,
zhichi meiguo, support America’s X→zhichi meiguo, support America’s
X→support America’s
zhichi meiguo1 lichang, X→X1 lichang, VV→VV-NR1 lichang,
X1 stand X→X1 stand
support America’s1 stand
</table>
<tableCaption confidence="0.970061">
Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases
and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)
POS tags of the corresponding word sequence in the source language.
</tableCaption>
<bodyText confidence="0.999020875">
Merging two neighboring non-terminals into a
single non-terminal, NRRs enable the translation
model to explore a wider search space. During train-
ing, we extract four types of NRRs and calculate
probabilities for each type. To speed up decoding,
we currently (i) only use monotone and swap NRRs
and (ii) limit the number of non-terminals in a NRR
to 2.
</bodyText>
<subsectionHeader confidence="0.961787">
2.3 Features and Decoding
</subsectionHeader>
<bodyText confidence="0.9974762">
Given e for the translation output in the target lan-
guage, s and t for strings of terminals and non-
terminals on the source and target side, respectively,
we use a feature set analogous to the default feature
set of Chiang (2007), including:
</bodyText>
<listItem confidence="0.999912222222222">
• Phd-hr (t|s) and Phd-hr (s|t), translation probabili-
ties for HD-HRs;
• Ple. (t|s) and Ple. (s|t), lexical translation proba-
bilities for HD-HRs;
• Ptyhd-hr = exp (−1), rule penalty for HD-HRs;
• Pnrr (t|s), translation probability for NRRs;
• Ptynrr = exp (−1), rule penalty for NRRs;
• Pl,n (e), language model;
• Ptyword (e) = exp (−|e|), word penalty.
</listItem>
<bodyText confidence="0.999959636363636">
Our decoder is based on CKY-style chart parsing
with beam search and searches for the best deriva-
tion bottom-up. For a source span [i, j], it applies
both types of HD-HRs and NRRs. However, HD-
HRs are only applied to generate derivations span-
ning no more than K words – the initial phrase
length limit used in training to extract HD-HRs –
while NRRs are applied to derivations spanning any
length. Unlike in Chiang’s HPB model, it is pos-
sible for a non-terminal generated by a NRR to be
included afterwards by a HD-HR or another NRR.
</bodyText>
<sectionHeader confidence="0.999574" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999973166666667">
We evaluate the performance of our HD-HPB model
and compare it with our implementation of Chiang’s
HPB model (Chiang, 2007), a source-side SAMT-
style refined version of HPB (SAMT-HPB), and the
Moses implementation of HPB. For fair compari-
son, we adopt the same parameter settings for our
HD-HPB and HPB systems, including initial phrase
length (as 10) in training, the maximum number of
non-terminals (as 2) in translation rules, maximum
number of non-terminals plus terminals (as 5) on
the source, beam threshold β (as 10−5) (to discard
derivations with a score worse than β times the best
score in the same chart cell), beam size b (as 200)
(i.e. each chart cell contains at most b derivations).
For Moses HPB, we use “grow-diag-final-and” to
obtain symmetric word alignments, 10 for the max-
imum phrase length, and the recommended default
values for all other parameters.
We train our model on a dataset with ˜1.5M sen-
tence pairs from the LDC dataset.2 We use the
2002 NIST MT evaluation test data (878 sentence
pairs) as the development data, and the 2003, 2004,
2005, 2006-news NIST MT evaluation test data
(919, 1788, 1082, and 616 sentence pairs, respec-
tively) as the test data. To find heads, we parse the
source sentences with the Berkeley Parser3 (Petrov
and Klein, 2007) trained on Chinese TreeBank 6.0
and use the Penn2Malt toolkit4 to obtain (unlabeled)
dependency structures.
We obtain the word alignments by running
</bodyText>
<footnote confidence="0.9964664">
2This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
3http://code.google.com/p/berkeleyparser/
4http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/
</footnote>
<page confidence="0.999455">
35
</page>
<bodyText confidence="0.999713422222222">
GIZA++ (Och and Ney, 2000) on the corpus in both
directions and applying “grow-diag-final-and” re-
finement (Koehn et al., 2003). We use the SRI lan-
guage modeling toolkit to train a 5-gram language
model on the Xinhua portion of the Gigaword corpus
and standard MERT (Och, 2003) to tune the feature
weights on the development data.
For evaluation, the NIST BLEU script (version
12) with the default settings is used to calculate the
BLEU scores. To test whether a performance differ-
ence is statistically significant, we conduct signifi-
cance tests following the paired bootstrap approach
(Koehn, 2004). In this paper, ‘**’ and ‘*’ de-
note p-values less than 0.01 and in-between [0.01,
0.05), respectively.
Table 2 lists the rule table sizes. The full rule ta-
ble size (including HD-HRs and NRRs) of our HD-
HPB model is &amp;quot;1.5 times that of Chiang’s, largely
due to refining the non-terminal symbol X in Chi-
ang’s model into head-informed ones in our model.
It is also unsurprising, that the test set-filtered rule
table size of our model is only &amp;quot;0.7 times that of Chi-
ang’s: this is due to the fact that some of the refined
translation rule patterns required by the test set are
unattested in the training data. Furthermore, the rule
table size of NRRs is much smaller than that of HD-
HRs since a NRR contains only two non-terminals.
Table 3 lists the translation performance with
BLEU scores. Note that our re-implementation of
Chiang’s original HPB model performs on a par with
Moses HPB. Table 3 shows that our HD-HPB model
significantly outperforms Chiang’s HPB model with
an average improvement of 1.91 in BLEU (and sim-
ilar improvements over Moses HPB).
Table 3 shows that the head-driven scheme out-
performs a SAMT-style approach (for each test set
p &lt; 0.01), indicating that head information is more
effective than (partial) CFG categories. Taking lian-
ming zhichi in Figure 1 as an example, HD-HPB
labels the span VV, as lianming is dominated by
zhichi, effecively ignoring lianming in the transla-
tion rule, while the SAMT label is ADVP:AD+VV5
which is more susceptible to data sparsity. In addi-
tion, SAMT resorts to X if a text span fails to satisify
pre-defined categories. Examining initial phrases
</bodyText>
<footnote confidence="0.9229475">
5the constituency structure for lianming zhichi is (VP (ADVP
(AD lianming)) (VP (VV zhichi) ...)).
</footnote>
<table confidence="0.993128">
System Total MT 03 MT 04 MT 05 MT 06 Avg.
HPB 39.6 2.8 4.7 3.3 3.0 3.4
HD-HPB 59.5/0.6 1.9/0.1 3.4/0.2 2.3/0.2 2.0/0.1 2.4/0.2
</table>
<tableCaption confidence="0.9612158">
Table 2: Rule table sizes (in million) of different mod-
els. Note: 1) For HD-HPB, the rule sizes separated by /
indicate HD-HRs and NRRs, respectively; 2) Except for
“Total”, the figures correspond to rules filtered on the cor-
responding test set.
</tableCaption>
<table confidence="0.999695333333333">
System MT 03 MT 04 MT 05 MT 06 Avg.
Moses HPB 32.94* 35.16 32.18 29.88* 32.54
HPB 33.59 35.39 32.20 30.60 32.95
HD-HPB 35.50** 37.61** 34.56** 31.78** 34.86
SAMT-HPB 34.07 36.52** 32.90* 30.66 33.54
HD-HR+Glue 34.58** 36.55** 33.84** 31.06 34.01
</table>
<tableCaption confidence="0.998453">
Table 3: BLEU (%) scores of different models. Note:
</tableCaption>
<reference confidence="0.538894">
1) SAMT-HPB indicates our HD-HPB model with non-
terminal scheme of Zollmann and Venugopal (2006);
2) HD-HR+Glue indicates our HD-HPB model replac-
ing NRRs with glue rules; 3) Significance tests for
Moses HPB, HD-HPB, SAMT-HPB, and HD-HR+Glue
are done against HPB.
</reference>
<bodyText confidence="0.986377894736842">
extracted from the SAMT training data shows that
28% of them are labeled as X.
In order to separate out the individual contribu-
tions of the novel HD-HRs and NRRs, we carry out
an additional experiment (HD-HR+Glue) using HD-
HRs with monotonic glue rules only (adjusted to re-
fined rule labels, but effectively switching off the ex-
tra reordering power of full NRRs). Table 3 shows
that on average more than half of the improvement
over HPB (Chiang and Moses) comes from the re-
fined HD-HRs, the rest from NRRs.
Examining translation rules extracted from the
training data shows that there are 72,366 types of
non-terminals with respect to 33 types of POS tags.
On average each sentence employs 16.6/5.2 HD-
HRs/NRRs in our HD-HPB model, compared to
15.9/3.6 hierarchical rules/glue rules in Chiang’s
model, providing further indication of the impor-
tance of NRRs in translation.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999238">
We present a head-driven hierarchical phrase-based
(HD-HPB) translation model, which adopts head in-
formation (derived through unlabeled dependency
analysis) in the definition of non-terminals to bet-
ter differentiate among translation rules. In ad-
</bodyText>
<page confidence="0.993686">
36
</page>
<bodyText confidence="0.999965125">
dition, improved and better integrated reordering
rules allow better reordering between consecutive
non-terminals through exploration of a larger search
space in the derivation. Experimental results on
Chinese-English translation across four test sets
demonstrate significant improvements of the HD-
HPB model over both Chiang’s HPB and a source-
side SAMT-style refined version of HPB.
</bodyText>
<sectionHeader confidence="0.997386" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999581777777778">
This work was supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. It was also partially
supported by Project 90920004 under the National
Natural Science Foundation of China and Project
2012AA011102 under the “863” National High-
Tech Research and Development of China. We
thank the reviewers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.998085" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999695246376812">
Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG
contextual labels in hierarchical phrase-based SMT. In
Proceedings of EAMT 2011, pages 281–288.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL 2007, pages
33–40.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132–
139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263–270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589–637.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857–868.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Maxi-
mum entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of EMNLP
2010, pages 555–563.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of EMNLP 2010, pages 138–
147.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48–54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings ofACL-HLT 2008, pages 1003–1011.
Markos Mylonakis and Khalil Sima’an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings ofACL-HLT 2011, pages 642–
652.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440–447.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160–167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404–411.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of EMNLP 2009, pages
72–80.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL 2006 - Workshop on Statistical
Machine Translation, pages 138–141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proceedings of ACL-HLT 2011, pages
1–11.
</reference>
<page confidence="0.999605">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.664081">
<title confidence="0.999839">Head-Driven Hierarchical Phrase-based Translation</title>
<author confidence="0.999684">Li Zhaopeng van</author>
<affiliation confidence="0.998589166666667">Centre for Next Generation School of Computing, Dublin City Lab. of Intelligent Info. Institute of Computing Technology, Chinese Academy of of Computer Science and Soochow University,</affiliation>
<email confidence="0.72887">tuzhaopeng@ict.ac.cngdzhou@suda.edu.cn</email>
<abstract confidence="0.994023357142857">This paper presents an extension of Chiang’s hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>1) SAMT-HPB indicates our HD-HPB model with nonterminal scheme of Zollmann and Venugopal (2006); 2) HD-HR+Glue indicates our HD-HPB model replacing NRRs with glue rules; 3) Significance tests for Moses HPB, HD-HPB, SAMT-HPB, and HD-HR+Glue are done against HPB.</title>
<marker></marker>
<rawString>1) SAMT-HPB indicates our HD-HPB model with nonterminal scheme of Zollmann and Venugopal (2006); 2) HD-HR+Glue indicates our HD-HPB model replacing NRRs with glue rules; 3) Significance tests for Moses HPB, HD-HPB, SAMT-HPB, and HD-HR+Glue are done against HPB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hala Almaghout</author>
<author>Jie Jiang</author>
<author>Andy Way</author>
</authors>
<title>CCG contextual labels in hierarchical phrase-based SMT.</title>
<date>2011</date>
<booktitle>In Proceedings of EAMT 2011,</booktitle>
<pages>281--288</pages>
<contexts>
<context position="2309" citStr="Almaghout et al. (2011)" startWordPosition="331" endWordPosition="334">l suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1Another non-terminal symbol S is used in glue rules. glue rule is adopted, it requires all rules above it to be glue rules. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides fl</context>
</contexts>
<marker>Almaghout, Jiang, Way, 2011</marker>
<rawString>Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG contextual labels in hierarchical phrase-based SMT. In Proceedings of EAMT 2011, pages 281–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>33--40</pages>
<contexts>
<context position="3094" citStr="Chan et al., 2007" startWordPosition="447" endWordPosition="450">ents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for C</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of ACL 2007, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL 2000,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="2579" citStr="Charniak, 2000" startWordPosition="369" endWordPosition="371">ion is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Ga</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL 2000, pages 132– 139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1161" citStr="Chiang, 2005" startWordPosition="154" endWordPosition="155"> (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. 1 Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a </context>
<context position="4264" citStr="Chiang (2005)" startWordPosition="633" endWordPosition="634">a, 8-14 July 2012. c�2012 Association for Computational Linguistics root Eight European countries jointly support America’s stand Figure 1: An example word alignment for a ChineseEnglish sentence pair with the dependency parse tree for the Chinese sentence. Here, each Chinese word is attached with its POS tag and Pinyin. ible reordering in a derivation in a natural way. Experiments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang’s HPB as well as a SAMT-style refined version of HPB. 2 Head-Driven HPB Translation Model Like Chiang (2005) and Chiang (2007), our HDHPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007), given a word sequence fi jfrom position i to position j, we first find heads and then concatenate the POS tags of these heads as fij’s non-terminal symbol. Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as: Definition 1. For word sequence fij, </context>
<context position="6198" citStr="Chiang, 2005" startWordPosition="966" endWordPosition="967">nal X on the source side to headinformed ones, while still using X on the target side. According to the occurrence of terminals in translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004) and Chiang’s HPB model (Chiang, 2005; Chiang, 2007). We extract HD-HRs and NRRs based on initial phrase pairs, respectively. 2.1 HD-HRs: Head-Driven Hierarchical Rules As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang’s HPB model (Chiang, 2007), except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1, Table 1 demonstrates the difference between hi</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL 2005, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1176" citStr="Chiang, 2007" startWordPosition="156" endWordPosition="157">ch incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. 1 Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way w</context>
<context position="4282" citStr="Chiang (2007)" startWordPosition="636" endWordPosition="637"> c�2012 Association for Computational Linguistics root Eight European countries jointly support America’s stand Figure 1: An example word alignment for a ChineseEnglish sentence pair with the dependency parse tree for the Chinese sentence. Here, each Chinese word is attached with its POS tag and Pinyin. ible reordering in a derivation in a natural way. Experiments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang’s HPB as well as a SAMT-style refined version of HPB. 2 Head-Driven HPB Translation Model Like Chiang (2005) and Chiang (2007), our HDHPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007), given a word sequence fi jfrom position i to position j, we first find heads and then concatenate the POS tags of these heads as fij’s non-terminal symbol. Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as: Definition 1. For word sequence fij, word fk (i G k G j</context>
<context position="6213" citStr="Chiang, 2007" startWordPosition="968" endWordPosition="969">ource side to headinformed ones, while still using X on the target side. According to the occurrence of terminals in translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004) and Chiang’s HPB model (Chiang, 2005; Chiang, 2007). We extract HD-HRs and NRRs based on initial phrase pairs, respectively. 2.1 HD-HRs: Head-Driven Hierarchical Rules As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang’s HPB model (Chiang, 2007), except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1, Table 1 demonstrates the difference between hierarchical rule</context>
<context position="8227" citStr="Chiang (2007)" startWordPosition="1289" endWordPosition="1290"> X2X1); • Discontinuous swap (Y — Y1Y2, X — X2 ... X1). KtM/NR Ouzhou AM/NN baguo A*/NN lichang JK-VAD lianming �cWVV zhichi �M/NR meiguo 34 phrase pairs hierarchical rule head-driven hierarchical rule NN→lichang, lichang, stand X→lichang, stand X→stand NN→meiguo NN1, meiguo lichang1, America’s stand1 X→meiguo X1, America’s X1 X→America’s X1 VV-NR→zhichi meiguo, zhichi meiguo, support America’s X→zhichi meiguo, support America’s X→support America’s zhichi meiguo1 lichang, X→X1 lichang, VV→VV-NR1 lichang, X1 stand X→X1 stand support America’s1 stand Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s) POS tags of the corresponding word sequence in the source language. Merging two neighboring non-terminals into a single non-terminal, NRRs enable the translation model to explore a wider search space. During training, we extract four types of NRRs and calculate probabilities for each type. To speed up decoding, we currently (i) only use monotone and swap NRRs and (ii) limit the number of non-terminals in a NRR to 2. 2.3 Features and </context>
<context position="10103" citStr="Chiang, 2007" startWordPosition="1607" endWordPosition="1608"> search and searches for the best derivation bottom-up. For a source span [i, j], it applies both types of HD-HRs and NRRs. However, HDHRs are only applied to generate derivations spanning no more than K words – the initial phrase length limit used in training to extract HD-HRs – while NRRs are applied to derivations spanning any length. Unlike in Chiang’s HPB model, it is possible for a non-terminal generated by a NRR to be included afterwards by a HD-HR or another NRR. 3 Experiments We evaluate the performance of our HD-HPB model and compare it with our implementation of Chiang’s HPB model (Chiang, 2007), a source-side SAMTstyle refined version of HPB (SAMT-HPB), and the Moses implementation of HPB. For fair comparison, we adopt the same parameter settings for our HD-HPB and HPB systems, including initial phrase length (as 10) in training, the maximum number of non-terminals (as 2) in translation rules, maximum number of non-terminals plus terminals (as 5) on the source, beam threshold β (as 10−5) (to discard derivations with a score worse than β times the best score in the same chart cell), beam size b (as 200) (i.e. each chart cell contains at most b derivations). For Moses HPB, we use “gro</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="2595" citStr="Collins, 2003" startWordPosition="372" endWordPosition="373"> how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011),</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Gao</author>
<author>Philipp Koehn</author>
<author>Alexandra Birch</author>
</authors>
<title>Soft dependency constraints for reordering in hierarchical phrase-based translation.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP 2011,</booktitle>
<pages>857--868</pages>
<contexts>
<context position="3194" citStr="Gao et al., 2011" startWordPosition="467" endWordPosition="470">00; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics root Eight European countries jointly support America’s stand Figure 1: An </context>
</contexts>
<marker>Gao, Koehn, Birch, 2011</marker>
<rawString>Yang Gao, Philipp Koehn, and Alexandra Birch. 2011. Soft dependency constraints for reordering in hierarchical phrase-based translation. In Proceedings of EMNLP 2011, pages 857–868.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Yao Meng</author>
<author>Hao Yu</author>
</authors>
<title>Maximum entropy based phrase reordering for hierarchical phrase-based translation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>555--563</pages>
<contexts>
<context position="3155" citStr="He et al., 2010" startWordPosition="459" endWordPosition="462">revious work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics root Eight European countries jointl</context>
</contexts>
<marker>He, Meng, Yu, 2010</marker>
<rawString>Zhongjun He, Yao Meng, and Hao Yu. 2010. Maximum entropy based phrase reordering for hierarchical phrase-based translation. In Proceedings of EMNLP 2010, pages 555–563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Martin Cmejrek</author>
<author>Bowen Zhou</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="3175" citStr="Huang et al., 2010" startWordPosition="463" endWordPosition="466">arsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics root Eight European countries jointly support America’s </context>
</contexts>
<marker>Huang, Cmejrek, Zhou, 2010</marker>
<rawString>Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou. 2010. Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions. In Proceedings of EMNLP 2010, pages 138– 147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>48--54</pages>
<contexts>
<context position="11751" citStr="Koehn et al., 2003" startWordPosition="1859" endWordPosition="1862">respectively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3http://code.google.com/p/berkeleyparser/ 4http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ 35 GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL 2003, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="12223" citStr="Koehn, 2004" startWordPosition="1938" endWordPosition="1939">Malt.html/ 35 GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule table sizes. The full rule table size (including HD-HRs and NRRs) of our HDHPB model is &amp;quot;1.5 times that of Chiang’s, largely due to refining the non-terminal symbol X in Chiang’s model into head-informed ones in our model. It is also unsurprising, that the test set-filtered rule table size of our model is only &amp;quot;0.7 times that of Chiang’s: this is due to the fact that some of the refined translation rule patterns required by the test set are unattested in the training d</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="3119" citStr="Marton and Resnik, 2008" startWordPosition="451" endWordPosition="454">onstituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics </context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings ofACL-HLT 2008, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Learning hierarchical translation structure with linguistic annotations.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL-HLT 2011,</booktitle>
<pages>642--652</pages>
<marker>Mylonakis, Sima’an, 2011</marker>
<rawString>Markos Mylonakis and Khalil Sima’an. 2011. Learning hierarchical translation structure with linguistic annotations. In Proceedings ofACL-HLT 2011, pages 642– 652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>440--447</pages>
<contexts>
<context position="11651" citStr="Och and Ney, 2000" startWordPosition="1844" endWordPosition="1847">2003, 2004, 2005, 2006-news NIST MT evaluation test data (919, 1788, 1082, and 616 sentence pairs, respectively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3http://code.google.com/p/berkeleyparser/ 4http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ 35 GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL 2000, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="6161" citStr="Och and Ney, 2004" startWordPosition="958" endWordPosition="961">that in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side. According to the occurrence of terminals in translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004) and Chiang’s HPB model (Chiang, 2005; Chiang, 2007). We extract HD-HRs and NRRs based on initial phrase pairs, respectively. 2.1 HD-HRs: Head-Driven Hierarchical Rules As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang’s HPB model (Chiang, 2007), except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1, Table 1 d</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11901" citStr="Och, 2003" startWordPosition="1888" endWordPosition="1889"> and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3http://code.google.com/p/berkeleyparser/ 4http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ 35 GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule table sizes. The full rule table size (including HD-HRs and NRRs) of our HDHPB model is &amp;quot;1.5 times that of Chiang’s, largely due to refining the non-te</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>404--411</pages>
<contexts>
<context position="11259" citStr="Petrov and Klein, 2007" startWordPosition="1801" endWordPosition="1804">rt cell contains at most b derivations). For Moses HPB, we use “grow-diag-final-and” to obtain symmetric word alignments, 10 for the maximum phrase length, and the recommended default values for all other parameters. We train our model on a dataset with ˜1.5M sentence pairs from the LDC dataset.2 We use the 2002 NIST MT evaluation test data (878 sentence pairs) as the development data, and the 2003, 2004, 2005, 2006-news NIST MT evaluation test data (919, 1788, 1082, and 616 sentence pairs, respectively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3http://code.google.com/p/berkeleyparser/ 4http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ 35 GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gig</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL 2007, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>72--80</pages>
<contexts>
<context position="3138" citStr="Shen et al., 2009" startWordPosition="455" endWordPosition="458">able. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics root Eight European</context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic and contextual information for statistical machine translation. In Proceedings of EMNLP 2009, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL 2006 - Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="2097" citStr="Zollmann and Venugopal (2006)" startWordPosition="296" endWordPosition="300">hion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1Another non-terminal symbol S is used in glue rules. glue rule is adopted, it requires all rules above it to be glue rules. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of NAACL 2006 - Workshop on Statistical Machine Translation, pages 138–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
</authors>
<title>A wordclass approach to labeling PSCFG rules for machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT 2011,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="2199" citStr="Zollmann and Vogel (2011)" startWordPosition="312" endWordPosition="315"> X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1Another non-terminal symbol S is used in glue rules. glue rule is adopted, it requires all rules above it to be glue rules. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads us</context>
</contexts>
<marker>Zollmann, Vogel, 2011</marker>
<rawString>Andreas Zollmann and Stephan Vogel. 2011. A wordclass approach to labeling PSCFG rules for machine translation. In Proceedings of ACL-HLT 2011, pages 1–11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>