<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007408">
<title confidence="0.989015">
Semisupervised condensed nearest neighbor for part-of-speech tagging
</title>
<author confidence="0.994536">
Anders Søgaard
</author>
<affiliation confidence="0.9970095">
Center for Language Technology
University of Copenhagen
</affiliation>
<address confidence="0.856861">
Njalsgade 142, DK-2300 Copenhagen S
</address>
<email confidence="0.998059">
soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998722181818182">
This paper introduces a new training set con-
densation technique designed for mixtures
of labeled and unlabeled data. It finds a
condensed set of labeled and unlabeled data
points, typically smaller than what is obtained
using condensed nearest neighbor on the la-
beled data only, and improves classification
accuracy. We evaluate the algorithm on semi-
supervised part-of-speech tagging and present
the best published result on the Wall Street
Journal data set.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962037037037">
Labeled data for natural language processing tasks
such as part-of-speech tagging is often in short sup-
ply. Semi-supervised learning algorithms are de-
signed to learn from a mixture of labeled and un-
labeled data. Many different semi-supervised algo-
rithms have been applied to natural language pro-
cessing tasks, but the simplest algorithm, namely
self-training, is the one that has attracted most atten-
tion, together with expectation maximization (Ab-
ney, 2008). The idea behind self-training is simply
to let a model trained on the labeled data label the
unlabeled data points and then to retrain the model
on the mixture of the original labeled data and the
newly labeled data.
The nearest neighbor algorithm (Cover and Hart,
1967) is a memory-based or so-called lazy learn-
ing algorithm. It is one of the most extensively
used nonparametric classification algorithms, sim-
ple to implement yet powerful, owing to its theo-
retical properties guaranteeing that for all distribu-
tions, its probability of error is bound by twice the
Bayes probability of error (Cover and Hart, 1967).
Memory-based learning has been applied to a wide
range of natural language processing tasks including
part-of-speech tagging (Daelemans et al., 1996), de-
pendency parsing (Nivre, 2003) and word sense dis-
ambiguation (K¨ubler and Zhekova, 2009). Memory-
based learning algorithms are said to be lazy be-
cause no model is learned from the labeled data
points. The labeled data points are the model. Con-
sequently, classification time is proportional to the
number of labeled data points. This is of course im-
practical. Many algorithms have been proposed to
make memory-based learning more efficient. The
intuition behind many of them is that the set of la-
beled data points can be reduced or condensed, since
many labeled data points are more or less redundant.
The algorithms try to extract a subset of the overall
training set that correctly classifies all the discarded
data points through the nearest neighbor rule. Intu-
itively, the model finds good representatives of clus-
ters in the data or discards the data points that are far
from the decision boundaries. Such algorithms are
called training set condensation algorithms.
The need for training set condensation is partic-
ularly important in semi-supervised learning where
we rely on a mixture of labeled and unlabeled data
points. While the number of labeled data points
is typically limited, the number of unlabeled data
points is typically high. In this paper, we intro-
duce a new semi-supervised learning algorithm that
combines self-training and condensation to produce
small subsets of labeled and unlabeled data points
that are highly relevant for determining good deci-
</bodyText>
<page confidence="0.991923">
48
</page>
<note confidence="0.6197575">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 48–52,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.920641307692308">
sion boundaries.
2 Semi-supervised condensed nearest
neighbor
The nearest neighbor (NN) algorithm (Cover and
Hart, 1967) is conceptually simple, yet very pow-
erful. Given a set of labeled data points T, label any
new data point (feature vector) x with y where x′
is the data point in T most similar to x and (x′, y).
Similarity is usually measured in terms of Euclidean
distance. The generalization of the nearest neighbor
algorithm, k nearest neighbor, finds the k most simi-
lar data points Tk to x and assigns x the label y� such
that:
</bodyText>
<equation confidence="0.7408645">
F&apos;(X′,y′)ETkE(x,x′)||y′ = y′′||
with E(·, ·) Euclidean distance and  ||·  ||= 1 if the
</equation>
<bodyText confidence="0.999930566666667">
argument is true (else 0). In other words, the k most
similar points take a weighted vote on the class of x.
Naive implementations of the algorithm store all
the labeled data points and compare each of them to
the data point that is to be classified. Several strate-
gies have been proposed to make nearest neighbor
classification more efficient (Angiulli, 2005). In
particular, training set condensation techniques have
been much studied.
The condensed nearest neighbor (CNN) algorithm
was first introduced in Hart (1968). Finding a sub-
set of the labeled data points may lead to faster
and more accurate classification, but finding the best
subset is an intractable problem (Wilfong, 1992).
CNN can be seen as a simple technique for approxi-
mating such a subset of labeled data points.
The CNN algorithm is defined in Figure 1 with T
the set of labeled data points and T (t) is label pre-
dicted for t by a nearest neighbor classifier ”trained”
on T.
Essentially we discard all labeled data points
whose label we can already predict with the cur-
rent subset of labeled data points. Note that we
have simplified the CNN algorithm a bit compared
to Hart (1968), as suggested, for example, in Alpay-
din (1997), iterating only once over data rather than
waiting for convergence. This will give us a smaller
set of labeled data points, and therefore classifica-
tion requires less space and time. Note that while
the NN rule is stable, and cannot be improved by
</bodyText>
<equation confidence="0.923156857142857">
T = {(x1, y1), ... , (x., y.)}, C = 0
for (x2, y2) E T do
if C(x2) =� y2 then
C = C U {(x2, y2)}
end if
end for
return C
</equation>
<figureCaption confidence="0.762829">
Figure 1: CONDENSED NEAREST NEIGHBOR.
</figureCaption>
<equation confidence="0.895109">
T = {(x1, y1), ... , (x., y.)}, C = 0
for (x2, y2) E T do
if C(x2) =� y2 or PC((x2, y2)|x2) &lt; 0.55 then
C = C U {(x2, y2)}
end if
end for
return C
</equation>
<figureCaption confidence="0.9912955">
Figure 2: WEAKENED CONDENSED NEAREST NEIGH-
BOR.
</figureCaption>
<bodyText confidence="0.999538777777778">
techniques such as bagging (Breiman, 1996), CNN
is unstable (Alpaydin, 1997).
We also introduce a weakened version of the al-
gorithm which not only includes misclassified data
points in the classifier C, but also correctly classi-
fied data points which were labeled with relatively
low confidence. So C includes all data points that
were misclassified and those whose correct label
was predicted with low confidence. The weakened
condensed nearest neighbor (WCNN) algorithm is
sketched in Figure 2.
C inspects k nearest neighbors when labeling
new data points, where k is estimated by cross-
validation. CNN was first generalized to k-NN in
Gates (1972).
Two related condensation techniques, namely re-
moving typical elements and removing elements by
class prediction strength, were argued not to be
useful for most problems in natural language pro-
cessing in Daelemans et al. (1999), but our experi-
ments showed that CNN often perform about as well
as NN, and our semi-supervised CNN algorithm
leads to substantial improvements. The condensa-
tion techniques are also very different: While re-
moving typical elements and removing elements by
class prediction strength are methods for removing
data points close to decision boundaries, CNN ide-
</bodyText>
<equation confidence="0.9806075">
y� = arg max
y″EY
</equation>
<page confidence="0.993632">
49
</page>
<figureCaption confidence="0.9997425">
Figure 3: Unlabeled data may help find better representa-
tives in condensed training sets.
</figureCaption>
<bodyText confidence="0.999499333333333">
ally only removes elements close to decision bound-
aries when the classifier has no use of them.
Intuitively, with relatively simple problems,
e.g. mixtures of Gaussians, CNN and WCNN try to
find the best possible representatives for each clus-
ter in the distribution of data, i.e. finding the points
closest to the center of each cluster. Ideally, CNN
returns one point for each cluster, namely the cen-
ter of each cluster. However, a sample of labeled
data may not include data points that are near the
center of a cluster. Consequently, CNN sometimes
needs several points to stabilize the representation of
a cluster; e.g. the two positives in Figure 3.
When a large number of unlabeled data points
that are labeled according to nearest neighbors pop-
ulates the clusters, chances increase that we find data
points near the centers of our clusters, e.g. the ”good
representative” in Figure 3. Of course the centers of
our clusters may move, but the positive results ob-
tained experimentally below suggest that it is more
likely that labeling unlabeled data by nearest neigh-
bors will enable us to do better training set conden-
sation.
This is exactly what semi-supervised condensed
nearest neighbor (SCNN) does. We first run a
WCNN C and obtain a condensed set of labeled data
points. To this set of labeled data points we add a
large number of unlabeled data points labeled by a
NN classifier T on the original data set. We use a
simple selection criterion and include all data points
</bodyText>
<listItem confidence="0.921506210526316">
1: T = {(x1, y1), ... , (xn, yn)}, C = 0, C′ = 0
2: U = {(x′1), ... , (x′,,t)} # unlabeled data
3: for (xi, yi) E T do
4: if C(xi) =� yi or PC((xi, yi)|xi) &lt; 0.55
then
5: C = C U {(xi, yi)}
6: end if
7: end for
8: for (x′i) E Udo
9: if PT((x′i, T(x′i))|wi) &gt; 0.90 then
10: C = C U {(x′i, T(x′i))}
11: end if
12: end for
13: for (xi, yi) E C do
14: if C′(xi) =� yi then
15: C′ = C′ U {(xi,yi)}
16: end if
17: end for
18: return C′
</listItem>
<figureCaption confidence="0.996492">
Figure 4: SEMI-SUPERVISED CONDENSED NEAREST
NEIGHBOR.
</figureCaption>
<bodyText confidence="0.99959975">
that are labeled with confidence greater than 90%.
We then obtain a new WCNN C′ from the new data
set which is a mixture of labeled and unlabeled data
points. See Figure 4 for details.
</bodyText>
<sectionHeader confidence="0.992627" genericHeader="method">
3 Part-of-speech tagging
</sectionHeader>
<bodyText confidence="0.9999765625">
Our part-of-speech tagging data set is the standard
data set from Wall Street Journal included in Penn-
III (Marcus et al., 1993). We use the standard splits
and construct our data set in the following way, fol-
lowing Søgaard (2010): Each word in the data wi
is associated with a feature vector xi = (x1i , x2i )
where x1i is the prediction on wi of a supervised part-
of-speech tagger, in our case SVMTool1 (Gimenez
and Marquez, 2004) trained on Sect. 0–18, and x2i
is a prediction on wi from an unsupervised part-of-
speech tagger (a cluster label), in our case Unsu-
pos (Biemann, 2006) trained on the British National
Corpus.2 We train a semi-supervised condensed
nearest neighbor classifier on Sect. 19 of the devel-
opment data and unlabeled data from the Brown cor-
pus and apply it to Sect. 22–24. The labeled data
</bodyText>
<footnote confidence="0.9997145">
1http://www.lsi.upc.es/—nlp/SVMTool/
2http://wortschatz.uni-leipzig.de/—cbiemann/software/
</footnote>
<page confidence="0.98534">
50
</page>
<bodyText confidence="0.5700595">
points are thus of the form (one data point or word
per line):
</bodyText>
<table confidence="0.941643">
JJ JJ 17*
NNS NNS 1
IN IN 428
DT DT 425
</table>
<bodyText confidence="0.999886541666667">
where the first column is the class labels or the
gold tags, the second column the predicted tags and
the third column is the ”tags” provided by the unsu-
pervised tagger. Words marked by ”*” are out-of-
vocabulary words, i.e. words that did not occur in
the British National Corpus. The unsupervised tag-
ger is used to cluster tokens in a meaningful way.
Intuitively, we try to learn part-of-speech tagging by
learning when to rely on SVMTool.
The best reported results in the literature on Wall
Street Journal Sect. 22–24 are 97.40% in Suzuki et
al. (2009) and 97.44% in Spoustova et al. (2009);
both systems use semi-supervised learning tech-
niques. Our semi-supervised condensed nearest
neighbor classifier achieves an accuracy of 97.50%.
Equally importantly it condensates the available data
points, from Sect. 19 and the Brown corpus, that
is more than 1.2M data points, to only 2249 data
points, making the classifier very fast. CNN alone is
a lot worse than the input tagger, with an accuracy
of 95.79%. Our approach is also significantly better
than Søgaard (2010) who apply tri-training (Li and
Zhou, 2005) to the output of SVMTool and Unsu-
pos.
</bodyText>
<table confidence="0.994856428571429">
acc (%) data points err.red
CNN 95.79 3,811
SCNN 97.50 2,249 40.6%
SVMTool 97.15 -
Søgaard 97.27 -
Suzuki et al. 97.40 -
Spoustova et al. 97.44 -
</table>
<bodyText confidence="0.999451727272727">
In our second experiment, where we vary the
amount of unlabeled data points, we only train our
ensemble on the first 5000 words in Sect. 19 and
evaluate on the first 5000 words in Sect. 22–24.
The derived learning curve for the semi-supervised
learner is depicted in Figure 5. The immediate drop
in the red scatter plot illustrates the condensation ef-
fect of semi-supervised learning: when we begin to
add unlabeled data, accuracy increases by more than
1.5% and the data set becomes more condensed.
Semi-supervised learning means that we populate
</bodyText>
<figureCaption confidence="0.9995905">
Figure 5: Normalized accuracy (range: 92.62–94.82) and
condensation (range: 310–512 data points).
</figureCaption>
<bodyText confidence="0.99841975">
clusters in the data, making it easier to identify rep-
resentative data points. Since we can easier identify
representative data points, training set condensation
becomes more effective.
</bodyText>
<sectionHeader confidence="0.999153" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.973692">
The implementation used in the experiments builds
on Orange 2.0b for Mac OS X (Python and C++).
In particular, we made use of the implementations
of Euclidean distance and random sampling in their
package. Our code is available at:
cst.dk/anders/sccn/
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999989533333334">
We have introduced a new learning algorithm that
simultaneously condensates labeled data and learns
from a mixture of labeled and unlabeled data. We
have compared the algorithm to condensed nearest
neighbor (Hart, 1968; Alpaydin, 1997) and showed
that the algorithm leads to more condensed models,
and that it performs significantly better than con-
densed nearest neighbor. For part-of-speech tag-
ging, the error reduction over condensed nearest
neighbor is more than 40%, and our model is 40%
smaller than the one induced by condensed nearest
neighbor. While we have provided no theory for
semi-supervised condensed nearest neighbor, we be-
lieve that these results demonstrate the potential of
the proposed method.
</bodyText>
<page confidence="0.99799">
51
</page>
<sectionHeader confidence="0.989521" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99969886440678">
Steven Abney. 2008. Semi-supervised learning for com-
putational linguistics. Chapman &amp; Hall.
Ethem Alpaydin. 1997. Voting over multiple con-
densed nearest neighbors. Artificial Intelligence Re-
view, 11:115–132.
Fabrizio Angiulli. 2005. Fast condensed nearest neigh-
bor rule. In Proceedings of the 22nd International
Conference on Machine Learning.
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
COLING-ACL Student Session.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123–140.
T. Cover and P. Hart. 1967. Nearest neighbor pattern
classification. IEEE Transactions on Information The-
ory, 13(1):21–27.
Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven
Gillis. 1996. MBT: a memory-based part-of-speech
tagger generator. In Proceedings of the 4th Workshop
on Very Large Corpora.
Walter Daelemans, Antal Van Den Bosch, and Jakub Za-
vrel. 1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34(1–3):11–41.
W Gates. 1972. The reduced nearest neighbor rule.
IEEE Transactions on Information Theory, 18(3):431–
433.
Jesus Gimenez and Lluis Marquez. 2004. SVMTool: a
general POS tagger generator based on support vector
machines. In LREC.
Peter Hart. 1968. The condensed nearest neighbor rule.
IEEE Transactions on Information Theory, 14:515–
516.
Sandra K¨ubler and Desislava Zhekova. 2009. Semi-
supervised learning for word-sense disambiguation:
quality vs. quantity. In RANLP.
Ming Li and Zhi-Hua Zhou. 2005. Tri-training: ex-
ploiting unlabeled data using three classifiers. IEEE
Transactions on Knowledge and Data Engineering,
17(11):1529–1541.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313–330.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies, pages
149–160.
Anders Søgaard. 2010. Simple semi-supervised training
of part-of-speech taggers. In ACL.
Drahomira Spoustova, Jan Hajic, Jan Raab, and Miroslav
Spousta. 2009. Semi-supervised training for the aver-
aged perceptron POS tagger. In EACL.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In EMNLP.
G. Wilfong. 1992. Nearest neighbor problems. Interna-
tional Journal of Computational Geometry and Appli-
cations, 2(4):383–416.
</reference>
<page confidence="0.998848">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.433900">
<title confidence="0.898909">Semisupervised condensed nearest neighbor for part-of-speech tagging</title>
<author confidence="0.817265">Anders</author>
<affiliation confidence="0.997411">Center for Language University of</affiliation>
<address confidence="0.92345">Njalsgade 142, DK-2300 Copenhagen</address>
<email confidence="0.990618">soegaard@hum.ku.dk</email>
<abstract confidence="0.945990083333333">This paper introduces a new training set condensation technique designed for mixtures of labeled and unlabeled data. It finds a condensed set of labeled and unlabeled data points, typically smaller than what is obtained using condensed nearest neighbor on the labeled data only, and improves classification accuracy. We evaluate the algorithm on semisupervised part-of-speech tagging and present the best published result on the Wall Street Journal data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Semi-supervised learning for computational linguistics.</title>
<date>2008</date>
<publisher>Chapman &amp; Hall.</publisher>
<contexts>
<context position="1136" citStr="Abney, 2008" startWordPosition="163" endWordPosition="165">evaluate the algorithm on semisupervised part-of-speech tagging and present the best published result on the Wall Street Journal data set. 1 Introduction Labeled data for natural language processing tasks such as part-of-speech tagging is often in short supply. Semi-supervised learning algorithms are designed to learn from a mixture of labeled and unlabeled data. Many different semi-supervised algorithms have been applied to natural language processing tasks, but the simplest algorithm, namely self-training, is the one that has attracted most attention, together with expectation maximization (Abney, 2008). The idea behind self-training is simply to let a model trained on the labeled data label the unlabeled data points and then to retrain the model on the mixture of the original labeled data and the newly labeled data. The nearest neighbor algorithm (Cover and Hart, 1967) is a memory-based or so-called lazy learning algorithm. It is one of the most extensively used nonparametric classification algorithms, simple to implement yet powerful, owing to its theoretical properties guaranteeing that for all distributions, its probability of error is bound by twice the Bayes probability of error (Cover</context>
</contexts>
<marker>Abney, 2008</marker>
<rawString>Steven Abney. 2008. Semi-supervised learning for computational linguistics. Chapman &amp; Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ethem Alpaydin</author>
</authors>
<title>Voting over multiple condensed nearest neighbors.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<pages>11--115</pages>
<contexts>
<context position="5399" citStr="Alpaydin (1997)" startWordPosition="863" endWordPosition="865">and more accurate classification, but finding the best subset is an intractable problem (Wilfong, 1992). CNN can be seen as a simple technique for approximating such a subset of labeled data points. The CNN algorithm is defined in Figure 1 with T the set of labeled data points and T (t) is label predicted for t by a nearest neighbor classifier ”trained” on T. Essentially we discard all labeled data points whose label we can already predict with the current subset of labeled data points. Note that we have simplified the CNN algorithm a bit compared to Hart (1968), as suggested, for example, in Alpaydin (1997), iterating only once over data rather than waiting for convergence. This will give us a smaller set of labeled data points, and therefore classification requires less space and time. Note that while the NN rule is stable, and cannot be improved by T = {(x1, y1), ... , (x., y.)}, C = 0 for (x2, y2) E T do if C(x2) =� y2 then C = C U {(x2, y2)} end if end for return C Figure 1: CONDENSED NEAREST NEIGHBOR. T = {(x1, y1), ... , (x., y.)}, C = 0 for (x2, y2) E T do if C(x2) =� y2 or PC((x2, y2)|x2) &lt; 0.55 then C = C U {(x2, y2)} end if end for return C Figure 2: WEAKENED CONDENSED NEAREST NEIGHBOR</context>
</contexts>
<marker>Alpaydin, 1997</marker>
<rawString>Ethem Alpaydin. 1997. Voting over multiple condensed nearest neighbors. Artificial Intelligence Review, 11:115–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Angiulli</author>
</authors>
<title>Fast condensed nearest neighbor rule.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning.</booktitle>
<contexts>
<context position="4559" citStr="Angiulli, 2005" startWordPosition="719" endWordPosition="720">an distance. The generalization of the nearest neighbor algorithm, k nearest neighbor, finds the k most similar data points Tk to x and assigns x the label y� such that: F&apos;(X′,y′)ETkE(x,x′)||y′ = y′′|| with E(·, ·) Euclidean distance and ||· ||= 1 if the argument is true (else 0). In other words, the k most similar points take a weighted vote on the class of x. Naive implementations of the algorithm store all the labeled data points and compare each of them to the data point that is to be classified. Several strategies have been proposed to make nearest neighbor classification more efficient (Angiulli, 2005). In particular, training set condensation techniques have been much studied. The condensed nearest neighbor (CNN) algorithm was first introduced in Hart (1968). Finding a subset of the labeled data points may lead to faster and more accurate classification, but finding the best subset is an intractable problem (Wilfong, 1992). CNN can be seen as a simple technique for approximating such a subset of labeled data points. The CNN algorithm is defined in Figure 1 with T the set of labeled data points and T (t) is label predicted for t by a nearest neighbor classifier ”trained” on T. Essentially w</context>
</contexts>
<marker>Angiulli, 2005</marker>
<rawString>Fabrizio Angiulli. 2005. Fast condensed nearest neighbor rule. In Proceedings of the 22nd International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised part-of-speech tagging employing efficient graph clustering.</title>
<date>2006</date>
<booktitle>In COLING-ACL Student Session.</booktitle>
<contexts>
<context position="10100" citStr="Biemann, 2006" startWordPosition="1717" endWordPosition="1718">details. 3 Part-of-speech tagging Our part-of-speech tagging data set is the standard data set from Wall Street Journal included in PennIII (Marcus et al., 1993). We use the standard splits and construct our data set in the following way, following Søgaard (2010): Each word in the data wi is associated with a feature vector xi = (x1i , x2i ) where x1i is the prediction on wi of a supervised partof-speech tagger, in our case SVMTool1 (Gimenez and Marquez, 2004) trained on Sect. 0–18, and x2i is a prediction on wi from an unsupervised part-ofspeech tagger (a cluster label), in our case Unsupos (Biemann, 2006) trained on the British National Corpus.2 We train a semi-supervised condensed nearest neighbor classifier on Sect. 19 of the development data and unlabeled data from the Brown corpus and apply it to Sect. 22–24. The labeled data 1http://www.lsi.upc.es/—nlp/SVMTool/ 2http://wortschatz.uni-leipzig.de/—cbiemann/software/ 50 points are thus of the form (one data point or word per line): JJ JJ 17* NNS NNS 1 IN IN 428 DT DT 425 where the first column is the class labels or the gold tags, the second column the predicted tags and the third column is the ”tags” provided by the unsupervised tagger. Wor</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Unsupervised part-of-speech tagging employing efficient graph clustering. In COLING-ACL Student Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="6043" citStr="Breiman, 1996" startWordPosition="998" endWordPosition="999">a rather than waiting for convergence. This will give us a smaller set of labeled data points, and therefore classification requires less space and time. Note that while the NN rule is stable, and cannot be improved by T = {(x1, y1), ... , (x., y.)}, C = 0 for (x2, y2) E T do if C(x2) =� y2 then C = C U {(x2, y2)} end if end for return C Figure 1: CONDENSED NEAREST NEIGHBOR. T = {(x1, y1), ... , (x., y.)}, C = 0 for (x2, y2) E T do if C(x2) =� y2 or PC((x2, y2)|x2) &lt; 0.55 then C = C U {(x2, y2)} end if end for return C Figure 2: WEAKENED CONDENSED NEAREST NEIGHBOR. techniques such as bagging (Breiman, 1996), CNN is unstable (Alpaydin, 1997). We also introduce a weakened version of the algorithm which not only includes misclassified data points in the classifier C, but also correctly classified data points which were labeled with relatively low confidence. So C includes all data points that were misclassified and those whose correct label was predicted with low confidence. The weakened condensed nearest neighbor (WCNN) algorithm is sketched in Figure 2. C inspects k nearest neighbors when labeling new data points, where k is estimated by crossvalidation. CNN was first generalized to k-NN in Gates</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>P Hart</author>
</authors>
<title>Nearest neighbor pattern classification.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="1408" citStr="Cover and Hart, 1967" startWordPosition="209" endWordPosition="212">emi-supervised learning algorithms are designed to learn from a mixture of labeled and unlabeled data. Many different semi-supervised algorithms have been applied to natural language processing tasks, but the simplest algorithm, namely self-training, is the one that has attracted most attention, together with expectation maximization (Abney, 2008). The idea behind self-training is simply to let a model trained on the labeled data label the unlabeled data points and then to retrain the model on the mixture of the original labeled data and the newly labeled data. The nearest neighbor algorithm (Cover and Hart, 1967) is a memory-based or so-called lazy learning algorithm. It is one of the most extensively used nonparametric classification algorithms, simple to implement yet powerful, owing to its theoretical properties guaranteeing that for all distributions, its probability of error is bound by twice the Bayes probability of error (Cover and Hart, 1967). Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (K¨ubler and Zhekova, 2009). Memorybased</context>
<context position="3698" citStr="Cover and Hart, 1967" startWordPosition="563" endWordPosition="566">ypically limited, the number of unlabeled data points is typically high. In this paper, we introduce a new semi-supervised learning algorithm that combines self-training and condensation to produce small subsets of labeled and unlabeled data points that are highly relevant for determining good deci48 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 48–52, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics sion boundaries. 2 Semi-supervised condensed nearest neighbor The nearest neighbor (NN) algorithm (Cover and Hart, 1967) is conceptually simple, yet very powerful. Given a set of labeled data points T, label any new data point (feature vector) x with y where x′ is the data point in T most similar to x and (x′, y). Similarity is usually measured in terms of Euclidean distance. The generalization of the nearest neighbor algorithm, k nearest neighbor, finds the k most similar data points Tk to x and assigns x the label y� such that: F&apos;(X′,y′)ETkE(x,x′)||y′ = y′′|| with E(·, ·) Euclidean distance and ||· ||= 1 if the argument is true (else 0). In other words, the k most similar points take a weighted vote on the cl</context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>T. Cover and P. Hart. 1967. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Peter Berck</author>
<author>Steven Gillis</author>
</authors>
<title>MBT: a memory-based part-of-speech tagger generator.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4th Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="1903" citStr="Daelemans et al., 1996" startWordPosition="285" endWordPosition="288">el on the mixture of the original labeled data and the newly labeled data. The nearest neighbor algorithm (Cover and Hart, 1967) is a memory-based or so-called lazy learning algorithm. It is one of the most extensively used nonparametric classification algorithms, simple to implement yet powerful, owing to its theoretical properties guaranteeing that for all distributions, its probability of error is bound by twice the Bayes probability of error (Cover and Hart, 1967). Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (K¨ubler and Zhekova, 2009). Memorybased learning algorithms are said to be lazy because no model is learned from the labeled data points. The labeled data points are the model. Consequently, classification time is proportional to the number of labeled data points. This is of course impractical. Many algorithms have been proposed to make memory-based learning more efficient. The intuition behind many of them is that the set of labeled data points can be reduced or condensed, since many labeled data points are more or less redunda</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. 1996. MBT: a memory-based part-of-speech tagger generator. In Proceedings of the 4th Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal Van Den Bosch</author>
<author>Jakub Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<marker>Daelemans, Van Den Bosch, Zavrel, 1999</marker>
<rawString>Walter Daelemans, Antal Van Den Bosch, and Jakub Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 34(1–3):11–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gates</author>
</authors>
<title>The reduced nearest neighbor rule.</title>
<date>1972</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>18</volume>
<issue>3</issue>
<pages>433</pages>
<contexts>
<context position="6650" citStr="Gates (1972)" startWordPosition="1095" endWordPosition="1096">1996), CNN is unstable (Alpaydin, 1997). We also introduce a weakened version of the algorithm which not only includes misclassified data points in the classifier C, but also correctly classified data points which were labeled with relatively low confidence. So C includes all data points that were misclassified and those whose correct label was predicted with low confidence. The weakened condensed nearest neighbor (WCNN) algorithm is sketched in Figure 2. C inspects k nearest neighbors when labeling new data points, where k is estimated by crossvalidation. CNN was first generalized to k-NN in Gates (1972). Two related condensation techniques, namely removing typical elements and removing elements by class prediction strength, were argued not to be useful for most problems in natural language processing in Daelemans et al. (1999), but our experiments showed that CNN often perform about as well as NN, and our semi-supervised CNN algorithm leads to substantial improvements. The condensation techniques are also very different: While removing typical elements and removing elements by class prediction strength are methods for removing data points close to decision boundaries, CNN idey� = arg max y″E</context>
</contexts>
<marker>Gates, 1972</marker>
<rawString>W Gates. 1972. The reduced nearest neighbor rule. IEEE Transactions on Information Theory, 18(3):431– 433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Gimenez</author>
<author>Lluis Marquez</author>
</authors>
<title>SVMTool: a general POS tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="9950" citStr="Gimenez and Marquez, 2004" startWordPosition="1688" endWordPosition="1691">ed with confidence greater than 90%. We then obtain a new WCNN C′ from the new data set which is a mixture of labeled and unlabeled data points. See Figure 4 for details. 3 Part-of-speech tagging Our part-of-speech tagging data set is the standard data set from Wall Street Journal included in PennIII (Marcus et al., 1993). We use the standard splits and construct our data set in the following way, following Søgaard (2010): Each word in the data wi is associated with a feature vector xi = (x1i , x2i ) where x1i is the prediction on wi of a supervised partof-speech tagger, in our case SVMTool1 (Gimenez and Marquez, 2004) trained on Sect. 0–18, and x2i is a prediction on wi from an unsupervised part-ofspeech tagger (a cluster label), in our case Unsupos (Biemann, 2006) trained on the British National Corpus.2 We train a semi-supervised condensed nearest neighbor classifier on Sect. 19 of the development data and unlabeled data from the Brown corpus and apply it to Sect. 22–24. The labeled data 1http://www.lsi.upc.es/—nlp/SVMTool/ 2http://wortschatz.uni-leipzig.de/—cbiemann/software/ 50 points are thus of the form (one data point or word per line): JJ JJ 17* NNS NNS 1 IN IN 428 DT DT 425 where the first column </context>
</contexts>
<marker>Gimenez, Marquez, 2004</marker>
<rawString>Jesus Gimenez and Lluis Marquez. 2004. SVMTool: a general POS tagger generator based on support vector machines. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hart</author>
</authors>
<title>The condensed nearest neighbor rule.</title>
<date>1968</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>14</volume>
<pages>516</pages>
<contexts>
<context position="4719" citStr="Hart (1968)" startWordPosition="741" endWordPosition="742">that: F&apos;(X′,y′)ETkE(x,x′)||y′ = y′′|| with E(·, ·) Euclidean distance and ||· ||= 1 if the argument is true (else 0). In other words, the k most similar points take a weighted vote on the class of x. Naive implementations of the algorithm store all the labeled data points and compare each of them to the data point that is to be classified. Several strategies have been proposed to make nearest neighbor classification more efficient (Angiulli, 2005). In particular, training set condensation techniques have been much studied. The condensed nearest neighbor (CNN) algorithm was first introduced in Hart (1968). Finding a subset of the labeled data points may lead to faster and more accurate classification, but finding the best subset is an intractable problem (Wilfong, 1992). CNN can be seen as a simple technique for approximating such a subset of labeled data points. The CNN algorithm is defined in Figure 1 with T the set of labeled data points and T (t) is label predicted for t by a nearest neighbor classifier ”trained” on T. Essentially we discard all labeled data points whose label we can already predict with the current subset of labeled data points. Note that we have simplified the CNN algori</context>
</contexts>
<marker>Hart, 1968</marker>
<rawString>Peter Hart. 1968. The condensed nearest neighbor rule. IEEE Transactions on Information Theory, 14:515– 516.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Desislava Zhekova</author>
</authors>
<title>Semisupervised learning for word-sense disambiguation: quality vs. quantity.</title>
<date>2009</date>
<booktitle>In RANLP.</booktitle>
<marker>K¨ubler, Zhekova, 2009</marker>
<rawString>Sandra K¨ubler and Desislava Zhekova. 2009. Semisupervised learning for word-sense disambiguation: quality vs. quantity. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Li</author>
<author>Zhi-Hua Zhou</author>
</authors>
<title>Tri-training: exploiting unlabeled data using three classifiers.</title>
<date>2005</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>17</volume>
<issue>11</issue>
<contexts>
<context position="11637" citStr="Li and Zhou, 2005" startWordPosition="1967" endWordPosition="1970">Street Journal Sect. 22–24 are 97.40% in Suzuki et al. (2009) and 97.44% in Spoustova et al. (2009); both systems use semi-supervised learning techniques. Our semi-supervised condensed nearest neighbor classifier achieves an accuracy of 97.50%. Equally importantly it condensates the available data points, from Sect. 19 and the Brown corpus, that is more than 1.2M data points, to only 2249 data points, making the classifier very fast. CNN alone is a lot worse than the input tagger, with an accuracy of 95.79%. Our approach is also significantly better than Søgaard (2010) who apply tri-training (Li and Zhou, 2005) to the output of SVMTool and Unsupos. acc (%) data points err.red CNN 95.79 3,811 SCNN 97.50 2,249 40.6% SVMTool 97.15 - Søgaard 97.27 - Suzuki et al. 97.40 - Spoustova et al. 97.44 - In our second experiment, where we vary the amount of unlabeled data points, we only train our ensemble on the first 5000 words in Sect. 19 and evaluate on the first 5000 words in Sect. 22–24. The derived learning curve for the semi-supervised learner is depicted in Figure 5. The immediate drop in the red scatter plot illustrates the condensation effect of semi-supervised learning: when we begin to add unlabeled</context>
</contexts>
<marker>Li, Zhou, 2005</marker>
<rawString>Ming Li and Zhi-Hua Zhou. 2005. Tri-training: exploiting unlabeled data using three classifiers. IEEE Transactions on Knowledge and Data Engineering, 17(11):1529–1541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Mary Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="9647" citStr="Marcus et al., 1993" startWordPosition="1631" endWordPosition="1634">: end for 8: for (x′i) E Udo 9: if PT((x′i, T(x′i))|wi) &gt; 0.90 then 10: C = C U {(x′i, T(x′i))} 11: end if 12: end for 13: for (xi, yi) E C do 14: if C′(xi) =� yi then 15: C′ = C′ U {(xi,yi)} 16: end if 17: end for 18: return C′ Figure 4: SEMI-SUPERVISED CONDENSED NEAREST NEIGHBOR. that are labeled with confidence greater than 90%. We then obtain a new WCNN C′ from the new data set which is a mixture of labeled and unlabeled data points. See Figure 4 for details. 3 Part-of-speech tagging Our part-of-speech tagging data set is the standard data set from Wall Street Journal included in PennIII (Marcus et al., 1993). We use the standard splits and construct our data set in the following way, following Søgaard (2010): Each word in the data wi is associated with a feature vector xi = (x1i , x2i ) where x1i is the prediction on wi of a supervised partof-speech tagger, in our case SVMTool1 (Gimenez and Marquez, 2004) trained on Sect. 0–18, and x2i is a prediction on wi from an unsupervised part-ofspeech tagger (a cluster label), in our case Unsupos (Biemann, 2006) trained on the British National Corpus.2 We train a semi-supervised condensed nearest neighbor classifier on Sect. 19 of the development data and </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell Marcus, Mary Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies,</booktitle>
<pages>149--160</pages>
<contexts>
<context position="1937" citStr="Nivre, 2003" startWordPosition="292" endWordPosition="293">a and the newly labeled data. The nearest neighbor algorithm (Cover and Hart, 1967) is a memory-based or so-called lazy learning algorithm. It is one of the most extensively used nonparametric classification algorithms, simple to implement yet powerful, owing to its theoretical properties guaranteeing that for all distributions, its probability of error is bound by twice the Bayes probability of error (Cover and Hart, 1967). Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (K¨ubler and Zhekova, 2009). Memorybased learning algorithms are said to be lazy because no model is learned from the labeled data points. The labeled data points are the model. Consequently, classification time is proportional to the number of labeled data points. This is of course impractical. Many algorithms have been proposed to make memory-based learning more efficient. The intuition behind many of them is that the set of labeled data points can be reduced or condensed, since many labeled data points are more or less redundant. The algorithms try to extract </context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies, pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Simple semi-supervised training of part-of-speech taggers.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9749" citStr="Søgaard (2010)" startWordPosition="1651" endWordPosition="1652">2: end for 13: for (xi, yi) E C do 14: if C′(xi) =� yi then 15: C′ = C′ U {(xi,yi)} 16: end if 17: end for 18: return C′ Figure 4: SEMI-SUPERVISED CONDENSED NEAREST NEIGHBOR. that are labeled with confidence greater than 90%. We then obtain a new WCNN C′ from the new data set which is a mixture of labeled and unlabeled data points. See Figure 4 for details. 3 Part-of-speech tagging Our part-of-speech tagging data set is the standard data set from Wall Street Journal included in PennIII (Marcus et al., 1993). We use the standard splits and construct our data set in the following way, following Søgaard (2010): Each word in the data wi is associated with a feature vector xi = (x1i , x2i ) where x1i is the prediction on wi of a supervised partof-speech tagger, in our case SVMTool1 (Gimenez and Marquez, 2004) trained on Sect. 0–18, and x2i is a prediction on wi from an unsupervised part-ofspeech tagger (a cluster label), in our case Unsupos (Biemann, 2006) trained on the British National Corpus.2 We train a semi-supervised condensed nearest neighbor classifier on Sect. 19 of the development data and unlabeled data from the Brown corpus and apply it to Sect. 22–24. The labeled data 1http://www.lsi.upc</context>
<context position="11594" citStr="Søgaard (2010)" startWordPosition="1962" endWordPosition="1963">rted results in the literature on Wall Street Journal Sect. 22–24 are 97.40% in Suzuki et al. (2009) and 97.44% in Spoustova et al. (2009); both systems use semi-supervised learning techniques. Our semi-supervised condensed nearest neighbor classifier achieves an accuracy of 97.50%. Equally importantly it condensates the available data points, from Sect. 19 and the Brown corpus, that is more than 1.2M data points, to only 2249 data points, making the classifier very fast. CNN alone is a lot worse than the input tagger, with an accuracy of 95.79%. Our approach is also significantly better than Søgaard (2010) who apply tri-training (Li and Zhou, 2005) to the output of SVMTool and Unsupos. acc (%) data points err.red CNN 95.79 3,811 SCNN 97.50 2,249 40.6% SVMTool 97.15 - Søgaard 97.27 - Suzuki et al. 97.40 - Spoustova et al. 97.44 - In our second experiment, where we vary the amount of unlabeled data points, we only train our ensemble on the first 5000 words in Sect. 19 and evaluate on the first 5000 words in Sect. 22–24. The derived learning curve for the semi-supervised learner is depicted in Figure 5. The immediate drop in the red scatter plot illustrates the condensation effect of semi-supervis</context>
</contexts>
<marker>Søgaard, 2010</marker>
<rawString>Anders Søgaard. 2010. Simple semi-supervised training of part-of-speech taggers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomira Spoustova</author>
<author>Jan Hajic</author>
<author>Jan Raab</author>
<author>Miroslav Spousta</author>
</authors>
<title>Semi-supervised training for the averaged perceptron POS tagger.</title>
<date>2009</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="11118" citStr="Spoustova et al. (2009)" startWordPosition="1886" endWordPosition="1889">* NNS NNS 1 IN IN 428 DT DT 425 where the first column is the class labels or the gold tags, the second column the predicted tags and the third column is the ”tags” provided by the unsupervised tagger. Words marked by ”*” are out-ofvocabulary words, i.e. words that did not occur in the British National Corpus. The unsupervised tagger is used to cluster tokens in a meaningful way. Intuitively, we try to learn part-of-speech tagging by learning when to rely on SVMTool. The best reported results in the literature on Wall Street Journal Sect. 22–24 are 97.40% in Suzuki et al. (2009) and 97.44% in Spoustova et al. (2009); both systems use semi-supervised learning techniques. Our semi-supervised condensed nearest neighbor classifier achieves an accuracy of 97.50%. Equally importantly it condensates the available data points, from Sect. 19 and the Brown corpus, that is more than 1.2M data points, to only 2249 data points, making the classifier very fast. CNN alone is a lot worse than the input tagger, with an accuracy of 95.79%. Our approach is also significantly better than Søgaard (2010) who apply tri-training (Li and Zhou, 2005) to the output of SVMTool and Unsupos. acc (%) data points err.red CNN 95.79 3,81</context>
</contexts>
<marker>Spoustova, Hajic, Raab, Spousta, 2009</marker>
<rawString>Drahomira Spoustova, Jan Hajic, Jan Raab, and Miroslav Spousta. 2009. Semi-supervised training for the averaged perceptron POS tagger. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="11080" citStr="Suzuki et al. (2009)" startWordPosition="1879" endWordPosition="1882">a point or word per line): JJ JJ 17* NNS NNS 1 IN IN 428 DT DT 425 where the first column is the class labels or the gold tags, the second column the predicted tags and the third column is the ”tags” provided by the unsupervised tagger. Words marked by ”*” are out-ofvocabulary words, i.e. words that did not occur in the British National Corpus. The unsupervised tagger is used to cluster tokens in a meaningful way. Intuitively, we try to learn part-of-speech tagging by learning when to rely on SVMTool. The best reported results in the literature on Wall Street Journal Sect. 22–24 are 97.40% in Suzuki et al. (2009) and 97.44% in Spoustova et al. (2009); both systems use semi-supervised learning techniques. Our semi-supervised condensed nearest neighbor classifier achieves an accuracy of 97.50%. Equally importantly it condensates the available data points, from Sect. 19 and the Brown corpus, that is more than 1.2M data points, to only 2249 data points, making the classifier very fast. CNN alone is a lot worse than the input tagger, with an accuracy of 95.79%. Our approach is also significantly better than Søgaard (2010) who apply tri-training (Li and Zhou, 2005) to the output of SVMTool and Unsupos. acc </context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Wilfong</author>
</authors>
<title>Nearest neighbor problems.</title>
<date>1992</date>
<journal>International Journal of Computational Geometry and Applications,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="4887" citStr="Wilfong, 1992" startWordPosition="769" endWordPosition="770">a weighted vote on the class of x. Naive implementations of the algorithm store all the labeled data points and compare each of them to the data point that is to be classified. Several strategies have been proposed to make nearest neighbor classification more efficient (Angiulli, 2005). In particular, training set condensation techniques have been much studied. The condensed nearest neighbor (CNN) algorithm was first introduced in Hart (1968). Finding a subset of the labeled data points may lead to faster and more accurate classification, but finding the best subset is an intractable problem (Wilfong, 1992). CNN can be seen as a simple technique for approximating such a subset of labeled data points. The CNN algorithm is defined in Figure 1 with T the set of labeled data points and T (t) is label predicted for t by a nearest neighbor classifier ”trained” on T. Essentially we discard all labeled data points whose label we can already predict with the current subset of labeled data points. Note that we have simplified the CNN algorithm a bit compared to Hart (1968), as suggested, for example, in Alpaydin (1997), iterating only once over data rather than waiting for convergence. This will give us a</context>
</contexts>
<marker>Wilfong, 1992</marker>
<rawString>G. Wilfong. 1992. Nearest neighbor problems. International Journal of Computational Geometry and Applications, 2(4):383–416.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>