<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.998301">
A Phrase-Based Alignment Model for Natural Language Inference
</title>
<author confidence="0.998106">
Bill MacCartney, Michel Galley, Christopher D. Manning
</author>
<affiliation confidence="0.961525">
Natural Language Processing Group, Stanford University
</affiliation>
<email confidence="0.997195">
{wcmac,mgalley,manning}@stanford.edu
</email>
<sectionHeader confidence="0.993884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997283857142857">
The alignment problem—establishing links
between corresponding phrases in two related
sentences—is as important in natural language
inference (NLI) as it is in machine transla-
tion (MT). But the tools and techniques of
MT alignment do not readily transfer to NLI,
where one cannot assume semantic equiva-
lence, and for which large volumes of bitext
are lacking. We present a new NLI aligner,
the MANLI system, designed to address these
challenges. It uses a phrase-based alignment
representation, exploits external lexical re-
sources, and capitalizes on a new set of su-
pervised training data. We compare the per-
formance of MANLI to existing NLI and MT
aligners on an NLI alignment task over the
well-known Recognizing Textual Entailment
data. We show that MANLI significantly out-
performs existing aligners, achieving gains of
6.2% in F, over a representative NLI aligner
and 10.5% over GIZA++.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994836">
The problem of natural language inference (NLI) is
to determine whether a natural-language hypothesis
H can reasonably be inferred from a given premise
text P. In order to recognize that Kennedy was killed
can be inferred from JFK was assassinated, one
must first recognize the correspondence between
Kennedy and JFK, and between killed and assas-
sinated. Consequently, most current approaches to
NLI rely, implicitly or explicitly, on a facility for
alignment—that is, establishing links between cor-
responding entities and predicates in P and H. Re-
cent entries in the annual Recognizing Textual En-
tailment (RTE) competition (Dagan et al., 2005)
have addressed the alignment problem in a variety
of ways, though often without distinguishing it as
a separate subproblem. Glickman et al. (2005) and
Jijkoun and de Rijke (2005), among others, have ex-
plored approaches based on measuring the degree of
lexical overlap between bags of words. While ig-
noring structure, such methods depend on matching
each word in H to the word in P with which it is
most similar—in effect, an alignment. At the other
extreme, Tatu and Moldovan (2007) and Bar-Haim
et al. (2007) have formulated the inference problem
as analogous to proof search, using inferential rules
which encode (among other things) knowledge of
lexical relatedness. In such approaches, the corre-
spondence between the words of P and H is implicit
in the steps of the proof.
Increasingly, however, the most successful RTE
systems have made the alignment problem explicit.
Marsi and Krahmer (2005) and MacCartney et al.
(2006) first advocated pipelined system architec-
tures containing a distinct alignment component, a
strategy crucial to the top-performing systems of
Hickl et al. (2006) and Hickl and Bensley (2007).
However, each of these systems has pursued align-
ment in idiosyncratic and poorly-documented ways,
often using proprietary data, making comparisons
and further development difficult.
In this paper we undertake the first systematic
study of alignment for NLI. We propose a new NLI
alignment system which uses a phrase-based repre-
sentation of alignment, exploits external resources
for knowledge of semantic relatedness, and capi-
talizes on the recent appearance of new supervised
training data for NLI alignment. In addition, we
examine the relation between NLI alignment and
MT alignment, and investigate whether existing MT
aligners can usefully be applied in the NLI setting.
</bodyText>
<sectionHeader confidence="0.637913" genericHeader="method">
2 NLI alignment vs. MT alignment
</sectionHeader>
<bodyText confidence="0.999406333333333">
The alignment problem is familiar in machine trans-
lation (MT), where recognizing that she came is a
good translation for elle est venue requires establish-
</bodyText>
<page confidence="0.963931">
802
</page>
<note confidence="0.9623045">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802–811,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9987263">
ing a correspondence between she and elle, and be-
tween came and est venue. The MT community has
developed not only an extensive literature on align-
ment (Brown et al., 1993; Vogel et al., 1996; Marcu
and Wong, 2002; DeNero et al., 2006), but also
standard, proven alignment tools such as GIZA++
(Och and Ney, 2003). Can off-the-shelf MT aligners
be applied to NLI? There is reason to be doubtful.
Alignment for NLI differs from alignment for MT
in several important respects, including:
</bodyText>
<listItem confidence="0.971415333333333">
1. Most obviously, it is monolingual rather than
cross-lingual, opening the door to utilizing
abundant (monolingual) sources of information
on semantic relatedness, such as WordNet.
2. It is intrinsically asymmetric: P is often much
longer than H, and commonly contains phrases
or clauses which have no counterpart in H.
3. Indeed, one cannot assume even approximate
semantic equivalence—usually a given in MT.
Because NLI problems include both valid and
invalid inferences, the semantic content of H
may diverge substantially from P. An NLI
aligner must be designed to accommodate fre-
quent unaligned words and phrases.
4. Little training data is available. MT align-
ment models are typically trained in unsu-
pervised fashion, inducing lexical correspon-
dences from massive quantities of sentence-
aligned bitexts. While NLI aligners could in
principle do the same, large volumes of suit-
able data are lacking. NLI aligners must there-
fore depend on smaller quantities of supervised
training data, supplemented by external lexi-
cal resources. Conversely, while existing MT
aligners can make use of dictionaries, they are
not designed to harness other sources of infor-
mation on degrees of semantic relatedness.
</listItem>
<bodyText confidence="0.976574">
Consequently, the tools and techniques of MT align-
ment may not transfer readily to NLI alignment. We
investigate the matter empirically in section 5.2.
</bodyText>
<sectionHeader confidence="0.997826" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.905373823529412">
Until recently, research on alignment for NLI has
been hampered by a paucity of high-quality, publicly
available data from which to learn. Happily, that has
begun to change, with the release by Microsoft Re-
search (MSR) of human-generated alignment anno-
In
most
Pacific
countries
there
are
very
few
women
in
parliament
.
</bodyText>
<figureCaption confidence="0.996338">
Figure 1: The MSR gold-standard alignment for problem
116 from the RTE2 development set.
</figureCaption>
<bodyText confidence="0.999800344827586">
tations (Brockett, 2007) for inference problems from
the second Recognizing Textual Entailment (RTE2)
challenge (Bar-Haim et al., 2006). To our knowl-
edge, this work is the first to exploit this data for
training and evaluation of NLI alignment models.
The RTE2 data consists of a development set and
a test set, each containing 800 inference problems.
Each problem consists of a premise and a hypoth-
esis. The premises contain 29 words on average;
the hypotheses, 11 words. Each problem is marked
as a valid or invalid inference (50% each); how-
ever, these annotations are ignored during align-
ment, since they would not be available during test-
ing of a complete NLI system.
The MSR annotations use an alignment repre-
sentation which is token-based, but many-to-many,
and thus allows implicit alignment of multi-word
phrases. Figure 1 shows an example in which very
few has been aligned with poorly represented.
In the MSR data, every alignment link is marked
as SURE or POSSIBLE. In making this distinction,
the annotators have followed a convention common
in MT, which permits alignment precision to be
measured against both SURE and POSSIBLE links,
while recall is measured against only SURE links.
In this work, however, we have chosen to ignore
POSSIBLE links, embracing the argument made by
(Fraser and Marcu, 2007) that their use has impeded
progress in MT alignment models, and that SURE-
</bodyText>
<page confidence="0.997779">
803
</page>
<bodyText confidence="0.999705714285714">
only annotation is to be preferred.
Each RTE2 problem was independently annotated
by three people, following carefully designed an-
notation guidelines. Inter-annotator agreement was
high: Brockett (2007) reports Fleiss’ kappa1 scores
of about 0.73 (“substantial agreement”) for map-
pings from H tokens to P tokens; and all three an-
notators agreed on —70% of proposed links, while
at least two of three agreed on more than 99.7%
of proposed links,2 attesting to the high quality of
the annotation data. For this work, we merged the
three independent annotations, using majority rule,3
to obtain a gold-standard annotation containing an
average of 7.3 links per RTE problem.
</bodyText>
<sectionHeader confidence="0.997866" genericHeader="method">
4 The MANLI aligner
</sectionHeader>
<bodyText confidence="0.99977325">
In this section, we describe the MANLI aligner, a
new alignment system designed expressly for NLI
alignment. The MANLI system consists of four el-
ements: (1) a phrase-based representation of align-
ment, (2) a feature-based linear scoring function for
alignments, (3) a decoder which uses simulated an-
nealing to find high-scoring alignments, and (4) per-
ceptron learning to optimize feature weights.
</bodyText>
<subsectionHeader confidence="0.997092">
4.1 A phrase-based alignment representation
</subsectionHeader>
<bodyText confidence="0.999423625">
MANLI uses an alignment representation which is
intrinsically phrase-based. (Following the usage
common in MT, we use “phrase” to mean any con-
tiguous span of tokens, not necessarily correspond-
ing to a syntactic phrase.) We represent an alignment
E between a premise P and a hypothesis H as a set
of phrase edits {e1, e2, ...1, each belonging to one
of four types:
</bodyText>
<listItem confidence="0.9340915">
• an EQ edit connects a phrase in P with an equal
(by word lemmas) phrase in H
• a SUB edit connects a phrase in P with an un-
equal phrase in H
• a DEL edit covers an unaligned phrase in P
• an INS edit covers an unaligned phrase in H
For example, the alignment shown in fig-
ure 1 can be represented by the set {DEL(In1),
</listItem>
<footnote confidence="0.995934666666667">
1Fleiss’ kappa generalizes Cohen’s kappa to the case where
there are more than two annotators.
2The SURE/POSSIBLE distinction is taken as significant in
computing all these figures.
3The handful of three-way disagreements were treated as
POSSIBLE links, and thus were not used here.
</footnote>
<equation confidence="0.9608302">
DEL(most2), DEL(Pacific3), DEL(countries4),
DEL(there5), EQ(are6, are2), SUB(very7 fewg,
poorly3 represented4), EQ(womeng, Women1),
EQ(in10, in5), EQ(parliament11, parliament6),
EQ(.12, .7)1.4
</equation>
<bodyText confidence="0.9999608">
Alignments are constrained to be one-to-one at
the phrase level: every token in P and H belongs
to exactly one phrase, which participates in exactly
one edit (possibly DEL or INS). However, the phrase
representation permits alignments which are many-
to-many at the token level. In fact, this is the chief
motivation for the phrase-based representation: we
can align very few and poorly represented as units,
without being forced to make an arbitrary choice as
to which word goes with which word. Moreover, our
scoring function can make use of lexical resources
which have information about semantic relatedness
of multi-word phrases, not merely individual words.
About 23% of the MSR gold-standard align-
ments are not one-to-one (at the token level), and
are therefore technically unreachable for MANLI,
which is constrained to generate one-to-one align-
ments. However, by merging contiguous token links
into phrase edits of size &gt; 1, most MSR align-
ments (about 92%) can be straightforwardly con-
verted into MANLI-reachable alignments. For the
purpose of model training (but not for the evalua-
tion described in section 5.4), we generated a ver-
sion of the MSR data in which all alignments were
converted to MANLI-reachable form.5
</bodyText>
<subsectionHeader confidence="0.99496">
4.2 A feature-based scoring function
</subsectionHeader>
<bodyText confidence="0.997984">
To score alignments, we use a simple feature-based
linear scoring function, in which the score of an
alignment is the sum of the scores of the edits it con-
tains (including not only SUB and EQ edits, but also
DEL and INS edits), and the score of an edit is the
dot product of a vector encoding its features and a
vector of weights. If E is a set of edits constituting
</bodyText>
<footnote confidence="0.5753811">
4DEL and INS edits of size &gt; 1 are possible in principle, but
are not used in our training data.
5About 8% of the MSR alignments contain non-contiguous
links, most commonly because P contains two references to
an entity (e.g., Christian Democrats and CDU) which are both
linked to a reference to the same entity in H (e.g., Christian
Democratic Union). In such cases, one or more links must be
eliminated to achieve a MANLI-reachable alignment. We used
a string-similarity heuristic to break such conflicts, but were
obliged to make an arbitrary choice in about 2% of cases.
</footnote>
<page confidence="0.994327">
804
</page>
<bodyText confidence="0.999379">
an alignment, and 4) is a vector of feature functions,
the score s is given by:
</bodyText>
<equation confidence="0.997187">
s(E) = � �s(e) = w · 4)(e)
eEE eEE
</equation>
<bodyText confidence="0.998646294117647">
We’ll explain how the feature weights w are set in
section 4.4. The features used to characterize each
edit are as follows:
Edit type features. We begin with boolean fea-
tures encoding the type of each edit. We expect EQs
to score higher than SUBs, and (since P is commonly
longer than H) DELs to score higher than INSs.
Phrase features. Next, we have features which
encode the sizes of the phrases involved in the edit,
and whether these phrases are non-constituents (in
syntactic parses of the sentences involved).
Lexical similarity feature. For SUB edits, a very
important feature represents the lexical similarity of
the substituends, as a real value in [0, 1]. This simi-
larity score is computed as a max over a number of
component scoring functions, some based on exter-
nal lexical resources, including:
</bodyText>
<listItem confidence="0.99127275">
• various string similarity functions, of which
most are applied to word lemmas
• measures of synonymy, hypernymy, antonymy,
and semantic relatedness, including a widely-
used measure due to Jiang and Conrath (1997),
based on manually constructed lexical re-
sources such as WordNet and NomBank
• a function based on the well-known distribu-
tional similarity metric of Lin (1998), which
automatically infers similarity of words and
phrases from their distributions in a very large
corpus of English text
</listItem>
<bodyText confidence="0.973601307692308">
The ability to leverage external lexical resources—
both manually and automatically constructed—is
critical to the success of MANLI.
Contextual features. Even when the lexical sim-
ilarity for a SUB edit is high, it may not be a
good match. If P or H contains multiple occur-
rences of the same word—which happens frequently
with function words, and occasionally with content
words—lexical similarity may not suffice to deter-
mine the right match. To remedy this, we introduce
contextual features for SUB and EQ edits. A real-
valued distortion feature measures the difference
Inputs
</bodyText>
<listItem confidence="0.9229905">
• an alignment problem (P, H)
• a number of iterations N (e.g. 100)
• initial temperature T0 (e.g. 40) and multiplier r (e.g. 0.9)
• a bound on edit size max (e.g. 6)
• an alignment scoring function, SCORE(E)
Initialize
• Let E be an “empty” alignment for (P, H) (containing
only DEL and INS edits, no EQ or SUB edits)
• Set E�= E
Repeat for i = 1 to N
• Let {F1, F2, ...} be the set of possible successors of E.
To generate this set:
– Consider every possible edit f up to size max
– Let C(E, f) be the set of edits in E which “con-
flict” with f (i.e., involve at least some of the same
tokens as f)
– LetF=EU{f}\C(E, f)
• Let s(F) be a map from successors of E to scores gener-
ated by SCORE
• Set p(F) = exp s(F), and then normalize p(F), trans-
forming the score map to a probability distribution
• Set Ti = r · Ti−1
• Set p(F) = p(F)1/T*, smoothing or sharpening p(F)
• Renormalize p(F)
• Choose a new value for E by sampling from p(F)
• If SCORE(E) &gt; SCORE( P), set E� = E
</listItem>
<figure confidence="0.539544">
Return E�
</figure>
<figureCaption confidence="0.999698">
Figure 2: The MANLI-ALIGN algorithm
</figureCaption>
<bodyText confidence="0.996773">
between the relative positions of the substituends
within their respective sentences, while boolean
matching neighbors features indicate whether the to-
kens before and after the substituends are equal or
similar.
</bodyText>
<subsectionHeader confidence="0.999738">
4.3 Decoding using simulated annealing
</subsectionHeader>
<bodyText confidence="0.999980066666667">
The problem of decoding—that is, finding a
high-scoring alignment for a particular inference
problem—is made more complex by our choice of a
phrase-based alignment representation. For a model
which uses a token-based representation (say, one
which simply maps H tokens to P tokens), decod-
ing is trivial, since each token can be aligned inde-
pendently of its neighbors. (This is the case for the
bag-of-words aligner described in section 5.1.) But
with a phrase-based representation, things are more
complicated. The segmentation into phrases is not
given in advance, and every phrase pair considered
for alignment must be consistent with its neighbors
with respect to segmentation. Consequently, the de-
coding problem cannot be factored into a number of
</bodyText>
<page confidence="0.992922">
805
</page>
<bodyText confidence="0.99963528">
independent decisions.
To address this difficulty, we have devised a
stochastic alignment algorithm, MANLI-ALIGN (fig-
ure 2), which uses a simulated annealing strategy.
Beginning from an arbitrary alignment, we make a
series of local steps, at each iteration sampling from
a set of possible successors according to scores as-
signed by our scoring function. The sampling is con-
trolled by a “temperature” which falls over time. At
the beginning of the process, successors are sampled
with nearly uniform probability, which helps to en-
sure that the space of possibilities is explored and
local maxima are avoided. As the temperature falls,
there is a ever-stronger bias toward high-scoring suc-
cessors, so that the algorithm converges on a near-
optimal alignment. Clever use of memoization helps
to ensure that computational costs remain manage-
able. Using the parameter values suggested in fig-
ure 2, aligning an average RTE problem takes about
two seconds.
While MANLI-ALIGN is not guaranteed to pro-
duce optimal alignments, there is reason to believe
that it usually comes very close. After training, the
alignment found by MANLI scored at least as high
as the gold alignment for 99.6% of RTE problems.6
</bodyText>
<subsectionHeader confidence="0.993888">
4.4 Perceptron learning
</subsectionHeader>
<bodyText confidence="0.999997733333334">
To tune the parameters w of the model, we use
an adaptation of the averaged perceptron algorithm
(Collins, 2002), which has proven successful on a
range of NLP tasks. The algorithm is shown in fig-
ure 3. After initializing w to 0, we perform N train-
ing epochs. (Our experiments used N = 50.) In
each epoch, we iterate through the training data, up-
dating the weight vector at each training example ac-
cording to the difference between the features of the
target alignment and the features of the alignment
produced by the decoder using the current weight
vector. The size of the update is controlled by a
learning rate which decreases over time. At the end
of each epoch, the weight vector is normalized and
stored. The final result is the average of the stored
</bodyText>
<footnote confidence="0.988549333333333">
6This figure is based on the MANLI-reachable version of
the gold-standard data described in section 4.1. For the raw
gold-standard data, the figure is 88.1%. The difference is almost
entirely attributable to unreachable gold alignments, which tend
to score higher simply because they contain more edits (and
because the learned weights are mostly positive).
</footnote>
<note confidence="0.377641">
Inputs
</note>
<listItem confidence="0.93683">
• training problems (Pj, Hj), j = 1..n
• corresponding gold-standard alignments Ej
• a number of learning epochs N (e.g. 50)
• a “burn-in” period N0 &lt; N (e.g. 10)
• initial learning rate R0 (e.g. 1) and multiplier r (e.g. 0.8)
• a vector of feature functions 4)(E)
• an alignment algorithm ALIGN(P, H; w) which finds a
good alignment for (P, H) using weight vector w
Initialize
• Setw=0
Repeat for i = 1 to N
• Set Ri = r • Ri_1, reducing the learning rate
• Randomly shuffle the training problems
• For j = 1 to n:
– Set Ej = ALIGN(Pj, Hj; w)
– Set w = w + Ri • (4P(Ej) − 4D(
• Set w = w/IIwII2 (L2 normalization)
• Set w[i] = w, storing the weight vector for this epoch
Return an averaged weight vector:
• wavy = 1/(N − N0) ENi�N0+1 w[i]
</listItem>
<figureCaption confidence="0.999735">
Figure 3: The MANLI-LEARN algorithm
</figureCaption>
<bodyText confidence="0.9996354">
weight vectors, omitting vectors from a fixed num-
ber of epochs at the beginning of the run (which tend
to be of poor quality). Using the parameter values
suggested in figure 3, training runs on the RTE2 de-
velopment set required about 20 hours.
</bodyText>
<sectionHeader confidence="0.818049" genericHeader="method">
5 Evaluating aligners on MSR data
</sectionHeader>
<bodyText confidence="0.9997515">
In this section, we describe experiments designed to
evaluate the performance of various alignment sys-
tems on the MSR gold-standard data described in
section 3. For each system, we report precision,
recall, and F-measure (F1).7 Note that these are
macro-averaged statistics, computed per problem by
counting aligned token pairs,8 and then averaged
over all problems in a problem set.9 We also re-
</bodyText>
<footnote confidence="0.872046714285714">
7MT researchers conventionally report results in terms of
alignment error rate (AER). Since we use only SURE links in the
gold-standard data (see section 3), AER is equivalent to 1− F1.
8For phrase-based alignments like those generated by
MANLI, two tokens are considered to be aligned iff they are
contained within phrases which are aligned.
9MT evaluations conventionally use micro-averaging, which
gives greater weight to problems containing more aligned pairs.
This makes sense in MT, where the purpose of alignment is to
induce phrase tables. But in NLI, where the ultimate goal is
to maximize the number of inference problems answered cor-
rectly, it is more fitting to give all problems equal weight, and
so we macro-average. We have also generated all results using
micro-averaging, and found that the relative comparisons are
</footnote>
<equation confidence="0.751222">
Ej))
</equation>
<page confidence="0.985407">
806
</page>
<bodyText confidence="0.9998975">
port the exact match rate, that is, the proportion of
problems in which the guessed alignment exactly
matches the gold alignment. The results are sum-
marized in table 1.
</bodyText>
<subsectionHeader confidence="0.96987">
5.1 A robust baseline: the bag-of-words aligner
</subsectionHeader>
<bodyText confidence="0.9998761">
As a baseline, we use a simple alignment algorithm
inspired by the lexical entailment model of Glick-
man et al. (2005), and similar to the simple heuristic
model described in (Och and Ney, 2003). Each hy-
pothesis word h is aligned to the premise word p to
which it is most similar, according to a lexical sim-
ilarity function sim(p, h) which returns scores in
[0, 1]. While Glickman et al. used a function based
on web co-occurrence statistics, we use a much sim-
pler function based on string edit distance:
</bodyText>
<table confidence="0.999572923076923">
System Data P % R % Fl % E %
Bag-of-words dev 57.8 81.2 67.5 3.5
(baseline) test 62.1 82.6 70.9 5.3
GIZA++ dev 83.0 66.4 72.1 9.4
(using lex, n) test 85.1 69.1 74.8 11.3
Cross-EM dev 67.6 80.1 72.1 1.3
(using lex, n) test 70.3 81.0 74.1 0.8
Stanford RTE dev 81.1 61.2 69.7 0.5
test 82.7 61.2 70.3 0.3
Stanford RTE dev 81.1 75.8 78.4 —
(punct. corr.) test 82.7 75.8 79.1 —
MANLI dev 83.4 85.5 84.4 21.7
(this work) test 85.4 85.3 85.3 21.3
</table>
<tableCaption confidence="0.97797725">
Table 1: Performance of various aligners on the MSR
RTE2 alignment data. The columns show the data set
used (800 problems each); average precision, recall, and
F-measure; and the exact match rate (see text).
</tableCaption>
<equation confidence="0.99524">
sim(w1, w2) = 1 − max(|lem(w1)|, |lem(w2)|)
dist(lem(w1), lem(w2))
</equation>
<bodyText confidence="0.992812956521739">
(Here lem(w) denotes the lemma of word w; dist()
denotes Levenshtein string edit distance; and  |·  |de-
notes string length.)
This model can be easily extended to generate an
alignment score, which will be of interest in sec-
tion 6. We define the score for a specific hypoth-
esis token h to be the log of its similarity with
the premise token p to which it is aligned, and the
score for the complete alignment of hypothesis H
to premise P to be the sum of the scores of the to-
kens in H, weighted by inverse document frequency
in a large corpus10 (so that common words get less
weight), and normalized by the length of H:
Despite the simplicity of this alignment model, its
performance is fairly robust, with good recall. Its
precision, however, its mediocre—chiefly because,
by design, it aligns every h with some p. The model
could surely be improved by allowing it to leave
some H tokens unaligned, but this was not pursued.
not greatly affected.
10We use idf(w) = log(N/N,,,), where N is the number of
documents in the corpus, and N. is the number of documents
containing word w.
</bodyText>
<subsectionHeader confidence="0.987303">
5.2 MT aligners: GIZA++ and Cross-EM
</subsectionHeader>
<bodyText confidence="0.957016692307692">
Given the importance of alignment for NLI, and the
availability of standard, proven tools for MT align-
ment, an obvious question presents itself: why not
use an off-the-shelf MT aligner for NLI? Although
we have argued (section 2) that this is unlikely to
succeed, to our knowledge, we are the first to inves-
tigate the matter empirically.11
The best-known MT aligner is undoubtedly
GIZA++ (Och and Ney, 2003), which contains im-
plementations of various IBM models (Brown et al.,
1993), as well as the HMM model of Vogel et al.
(1996). Most practitioners use GIZA++ as a black
box, via the Moses MT toolkit (Koehn et al., 2007).
We followed this practice, running with Moses’ de-
fault parameters on the RTE2 data to obtain asym-
metric word alignments in both directions (P-to-H
and H-to-P). We then performed symmetrization
using the well-known INTERSECTION heuristic.
Unsurprisingly, the out-of-the-box performance
was quite poor, with most words aligned apparently
at random. Precision was fair (72%) but recall was
very poor (46%). Even equal words were usually not
aligned—because GIZA++ is designed for cross-
linguistic use, it does not consider word equality be-
tween source and target sentences. To remedy this,
we supplied GIZA++ with a lexicon, using a trick
</bodyText>
<note confidence="0.4936825">
11However, Dolan et al. (2004) explore a closely-related
topic: using an MT aligner to identify paraphrases.
</note>
<equation confidence="0.9731145">
score(h|P) = logmax sim(p, h)
PEP
score(H|P) = |H |1: idf (h) · score (h|P)
hEH
</equation>
<page confidence="0.987967">
807
</page>
<bodyText confidence="0.999910191489362">
common in MT: we supplemented the training data
with synthetic data consisting of matched pairs of
equal words. This gives GIZA++ a better chance
of learning that, e.g., man should align with man.
The result was a big boost in recall (+23%), and a
smaller gain in precision. The results for GIZA++
shown in table 1 are based on using the lexicon and
INTERSECTION. With these settings, GIZA++ prop-
erly aligned most pairs of equal words, but contin-
ued to align other words apparently at random.
Next, we compared the performance of INTER-
SECTION with other symmetrization heuristics de-
fined in Moses—including UNION, GROW, GROW-
DIAG, GROW-DIAG-FINAL (the default), and GROW-
DIAG-FINAL-AND—and with asymmetric align-
ments in both directions. While all these alterna-
tives achieved better recall than INTERSECTION, all
showed substantially worse precision and F1. On
the RTE2 test set, the asymmetric alignment from
H to P scored 68% in F1; GROW scored 58%; and
all other alternatives scored below 52%.
As an additional experiment, we tested the Cross-
EM aligner (Liang et al., 2006) from the Berke-
leyAligner package on the MSR data. While this
aligner is in many ways simpler than GIZA++ (it
lacks any model of fertility, for example), its method
of jointly training two simple asymmetric HMM
models has outperformed GIZA++ on standard eval-
uations of MT alignment. As with GIZA++, we ex-
perimented with a variety of symmetrization heuris-
tics, and ran trials with and without a supplemental
lexicon. The results were broadly similar: INTER-
SECTION greatly outperformed alternative heuris-
tics, and using a lexicon provided a big boost (up
to 12% in F1). Under optimal settings, the Cross-
EM aligner showed better recall and worse preci-
sion than GIZA++, with F1 just slightly lower. Like
GIZA++, it did well at aligning equal words, but
aligned most other words at random.
The mediocre performance of MT aligners on
NLI alignment comes as no surprise, for reasons dis-
cussed in section 2. Above all, the quantity of train-
ing data is simply too small for unsupervised learn-
ing to succeed. A successful NLI aligner will need
to exploit supervised training data, and will need ac-
cess to additional sources of knowledge about lexi-
cal relatedness.
</bodyText>
<subsectionHeader confidence="0.996443">
5.3 The Stanford RTE aligner
</subsectionHeader>
<bodyText confidence="0.999988295454546">
A better comparison is thus to an alignment sys-
tem expressly designed for NLI. For this purpose,
we used the alignment component of the Stanford
RTE system (Chambers et al., 2007). The Stanford
aligner performs decoding and learning in a simi-
lar fashion to MANLI, but uses a simpler, token-
based alignment representation, along with a richer
set of features for alignment scoring. It represents
alignments as an injective map from H tokens to
P tokens. Phrase alignments are not directly repre-
sentable, although the effect can be approximated by
a pre-processing step which collapses multi-token
named entities and certain collocations into single
tokens. The features used for alignment scoring in-
clude not only measures of lexical similarity, but
also syntactic features intended to promote the align-
ment of similar predicate-argument structures.
Despite this sophistication, the out-of-the-box
performance of the Stanford aligner is mediocre, as
shown in table 1. The low recall figures are partic-
ularly noteworthy. However, a partial explanation
is readily available: by design, the Stanford system
ignores punctuation.12 Because punctuation tokens
constitute about 15% of the aligned pairs in the MSR
data, this sharply reduces measured recall. However,
since punctuation matters little in inference, such re-
call errors probably should be forgiven. Thus, ta-
ble 1 also shows adjusted statistics for the Stanford
system in which all recall errors involving punctua-
tion are (generously) ignored.
Even after this adjustment, the recall figures are
unimpressive. Error analysis reveals that the Stan-
ford aligner does a poor job of aligning function
words. About 13% of the aligned pairs in the MSR
data are matching prepositions or articles; the Stan-
ford aligner misses about 67% of such pairs. (By
contrast, MANLI misses only 10% of such pairs.)
While function words matter less in inference than
nouns and verbs, they are not irrelevant, and because
sentences often contain multiple instances of a par-
ticular function word, matching them properly is by
no means trivial. If matching prepositions and ar-
ticles were ignored (in addition to punctuation), the
gap in F1 between the MANLI and Stanford systems
</bodyText>
<footnote confidence="0.922641">
12In fact, it operates on a dependency-graph representation
from which punctuation is omitted.
</footnote>
<page confidence="0.995378">
808
</page>
<bodyText confidence="0.9980665">
would narrow to about 2.8%.
Finally, the Stanford aligner is handicapped by its
token-based alignment representation, often failing
(partly or completely) to align multi-word phrases
such as peace activists with protesters, or hackers
with non-authorized personnel.
</bodyText>
<subsectionHeader confidence="0.984724">
5.4 The MANLI aligner
</subsectionHeader>
<bodyText confidence="0.999965578947368">
As table 1 indicates, the MANLI aligner was found
to outperform all other aligners evaluated on ev-
ery measure of performance, achieving an F1 score
10.5% higher than GIZA++ and 6.2% higher than
the Stanford aligner (even with the punctuation cor-
rection).13 MANLI achieved a good balance be-
tween precision and recall, and matched more than
20% of the gold-standard alignments exactly.
Three factors seem to have contributed most to
MANLI’s success. First, MANLI is able to outper-
form the MT aligners principally because it is able
to leverage lexical resources to identify the similar-
ity between pairs of words such as jail and prison,
prevent and stop, or injured and wounded. Second,
MANLI’s contextual features enable it to do bet-
ter than the Stanford aligner at matching function
words, a weakness of the Stanford aligner discussed
in section 5.3. Third, MANLI gains a marginal ad-
vantage because its phrase-based representation of
alignment permits it to properly align phrase pairs
such as death penalty and capital punishment, or ab-
dicate and give up.
However, the phrase-based representation con-
tributed far less than we had hoped. Setting
MANLI’s maximum phrase size to 1 (effectively,
restricting it to token-based alignments) caused F1
to fall by just 0.2%. We do not interpret this to
mean that phrase alignments are not useful—indeed,
about 2.6% of the links in the gold-standard data in-
volve phrases of size &gt; 1. Rather, we think it shows
that we have failed to fully exploit the advantages
of the phrase-based representation, chiefly because
we lack lexical resources providing good informa-
tion on similarity of multi-word phrases.
Error analysis suggests that there is ample room
for improvement. A large proportion of recall errors
(perhaps 40%) occur because the lexical similarity
function assigns too low a value to pairs of words
</bodyText>
<footnote confidence="0.812045">
13Reported results for MANLI are averages over 10 runs.
</footnote>
<bodyText confidence="0.999754535714286">
or phrases which are clearly similar, such as con-
servation and protecting, server and computer net-
works, organization and agencies, or bone fragility
and osteoporosis. Better exploitation of lexical re-
sources could help to reduce such errors. Another
important category of recall errors (about 12%) re-
sult from the failure to identify one- and multi-word
versions of the name of some entity, such as Lennon
and John Lennon, or Nike Inc. and Nike. A special-
purpose similarity function could help here. Note,
however, that about 10% of recall errors are un-
avoidable, given our choice of alignment represen-
tation, since they involve cases where the gold stan-
dard aligns one or more tokens on one side to a non-
contiguous set of tokens on the other side.
Precision errors may be harder to reduce. These
errors are dominated by cases where we mistakenly
align two equal function words (49% of precision er-
rors), two forms of the verb to be (21%), two equal
punctuation marks (7%), or two words or phrases
of other types having equal lemmas (18%). Be-
cause such errors often occur because the aligner
is forced to choose between nearly equivalent alter-
natives, they may be difficult to eliminate. The re-
maining 5% of precision errors result mostly from
aligning words or phrases rightly judged to be highly
similar, such as expanding and increasing, labor and
birth, figures and number, or 223,000 and 220,000.
</bodyText>
<sectionHeader confidence="0.907689" genericHeader="method">
6 Using alignment to predict RTE answers
</sectionHeader>
<bodyText confidence="0.999966">
In section 5, we evaluated the ability of aligners to
recover gold-standard alignments. But since align-
ment is just one component of the NLI problem, we
might also examine the impact of different align-
ers on the ability to recognize valid inferences. If a
high-scoring alignment indicates a close correspon-
dence between H and P, does this also indicate a
valid inference? We have previously emphasized
(MacCartney et al., 2006) that there is more to infer-
ential validity than close lexical or structural corre-
spondence: negations, modals, non-factive and im-
plicative verbs, and other linguistic constructs can
affect validity in ways hard to capture in alignment.
Nevertheless, alignment score can be a strong pre-
dictor of inferential validity, and some NLI systems
(e.g., (Glickman et al., 2005)) rely entirely on some
measure of alignment quality to predict validity.
</bodyText>
<page confidence="0.994999">
809
</page>
<table confidence="0.995493777777778">
System data acc % avgP %
Bag-of-words aligner dev 61.3 61.5
test 57.9 58.9
Stanford RTE aligner dev 63.1 64.9
test 60.9 59.2
MANLI aligner dev 59.3 69.0
(this work) test 60.3 61.0
RTE2 entries (average) test 58.5 59.1
LCC (Hickl et al., 2006) test 75.4 80.8
</table>
<tableCaption confidence="0.78196125">
Table 2: Performance of various aligners and complete
RTE systems in predicting RTE2 answers. The columns
show the data set used, accuracy, and average precision
(the recommended metric for RTE2).
</tableCaption>
<bodyText confidence="0.999713947368421">
If an aligner generates real-valued alignment
scores, we can use the RTE data to test its ability to
predict inferential validity with the following simple
method. For a given RTE problem, we predict YES
(valid) if its alignment score14 exceeds a threshold
T, and NO otherwise. We tune T to maximize accu-
racy on the RTE2 development set, and then measure
performance on the RTE2 test set using the same T.
Table 2 shows results for several NLI aligners,
along with some results for complete RTE systems,
including the LCC system (the top performer at
RTE2) and an average of all systems participating in
RTE2. While none of the aligners rivals the perfor-
mance of the LCC system, all achieve respectable
results, and the Stanford and MANLI aligners out-
perform the average RTE2 entry. Thus, even if align-
ment quality does not determine inferential validity,
many NLI systems could be improved by harnessing
a well-designed NLI aligner.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.992307380952381">
Given the extensive literature on phrase-based MT,
it may be helpful further to situate our phrase-based
alignment model in relation to past work. The stan-
dard approach to training a phrase-based MT system
is to apply phrase extraction heuristics using word-
aligned training sets (Och and Ney, 2003; Koehn
et al., 2007). Unfortunately, word alignment mod-
els assume that source words are individually trans-
14For good results, it may be necessary to normalize the
alignment score. Scores from MANLI were normalized by the
number of tokens in the problem. The Stanford aligner performs
a similar normalization internally.
lated into target words, which stands at odds with
the key assumption in phrase-based systems that
many translations are non-compositional. More re-
cently, several works (Marcu and Wong, 2002; De-
Nero et al., 2006; Birch et al., 2006; DeNero and
Klein, 2008) have presented more unified phrase-
based systems that jointly align and weight phrases,
though these systems have not come close to the
state of the art when evaluated in terms of MT per-
formance.
We would argue that previous work in MT phrase
alignment is orthogonal to our work. In MANLI,
the need for phrases arises when word-based rep-
resentations are not appropriate for alignment (e.g.,
between close down and terminate), though longer
phrases are not needed to achieve good alignment
quality. In MT phrase alignment, it is beneficial to
account for arbitrarily large phrases, since the larger
contexts offered by these phrases can help realize
more dependencies among translated words (e.g.,
word order, agreement, subcategorization). Per-
haps because MT phrase alignment is dealing with
much larger contexts, no existing work in MT phrase
alignment (to our knowledge) directly models word
insertions and deletions, as in MANLI. For exam-
ple, in figure 1, MANLI can just skip In most Pacific
countries there, while an MT phrase-based model
would presumably align In most Pacific countries
there are to Women are. Hence, previous work is
of limited applicability to our problem.
</bodyText>
<sectionHeader confidence="0.997808" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999710642857143">
While MT aligners succeed by unsupervised learn-
ing of word correspondences from massive amounts
of bitext, NLI aligners are forced to rely on smaller
quantities of supervised training data. With the
MANLI system, we have demonstrated how to over-
come this lack of data by utilizing external lexical
resources, and how to gain additional power from a
phrase-based representation of alignment.
Acknowledgements The authors wish to thank the
anonymous reviewers for their helpful comments on
an earlier draft of this paper. This paper is based
on work funded in part by the Defense Advanced
Research Projects Agency through IBM and in part
by the CIA ATP as part of the OCCAM project.
</bodyText>
<page confidence="0.993394">
810
</page>
<sectionHeader confidence="0.995888" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941284210526">
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-
colo, B. Magnini, and I. Szpektor. 2006. The Sec-
ond PASCAL Recognising Textual Entailment Chal-
lenge. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch. 2007.
Semantic Inference at the Lexical-Syntactic Level. In
Proceedings of AAAI-07.
A. Birch, C. Callison-Burch, M. Osborne, and P. Koehn.
2006. Constraining the Phrase-Based, Joint Probabil-
ity Statistical Translation Model. In Proceedings of the
ACL-06 Workshop on Statistical Machine Translation.
C. Brockett. 2007. Aligning the RTE 2006 Corpus. Tech-
nical Report MSR-TR-2007-77, Microsoft Research.
P. F. Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer.
1993. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 19(2):263–311.
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M. C. de Marneffe, D. Ramage,
E. Yeh, and C. D. Manning. 2007. Learning Align-
ments and Leveraging Natural Logic. In Proceedings
of the ACL-07 Workshop on Textual Entailment and
Paraphrasing.
M. Collins. 2002. Discriminative training methods for
hidden Markov models. In Proceedings of EMNLP-
02.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL Recognising Textual Entailment Challenge. In
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
J. DeNero and D. Klein. 2008. The Complexity of Phrase
Alignment Problems. In Proceedings ofACL/HLT-08:
Short Papers, pages 25–28.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Surface
Heuristics. In Proceedings of the ACL-06 Workshop on
Statistical Machine Translation, pages 31–38.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised
construction of large paraphrase corpora. In Proceed-
ings of COLING-04.
A. Fraser and D. Marcu. 2007. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, 33(3):293–303.
O. Glickman, I. Dagan, and M. Koppel. 2005. Web
based probabilistic textual entailment. In Proceedings
of the PASCAL Challenges Workshop on Recognizing
Textual Entailment.
A. Hickl and J. Bensley. 2007. A Discourse
Commitment-Based Framework for Recognizing Tex-
tual Entailment. In ACL-07 Workshop on Textual En-
tailment and Paraphrasing, Prague.
A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink,
and Y. Shi. 2006. Recognizing Textual Entailment
with LCC’s GROUNDHOG System. In Proceedings
of the Second PASCAL Challenges Workshop on Rec-
ognizing Textual Entailment.
J. J. Jiang and D. W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics.
V. Jijkoun and M. de Rijke. 2005. Recognizing tex-
tual entailment using lexical similarity. In Proceedings
of the PASCAL Challenges Workshop on Recognizing
Textual Entailment, pages 73–76.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL-07, demonstration session.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
Agreement. In Proceedings of NAACL-06, New York.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING/ACL-98, pages
768–774, Montreal, Canada.
B. MacCartney, T. Grenager, M. C. de Marneffe, D. Cer,
and C. D. Manning. 2006. Learning to Recognize
Features of Valid Textual Entailments. In Proceedings
of NAACL-06, New York.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proceedings of EMNLP-02, pages 133–139.
E. Marsi and E. Krahmer. 2005. Classification of se-
mantic relations by humans and machines. In ACL-05
Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, Ann Arbor.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19–51.
M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In
Proceedings of ACL-07.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of COLING-96, pages 836–841, Copen-
hagen, Denmark.
</reference>
<page confidence="0.998369">
811
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949205">
<title confidence="0.997879">A Phrase-Based Alignment Model for Natural Language Inference</title>
<author confidence="0.998208">Bill MacCartney</author>
<author confidence="0.998208">Michel Galley</author>
<author confidence="0.998208">D Christopher</author>
<affiliation confidence="0.992519">Natural Language Processing Group, Stanford University</affiliation>
<abstract confidence="0.998100590909091">The alignment problem—establishing links between corresponding phrases in two related sentences—is as important in natural language inference (NLI) as it is in machine translation (MT). But the tools and techniques of MT alignment do not readily transfer to NLI, where one cannot assume semantic equivalence, and for which large volumes of bitext are lacking. We present a new NLI aligner, the MANLI system, designed to address these challenges. It uses a phrase-based alignment representation, exploits external lexical resources, and capitalizes on a new set of supervised training data. We compare the performance of MANLI to existing NLI and MT aligners on an NLI alignment task over the well-known Recognizing Textual Entailment data. We show that MANLI significantly outperforms existing aligners, achieving gains of in a representative NLI aligner and 10.5% over GIZA++.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>I Dagan</author>
<author>B Dolan</author>
<author>L Ferro</author>
<author>D Giampiccolo</author>
<author>B Magnini</author>
<author>I Szpektor</author>
</authors>
<title>The Second PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="6321" citStr="Bar-Haim et al., 2006" startWordPosition="974" endWordPosition="977">to NLI alignment. We investigate the matter empirically in section 5.2. 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment annoIn most Pacific countries there are very few women in parliament . Figure 1: The MSR gold-standard alignment for problem 116 from the RTE2 development set. tations (Brockett, 2007) for inference problems from the second Recognizing Textual Entailment (RTE2) challenge (Bar-Haim et al., 2006). To our knowledge, this work is the first to exploit this data for training and evaluation of NLI alignment models. The RTE2 data consists of a development set and a test set, each containing 800 inference problems. Each problem consists of a premise and a hypothesis. The premises contain 29 words on average; the hypotheses, 11 words. Each problem is marked as a valid or invalid inference (50% each); however, these annotations are ignored during alignment, since they would not be available during testing of a complete NLI system. The MSR annotations use an alignment representation which is to</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor. 2006. The Second PASCAL Recognising Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>I Dagan</author>
<author>I Greental</author>
<author>E Shnarch</author>
</authors>
<title>Semantic Inference at the Lexical-Syntactic Level.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI-07.</booktitle>
<contexts>
<context position="2278" citStr="Bar-Haim et al. (2007)" startWordPosition="349" endWordPosition="352">s in P and H. Recent entries in the annual Recognizing Textual Entailment (RTE) competition (Dagan et al., 2005) have addressed the alignment problem in a variety of ways, though often without distinguishing it as a separate subproblem. Glickman et al. (2005) and Jijkoun and de Rijke (2005), among others, have explored approaches based on measuring the degree of lexical overlap between bags of words. While ignoring structure, such methods depend on matching each word in H to the word in P with which it is most similar—in effect, an alignment. At the other extreme, Tatu and Moldovan (2007) and Bar-Haim et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is implicit in the steps of the proof. Increasingly, however, the most successful RTE systems have made the alignment problem explicit. Marsi and Krahmer (2005) and MacCartney et al. (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al. (2006) and Hickl and Bensl</context>
</contexts>
<marker>Bar-Haim, Dagan, Greental, Shnarch, 2007</marker>
<rawString>R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch. 2007. Semantic Inference at the Lexical-Syntactic Level. In Proceedings of AAAI-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>Constraining the Phrase-Based, Joint Probability Statistical Translation Model.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL-06 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="36071" citStr="Birch et al., 2006" startWordPosition="5950" endWordPosition="5953">tion heuristics using wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebased systems that jointly align and weight phrases, though these systems have not come close to the state of the art when evaluated in terms of MT performance. We would argue that previous work in MT phrase alignment is orthogonal to our work. In MANLI, the need for phrases arises when word-based representations are not appropriate for alignment (e.g., between close down and terminate), though longer phrases are not needed to achieve good alignment quality. In MT phrase alignment, it is beneficial to account for arbitrarily large phr</context>
</contexts>
<marker>Birch, Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>A. Birch, C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Constraining the Phrase-Based, Joint Probability Statistical Translation Model. In Proceedings of the ACL-06 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockett</author>
</authors>
<title>Aligning the RTE</title>
<date>2007</date>
<tech>Technical Report MSR-TR-2007-77, Microsoft Research.</tech>
<contexts>
<context position="6210" citStr="Brockett, 2007" startWordPosition="961" endWordPosition="962">f semantic relatedness. Consequently, the tools and techniques of MT alignment may not transfer readily to NLI alignment. We investigate the matter empirically in section 5.2. 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment annoIn most Pacific countries there are very few women in parliament . Figure 1: The MSR gold-standard alignment for problem 116 from the RTE2 development set. tations (Brockett, 2007) for inference problems from the second Recognizing Textual Entailment (RTE2) challenge (Bar-Haim et al., 2006). To our knowledge, this work is the first to exploit this data for training and evaluation of NLI alignment models. The RTE2 data consists of a development set and a test set, each containing 800 inference problems. Each problem consists of a premise and a hypothesis. The premises contain 29 words on average; the hypotheses, 11 words. Each problem is marked as a valid or invalid inference (50% each); however, these annotations are ignored during alignment, since they would not be ava</context>
<context position="7783" citStr="Brockett (2007)" startWordPosition="1214" endWordPosition="1215">king this distinction, the annotators have followed a convention common in MT, which permits alignment precision to be measured against both SURE and POSSIBLE links, while recall is measured against only SURE links. In this work, however, we have chosen to ignore POSSIBLE links, embracing the argument made by (Fraser and Marcu, 2007) that their use has impeded progress in MT alignment models, and that SURE803 only annotation is to be preferred. Each RTE2 problem was independently annotated by three people, following carefully designed annotation guidelines. Inter-annotator agreement was high: Brockett (2007) reports Fleiss’ kappa1 scores of about 0.73 (“substantial agreement”) for mappings from H tokens to P tokens; and all three annotators agreed on —70% of proposed links, while at least two of three agreed on more than 99.7% of proposed links,2 attesting to the high quality of the annotation data. For this work, we merged the three independent annotations, using majority rule,3 to obtain a gold-standard annotation containing an average of 7.3 links per RTE problem. 4 The MANLI aligner In this section, we describe the MANLI aligner, a new alignment system designed expressly for NLI alignment. Th</context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>C. Brockett. 2007. Aligning the RTE 2006 Corpus. Technical Report MSR-TR-2007-77, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4102" citStr="Brown et al., 1993" startWordPosition="627" endWordPosition="630">stigate whether existing MT aligners can usefully be applied in the NLI setting. 2 NLI alignment vs. MT alignment The alignment problem is familiar in machine translation (MT), where recognizing that she came is a good translation for elle est venue requires establish802 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802–811, Honolulu, October 2008.c�2008 Association for Computational Linguistics ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clau</context>
<context position="23774" citStr="Brown et al., 1993" startWordPosition="3954" endWordPosition="3957">re N is the number of documents in the corpus, and N. is the number of documents containing word w. 5.2 MT aligners: GIZA++ and Cross-EM Given the importance of alignment for NLI, and the availability of standard, proven tools for MT alignment, an obvious question presents itself: why not use an off-the-shelf MT aligner for NLI? Although we have argued (section 2) that this is unlikely to succeed, to our knowledge, we are the first to investigate the matter empirically.11 The best-known MT aligner is undoubtedly GIZA++ (Och and Ney, 2003), which contains implementations of various IBM models (Brown et al., 1993), as well as the HMM model of Vogel et al. (1996). Most practitioners use GIZA++ as a black box, via the Moses MT toolkit (Koehn et al., 2007). We followed this practice, running with Moses’ default parameters on the RTE2 data to obtain asymmetric word alignments in both directions (P-to-H and H-to-P). We then performed symmetrization using the well-known INTERSECTION heuristic. Unsurprisingly, the out-of-the-box performance was quite poor, with most words aligned apparently at random. Precision was fair (72%) but recall was very poor (46%). Even equal words were usually not aligned—because GI</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>D Cer</author>
<author>T Grenager</author>
<author>D Hall</author>
<author>C Kiddon</author>
<author>B MacCartney</author>
<author>M C de Marneffe</author>
<author>D Ramage</author>
<author>E Yeh</author>
<author>C D Manning</author>
</authors>
<title>Learning Alignments and Leveraging Natural Logic.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-07 Workshop on Textual Entailment and Paraphrasing.</booktitle>
<marker>Chambers, Cer, Grenager, Hall, Kiddon, MacCartney, de Marneffe, Ramage, Yeh, Manning, 2007</marker>
<rawString>N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kiddon, B. MacCartney, M. C. de Marneffe, D. Ramage, E. Yeh, and C. D. Manning. 2007. Learning Alignments and Leveraging Natural Logic. In Proceedings of the ACL-07 Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP02.</booktitle>
<contexts>
<context position="17439" citStr="Collins, 2002" startWordPosition="2840" endWordPosition="2841">converges on a nearoptimal alignment. Clever use of memoization helps to ensure that computational costs remain manageable. Using the parameter values suggested in figure 2, aligning an average RTE problem takes about two seconds. While MANLI-ALIGN is not guaranteed to produce optimal alignments, there is reason to believe that it usually comes very close. After training, the alignment found by MANLI scored at least as high as the gold alignment for 99.6% of RTE problems.6 4.4 Perceptron learning To tune the parameters w of the model, we use an adaptation of the averaged perceptron algorithm (Collins, 2002), which has proven successful on a range of NLP tasks. The algorithm is shown in figure 3. After initializing w to 0, we perform N training epochs. (Our experiments used N = 50.) In each epoch, we iterate through the training data, updating the weight vector at each training example according to the difference between the features of the target alignment and the features of the alignment produced by the decoder using the current weight vector. The size of the update is controlled by a learning rate which decreases over time. At the end of each epoch, the weight vector is normalized and stored.</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models. In Proceedings of EMNLP02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1768" citStr="Dagan et al., 2005" startWordPosition="262" endWordPosition="265">nguage inference (NLI) is to determine whether a natural-language hypothesis H can reasonably be inferred from a given premise text P. In order to recognize that Kennedy was killed can be inferred from JFK was assassinated, one must first recognize the correspondence between Kennedy and JFK, and between killed and assassinated. Consequently, most current approaches to NLI rely, implicitly or explicitly, on a facility for alignment—that is, establishing links between corresponding entities and predicates in P and H. Recent entries in the annual Recognizing Textual Entailment (RTE) competition (Dagan et al., 2005) have addressed the alignment problem in a variety of ways, though often without distinguishing it as a separate subproblem. Glickman et al. (2005) and Jijkoun and de Rijke (2005), among others, have explored approaches based on measuring the degree of lexical overlap between bags of words. While ignoring structure, such methods depend on matching each word in H to the word in P with which it is most similar—in effect, an alignment. At the other extreme, Tatu and Moldovan (2007) and Bar-Haim et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rul</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL Recognising Textual Entailment Challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>The Complexity of Phrase Alignment Problems.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL/HLT-08: Short Papers,</booktitle>
<pages>25--28</pages>
<contexts>
<context position="36096" citStr="DeNero and Klein, 2008" startWordPosition="5954" endWordPosition="5957">g wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebased systems that jointly align and weight phrases, though these systems have not come close to the state of the art when evaluated in terms of MT performance. We would argue that previous work in MT phrase alignment is orthogonal to our work. In MANLI, the need for phrases arises when word-based representations are not appropriate for alignment (e.g., between close down and terminate), though longer phrases are not needed to achieve good alignment quality. In MT phrase alignment, it is beneficial to account for arbitrarily large phrases, since the larger co</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>J. DeNero and D. Klein. 2008. The Complexity of Phrase Alignment Problems. In Proceedings ofACL/HLT-08: Short Papers, pages 25–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Gillick</author>
<author>J Zhang</author>
<author>D Klein</author>
</authors>
<title>Why Generative Phrase Models Underperform Surface Heuristics.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL-06 Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="4166" citStr="DeNero et al., 2006" startWordPosition="639" endWordPosition="642">n the NLI setting. 2 NLI alignment vs. MT alignment The alignment problem is familiar in machine translation (MT), where recognizing that she came is a good translation for elle est venue requires establish802 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802–811, Honolulu, October 2008.c�2008 Association for Computational Linguistics ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3. Indeed, one cannot assume</context>
<context position="36051" citStr="DeNero et al., 2006" startWordPosition="5945" endWordPosition="5949">o apply phrase extraction heuristics using wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebased systems that jointly align and weight phrases, though these systems have not come close to the state of the art when evaluated in terms of MT performance. We would argue that previous work in MT phrase alignment is orthogonal to our work. In MANLI, the need for phrases arises when word-based representations are not appropriate for alignment (e.g., between close down and terminate), though longer phrases are not needed to achieve good alignment quality. In MT phrase alignment, it is beneficial to account for a</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006. Why Generative Phrase Models Underperform Surface Heuristics. In Proceedings of the ACL-06 Workshop on Statistical Machine Translation, pages 31–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04.</booktitle>
<contexts>
<context position="24583" citStr="Dolan et al. (2004)" startWordPosition="4084" endWordPosition="4087"> default parameters on the RTE2 data to obtain asymmetric word alignments in both directions (P-to-H and H-to-P). We then performed symmetrization using the well-known INTERSECTION heuristic. Unsurprisingly, the out-of-the-box performance was quite poor, with most words aligned apparently at random. Precision was fair (72%) but recall was very poor (46%). Even equal words were usually not aligned—because GIZA++ is designed for crosslinguistic use, it does not consider word equality between source and target sentences. To remedy this, we supplied GIZA++ with a lexicon, using a trick 11However, Dolan et al. (2004) explore a closely-related topic: using an MT aligner to identify paraphrases. score(h|P) = logmax sim(p, h) PEP score(H|P) = |H |1: idf (h) · score (h|P) hEH 807 common in MT: we supplemented the training data with synthetic data consisting of matched pairs of equal words. This gives GIZA++ a better chance of learning that, e.g., man should align with man. The result was a big boost in recall (+23%), and a smaller gain in precision. The results for GIZA++ shown in table 1 are based on using the lexicon and INTERSECTION. With these settings, GIZA++ properly aligned most pairs of equal words, b</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised construction of large paraphrase corpora. In Proceedings of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Measuring Word Alignment Quality for Statistical Machine Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="7503" citStr="Fraser and Marcu, 2007" startWordPosition="1171" endWordPosition="1174">e an alignment representation which is token-based, but many-to-many, and thus allows implicit alignment of multi-word phrases. Figure 1 shows an example in which very few has been aligned with poorly represented. In the MSR data, every alignment link is marked as SURE or POSSIBLE. In making this distinction, the annotators have followed a convention common in MT, which permits alignment precision to be measured against both SURE and POSSIBLE links, while recall is measured against only SURE links. In this work, however, we have chosen to ignore POSSIBLE links, embracing the argument made by (Fraser and Marcu, 2007) that their use has impeded progress in MT alignment models, and that SURE803 only annotation is to be preferred. Each RTE2 problem was independently annotated by three people, following carefully designed annotation guidelines. Inter-annotator agreement was high: Brockett (2007) reports Fleiss’ kappa1 scores of about 0.73 (“substantial agreement”) for mappings from H tokens to P tokens; and all three annotators agreed on —70% of proposed links, while at least two of three agreed on more than 99.7% of proposed links,2 attesting to the high quality of the annotation data. For this work, we merg</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>A. Fraser and D. Marcu. 2007. Measuring Word Alignment Quality for Statistical Machine Translation. Computational Linguistics, 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Glickman</author>
<author>I Dagan</author>
<author>M Koppel</author>
</authors>
<title>Web based probabilistic textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment.</booktitle>
<contexts>
<context position="1915" citStr="Glickman et al. (2005)" startWordPosition="285" endWordPosition="288">to recognize that Kennedy was killed can be inferred from JFK was assassinated, one must first recognize the correspondence between Kennedy and JFK, and between killed and assassinated. Consequently, most current approaches to NLI rely, implicitly or explicitly, on a facility for alignment—that is, establishing links between corresponding entities and predicates in P and H. Recent entries in the annual Recognizing Textual Entailment (RTE) competition (Dagan et al., 2005) have addressed the alignment problem in a variety of ways, though often without distinguishing it as a separate subproblem. Glickman et al. (2005) and Jijkoun and de Rijke (2005), among others, have explored approaches based on measuring the degree of lexical overlap between bags of words. While ignoring structure, such methods depend on matching each word in H to the word in P with which it is most similar—in effect, an alignment. At the other extreme, Tatu and Moldovan (2007) and Bar-Haim et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is impli</context>
<context position="21076" citStr="Glickman et al. (2005)" startWordPosition="3470" endWordPosition="3474"> NLI, where the ultimate goal is to maximize the number of inference problems answered correctly, it is more fitting to give all problems equal weight, and so we macro-average. We have also generated all results using micro-averaging, and found that the relative comparisons are Ej)) 806 port the exact match rate, that is, the proportion of problems in which the guessed alignment exactly matches the gold alignment. The results are summarized in table 1. 5.1 A robust baseline: the bag-of-words aligner As a baseline, we use a simple alignment algorithm inspired by the lexical entailment model of Glickman et al. (2005), and similar to the simple heuristic model described in (Och and Ney, 2003). Each hypothesis word h is aligned to the premise word p to which it is most similar, according to a lexical similarity function sim(p, h) which returns scores in [0, 1]. While Glickman et al. used a function based on web co-occurrence statistics, we use a much simpler function based on string edit distance: System Data P % R % Fl % E % Bag-of-words dev 57.8 81.2 67.5 3.5 (baseline) test 62.1 82.6 70.9 5.3 GIZA++ dev 83.0 66.4 72.1 9.4 (using lex, n) test 85.1 69.1 74.8 11.3 Cross-EM dev 67.6 80.1 72.1 1.3 (using lex,</context>
<context position="33740" citStr="Glickman et al., 2005" startWordPosition="5566" endWordPosition="5569">e impact of different aligners on the ability to recognize valid inferences. If a high-scoring alignment indicates a close correspondence between H and P, does this also indicate a valid inference? We have previously emphasized (MacCartney et al., 2006) that there is more to inferential validity than close lexical or structural correspondence: negations, modals, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment. Nevertheless, alignment score can be a strong predictor of inferential validity, and some NLI systems (e.g., (Glickman et al., 2005)) rely entirely on some measure of alignment quality to predict validity. 809 System data acc % avgP % Bag-of-words aligner dev 61.3 61.5 test 57.9 58.9 Stanford RTE aligner dev 63.1 64.9 test 60.9 59.2 MANLI aligner dev 59.3 69.0 (this work) test 60.3 61.0 RTE2 entries (average) test 58.5 59.1 LCC (Hickl et al., 2006) test 75.4 80.8 Table 2: Performance of various aligners and complete RTE systems in predicting RTE2 answers. The columns show the data set used, accuracy, and average precision (the recommended metric for RTE2). If an aligner generates real-valued alignment scores, we can use th</context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>O. Glickman, I. Dagan, and M. Koppel. 2005. Web based probabilistic textual entailment. In Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>J Bensley</author>
</authors>
<title>A Discourse Commitment-Based Framework for Recognizing Textual Entailment.</title>
<date>2007</date>
<booktitle>In ACL-07 Workshop on Textual Entailment and Paraphrasing,</booktitle>
<location>Prague.</location>
<contexts>
<context position="2887" citStr="Hickl and Bensley (2007)" startWordPosition="442" endWordPosition="445">m et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is implicit in the steps of the proof. Increasingly, however, the most successful RTE systems have made the alignment problem explicit. Marsi and Krahmer (2005) and MacCartney et al. (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al. (2006) and Hickl and Bensley (2007). However, each of these systems has pursued alignment in idiosyncratic and poorly-documented ways, often using proprietary data, making comparisons and further development difficult. In this paper we undertake the first systematic study of alignment for NLI. We propose a new NLI alignment system which uses a phrase-based representation of alignment, exploits external resources for knowledge of semantic relatedness, and capitalizes on the recent appearance of new supervised training data for NLI alignment. In addition, we examine the relation between NLI alignment and MT alignment, and investi</context>
</contexts>
<marker>Hickl, Bensley, 2007</marker>
<rawString>A. Hickl and J. Bensley. 2007. A Discourse Commitment-Based Framework for Recognizing Textual Entailment. In ACL-07 Workshop on Textual Entailment and Paraphrasing, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>J Williams</author>
<author>J Bensley</author>
<author>K Roberts</author>
<author>B Rink</author>
<author>Y Shi</author>
</authors>
<title>Recognizing Textual Entailment with LCC’s GROUNDHOG System.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</booktitle>
<contexts>
<context position="2858" citStr="Hickl et al. (2006)" startWordPosition="437" endWordPosition="440">dovan (2007) and Bar-Haim et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is implicit in the steps of the proof. Increasingly, however, the most successful RTE systems have made the alignment problem explicit. Marsi and Krahmer (2005) and MacCartney et al. (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al. (2006) and Hickl and Bensley (2007). However, each of these systems has pursued alignment in idiosyncratic and poorly-documented ways, often using proprietary data, making comparisons and further development difficult. In this paper we undertake the first systematic study of alignment for NLI. We propose a new NLI alignment system which uses a phrase-based representation of alignment, exploits external resources for knowledge of semantic relatedness, and capitalizes on the recent appearance of new supervised training data for NLI alignment. In addition, we examine the relation between NLI alignment </context>
<context position="34060" citStr="Hickl et al., 2006" startWordPosition="5622" endWordPosition="5625">ctural correspondence: negations, modals, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment. Nevertheless, alignment score can be a strong predictor of inferential validity, and some NLI systems (e.g., (Glickman et al., 2005)) rely entirely on some measure of alignment quality to predict validity. 809 System data acc % avgP % Bag-of-words aligner dev 61.3 61.5 test 57.9 58.9 Stanford RTE aligner dev 63.1 64.9 test 60.9 59.2 MANLI aligner dev 59.3 69.0 (this work) test 60.3 61.0 RTE2 entries (average) test 58.5 59.1 LCC (Hickl et al., 2006) test 75.4 80.8 Table 2: Performance of various aligners and complete RTE systems in predicting RTE2 answers. The columns show the data set used, accuracy, and average precision (the recommended metric for RTE2). If an aligner generates real-valued alignment scores, we can use the RTE data to test its ability to predict inferential validity with the following simple method. For a given RTE problem, we predict YES (valid) if its alignment score14 exceeds a threshold T, and NO otherwise. We tune T to maximize accuracy on the RTE2 development set, and then measure performance on the RTE2 test set</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink, and Y. Shi. 2006. Recognizing Textual Entailment with LCC’s GROUNDHOG System. In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics.</booktitle>
<contexts>
<context position="13216" citStr="Jiang and Conrath (1997)" startWordPosition="2122" endWordPosition="2125"> involved in the edit, and whether these phrases are non-constituents (in syntactic parses of the sentences involved). Lexical similarity feature. For SUB edits, a very important feature represents the lexical similarity of the substituends, as a real value in [0, 1]. This similarity score is computed as a max over a number of component scoring functions, some based on external lexical resources, including: • various string similarity functions, of which most are applied to word lemmas • measures of synonymy, hypernymy, antonymy, and semantic relatedness, including a widelyused measure due to Jiang and Conrath (1997), based on manually constructed lexical resources such as WordNet and NomBank • a function based on the well-known distributional similarity metric of Lin (1998), which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources— both manually and automatically constructed—is critical to the success of MANLI. Contextual features. Even when the lexical similarity for a SUB edit is high, it may not be a good match. If P or H contains multiple occurrences of the same word—which happens freq</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. J. Jiang and D. W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M de Rijke</author>
</authors>
<title>Recognizing textual entailment using lexical similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment,</booktitle>
<pages>73--76</pages>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>V. Jijkoun and M. de Rijke. 2005. Recognizing textual entailment using lexical similarity. In Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment, pages 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07, demonstration session.</booktitle>
<contexts>
<context position="23916" citStr="Koehn et al., 2007" startWordPosition="3982" endWordPosition="3985"> the importance of alignment for NLI, and the availability of standard, proven tools for MT alignment, an obvious question presents itself: why not use an off-the-shelf MT aligner for NLI? Although we have argued (section 2) that this is unlikely to succeed, to our knowledge, we are the first to investigate the matter empirically.11 The best-known MT aligner is undoubtedly GIZA++ (Och and Ney, 2003), which contains implementations of various IBM models (Brown et al., 1993), as well as the HMM model of Vogel et al. (1996). Most practitioners use GIZA++ as a black box, via the Moses MT toolkit (Koehn et al., 2007). We followed this practice, running with Moses’ default parameters on the RTE2 data to obtain asymmetric word alignments in both directions (P-to-H and H-to-P). We then performed symmetrization using the well-known INTERSECTION heuristic. Unsurprisingly, the out-of-the-box performance was quite poor, with most words aligned apparently at random. Precision was fair (72%) but recall was very poor (46%). Even equal words were usually not aligned—because GIZA++ is designed for crosslinguistic use, it does not consider word equality between source and target sentences. To remedy this, we supplied </context>
<context position="35540" citStr="Koehn et al., 2007" startWordPosition="5867" endWordPosition="5870">performance of the LCC system, all achieve respectable results, and the Stanford and MANLI aligners outperform the average RTE2 entry. Thus, even if alignment quality does not determine inferential validity, many NLI systems could be improved by harnessing a well-designed NLI aligner. 7 Related work Given the extensive literature on phrase-based MT, it may be helpful further to situate our phrase-based alignment model in relation to past work. The standard approach to training a phrase-based MT system is to apply phrase extraction heuristics using wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebased sys</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL-07, demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by Agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL-06,</booktitle>
<location>New York.</location>
<contexts>
<context position="25819" citStr="Liang et al., 2006" startWordPosition="4288" endWordPosition="4291">lign other words apparently at random. Next, we compared the performance of INTERSECTION with other symmetrization heuristics defined in Moses—including UNION, GROW, GROWDIAG, GROW-DIAG-FINAL (the default), and GROWDIAG-FINAL-AND—and with asymmetric alignments in both directions. While all these alternatives achieved better recall than INTERSECTION, all showed substantially worse precision and F1. On the RTE2 test set, the asymmetric alignment from H to P scored 68% in F1; GROW scored 58%; and all other alternatives scored below 52%. As an additional experiment, we tested the CrossEM aligner (Liang et al., 2006) from the BerkeleyAligner package on the MSR data. While this aligner is in many ways simpler than GIZA++ (it lacks any model of fertility, for example), its method of jointly training two simple asymmetric HMM models has outperformed GIZA++ on standard evaluations of MT alignment. As with GIZA++, we experimented with a variety of symmetrization heuristics, and ran trials with and without a supplemental lexicon. The results were broadly similar: INTERSECTION greatly outperformed alternative heuristics, and using a lexicon provided a big boost (up to 12% in F1). Under optimal settings, the Cros</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by Agreement. In Proceedings of NAACL-06, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-98,</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="13377" citStr="Lin (1998)" startWordPosition="2150" endWordPosition="2151">ant feature represents the lexical similarity of the substituends, as a real value in [0, 1]. This similarity score is computed as a max over a number of component scoring functions, some based on external lexical resources, including: • various string similarity functions, of which most are applied to word lemmas • measures of synonymy, hypernymy, antonymy, and semantic relatedness, including a widelyused measure due to Jiang and Conrath (1997), based on manually constructed lexical resources such as WordNet and NomBank • a function based on the well-known distributional similarity metric of Lin (1998), which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources— both manually and automatically constructed—is critical to the success of MANLI. Contextual features. Even when the lexical similarity for a SUB edit is high, it may not be a good match. If P or H contains multiple occurrences of the same word—which happens frequently with function words, and occasionally with content words—lexical similarity may not suffice to determine the right match. To remedy this, we introduce con</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL-98, pages 768–774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>T Grenager</author>
<author>M C de Marneffe</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>Learning to Recognize Features of Valid Textual Entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL-06,</booktitle>
<location>New York.</location>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>B. MacCartney, T. Grenager, M. C. de Marneffe, D. Cer, and C. D. Manning. 2006. Learning to Recognize Features of Valid Textual Entailments. In Proceedings of NAACL-06, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-02,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="4144" citStr="Marcu and Wong, 2002" startWordPosition="635" endWordPosition="638"> usefully be applied in the NLI setting. 2 NLI alignment vs. MT alignment The alignment problem is familiar in machine translation (MT), where recognizing that she came is a good translation for elle est venue requires establish802 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802–811, Honolulu, October 2008.c�2008 Association for Computational Linguistics ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3. Ind</context>
<context position="36030" citStr="Marcu and Wong, 2002" startWordPosition="5941" endWordPosition="5944">e-based MT system is to apply phrase extraction heuristics using wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebased systems that jointly align and weight phrases, though these systems have not come close to the state of the art when evaluated in terms of MT performance. We would argue that previous work in MT phrase alignment is orthogonal to our work. In MANLI, the need for phrases arises when word-based representations are not appropriate for alignment (e.g., between close down and terminate), though longer phrases are not needed to achieve good alignment quality. In MT phrase alignment, it is benefi</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and W. Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of EMNLP-02, pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Marsi</author>
<author>E Krahmer</author>
</authors>
<title>Classification of semantic relations by humans and machines.</title>
<date>2005</date>
<booktitle>In ACL-05 Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<location>Ann Arbor.</location>
<contexts>
<context position="2667" citStr="Marsi and Krahmer (2005)" startWordPosition="409" endWordPosition="412">n bags of words. While ignoring structure, such methods depend on matching each word in H to the word in P with which it is most similar—in effect, an alignment. At the other extreme, Tatu and Moldovan (2007) and Bar-Haim et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is implicit in the steps of the proof. Increasingly, however, the most successful RTE systems have made the alignment problem explicit. Marsi and Krahmer (2005) and MacCartney et al. (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al. (2006) and Hickl and Bensley (2007). However, each of these systems has pursued alignment in idiosyncratic and poorly-documented ways, often using proprietary data, making comparisons and further development difficult. In this paper we undertake the first systematic study of alignment for NLI. We propose a new NLI alignment system which uses a phrase-based representation of alignment, exploits external resources</context>
</contexts>
<marker>Marsi, Krahmer, 2005</marker>
<rawString>E. Marsi and E. Krahmer. 2005. Classification of semantic relations by humans and machines. In ACL-05 Workshop on Empirical Modeling of Semantic Equivalence and Entailment, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4244" citStr="Och and Ney, 2003" startWordPosition="652" endWordPosition="655">iliar in machine translation (MT), where recognizing that she came is a good translation for elle est venue requires establish802 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802–811, Honolulu, October 2008.c�2008 Association for Computational Linguistics ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3. Indeed, one cannot assume even approximate semantic equivalence—usually a given in MT. Because NLI prob</context>
<context position="21152" citStr="Och and Ney, 2003" startWordPosition="3484" endWordPosition="3487">swered correctly, it is more fitting to give all problems equal weight, and so we macro-average. We have also generated all results using micro-averaging, and found that the relative comparisons are Ej)) 806 port the exact match rate, that is, the proportion of problems in which the guessed alignment exactly matches the gold alignment. The results are summarized in table 1. 5.1 A robust baseline: the bag-of-words aligner As a baseline, we use a simple alignment algorithm inspired by the lexical entailment model of Glickman et al. (2005), and similar to the simple heuristic model described in (Och and Ney, 2003). Each hypothesis word h is aligned to the premise word p to which it is most similar, according to a lexical similarity function sim(p, h) which returns scores in [0, 1]. While Glickman et al. used a function based on web co-occurrence statistics, we use a much simpler function based on string edit distance: System Data P % R % Fl % E % Bag-of-words dev 57.8 81.2 67.5 3.5 (baseline) test 62.1 82.6 70.9 5.3 GIZA++ dev 83.0 66.4 72.1 9.4 (using lex, n) test 85.1 69.1 74.8 11.3 Cross-EM dev 67.6 80.1 72.1 1.3 (using lex, n) test 70.3 81.0 74.1 0.8 Stanford RTE dev 81.1 61.2 69.7 0.5 test 82.7 61</context>
<context position="23699" citStr="Och and Ney, 2003" startWordPosition="3942" endWordPosition="3945"> was not pursued. not greatly affected. 10We use idf(w) = log(N/N,,,), where N is the number of documents in the corpus, and N. is the number of documents containing word w. 5.2 MT aligners: GIZA++ and Cross-EM Given the importance of alignment for NLI, and the availability of standard, proven tools for MT alignment, an obvious question presents itself: why not use an off-the-shelf MT aligner for NLI? Although we have argued (section 2) that this is unlikely to succeed, to our knowledge, we are the first to investigate the matter empirically.11 The best-known MT aligner is undoubtedly GIZA++ (Och and Ney, 2003), which contains implementations of various IBM models (Brown et al., 1993), as well as the HMM model of Vogel et al. (1996). Most practitioners use GIZA++ as a black box, via the Moses MT toolkit (Koehn et al., 2007). We followed this practice, running with Moses’ default parameters on the RTE2 data to obtain asymmetric word alignments in both directions (P-to-H and H-to-P). We then performed symmetrization using the well-known INTERSECTION heuristic. Unsurprisingly, the out-of-the-box performance was quite poor, with most words aligned apparently at random. Precision was fair (72%) but recal</context>
<context position="35519" citStr="Och and Ney, 2003" startWordPosition="5863" endWordPosition="5866">ligners rivals the performance of the LCC system, all achieve respectable results, and the Stanford and MANLI aligners outperform the average RTE2 entry. Thus, even if alignment quality does not determine inferential validity, many NLI systems could be improved by harnessing a well-designed NLI aligner. 7 Related work Given the extensive literature on phrase-based MT, it may be helpful further to situate our phrase-based alignment model in relation to past work. The standard approach to training a phrase-based MT system is to apply phrase extraction heuristics using wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more un</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tatu</author>
<author>D Moldovan</author>
</authors>
<title>COGEX at RTE3.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07.</booktitle>
<contexts>
<context position="2251" citStr="Tatu and Moldovan (2007)" startWordPosition="344" endWordPosition="347">onding entities and predicates in P and H. Recent entries in the annual Recognizing Textual Entailment (RTE) competition (Dagan et al., 2005) have addressed the alignment problem in a variety of ways, though often without distinguishing it as a separate subproblem. Glickman et al. (2005) and Jijkoun and de Rijke (2005), among others, have explored approaches based on measuring the degree of lexical overlap between bags of words. While ignoring structure, such methods depend on matching each word in H to the word in P with which it is most similar—in effect, an alignment. At the other extreme, Tatu and Moldovan (2007) and Bar-Haim et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is implicit in the steps of the proof. Increasingly, however, the most successful RTE systems have made the alignment problem explicit. Marsi and Krahmer (2005) and MacCartney et al. (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al.</context>
</contexts>
<marker>Tatu, Moldovan, 2007</marker>
<rawString>M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In Proceedings of ACL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMMbased word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="4122" citStr="Vogel et al., 1996" startWordPosition="631" endWordPosition="634">ting MT aligners can usefully be applied in the NLI setting. 2 NLI alignment vs. MT alignment The alignment problem is familiar in machine translation (MT), where recognizing that she came is a good translation for elle est venue requires establish802 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802–811, Honolulu, October 2008.c�2008 Association for Computational Linguistics ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no co</context>
<context position="23823" citStr="Vogel et al. (1996)" startWordPosition="3965" endWordPosition="3968">d N. is the number of documents containing word w. 5.2 MT aligners: GIZA++ and Cross-EM Given the importance of alignment for NLI, and the availability of standard, proven tools for MT alignment, an obvious question presents itself: why not use an off-the-shelf MT aligner for NLI? Although we have argued (section 2) that this is unlikely to succeed, to our knowledge, we are the first to investigate the matter empirically.11 The best-known MT aligner is undoubtedly GIZA++ (Och and Ney, 2003), which contains implementations of various IBM models (Brown et al., 1993), as well as the HMM model of Vogel et al. (1996). Most practitioners use GIZA++ as a black box, via the Moses MT toolkit (Koehn et al., 2007). We followed this practice, running with Moses’ default parameters on the RTE2 data to obtain asymmetric word alignments in both directions (P-to-H and H-to-P). We then performed symmetrization using the well-known INTERSECTION heuristic. Unsurprisingly, the out-of-the-box performance was quite poor, with most words aligned apparently at random. Precision was fair (72%) but recall was very poor (46%). Even equal words were usually not aligned—because GIZA++ is designed for crosslinguistic use, it does</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMMbased word alignment in statistical translation. In Proceedings of COLING-96, pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>