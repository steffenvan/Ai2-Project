<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001630">
<title confidence="0.981615">
Generalizing a Strongly Lexicalized Parser using Unlabeled Data
</title>
<author confidence="0.998964">
Tejaswini Deoskar&apos;, Christos Christodoulopoulos2, Alexandra Birch&apos;, Mark Steedman&apos;
</author>
<affiliation confidence="0.9992285">
&apos;School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB
2University of Illinois, Urbana-Champaign, Urbana, IL 61801
</affiliation>
<email confidence="0.997331">
{tdeoskar,abmayne,steedman}@inf.ed.ac.uk, christod@illinois.edu
</email>
<sectionHeader confidence="0.993873" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925894736842">
Statistical parsers trained on labeled data
suffer from sparsity, both grammatical and
lexical. For parsers based on strongly
lexicalized grammar formalisms (such as
CCG, which has complex lexical cate-
gories but simple combinatory rules), the
problem of sparsity can be isolated to
the lexicon. In this paper, we show that
semi-supervised Viterbi-EM can be used
to extend the lexicon of a generative CCG
parser. By learning complex lexical entries
for low-frequency and unseen words from
unlabeled data, we obtain improvements
over our supervised model for both in-
domain (WSJ) and out-of-domain (ques-
tions and Wikipedia) data. Our learnt
lexicons when used with a discriminative
parser such as C&amp;C also significantly im-
prove its performance on unseen words.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927745762712">
An important open problem in natural language
parsing is to generalize supervised parsers, which
are trained on hand-labeled data, using unlabeled
data. The problem arises because further hand-
labeled data in the amounts necessary to signif-
icantly improve supervised parsers are very un-
likely to be made available. Generalization is also
necessary in order to achieve good performance on
parsing in textual domains other than the domain
of the available labeled data. For example, parsers
trained on Wall Street Journal (WSJ) data suffer a
fall in accuracy on other domains (Gildea, 2001).
In this paper, we use self-training to generalize
the lexicon of a Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000) parser. CCG is a
strongly lexicalized formalism, in which every
word is associated with a syntactic category (sim-
ilar to an elementary syntactic structure) indicat-
ing its subcategorization potential. Lexical en-
tries are fine-grained and expressive, and contain
a large amount of language-specific grammatical
information. For parsers based on strongly lexical-
ized formalisms, the problem of grammar general-
ization can be cast largely as a problem of lexical
extension.
The present paper focuses on learning lexi-
cal categories for words that are unseen or low-
frequency in labeled data, from unlabeled data.
Since lexical categories in a strongly lexicalized
formalism are complex, fine-grained (and far more
numerous than simple part-of-speech tags), they
are relatively sparse in labeled data. Despite per-
forming at state-of-the-art levels, a major source
of error made by CCG parsers is related to unseen
and low-frequency words (Hockenmaier, 2003;
Clark and Curran, 2007; Thomforde and Steed-
man, 2011). The unseen words for which we learn
categories are surprisingly commonplace words of
English; examples are conquered, apprehended,
subdivided, scoring, denotes, hunted, obsessed,
residing, migrated (Wikipedia). Correctly learn-
ing to parse the predicate-argument structures as-
sociated with such words (expressed as lexical cat-
egories in the case of CCG), is important for open-
domain parsing, not only for CCG but indeed for
any parser.
We show that a simple self-training method,
Viterbi-EM (Neal and Hinton, 1998) when used
to enhance the lexicon of a strongly-lexicalized
parser can be an effective strategy for self-training
and domain-adaptation. Our learnt lexicons im-
prove on the lexical category accuracy of two su-
pervised CCG parsers (Hockenmaier (2003) and
the Clark and Curran (2007) parser, C&amp;C) on
within-domain (WSJ) and out-of-domain test sets
(a question corpus and a Wikipedia corpus).
In most prior work, when EM was initialized
based on labeled data, its performance did not im-
prove over the supervised model (Merialdo, 1994;
</bodyText>
<page confidence="0.977894">
126
</page>
<note confidence="0.993315">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126–134,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.993627545454546">
Charniak, 1993). We found that in order for per-
formance to improve, unlabeled data should be
used only for parameters which are not well cov-
ered by the labeled data, while those that are well
covered should remain fixed.
In an additional contribution, we compare two
strategies for treating unseen words (a smoothing-
based, and a part-of-speech back-off method) and
find that a smoothing-based strategy for treat-
ing unseen words is more effective for semi-
supervised learning than part-of-speech back-off.
</bodyText>
<sectionHeader confidence="0.992793" genericHeader="introduction">
2 Combinatory Categorial Grammar
</sectionHeader>
<bodyText confidence="0.999292434782609">
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a strongly lexicalized grammar
formalism, in which the lexicon contains all
language-specific grammatical information. The
lexical entry of a word consists of a syntactic cat-
egory which expresses the subcategorization po-
tential of the word, and a semantic interpretation
which defines the compositional semantics (Lewis
and Steedman, 2013). A small number of combi-
natory rules are used to combine constituents, and
it is straightforward to map syntactic categories to
a logical form for semantic interpretation.
For statistical CCG parsers, the lexicon is learnt
from labeled data, and is subject to sparsity due
to the fine-grained nature of the categories. Fig-
ure 1 illustrates this with a simple CCG deriva-
tion. In this sentence, bake is used as a ditransi-
tive verb and is assigned the ditransitive category
S\NP/NP/NP . This category defines the verb syn-
tactically as mapping three NP arguments to a sen-
tence S , and semantically as a ternary relation be-
tween its three arguments, thus providing a com-
plete analysis of the sentence.
</bodyText>
<equation confidence="0.680480428571429">
[NNP John ] [V BD baked] [NNP Mary] [DT a ] [NN cake]
NP S\NP/NP/NP NP NP/N N
&gt; &gt;
S\NP/NP NP
S\NP
S
‘John baked Mary a cake’
</equation>
<figureCaption confidence="0.999056">
Figure 1: Example CCG derivation
</figureCaption>
<bodyText confidence="0.999959567567568">
For a CCG parser to obtain the correct deriva-
tion above, its lexicon must include the ditransitive
category S\NP/NP/NP for the verb bake. It is not
sufficient to have simply seen the verb in another
context (say a transitive context like “John baked a
cake”, which is a more common context). This is
in contrast to standard treebank parsers where the
verbal category is simply VBD (past tense verb)
and a ditransitive analysis of the sentence is not
ruled out as a result of the lexical category.
In addition to sparsity related to open-class
words like verbs as in the above example, there are
also missing categories in labeled data for closed-
class words like question words, due to the small
number of questions in the Penn Treebank. In gen-
eral, lexical sparsity for a statistical CCG parser
can be broken down into three types: (i) where a
word is unseen in training data but is present in
test data, (ii) where a word is seen in the train-
ing data but not with the category type required
in the test data (but the category type is seen with
other words) and (iii) where a word bears a cate-
gory type required in the test data but the category
type is completely unseen in the training data.
In this paper, we deal with the first two kinds.
The third kind is more prevalent when the size
of labeled data is comparatively small (although,
even in the case of the English WSJ CCG tree-
bank, there are several attested category types that
are entirely missing from the lexicon, Clark et al.,
2004). We make the assumption here that all cat-
egory types in the language have been seen in the
labeled data. In principle new category types may
be introduced independently without affecting our
semi-supervised process (for instance, manually,
or via a method that predicts new category types
from those seen in labeled data).
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999929684210526">
Previous attempts at harnessing unlabeled data to
improve supervised CCG models using methods
like self-training or co-training have been unsat-
isfactory (Steedman et al., 2003, 43-44). Steed-
man et al. (2003) experimented with self-training
a generative CCG parser, and co-training a genera-
tive parser with an HMM-based supertagger. Co-
training (but not self-training) improved the results
of the parser when the seed labeled data was small.
When the seed data was large (the full treebank),
i.e., the supervised baseline was high, co-training
and self-training both failed to improve the parser.
More recently, Honnibal et al. (2009) improved
the performance of the C&amp;C parser on a domain-
adaptation task (adaptation to Wikipedia text) us-
ing self-training. Instead of self-training the pars-
ing model, they re-train the supertagging model,
which in turn affects parsing accuracy. They
obtained an improvement of 1.09% (dependency
</bodyText>
<equation confidence="0.683417">
&gt;
&lt;
</equation>
<page confidence="0.98407">
127
</page>
<bodyText confidence="0.99698604">
score) on supertagger accuracy on Wikipedia (al-
though performance on WSJ text dropped) but did
not attempt to re-train the parsing model.
An orthogonal approach for extending a CCG
lexicon using unlabeled data is that of Thomforde
and Steedman (2011), in which a CCG category for
an unknown word is derived from partial parses
of sentences with just that one word unknown.
The method is capable of inducing unseen cate-
gories types (the third kind of sparsity mentioned
in §2.1), but due to algorithmic and efficiency is-
sues, it did not achieve the broad-coverage needed
for grammar generalisation of a high-end parser. It
is more relevant for low-resource languages which
do not have substantial labeled data and category
type discovery is important.
Some notable positive results for non-CCG
parsers are McClosky et al. (2006) who use a
parser-reranker combination. Koo et al. (2008)
and Suzuki et al. (2009) use unsupervised word-
clusters as features in a dependency parser to get
lexical dependencies. This has some notional sim-
ilarity to categories, since, like categories, clus-
ters are less fine-grained than words but more fine-
grained than POS-tags.
</bodyText>
<sectionHeader confidence="0.980689" genericHeader="method">
4 Supervised Parser
</sectionHeader>
<bodyText confidence="0.998213142857143">
The CCG parser used in this paper is a re-
implementation of the generative parser of Hock-
enmaier and Steedman (2002) and Hockenmaier
(2003)1, except for the treatment of unseen and
low-frequency words.
We use a model (the LexCat model in Hock-
enmaier (2003)) that conditions the generation of
constituents in the parse tree on the lexical cate-
gory of the head word of the constituent, but not on
the head word itself. While fully-lexicalized mod-
els that condition on words (and thus model word-
to-word dependencies) are more accurate than un-
lexicalized ones like the LexCat model, we use
an unlexicalized model2 for two reasons: first,
1These generative models are similar to the Collins’ head-
based models (Collins, 1997), where for every node, a head is
generated first, and then a sister conditioned on the head. De-
tails of the models are in Hockenmaier and Steedman (2002)
and Hockenmaier 2003:pg 166.
2A terminological clarification: unlexicalized here refers
to the model, in the sense that head-word information is
not used for rule-expansion. The formalism itself (CCG)
is referred to as strongly-lexicalized, as used in the title of
the paper. Formalisms like CCG and LTAG are consid-
ered strongly-lexicalized since linguistic knowledge (func-
tions mapping words to syntactic structures/semantic inter-
pretations) is included in the lexicon.
our lexicon smoothing procedure (described in the
next section) introduces new words and new cat-
egories for words into the lexicon. Lexical cate-
gories are added to the lexicon for seen and un-
seen words, but no new category types are intro-
duced. Since the LexCat model conditions rule ex-
pansions on lexical categories, but not on words, it
is still able to produce parses for sentences with
new words. In contrast, a fully lexicalized model
would need all components of the grammar to be
smoothed, a task that is far from trivial due to the
resulting explosion in grammar size (and one that
we leave for future work).
Second, although lexicalized models perform
better on in-domain WSJ data (the LexCat model
has an accuracy of 87.9% on Section 23, as op-
posed to 91.03% for the head-lexicalized model
in Hockenmaier (2003) and 91.9% for the C&amp;C
parser), our parser is more accurate on a question
corpus, with a lexical category accuracy of 82.3%,
as opposed to 71.6% and 78.6% for the C&amp;C and
Hockenmaier (2003) respectively.
</bodyText>
<subsectionHeader confidence="0.999783">
4.1 Handling rare and unseen words
</subsectionHeader>
<bodyText confidence="0.999978037037037">
Existing CCG parsers (Hockenmaier (2003) and
Clark and Curran (2007)) back-off rare and unseen
words to their POS tag. The POS-backoff strategy
is essentially a pipeline approach, where words
are first tagged with coarse tags (POS tags) and
finer tags (CCG categories) are later assigned, by
the parser (Hockenmaier, 2003) or the supertag-
ger (Clark and Curran, 2007). As POS-taggers
are much more accurate than parsers, this strat-
egy has given good performance in general for
CCG parsers, but it has the disadvantage that POS-
tagging errors are propagated. The parser can
never recover from a tagging error, a problem that
is serious for words in the Zipfian tail, where these
words might also be unseen for the POS tagger
and hence more likely to be tagged incorrectly.
This issue is in fact more generally relevant than
for CCG parsers alone—the dependence of parsers
on POS-taggers was cited as one of the problems
in domain-adaptation of parsers in the NAACL-
2012 shared task on parsing the web (Petrov and
McDonald, 2012). Lease and Charniak (2005)
obtained an improvement in the accuracy of the
Charniak (2000) parser on a biomedical domain
simply by training a new POS tagger model.
In the following section, we describe an alter-
native smoothing-based approach to handling un-
</bodyText>
<page confidence="0.990868">
128
</page>
<bodyText confidence="0.999976">
seen and rare words. This method is less sen-
sitive to POS tagging errors, as described below.
In this approach, in a pre-processing step prior
to parsing, categories are introduced into the lex-
icon for unseen and rare words from the data to
be parsed. Some probability mass is taken from
seen words/categories and given to unseen word
and category pairs. Thus, at parse time, no word is
unseen for the parser.
</bodyText>
<subsectionHeader confidence="0.955613">
4.1.1 Smoothing
</subsectionHeader>
<bodyText confidence="0.999989297297297">
In our approach, we introduce lexical entries for
words from the unlabeled corpus that are unseen
in the labeled data, and also add categories to ex-
isting entries for rarely seen words. The most gen-
eral case of this would be to assign all known cat-
egories to a word. However, doing this reduces
the lexical category accuracy.3 A second option,
chosen here, is to limit the number of categories
assigned to the word by using some information
about the word (for instance, its part-of-speech).
Based on the part-of-speech of an unseen word in
the unlabeled or test corpus, we add an entry to the
lexicon of the word with the top n categories that
have been seen with that part-of-speech in the la-
beled data. Each new entry of (w, cat), where w
is a word and cat is a CCG category, is associated
with a count c(w, cat), obtained as described be-
low. Once all (w, cat) entries are added to the lex-
icon along with their counts, a probability model
P(w|cat) is calculated over the entire lexicon.
Our smoothing method is based on a method
used in Deoskar (2008) for smoothing a PCFG
lexicon. Eq. 1 and 2 apply it to CCG entries for
unseen and rare words. In the first step, an out-
of-the-box POS tagger is used to tag the unlabeled
or test corpus (we use the C&amp;C tagger). Counts
of words and POS-tags ccorpus(w, T) are obtained
from the tagged corpus. For the CCG lexicon, we
ultimately need a count for a word w and a CCG
category cat. To get this count, we split the count
of a word and POS-tag amongst all categories seen
with that tag in the supervised data in the same
ratio as the ratio of the categories in the super-
vised data. In Eq. 1, this ratio is ctb(catT)/ctb(T)
where ctb(catT) is the treebank count of a cate-
gory catT seen with a POS-tag T, and ctb(T) is the
marginal count of the tag T in the treebank. This
</bodyText>
<footnote confidence="0.927467">
3For instance, we find that assigning all categories to un-
seen verbs gives a lexical category accuracy of 52.25 %, as
opposed to an accuracy of 65.4% by using top 15 categories,
which gave us the best results, as reported later in Table 3.
</footnote>
<bodyText confidence="0.998805727272727">
ratio makes a more frequent category type more
likely than a rarer one for an unseen word. For ex-
ample, for unseen verbs, it would make the transi-
tive category more likely than a ditransitive one
(since transitives are more frequent than ditran-
sitives). There is an underlying assumption here
that relative frequencies of categories and POS-
tags in the labeled data are maintained in the un-
labeled data, which in fact can be thought of as
a prior while estimating from unlabeled data (De-
oskar et al., 2012).
</bodyText>
<equation confidence="0.987722">
ctb(catT)
ccorpus(w, cat) = ctb(T)
</equation>
<bodyText confidence="0.99845">
Additionally, for seen but low-frequency words,
we make use of the existing entry in the lexicon.
Thus in a second step, we interpolate the count
ccorpus(w, cat) of a word and category with the
supervised count of the same ctb(w, cat) (if it ex-
ists) to give the final smoothed count of a word and
category csmooth(w, cat) (Eq. 2).
</bodyText>
<equation confidence="0.9734365">
csmooth(w, cat) = λ · ctb(w, cat) +
(1 − λ) · ccorpus(w, cat)
</equation>
<bodyText confidence="0.953955875">
(2)
When this smoothed lexicon is used with a
parser, POS-backoff is not necessary since all
needed words are now in the lexicon. Lexical en-
tries for words in the parse are determined not by
the POS-tag from a tagger, but directly by the pars-
ing model, thus making the parse less susceptible
to tagging errors.
</bodyText>
<sectionHeader confidence="0.998172" genericHeader="method">
5 Semi-supervised Learning
</sectionHeader>
<bodyText confidence="0.999939368421053">
We use Viterbi-EM (Neal and Hinton, 1998) as
the self-training method. Viterbi-EM is an alter-
native to EM where instead of using the model
parameters to find a true posterior from unlabeled
data, a posterior based on the single maximum-
probability (Viterbi) parse is used. Viterbi-EM
has been used in various NLP tasks before and
often performs better than classic EM (Cohen
and Smith, 2010; Goldwater and Johnson, 2005;
Spitkovsky et al., 2010). In practice, a given pars-
ing model is used to obtain Viterbi parses of un-
labeled sentences. The Viterbi parses are then
treated as training data for a new model. This pro-
cess is iterated until convergence.
Since we are interested in learning the lexi-
con, we only consider lexical counts from Viterbi
parses of the unlabeled sentences. Other parame-
ters of the model are held at their supervised val-
ues. We conducted some experiments where we
</bodyText>
<equation confidence="0.985889">
· ccorpus(w, T) (1)
</equation>
<page confidence="0.986107">
129
</page>
<bodyText confidence="0.999916340425532">
self-trained all components of the parsing model,
which is the usual case of self-training. We ob-
tained negative results similar to Steedman et al.
(2003), where self-training reduced the perfor-
mance of the parsing model. We do not report
them here. Thus, using unlabeled data only to es-
timate parameters that are badly estimated from
labeled data (lexical entries in CCG, due to lexi-
cal sparsity) results in improvements, in contrast
to prior work with semi-supervised EM.
As is common in semi-supervised settings, we
treated the count of each lexical event as the
weighted count of that event in the labeled data
(treebank)4 and the count from the Viterbi-parses
of unlabeled data. Here we follow Bacchiani et al.
(2006) and McClosky et al. (2006) who show that
count merging is more effective than model inter-
polation.
We placed an additional constraint on the con-
tribution that the unlabeled data makes to the semi-
supervised model—we only use counts (from un-
labeled data) of lexical events that are rarely
seen/unseen in the labeled data. Our reasoning
was that many lexical entries are estimated accu-
rately from the treebank (for example, those re-
lated to function words and other high-frequency
words) and estimation from unlabeled data might
hurt them. We thus had a cut-off frequency (of
words in labeled data) above which we did not
allow the unlabeled counts to affect the semi-
supervised model. In practise, our experiments
turned out to be fairly insensitive to the value of
this parameter, on evaluations over rare or un-
seen verbs. However, overall accuracy would drop
slightly if this cut-off was increased. We experi-
mented with cut-offs of 5, 10 and 15, and found
that the most conservative value (of 5) gave the
best results on in-domain WSJ experiments, and a
higher value of 10 gave the best results for out-of-
domain experiments.
We also conducted some limited experiments
with classical semi-supervised EM, with similar
settings of weighting labeled counts, and using un-
labeled counts only for rare/unseen events. Since
it is a much more computationally expensive pro-
cedure, and most of the results did not come close
to the results of Viterbi-EM, we did not pursue it.
</bodyText>
<footnote confidence="0.9002735">
4The labeled count is weighted in order to scale up the la-
beled data which is usually smaller in size than the unlabeled
data, to avoid swamping the labeled counts with much larger
unlabeled counts.
</footnote>
<subsectionHeader confidence="0.948612">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999977">
Labeled: Sec. 02-21 of CCGbank (Hockenmaier
and Steedman, 2007). In one experiment, we used
Sec. 02-21 minus 1575 sentences that were held
out to simulate test data containing unseen verbs—
see §6.2 for details.
Unlabeled: For in-domain experiments, we used
sentences from the unlabeled WSJ portion of the
ACL/DCI corpus (LDC93T1, 1993), and the WSJ
portion of the ANC corpus (Reppen et al., 2005),
limited to sentences containing 20 words or less,
creating datasets of approximately 10, 20 and 40
million words each. Additionally, we have a
dataset of 140 million words – 40M WSJ words
plus an additional 100M from the New York
Times.
For domain-adaptation experiments, we use
two different datasets. The first one consists
of question-sentences – 1328 unlabeled ques-
tions, obtained by removing the manual annota-
tion of the question corpus from Rimell and Clark
(2008). The second out-of-domain dataset con-
sists of Wikipedia data, approximately 40 million
words in size, with sentence length &lt; 20 words.
</bodyText>
<subsectionHeader confidence="0.994254">
5.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999990230769231">
We ran our semi-supervised method using our
parser with a smoothed lexicon (from §4.1.1) as
the initial model, on unlabeled data of different
sizes/domains. For comparison, we also ran ex-
periments using a POS-backed off parser (the orig-
inal Hockenmaier and Steedman (2002) LexCat
model) as the initial model. Viterbi-EM converged
at 4-5 iterations. We then parsed various test sets
using the semi-supervised lexicons thus obtained.
In all experiments, the labeled data was scaled to
match the size of the unlabeled data. Thus, the
scaling factor of labeled data was 10 for unlabeled
data of 10M words, 20 for 20M words, etc.
</bodyText>
<subsectionHeader confidence="0.980077">
5.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999935625">
We focused our evaluations on unseen and low-
frequency verbs, since verbs are the most impor-
tant open-class lexical entries and the most am-
biguous to learn from unlabeled data (approx. 600
categories, versus 150 for nouns). We report lexi-
cal category accuracy in parses produced using our
semi-supervised lexicon, since it is a direct mea-
sure of the effect of the lexicon.5 We discuss four
</bodyText>
<footnote confidence="0.9403085">
5Dependency recovery accuracy is also used to evaluate
performance of CCG parsers and is correlated with lexical
</footnote>
<page confidence="0.985335">
130
</page>
<table confidence="0.998127777777778">
All words All Verbs Unseen
Verbs
SUP 87.76 78.10 52.54
SEMISUP 88.14 78.46 **57.28
SUPbkoff 87.91 76.08 54.14
SEMISUPbkoff 87.79 75.68 54.60
Freq. Bin 1-5 6-10 11-20
SUP 64.13 75.19 77.6
SEMISUP 66.72 76.21 79.8
</table>
<tableCaption confidence="0.910411666666667">
Table 2: Seen but rare verbs, TEST-4SEC
Table 1: Lexical category accuracy on TEST-4SEC
**: p &lt; 0.004, McNemar test
</tableCaption>
<bodyText confidence="0.837853">
experiments below. The first two are on in-domain
(WSJ) data. The last two are on out-of-domain
data – a question corpus and a Wikipedia corpus.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.998239">
6.1 In-domain: WSJ unseen verbs
</subsectionHeader>
<bodyText confidence="0.9997824375">
Our first testset consists of a concatenation of 4
sections of CCGbank (01, 22, 24, 23), a total of
7417 sentences, to form a testset called TEST-
4SEC. We use all these sections in order to get
a reasonable token count of unseen verbs, which
was not possible with Sec. 23 alone.
Table 1 shows the performance of the smoothed
supervised model (SUP) and the semi-supervised
model (SEMISUP) on this testset. There is a sig-
nificant improvement in performance on unseen
verbs, showing that the semi-supervised model
learns good entries for unseen verbs over and
above the smoothed entry in the supervised lexi-
con. This results in an improvement in the over-
all lexical category accuracy of the parser on all
words, and all verbs.
We also performed semi-supervised training us-
ing a supervised model that treated unseen words
with a POS-backoff strategy SUPbkoff. We used
the same settings of cut-off and the same scal-
ing of labeled counts as before. The supervised
backed-off model performs somewhat better than
the supervised smoothed model. However, it did
not improve as much as the smoothed one from
unlabeled data. Additionally, the overall accuracy
of SEMISUPbkoff fell below the supervised level,
in contrast to the smoothed model, where overall
numbers improved. This could indicate that the
accuracy of a POS tagger on unseen words, es-
pecially verbs, may be an important bottleneck in
semi-supervised learning.
Low-frequency verbs We also obtain improve-
ments on verbs that are seen but with a low fre-
quency in the labeled data (Table 2). We divided
category accuracy, but a dependency evaluation is more rele-
vant when comparing performance with parsers in other for-
malisms and does not have much utility here.
verbs occurring in TEST-4SEC into different bins
according to their occurrence frequency in the la-
beled data (bins of frequency 1-5, 6-10 and 11-20).
Semi-supervised training improves over the super-
vised baseline for all bins of low-frequency verbs.
Note that our cut-off frequency for using unlabeled
data is 5, but there are improvements in the 6-10
and 11-20 bins as well, suggesting that learning
better categories for rare words (below the cut-off)
impacts the accuracy of words above the cut-off as
well, by affecting the rest of the parse positively.
</bodyText>
<subsectionHeader confidence="0.995974">
6.2 In-domain : heldout unseen verbs
</subsectionHeader>
<bodyText confidence="0.99368971875">
The previous section showed significant improve-
ment in learning categories for verbs that are un-
seen in the training sections of CCGbank. How-
ever, these verbs are in the Zipfian tail, and for this
reason have fairly low occurrence frequencies in
the unlabeled corpus. In order to estimate whether
our method will give further improvements in the
lexical categories for these verbs, we would need
unlabeled data of a much larger size. We there-
fore designed an experimental scenario in which
we would be able to get high counts of unseen
verbs from a similar size of unlabeled data. We
first made a list of N verbs from the treebank and
then extracted all sentences containing them (ei-
ther as verbs or otherwise) from CCGbank training
sections. These sentences form a testset of 1575
sentences, called TEST-HOV (for held out verbs).
The verbs in the list were chosen based on occur-
rence frequency f in the treebank, choosing all
verbs that occurred with a frequency of f = 11.
This number gave us a large enough set and a
good type/token ratio to reliably evaluate and ana-
lyze our semi-supervised models—112 verb types,
with 1115 token occurrences 6. Since these verbs
are actually mid-frequency verbs in the supervised
data, they have a correspondingly large occurrence
frequency in the unlabeled data, occurring much
more often than true unseen verbs. Thus, the un-
labeled data size is effectively magnified—as far
as these verbs are concerned, the unlabeled data is
approximately 11 times larger than it actually is.
Table 3 shows lexical category accuracy on
</bodyText>
<footnote confidence="0.7409655">
6Selecting a different but close value of f such as f = 10
or f = 12 would have also served this purpose.
</footnote>
<page confidence="0.986433">
131
</page>
<table confidence="0.998225666666667">
All Words All Verbs Unseen
Verbs
SUP 87.26 74.55 65.49
SEMISUP 87.78 75.30 *** 70.43
SUPbkoff 87.58 73.06 67.25
SEMISUPbkoff 87.52 72.89 68.05
</table>
<tableCaption confidence="0.7584765">
Table 3: Lexical category accuracy in TEST-HOV.
***p&lt;0.0001, McNemar test
</tableCaption>
<figure confidence="0.9391975">
0 10 20 40 140
Size of Unlabelled Data (in millions of words)
</figure>
<figureCaption confidence="0.98767">
Figure 2: Increasing accuracy on unseen verbs
with increasing amounts of unlabeled data.
</figureCaption>
<bodyText confidence="0.999440238095238">
this testset. The baseline accuracy of the parser
on these verbs is much higher than that on the
truly unseen verbs.7 The semi-supervised model
(SEMISUP) improves over the supervised model
SUP very significantly on these unseen verbs. We
also see an overall improvement on all verbs (seen
and unseen) in the test data, and in the over-
all lexical category accuracy as well. Again, the
backed-off model does not improve as much as
the smoothed model, and moreover, overall per-
formance falls below the supervised level.
Figure 2 shows the effect of different sizes of
unlabeled data on accuracy of unseen verbs for
the two testsets TEST-HOV and TEST-4SEC . Im-
provements are monotonic with increasing unla-
beled data sizes, up to 40M words. The additional
100M words of NYT also improve the models but
to a lesser degree, possibly due to the difference in
domain. The graphs indicate that the method will
lead to more improvements as more unlabeled data
(especially WSJ data) is added.
</bodyText>
<tableCaption confidence="0.5231524">
7This could be because verbs in the Zipfian tail have more
idiosyncratic subcategorization patterns than mid-frequency
verbs, and thus are harder for a parser. Another reason is that
they may have been seen as nouns or other parts of speech,
leading to greater ambiguity in their case.
</tableCaption>
<table confidence="0.9991306">
QUESTIONS WIKIPEDIA
All wh All Unseen
words words words words
SUP 82.36 61.77 84.31 79.5
SEMISUP *83.21 63.22 *85.6 80.25
</table>
<tableCaption confidence="0.9084305">
Table 4: Out-of-domain: Questions and
Wikipedia, *p&lt;0.05, McNemar test
</tableCaption>
<subsectionHeader confidence="0.907225">
6.2.1 Out-of-Domain
</subsectionHeader>
<bodyText confidence="0.999852232558139">
Questions The question corpus is not strictly a
different domain (since questions form a differ-
ent kind of construction rather than a different do-
main), but it is an interesting case of adaptation
for several reasons: WSJ parsers perform poorly
on questions due to the small number of questions
in the Penn Treebank/CCGbank. Secondly, unsu-
pervised adaptation to questions has not been at-
tempted before for CCG (Rimell and Clark (2008)
did supervised adaptation of their supertagger).
The supervised model SUP already performs
at state-of-the-art on this corpus, on both overall
scores and on wh(question)-words alone. C&amp;C
and Hockenmaier (2003) get 71.6 and 78.6% over-
all accuracies respectively, and only 33.6 and 50.7
on wh-words alone. To our original unlabeled
WSJ data (40M words), we add 1328 unlabeled
question-sentences from Rimell and Clark, 2008,
scaled by ten, so that each is counted ten times. We
then evaluated on a testset containing questions
(500 question sentences, from Rimell and Clark
(2008)). The overall lexical category accuracy on
this testset improves significantly as a result of the
semi-supervised learning (Table 4). The accuracy
on the question words alone (who, what, where,
when, which, how, whose, whom) also improves
numerically, but by a small amount (the number
of tokens that improve are only 7). This could be
an effect of the small size of the testset (500 sen-
tences, i.e. 500 wh-words).
Wikipedia We obtain statistically significant im-
provements in overall scores over a testset consist-
ing of Wikipedia sentences hand-annotated with
CCG categories (from Honnibal et al. (2009)) (Ta-
ble 4). We also obtained improvements in lexical
category accuracy on unseen words, and on un-
seen verbs alone (not shown), but could not prove
significance. This testset contains only 200 sen-
tences, and counts for unseen words are too small
for significance tests, although there are numeric
improvements. However, the overall improvement
is statistically significantly, showing that adapting
the lexicon alone is effective for a new domain.
</bodyText>
<figure confidence="0.91834475">
Lexical Category Accuracy for Unseen Verbs
55 60 65 70
Test:HOV
Test:4Sec
</figure>
<page confidence="0.942998">
132
</page>
<subsectionHeader confidence="0.8816925">
6.3 Using semi-supervised lexicons with the
C&amp;C parser
</subsectionHeader>
<bodyText confidence="0.999978655172414">
To show that the learnt lexical entries may be use-
ful to parsers other than our own, we incorpo-
rate our semi-supervised lexical entries into the
C&amp;C parser to see if it benefits performance. We
do this in a naive manner, as a proof of concept,
making no attempt to optimize the performance
of the C&amp;C parser (since we do not have access
to its internal workings). We take all entries of
unseen words from our best semi-supervised lex-
icon (word, category and count) and add them to
the dictionary of the C&amp;C supertagger (tagdict).
The C&amp;C is a discriminative, lexicalized model
that is more accurate than an unlexicalized model.
Even so, the lexical entries that we learn improve
the C&amp;C parsers performance over and above its
back-off strategy for unseen words. Table 5 shows
the results on WSJ data TEST-4SEC and TEST-
HOV. There were numeric improvements on the
TEST-4SEC test set as shown in Table 58. We ob-
tain significance on the TEST-HOV testset which
has a larger number of tokens of unseen verbs and
entries that were learnt from effectively larger un-
labeled data. We tested two cases: when these
verbs were seen for the POS tagger used to tag
the test data, and when they were unseen for the
POS tagger, and found statistically significant im-
provement for the case when the verbs were un-
seen for the POS tagger9, indicating sensitivity to
POS-tagger errors.
</bodyText>
<subsectionHeader confidence="0.987463">
6.4 Entropy and KL-divergence
</subsectionHeader>
<bodyText confidence="0.999982833333333">
We also evaluated the quality of the semi-
supervised lexical entries by measuring the over-
all entropy and the average Kullback-Leibler (KL)
divergence of the learnt entries of unseen verbs
from entries in the gold testset. The gold entry
for each verb from the TEST-HOV testset was ob-
tained from the heldout gold treebank trees. Su-
pervised (smoothed) and semi-supervised entries
were obtained from the respective lexicons. These
metrics use the conditional probability of a cate-
gory given a word, which is not a factor in the
generative model (which considers probabilities of
</bodyText>
<footnote confidence="0.8225205">
8There were also improvements on the question and
Wikipedia testsets (not shown) (8 and 6 tokens each) but the
size of these testsets is too small for significance.
9Note that for this testset TEST-HOV, the numbers are the
supertagger’s accuracy, and not the parser’s. We were only
able to retrain the supertagger on training data with TEST-
HOV sentences heldout, but could not retrain the parser, de-
spite consultation with the authors.
</footnote>
<table confidence="0.992608833333333">
TEST-4SEC TEST-HOV
(590) POS-seen POS-unseen
(1134) (1134)
C&amp;C 62.03 (366) 76.71 (870) 72.39 (821)
C&amp;C 63.89 (377) 77.34 (877) *73.98 (839)
(enhanced)
</table>
<tableCaption confidence="0.82548">
Table 5: TEST-4SEC: Lexical category accuracy of
C&amp;C parser on unseen verbs. Numbers in brackets
are the number of tokens.*p&lt;0.05, McNemar test
</tableCaption>
<bodyText confidence="0.9981196">
words given categories), but provide a good mea-
sure of how close the learnt lexicons are to the gold
lexicon. We find that the average KL divergence
reduces from 2.17 for the baseline supervised en-
tries to 1.40 for the semi-supervised entries. The
overall entropy for unseen verb distributions also
goes down from 2.23 (supervised) to 1.37 (semi-
supervised), showing that semi-supervised distri-
butions are more peaked, and bringing them closer
to the true entropy of the gold distribution (0.93).
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999984863636364">
We have shown that it is possible to learn CCG lex-
ical entries for unseen and low-frequency words
from unlabeled data. When restricted to learning
only lexical entries, Viterbi-EM improved the per-
formance of the supervised parser (both in-domain
and out-of-domain). Updating all parameters of
the parsing model resulted in a decrease in the ac-
curacy of the parser. We showed that the entries
we learnt with an unlexicalized model were accu-
rate enough to also be useful to a highly-accurate
lexicalized parser. It is likely that a lexicalized
parser will provide even better lexical entries. The
lexical entries continued to improve with increas-
ing size of unlabeled data. For the out-of-domain
testsets, we obtained statistically significant over-
all improvements, but we were hampered by the
small sizes of the testsets in evaluating unseen/wh
words.
In future work, we would like to add unseen but
predicted category types to the initial lexicon using
an independent method, and then apply the same
semi-supervised learning to words of these types.
</bodyText>
<sectionHeader confidence="0.997514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9979505">
We thank Mike Lewis, Shay Cohen and the three
anonymous EACL reviewers for helpful com-
ments. This work was supported by the ERC Ad-
vanced Fellowship 249520 GRAMPLUS.
</bodyText>
<page confidence="0.998791">
133
</page>
<sectionHeader confidence="0.995885" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935474226805">
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. MAP adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41–68.
Eugene Charniak. 1993. Statistical Language Learning. MIT
Press.
Stephen Clark and James R. Curran. 2007. Wide-Coverage
Efficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493–552.
Stephen Clark, Mark Steedman, and James Curran. 2004.
Object-extraction and question-parsing using CCG. In
Proceedings of EMNLP 2004.
Shay Cohen and Noah Smith. 2010. Viterbi Training for
PCFGs: Hardness Results and Competitiveness of Uni-
form Initialization. In Proceedings of ACL 2010.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of the 35th ACL.
Tejaswini Deoskar. 2008. Re-estimation of Lexical Param-
eters for Treebank PCFGs. In Proceedings of COLING
2008.
Tejaswini Deoskar, Markos Mylonakis, and Khalil Sima’an.
2012. Learning Structural Dependencies of Words in the
Zipfian Tail. Journal of Logic and Computation.
Daniel Gildea. 2001. Corpus Variation and Parser Perfor-
mance. In Proceedings of EMNLP 2001.
Sharon Goldwater and Mark Johnson. 2005. Bias in learning
syllable structure. In Proceedings of CoNLL05.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.
Julia Hockenmaier and Mark Steedman. 2002. Generative
Models for Statistical Parsing with Combinatory Catego-
rial Grammar. In ACL40.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A
Corpus of CCG Derivations and Dependency Structures
Extracted from the Penn Treebank. Computational Lin-
guistics, 33:355–396.
Matthew Honnibal, Joel Nothman, and James R. Curran.
2009. Evaluating a Statistial CCG Parser on Wikipedia.
In Proceedings of the 2009 Workshop on the People’s Web
Meets NLP, ACL-IJCNLP.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Sim-
ple Semi-supervised Dependency Parsing. In Proceedings
of ACL-08: HLT, pages 595–603. Association for Com-
putational Linguistics, Columbus, Ohio.
LDC93T1. 1993. LDC93T1. Linguistic Data Consortium,
Philadelphia.
Matthew Lease and Eugene Charniak. 2005. Parsing Biomed-
ical Literature. In R. Dale, K.-F. Wong, J. Su, and
O. Kwong, eds., Proceedings of the 2nd International
Joint Conference on Natural Language Processing (IJC-
NLP’05), vol. 3651 of Lecture Notes in Computer Science,
pages 58 – 69. Springer-Verlag, Jeju Island, Korea.
Mike Lewis and Mark Steedman. 2013. Combined Distribu-
tional and Logical Semantics. Transactions of the Associ-
ation for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective Self-Training for Parsing. In Proceedings
of HLT-NAACL 2006.
Bernard Merialdo. 1994. Tagging English Text with a Prob-
abilistic Model. Computational Linguistics, 20(2):155–
171.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view of
the EM algorithm that justifies incremental, sparse, and
other variants. In Learning and Graphical Models, pages
355 – 368. Kluwer Academic Publishers.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In First Work-
shop on Syntactic Analysis of Non-Canonical Language
(SANCL) Workshop at NAACL 2012.
Randi Reppen, Nancy Ide, and Keith Suderman. 2005.
LDC2005T35, American National Corpus (ANC) Second
Release. Linguistic Data Consortium, Philadelphia.
Laura Rimell and Stephen Clark. 2008. Adapting a
Lexicalized-Grammar Parser to Contrasting Domains. In
Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-08).
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and
Christopher D. Manning. 2010. Viterbi Training Improves
Unsupervised Dependency Parsing. In Proceedings of
CoNLL-2010.
Mark Steedman. 2000. The Syntactic Process. MIT
Press/Bradford Books.
Mark Steedman, Steven Baker, Jeremiah Crim, Stephen
Clark, Julia Hockenmaier, Rebecca Hwa, Miles Osbornn,
Paul Ruhlen, and Anoop Sarkar. 2003. Semi-Supervised
Training for Statistical Parsing. Tech. rep., CLSP WS-02.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An Empirical Study of Semi-supervised
Structured Conditional Models for Dependency Parsing.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 551–
560. Association for Computational Linguistics, Singa-
pore.
Emily Thomforde and Mark Steedman. 2011. Semi-
supervised CCG Lexicon Extension. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, Edinburgh UK.
</reference>
<page confidence="0.998631">
134
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.708342">
<title confidence="0.99997">Generalizing a Strongly Lexicalized Parser using Unlabeled Data</title>
<author confidence="0.997603">Christos Alexandra Mark</author>
<address confidence="0.810948">of Informatics, University of Edinburgh, Edinburgh, EH8 of Illinois, Urbana-Champaign, Urbana, IL</address>
<email confidence="0.997148">christod@illinois.edu</email>
<abstract confidence="0.9991673">Statistical parsers trained on labeled data suffer from sparsity, both grammatical and lexical. For parsers based on strongly lexicalized grammar formalisms (such as which has complex lexical categories but simple combinatory rules), the problem of sparsity can be isolated to the lexicon. In this paper, we show that semi-supervised Viterbi-EM can be used extend the lexicon of a generative parser. By learning complex lexical entries for low-frequency and unseen words from unlabeled data, we obtain improvements over our supervised model for both indomain (WSJ) and out-of-domain (questions and Wikipedia) data. Our learnt lexicons when used with a discriminative parser such as C&amp;C also significantly improve its performance on unseen words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="18903" citStr="Bacchiani et al. (2006)" startWordPosition="3121" endWordPosition="3124">ined negative results similar to Steedman et al. (2003), where self-training reduced the performance of the parsing model. We do not report them here. Thus, using unlabeled data only to estimate parameters that are badly estimated from labeled data (lexical entries in CCG, due to lexical sparsity) results in improvements, in contrast to prior work with semi-supervised EM. As is common in semi-supervised settings, we treated the count of each lexical event as the weighted count of that event in the labeled data (treebank)4 and the count from the Viterbi-parses of unlabeled data. Here we follow Bacchiani et al. (2006) and McClosky et al. (2006) who show that count merging is more effective than model interpolation. We placed an additional constraint on the contribution that the unlabeled data makes to the semisupervised model—we only use counts (from unlabeled data) of lexical events that are rarely seen/unseen in the labeled data. Our reasoning was that many lexical entries are estimated accurately from the treebank (for example, those related to function words and other high-frequency words) and estimation from unlabeled data might hurt them. We thus had a cut-off frequency (of words in labeled data) abo</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer Speech and Language, 20(1):41–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4109" citStr="Charniak, 1993" startWordPosition="603" endWordPosition="604">nt lexicons improve on the lexical category accuracy of two supervised CCG parsers (Hockenmaier (2003) and the Clark and Curran (2007) parser, C&amp;C) on within-domain (WSJ) and out-of-domain test sets (a question corpus and a Wikipedia corpus). In most prior work, when EM was initialized based on labeled data, its performance did not improve over the supervised model (Merialdo, 1994; 126 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126–134, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Charniak, 1993). We found that in order for performance to improve, unlabeled data should be used only for parameters which are not well covered by the labeled data, while those that are well covered should remain fixed. In an additional contribution, we compare two strategies for treating unseen words (a smoothingbased, and a part-of-speech back-off method) and find that a smoothing-based strategy for treating unseen words is more effective for semisupervised learning than part-of-speech back-off. 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a strongly lexicalize</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Eugene Charniak. 1993. Statistical Language Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="2803" citStr="Clark and Curran, 2007" startWordPosition="407" endWordPosition="410">y lexicalized formalisms, the problem of grammar generalization can be cast largely as a problem of lexical extension. The present paper focuses on learning lexical categories for words that are unseen or lowfrequency in labeled data, from unlabeled data. Since lexical categories in a strongly lexicalized formalism are complex, fine-grained (and far more numerous than simple part-of-speech tags), they are relatively sparse in labeled data. Despite performing at state-of-the-art levels, a major source of error made by CCG parsers is related to unseen and low-frequency words (Hockenmaier, 2003; Clark and Curran, 2007; Thomforde and Steedman, 2011). The unseen words for which we learn categories are surprisingly commonplace words of English; examples are conquered, apprehended, subdivided, scoring, denotes, hunted, obsessed, residing, migrated (Wikipedia). Correctly learning to parse the predicate-argument structures associated with such words (expressed as lexical categories in the case of CCG), is important for opendomain parsing, not only for CCG but indeed for any parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexica</context>
<context position="12299" citStr="Clark and Curran (2007)" startWordPosition="1958" endWordPosition="1961">hat is far from trivial due to the resulting explosion in grammar size (and one that we leave for future work). Second, although lexicalized models perform better on in-domain WSJ data (the LexCat model has an accuracy of 87.9% on Section 23, as opposed to 91.03% for the head-lexicalized model in Hockenmaier (2003) and 91.9% for the C&amp;C parser), our parser is more accurate on a question corpus, with a lexical category accuracy of 82.3%, as opposed to 71.6% and 78.6% for the C&amp;C and Hockenmaier (2003) respectively. 4.1 Handling rare and unseen words Existing CCG parsers (Hockenmaier (2003) and Clark and Curran (2007)) back-off rare and unseen words to their POS tag. The POS-backoff strategy is essentially a pipeline approach, where words are first tagged with coarse tags (POS tags) and finer tags (CCG categories) are later assigned, by the parser (Hockenmaier, 2003) or the supertagger (Clark and Curran, 2007). As POS-taggers are much more accurate than parsers, this strategy has given good performance in general for CCG parsers, but it has the disadvantage that POStagging errors are propagated. The parser can never recover from a tagging error, a problem that is serious for words in the Zipfian tail, wher</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James Curran</author>
</authors>
<title>Object-extraction and question-parsing using CCG.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="7378" citStr="Clark et al., 2004" startWordPosition="1163" endWordPosition="1166">but is present in test data, (ii) where a word is seen in the training data but not with the category type required in the test data (but the category type is seen with other words) and (iii) where a word bears a category type required in the test data but the category type is completely unseen in the training data. In this paper, we deal with the first two kinds. The third kind is more prevalent when the size of labeled data is comparatively small (although, even in the case of the English WSJ CCG treebank, there are several attested category types that are entirely missing from the lexicon, Clark et al., 2004). We make the assumption here that all category types in the language have been seen in the labeled data. In principle new category types may be introduced independently without affecting our semi-supervised process (for instance, manually, or via a method that predicts new category types from those seen in labeled data). 3 Related Work Previous attempts at harnessing unlabeled data to improve supervised CCG models using methods like self-training or co-training have been unsatisfactory (Steedman et al., 2003, 43-44). Steedman et al. (2003) experimented with self-training a generative CCG pars</context>
</contexts>
<marker>Clark, Steedman, Curran, 2004</marker>
<rawString>Stephen Clark, Mark Steedman, and James Curran. 2004. Object-extraction and question-parsing using CCG. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay Cohen</author>
<author>Noah Smith</author>
</authors>
<title>Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="17659" citStr="Cohen and Smith, 2010" startWordPosition="2912" endWordPosition="2915">are now in the lexicon. Lexical entries for words in the parse are determined not by the POS-tag from a tagger, but directly by the parsing model, thus making the parse less susceptible to tagging errors. 5 Semi-supervised Learning We use Viterbi-EM (Neal and Hinton, 1998) as the self-training method. Viterbi-EM is an alternative to EM where instead of using the model parameters to find a true posterior from unlabeled data, a posterior based on the single maximumprobability (Viterbi) parse is used. Viterbi-EM has been used in various NLP tasks before and often performs better than classic EM (Cohen and Smith, 2010; Goldwater and Johnson, 2005; Spitkovsky et al., 2010). In practice, a given parsing model is used to obtain Viterbi parses of unlabeled sentences. The Viterbi parses are then treated as training data for a new model. This process is iterated until convergence. Since we are interested in learning the lexicon, we only consider lexical counts from Viterbi parses of the unlabeled sentences. Other parameters of the model are held at their supervised values. We conducted some experiments where we · ccorpus(w, T) (1) 129 self-trained all components of the parsing model, which is the usual case of s</context>
</contexts>
<marker>Cohen, Smith, 2010</marker>
<rawString>Shay Cohen and Noah Smith. 2010. Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th ACL.</booktitle>
<contexts>
<context position="10545" citStr="Collins, 1997" startWordPosition="1672" endWordPosition="1673">dman (2002) and Hockenmaier (2003)1, except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexical category of the head word of the constituent, but not on the head word itself. While fully-lexicalized models that condition on words (and thus model wordto-word dependencies) are more accurate than unlexicalized ones like the LexCat model, we use an unlexicalized model2 for two reasons: first, 1These generative models are similar to the Collins’ headbased models (Collins, 1997), where for every node, a head is generated first, and then a sister conditioned on the head. Details of the models are in Hockenmaier and Steedman (2002) and Hockenmaier 2003:pg 166. 2A terminological clarification: unlexicalized here refers to the model, in the sense that head-word information is not used for rule-expansion. The formalism itself (CCG) is referred to as strongly-lexicalized, as used in the title of the paper. Formalisms like CCG and LTAG are considered strongly-lexicalized since linguistic knowledge (functions mapping words to syntactic structures/semantic interpretations) is</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tejaswini Deoskar</author>
</authors>
<title>Re-estimation of Lexical Parameters for Treebank PCFGs.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="14995" citStr="Deoskar (2008)" startWordPosition="2429" endWordPosition="2430"> about the word (for instance, its part-of-speech). Based on the part-of-speech of an unseen word in the unlabeled or test corpus, we add an entry to the lexicon of the word with the top n categories that have been seen with that part-of-speech in the labeled data. Each new entry of (w, cat), where w is a word and cat is a CCG category, is associated with a count c(w, cat), obtained as described below. Once all (w, cat) entries are added to the lexicon along with their counts, a probability model P(w|cat) is calculated over the entire lexicon. Our smoothing method is based on a method used in Deoskar (2008) for smoothing a PCFG lexicon. Eq. 1 and 2 apply it to CCG entries for unseen and rare words. In the first step, an outof-the-box POS tagger is used to tag the unlabeled or test corpus (we use the C&amp;C tagger). Counts of words and POS-tags ccorpus(w, T) are obtained from the tagged corpus. For the CCG lexicon, we ultimately need a count for a word w and a CCG category cat. To get this count, we split the count of a word and POS-tag amongst all categories seen with that tag in the supervised data in the same ratio as the ratio of the categories in the supervised data. In Eq. 1, this ratio is ctb</context>
</contexts>
<marker>Deoskar, 2008</marker>
<rawString>Tejaswini Deoskar. 2008. Re-estimation of Lexical Parameters for Treebank PCFGs. In Proceedings of COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tejaswini Deoskar</author>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Learning Structural Dependencies of Words in the Zipfian Tail.</title>
<date>2012</date>
<journal>Journal of Logic and Computation.</journal>
<marker>Deoskar, Mylonakis, Sima’an, 2012</marker>
<rawString>Tejaswini Deoskar, Markos Mylonakis, and Khalil Sima’an. 2012. Learning Structural Dependencies of Words in the Zipfian Tail. Journal of Logic and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus Variation and Parser Performance.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="1708" citStr="Gildea, 2001" startWordPosition="244" endWordPosition="245">een words. 1 Introduction An important open problem in natural language parsing is to generalize supervised parsers, which are trained on hand-labeled data, using unlabeled data. The problem arises because further handlabeled data in the amounts necessary to significantly improve supervised parsers are very unlikely to be made available. Generalization is also necessary in order to achieve good performance on parsing in textual domains other than the domain of the available labeled data. For example, parsers trained on Wall Street Journal (WSJ) data suffer a fall in accuracy on other domains (Gildea, 2001). In this paper, we use self-training to generalize the lexicon of a Combinatory Categorial Grammar (CCG) (Steedman, 2000) parser. CCG is a strongly lexicalized formalism, in which every word is associated with a syntactic category (similar to an elementary syntactic structure) indicating its subcategorization potential. Lexical entries are fine-grained and expressive, and contain a large amount of language-specific grammatical information. For parsers based on strongly lexicalized formalisms, the problem of grammar generalization can be cast largely as a problem of lexical extension. The pres</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus Variation and Parser Performance. In Proceedings of EMNLP 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Bias in learning syllable structure.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL05.</booktitle>
<contexts>
<context position="17688" citStr="Goldwater and Johnson, 2005" startWordPosition="2916" endWordPosition="2919"> Lexical entries for words in the parse are determined not by the POS-tag from a tagger, but directly by the parsing model, thus making the parse less susceptible to tagging errors. 5 Semi-supervised Learning We use Viterbi-EM (Neal and Hinton, 1998) as the self-training method. Viterbi-EM is an alternative to EM where instead of using the model parameters to find a true posterior from unlabeled data, a posterior based on the single maximumprobability (Viterbi) parse is used. Viterbi-EM has been used in various NLP tasks before and often performs better than classic EM (Cohen and Smith, 2010; Goldwater and Johnson, 2005; Spitkovsky et al., 2010). In practice, a given parsing model is used to obtain Viterbi parses of unlabeled sentences. The Viterbi parses are then treated as training data for a new model. This process is iterated until convergence. Since we are interested in learning the lexicon, we only consider lexical counts from Viterbi parses of the unlabeled sentences. Other parameters of the model are held at their supervised values. We conducted some experiments where we · ccorpus(w, T) (1) 129 self-trained all components of the parsing model, which is the usual case of self-training. We obtained neg</context>
</contexts>
<marker>Goldwater, Johnson, 2005</marker>
<rawString>Sharon Goldwater and Mark Johnson. 2005. Bias in learning syllable structure. In Proceedings of CoNLL05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="2779" citStr="Hockenmaier, 2003" startWordPosition="405" endWordPosition="406">rs based on strongly lexicalized formalisms, the problem of grammar generalization can be cast largely as a problem of lexical extension. The present paper focuses on learning lexical categories for words that are unseen or lowfrequency in labeled data, from unlabeled data. Since lexical categories in a strongly lexicalized formalism are complex, fine-grained (and far more numerous than simple part-of-speech tags), they are relatively sparse in labeled data. Despite performing at state-of-the-art levels, a major source of error made by CCG parsers is related to unseen and low-frequency words (Hockenmaier, 2003; Clark and Curran, 2007; Thomforde and Steedman, 2011). The unseen words for which we learn categories are surprisingly commonplace words of English; examples are conquered, apprehended, subdivided, scoring, denotes, hunted, obsessed, residing, migrated (Wikipedia). Correctly learning to parse the predicate-argument structures associated with such words (expressed as lexical categories in the case of CCG), is important for opendomain parsing, not only for CCG but indeed for any parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexi</context>
<context position="9965" citStr="Hockenmaier (2003)" startWordPosition="1576" endWordPosition="1577"> labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 Supervised Parser The CCG parser used in this paper is a reimplementation of the generative parser of Hockenmaier and Steedman (2002) and Hockenmaier (2003)1, except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexical category of the head word of the constituent, but not on the head word itself. While fully-lexicalized models that condition on words (and thus model wordto-word dependencies) are more accurate than unlexicalized ones like the LexCat model, we use an unlexicalized model2 for two reasons: first, 1These generative models are similar to the Collins’ headbased models (Collins, 1997), where for every no</context>
<context position="11992" citStr="Hockenmaier (2003)" startWordPosition="1909" endWordPosition="1910">but no new category types are introduced. Since the LexCat model conditions rule expansions on lexical categories, but not on words, it is still able to produce parses for sentences with new words. In contrast, a fully lexicalized model would need all components of the grammar to be smoothed, a task that is far from trivial due to the resulting explosion in grammar size (and one that we leave for future work). Second, although lexicalized models perform better on in-domain WSJ data (the LexCat model has an accuracy of 87.9% on Section 23, as opposed to 91.03% for the head-lexicalized model in Hockenmaier (2003) and 91.9% for the C&amp;C parser), our parser is more accurate on a question corpus, with a lexical category accuracy of 82.3%, as opposed to 71.6% and 78.6% for the C&amp;C and Hockenmaier (2003) respectively. 4.1 Handling rare and unseen words Existing CCG parsers (Hockenmaier (2003) and Clark and Curran (2007)) back-off rare and unseen words to their POS tag. The POS-backoff strategy is essentially a pipeline approach, where words are first tagged with coarse tags (POS tags) and finer tags (CCG categories) are later assigned, by the parser (Hockenmaier, 2003) or the supertagger (Clark and Curran, </context>
<context position="29734" citStr="Hockenmaier (2003)" startWordPosition="4913" endWordPosition="4914">pus is not strictly a different domain (since questions form a different kind of construction rather than a different domain), but it is an interesting case of adaptation for several reasons: WSJ parsers perform poorly on questions due to the small number of questions in the Penn Treebank/CCGbank. Secondly, unsupervised adaptation to questions has not been attempted before for CCG (Rimell and Clark (2008) did supervised adaptation of their supertagger). The supervised model SUP already performs at state-of-the-art on this corpus, on both overall scores and on wh(question)-words alone. C&amp;C and Hockenmaier (2003) get 71.6 and 78.6% overall accuracies respectively, and only 33.6 and 50.7 on wh-words alone. To our original unlabeled WSJ data (40M words), we add 1328 unlabeled question-sentences from Rimell and Clark, 2008, scaled by ten, so that each is counted ten times. We then evaluated on a testset containing questions (500 question sentences, from Rimell and Clark (2008)). The overall lexical category accuracy on this testset improves significantly as a result of the semi-supervised learning (Table 4). The accuracy on the question words alone (who, what, where, when, which, how, whose, whom) also i</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In ACL40.</booktitle>
<contexts>
<context position="9942" citStr="Hockenmaier and Steedman (2002)" startWordPosition="1570" endWordPosition="1574">guages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 Supervised Parser The CCG parser used in this paper is a reimplementation of the generative parser of Hockenmaier and Steedman (2002) and Hockenmaier (2003)1, except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexical category of the head word of the constituent, but not on the head word itself. While fully-lexicalized models that condition on words (and thus model wordto-word dependencies) are more accurate than unlexicalized ones like the LexCat model, we use an unlexicalized model2 for two reasons: first, 1These generative models are similar to the Collins’ headbased models (Collins, 19</context>
<context position="21877" citStr="Hockenmaier and Steedman (2002)" startWordPosition="3610" endWordPosition="3613">, we use two different datasets. The first one consists of question-sentences – 1328 unlabeled questions, obtained by removing the manual annotation of the question corpus from Rimell and Clark (2008). The second out-of-domain dataset consists of Wikipedia data, approximately 40 million words in size, with sentence length &lt; 20 words. 5.2 Experimental setup We ran our semi-supervised method using our parser with a smoothed lexicon (from §4.1.1) as the initial model, on unlabeled data of different sizes/domains. For comparison, we also ran experiments using a POS-backed off parser (the original Hockenmaier and Steedman (2002) LexCat model) as the initial model. Viterbi-EM converged at 4-5 iterations. We then parsed various test sets using the semi-supervised lexicons thus obtained. In all experiments, the labeled data was scaled to match the size of the unlabeled data. Thus, the scaling factor of labeled data was 10 for unlabeled data of 10M words, 20 for 20M words, etc. 5.3 Evaluation We focused our evaluations on unseen and lowfrequency verbs, since verbs are the most important open-class lexical entries and the most ambiguous to learn from unlabeled data (approx. 600 categories, versus 150 for nouns). We report</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative Models for Statistical Parsing with Combinatory Categorial Grammar. In ACL40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<date>2007</date>
<booktitle>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics,</booktitle>
<pages>33--355</pages>
<contexts>
<context position="20640" citStr="Hockenmaier and Steedman, 2007" startWordPosition="3412" endWordPosition="3415">omain experiments. We also conducted some limited experiments with classical semi-supervised EM, with similar settings of weighting labeled counts, and using unlabeled counts only for rare/unseen events. Since it is a much more computationally expensive procedure, and most of the results did not come close to the results of Viterbi-EM, we did not pursue it. 4The labeled count is weighted in order to scale up the labeled data which is usually smaller in size than the unlabeled data, to avoid swamping the labeled counts with much larger unlabeled counts. 5.1 Data Labeled: Sec. 02-21 of CCGbank (Hockenmaier and Steedman, 2007). In one experiment, we used Sec. 02-21 minus 1575 sentences that were held out to simulate test data containing unseen verbs— see §6.2 for details. Unlabeled: For in-domain experiments, we used sentences from the unlabeled WSJ portion of the ACL/DCI corpus (LDC93T1, 1993), and the WSJ portion of the ANC corpus (Reppen et al., 2005), limited to sentences containing 20 words or less, creating datasets of approximately 10, 20 and 40 million words each. Additionally, we have a dataset of 140 million words – 40M WSJ words plus an additional 100M from the New York Times. For domain-adaptation exper</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33:355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>Joel Nothman</author>
<author>James R Curran</author>
</authors>
<title>Evaluating a Statistial CCG Parser on Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP.</booktitle>
<contexts>
<context position="8349" citStr="Honnibal et al. (2009)" startWordPosition="1314" endWordPosition="1317">ttempts at harnessing unlabeled data to improve supervised CCG models using methods like self-training or co-training have been unsatisfactory (Steedman et al., 2003, 43-44). Steedman et al. (2003) experimented with self-training a generative CCG parser, and co-training a generative parser with an HMM-based supertagger. Cotraining (but not self-training) improved the results of the parser when the seed labeled data was small. When the seed data was large (the full treebank), i.e., the supervised baseline was high, co-training and self-training both failed to improve the parser. More recently, Honnibal et al. (2009) improved the performance of the C&amp;C parser on a domainadaptation task (adaptation to Wikipedia text) using self-training. Instead of self-training the parsing model, they re-train the supertagging model, which in turn affects parsing accuracy. They obtained an improvement of 1.09% (dependency &gt; &lt; 127 score) on supertagger accuracy on Wikipedia (although performance on WSJ text dropped) but did not attempt to re-train the parsing model. An orthogonal approach for extending a CCG lexicon using unlabeled data is that of Thomforde and Steedman (2011), in which a CCG category for an unknown word i</context>
<context position="30707" citStr="Honnibal et al. (2009)" startWordPosition="5067" endWordPosition="5070"> and Clark (2008)). The overall lexical category accuracy on this testset improves significantly as a result of the semi-supervised learning (Table 4). The accuracy on the question words alone (who, what, where, when, which, how, whose, whom) also improves numerically, but by a small amount (the number of tokens that improve are only 7). This could be an effect of the small size of the testset (500 sentences, i.e. 500 wh-words). Wikipedia We obtain statistically significant improvements in overall scores over a testset consisting of Wikipedia sentences hand-annotated with CCG categories (from Honnibal et al. (2009)) (Table 4). We also obtained improvements in lexical category accuracy on unseen words, and on unseen verbs alone (not shown), but could not prove significance. This testset contains only 200 sentences, and counts for unseen words are too small for significance tests, although there are numeric improvements. However, the overall improvement is statistically significantly, showing that adapting the lexicon alone is effective for a new domain. Lexical Category Accuracy for Unseen Verbs 55 60 65 70 Test:HOV Test:4Sec 132 6.3 Using semi-supervised lexicons with the C&amp;C parser To show that the lea</context>
</contexts>
<marker>Honnibal, Nothman, Curran, 2009</marker>
<rawString>Matthew Honnibal, Joel Nothman, and James R. Curran. 2009. Evaluating a Statistial CCG Parser on Wikipedia. In Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="9536" citStr="Koo et al. (2008)" startWordPosition="1504" endWordPosition="1507">category for an unknown word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 Supervised Parser The CCG parser used in this paper is a reimplementation of the generative parser of Hockenmaier and Steedman (2002) and Hockenmaier (2003)1, except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in th</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proceedings of ACL-08: HLT, pages 595–603. Association for Computational Linguistics, Columbus, Ohio.</rawString>
</citation>
<citation valid="false">
<booktitle>LDC93T1. 1993. LDC93T1. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<marker></marker>
<rawString>LDC93T1. 1993. LDC93T1. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
</authors>
<title>Parsing Biomedical Literature. In</title>
<date>2005</date>
<booktitle>Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP’05),</booktitle>
<volume>3651</volume>
<editor>R. Dale, K.-F. Wong, J. Su, and O. Kwong, eds.,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Jeju Island,</location>
<contexts>
<context position="13282" citStr="Lease and Charniak (2005)" startWordPosition="2123" endWordPosition="2126">trategy has given good performance in general for CCG parsers, but it has the disadvantage that POStagging errors are propagated. The parser can never recover from a tagging error, a problem that is serious for words in the Zipfian tail, where these words might also be unseen for the POS tagger and hence more likely to be tagged incorrectly. This issue is in fact more generally relevant than for CCG parsers alone—the dependence of parsers on POS-taggers was cited as one of the problems in domain-adaptation of parsers in the NAACL2012 shared task on parsing the web (Petrov and McDonald, 2012). Lease and Charniak (2005) obtained an improvement in the accuracy of the Charniak (2000) parser on a biomedical domain simply by training a new POS tagger model. In the following section, we describe an alternative smoothing-based approach to handling un128 seen and rare words. This method is less sensitive to POS tagging errors, as described below. In this approach, in a pre-processing step prior to parsing, categories are introduced into the lexicon for unseen and rare words from the data to be parsed. Some probability mass is taken from seen words/categories and given to unseen word and category pairs. Thus, at par</context>
</contexts>
<marker>Lease, Charniak, 2005</marker>
<rawString>Matthew Lease and Eugene Charniak. 2005. Parsing Biomedical Literature. In R. Dale, K.-F. Wong, J. Su, and O. Kwong, eds., Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP’05), vol. 3651 of Lecture Notes in Computer Science, pages 58 – 69. Springer-Verlag, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Combined Distributional and Logical Semantics. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="5027" citStr="Lewis and Steedman, 2013" startWordPosition="739" endWordPosition="742">moothingbased, and a part-of-speech back-off method) and find that a smoothing-based strategy for treating unseen words is more effective for semisupervised learning than part-of-speech back-off. 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a strongly lexicalized grammar formalism, in which the lexicon contains all language-specific grammatical information. The lexical entry of a word consists of a syntactic category which expresses the subcategorization potential of the word, and a semantic interpretation which defines the compositional semantics (Lewis and Steedman, 2013). A small number of combinatory rules are used to combine constituents, and it is straightforward to map syntactic categories to a logical form for semantic interpretation. For statistical CCG parsers, the lexicon is learnt from labeled data, and is subject to sparsity due to the fine-grained nature of the categories. Figure 1 illustrates this with a simple CCG derivation. In this sentence, bake is used as a ditransitive verb and is assigned the ditransitive category S\NP/NP/NP . This category defines the verb syntactically as mapping three NP arguments to a sentence S , and semantically as a </context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combined Distributional and Logical Semantics. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective Self-Training for Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="9479" citStr="McClosky et al. (2006)" startWordPosition="1495" endWordPosition="1498">data is that of Thomforde and Steedman (2011), in which a CCG category for an unknown word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 Supervised Parser The CCG parser used in this paper is a reimplementation of the generative parser of Hockenmaier and Steedman (2002) and Hockenmaier (2003)1, except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (20</context>
<context position="18930" citStr="McClosky et al. (2006)" startWordPosition="3126" endWordPosition="3129">r to Steedman et al. (2003), where self-training reduced the performance of the parsing model. We do not report them here. Thus, using unlabeled data only to estimate parameters that are badly estimated from labeled data (lexical entries in CCG, due to lexical sparsity) results in improvements, in contrast to prior work with semi-supervised EM. As is common in semi-supervised settings, we treated the count of each lexical event as the weighted count of that event in the labeled data (treebank)4 and the count from the Viterbi-parses of unlabeled data. Here we follow Bacchiani et al. (2006) and McClosky et al. (2006) who show that count merging is more effective than model interpolation. We placed an additional constraint on the contribution that the unlabeled data makes to the semisupervised model—we only use counts (from unlabeled data) of lexical events that are rarely seen/unseen in the labeled data. Our reasoning was that many lexical entries are estimated accurately from the treebank (for example, those related to function words and other high-frequency words) and estimation from unlabeled data might hurt them. We thus had a cut-off frequency (of words in labeled data) above which we did not allow t</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective Self-Training for Parsing. In Proceedings of HLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English Text with a Probabilistic Model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<pages>171</pages>
<contexts>
<context position="3877" citStr="Merialdo, 1994" startWordPosition="573" endWordPosition="574">parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexicalized parser can be an effective strategy for self-training and domain-adaptation. Our learnt lexicons improve on the lexical category accuracy of two supervised CCG parsers (Hockenmaier (2003) and the Clark and Curran (2007) parser, C&amp;C) on within-domain (WSJ) and out-of-domain test sets (a question corpus and a Wikipedia corpus). In most prior work, when EM was initialized based on labeled data, its performance did not improve over the supervised model (Merialdo, 1994; 126 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126–134, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Charniak, 1993). We found that in order for performance to improve, unlabeled data should be used only for parameters which are not well covered by the labeled data, while those that are well covered should remain fixed. In an additional contribution, we compare two strategies for treating unseen words (a smoothingbased, and a part-of-speech back-off method) and find that a smooth</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English Text with a Probabilistic Model. Computational Linguistics, 20(2):155– 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>In Learning and Graphical Models,</booktitle>
<pages>355--368</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3349" citStr="Neal and Hinton, 1998" startWordPosition="488" endWordPosition="491">to unseen and low-frequency words (Hockenmaier, 2003; Clark and Curran, 2007; Thomforde and Steedman, 2011). The unseen words for which we learn categories are surprisingly commonplace words of English; examples are conquered, apprehended, subdivided, scoring, denotes, hunted, obsessed, residing, migrated (Wikipedia). Correctly learning to parse the predicate-argument structures associated with such words (expressed as lexical categories in the case of CCG), is important for opendomain parsing, not only for CCG but indeed for any parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexicalized parser can be an effective strategy for self-training and domain-adaptation. Our learnt lexicons improve on the lexical category accuracy of two supervised CCG parsers (Hockenmaier (2003) and the Clark and Curran (2007) parser, C&amp;C) on within-domain (WSJ) and out-of-domain test sets (a question corpus and a Wikipedia corpus). In most prior work, when EM was initialized based on labeled data, its performance did not improve over the supervised model (Merialdo, 1994; 126 Proceedings of the 14th Conference of the European Chapter of the </context>
<context position="17311" citStr="Neal and Hinton, 1998" startWordPosition="2854" endWordPosition="2857">pus(w, cat) of a word and category with the supervised count of the same ctb(w, cat) (if it exists) to give the final smoothed count of a word and category csmooth(w, cat) (Eq. 2). csmooth(w, cat) = λ · ctb(w, cat) + (1 − λ) · ccorpus(w, cat) (2) When this smoothed lexicon is used with a parser, POS-backoff is not necessary since all needed words are now in the lexicon. Lexical entries for words in the parse are determined not by the POS-tag from a tagger, but directly by the parsing model, thus making the parse less susceptible to tagging errors. 5 Semi-supervised Learning We use Viterbi-EM (Neal and Hinton, 1998) as the self-training method. Viterbi-EM is an alternative to EM where instead of using the model parameters to find a true posterior from unlabeled data, a posterior based on the single maximumprobability (Viterbi) parse is used. Viterbi-EM has been used in various NLP tasks before and often performs better than classic EM (Cohen and Smith, 2010; Goldwater and Johnson, 2005; Spitkovsky et al., 2010). In practice, a given parsing model is used to obtain Viterbi parses of unlabeled sentences. The Viterbi parses are then treated as training data for a new model. This process is iterated until co</context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>Radford M. Neal and Geoffrey E. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning and Graphical Models, pages 355 – 368. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 Shared Task on Parsing the Web. In</title>
<date>2012</date>
<booktitle>First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL) Workshop at NAACL</booktitle>
<contexts>
<context position="13255" citStr="Petrov and McDonald, 2012" startWordPosition="2119" endWordPosition="2122">ccurate than parsers, this strategy has given good performance in general for CCG parsers, but it has the disadvantage that POStagging errors are propagated. The parser can never recover from a tagging error, a problem that is serious for words in the Zipfian tail, where these words might also be unseen for the POS tagger and hence more likely to be tagged incorrectly. This issue is in fact more generally relevant than for CCG parsers alone—the dependence of parsers on POS-taggers was cited as one of the problems in domain-adaptation of parsers in the NAACL2012 shared task on parsing the web (Petrov and McDonald, 2012). Lease and Charniak (2005) obtained an improvement in the accuracy of the Charniak (2000) parser on a biomedical domain simply by training a new POS tagger model. In the following section, we describe an alternative smoothing-based approach to handling un128 seen and rare words. This method is less sensitive to POS tagging errors, as described below. In this approach, in a pre-processing step prior to parsing, categories are introduced into the lexicon for unseen and rare words from the data to be parsed. Some probability mass is taken from seen words/categories and given to unseen word and c</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 Shared Task on Parsing the Web. In First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL) Workshop at NAACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randi Reppen</author>
<author>Nancy Ide</author>
<author>Keith Suderman</author>
</authors>
<date>2005</date>
<booktitle>LDC2005T35, American National Corpus (ANC) Second Release. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="20974" citStr="Reppen et al., 2005" startWordPosition="3467" endWordPosition="3470">not pursue it. 4The labeled count is weighted in order to scale up the labeled data which is usually smaller in size than the unlabeled data, to avoid swamping the labeled counts with much larger unlabeled counts. 5.1 Data Labeled: Sec. 02-21 of CCGbank (Hockenmaier and Steedman, 2007). In one experiment, we used Sec. 02-21 minus 1575 sentences that were held out to simulate test data containing unseen verbs— see §6.2 for details. Unlabeled: For in-domain experiments, we used sentences from the unlabeled WSJ portion of the ACL/DCI corpus (LDC93T1, 1993), and the WSJ portion of the ANC corpus (Reppen et al., 2005), limited to sentences containing 20 words or less, creating datasets of approximately 10, 20 and 40 million words each. Additionally, we have a dataset of 140 million words – 40M WSJ words plus an additional 100M from the New York Times. For domain-adaptation experiments, we use two different datasets. The first one consists of question-sentences – 1328 unlabeled questions, obtained by removing the manual annotation of the question corpus from Rimell and Clark (2008). The second out-of-domain dataset consists of Wikipedia data, approximately 40 million words in size, with sentence length &lt; 20</context>
</contexts>
<marker>Reppen, Ide, Suderman, 2005</marker>
<rawString>Randi Reppen, Nancy Ide, and Keith Suderman. 2005. LDC2005T35, American National Corpus (ANC) Second Release. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a Lexicalized-Grammar Parser to Contrasting Domains.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-08).</booktitle>
<contexts>
<context position="21446" citStr="Rimell and Clark (2008)" startWordPosition="3543" endWordPosition="3546">ts, we used sentences from the unlabeled WSJ portion of the ACL/DCI corpus (LDC93T1, 1993), and the WSJ portion of the ANC corpus (Reppen et al., 2005), limited to sentences containing 20 words or less, creating datasets of approximately 10, 20 and 40 million words each. Additionally, we have a dataset of 140 million words – 40M WSJ words plus an additional 100M from the New York Times. For domain-adaptation experiments, we use two different datasets. The first one consists of question-sentences – 1328 unlabeled questions, obtained by removing the manual annotation of the question corpus from Rimell and Clark (2008). The second out-of-domain dataset consists of Wikipedia data, approximately 40 million words in size, with sentence length &lt; 20 words. 5.2 Experimental setup We ran our semi-supervised method using our parser with a smoothed lexicon (from §4.1.1) as the initial model, on unlabeled data of different sizes/domains. For comparison, we also ran experiments using a POS-backed off parser (the original Hockenmaier and Steedman (2002) LexCat model) as the initial model. Viterbi-EM converged at 4-5 iterations. We then parsed various test sets using the semi-supervised lexicons thus obtained. In all ex</context>
<context position="29524" citStr="Rimell and Clark (2008)" startWordPosition="4882" endWordPosition="4885">wh All Unseen words words words words SUP 82.36 61.77 84.31 79.5 SEMISUP *83.21 63.22 *85.6 80.25 Table 4: Out-of-domain: Questions and Wikipedia, *p&lt;0.05, McNemar test 6.2.1 Out-of-Domain Questions The question corpus is not strictly a different domain (since questions form a different kind of construction rather than a different domain), but it is an interesting case of adaptation for several reasons: WSJ parsers perform poorly on questions due to the small number of questions in the Penn Treebank/CCGbank. Secondly, unsupervised adaptation to questions has not been attempted before for CCG (Rimell and Clark (2008) did supervised adaptation of their supertagger). The supervised model SUP already performs at state-of-the-art on this corpus, on both overall scores and on wh(question)-words alone. C&amp;C and Hockenmaier (2003) get 71.6 and 78.6% overall accuracies respectively, and only 33.6 and 50.7 on wh-words alone. To our original unlabeled WSJ data (40M words), we add 1328 unlabeled question-sentences from Rimell and Clark, 2008, scaled by ten, so that each is counted ten times. We then evaluated on a testset containing questions (500 question sentences, from Rimell and Clark (2008)). The overall lexical</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a Lexicalized-Grammar Parser to Contrasting Domains. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Viterbi Training Improves Unsupervised Dependency Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL-2010.</booktitle>
<contexts>
<context position="17714" citStr="Spitkovsky et al., 2010" startWordPosition="2920" endWordPosition="2923"> the parse are determined not by the POS-tag from a tagger, but directly by the parsing model, thus making the parse less susceptible to tagging errors. 5 Semi-supervised Learning We use Viterbi-EM (Neal and Hinton, 1998) as the self-training method. Viterbi-EM is an alternative to EM where instead of using the model parameters to find a true posterior from unlabeled data, a posterior based on the single maximumprobability (Viterbi) parse is used. Viterbi-EM has been used in various NLP tasks before and often performs better than classic EM (Cohen and Smith, 2010; Goldwater and Johnson, 2005; Spitkovsky et al., 2010). In practice, a given parsing model is used to obtain Viterbi parses of unlabeled sentences. The Viterbi parses are then treated as training data for a new model. This process is iterated until convergence. Since we are interested in learning the lexicon, we only consider lexical counts from Viterbi parses of the unlabeled sentences. Other parameters of the model are held at their supervised values. We conducted some experiments where we · ccorpus(w, T) (1) 129 self-trained all components of the parsing model, which is the usual case of self-training. We obtained negative results similar to S</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D. Manning. 2010. Viterbi Training Improves Unsupervised Dependency Parsing. In Proceedings of CoNLL-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press/Bradford Books.</publisher>
<contexts>
<context position="1830" citStr="Steedman, 2000" startWordPosition="263" endWordPosition="264">ch are trained on hand-labeled data, using unlabeled data. The problem arises because further handlabeled data in the amounts necessary to significantly improve supervised parsers are very unlikely to be made available. Generalization is also necessary in order to achieve good performance on parsing in textual domains other than the domain of the available labeled data. For example, parsers trained on Wall Street Journal (WSJ) data suffer a fall in accuracy on other domains (Gildea, 2001). In this paper, we use self-training to generalize the lexicon of a Combinatory Categorial Grammar (CCG) (Steedman, 2000) parser. CCG is a strongly lexicalized formalism, in which every word is associated with a syntactic category (similar to an elementary syntactic structure) indicating its subcategorization potential. Lexical entries are fine-grained and expressive, and contain a large amount of language-specific grammatical information. For parsers based on strongly lexicalized formalisms, the problem of grammar generalization can be cast largely as a problem of lexical extension. The present paper focuses on learning lexical categories for words that are unseen or lowfrequency in labeled data, from unlabeled</context>
<context position="4684" citStr="Steedman, 2000" startWordPosition="691" endWordPosition="693">omputational Linguistics Charniak, 1993). We found that in order for performance to improve, unlabeled data should be used only for parameters which are not well covered by the labeled data, while those that are well covered should remain fixed. In an additional contribution, we compare two strategies for treating unseen words (a smoothingbased, and a part-of-speech back-off method) and find that a smoothing-based strategy for treating unseen words is more effective for semisupervised learning than part-of-speech back-off. 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a strongly lexicalized grammar formalism, in which the lexicon contains all language-specific grammatical information. The lexical entry of a word consists of a syntactic category which expresses the subcategorization potential of the word, and a semantic interpretation which defines the compositional semantics (Lewis and Steedman, 2013). A small number of combinatory rules are used to combine constituents, and it is straightforward to map syntactic categories to a logical form for semantic interpretation. For statistical CCG parsers, the lexicon is learnt from labeled data, and is subject</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press/Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Rebecca Hwa</author>
<author>Miles Osbornn</author>
<author>Paul Ruhlen</author>
<author>Anoop Sarkar</author>
</authors>
<title>Semi-Supervised Training for Statistical Parsing.</title>
<date>2003</date>
<tech>Tech. rep., CLSP WS-02.</tech>
<contexts>
<context position="7892" citStr="Steedman et al., 2003" startWordPosition="1243" endWordPosition="1246">, there are several attested category types that are entirely missing from the lexicon, Clark et al., 2004). We make the assumption here that all category types in the language have been seen in the labeled data. In principle new category types may be introduced independently without affecting our semi-supervised process (for instance, manually, or via a method that predicts new category types from those seen in labeled data). 3 Related Work Previous attempts at harnessing unlabeled data to improve supervised CCG models using methods like self-training or co-training have been unsatisfactory (Steedman et al., 2003, 43-44). Steedman et al. (2003) experimented with self-training a generative CCG parser, and co-training a generative parser with an HMM-based supertagger. Cotraining (but not self-training) improved the results of the parser when the seed labeled data was small. When the seed data was large (the full treebank), i.e., the supervised baseline was high, co-training and self-training both failed to improve the parser. More recently, Honnibal et al. (2009) improved the performance of the C&amp;C parser on a domainadaptation task (adaptation to Wikipedia text) using self-training. Instead of self-trai</context>
<context position="18335" citStr="Steedman et al. (2003)" startWordPosition="3027" endWordPosition="3030">). In practice, a given parsing model is used to obtain Viterbi parses of unlabeled sentences. The Viterbi parses are then treated as training data for a new model. This process is iterated until convergence. Since we are interested in learning the lexicon, we only consider lexical counts from Viterbi parses of the unlabeled sentences. Other parameters of the model are held at their supervised values. We conducted some experiments where we · ccorpus(w, T) (1) 129 self-trained all components of the parsing model, which is the usual case of self-training. We obtained negative results similar to Steedman et al. (2003), where self-training reduced the performance of the parsing model. We do not report them here. Thus, using unlabeled data only to estimate parameters that are badly estimated from labeled data (lexical entries in CCG, due to lexical sparsity) results in improvements, in contrast to prior work with semi-supervised EM. As is common in semi-supervised settings, we treated the count of each lexical event as the weighted count of that event in the labeled data (treebank)4 and the count from the Viterbi-parses of unlabeled data. Here we follow Bacchiani et al. (2006) and McClosky et al. (2006) who </context>
</contexts>
<marker>Steedman, Baker, Crim, Clark, Hockenmaier, Hwa, Osbornn, Ruhlen, Sarkar, 2003</marker>
<rawString>Mark Steedman, Steven Baker, Jeremiah Crim, Stephen Clark, Julia Hockenmaier, Rebecca Hwa, Miles Osbornn, Paul Ruhlen, and Anoop Sarkar. 2003. Semi-Supervised Training for Statistical Parsing. Tech. rep., CLSP WS-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>551--560</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics, Singapore.</institution>
<contexts>
<context position="9561" citStr="Suzuki et al. (2009)" startWordPosition="1509" endWordPosition="1512">n word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 Supervised Parser The CCG parser used in this paper is a reimplementation of the generative parser of Hockenmaier and Steedman (2002) and Hockenmaier (2003)1, except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexic</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551– 560. Association for Computational Linguistics, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Thomforde</author>
<author>Mark Steedman</author>
</authors>
<title>Semisupervised CCG Lexicon Extension.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>UK.</publisher>
<location>Edinburgh</location>
<contexts>
<context position="2834" citStr="Thomforde and Steedman, 2011" startWordPosition="411" endWordPosition="415">, the problem of grammar generalization can be cast largely as a problem of lexical extension. The present paper focuses on learning lexical categories for words that are unseen or lowfrequency in labeled data, from unlabeled data. Since lexical categories in a strongly lexicalized formalism are complex, fine-grained (and far more numerous than simple part-of-speech tags), they are relatively sparse in labeled data. Despite performing at state-of-the-art levels, a major source of error made by CCG parsers is related to unseen and low-frequency words (Hockenmaier, 2003; Clark and Curran, 2007; Thomforde and Steedman, 2011). The unseen words for which we learn categories are surprisingly commonplace words of English; examples are conquered, apprehended, subdivided, scoring, denotes, hunted, obsessed, residing, migrated (Wikipedia). Correctly learning to parse the predicate-argument structures associated with such words (expressed as lexical categories in the case of CCG), is important for opendomain parsing, not only for CCG but indeed for any parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexicalized parser can be an effectiv</context>
<context position="8902" citStr="Thomforde and Steedman (2011)" startWordPosition="1401" endWordPosition="1404">ing both failed to improve the parser. More recently, Honnibal et al. (2009) improved the performance of the C&amp;C parser on a domainadaptation task (adaptation to Wikipedia text) using self-training. Instead of self-training the parsing model, they re-train the supertagging model, which in turn affects parsing accuracy. They obtained an improvement of 1.09% (dependency &gt; &lt; 127 score) on supertagger accuracy on Wikipedia (although performance on WSJ text dropped) but did not attempt to re-train the parsing model. An orthogonal approach for extending a CCG lexicon using unlabeled data is that of Thomforde and Steedman (2011), in which a CCG category for an unknown word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reran</context>
</contexts>
<marker>Thomforde, Steedman, 2011</marker>
<rawString>Emily Thomforde and Mark Steedman. 2011. Semisupervised CCG Lexicon Extension. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Edinburgh UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>