<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012173">
<title confidence="0.9885245">
What to do when lexicalization fails: parsing German with suffix analysis
and smoothing
</title>
<author confidence="0.990159">
Amit Dubey
</author>
<affiliation confidence="0.997067">
University of Edinburgh
</affiliation>
<email confidence="0.995085">
Amit.Dubey@ed.ac.uk
</email>
<sectionHeader confidence="0.993818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998788">
In this paper, we present an unlexical-
ized parser for German which employs
smoothing and suffix analysis to achieve
a labelled bracket F-score of 76.2, higher
than previously reported results on the
NEGRA corpus. In addition to the high
accuracy of the model, the use of smooth-
ing in an unlexicalized parser allows us
to better examine the interplay between
smoothing and parsing results.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999850636363637">
Recent research on German statistical parsing has
shown that lexicalization adds little to parsing per-
formance in German (Dubey and Keller, 2003; Beil
et al., 1999). A likely cause is the relative produc-
tivity of German morphology compared to that of
English: German has a higher type/token ratio for
words, making sparse data problems more severe.
There are at least two solutions to this problem: first,
to use better models of morphology or, second, to
make unlexicalized parsing more accurate.
We investigate both approaches in this paper. In
particular, we develop a parser for German which at-
tains the highest performance known to us by mak-
ing use of smoothing and a highly-tuned suffix ana-
lyzer for guessing part-of-speech (POS) tags from
the input text. Rather than relying on smoothing
and suffix analysis alone, we also utilize treebank
transformations (Johnson, 1998; Klein and Man-
ning, 2003) instead of a grammar induced directly
from a treebank.
The organization of the paper is as follows: Sec-
tion 2 summarizes some important aspects of our
</bodyText>
<page confidence="0.987336">
314
</page>
<bodyText confidence="0.999779166666667">
treebank corpus. In Section 3 we outline several
techniques for improving the performance of unlex-
icalized parsing without using smoothing, including
treebank transformations, and the use of suffix anal-
ysis. We show that suffix analysis is not helpful
on the treebank grammar, but it does increase per-
formance if used in combination with the treebank
transformations we present. Section 4 describes how
smoothing can be incorporated into an unlexicalized
grammar to achieve state-of-the-art results in Ger-
man. Rather using one smoothing algorithm, we use
three different approaches, allowing us to compare
the relative performance of each. An error analy-
sis is presented in Section 5, which points to several
possible areas of future research. We follow the er-
ror analysis with a comparison with related work in
Section 6. Finally we offer concluding remarks in
Section 7.
</bodyText>
<sectionHeader confidence="0.994543" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.9999698">
The parsing models we present are trained and tested
on the NEGRA corpus (Skut et al., 1997), a hand-
parsed corpus of German newspaper text containing
approximately 20,000 sentences. It is available in
several formats, and in this paper, we use the Penn
Treebank (Marcus et al., 1993) format of NEGRA.
The annotation used in NEGRA is similar to that
used in the English Penn Treebank, with some dif-
ferences which make it easier to annotate German
syntax. German’s flexible word order would have
required an explosion in long-distance dependencies
(LDDs) had annotation of NEGRA more closely
resembled that of the Penn Treebank. The NE-
GRA designers therefore chose to use relatively flat
trees, encoding elements of flexible word order us-
</bodyText>
<note confidence="0.984924">
Proceedings of the 43rd Annual Meeting of the ACL, pages 314–321,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.998513235294118">
ing grammatical functions (GFs) rather than LDDs
wherever possible.
To illustrate flexible word order, consider the sen-
tences Der Mann sieht den Jungen (‘The man sees
the boy’) and Den Jungen sieht der Mann. Despite
the fact the subject and object are swapped in the
second sentence, the meaning of both are essentially
the same.1 The two possible word orders are dis-
ambiguated by the use of the nominative case for
the subject (marked by the article der) and the ac-
cusative case for the object (marked by den) rather
than their position in the sentence.
Whenever the subject appears after the verb, the
non-standard position may be annotated using a
long-distance dependency (LDD). However, as men-
tioned above, this information can also be retrieved
from the grammatical function of the respective
noun phrases: the GFs of the two NPs above would
be ‘subject’ and ‘accusative object’ regardless of
their position in the sentence. These labels may
therefore be used to recover the underlying depen-
dencies without having to resort to LDDs. This is
the approach used in NEGRA. It does have limita-
tions: it is only possible to use GF labels instead of
LDDs when all the nodes of interest are dominated
by the same parent. To maximize cases where all
necessary nodes are dominated by the same parent,
NEGRA uses flat ‘dependency-style’ rules. For ex-
ample, there is no VP node when there is no overt
auxiliary verb. category. Under the NEGRA anno-
tation scheme, the first sentence above would have
a rule S NP-SB VVFIN NP-OA and the second,
S NP-OA VVFIN NP-SB, where SB denotes sub-
ject and OA denotes accusative object.
</bodyText>
<sectionHeader confidence="0.612508" genericHeader="method">
3 Parsing with Grammatical Functions
</sectionHeader>
<subsectionHeader confidence="0.99823">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.985906571428571">
As explained above, this paper focuses on unlexi-
calized grammars. In particular, we make use of
probabilistic context-free grammars (PCFGs; Booth
(1969)) for our experiments. A PCFG assigns each
context-free rule LHS RHS a conditional prob-
ability Pr RHSLHS . If a parser were to be given
POS tags as input, this would be the only distribution
</bodyText>
<footnote confidence="0.929933666666667">
&apos;Pragmatically speaking, the second sentence has a slightly
different meaning. A better translation might be: ‘It is the boy
the man sees.’
</footnote>
<bodyText confidence="0.997017">
required. However, in this paper we are concerned
with the more realistic problem of accepting text as
input. Therefore, the parser also needs a probabil-
ity distribution Pw wLHS to generate words. The
probability of a tree is calculated by multiplying the
probabilities all the rules and words generated in the
derivation of the tree.
The rules are simply read out from the treebank,
and the probabilities are estimated from the fre-
quency of rules in the treebank. More formally:
</bodyText>
<equation confidence="0.974443333333333">
c LHS RHS
Pr RHSLHS (1)
c LHS
</equation>
<bodyText confidence="0.97236">
The probabilities of words given tags are simi-
larly estimated from the frequency of word-tag co-
occurrences:
</bodyText>
<equation confidence="0.8358395">
(2)
c LHS
</equation>
<bodyText confidence="0.926158954545454">
To handle unseen or infrequent words, all words
whose frequency falls below a threshold Ω are
grouped together in an ‘unknown word’ token,
which is then treated like an additional word. For
our experiments, we use Ω 10.
We consider several variations of this simple
model by changing both Pr and Pw. In addition to
the standard formulation in Equation (1), we con-
sider two alternative variants of Pr. The first is a
Markov context-free rule (Magerman, 1995; Char-
niak, 2000). A rule may be turned into a Markov
rule by first binarizing it, then making independence
assumptions on the new binarized rules. Binarizing
the rule A B1 Bn results in a number of smaller
rules A B1AB1, AB1 B2AB1B2, , AB1Bn 1
Bn. Binarization does not change the probability of
the rule:
P BiA B1 Bi 1
Making the 2nd order Markov assumption ‘forgets’
everything earlier then 2 previous sisters. A rule
would now be in the form ABi 2Bi 1 BiABi 1Bi, and
the probability would be:
</bodyText>
<equation confidence="0.987004818181818">
Pw wLHS
c LHS w
P B1 BnA
i 1
∏
n
P B1 BnA
i 1
∏
n
P BiA Bi 2 Bi 1
</equation>
<page confidence="0.994808">
315
</page>
<bodyText confidence="0.999458568181819">
The other rule type we consider are linear prece-
dence/immediate dominance (LP/ID) rules (Gazdar
et al., 1985). If a context-free rule can be thought
of as a LHS token with an ordered list of tokens on
the RHS, then an LP/ID rule can be thought of as
a LHS token with a multiset of tokens on the RHS
together with some constraints on the possible or-
ders of tokens on the RHS. Uszkoreit (1987) argues
that LP/ID rules with violatable ‘soft’ constraints
are suitable for modelling some aspects of German
word order. This makes a probabilistic formulation
of LP/ID rules ideal: probabilities act as soft con-
straints.
Our treatment of probabilistic LP/ID rules gener-
ate children one constituent at a time, conditioning
upon the parent and a multiset of previously gener-
ated children. Formally, the the probability of the
rule is approximated as:
In addition to the two additional formulations of
the Pr distribution, we also consider one variant of
the Pw distribution, which includes the suffix anal-
ysis. It is important to clarify that we only change
the handling of uncommon and unknown words;
those which occur often are handled as normal. sug-
gested different choices for Pw in the face of un-
known words: Schiehlen (2004) suggests using a
different unknown word token for capitalized ver-
sus uncapitalized unknown words (German orthog-
raphy dictates that all common nouns are capital-
ized) and Levy and Manning (2004) consider in-
specting the last letter the unknown word to guess
the part-of-speech (POS) tags. Both of these models
are relatively impoverished when compared to the
approaches of handling unknown words which have
been proposed in the POS tagging literature. Brants
(2000) describes a POS tagger with a highly tuned
suffix analyzer which considers both capitalization
and suffixes as long as 10 letters long. This tagger
was developed with German in mind, but neither it
nor any other advanced POS tagger morphology an-
alyzer has ever been tested with a full parser. There-
fore, we take the novel step of integrating this suffix
analyzer into the parser for the second Pw distribu-
tion.
</bodyText>
<subsectionHeader confidence="0.997083">
3.2 Treebank Re-annotation
</subsectionHeader>
<bodyText confidence="0.99986711627907">
Automatic treebank transformations are an impor-
tant step in developing an accurate unlexicalized
parser (Johnson, 1998; Klein and Manning, 2003).
Most of our transformations focus upon one part of
the NEGRA treebank in particular: the GF labels.
Below is a list of GF re-annotations we utilise:
Coord GF In NEGRA, a co-ordinated accusative
NP rule might look like NP-OA NP-CJ KON NP-
CJ. KON is the POS tag for a conjunct, and CJ
denotes the function of the NP is a coordinate sis-
ter. Such a rule hides an important fact: the two
co-ordinate sisters are also accusative objects. The
Coord GF re-annotation would therefore replace the
above rule with NP-OA NP-OA KON NP-OA.
NP case German articles and pronouns are
strongly marked for case. However, the grammati-
cal function of all articles is usually NK, meaning
noun kernel. To allow case markings in articles and
pronouns to ‘communicate’ with the case labels on
the GFs of NPs, we copy these GFs down into the
POS tags of articles and pronouns. For example,
a rule like NP-OA ART-NK NN-NK would be
replaced by NP-OA ART-OA NN-NK. A simi-
lar improvement has been independently noted by
Schiehlen (2004).
PP case Prepositions determine the case of the NP
they govern. While the case is often unambiguous
(i.e. f¨ur ‘for’ always takes an accusative NP), at
times the case may be ambiguous. For instance,
in ‘in’ may take either an accusative or dative NP.
We use the labels -OA, -OD, etc. for unambiguous
prepositions, and introduce new categories AD (ac-
cusative/dative ambiguous) and DG (dative/genitive
ambiguous) for the ambiguous categories. For ex-
ample, a rule such as PP P ART-NK NN-NK is
replaced with PP P-AD ART-AD NN-NK if it is
headed by the preposition in.
SBAR marking German subordinate clauses have
a different word order than main clauses. While sub-
ordinate clauses can usually be distinguished from
main clauses by their GF, there are some GFs which
are used in both cases. This transformation adds
an SBAR category to explicitly disambiguate these
</bodyText>
<equation confidence="0.9991954">
P B1 BnA
i 1
∏
n
PBiA Bj j i
</equation>
<page confidence="0.996724">
316
</page>
<table confidence="0.999391">
No suffix With suffix
F-score F-score
Normal rules 66.3 66.2
LP/ID rules 66.5 66.6
Markov rules 69.4 69.1
</table>
<tableCaption confidence="0.999569">
Table 1: Effect of rule type and suffix analysis.
</tableCaption>
<bodyText confidence="0.948887066666667">
cases. The transformation does not add any extra
nonterminals, rather it replaces rules such as S
KOUS NP V NP (where KOUS is a complementizer
POS tag) with SBAR KOUS NP V NP.
S GF One may argue that, as far as syntactic dis-
ambiguation is concerned, GFs on S categories pri-
marily serve to distinguish main clauses from sub-
ordinate clauses. As we have explicitly done this
in the previous transformation, it stands to reason
that the GF tags on S nodes may therefore be re-
moved without penalty. If the tags are necessary for
semantic interpretation, presumably they could be
re-inserted using a strategy such as that of Blaheta
and Charniak (2000) The last transformation there-
fore removes the GF of S nodes.
</bodyText>
<subsectionHeader confidence="0.998216">
3.3 Method
</subsectionHeader>
<bodyText confidence="0.989733181818182">
To allow comparisons with earlier work on NEGRA
parsing, we use the same split of training, develop-
ment and testing data as used in Dubey and Keller
(2003). The first 18,602 sentences are used as train-
ing data, the following 1,000 form the development
set, and the last 1,000 are used as the test set. We re-
move long-distance dependencies from all sets, and
only consider sentences of length 40 or less for ef-
ficiency and memory concerns. The parser is given
untagged words as input to simulate a realistic pars-
ing task. A probabilistic CYK parsing algorithm is
used to compute the Viterbi parse.
We perform two sets of experiments. In the
first set, we vary the rule type, and in the second,
we report the additive results of the treebank re-
annotations described in Section 3.2. The three rule
types used in the first set of experiments are stan-
dard CFG rules, our version of LP/ID rules, and 2nd
order Markov CFG rules. The second battery of ex-
periments was performed on the model with Markov
rules.
In both cases, we report PARSEVAL labeled
</bodyText>
<table confidence="0.99995175">
No suffix With suffix
F-score F-score
GF Baseline 69.4 69.1
+Coord GF 70.2 71.5
+NP case 71.1 72.4
+PP case 71.0 72.7
+SBAR 70.9 72.6
+S GF 71.3 73.1
</table>
<tableCaption confidence="0.979166">
Table 2: Effect of re-annotation and suffix analysis
with Markov rules.
</tableCaption>
<bodyText confidence="0.9928138">
bracket scores (Magerman, 1995), with the brackets
labeled by syntactic categories but not grammatical
functions. Rather than reporting precision and recall
of labelled brackets, we report only the F-score, i.e.
the harmonic mean of precision and recall.
</bodyText>
<sectionHeader confidence="0.926313" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.999449388888889">
Table 1 shows the effect of rule type choice, and Ta-
ble 2 lists the effect of the GF re-annotations. From
Table 1, we see that Markov rules achieve the best
performance, ahead of both standard rules as well as
our formulation of probabilistic LP/ID rules.
In the first group of experiments, suffix analysis
marginally lowers performance. However, a differ-
ent pattern emerges in the second set of experiments.
Suffix analysis consistently does better than the sim-
pler word generation probability model.
Looking at the treebank transformations with suf-
fix analysis enabled, we find the coordination re-
annotation provides the greatest benefit, boosting
performance by 2.4 to 71.5. The NP and PP case
re-annotations together raise performance by 1.2 to
72.7. While the SBAR annotation slightly lowers
performance, removing the GF labels from S nodes
increased performance to 73.1.
</bodyText>
<sectionHeader confidence="0.558583" genericHeader="method">
3.5 Discussion
</sectionHeader>
<bodyText confidence="0.999577888888889">
There are two primary results: first, although LP/ID
rules have been suggested as suitable for German’s
flexible word order, it appears that Markov rules ac-
tually perform better. Second, adding suffix analysis
provides a clear benefit, but only after the inclusion
of the Coord GF transformation.
While the SBAR transformation slightly reduces
performance, recall that we argued the S GF trans-
formation only made sense if the SBAR transforma-
</bodyText>
<page confidence="0.955778">
317
</page>
<equation confidence="0.997204722222222">
λ1 λ2 λ3 0
for each trigram x1x2x3 with cx1x2x3 0
cxixi 1xi 2 1 if c xi 1 xi 2 1
c xi 1 xi 2 1
0 if c xi 1 xi 2 1
cxixi 1 1 if c xi 1 1
c xi 1 1
0 if c xi 1 1
cxi 1
d1 N 1
if d3 max d1 d2 d3 then
λ3 λ3 c xi xi 1 xi 2
elseif d2 max d1 d2 d3 then
λ2 λ2 c xi xi 1 xi 2
else
λ1 λ1 c xi xi 1 xi 2
d3
d2
</equation>
<bodyText confidence="0.9977282">
tion is already in place. To test if this was indeed the
case, we re-ran the final experiment, but excluded
the SBAR transformation. We did indeed find that
applying S GF without the SBAR transformation re-
duced performance.
</bodyText>
<sectionHeader confidence="0.874796" genericHeader="method">
4 Smoothing &amp; Search
</sectionHeader>
<bodyText confidence="0.999989666666667">
With the exception of DOP models (Bod, 1995), it is
uncommon to smooth unlexicalized grammars. This
is in part for the sake of simplicity: unlexicalized
grammars are interesting because they are simple
to estimate and parse, and adding smoothing makes
both estimation and parsing nearly as complex as
with fully lexicalized models. However, because
lexicalization adds little to the performance of Ger-
man parsing models, it is therefore interesting to in-
vestigate the impact of smoothing on unlexicalized
parsing models for German.
Parsing an unsmoothed unlexicalized grammar is
relatively efficient because the grammar constraints
the search space. As a smoothed grammar does not
have a constrained search space, it is necessary to
find other means to make parsing faster. Although
it is possible to efficiently compute the Viterbi parse
(Klein and Manning, 2002) using a smoothed gram-
mar, the most common approach to increase parsing
speed is to use some form of beam search (cf. Good-
man (1998)), a strategy we follow here.
</bodyText>
<subsectionHeader confidence="0.98678">
4.1 Models
</subsectionHeader>
<bodyText confidence="0.999952153846154">
We experiment with three different smoothing mod-
els: the modified Witten-Bell algorithm employed
by Collins (1999), the modified Kneser-Ney algo-
rithm of Chen and Goodman (1998) the smooth-
ing algorithm used in the POS tagger of Brants
(2000). All are variants of linear interpolation, and
are used with 2nd order Markovization. Under this
regime, the probability of adding the ith child to
A B1 Bn is estimated as
The models differ in how the λ’s are estimated. For
both the Witten-Bell and Kneser-Ney algorithms,
the λ’s are a function of the context A Bi 2 Bi 1. By
contrast, in Brants’ algorithm the λ’s are constant
</bodyText>
<equation confidence="0.997918714285714">
end
λ1
λ1 λ1 λ2 λ 3
λ2
λ2 λ1 λ2 λ 3
λ3
λ3 λ1 λ2 λ 3
</equation>
<figureCaption confidence="0.953457">
Figure 1: Smoothing estimation based on the Brants
(2000) approach for POS tagging.
</figureCaption>
<bodyText confidence="0.998185857142857">
for all possible contexts. As both the Witten-Bell
and Kneser-Ney variants are fairly well known, we
do not describe them further. However, as Brants’
approach (to our knowledge) has not been used else-
where, and because it needs to be modified for our
purposes, we show the version of the algorithm we
use in Figure 1.
</bodyText>
<subsectionHeader confidence="0.991793">
4.2 Method
</subsectionHeader>
<bodyText confidence="0.999993166666667">
The purpose of this is experiment is not only to im-
prove parsing results, but also to investigate the over-
all effect of smoothing on parse accuracy. Therefore,
we do not simply report results with the best model
from Section 3. Rather, we re-do each modification
in Section 3 with both search strategies (Viterbi and
beam) in the unsmoothed case, and with all three
smoothing algorithms with beam search. The beam
has a variable width, which means an arbitrary num-
ber of edges may be considered, as long as their
probability is within 4 10 3 of the best edge in a
given span.
</bodyText>
<subsectionHeader confidence="0.7859">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.8274535">
Table 3 summarizes the results. The best result in
each column is italicized, and the overall best result
</bodyText>
<table confidence="0.961171">
P BiA Bi 1 Bi 2
λ1P BiA Bi 1 Bi 2
λ2P BiA Bi 1 λ3P BiA λ4P Bi
318
No Smoothing No Smoothing Brants Kneser-Ney Witten-Bell
Viterbi Beam Beam Beam Beam
GF Baseline 69.1 70.3 72.3 72.6 72.3
+Coord GF 71.5 72.7 75.2 75.4 74.5
+NP case 72.4 73.3 76.0 76.1 75.6
+PP case 72.7 73.2 76.1 76.2 75.7
+SBAR 72.6 73.1 76.3 76.0 75.3
+S GF Removal 73.1 72.6 75.7 75.3 75.1
</table>
<tableCaption confidence="0.999911">
Table 3: Effect of various smoothing algorithms.
</tableCaption>
<bodyText confidence="0.998601090909091">
in shown in bold. The column titled Viterbi repro-
duces the second column of Table 2 whereas the col-
umn titled Beam shows the result of re-annotation
using beam search, but no smoothing. The best re-
sult with beam search is 73.3, slightly higher than
without beam search.
Among smoothing algorithms, the Brants ap-
proach yields the highest results, of 76.3, with the
modified Kneser-Ney algorithm close behind, at
76.2. The modified Witten-Bell algorithm achieved
an F-score of 75.7.
</bodyText>
<subsectionHeader confidence="0.983414">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999935275862069">
Overall, the best-performing model, using Brants
smoothing, achieves a labelled bracketing F-score
of 76.2, higher than earlier results reported by Dubey
and Keller (2003) and Schiehlen (2004).
It is surprisingly that the Brants algorithm per-
forms favourably compared to the better-known
modified Kneser-Ney algorithm. This might be due
to the heritage of the two algorithms. Kneser-Ney
smoothing was designed for language modelling,
where there are tens of thousands or hundreds of
thousands of tokens having a Zipfian distribution.
With all transformations included, the nonterminals
of our grammar did have a Zipfian marginal distri-
bution, but there were only several hundred tokens.
The Brants algorithm was specifically designed for
distributions with fewer tokens.
Also surprising is the fact that each smoothing al-
gorithm reacted differently to the various treebank
transformations. It is obvious that the choice of
search and smoothing algorithm add bias to the final
result. However, our results indicate that the choice
of search and smoothing algorithm also add a degree
of variance as improvements are added to the parser.
This is worrying: at times in the literature, details
of search or smoothing are left out (e.g. Charniak
(2000)). Given the degree of variance due to search
and smoothing, it raises the question if it is in fact
possible to reproduce such results without the nec-
essary details.2
</bodyText>
<sectionHeader confidence="0.998657" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.999948962962963">
While it is uncommon to offer an error analysis for
probabilistic parsing, Levy and Manning (2003) ar-
gue that a careful error classification can reveal pos-
sible improvements. Although we leave the imple-
mentation of any improvements to future research,
we do discuss several common errors. Because the
parser with Brants smoothing performed best, we
use that as the basis of our error analysis.
First, we found that POS tagging errors had a
strong effect on parsing results. This is surpris-
ing, given that the parser is able to assign POS tags
with a high degree of accuracy. POS tagging results
are comparable to the best stand-alone POS taggers,
achieving results of 97.1% on the test set, match-
ing the performance of the POS tagger described
by Brants (2000) When GF labels are included (e.g.
considering ART-SB instead of just ART), tagging
accuracy falls to 90.1%. To quantify the effect of
POS tagging errors, we re-parsed with correct POS
tags (rather than letting the parser guess the tags),
and found that labelled bracket F-scores increase
from 76.3 to 85.2. A manual inspection of 100 sen-
tences found that GF mislabelling can accounts for
at most two-thirds of the mistakes due to POS tags.
Over one third was due to genuine POS tagging er-
rors. The most common problem was verb mistag-
ging: they are either confused with adjectives (both
</bodyText>
<footnote confidence="0.991403">
2As an anonymous reviewer pointed out, it is not always
straightforward to reproduce statistical parsing results even
when the implementation details are given (Bikel, 2004).
</footnote>
<page confidence="0.995485">
319
</page>
<table confidence="0.98239775">
Model LB F-score
This paper 76.3
Dubey and Keller (2003) 74.1
Schiehlen (2004) 71.1
</table>
<tableCaption confidence="0.999916">
Table 4: Comparison with previous work.
</tableCaption>
<bodyText confidence="0.999948181818182">
take the common -en suffix), or the tense was incor-
rect. Mistagged verb are a serious problem: it entails
an entire clause is parsed incorrectly. Verb mistag-
ging is also a problem for other languages: Levy and
Manning (2003) describe a similar problem in Chi-
nese for noun/verb ambiguity. This problem might
be alleviated by using a more detailed model of mor-
phology than our suffix analyzer provides.
To investigate pure parsing errors, we manu-
ally examined 100 sentences which were incorrectly
parsed, but which nevertheless were assigned the
correct POS tags. Incorrect modifier attachment ac-
counted for for 39% of all parsing errors (of which
77% are due to PP attachment alone). Misparsed co-
ordination was the second most common problem,
accounting for 15% of all mistakes. Another class
of error appears to be due to Markovization. The
boundaries of VPs are sometimes incorrect, with the
parser attaching dependents directly to the S node
rather than the VP. In the most extreme cases, the
VP had no verb, with the main verb heading a sub-
ordinate clause.
</bodyText>
<sectionHeader confidence="0.960511" genericHeader="method">
6 Comparison with Previous Work
</sectionHeader>
<bodyText confidence="0.999946371428572">
Table 4 lists the result of the best model presented
here against the earlier work on NEGRA parsing de-
scribed in Dubey and Keller (2003) and Schiehlen
(2004). Dubey and Keller use a variant of the lex-
icalized Collins (1999) model to achieve a labelled
bracketing F-score of 74.1%. Schiehlen presents a
number of unlexicalized models. The best model on
labelled bracketing achieves an F-score of 71.8%.
The work of Schiehlen is particularly interest-
ing as he also considers a number of transforma-
tions to improve the performance of an unlexicalized
parser. Unlike the work presented here, Schiehlen
does not attempt to perform any suffix or morpho-
logical analysis of the input text. However, he does
suggest a number of treebank transformations. One
such transformation is similar to one we prosed here,
the NP case transformation. His implementation is
different from ours: he annotates the case of pro-
nouns and common nouns, whereas we focus on ar-
ticles and pronouns (articles are pronouns are more
strongly marked for case than common nouns). The
remaining transformations we present are different
from those Schiehlen describes; it is possible that an
even better parser may result if all the transforma-
tions were combined.
Schiehlen also makes use of a morphological ana-
lyzer tool. While this includes more complete infor-
mation about German morphology, our suffix analy-
sis model allows us to integrate morphological am-
biguities into the parsing system by means of lexical
generation probabilities.
Levy and Manning (2004) also present work on
the NEGRA treebank, but are primarily interested
in long-distance dependencies, and therefore do not
report results on local dependencies, as we do here.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999981192307692">
In this paper, we presented the best-performing
parser for German, as measured by labelled bracket
scores. The high performance was due to three fac-
tors: (i) treebank transformations (ii) an integrated
model of morphology in the form of a suffix ana-
lyzer and (iii) the use of smoothing in an unlexical-
ized grammar. Moreover, there are possible paths
for improvement: lexicalization could be added to
the model, as could some of the treebank transfor-
mations suggested by Schiehlen (2004). Indeed, the
suffix analyzer could well be of value in a lexicalized
model.
While we only presented results on the German
NEGRA corpus, there is reason to believe that the
techniques we presented here are also important to
other languages where lexicalization provides lit-
tle benefit: smoothing is a broadly-applicable tech-
nique, and if difficulties with lexicalization are due
to sparse lexical data, then suffix analysis provides
a useful way to get more information from lexical
elements which were unseen while training.
In addition to our primary results, we also pro-
vided a detailed error analysis which shows that
PP attachment and co-ordination are problematic
for our parser. Furthermore, while POS tagging is
highly accurate, the error analysis also shows it does
</bodyText>
<page confidence="0.988559">
320
</page>
<bodyText confidence="0.9998708">
have surprisingly large effect on parsing errors. Be-
cause of the strong impact of POS tagging on pars-
ing results, we conjecture that increasing POS tag-
ging accuracy may be another fruitful area for future
parsing research.
</bodyText>
<sectionHeader confidence="0.999178" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856425">
Franz Beil, Glenn Carroll, Detlef Prescher, Stefan Rie-
zler, and Mats Rooth. 1999. Inside-Outside Estima-
tion of a Lexicalized PCFG for German. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, University of Maryland,
College Park.
Daniel M. Bikel. 2004. Intricacies of Collins’ Parsing
Model. Computational Linguistics, 30(4).
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the 1st
Conference of the North American Chapter of the ACL
(NAACL), Seattle, Washington., pages 234–240.
Rens Bod. 1995. Enriching Linguistics with Statistics:
Performance Models ofNatural Language. Ph.D. the-
sis, University of Amsterdam.
Taylor L. Booth. 1969. Probabilistic Representation of
Formal Languages. In Tenth Annual IEEE Symposium
on Switching and Automata Theory, pages 74–81.
Thorsten Brants. 2000. TnT: A statistical part-of-speech
tagger. In Proceedings of the 6th Conference on Ap-
plied Natural Language Processing, Seattle.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the 1st Conference ofNorth
American Chapter of the Association for Computa-
tional Linguistics, pages 132–139, Seattle, WA.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Center for Research
in Computing Technology, Harvard University.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Amit Dubey and Frank Keller. 2003. Parsing German
with Sister-head Dependencies. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 96–103, Sapporo, Japan.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan
Sag. 1985. Generalized Phase Structure Grammar.
Basil Blackwell, Oxford, England.
Joshua Goodman. 1998. Parsing inside-out. Ph.D. the-
sis, Harvard University.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613–632.
Dan Klein and Christopher D. Manning. 2002. A* Pars-
ing: Fast Exact Viterbi Parse Selection. Technical Re-
port dbpubs/2002-16, Stanford University.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan.
Roger Levy and Christopher D. Manning. 2003. Is it
Harder to Parse Chinese, or the Chinese Treebank? In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics.
Roger Levy and Christopher D. Manning. 2004. Deep
Dependencies from Context-Free Statistical Parsers:
Correcting the Surface Dependency Approximation.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics.
David M. Magerman. 1995. Statistical Decision-Tree
Models for Parsing. In Proceedings ofthe 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 276–283, Cambridge, MA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Micheal Schiehlen. 2004. Annotation Strategies for
Probabilistic Parsing in German. In Proceedings of
the 20th International Conference on Computational
Linguistics.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In Proceedings of the 5th
Conference on Applied Natural Language Processing,
Washington, DC.
Hans Uszkoreit. 1987. Word Order and Constituent
Structure in German. CSLI Publications, Stanford,
CA.
</reference>
<page confidence="0.998757">
321
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898725">
<title confidence="0.9874895">What to do when lexicalization fails: parsing German with suffix analysis and smoothing</title>
<author confidence="0.994965">Amit Dubey</author>
<affiliation confidence="0.999952">University of Edinburgh</affiliation>
<email confidence="0.937897">Amit.Dubey@ed.ac.uk</email>
<abstract confidence="0.998669909090909">In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve labelled bracket of 76.2, higher than previously reported results on the NEGRA corpus. In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Franz Beil</author>
<author>Glenn Carroll</author>
<author>Detlef Prescher</author>
<author>Stefan Riezler</author>
<author>Mats Rooth</author>
</authors>
<title>Inside-Outside Estimation of a Lexicalized PCFG for German.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association</booktitle>
<institution>for Computational Linguistics, University of Maryland, College Park.</institution>
<contexts>
<context position="720" citStr="Beil et al., 1999" startWordPosition="108" endWordPosition="111">versity of Edinburgh Amit.Dubey@ed.ac.uk Abstract In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2, higher than previously reported results on the NEGRA corpus. In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results. 1 Introduction Recent research on German statistical parsing has shown that lexicalization adds little to parsing performance in German (Dubey and Keller, 2003; Beil et al., 1999). A likely cause is the relative productivity of German morphology compared to that of English: German has a higher type/token ratio for words, making sparse data problems more severe. There are at least two solutions to this problem: first, to use better models of morphology or, second, to make unlexicalized parsing more accurate. We investigate both approaches in this paper. In particular, we develop a parser for German which attains the highest performance known to us by making use of smoothing and a highly-tuned suffix analyzer for guessing part-of-speech (POS) tags from the input text. Ra</context>
</contexts>
<marker>Beil, Carroll, Prescher, Riezler, Rooth, 1999</marker>
<rawString>Franz Beil, Glenn Carroll, Detlef Prescher, Stefan Riezler, and Mats Rooth. 1999. Inside-Outside Estimation of a Lexicalized PCFG for German. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ Parsing Model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="22292" citStr="Bikel, 2004" startWordPosition="3804" endWordPosition="3805">rrors, we re-parsed with correct POS tags (rather than letting the parser guess the tags), and found that labelled bracket F-scores increase from 76.3 to 85.2. A manual inspection of 100 sentences found that GF mislabelling can accounts for at most two-thirds of the mistakes due to POS tags. Over one third was due to genuine POS tagging errors. The most common problem was verb mistagging: they are either confused with adjectives (both 2As an anonymous reviewer pointed out, it is not always straightforward to reproduce statistical parsing results even when the implementation details are given (Bikel, 2004). 319 Model LB F-score This paper 76.3 Dubey and Keller (2003) 74.1 Schiehlen (2004) 71.1 Table 4: Comparison with previous work. take the common -en suffix), or the tense was incorrect. Mistagged verb are a serious problem: it entails an entire clause is parsed incorrectly. Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. This problem might be alleviated by using a more detailed model of morphology than our suffix analyzer provides. To investigate pure parsing errors, we manually examined 100 sentence</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ Parsing Model. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Blaheta</author>
<author>Eugene Charniak</author>
</authors>
<title>Assigning function tags to parsed text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the ACL (NAACL),</booktitle>
<pages>234--240</pages>
<location>Seattle, Washington.,</location>
<contexts>
<context position="12134" citStr="Blaheta and Charniak (2000)" startWordPosition="2047" endWordPosition="2050">mation does not add any extra nonterminals, rather it replaces rules such as S KOUS NP V NP (where KOUS is a complementizer POS tag) with SBAR KOUS NP V NP. S GF One may argue that, as far as syntactic disambiguation is concerned, GFs on S categories primarily serve to distinguish main clauses from subordinate clauses. As we have explicitly done this in the previous transformation, it stands to reason that the GF tags on S nodes may therefore be removed without penalty. If the tags are necessary for semantic interpretation, presumably they could be re-inserted using a strategy such as that of Blaheta and Charniak (2000) The last transformation therefore removes the GF of S nodes. 3.3 Method To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). The first 18,602 sentences are used as training data, the following 1,000 form the development set, and the last 1,000 are used as the test set. We remove long-distance dependencies from all sets, and only consider sentences of length 40 or less for efficiency and memory concerns. The parser is given untagged words as input to simulate a realistic parsing task. A prob</context>
</contexts>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>Don Blaheta and Eugene Charniak. 2000. Assigning function tags to parsed text. In Proceedings of the 1st Conference of the North American Chapter of the ACL (NAACL), Seattle, Washington., pages 234–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Enriching Linguistics with Statistics: Performance Models ofNatural Language.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="15664" citStr="Bod, 1995" startWordPosition="2688" endWordPosition="2689">the SBAR transforma317 λ1 λ2 λ3 0 for each trigram x1x2x3 with cx1x2x3 0 cxixi 1xi 2 1 if c xi 1 xi 2 1 c xi 1 xi 2 1 0 if c xi 1 xi 2 1 cxixi 1 1 if c xi 1 1 c xi 1 1 0 if c xi 1 1 cxi 1 d1 N 1 if d3 max d1 d2 d3 then λ3 λ3 c xi xi 1 xi 2 elseif d2 max d1 d2 d3 then λ2 λ2 c xi xi 1 xi 2 else λ1 λ1 c xi xi 1 xi 2 d3 d2 tion is already in place. To test if this was indeed the case, we re-ran the final experiment, but excluded the SBAR transformation. We did indeed find that applying S GF without the SBAR transformation reduced performance. 4 Smoothing &amp; Search With the exception of DOP models (Bod, 1995), it is uncommon to smooth unlexicalized grammars. This is in part for the sake of simplicity: unlexicalized grammars are interesting because they are simple to estimate and parse, and adding smoothing makes both estimation and parsing nearly as complex as with fully lexicalized models. However, because lexicalization adds little to the performance of German parsing models, it is therefore interesting to investigate the impact of smoothing on unlexicalized parsing models for German. Parsing an unsmoothed unlexicalized grammar is relatively efficient because the grammar constraints the search s</context>
</contexts>
<marker>Bod, 1995</marker>
<rawString>Rens Bod. 1995. Enriching Linguistics with Statistics: Performance Models ofNatural Language. Ph.D. thesis, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor L Booth</author>
</authors>
<title>Probabilistic Representation of Formal Languages.</title>
<date>1969</date>
<booktitle>In Tenth Annual IEEE Symposium on Switching and Automata Theory,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="5182" citStr="Booth (1969)" startWordPosition="842" endWordPosition="843">the same parent. To maximize cases where all necessary nodes are dominated by the same parent, NEGRA uses flat ‘dependency-style’ rules. For example, there is no VP node when there is no overt auxiliary verb. category. Under the NEGRA annotation scheme, the first sentence above would have a rule S NP-SB VVFIN NP-OA and the second, S NP-OA VVFIN NP-SB, where SB denotes subject and OA denotes accusative object. 3 Parsing with Grammatical Functions 3.1 Model As explained above, this paper focuses on unlexicalized grammars. In particular, we make use of probabilistic context-free grammars (PCFGs; Booth (1969)) for our experiments. A PCFG assigns each context-free rule LHS RHS a conditional probability Pr RHSLHS . If a parser were to be given POS tags as input, this would be the only distribution &apos;Pragmatically speaking, the second sentence has a slightly different meaning. A better translation might be: ‘It is the boy the man sees.’ required. However, in this paper we are concerned with the more realistic problem of accepting text as input. Therefore, the parser also needs a probability distribution Pw wLHS to generate words. The probability of a tree is calculated by multiplying the probabilities</context>
</contexts>
<marker>Booth, 1969</marker>
<rawString>Taylor L. Booth. 1969. Probabilistic Representation of Formal Languages. In Tenth Annual IEEE Symposium on Switching and Automata Theory, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT: A statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing,</booktitle>
<location>Seattle.</location>
<contexts>
<context position="8847" citStr="Brants (2000)" startWordPosition="1482" endWordPosition="1483">on and unknown words; those which occur often are handled as normal. suggested different choices for Pw in the face of unknown words: Schiehlen (2004) suggests using a different unknown word token for capitalized versus uncapitalized unknown words (German orthography dictates that all common nouns are capitalized) and Levy and Manning (2004) consider inspecting the last letter the unknown word to guess the part-of-speech (POS) tags. Both of these models are relatively impoverished when compared to the approaches of handling unknown words which have been proposed in the POS tagging literature. Brants (2000) describes a POS tagger with a highly tuned suffix analyzer which considers both capitalization and suffixes as long as 10 letters long. This tagger was developed with German in mind, but neither it nor any other advanced POS tagger morphology analyzer has ever been tested with a full parser. Therefore, we take the novel step of integrating this suffix analyzer into the parser for the second Pw distribution. 3.2 Treebank Re-annotation Automatic treebank transformations are an important step in developing an accurate unlexicalized parser (Johnson, 1998; Klein and Manning, 2003). Most of our tra</context>
<context position="16897" citStr="Brants (2000)" startWordPosition="2883" endWordPosition="2884">grammar does not have a constrained search space, it is necessary to find other means to make parsing faster. Although it is possible to efficiently compute the Viterbi parse (Klein and Manning, 2002) using a smoothed grammar, the most common approach to increase parsing speed is to use some form of beam search (cf. Goodman (1998)), a strategy we follow here. 4.1 Models We experiment with three different smoothing models: the modified Witten-Bell algorithm employed by Collins (1999), the modified Kneser-Ney algorithm of Chen and Goodman (1998) the smoothing algorithm used in the POS tagger of Brants (2000). All are variants of linear interpolation, and are used with 2nd order Markovization. Under this regime, the probability of adding the ith child to A B1 Bn is estimated as The models differ in how the λ’s are estimated. For both the Witten-Bell and Kneser-Ney algorithms, the λ’s are a function of the context A Bi 2 Bi 1. By contrast, in Brants’ algorithm the λ’s are constant end λ1 λ1 λ1 λ2 λ 3 λ2 λ2 λ1 λ2 λ 3 λ3 λ3 λ1 λ2 λ 3 Figure 1: Smoothing estimation based on the Brants (2000) approach for POS tagging. for all possible contexts. As both the Witten-Bell and Kneser-Ney variants are fairly</context>
<context position="21532" citStr="Brants (2000)" startWordPosition="3679" endWordPosition="3680">veal possible improvements. Although we leave the implementation of any improvements to future research, we do discuss several common errors. Because the parser with Brants smoothing performed best, we use that as the basis of our error analysis. First, we found that POS tagging errors had a strong effect on parsing results. This is surprising, given that the parser is able to assign POS tags with a high degree of accuracy. POS tagging results are comparable to the best stand-alone POS taggers, achieving results of 97.1% on the test set, matching the performance of the POS tagger described by Brants (2000) When GF labels are included (e.g. considering ART-SB instead of just ART), tagging accuracy falls to 90.1%. To quantify the effect of POS tagging errors, we re-parsed with correct POS tags (rather than letting the parser guess the tags), and found that labelled bracket F-scores increase from 76.3 to 85.2. A manual inspection of 100 sentences found that GF mislabelling can accounts for at most two-thirds of the mistakes due to POS tags. Over one third was due to genuine POS tagging errors. The most common problem was verb mistagging: they are either confused with adjectives (both 2As an anonym</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT: A statistical part-of-speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference ofNorth American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="6614" citStr="Charniak, 2000" startWordPosition="1087" endWordPosition="1089">RHS Pr RHSLHS (1) c LHS The probabilities of words given tags are similarly estimated from the frequency of word-tag cooccurrences: (2) c LHS To handle unseen or infrequent words, all words whose frequency falls below a threshold Ω are grouped together in an ‘unknown word’ token, which is then treated like an additional word. For our experiments, we use Ω 10. We consider several variations of this simple model by changing both Pr and Pw. In addition to the standard formulation in Equation (1), we consider two alternative variants of Pr. The first is a Markov context-free rule (Magerman, 1995; Charniak, 2000). A rule may be turned into a Markov rule by first binarizing it, then making independence assumptions on the new binarized rules. Binarizing the rule A B1 Bn results in a number of smaller rules A B1AB1, AB1 B2AB1B2, , AB1Bn 1 Bn. Binarization does not change the probability of the rule: P BiA B1 Bi 1 Making the 2nd order Markov assumption ‘forgets’ everything earlier then 2 previous sisters. A rule would now be in the form ABi 2Bi 1 BiABi 1Bi, and the probability would be: Pw wLHS c LHS w P B1 BnA i 1 ∏ n P B1 BnA i 1 ∏ n P BiA Bi 2 Bi 1 315 The other rule type we consider are linear precede</context>
<context position="12134" citStr="Charniak (2000)" startWordPosition="2049" endWordPosition="2050">not add any extra nonterminals, rather it replaces rules such as S KOUS NP V NP (where KOUS is a complementizer POS tag) with SBAR KOUS NP V NP. S GF One may argue that, as far as syntactic disambiguation is concerned, GFs on S categories primarily serve to distinguish main clauses from subordinate clauses. As we have explicitly done this in the previous transformation, it stands to reason that the GF tags on S nodes may therefore be removed without penalty. If the tags are necessary for semantic interpretation, presumably they could be re-inserted using a strategy such as that of Blaheta and Charniak (2000) The last transformation therefore removes the GF of S nodes. 3.3 Method To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). The first 18,602 sentences are used as training data, the following 1,000 form the development set, and the last 1,000 are used as the test set. We remove long-distance dependencies from all sets, and only consider sentences of length 40 or less for efficiency and memory concerns. The parser is given untagged words as input to simulate a realistic parsing task. A prob</context>
<context position="20587" citStr="Charniak (2000)" startWordPosition="3518" endWordPosition="3519">stribution, but there were only several hundred tokens. The Brants algorithm was specifically designed for distributions with fewer tokens. Also surprising is the fact that each smoothing algorithm reacted differently to the various treebank transformations. It is obvious that the choice of search and smoothing algorithm add bias to the final result. However, our results indicate that the choice of search and smoothing algorithm also add a degree of variance as improvements are added to the parser. This is worrying: at times in the literature, details of search or smoothing are left out (e.g. Charniak (2000)). Given the degree of variance due to search and smoothing, it raises the question if it is in fact possible to reproduce such results without the necessary details.2 5 Error Analysis While it is uncommon to offer an error analysis for probabilistic parsing, Levy and Manning (2003) argue that a careful error classification can reveal possible improvements. Although we leave the implementation of any improvements to future research, we do discuss several common errors. Because the parser with Brants smoothing performed best, we use that as the basis of our error analysis. First, we found that </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the 1st Conference ofNorth American Chapter of the Association for Computational Linguistics, pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Center for Research in Computing Technology, Harvard University.</institution>
<contexts>
<context position="16833" citStr="Chen and Goodman (1998)" startWordPosition="2869" endWordPosition="2872">efficient because the grammar constraints the search space. As a smoothed grammar does not have a constrained search space, it is necessary to find other means to make parsing faster. Although it is possible to efficiently compute the Viterbi parse (Klein and Manning, 2002) using a smoothed grammar, the most common approach to increase parsing speed is to use some form of beam search (cf. Goodman (1998)), a strategy we follow here. 4.1 Models We experiment with three different smoothing models: the modified Witten-Bell algorithm employed by Collins (1999), the modified Kneser-Ney algorithm of Chen and Goodman (1998) the smoothing algorithm used in the POS tagger of Brants (2000). All are variants of linear interpolation, and are used with 2nd order Markovization. Under this regime, the probability of adding the ith child to A B1 Bn is estimated as The models differ in how the λ’s are estimated. For both the Witten-Bell and Kneser-Ney algorithms, the λ’s are a function of the context A Bi 2 Bi 1. By contrast, in Brants’ algorithm the λ’s are constant end λ1 λ1 λ1 λ2 λ 3 λ2 λ2 λ1 λ2 λ 3 λ3 λ3 λ1 λ2 λ 3 Figure 1: Smoothing estimation based on the Brants (2000) approach for POS tagging. for all possible cont</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Center for Research in Computing Technology, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="16771" citStr="Collins (1999)" startWordPosition="2861" endWordPosition="2862">ng an unsmoothed unlexicalized grammar is relatively efficient because the grammar constraints the search space. As a smoothed grammar does not have a constrained search space, it is necessary to find other means to make parsing faster. Although it is possible to efficiently compute the Viterbi parse (Klein and Manning, 2002) using a smoothed grammar, the most common approach to increase parsing speed is to use some form of beam search (cf. Goodman (1998)), a strategy we follow here. 4.1 Models We experiment with three different smoothing models: the modified Witten-Bell algorithm employed by Collins (1999), the modified Kneser-Ney algorithm of Chen and Goodman (1998) the smoothing algorithm used in the POS tagger of Brants (2000). All are variants of linear interpolation, and are used with 2nd order Markovization. Under this regime, the probability of adding the ith child to A B1 Bn is estimated as The models differ in how the λ’s are estimated. For both the Witten-Bell and Kneser-Ney algorithms, the λ’s are a function of the context A Bi 2 Bi 1. By contrast, in Brants’ algorithm the λ’s are constant end λ1 λ1 λ1 λ2 λ 3 λ2 λ2 λ1 λ2 λ 3 λ3 λ3 λ1 λ2 λ 3 Figure 1: Smoothing estimation based on the</context>
<context position="23737" citStr="Collins (1999)" startWordPosition="4048" endWordPosition="4049">tion was the second most common problem, accounting for 15% of all mistakes. Another class of error appears to be due to Markovization. The boundaries of VPs are sometimes incorrect, with the parser attaching dependents directly to the S node rather than the VP. In the most extreme cases, the VP had no verb, with the main verb heading a subordinate clause. 6 Comparison with Previous Work Table 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). Dubey and Keller use a variant of the lexicalized Collins (1999) model to achieve a labelled bracketing F-score of 74.1%. Schiehlen presents a number of unlexicalized models. The best model on labelled bracketing achieves an F-score of 71.8%. The work of Schiehlen is particularly interesting as he also considers a number of transformations to improve the performance of an unlexicalized parser. Unlike the work presented here, Schiehlen does not attempt to perform any suffix or morphological analysis of the input text. However, he does suggest a number of treebank transformations. One such transformation is similar to one we prosed here, the NP case transfor</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
<author>Frank Keller</author>
</authors>
<title>Parsing German with Sister-head Dependencies.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>96--103</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="700" citStr="Dubey and Keller, 2003" startWordPosition="104" endWordPosition="107">smoothing Amit Dubey University of Edinburgh Amit.Dubey@ed.ac.uk Abstract In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2, higher than previously reported results on the NEGRA corpus. In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results. 1 Introduction Recent research on German statistical parsing has shown that lexicalization adds little to parsing performance in German (Dubey and Keller, 2003; Beil et al., 1999). A likely cause is the relative productivity of German morphology compared to that of English: German has a higher type/token ratio for words, making sparse data problems more severe. There are at least two solutions to this problem: first, to use better models of morphology or, second, to make unlexicalized parsing more accurate. We investigate both approaches in this paper. In particular, we develop a parser for German which attains the highest performance known to us by making use of smoothing and a highly-tuned suffix analyzer for guessing part-of-speech (POS) tags fro</context>
<context position="12362" citStr="Dubey and Keller (2003)" startWordPosition="2088" endWordPosition="2091">Fs on S categories primarily serve to distinguish main clauses from subordinate clauses. As we have explicitly done this in the previous transformation, it stands to reason that the GF tags on S nodes may therefore be removed without penalty. If the tags are necessary for semantic interpretation, presumably they could be re-inserted using a strategy such as that of Blaheta and Charniak (2000) The last transformation therefore removes the GF of S nodes. 3.3 Method To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). The first 18,602 sentences are used as training data, the following 1,000 form the development set, and the last 1,000 are used as the test set. We remove long-distance dependencies from all sets, and only consider sentences of length 40 or less for efficiency and memory concerns. The parser is given untagged words as input to simulate a realistic parsing task. A probabilistic CYK parsing algorithm is used to compute the Viterbi parse. We perform two sets of experiments. In the first set, we vary the rule type, and in the second, we report the additive results of the treebank reannotations d</context>
<context position="19512" citStr="Dubey and Keller (2003)" startWordPosition="3350" endWordPosition="3353">reproduces the second column of Table 2 whereas the column titled Beam shows the result of re-annotation using beam search, but no smoothing. The best result with beam search is 73.3, slightly higher than without beam search. Among smoothing algorithms, the Brants approach yields the highest results, of 76.3, with the modified Kneser-Ney algorithm close behind, at 76.2. The modified Witten-Bell algorithm achieved an F-score of 75.7. 4.4 Discussion Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). It is surprisingly that the Brants algorithm performs favourably compared to the better-known modified Kneser-Ney algorithm. This might be due to the heritage of the two algorithms. Kneser-Ney smoothing was designed for language modelling, where there are tens of thousands or hundreds of thousands of tokens having a Zipfian distribution. With all transformations included, the nonterminals of our grammar did have a Zipfian marginal distribution, but there were only several hundred tokens. The Brants algorithm was specifically designed for distributions with fewer tokens. </context>
<context position="22354" citStr="Dubey and Keller (2003)" startWordPosition="3813" endWordPosition="3816">han letting the parser guess the tags), and found that labelled bracket F-scores increase from 76.3 to 85.2. A manual inspection of 100 sentences found that GF mislabelling can accounts for at most two-thirds of the mistakes due to POS tags. Over one third was due to genuine POS tagging errors. The most common problem was verb mistagging: they are either confused with adjectives (both 2As an anonymous reviewer pointed out, it is not always straightforward to reproduce statistical parsing results even when the implementation details are given (Bikel, 2004). 319 Model LB F-score This paper 76.3 Dubey and Keller (2003) 74.1 Schiehlen (2004) 71.1 Table 4: Comparison with previous work. take the common -en suffix), or the tense was incorrect. Mistagged verb are a serious problem: it entails an entire clause is parsed incorrectly. Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. This problem might be alleviated by using a more detailed model of morphology than our suffix analyzer provides. To investigate pure parsing errors, we manually examined 100 sentences which were incorrectly parsed, but which nevertheless were a</context>
<context position="23650" citStr="Dubey and Keller (2003)" startWordPosition="4031" endWordPosition="4034"> for 39% of all parsing errors (of which 77% are due to PP attachment alone). Misparsed coordination was the second most common problem, accounting for 15% of all mistakes. Another class of error appears to be due to Markovization. The boundaries of VPs are sometimes incorrect, with the parser attaching dependents directly to the S node rather than the VP. In the most extreme cases, the VP had no verb, with the main verb heading a subordinate clause. 6 Comparison with Previous Work Table 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). Dubey and Keller use a variant of the lexicalized Collins (1999) model to achieve a labelled bracketing F-score of 74.1%. Schiehlen presents a number of unlexicalized models. The best model on labelled bracketing achieves an F-score of 71.8%. The work of Schiehlen is particularly interesting as he also considers a number of transformations to improve the performance of an unlexicalized parser. Unlike the work presented here, Schiehlen does not attempt to perform any suffix or morphological analysis of the input text. However, he does suggest a number of treebank transfor</context>
</contexts>
<marker>Dubey, Keller, 2003</marker>
<rawString>Amit Dubey and Frank Keller. 2003. Parsing German with Sister-head Dependencies. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 96–103, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phase Structure Grammar. Basil Blackwell,</title>
<date>1985</date>
<location>Oxford, England.</location>
<contexts>
<context position="7273" citStr="Gazdar et al., 1985" startWordPosition="1217" endWordPosition="1220">le by first binarizing it, then making independence assumptions on the new binarized rules. Binarizing the rule A B1 Bn results in a number of smaller rules A B1AB1, AB1 B2AB1B2, , AB1Bn 1 Bn. Binarization does not change the probability of the rule: P BiA B1 Bi 1 Making the 2nd order Markov assumption ‘forgets’ everything earlier then 2 previous sisters. A rule would now be in the form ABi 2Bi 1 BiABi 1Bi, and the probability would be: Pw wLHS c LHS w P B1 BnA i 1 ∏ n P B1 BnA i 1 ∏ n P BiA Bi 2 Bi 1 315 The other rule type we consider are linear precedence/immediate dominance (LP/ID) rules (Gazdar et al., 1985). If a context-free rule can be thought of as a LHS token with an ordered list of tokens on the RHS, then an LP/ID rule can be thought of as a LHS token with a multiset of tokens on the RHS together with some constraints on the possible orders of tokens on the RHS. Uszkoreit (1987) argues that LP/ID rules with violatable ‘soft’ constraints are suitable for modelling some aspects of German word order. This makes a probabilistic formulation of LP/ID rules ideal: probabilities act as soft constraints. Our treatment of probabilistic LP/ID rules generate children one constituent at a time, conditio</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag. 1985. Generalized Phase Structure Grammar. Basil Blackwell, Oxford, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing inside-out.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="16616" citStr="Goodman (1998)" startWordPosition="2837" endWordPosition="2839"> performance of German parsing models, it is therefore interesting to investigate the impact of smoothing on unlexicalized parsing models for German. Parsing an unsmoothed unlexicalized grammar is relatively efficient because the grammar constraints the search space. As a smoothed grammar does not have a constrained search space, it is necessary to find other means to make parsing faster. Although it is possible to efficiently compute the Viterbi parse (Klein and Manning, 2002) using a smoothed grammar, the most common approach to increase parsing speed is to use some form of beam search (cf. Goodman (1998)), a strategy we follow here. 4.1 Models We experiment with three different smoothing models: the modified Witten-Bell algorithm employed by Collins (1999), the modified Kneser-Ney algorithm of Chen and Goodman (1998) the smoothing algorithm used in the POS tagger of Brants (2000). All are variants of linear interpolation, and are used with 2nd order Markovization. Under this regime, the probability of adding the ith child to A B1 Bn is estimated as The models differ in how the λ’s are estimated. For both the Witten-Bell and Kneser-Ney algorithms, the λ’s are a function of the context A Bi 2 B</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>Joshua Goodman. 1998. Parsing inside-out. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="1433" citStr="Johnson, 1998" startWordPosition="225" endWordPosition="226">an has a higher type/token ratio for words, making sparse data problems more severe. There are at least two solutions to this problem: first, to use better models of morphology or, second, to make unlexicalized parsing more accurate. We investigate both approaches in this paper. In particular, we develop a parser for German which attains the highest performance known to us by making use of smoothing and a highly-tuned suffix analyzer for guessing part-of-speech (POS) tags from the input text. Rather than relying on smoothing and suffix analysis alone, we also utilize treebank transformations (Johnson, 1998; Klein and Manning, 2003) instead of a grammar induced directly from a treebank. The organization of the paper is as follows: Section 2 summarizes some important aspects of our 314 treebank corpus. In Section 3 we outline several techniques for improving the performance of unlexicalized parsing without using smoothing, including treebank transformations, and the use of suffix analysis. We show that suffix analysis is not helpful on the treebank grammar, but it does increase performance if used in combination with the treebank transformations we present. Section 4 describes how smoothing can b</context>
<context position="9404" citStr="Johnson, 1998" startWordPosition="1572" endWordPosition="1573">en proposed in the POS tagging literature. Brants (2000) describes a POS tagger with a highly tuned suffix analyzer which considers both capitalization and suffixes as long as 10 letters long. This tagger was developed with German in mind, but neither it nor any other advanced POS tagger morphology analyzer has ever been tested with a full parser. Therefore, we take the novel step of integrating this suffix analyzer into the parser for the second Pw distribution. 3.2 Treebank Re-annotation Automatic treebank transformations are an important step in developing an accurate unlexicalized parser (Johnson, 1998; Klein and Manning, 2003). Most of our transformations focus upon one part of the NEGRA treebank in particular: the GF labels. Below is a list of GF re-annotations we utilise: Coord GF In NEGRA, a co-ordinated accusative NP rule might look like NP-OA NP-CJ KON NPCJ. KON is the POS tag for a conjunct, and CJ denotes the function of the NP is a coordinate sister. Such a rule hides an important fact: the two co-ordinate sisters are also accusative objects. The Coord GF re-annotation would therefore replace the above rule with NP-OA NP-OA KON NP-OA. NP case German articles and pronouns are strong</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* Parsing: Fast Exact Viterbi Parse Selection.</title>
<date>2002</date>
<tech>Technical Report dbpubs/2002-16,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="16484" citStr="Klein and Manning, 2002" startWordPosition="2811" endWordPosition="2814">thing makes both estimation and parsing nearly as complex as with fully lexicalized models. However, because lexicalization adds little to the performance of German parsing models, it is therefore interesting to investigate the impact of smoothing on unlexicalized parsing models for German. Parsing an unsmoothed unlexicalized grammar is relatively efficient because the grammar constraints the search space. As a smoothed grammar does not have a constrained search space, it is necessary to find other means to make parsing faster. Although it is possible to efficiently compute the Viterbi parse (Klein and Manning, 2002) using a smoothed grammar, the most common approach to increase parsing speed is to use some form of beam search (cf. Goodman (1998)), a strategy we follow here. 4.1 Models We experiment with three different smoothing models: the modified Witten-Bell algorithm employed by Collins (1999), the modified Kneser-Ney algorithm of Chen and Goodman (1998) the smoothing algorithm used in the POS tagger of Brants (2000). All are variants of linear interpolation, and are used with 2nd order Markovization. Under this regime, the probability of adding the ith child to A B1 Bn is estimated as The models dif</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. A* Parsing: Fast Exact Viterbi Parse Selection. Technical Report dbpubs/2002-16, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1459" citStr="Klein and Manning, 2003" startWordPosition="227" endWordPosition="231"> type/token ratio for words, making sparse data problems more severe. There are at least two solutions to this problem: first, to use better models of morphology or, second, to make unlexicalized parsing more accurate. We investigate both approaches in this paper. In particular, we develop a parser for German which attains the highest performance known to us by making use of smoothing and a highly-tuned suffix analyzer for guessing part-of-speech (POS) tags from the input text. Rather than relying on smoothing and suffix analysis alone, we also utilize treebank transformations (Johnson, 1998; Klein and Manning, 2003) instead of a grammar induced directly from a treebank. The organization of the paper is as follows: Section 2 summarizes some important aspects of our 314 treebank corpus. In Section 3 we outline several techniques for improving the performance of unlexicalized parsing without using smoothing, including treebank transformations, and the use of suffix analysis. We show that suffix analysis is not helpful on the treebank grammar, but it does increase performance if used in combination with the treebank transformations we present. Section 4 describes how smoothing can be incorporated into an unl</context>
<context position="9430" citStr="Klein and Manning, 2003" startWordPosition="1574" endWordPosition="1577">the POS tagging literature. Brants (2000) describes a POS tagger with a highly tuned suffix analyzer which considers both capitalization and suffixes as long as 10 letters long. This tagger was developed with German in mind, but neither it nor any other advanced POS tagger morphology analyzer has ever been tested with a full parser. Therefore, we take the novel step of integrating this suffix analyzer into the parser for the second Pw distribution. 3.2 Treebank Re-annotation Automatic treebank transformations are an important step in developing an accurate unlexicalized parser (Johnson, 1998; Klein and Manning, 2003). Most of our transformations focus upon one part of the NEGRA treebank in particular: the GF labels. Below is a list of GF re-annotations we utilise: Coord GF In NEGRA, a co-ordinated accusative NP rule might look like NP-OA NP-CJ KON NPCJ. KON is the POS tag for a conjunct, and CJ denotes the function of the NP is a coordinate sister. Such a rule hides an important fact: the two co-ordinate sisters are also accusative objects. The Coord GF re-annotation would therefore replace the above rule with NP-OA NP-OA KON NP-OA. NP case German articles and pronouns are strongly marked for case. Howeve</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>Is it Harder to Parse Chinese, or the Chinese Treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="20870" citStr="Levy and Manning (2003)" startWordPosition="3564" endWordPosition="3567">at the choice of search and smoothing algorithm add bias to the final result. However, our results indicate that the choice of search and smoothing algorithm also add a degree of variance as improvements are added to the parser. This is worrying: at times in the literature, details of search or smoothing are left out (e.g. Charniak (2000)). Given the degree of variance due to search and smoothing, it raises the question if it is in fact possible to reproduce such results without the necessary details.2 5 Error Analysis While it is uncommon to offer an error analysis for probabilistic parsing, Levy and Manning (2003) argue that a careful error classification can reveal possible improvements. Although we leave the implementation of any improvements to future research, we do discuss several common errors. Because the parser with Brants smoothing performed best, we use that as the basis of our error analysis. First, we found that POS tagging errors had a strong effect on parsing results. This is surprising, given that the parser is able to assign POS tags with a high degree of accuracy. POS tagging results are comparable to the best stand-alone POS taggers, achieving results of 97.1% on the test set, matchin</context>
<context position="22646" citStr="Levy and Manning (2003)" startWordPosition="3862" endWordPosition="3865">rs. The most common problem was verb mistagging: they are either confused with adjectives (both 2As an anonymous reviewer pointed out, it is not always straightforward to reproduce statistical parsing results even when the implementation details are given (Bikel, 2004). 319 Model LB F-score This paper 76.3 Dubey and Keller (2003) 74.1 Schiehlen (2004) 71.1 Table 4: Comparison with previous work. take the common -en suffix), or the tense was incorrect. Mistagged verb are a serious problem: it entails an entire clause is parsed incorrectly. Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. This problem might be alleviated by using a more detailed model of morphology than our suffix analyzer provides. To investigate pure parsing errors, we manually examined 100 sentences which were incorrectly parsed, but which nevertheless were assigned the correct POS tags. Incorrect modifier attachment accounted for for 39% of all parsing errors (of which 77% are due to PP attachment alone). Misparsed coordination was the second most common problem, accounting for 15% of all mistakes. Another class of error appears to be due to Ma</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher D. Manning. 2003. Is it Harder to Parse Chinese, or the Chinese Treebank? In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>Deep Dependencies from Context-Free Statistical Parsers: Correcting the Surface Dependency Approximation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8577" citStr="Levy and Manning (2004)" startWordPosition="1438" endWordPosition="1441">he the probability of the rule is approximated as: In addition to the two additional formulations of the Pr distribution, we also consider one variant of the Pw distribution, which includes the suffix analysis. It is important to clarify that we only change the handling of uncommon and unknown words; those which occur often are handled as normal. suggested different choices for Pw in the face of unknown words: Schiehlen (2004) suggests using a different unknown word token for capitalized versus uncapitalized unknown words (German orthography dictates that all common nouns are capitalized) and Levy and Manning (2004) consider inspecting the last letter the unknown word to guess the part-of-speech (POS) tags. Both of these models are relatively impoverished when compared to the approaches of handling unknown words which have been proposed in the POS tagging literature. Brants (2000) describes a POS tagger with a highly tuned suffix analyzer which considers both capitalization and suffixes as long as 10 letters long. This tagger was developed with German in mind, but neither it nor any other advanced POS tagger morphology analyzer has ever been tested with a full parser. Therefore, we take the novel step of</context>
<context position="25039" citStr="Levy and Manning (2004)" startWordPosition="4252" endWordPosition="4255">nouns and common nouns, whereas we focus on articles and pronouns (articles are pronouns are more strongly marked for case than common nouns). The remaining transformations we present are different from those Schiehlen describes; it is possible that an even better parser may result if all the transformations were combined. Schiehlen also makes use of a morphological analyzer tool. While this includes more complete information about German morphology, our suffix analysis model allows us to integrate morphological ambiguities into the parsing system by means of lexical generation probabilities. Levy and Manning (2004) also present work on the NEGRA treebank, but are primarily interested in long-distance dependencies, and therefore do not report results on local dependencies, as we do here. 7 Conclusions In this paper, we presented the best-performing parser for German, as measured by labelled bracket scores. The high performance was due to three factors: (i) treebank transformations (ii) an integrated model of morphology in the form of a suffix analyzer and (iii) the use of smoothing in an unlexicalized grammar. Moreover, there are possible paths for improvement: lexicalization could be added to the model,</context>
</contexts>
<marker>Levy, Manning, 2004</marker>
<rawString>Roger Levy and Christopher D. Manning. 2004. Deep Dependencies from Context-Free Statistical Parsers: Correcting the Surface Dependency Approximation. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical Decision-Tree Models for Parsing.</title>
<date>1995</date>
<booktitle>In Proceedings ofthe 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="6597" citStr="Magerman, 1995" startWordPosition="1085" endWordPosition="1086">formally: c LHS RHS Pr RHSLHS (1) c LHS The probabilities of words given tags are similarly estimated from the frequency of word-tag cooccurrences: (2) c LHS To handle unseen or infrequent words, all words whose frequency falls below a threshold Ω are grouped together in an ‘unknown word’ token, which is then treated like an additional word. For our experiments, we use Ω 10. We consider several variations of this simple model by changing both Pr and Pw. In addition to the standard formulation in Equation (1), we consider two alternative variants of Pr. The first is a Markov context-free rule (Magerman, 1995; Charniak, 2000). A rule may be turned into a Markov rule by first binarizing it, then making independence assumptions on the new binarized rules. Binarizing the rule A B1 Bn results in a number of smaller rules A B1AB1, AB1 B2AB1B2, , AB1Bn 1 Bn. Binarization does not change the probability of the rule: P BiA B1 Bi 1 Making the 2nd order Markov assumption ‘forgets’ everything earlier then 2 previous sisters. A rule would now be in the form ABi 2Bi 1 BiABi 1Bi, and the probability would be: Pw wLHS c LHS w P B1 BnA i 1 ∏ n P B1 BnA i 1 ∏ n P BiA Bi 2 Bi 1 315 The other rule type we consider a</context>
<context position="13504" citStr="Magerman, 1995" startWordPosition="2292" endWordPosition="2293">he second, we report the additive results of the treebank reannotations described in Section 3.2. The three rule types used in the first set of experiments are standard CFG rules, our version of LP/ID rules, and 2nd order Markov CFG rules. The second battery of experiments was performed on the model with Markov rules. In both cases, we report PARSEVAL labeled No suffix With suffix F-score F-score GF Baseline 69.4 69.1 +Coord GF 70.2 71.5 +NP case 71.1 72.4 +PP case 71.0 72.7 +SBAR 70.9 72.6 +S GF 71.3 73.1 Table 2: Effect of re-annotation and suffix analysis with Markov rules. bracket scores (Magerman, 1995), with the brackets labeled by syntactic categories but not grammatical functions. Rather than reporting precision and recall of labelled brackets, we report only the F-score, i.e. the harmonic mean of precision and recall. 3.4 Results Table 1 shows the effect of rule type choice, and Table 2 lists the effect of the GF re-annotations. From Table 1, we see that Markov rules achieve the best performance, ahead of both standard rules as well as our formulation of probabilistic LP/ID rules. In the first group of experiments, suffix analysis marginally lowers performance. However, a different patte</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical Decision-Tree Models for Parsing. In Proceedings ofthe 33rd Annual Meeting of the Association for Computational Linguistics, pages 276–283, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2778" citStr="Marcus et al., 1993" startWordPosition="442" endWordPosition="445">thm, we use three different approaches, allowing us to compare the relative performance of each. An error analysis is presented in Section 5, which points to several possible areas of future research. We follow the error analysis with a comparison with related work in Section 6. Finally we offer concluding remarks in Section 7. 2 Data The parsing models we present are trained and tested on the NEGRA corpus (Skut et al., 1997), a handparsed corpus of German newspaper text containing approximately 20,000 sentences. It is available in several formats, and in this paper, we use the Penn Treebank (Marcus et al., 1993) format of NEGRA. The annotation used in NEGRA is similar to that used in the English Penn Treebank, with some differences which make it easier to annotate German syntax. German’s flexible word order would have required an explosion in long-distance dependencies (LDDs) had annotation of NEGRA more closely resembled that of the Penn Treebank. The NEGRA designers therefore chose to use relatively flat trees, encoding elements of flexible word order usProceedings of the 43rd Annual Meeting of the ACL, pages 314–321, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics ing gramma</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micheal Schiehlen</author>
</authors>
<title>Annotation Strategies for Probabilistic Parsing in German.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="8384" citStr="Schiehlen (2004)" startWordPosition="1410" endWordPosition="1411">traints. Our treatment of probabilistic LP/ID rules generate children one constituent at a time, conditioning upon the parent and a multiset of previously generated children. Formally, the the probability of the rule is approximated as: In addition to the two additional formulations of the Pr distribution, we also consider one variant of the Pw distribution, which includes the suffix analysis. It is important to clarify that we only change the handling of uncommon and unknown words; those which occur often are handled as normal. suggested different choices for Pw in the face of unknown words: Schiehlen (2004) suggests using a different unknown word token for capitalized versus uncapitalized unknown words (German orthography dictates that all common nouns are capitalized) and Levy and Manning (2004) consider inspecting the last letter the unknown word to guess the part-of-speech (POS) tags. Both of these models are relatively impoverished when compared to the approaches of handling unknown words which have been proposed in the POS tagging literature. Brants (2000) describes a POS tagger with a highly tuned suffix analyzer which considers both capitalization and suffixes as long as 10 letters long. </context>
<context position="10437" citStr="Schiehlen (2004)" startWordPosition="1753" endWordPosition="1754">nate sisters are also accusative objects. The Coord GF re-annotation would therefore replace the above rule with NP-OA NP-OA KON NP-OA. NP case German articles and pronouns are strongly marked for case. However, the grammatical function of all articles is usually NK, meaning noun kernel. To allow case markings in articles and pronouns to ‘communicate’ with the case labels on the GFs of NPs, we copy these GFs down into the POS tags of articles and pronouns. For example, a rule like NP-OA ART-NK NN-NK would be replaced by NP-OA ART-OA NN-NK. A similar improvement has been independently noted by Schiehlen (2004). PP case Prepositions determine the case of the NP they govern. While the case is often unambiguous (i.e. f¨ur ‘for’ always takes an accusative NP), at times the case may be ambiguous. For instance, in ‘in’ may take either an accusative or dative NP. We use the labels -OA, -OD, etc. for unambiguous prepositions, and introduce new categories AD (accusative/dative ambiguous) and DG (dative/genitive ambiguous) for the ambiguous categories. For example, a rule such as PP P ART-NK NN-NK is replaced with PP P-AD ART-AD NN-NK if it is headed by the preposition in. SBAR marking German subordinate cla</context>
<context position="19533" citStr="Schiehlen (2004)" startWordPosition="3355" endWordPosition="3356"> of Table 2 whereas the column titled Beam shows the result of re-annotation using beam search, but no smoothing. The best result with beam search is 73.3, slightly higher than without beam search. Among smoothing algorithms, the Brants approach yields the highest results, of 76.3, with the modified Kneser-Ney algorithm close behind, at 76.2. The modified Witten-Bell algorithm achieved an F-score of 75.7. 4.4 Discussion Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). It is surprisingly that the Brants algorithm performs favourably compared to the better-known modified Kneser-Ney algorithm. This might be due to the heritage of the two algorithms. Kneser-Ney smoothing was designed for language modelling, where there are tens of thousands or hundreds of thousands of tokens having a Zipfian distribution. With all transformations included, the nonterminals of our grammar did have a Zipfian marginal distribution, but there were only several hundred tokens. The Brants algorithm was specifically designed for distributions with fewer tokens. Also surprising is th</context>
<context position="22376" citStr="Schiehlen (2004)" startWordPosition="3818" endWordPosition="3819">the tags), and found that labelled bracket F-scores increase from 76.3 to 85.2. A manual inspection of 100 sentences found that GF mislabelling can accounts for at most two-thirds of the mistakes due to POS tags. Over one third was due to genuine POS tagging errors. The most common problem was verb mistagging: they are either confused with adjectives (both 2As an anonymous reviewer pointed out, it is not always straightforward to reproduce statistical parsing results even when the implementation details are given (Bikel, 2004). 319 Model LB F-score This paper 76.3 Dubey and Keller (2003) 74.1 Schiehlen (2004) 71.1 Table 4: Comparison with previous work. take the common -en suffix), or the tense was incorrect. Mistagged verb are a serious problem: it entails an entire clause is parsed incorrectly. Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. This problem might be alleviated by using a more detailed model of morphology than our suffix analyzer provides. To investigate pure parsing errors, we manually examined 100 sentences which were incorrectly parsed, but which nevertheless were assigned the correct PO</context>
<context position="23671" citStr="Schiehlen (2004)" startWordPosition="4036" endWordPosition="4037">rs (of which 77% are due to PP attachment alone). Misparsed coordination was the second most common problem, accounting for 15% of all mistakes. Another class of error appears to be due to Markovization. The boundaries of VPs are sometimes incorrect, with the parser attaching dependents directly to the S node rather than the VP. In the most extreme cases, the VP had no verb, with the main verb heading a subordinate clause. 6 Comparison with Previous Work Table 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). Dubey and Keller use a variant of the lexicalized Collins (1999) model to achieve a labelled bracketing F-score of 74.1%. Schiehlen presents a number of unlexicalized models. The best model on labelled bracketing achieves an F-score of 71.8%. The work of Schiehlen is particularly interesting as he also considers a number of transformations to improve the performance of an unlexicalized parser. Unlike the work presented here, Schiehlen does not attempt to perform any suffix or morphological analysis of the input text. However, he does suggest a number of treebank transformations. One such tra</context>
<context position="25715" citStr="Schiehlen (2004)" startWordPosition="4362" endWordPosition="4363">interested in long-distance dependencies, and therefore do not report results on local dependencies, as we do here. 7 Conclusions In this paper, we presented the best-performing parser for German, as measured by labelled bracket scores. The high performance was due to three factors: (i) treebank transformations (ii) an integrated model of morphology in the form of a suffix analyzer and (iii) the use of smoothing in an unlexicalized grammar. Moreover, there are possible paths for improvement: lexicalization could be added to the model, as could some of the treebank transformations suggested by Schiehlen (2004). Indeed, the suffix analyzer could well be of value in a lexicalized model. While we only presented results on the German NEGRA corpus, there is reason to believe that the techniques we presented here are also important to other languages where lexicalization provides little benefit: smoothing is a broadly-applicable technique, and if difficulties with lexicalization are due to sparse lexical data, then suffix analysis provides a useful way to get more information from lexical elements which were unseen while training. In addition to our primary results, we also provided a detailed error anal</context>
</contexts>
<marker>Schiehlen, 2004</marker>
<rawString>Micheal Schiehlen. 2004. Annotation Strategies for Probabilistic Parsing in German. In Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorsten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="2587" citStr="Skut et al., 1997" startWordPosition="411" endWordPosition="414">nsformations we present. Section 4 describes how smoothing can be incorporated into an unlexicalized grammar to achieve state-of-the-art results in German. Rather using one smoothing algorithm, we use three different approaches, allowing us to compare the relative performance of each. An error analysis is presented in Section 5, which points to several possible areas of future research. We follow the error analysis with a comparison with related work in Section 6. Finally we offer concluding remarks in Section 7. 2 Data The parsing models we present are trained and tested on the NEGRA corpus (Skut et al., 1997), a handparsed corpus of German newspaper text containing approximately 20,000 sentences. It is available in several formats, and in this paper, we use the Penn Treebank (Marcus et al., 1993) format of NEGRA. The annotation used in NEGRA is similar to that used in the English Penn Treebank, with some differences which make it easier to annotate German syntax. German’s flexible word order would have required an explosion in long-distance dependencies (LDDs) had annotation of NEGRA more closely resembled that of the Penn Treebank. The NEGRA designers therefore chose to use relatively flat trees,</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word order languages. In Proceedings of the 5th Conference on Applied Natural Language Processing, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Uszkoreit</author>
</authors>
<title>Word Order and Constituent Structure in German.</title>
<date>1987</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="7555" citStr="Uszkoreit (1987)" startWordPosition="1276" endWordPosition="1277">Markov assumption ‘forgets’ everything earlier then 2 previous sisters. A rule would now be in the form ABi 2Bi 1 BiABi 1Bi, and the probability would be: Pw wLHS c LHS w P B1 BnA i 1 ∏ n P B1 BnA i 1 ∏ n P BiA Bi 2 Bi 1 315 The other rule type we consider are linear precedence/immediate dominance (LP/ID) rules (Gazdar et al., 1985). If a context-free rule can be thought of as a LHS token with an ordered list of tokens on the RHS, then an LP/ID rule can be thought of as a LHS token with a multiset of tokens on the RHS together with some constraints on the possible orders of tokens on the RHS. Uszkoreit (1987) argues that LP/ID rules with violatable ‘soft’ constraints are suitable for modelling some aspects of German word order. This makes a probabilistic formulation of LP/ID rules ideal: probabilities act as soft constraints. Our treatment of probabilistic LP/ID rules generate children one constituent at a time, conditioning upon the parent and a multiset of previously generated children. Formally, the the probability of the rule is approximated as: In addition to the two additional formulations of the Pr distribution, we also consider one variant of the Pw distribution, which includes the suffix </context>
</contexts>
<marker>Uszkoreit, 1987</marker>
<rawString>Hans Uszkoreit. 1987. Word Order and Constituent Structure in German. CSLI Publications, Stanford, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>