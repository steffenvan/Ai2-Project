<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.996488">
Improving Vector Space Word Representations
Using Multilingual Correlation
</title>
<author confidence="0.980871">
Manaal Faruqui and Chris Dyer
</author>
<affiliation confidence="0.747158">
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
</affiliation>
<email confidence="0.998467">
{mfaruqui, cdyer}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935166666667">
The distributional hypothesis of Harris
(1954), according to which the meaning
of words is evidenced by the contexts
they occur in, has motivated several effec-
tive techniques for obtaining vector space
semantic representations of words using
unannotated text corpora. This paper ar-
gues that lexico-semantic content should
additionally be invariant across languages
and proposes a simple technique based
on canonical correlation analysis (CCA)
for incorporating multilingual evidence
into vectors generated monolingually. We
evaluate the resulting word representations
on standard lexical semantic evaluation
tasks and show that our method produces
substantially better semantic representa-
tions than monolingual techniques.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986793590163935">
Data-driven learning of vector-space word embed-
dings that capture lexico-semantic properties is
a technique of central importance in natural lan-
guage processing. Using cooccurrence statistics
from a large corpus of text (Deerwester et al.,
1990; Turney and Pantel, 2010),1 it is possible
to construct high-quality semantic vectors — as
judged by both correlations with human judge-
ments of semantic relatedness (Turney, 2006;
Agirre et al., 2009) and as features for downstream
applications (Turian et al., 2010).
The observation that vectors representing cooc-
currence tendencies would capture meaning is ex-
pected according to the distributional hypothe-
sis (Harris, 1954), famously articulated by Firth
1Related approaches use the internal representations from
neural network models of word sequences (Collobert and We-
ston, 2008) or continuous bags-of-context wordsels (Mikolov
et al., 2013a) to arrive at vector representations that likewise
capture cooccurence tendencies and meanings.
(1957) as You shall know a word by the company
it keeps. Although there is much evidence in fa-
vor of the distributional hypothesis, in this paper
we argue for incorporating translational context
when constructing vector space semantic models
(VSMs). Simply put: knowing how words trans-
late is a valuable source of lexico-semantic infor-
mation and should lead to better VSMs.
Parallel corpora have long been recognized as
valuable for lexical semantic applications, in-
cluding identifying word senses (Diab, 2003;
Resnik and Yarowsky, 1999) and paraphrase and
synonymy relationships (Bannard and Callison-
Burch, 2005). The latter work (which we build on)
shows that if different words or phrases in one lan-
guage often translate into a single word or phrase
type in a second language, this is good evidence
that they are synonymous. To illustrate: the En-
glish word forms aeroplane, airplane, and plane
are observed to translate into the same Hindi word:
TTZJ ZT-T (vaayuyaan). Thus, even if we did not
know the relationship between the English words,
this translation fact is evidence that they all have
the same meaning.
How can we exploit information like this when
constructing VSMs? We propose a technique that
first constructs independent VSMs in two lan-
guages and then projects them onto a common
vector space such that translation pairs (as deter-
mined by automatic word alignments) should be
maximally correlated (§2). We review latent se-
mantic analysis (LSA), which serves as our mono-
lingual VSM baseline (§3), and a suite of stan-
dard evaluation tasks that we use to measure the
quality of the embeddings (§4). We then turn to
experiments. We first show that our technique
leads to substantial improvements over monolin-
gual LSA (§5), and then examine how our tech-
nique fares with vectors learned using two dif-
ferent neural networks, one that models word se-
quences and a second that models bags-of-context
</bodyText>
<page confidence="0.986623">
462
</page>
<note confidence="0.9989155">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462–471,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9933625">
Figure 1: Cross-lingual word vector projection us-
ing CCA.
</figureCaption>
<bodyText confidence="0.933311">
words. We observe substantial improvements over
the sequential model using multilingual evidence
but more mixed results relative to using the bags-
of-contexts model (§6).
</bodyText>
<sectionHeader confidence="0.990375" genericHeader="introduction">
2 Multilingual Correlation with CCA
</sectionHeader>
<bodyText confidence="0.9736995">
To gain information from the translation of a given
word in other languages the most basic thing to do
would be to just append the given word represen-
tation with the word representations of its transla-
tion in the other language. This has three draw-
backs: first, it increases the number of dimensions
in the vector; second, it can pull irrelevant infor-
mation from the other language that doesn’t gen-
eralize across languages and finally the given word
might be out of vocabulary of the parallel corpus
or dictionary.
To counter these problems we use CCA2 which
is a way of measuring the linear relationship be-
tween two multidimensional variables. It finds two
projection vectors, one for each variable, that are
optimal with respect to correlations. The dimen-
sionality of these new projected vectors is equal to
or less than the smaller dimensionality of the two
variables.
Let Σ E Rn1xd1 and Ω E Rn2xd2 be vector
</bodyText>
<footnote confidence="0.872526">
2We use the MATLAB module for CCA: http://www.
mathworks.com/help/stats/canoncorr.html
</footnote>
<bodyText confidence="0.9873852">
space embeddings of two different vocabularies
where rows represent words. Since the two vo-
cabularies are of different sizes (n1 and n2) and
there might not exist translation for every word
of Σ in Ω, let Σ0 C_ Σ where every word in Σ0
is translated to one other word3 in Ω0 C_ Ω and
Σ E Rnxd1 and Ω E Rnxd2.
Let x and y be two corresponding vectors from
Σ0 and Ω0, and v and w be two projection direc-
tions. Then, the projected vectors are:
</bodyText>
<equation confidence="0.798495">
= yw (1)
</equation>
<bodyText confidence="0.9895075">
and the correlation between the projected vectors
can be written as:
</bodyText>
<equation confidence="0.997893">
ρ(x�, y) = [ �E[x02]E[y0 2] (2)
</equation>
<bodyText confidence="0.730462">
CCA maximizes ρ for the given set of vectors Σ0
and Ω0 and outputs two projection vectors v and
w:
v, w = CCA(x, y)
</bodyText>
<equation confidence="0.99672925">
= arg max ρ(xv, yw) (3)
v,w
CCA(Σ0,Ω0) (4)
E* = ΣV Ω* = ΩW (5)
</equation>
<bodyText confidence="0.9990322">
where, V E Rd1xd, W E Rd2xd con-
tain the projection vectors and d =
min{rank(V), rank(W)J. Thus, the result-
ing vectors cannot be longer than the original
vectors. Since V and W can be used to project
the whole vocabulary, CCA also solves the
problem of not having translations of a particular
word in the dictionary. The schema of performing
CCA on the monolingual word representations of
two languages is shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.852239">
Further Dimensionality Reduction: Since
</subsectionHeader>
<bodyText confidence="0.9627077">
CCA gives us correlations and corresponding
projection vectors across d dimensions which
can be large, we perform experiments by taking
projections of the original word vectors across
only the top k correlated dimensions. This is
trivial to implement as the projection vectors V ,
Using these two projection vectors we can project
the entire vocabulary of the two languages
and
using equation 1. Summari
</bodyText>
<equation confidence="0.8978796">
Σ
Ω
zing:
V , W =
3Further information on how these one-to-one translations
are
obtained in §5
0
x= xv y
0
</equation>
<page confidence="0.997312">
463
</page>
<bodyText confidence="0.95286575">
W in equation 4 are already sorted in descending
order of correlation. Therefore in,
they capture both semantic and syntactic aspects
of the representations.
</bodyText>
<equation confidence="0.761786">
E∗k = EVk SZ∗k = SZWk (6)
</equation>
<bodyText confidence="0.998941333333333">
E∗k and SZ∗ k are now word vector projections along
the top k correlated dimensions, where, V k and
Wk are the column truncated matrices.
</bodyText>
<sectionHeader confidence="0.9365" genericHeader="method">
3 Latent Semantic Analysis
</sectionHeader>
<bodyText confidence="0.995862521739131">
We perform latent semantic analysis (Deerwester
et al., 1990) on a word-word co-occurrence ma-
trix. We construct a word co-occurrence frequency
matrix F for a given training corpus where each
row w, represents one word in the corpus and ev-
ery column c, is the context feature in which the
word is observed. In our case, every column is
a word which occurs in a given window length
around the target word. For scalability reasons, we
only select words with frequency greater than 10
as features. We also remove the top 100 most fre-
quent words (mostly stop words) from the column
features.
We then replace every entry in the sparse fre-
quency matrix F by its pointwise mutual infor-
mation (PMI) (Church and Hanks, 1990; Turney,
2001) resulting in X. PMI is designed to give a
high value to xij where there is a interesting rela-
tion between wi and cj, a small or negative value
of xij indicates that the occurrence of wi in cj is
uninformative. Finally, we factorize the matrix X
using singular value decomposition (SVD). SVD
decomposes X into the product of three matrices:
</bodyText>
<equation confidence="0.999446">
X = U*VT (7)
</equation>
<bodyText confidence="0.999816">
where, U and V are in column orthonormal
form and * is a diagonal matrix of singular val-
ues (Golub and Van Loan, 1996). We obtain a re-
duced dimensional representation of words from
size IVI to k:
</bodyText>
<equation confidence="0.967213">
A = Uk*k (8)
</equation>
<bodyText confidence="0.999996285714286">
where k can be controlled to trade off between re-
construction error and number of parameters, *k
is the diagonal matrix containing the top k singular
values, Uk is the matrix produced by selecting the
corresponding columns from U and A represents
the new matrix containing word vector representa-
tions in the reduced dimensional space.
</bodyText>
<sectionHeader confidence="0.990184" genericHeader="method">
4 Word Representation Evaluation
</sectionHeader>
<bodyText confidence="0.9998105">
We evaluate the quality of our word vector repre-
sentations on a number of tasks that test how well
</bodyText>
<subsectionHeader confidence="0.999325">
4.1 Word Similarity
</subsectionHeader>
<bodyText confidence="0.999938935483871">
We evaluate our word representations on four dif-
ferent benchmarks that have been widely used to
measure word similarity. The first one is the WS-
353 dataset (Finkelstein et al., 2001) containing
353 pairs of English words that have been assigned
similarity ratings by humans. This data was fur-
ther divided into two fragments by Agirre et al.
(2009) who claimed that similarity (WS-SIM) and
relatedness (WS-REL) are two different kinds of
relations and should be dealt with separately. We
present results on the whole set and on the individ-
ual fragments as well.
The second and third benchmarks are the RG-
65 (Rubenstein and Goodenough, 1965) and the
MC-30 (Miller and Charles, 1991) datasets that
contain 65 and 30 pairs of nouns respectively and
have been given similarity rankings by humans.
These differ from WS-353 in that it contains only
nouns whereas the former contains all kinds of
words. The fourth benchmark is the MTurk-287
(Radinsky et al., 2011) dataset that constitutes of
287 pairs of words and is different from the above
two benchmarks in that it has been constructed by
crowdsourcing the human similarity ratings using
Amazon Mechanical Turk.
We calculate similarity between a given pair
of words by the cosine similarity between their
corresponding vector representation. We then re-
port Spearman’s rank correlation coefficient (My-
ers and Well, 1995) between the rankings pro-
duced by our model against the human rankings.
</bodyText>
<subsectionHeader confidence="0.998744">
4.2 Semantic Relations (SEM-REL)
</subsectionHeader>
<bodyText confidence="0.937652">
Mikolov et al. (2013a) present a new semantic re-
lation dataset composed of analogous word pairs.
It contains pairs of tuples of word relations that
follow a common semantic relation. For example,
in England : London :: France : Paris, the two
given pairs of words follow the country-capital re-
lation. There are three other such kinds of rela-
tions: country-currency, man-woman, city-in-state
and overall 8869 such pairs of words4.
The task here is to find a word d that best fits
the following relationship: a : b :: c : d given a, b
and c. We use the vector offset method described
4107 pairs were out of vocabulary for our vectors and
were ignored.
</bodyText>
<page confidence="0.998246">
464
</page>
<bodyText confidence="0.9920252">
in Mikolov et al. (2013a) that computes the vector
y = xa − xb + x, where, xa, xb and x, are word
vectors of a, b and c respectively and returns the
vector xw from the whole vocabulary which has
the highest cosine similarity to y:
</bodyText>
<equation confidence="0.9428525">
xw = arg max
Xw
</equation>
<bodyText confidence="0.999406333333333">
It is worth noting that this is a non-trivial |V |-way
classification task where V is the size of the vo-
cabulary.
</bodyText>
<subsectionHeader confidence="0.999548">
4.3 Syntactic Relations (SYN-REL)
</subsectionHeader>
<bodyText confidence="0.999967384615384">
This dataset contains word pairs that are differ-
ent syntactic forms of a given word and was pre-
pared by Mikolov et al. (2013a). For exam-
ple, in walking and walked, the second word is
the past tense of the first word. There are nine
such different kinds of relations: adjective-adverb,
opposites, comaparative, superlative, present-
participle, nation-nationality, past tense, plural
nouns and plural verbs. Overall there are 10675
such syntactic pairs of word tuples. The task here
again is identifying a word d that best fits the fol-
lowing relationship: a : b :: c : d and we solve it
using the method described in §4.2.
</bodyText>
<sectionHeader confidence="0.998909" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.975884">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999101722222222">
For English, German and Spanish we used the
WMT-20115 monolingual news corpora and for
French we combined the WMT-2011 and 20126
monolingual news corpora so that we have around
300 million tokens for each language to train the
word vectors.
For CCA, a one-to-one correspondence be-
tween the two sets of vectors is required. Obvi-
ously, the vocabulary of two languages are of dif-
ferent sizes and hence to obtain one-to-one map-
ping, for every English word we choose a word
from the other language to which it has been
aligned the maximum number of times7 in a paral-
lel corpus. We got these word alignment counts
using cdec (Dyer et al., 2010) from the paral-
lel news commentary corpora (WMT 2006-10)
combined with the Europarl corpus for English-
{German, French, Spanish}.
</bodyText>
<footnote confidence="0.998648">
5http://www.statmt.org/wmt11/
6http://www.statmt.org/wmt12/
7We also tried weighted average of vectors across all
aligned words and did not observe any significant difference
in results.
</footnote>
<subsectionHeader confidence="0.987875">
5.2 Methodology
</subsectionHeader>
<bodyText confidence="0.999906307692308">
We construct LSA word vectors of length 6408 for
English, German, French and Spanish. We project
the English word vectors using CCA by pairing
them with German, French and Spanish vectors.
For every language pair we take the top k cor-
related dimensions (cf. equation 6), where k E
10%,20%,... 100% and tune the performance on
WS-353 task. We then select the k that gives
us the best average performance across language
pairs, which is k = 80%, and evaluate the cor-
responding vectors on all other benchmarks. This
prevents us from over-fitting k for every individual
task.
</bodyText>
<subsectionHeader confidence="0.516396">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999795333333333">
Table 1 shows the Spearman’s correlation ratio ob-
tained by using word vectors to compute the sim-
ilarity between two given words and compare the
ranked list against human rankings. The first row
in the table shows the baseline scores obtained
by using only the monolingual English vectors
whereas the other rows correspond to the multi-
lingual cases. The last row shows the average per-
formance of the three language pairs. For all the
tasks we get at least an absolute gain of 20 points
over the baseline. These results are highly assur-
ing of our hypothesis that multilingual context can
help in improving the semantic similarity between
similar words as described in the example in §1.
Results across language pairs remain almost the
same and the differences are most of the times sta-
tistically insignificant.
Table 1 also shows the accuracy obtained on
predicting different kinds of relations between
word pairs. For the SEM-REL task the average
improvement in accuracy is an absolute 30 points
over the baseline which is highly statistically sig-
nificant (p &lt; 0.01) according to the McNemar’s
test (Dietterich, 1998). The same holds true for
the SYN-REL task where we get an average im-
provement of absolute 8 points over the baseline
across the language pairs. Such an improvement
in scores across these relation prediction tasks fur-
ther enforces our claim that cross-lingual context
can be exploited using the method described in §2
and it does help in encoding the meaning of a word
better in a word vector than monolingual informa-
tion alone.
</bodyText>
<footnote confidence="0.498347">
8See section 5.5 for further discussion on vector length.
</footnote>
<table confidence="0.809659666666667">
xw &apos; y
|xw|&apos;|y|
465
Lang Dim WS-353 WS-SIM WS-REL RG-65 MC-30 MTurk-287 SEM-REL SYN-REL
En 640 46.7 56.2 36.5 50.7 42.3 51.2 14.5 36.8
De-En 512 68.0 74.4 64.6 75.5 81.9 53.6 43.9 45.5
Fr-En 512 68.4 73.3 65.7 73.5 81.3 55.5 43.9 44.3
Es-En 512 67.2 71.6 64.5 70.5 78.2 53.6 44.2 44.5
Average – 56.6 64.5 51.0 62.0 65.5 60.8 44 44.7
</table>
<tableCaption confidence="0.999475">
Table 1: Spearman’s correlation (left) and accuracy (right) on different tasks.
</tableCaption>
<figureCaption confidence="0.996671">
Figure 2: Monolingual (top) and multilingual (bottom; marked with apostrophe) word projections of the
antonyms (shown in red) and synonyms of “beautiful”.
</figureCaption>
<subsectionHeader confidence="0.987142">
5.4 Qualitative Example
</subsectionHeader>
<bodyText confidence="0.994182529411765">
To understand how multilingual evidence leads to
better results in semantic evaluation tasks, we plot
the word representations obtained in §3 of sev-
eral synonyms and antonyms of the word “beau-
tiful” by projecting both the transformed and un-
transformed vectors onto R2 using the t-SNE
tool (van der Maaten and Hinton, 2008). The
untransformed LSA vectors are in the upper part
of Fig. 2, and the CCA-projected vectors are in
the lower part. By comparing the two regions,
we see that in the untransformed representations,
the antonyms are in two clusters separated by the
synonyms, whereas in the transformed representa-
tion, both the antonyms and synonyms are in their
own cluster. Furthermore, the average intra-class
distance between synonyms and antonyms is re-
duced.
</bodyText>
<figureCaption confidence="0.674035">
Figure 3: Performance of monolingual and mul-
tilingual vectors on WS-353 for different vector
lengths.
</figureCaption>
<subsectionHeader confidence="0.922721">
5.5 Variation in Vector Length
</subsectionHeader>
<bodyText confidence="0.986347">
In order to demonstrate that the gains in perfor-
mance by using multilingual correlation sustains
</bodyText>
<page confidence="0.998331">
466
</page>
<bodyText confidence="0.999987428571428">
for different number of dimensions, we compared
the performance of the monolingual and (German-
English) multilingual vectors with k = 80% (cf.
§5.2). It can be see in figure 3 that the perfor-
mance improvement for multilingual vectors re-
mains almost the same for different vector lengths
strengthening the reliability of our approach.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="method">
6 Neural Network Word Representations
</sectionHeader>
<bodyText confidence="0.999941">
Other kinds of vectors shown to be useful in many
NLP tasks are word embeddings obtained from
neural networks. These word embeddings capture
more complex information than just co-occurrence
counts as explained in the next section. We test
our multilingual projection method on two types
of such vectors by keeping the experimental set-
ting exactly the same as in §5.2.
</bodyText>
<subsectionHeader confidence="0.979334">
6.1 RNN Vectors
</subsectionHeader>
<bodyText confidence="0.999978769230769">
The recurrent neural network language model
maximizes the log-likelihood of the training cor-
pus. The architecture (Mikolov et al., 2013b) con-
sists of an input layer, a hidden layer with recur-
rent connections to itself, an output layer and the
corresponding weight matrices. The input vector
w(t) represents input word at time t encoded us-
ing 1-of-N encoding and the output layer y(t) pro-
duces a probability distribution over words in the
vocabulary V . The hidden layer maintains a repre-
sentation of the sentence history in s(t). The val-
ues in the hidden and output layer are computed as
follows:
</bodyText>
<equation confidence="0.9999945">
s(t) = f(Uw(t) + Ws(t − 1)) (9)
y(t) = g(V s(t)) (10)
</equation>
<bodyText confidence="0.999994125">
where, f and g are the logistic and softmax func-
tions respectively. U and V are weight matri-
ces and the word representations are found in the
columns of U . The model is trained using back-
propagation. Training such a purely lexical model
will induce representations with syntactic and se-
mantic properties. We use the RNNLM toolkit9 to
induce these word representations.
</bodyText>
<subsectionHeader confidence="0.998604">
6.2 Skip Gram Vectors
</subsectionHeader>
<bodyText confidence="0.998663333333333">
In the RNN model (§6.1) most of the complexity
is caused by the non-linear hidden layer. This is
avoided in the new model proposed in Mikolov
</bodyText>
<footnote confidence="0.964656">
9http://www.fit.vutbr.cz/˜imikolov/
rnnlm/
</footnote>
<bodyText confidence="0.999838555555555">
et al. (2013a) where they remove the non-linear
hidden layer and there is a single projection layer
for the input word. Precisely, each current word is
used as an input to a log-linear classifier with con-
tinuous projection layer and words within a cer-
tain range before and after the word are predicted.
These vectors are called the skip-gram (SG) vec-
tors. We used the tool10 for obtaining these word
vectors with default settings.
</bodyText>
<subsectionHeader confidence="0.752626">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.99956741025641">
We compare the best results obtained by using dif-
ferent types of monolingual word representations
across all language pairs. For brevity we do not
show the results individually for all language pairs
as they follow the same pattern when compared to
the baseline for every vector type. We train word
vectors of length 80 because it was computation-
ally intractable to train the neural embeddings for
higher dimensions. For multilingual vectors, we
obtain k = 60% (cf. §5.2).
Table 2 shows the correlation ratio and the accu-
racies for the respective evaluation tasks. For the
RNN vectors the performance improves upon in-
clusion of multilingual context for almost all tasks
except for SYN-REL where the loss is statistically
significant (p &lt; 0.01). For MC-30 and SEM-
REL the small drop in performance is not statis-
tically significant. Interestingly, the performance
gain/loss for the SG vectors in most of the cases is
not statistically significant, which means that in-
clusion of multilingual context is not very helpful.
In fact, for SYN-REL the loss is statistically sig-
nificant (p &lt; 0.05) which is similar to the perfor-
mance of RNN case. Overall, the best results are
obtained by the SG vectors in six out of eight eval-
uation tasks whereas SVD vectors give the best
performance in two tasks: RG-65, MC-30. This is
an encouraging result as SVD vectors are the eas-
iest and fastest to obtain as compared to the other
two vector types.
To further understand why multilingual context
is highly effective for SVD vectors and to a large
extent for RNN vectors as well, we plot (Figure 4)
the correlation ratio obtained by varying the length
of word representations by using equation 6 for the
three different vector types on two word similarity
tasks: WS-353 and RG-65.
SVD vectors improve performance upon the in-
crease of the number of dimensions and tend to
</bodyText>
<footnote confidence="0.842973">
10https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.996541">
467
</page>
<table confidence="0.999844428571429">
Vectors Dim Lang WS-353 WS-SIM WS-REL RG-65 MC-30 MTurk SEM-REL SYN-REL
SVD 80 Mono 34.8 45.5 23.4 30.8 21.0 46.6 13.5 24.4
48 Multi 58.1 65.3 52.7 62.7 67.7 62.1 23.4 33.2
RNN 80 Mono 23.6 35.6 17.5 26.2 47.7 32.9 4.7 18.2
48 Multi 35.4 47.3 29.8 36.6 46.5 43.8 4.1 12.2
SG 80 Mono 63.9 69.9 60.9 54.6 62.8 66.9 47.8 47.8
48 Multi 63.1 70.4 57.6 54.9 64.7 58.7 46.5 44.2
</table>
<tableCaption confidence="0.9945035">
Table 2: Spearman’s correlation (left) and accuracy (right) on different tasks. Bold indicates best result
across all vector types. Mono: monolingual and Multi: multilingual.
</tableCaption>
<figure confidence="0.537815">
Number of dimensions
</figure>
<figureCaption confidence="0.993218">
Figure 4: Performance as a function of vector length on word similarity tasks. The monolingual vectors
always have a fixed length of 80, they are just shown in the plots for comparison.
</figureCaption>
<figure confidence="0.9702552">
WS-353 RG-65
SVD
RNN
Correlation ratio (%)
SG
</figure>
<page confidence="0.99933">
468
</page>
<bodyText confidence="0.999990260869565">
saturate towards the end. For all the three lan-
guage pairs the SVD vectors show uniform pat-
tern of performance which gives us the liberty to
use any language pair at hand. This is not true
for the RNN vectors whose curves are signifi-
cantly different for every language pair. SG vec-
tors show a uniform pattern across different lan-
guage pairs and the performance with multilin-
gual context converges to the monolingual perfor-
mance when the vector length becomes equal to
the monolingual case (k = 80). The fact that both
SG and SVD vectors have similar behavior across
language pairs can be treated as evidence that se-
mantics or information at a conceptual level (since
both of them basically model word cooccurrence
counts) transfers well across languages (Dyvik,
2004) although syntax has been projected across
languages as well (Hwa et al., 2005; Yarowsky and
Ngai, 2001). The pattern of results in the case of
RNN vectors are indicative of the fact that these
vectors encode syntactic information as explained
in §6 which might not generalize well as compared
to semantic information.
</bodyText>
<sectionHeader confidence="0.999871" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999877108695652">
Our method of learning multilingual word vectors
is most closely associated to Zou et al. (2013) who
learn bilingual word embeddings and show their
utility in machine translation. They optimize the
monolingual and the bilingual objective together
whereas we do it in two separate steps and project
to a common vector space to maximize correla-
tion between the two. Vuli´c and Moens (2013)
learn bilingual vector spaces from non parallel
data induced using a seed lexicon. Our method
can also be seen as an application of multi-view
learning (Chang et al., 2013; Collobert and We-
ston, 2008), where one of the views can be used
to capture cross-lingual information. Klementiev
et al. (2012) use a multitask learning framework
to encourage the word representations learned by
neural language models to agree cross-lingually.
CCA can be used for dimension reduction and
to draw correspondences between two sets of
data.Haghighi et al. (2008) use CCA to draw trans-
lation lexicons between words of two different lan-
guages using only monolingual corpora. CCA
has also been used for constructing monolingual
word representations by correlating word vectors
that capture aspects of word meaning and dif-
ferent types of distributional profile of the word
(Dhillon et al., 2011). Although our primary ex-
perimental emphasis was on LSA based monolin-
gual word representations, which we later gener-
alized to two different neural network based word
embeddings, these monolingual word vectors can
also be obtained using other continuous models of
language (Collobert and Weston, 2008; Mnih and
Hinton, 2008; Morin and Bengio, 2005; Huang et
al., 2012).
Bilingual representations have previously been
explored with manually designed vector space
models (Peirsman and Pad´o, 2010; Sumita, 2000)
and with unsupervised algorithms like LDA and
LSA (Boyd-Graber and Blei, 2012; Zhao and
Xing, 2006). Bilingual evidence has also been ex-
ploited for word clustering which is yet another
form of representation learning, using both spec-
tral methods (Zhao et al., 2005) and structured
prediction approaches (T¨ackstr¨om et al., 2012;
Faruqui and Dyer, 2013).
</bodyText>
<sectionHeader confidence="0.99145" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999980166666667">
We have presented a canonical correlation anal-
ysis based method for incorporating multilingual
context into word representations generated using
only monolingual information and shown its ap-
plicability across three different ways of generat-
ing monolingual vectors on a variety of evalua-
tion benchmarks. These word representations ob-
tained after using multilingual evidence perform
significantly better on the evaluation tasks com-
pared to the monolingual vectors. We have also
shown that our method is more suitable for vec-
tors that encode semantic information than those
that encode syntactic information. Our work sug-
gests that multilingual evidence is an important
resource even for purely monolingual, semanti-
cally aware applications. The tool for projecting
word vectors can be found at http://cs.cmu.
edu/˜mfaruqui/soft.html.
</bodyText>
<sectionHeader confidence="0.975805" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999793">
We thanks Kevin Gimpel, Noah Smith, and David
Bamman for helpful comments on earlier drafts
of this paper. This research was supported by the
NSF through grant IIS-1352440.
</bodyText>
<sectionHeader confidence="0.997779" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8488825">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
</reference>
<page confidence="0.998346">
469
</page>
<reference confidence="0.996163925233645">
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In Pro-
ceedings of North American Chapter of the Associ-
ation for Computational Linguistics, NAACL ’09,
pages 19–27, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL.
Jordan L. Boyd-Graber and David M. Blei. 2012. Mul-
tilingual topic models for unaligned text. CoRR,
abs/1205.2657.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602–1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22–29, March.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, ICML ’08, pages 160–167, New
York, NY, USA. ACM.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science.
Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-
gar. 2011. Multi-view learning of word embeddings
via cca. In NIPS, pages 199–207.
Mona Talat Diab. 2003. Word sense disambiguation
within a multilingual framework. Ph.D. thesis, Uni-
versity of Maryland at College Park, College Park,
MD, USA. AAI3115805.
Thomas G. Dietterich. 1998. Approximate statis-
tical tests for comparing supervised classification
learning algorithms. Neural Computation, 10:1895–
1923.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Hendra Setiawan, Ferhan Ture, Vladimir Ei-
delman, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In In Proceedings of ACL System Demonstrations.
Helge Dyvik. 2004. Translations as semantic mir-
rors: from parallel corpus to wordnet. Language
and Computers, 49(1):311–326.
Manaal Faruqui and Chris Dyer. 2013. An informa-
tion theoretic approach to bilingual word clustering.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 777–783, Sofia, Bulgaria, Au-
gust.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In WWW ’01: Proceedings of the
10th international conference on World Wide Web,
pages 406–414, New York, NY, USA. ACM Press.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis, pages 1–32.
Gene H. Golub and Charles F. Van Loan. 1996. Matrix
computations (3rd ed.). Johns Hopkins University
Press, Baltimore, MD, USA.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11:11–311.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1–28.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In In NIPS.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
AISTATS05, pages 246–252.
</reference>
<page confidence="0.975368">
470
</page>
<reference confidence="0.999909989010989">
Jerome L. Myers and Arnold D. Well. 1995. Research
Design &amp; Statistical Analysis. Routledge, 1 edition,
June.
Yves Peirsman and Sebastian Pad´o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics, HLT ’10, pages 921–929,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
WWW ’11, pages 337–346, New York, NY, USA.
ACM.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evalua-
tion methods for word sense disambiguation. Nat.
Lang. Eng., 5(2):113–133, June.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.
Eiichiro Sumita. 2000. Lexical transfer using a vector-
space model. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, ACL ’00, pages 425–431, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In The 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, volume 1, page 11. Association
for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 384–394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning : Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
pages 141–188.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the 12th
European Conference on Machine Learning, EMCL
’01, pages 491–502, London, UK, UK. Springer-
Verlag.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Comput. Linguist., 32(3):379–416, Septem-
ber.
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing Data using t-SNE. Journal of Machine
Learning Research, 9:2579–2605, November.
Ivan Vuli´c and Marie-Francine Moens. 2013. A study
on bootstrapping bilingual vector spaces from non-
parallel data (and nothing else). In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1613–1624, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies, NAACL ’01, pages 1–
8, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL06.
Bing Zhao, Eric P. Xing, and Alex Waibel. 2005.
Bilingual word spectral clustering for statistical ma-
chine translation. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts, ParaText
’05, pages 25–32, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393–
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.998731">
471
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960716">
<title confidence="0.9979765">Improving Vector Space Word Using Multilingual Correlation</title>
<author confidence="0.973818">Faruqui</author>
<affiliation confidence="0.998898">Carnegie Mellon</affiliation>
<address confidence="0.998559">Pittsburgh, PA, 15213,</address>
<abstract confidence="0.999603263157895">The distributional hypothesis of Harris (1954), according to which the meaning of words is evidenced by the contexts they occur in, has motivated several effective techniques for obtaining vector space semantic representations of words using unannotated text corpora. This paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually. We evaluate the resulting word representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<date>2009</date>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.</rawString>
</citation>
<citation valid="true">
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date></date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker></marker>
<rawString>A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 19–27, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan L Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Multilingual topic models for unaligned text.</title>
<date>2012</date>
<location>CoRR, abs/1205.2657.</location>
<contexts>
<context position="25134" citStr="Boyd-Graber and Blei, 2012" startWordPosition="4165" endWordPosition="4168">ofile of the word (Dhillon et al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). 8 Conclusion We have presented a canonical correlation analysis based method for incorporating multilingual context into word representations generated using only monolingual information and shown its applicability across three different ways of generating monolingual vectors on a variety of evaluation benchmarks. These </context>
</contexts>
<marker>Boyd-Graber, Blei, 2012</marker>
<rawString>Jordan L. Boyd-Graber and David M. Blei. 2012. Multilingual topic models for unaligned text. CoRR, abs/1205.2657.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
</authors>
<title>Multi-relational latent semantic analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1602--1612</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="23842" citStr="Chang et al., 2013" startWordPosition="3968" endWordPosition="3971">e well as compared to semantic information. 7 Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a multitask learning framework to encourage the word representations learned by neural language models to agree cross-lingually. CCA can be used for dimension reduction and to draw correspondences between two sets of data.Haghighi et al. (2008) use CCA to draw translation lexicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that captur</context>
</contexts>
<marker>Chang, Yih, Meek, 2013</marker>
<rawString>Kai-Wei Chang, Wen-tau Yih, and Christopher Meek. 2013. Multi-relational latent semantic analysis. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602–1612, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="8079" citStr="Church and Hanks, 1990" startWordPosition="1306" endWordPosition="1309">matrix. We construct a word co-occurrence frequency matrix F for a given training corpus where each row w, represents one word in the corpus and every column c, is the context feature in which the word is observed. In our case, every column is a word which occurs in a given window length around the target word. For scalability reasons, we only select words with frequency greater than 10 as features. We also remove the top 100 most frequent words (mostly stop words) from the column features. We then replace every entry in the sparse frequency matrix F by its pointwise mutual information (PMI) (Church and Hanks, 1990; Turney, 2001) resulting in X. PMI is designed to give a high value to xij where there is a interesting relation between wi and cj, a small or negative value of xij indicates that the occurrence of wi in cj is uninformative. Finally, we factorize the matrix X using singular value decomposition (SVD). SVD decomposes X into the product of three matrices: X = U*VT (7) where, U and V are in column orthonormal form and * is a diagonal matrix of singular values (Golub and Van Loan, 1996). We obtain a reduced dimensional representation of words from size IVI to k: A = Uk*k (8) where k can be control</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1):22–29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning, ICML ’08,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1763" citStr="Collobert and Weston, 2008" startWordPosition="237" endWordPosition="241">rge corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. (1957) as You shall know a word by the company it keeps. Although there is much evidence in favor of the distributional hypothesis, in this paper we argue for incorporating translational context when constructing vector space semantic models (VSMs). Simply put: knowing how words translate is a valuable source of lexico-semantic information and should lead to better VSMs. Parallel corpora have long been recognized as valuable for lexical </context>
<context position="23871" citStr="Collobert and Weston, 2008" startWordPosition="3972" endWordPosition="3976">o semantic information. 7 Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a multitask learning framework to encourage the word representations learned by neural language models to agree cross-lingually. CCA can be used for dimension reduction and to draw correspondences between two sets of data.Haghighi et al. (2008) use CCA to draw translation lexicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that capture aspects of word meaning and</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, ICML ’08, pages 160–167, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science.</journal>
<contexts>
<context position="1179" citStr="Deerwester et al., 1990" startWordPosition="153" endWordPosition="156">uages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually. We evaluate the resulting word representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques. 1 Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using cooccurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous b</context>
<context position="7427" citStr="Deerwester et al., 1990" startWordPosition="1189" endWordPosition="1192">ection vectors we can project the entire vocabulary of the two languages and using equation 1. Summari Σ Ω zing: V , W = 3Further information on how these one-to-one translations are obtained in §5 0 x= xv y 0 463 W in equation 4 are already sorted in descending order of correlation. Therefore in, they capture both semantic and syntactic aspects of the representations. E∗k = EVk SZ∗k = SZWk (6) E∗k and SZ∗ k are now word vector projections along the top k correlated dimensions, where, V k and Wk are the column truncated matrices. 3 Latent Semantic Analysis We perform latent semantic analysis (Deerwester et al., 1990) on a word-word co-occurrence matrix. We construct a word co-occurrence frequency matrix F for a given training corpus where each row w, represents one word in the corpus and every column c, is the context feature in which the word is observed. In our case, every column is a word which occurs in a given window length around the target word. For scalability reasons, we only select words with frequency greater than 10 as features. We also remove the top 100 most frequent words (mostly stop words) from the column features. We then replace every entry in the sparse frequency matrix F by its pointw</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca. In</title>
<date>2011</date>
<booktitle>NIPS,</booktitle>
<pages>199--207</pages>
<contexts>
<context position="24548" citStr="Dhillon et al., 2011" startWordPosition="4078" endWordPosition="4081">ngual information. Klementiev et al. (2012) use a multitask learning framework to encourage the word representations learned by neural language models to agree cross-lingually. CCA can be used for dimension reduction and to draw correspondences between two sets of data.Haghighi et al. (2008) use CCA to draw translation lexicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that capture aspects of word meaning and different types of distributional profile of the word (Dhillon et al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xin</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Ungar. 2011. Multi-view learning of word embeddings via cca. In NIPS, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Talat Diab</author>
</authors>
<title>Word sense disambiguation within a multilingual framework.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland at College Park,</institution>
<location>College Park, MD, USA. AAI3115805.</location>
<contexts>
<context position="2431" citStr="Diab, 2003" startWordPosition="340" endWordPosition="341">013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. (1957) as You shall know a word by the company it keeps. Although there is much evidence in favor of the distributional hypothesis, in this paper we argue for incorporating translational context when constructing vector space semantic models (VSMs). Simply put: knowing how words translate is a valuable source of lexico-semantic information and should lead to better VSMs. Parallel corpora have long been recognized as valuable for lexical semantic applications, including identifying word senses (Diab, 2003; Resnik and Yarowsky, 1999) and paraphrase and synonymy relationships (Bannard and CallisonBurch, 2005). The latter work (which we build on) shows that if different words or phrases in one language often translate into a single word or phrase type in a second language, this is good evidence that they are synonymous. To illustrate: the English word forms aeroplane, airplane, and plane are observed to translate into the same Hindi word: TTZJ ZT-T (vaayuyaan). Thus, even if we did not know the relationship between the English words, this translation fact is evidence that they all have the same m</context>
</contexts>
<marker>Diab, 2003</marker>
<rawString>Mona Talat Diab. 2003. Word sense disambiguation within a multilingual framework. Ph.D. thesis, University of Maryland at College Park, College Park, MD, USA. AAI3115805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<contexts>
<context position="14978" citStr="Dietterich, 1998" startWordPosition="2484" endWordPosition="2485">ine. These results are highly assuring of our hypothesis that multilingual context can help in improving the semantic similarity between similar words as described in the example in §1. Results across language pairs remain almost the same and the differences are most of the times statistically insignificant. Table 1 also shows the accuracy obtained on predicting different kinds of relations between word pairs. For the SEM-REL task the average improvement in accuracy is an absolute 30 points over the baseline which is highly statistically significant (p &lt; 0.01) according to the McNemar’s test (Dietterich, 1998). The same holds true for the SYN-REL task where we get an average improvement of absolute 8 points over the baseline across the language pairs. Such an improvement in scores across these relation prediction tasks further enforces our claim that cross-lingual context can be exploited using the method described in §2 and it does help in encoding the meaning of a word better in a word vector than monolingual information alone. 8See section 5.5 for further discussion on vector length. xw &apos; y |xw|&apos;|y| 465 Lang Dim WS-353 WS-SIM WS-REL RG-65 MC-30 MTurk-287 SEM-REL SYN-REL En 640 46.7 56.2 36.5 50.</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10:1895– 1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Hendra Setiawan</author>
<author>Ferhan Ture</author>
<author>Vladimir Eidelman</author>
<author>Phil Blunsom</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In</title>
<date>2010</date>
<booktitle>In Proceedings of ACL System Demonstrations.</booktitle>
<contexts>
<context position="12946" citStr="Dyer et al., 2010" startWordPosition="2154" endWordPosition="2157">and Spanish we used the WMT-20115 monolingual news corpora and for French we combined the WMT-2011 and 20126 monolingual news corpora so that we have around 300 million tokens for each language to train the word vectors. For CCA, a one-to-one correspondence between the two sets of vectors is required. Obviously, the vocabulary of two languages are of different sizes and hence to obtain one-to-one mapping, for every English word we choose a word from the other language to which it has been aligned the maximum number of times7 in a parallel corpus. We got these word alignment counts using cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English{German, French, Spanish}. 5http://www.statmt.org/wmt11/ 6http://www.statmt.org/wmt12/ 7We also tried weighted average of vectors across all aligned words and did not observe any significant difference in results. 5.2 Methodology We construct LSA word vectors of length 6408 for English, German, French and Spanish. We project the English word vectors using CCA by pairing them with German, French and Spanish vectors. For every language pair we take the top k correlated dimensions (cf. equation 6</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Setiawan, Ture, Eidelman, Blunsom, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Hendra Setiawan, Ferhan Ture, Vladimir Eidelman, Phil Blunsom, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In In Proceedings of ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helge Dyvik</author>
</authors>
<title>Translations as semantic mirrors: from parallel corpus to wordnet.</title>
<date>2004</date>
<journal>Language and Computers,</journal>
<volume>49</volume>
<issue>1</issue>
<contexts>
<context position="22949" citStr="Dyvik, 2004" startWordPosition="3823" endWordPosition="3824">ge pair at hand. This is not true for the RNN vectors whose curves are significantly different for every language pair. SG vectors show a uniform pattern across different language pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case (k = 80). The fact that both SG and SVD vectors have similar behavior across language pairs can be treated as evidence that semantics or information at a conceptual level (since both of them basically model word cooccurrence counts) transfers well across languages (Dyvik, 2004) although syntax has been projected across languages as well (Hwa et al., 2005; Yarowsky and Ngai, 2001). The pattern of results in the case of RNN vectors are indicative of the fact that these vectors encode syntactic information as explained in §6 which might not generalize well as compared to semantic information. 7 Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in</context>
</contexts>
<marker>Dyvik, 2004</marker>
<rawString>Helge Dyvik. 2004. Translations as semantic mirrors: from parallel corpus to wordnet. Language and Computers, 49(1):311–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>An information theoretic approach to bilingual word clustering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>777--783</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="25410" citStr="Faruqui and Dyer, 2013" startWordPosition="4208" endWordPosition="4211">er continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). 8 Conclusion We have presented a canonical correlation analysis based method for incorporating multilingual context into word representations generated using only monolingual information and shown its applicability across three different ways of generating monolingual vectors on a variety of evaluation benchmarks. These word representations obtained after using multilingual evidence perform significantly better on the evaluation tasks compared to the monolingual vectors. We have also shown that our method is more suitable for vectors that encode semantic information than those that encode sy</context>
</contexts>
<marker>Faruqui, Dyer, 2013</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2013. An information theoretic approach to bilingual word clustering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 777–783, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2001</date>
<booktitle>In WWW ’01: Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>406--414</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9326" citStr="Finkelstein et al., 2001" startWordPosition="1527" endWordPosition="1530">en reconstruction error and number of parameters, *k is the diagonal matrix containing the top k singular values, Uk is the matrix produced by selecting the corresponding columns from U and A represents the new matrix containing word vector representations in the reduced dimensional space. 4 Word Representation Evaluation We evaluate the quality of our word vector representations on a number of tasks that test how well 4.1 Word Similarity We evaluate our word representations on four different benchmarks that have been widely used to measure word similarity. The first one is the WS353 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans. This data was further divided into two fragments by Agirre et al. (2009) who claimed that similarity (WS-SIM) and relatedness (WS-REL) are two different kinds of relations and should be dealt with separately. We present results on the whole set and on the individual fragments as well. The second and third benchmarks are the RG65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) datasets that contain 65 and 30 pairs of nouns respectively and have been given similarity rankings b</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. In WWW ’01: Proceedings of the 10th international conference on World Wide Web, pages 406–414, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis,</title>
<date>1957</date>
<pages>1--32</pages>
<marker>Firth, 1957</marker>
<rawString>J.R. Firth. 1957. A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis, pages 1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene H Golub</author>
<author>Charles F Van Loan</author>
</authors>
<date>1996</date>
<booktitle>Matrix computations (3rd ed.). Johns Hopkins</booktitle>
<publisher>University Press,</publisher>
<location>Baltimore, MD, USA.</location>
<marker>Golub, Van Loan, 1996</marker>
<rawString>Gene H. Golub and Charles F. Van Loan. 1996. Matrix computations (3rd ed.). Johns Hopkins University Press, Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="24219" citStr="Haghighi et al. (2008)" startWordPosition="4026" endWordPosition="4029">or space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a multitask learning framework to encourage the word representations learned by neural language models to agree cross-lingually. CCA can be used for dimension reduction and to draw correspondences between two sets of data.Haghighi et al. (2008) use CCA to draw translation lexicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that capture aspects of word meaning and different types of distributional profile of the word (Dhillon et al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1605" citStr="Harris, 1954" startWordPosition="218" endWordPosition="219"> capture lexico-semantic properties is a technique of central importance in natural language processing. Using cooccurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. (1957) as You shall know a word by the company it keeps. Although there is much evidence in favor of the distributional hypothesis, in this paper we argue for incorporating translational context when constructing vector space semantic models (VSMs). Simply put: knowing how words tra</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24915" citStr="Huang et al., 2012" startWordPosition="4135" endWordPosition="4138">s using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that capture aspects of word meaning and different types of distributional profile of the word (Dhillon et al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). 8 Conclusion We have presented a canonical correlation analysis based method for incorporating multilin</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering,</title>
<date>2005</date>
<pages>11--11</pages>
<contexts>
<context position="23027" citStr="Hwa et al., 2005" startWordPosition="3834" endWordPosition="3837">nificantly different for every language pair. SG vectors show a uniform pattern across different language pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case (k = 80). The fact that both SG and SVD vectors have similar behavior across language pairs can be treated as evidence that semantics or information at a conceptual level (since both of them basically model word cooccurrence counts) transfers well across languages (Dyvik, 2004) although syntax has been projected across languages as well (Hwa et al., 2005; Yarowsky and Ngai, 2001). The pattern of results in the case of RNN vectors are indicative of the fact that these vectors encode syntactic information as explained in §6 which might not generalize well as compared to semantic information. 7 Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlati</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, 11:11–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="23970" citStr="Klementiev et al. (2012)" startWordPosition="3989" endWordPosition="3992">y associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a multitask learning framework to encourage the word representations learned by neural language models to agree cross-lingually. CCA can be used for dimension reduction and to draw correspondences between two sets of data.Haghighi et al. (2008) use CCA to draw translation lexicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that capture aspects of word meaning and different types of distributional profile of the word (Dhillon et al., 2011). Although our primary</context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="1824" citStr="Mikolov et al., 2013" startWordPosition="246" endWordPosition="249">),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. (1957) as You shall know a word by the company it keeps. Although there is much evidence in favor of the distributional hypothesis, in this paper we argue for incorporating translational context when constructing vector space semantic models (VSMs). Simply put: knowing how words translate is a valuable source of lexico-semantic information and should lead to better VSMs. Parallel corpora have long been recognized as valuable for lexical semantic applications, including identifying word senses (Dia</context>
<context position="10636" citStr="Mikolov et al. (2013" startWordPosition="1739" endWordPosition="1742">ins all kinds of words. The fourth benchmark is the MTurk-287 (Radinsky et al., 2011) dataset that constitutes of 287 pairs of words and is different from the above two benchmarks in that it has been constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk. We calculate similarity between a given pair of words by the cosine similarity between their corresponding vector representation. We then report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. 4.2 Semantic Relations (SEM-REL) Mikolov et al. (2013a) present a new semantic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common semantic relation. For example, in England : London :: France : Paris, the two given pairs of words follow the country-capital relation. There are three other such kinds of relations: country-currency, man-woman, city-in-state and overall 8869 such pairs of words4. The task here is to find a word d that best fits the following relationship: a : b :: c : d given a, b and c. We use the vector offset method described 4107 pairs were out of vocabulary for </context>
<context position="17949" citStr="Mikolov et al., 2013" startWordPosition="2967" endWordPosition="2970"> vector lengths strengthening the reliability of our approach. 6 Neural Network Word Representations Other kinds of vectors shown to be useful in many NLP tasks are word embeddings obtained from neural networks. These word embeddings capture more complex information than just co-occurrence counts as explained in the next section. We test our multilingual projection method on two types of such vectors by keeping the experimental setting exactly the same as in §5.2. 6.1 RNN Vectors The recurrent neural network language model maximizes the log-likelihood of the training corpus. The architecture (Mikolov et al., 2013b) consists of an input layer, a hidden layer with recurrent connections to itself, an output layer and the corresponding weight matrices. The input vector w(t) represents input word at time t encoded using 1-of-N encoding and the output layer y(t) produces a probability distribution over words in the vocabulary V . The hidden layer maintains a representation of the sentence history in s(t). The values in the hidden and output layer are computed as follows: s(t) = f(Uw(t) + Ws(t − 1)) (9) y(t) = g(V s(t)) (10) where, f and g are the logistic and softmax functions respectively. U and V are weig</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>746--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1824" citStr="Mikolov et al., 2013" startWordPosition="246" endWordPosition="249">),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. (1957) as You shall know a word by the company it keeps. Although there is much evidence in favor of the distributional hypothesis, in this paper we argue for incorporating translational context when constructing vector space semantic models (VSMs). Simply put: knowing how words translate is a valuable source of lexico-semantic information and should lead to better VSMs. Parallel corpora have long been recognized as valuable for lexical semantic applications, including identifying word senses (Dia</context>
<context position="10636" citStr="Mikolov et al. (2013" startWordPosition="1739" endWordPosition="1742">ins all kinds of words. The fourth benchmark is the MTurk-287 (Radinsky et al., 2011) dataset that constitutes of 287 pairs of words and is different from the above two benchmarks in that it has been constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk. We calculate similarity between a given pair of words by the cosine similarity between their corresponding vector representation. We then report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. 4.2 Semantic Relations (SEM-REL) Mikolov et al. (2013a) present a new semantic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common semantic relation. For example, in England : London :: France : Paris, the two given pairs of words follow the country-capital relation. There are three other such kinds of relations: country-currency, man-woman, city-in-state and overall 8869 such pairs of words4. The task here is to find a word d that best fits the following relationship: a : b :: c : d given a, b and c. We use the vector offset method described 4107 pairs were out of vocabulary for </context>
<context position="17949" citStr="Mikolov et al., 2013" startWordPosition="2967" endWordPosition="2970"> vector lengths strengthening the reliability of our approach. 6 Neural Network Word Representations Other kinds of vectors shown to be useful in many NLP tasks are word embeddings obtained from neural networks. These word embeddings capture more complex information than just co-occurrence counts as explained in the next section. We test our multilingual projection method on two types of such vectors by keeping the experimental setting exactly the same as in §5.2. 6.1 RNN Vectors The recurrent neural network language model maximizes the log-likelihood of the training corpus. The architecture (Mikolov et al., 2013b) consists of an input layer, a hidden layer with recurrent connections to itself, an output layer and the corresponding weight matrices. The input vector w(t) represents input word at time t encoded using 1-of-N encoding and the output layer y(t) produces a probability distribution over words in the vocabulary V . The hidden layer maintains a representation of the sentence history in s(t). The values in the hidden and output layer are computed as follows: s(t) = f(Uw(t) + Ws(t − 1)) (9) y(t) = g(V s(t)) (10) where, f and g are the logistic and softmax functions respectively. U and V are weig</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="9824" citStr="Miller and Charles, 1991" startWordPosition="1610" endWordPosition="1613">benchmarks that have been widely used to measure word similarity. The first one is the WS353 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans. This data was further divided into two fragments by Agirre et al. (2009) who claimed that similarity (WS-SIM) and relatedness (WS-REL) are two different kinds of relations and should be dealt with separately. We present results on the whole set and on the individual fragments as well. The second and third benchmarks are the RG65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) datasets that contain 65 and 30 pairs of nouns respectively and have been given similarity rankings by humans. These differ from WS-353 in that it contains only nouns whereas the former contains all kinds of words. The fourth benchmark is the MTurk-287 (Radinsky et al., 2011) dataset that constitutes of 287 pairs of words and is different from the above two benchmarks in that it has been constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk. We calculate similarity between a given pair of words by the cosine similarity between their corresponding vector repres</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In In NIPS.</booktitle>
<contexts>
<context position="24870" citStr="Mnih and Hinton, 2008" startWordPosition="4127" endWordPosition="4130">exicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that capture aspects of word meaning and different types of distributional profile of the word (Dhillon et al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). 8 Conclusion We have presented a canonical correlation ana</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2008. A scalable hierarchical distributed language model. In In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In AISTATS05,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="24894" citStr="Morin and Bengio, 2005" startWordPosition="4131" endWordPosition="4134">f two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that capture aspects of word meaning and different types of distributional profile of the word (Dhillon et al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). 8 Conclusion We have presented a canonical correlation analysis based method for i</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In AISTATS05, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome L Myers</author>
<author>Arnold D Well</author>
</authors>
<date>1995</date>
<journal>Research Design &amp; Statistical Analysis. Routledge,</journal>
<volume>1</volume>
<pages>edition,</pages>
<contexts>
<context position="10511" citStr="Myers and Well, 1995" startWordPosition="1718" endWordPosition="1722">ave been given similarity rankings by humans. These differ from WS-353 in that it contains only nouns whereas the former contains all kinds of words. The fourth benchmark is the MTurk-287 (Radinsky et al., 2011) dataset that constitutes of 287 pairs of words and is different from the above two benchmarks in that it has been constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk. We calculate similarity between a given pair of words by the cosine similarity between their corresponding vector representation. We then report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. 4.2 Semantic Relations (SEM-REL) Mikolov et al. (2013a) present a new semantic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common semantic relation. For example, in England : London :: France : Paris, the two given pairs of words follow the country-capital relation. There are three other such kinds of relations: country-currency, man-woman, city-in-state and overall 8869 such pairs of words4. The task here is to find a word d that best fits the following re</context>
</contexts>
<marker>Myers, Well, 1995</marker>
<rawString>Jerome L. Myers and Arnold D. Well. 1995. Research Design &amp; Statistical Analysis. Routledge, 1 edition, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Crosslingual induction of selectional preferences with bilingual vector spaces.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>921--929</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Peirsman, Pad´o, 2010</marker>
<rawString>Yves Peirsman and Sebastian Pad´o. 2010. Crosslingual induction of selectional preferences with bilingual vector spaces. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 921–929, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eugene Agichtein</author>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>A word at a time: computing word relatedness using temporal semantic analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web, WWW ’11,</booktitle>
<pages>337--346</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10101" citStr="Radinsky et al., 2011" startWordPosition="1656" endWordPosition="1659">t al. (2009) who claimed that similarity (WS-SIM) and relatedness (WS-REL) are two different kinds of relations and should be dealt with separately. We present results on the whole set and on the individual fragments as well. The second and third benchmarks are the RG65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) datasets that contain 65 and 30 pairs of nouns respectively and have been given similarity rankings by humans. These differ from WS-353 in that it contains only nouns whereas the former contains all kinds of words. The fourth benchmark is the MTurk-287 (Radinsky et al., 2011) dataset that constitutes of 287 pairs of words and is different from the above two benchmarks in that it has been constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk. We calculate similarity between a given pair of words by the cosine similarity between their corresponding vector representation. We then report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. 4.2 Semantic Relations (SEM-REL) Mikolov et al. (2013a) present a new semantic relation dataset composed of analogous </context>
</contexts>
<marker>Radinsky, Agichtein, Gabrilovich, Markovitch, 2011</marker>
<rawString>Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: computing word relatedness using temporal semantic analysis. In Proceedings of the 20th international conference on World wide web, WWW ’11, pages 337–346, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>Distinguishing systems and distinguishing senses: new evaluation methods for word sense disambiguation.</title>
<date>1999</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="2459" citStr="Resnik and Yarowsky, 1999" startWordPosition="342" endWordPosition="345">ive at vector representations that likewise capture cooccurence tendencies and meanings. (1957) as You shall know a word by the company it keeps. Although there is much evidence in favor of the distributional hypothesis, in this paper we argue for incorporating translational context when constructing vector space semantic models (VSMs). Simply put: knowing how words translate is a valuable source of lexico-semantic information and should lead to better VSMs. Parallel corpora have long been recognized as valuable for lexical semantic applications, including identifying word senses (Diab, 2003; Resnik and Yarowsky, 1999) and paraphrase and synonymy relationships (Bannard and CallisonBurch, 2005). The latter work (which we build on) shows that if different words or phrases in one language often translate into a single word or phrase type in a second language, this is good evidence that they are synonymous. To illustrate: the English word forms aeroplane, airplane, and plane are observed to translate into the same Hindi word: TTZJ ZT-T (vaayuyaan). Thus, even if we did not know the relationship between the English words, this translation fact is evidence that they all have the same meaning. How can we exploit i</context>
</contexts>
<marker>Resnik, Yarowsky, 1999</marker>
<rawString>Philip Resnik and David Yarowsky. 1999. Distinguishing systems and distinguishing senses: new evaluation methods for word sense disambiguation. Nat. Lang. Eng., 5(2):113–133, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Commun. ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="9783" citStr="Rubenstein and Goodenough, 1965" startWordPosition="1603" endWordPosition="1606">uate our word representations on four different benchmarks that have been widely used to measure word similarity. The first one is the WS353 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans. This data was further divided into two fragments by Agirre et al. (2009) who claimed that similarity (WS-SIM) and relatedness (WS-REL) are two different kinds of relations and should be dealt with separately. We present results on the whole set and on the individual fragments as well. The second and third benchmarks are the RG65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) datasets that contain 65 and 30 pairs of nouns respectively and have been given similarity rankings by humans. These differ from WS-353 in that it contains only nouns whereas the former contains all kinds of words. The fourth benchmark is the MTurk-287 (Radinsky et al., 2011) dataset that constitutes of 287 pairs of words and is different from the above two benchmarks in that it has been constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk. We calculate similarity between a given pair of words by the cosine similarity </context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM, 8(10):627–633, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiichiro Sumita</author>
</authors>
<title>Lexical transfer using a vectorspace model.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00,</booktitle>
<pages>425--431</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="25056" citStr="Sumita, 2000" startWordPosition="4155" endWordPosition="4156"> aspects of word meaning and different types of distributional profile of the word (Dhillon et al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). 8 Conclusion We have presented a canonical correlation analysis based method for incorporating multilingual context into word representations generated using only monolingual information and shown its applicability across three different ways o</context>
</contexts>
<marker>Sumita, 2000</marker>
<rawString>Eiichiro Sumita. 2000. Lexical transfer using a vectorspace model. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, pages 425–431, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>page</pages>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, volume 1, page 11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1446" citStr="Turian et al., 2010" startWordPosition="194" endWordPosition="197">ur method produces substantially better semantic representations than monolingual techniques. 1 Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using cooccurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. (1957) as You shall know a word by the company it keeps. Although there is much evidence in favor of the distributional hypo</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning : Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>141--188</pages>
<contexts>
<context position="1205" citStr="Turney and Pantel, 2010" startWordPosition="157" endWordPosition="160">le technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually. We evaluate the resulting word representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques. 1 Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using cooccurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (M</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning : Vector space models of semantics. Journal of Artificial Intelligence Research, pages 141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning, EMCL ’01,</booktitle>
<pages>491--502</pages>
<publisher>UK. SpringerVerlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="8094" citStr="Turney, 2001" startWordPosition="1310" endWordPosition="1311">ord co-occurrence frequency matrix F for a given training corpus where each row w, represents one word in the corpus and every column c, is the context feature in which the word is observed. In our case, every column is a word which occurs in a given window length around the target word. For scalability reasons, we only select words with frequency greater than 10 as features. We also remove the top 100 most frequent words (mostly stop words) from the column features. We then replace every entry in the sparse frequency matrix F by its pointwise mutual information (PMI) (Church and Hanks, 1990; Turney, 2001) resulting in X. PMI is designed to give a high value to xij where there is a interesting relation between wi and cj, a small or negative value of xij indicates that the occurrence of wi in cj is uninformative. Finally, we factorize the matrix X using singular value decomposition (SVD). SVD decomposes X into the product of three matrices: X = U*VT (7) where, U and V are in column orthonormal form and * is a diagonal matrix of singular values (Golub and Van Loan, 1996). We obtain a reduced dimensional representation of words from size IVI to k: A = Uk*k (8) where k can be controlled to trade of</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In Proceedings of the 12th European Conference on Machine Learning, EMCL ’01, pages 491–502, London, UK, UK. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Comput. Linguist.,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1358" citStr="Turney, 2006" startWordPosition="182" endWordPosition="183">rd representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques. 1 Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using cooccurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. (1957) as You shall know a word by t</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Comput. Linguist., 32(3):379–416, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing Data using t-SNE.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--2579</pages>
<marker>van der Maaten, Hinton, 2008</marker>
<rawString>Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine Learning Research, 9:2579–2605, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Marie-Francine Moens</author>
</authors>
<title>A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else).</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1613--1624</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<marker>Vuli´c, Moens, 2013</marker>
<rawString>Ivan Vuli´c and Marie-Francine Moens. 2013. A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else). In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1613–1624, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23053" citStr="Yarowsky and Ngai, 2001" startWordPosition="3838" endWordPosition="3841">nt for every language pair. SG vectors show a uniform pattern across different language pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case (k = 80). The fact that both SG and SVD vectors have similar behavior across language pairs can be treated as evidence that semantics or information at a conceptual level (since both of them basically model word cooccurrence counts) transfers well across languages (Dyvik, 2004) although syntax has been projected across languages as well (Hwa et al., 2005; Yarowsky and Ngai, 2001). The pattern of results in the case of RNN vectors are indicative of the fact that these vectors encode syntactic information as explained in §6 which might not generalize well as compared to semantic information. 7 Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01, pages 1– 8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>Bitam: Bilingual topic admixture models for word alignment. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL06.</booktitle>
<contexts>
<context position="25156" citStr="Zhao and Xing, 2006" startWordPosition="4169" endWordPosition="4172">t al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). 8 Conclusion We have presented a canonical correlation analysis based method for incorporating multilingual context into word representations generated using only monolingual information and shown its applicability across three different ways of generating monolingual vectors on a variety of evaluation benchmarks. These word representations o</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual topic admixture models for word alignment. In In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
<author>Alex Waibel</author>
</authors>
<title>Bilingual word spectral clustering for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts, ParaText ’05,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="25322" citStr="Zhao et al., 2005" startWordPosition="4196" endWordPosition="4199">ased word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). 8 Conclusion We have presented a canonical correlation analysis based method for incorporating multilingual context into word representations generated using only monolingual information and shown its applicability across three different ways of generating monolingual vectors on a variety of evaluation benchmarks. These word representations obtained after using multilingual evidence perform significantly better on the evaluation tasks compared to the monolingual vectors. We have also shown that our method</context>
</contexts>
<marker>Zhao, Xing, Waibel, 2005</marker>
<rawString>Bing Zhao, Eric P. Xing, and Alex Waibel. 2005. Bilingual word spectral clustering for statistical machine translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, ParaText ’05, pages 25–32, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1393--1398</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="23379" citStr="Zou et al. (2013)" startWordPosition="3892" endWordPosition="3895"> can be treated as evidence that semantics or information at a conceptual level (since both of them basically model word cooccurrence counts) transfers well across languages (Dyvik, 2004) although syntax has been projected across languages as well (Hwa et al., 2005; Yarowsky and Ngai, 2001). The pattern of results in the case of RNN vectors are indicative of the fact that these vectors encode syntactic information as explained in §6 which might not generalize well as compared to semantic information. 7 Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a mu</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393– 1398, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>