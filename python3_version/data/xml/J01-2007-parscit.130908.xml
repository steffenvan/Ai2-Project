<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004181">
<note confidence="0.294162">
Book Reviews
</note>
<title confidence="0.9561215">
Natural Language Processing and Knowledge Representation:
Language for Knowledge and Knowledge for Language
</title>
<author confidence="0.631826">
Lucja M. Iwatiska and Stuart C. Shapiro (editors)
</author>
<affiliation confidence="0.421215333333333">
(Wayne State University and State University of New York at Buffalo)
Menlo Park, CA and Cambridge, MA:
AAAI Press and The MIT Press, 2000,
</affiliation>
<figure confidence="0.61693775">
xix+460 pp; paperbound, ISBN
0-262-59021-2, $40.00
Reviewed by
Robert E. Mercer
</figure>
<affiliation confidence="0.692295">
University of Western Ontario
</affiliation>
<bodyText confidence="0.999852714285714">
The strong connection between knowledge representation and reasoning and natu-
ral language is the main theme of this book. The book is divided into two parts.
Part I, consisting of five original or updated papers, considers the connection from the
viewpoint of knowledge for language. The six original or updated papers in Part II
describe using language to gather or enrich knowledge. Most of the papers discuss
significant software systems that are capable of dealing with a variety of corpora. A
brief overview of the eleven papers will give some indication of the coverage of the
book.
The following five papers constitute Part I of the book. Lucia M. Iwanska, in
&amp;quot;Natural language is a powerful knowledge representation system: The UNO sys-
tem,&amp;quot; argues for her conjecture that natural language is a powerful representational
language which is particularly suitable for representing and using knowledge in not-
well-formalized domains. David A. McAllester and Robert Givan, in &amp;quot;Natural lan-
guage syntax and first order logic,&amp;quot; present a polynomial-time (incomplete) inference
procedure for first-order logic given in a nonstandard syntactic representation that
takes advantage of properties found in natural language syntax. David D. McDon-
ald, in &amp;quot;Issues in the representation of real texts: The design of KRISP,&amp;quot; discusses
the need for fundamental changes in our representational languages in order to deal
with the analysis and synthesis of real texts in real time. Lenhart K. Schubert and
Chung Hee Hwang, in &amp;quot;Episodic logic meets Little Red Riding Hood—A compre-
hensive natural representation for language understanding,&amp;quot; describe their logic for
representing discourse content and linguistic and world knowledge that readily cap-
tures the meaning of the text and enables appropriate inferences to be made. Stuart
C. Shapiro, in &amp;quot;SNePS: A logic for natural language understanding and commonsense
reasoning,&amp;quot; describes a number of features that are incorporated in the latest version
of SNePS.
Although the papers in this part vary somewhat in what they espouse, the primary
theme is the value of appropriate representation. It is this aspect which is the important
contribution of this section of the book. Although the value of representation is obvious
in McAllester and Givan&apos;s paper, the theme of their paper is somewhat different from
that of the other four, which describe working natural language understanding or
processing systems. Shapiro&apos;s paper is a terse list of some features found in SNePS
2.4, giving some motivation and a few examples. The remaining three (significantly
longer) papers carefully motivate their respective positions and show the applicability
of their representational languages with numerous examples.
</bodyText>
<page confidence="0.988185">
295
</page>
<note confidence="0.633964">
Computational Linguistics Volume 27, Number 2
</note>
<bodyText confidence="0.999960078431373">
The next three papers form the first subtheme of Part II of the book, a discus-
sion of uniform and nonuniform representation and reasoning frameworks. These
three contributions describe significant natural language systems for machine trans-
lation, reasoning about space and time, and dialogue processing in a tutoring sys-
tem, respectively. Bonnie J. Dorr and Clare R. Voss, in &amp;quot;A multi-level approach to
interlingual machine translation: Defining the interface between representational lan-
guages,&amp;quot; argue that a variety of knowledge representation types is advantageous in
their machine translation task. Lucja M. Iwariska, in &amp;quot;Uniform natural (language)
spatio-temporal logic: Reasoning about absolute and relative space and time,&amp;quot; proposes
a uniform knowledge representation and reasoning approach for spatio-temporal rea-
soning, again returning to her conjecture from Chapter 1 that natural language pro-
vides the appropriate representation and reasoning mechanism. Susan W. McRoy,
Syed S. Ali, and Susan M. Haller, in &amp;quot;Mixed depth representations for dialog process-
ing,&amp;quot; use a uniform knowledge-representation framework to reason about the domain
knowledge and the discourse in their tutoring system.
The final three papers constitute the second subtheme of Part II. The underly-
ing theme of two of the papers is the use of linguistic knowledge to enrich knowl-
edge representation or knowledge acquisition techniques (the paper by Iwariska, Mata,
and Kruger does not use &amp;quot;a priori existing knowledge&amp;quot; [p. 3361 to acquire taxonomic
knowledge from texts). Sanda M. Harabagiu and Dan I. Moldovan, in &amp;quot;Enriching the
WordNet taxonomy with contextual knowledge acquired from text,&amp;quot; discuss their con-
textualization of concepts in WordNet, enabling pragmatic inferences such as Gricean
implicatures to be derived. Lucja M. Iwatiska, Naveen Mata, and Kellyn Kruger, in
&amp;quot;Fully automatic acquisition of taxonomic knowledge from large corpora of texts:
Limited-syntax knowledge representation system based on natural language,&amp;quot; employ
weak, local-context-based methods to extract taxonomic knowledge from text to be
represented using the UNO representation language that was introduced in Iwariska&apos;s
first paper in the book. William J. Rapaport and Karen Ehrlich, in &amp;quot;A computational
theory of vocabulary acquisition,&amp;quot; use context (surrounding text, grammatical infor-
mation, and background knowledge) to learn the meaning of new words (or word
senses) encountered in text.
The papers in this collection, for the most part, describe major projects, some that
span one or two decades (and for some, their roots can be traced further). The com-
ments that I make below should not be taken to reflect on the authors&apos; contributions
to this book.
In the preface, the editors state that this book &amp;quot;contains the most recent theoreti-
cal and practical computational approaches to representing and utilizing the meaning
of natural language.&amp;quot; Unfortunately, the book seems to have taken some time to be
published. This delay is evident in a number of ways: Most of the papers have no
references beyond 1997 and the few that do appear are self-references. Also, a table in
the preface noting some professional activities associated with the topic of the book
ends with a 1997 entry. The editors&apos; claim is also undermined by the fact that two
of the papers are updated versions of papers from Expert Systems 1996 and one is
an update of a 1992 Artificial Intelligence paper. If the 1997 date is not a coincidence,
there has been more recent work done. For example, the publication of Blackburn et
al. 1998 marks the beginning of the DORIS project at the University of the Saarland
(http: //www.coli.uni-sb.de/ —bos/doris I). DRT-oriented and based on classical log-
ics (representation and inference), it would provide a different perspective on this
discussion, somewhat akin to the uniform-nonuniform dichotomy in Part II.
In addition to the eleven papers, three appendices are included &amp;quot;to make this book
self-contained&amp;quot; (p. xvii). Unfortunately, Appendix A, entitled &amp;quot;Propositional, First-
</bodyText>
<page confidence="0.991797">
296
</page>
<subsectionHeader confidence="0.837073">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999841166666667">
Order and Higher-Order Logics,&amp;quot; gives eight pages each to propositional and first-
order logic (which the editors state are inadequate for natural language) and only half
a page to the last topic which is much better suited to the task. Also contributing to
the lack of self-containedness (but in a minor way) is the lack of cross-references to the
papers contained in the book even though references to prior work by contributors
are made (especially in Part I).
The editors might have chosen a slightly different set of papers (four of the eleven
papers are authored or coauthored by the editors). The DORIS project, mentioned
earlier, would be a candidate for Part I.
Because all of the bibliographies have been moved to the end of the book, I would
have liked to have seen the bibliographic entries in the index. Having a reverse index
of the references is appealing when the bibliography is presented in this manner.
</bodyText>
<reference confidence="0.873668555555556">
Reference language understanding. CADE-15
Blackburn, Patrick, Johan Bos, Michael Workshop Problem-solving Methodologies
Kohlhase, and Hans de Nivelle. 1998. with Automated Deduction.
Automated theorem proving for natural
Robert Mercer is an Associate Professor in the Department of Computer Science at The University
of Western Ontario. His interests include nonmonotonic logics and their use in the representa-
tion of natural language knowledge and reasoning. Mercer&apos;s address is Department of Com-
puter Science, Middlesex College, The University of Western Ontario, London, Ontario, Canada
N6A 5B7; e-mail: mercer@csd.uwo.ca.
</reference>
<page confidence="0.997413">
297
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.111207">
<title confidence="0.998433666666667">Book Reviews Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language</title>
<author confidence="0.996926">Lucja M Iwatiska</author>
<author confidence="0.996926">Stuart C Shapiro</author>
<affiliation confidence="0.97125">(Wayne State University and State University of New York at Buffalo)</affiliation>
<address confidence="0.909136">Menlo Park, CA and Cambridge, MA:</address>
<note confidence="0.872942">AAAI Press and The MIT Press, 2000, xix+460 pp; paperbound, ISBN 0-262-59021-2, $40.00 Reviewed by</note>
<author confidence="0.999974">Robert E Mercer</author>
<affiliation confidence="0.997198">University of Western Ontario</affiliation>
<abstract confidence="0.99371918627451">The strong connection between knowledge representation and reasoning and natural language is the main theme of this book. The book is divided into two parts. Part I, consisting of five original or updated papers, considers the connection from the viewpoint of knowledge for language. The six original or updated papers in Part II describe using language to gather or enrich knowledge. Most of the papers discuss significant software systems that are capable of dealing with a variety of corpora. A brief overview of the eleven papers will give some indication of the coverage of the book. following five papers constitute Part the book. Lucia M. Iwanska, in &amp;quot;Natural language is a powerful knowledge representation system: The UNO system,&amp;quot; argues for her conjecture that natural language is a powerful representational language which is particularly suitable for representing and using knowledge in notwell-formalized domains. David A. McAllester and Robert Givan, in &amp;quot;Natural language syntax and first order logic,&amp;quot; present a polynomial-time (incomplete) inference procedure for first-order logic given in a nonstandard syntactic representation that takes advantage of properties found in natural language syntax. David D. McDonald, in &amp;quot;Issues in the representation of real texts: The design of KRISP,&amp;quot; discusses the need for fundamental changes in our representational languages in order to deal with the analysis and synthesis of real texts in real time. Lenhart K. Schubert and Chung Hee Hwang, in &amp;quot;Episodic logic meets Little Red Riding Hood—A comprehensive natural representation for language understanding,&amp;quot; describe their logic for representing discourse content and linguistic and world knowledge that readily captures the meaning of the text and enables appropriate inferences to be made. Stuart C. Shapiro, in &amp;quot;SNePS: A logic for natural language understanding and commonsense reasoning,&amp;quot; describes a number of features that are incorporated in the latest version of SNePS. Although the papers in this part vary somewhat in what they espouse, the primary theme is the value of appropriate representation. It is this aspect which is the important contribution of this section of the book. Although the value of representation is obvious in McAllester and Givan&apos;s paper, the theme of their paper is somewhat different from that of the other four, which describe working natural language understanding or processing systems. Shapiro&apos;s paper is a terse list of some features found in SNePS 2.4, giving some motivation and a few examples. The remaining three (significantly longer) papers carefully motivate their respective positions and show the applicability of their representational languages with numerous examples. 295 Computational Linguistics Volume 27, Number 2 The next three papers form the first subtheme of Part II of the book, a discussion of uniform and nonuniform representation and reasoning frameworks. These three contributions describe significant natural language systems for machine translation, reasoning about space and time, and dialogue processing in a tutoring system, respectively. Bonnie J. Dorr and Clare R. Voss, in &amp;quot;A multi-level approach to interlingual machine translation: Defining the interface between representational languages,&amp;quot; argue that a variety of knowledge representation types is advantageous in their machine translation task. Lucja M. Iwariska, in &amp;quot;Uniform natural (language) spatio-temporal logic: Reasoning about absolute and relative space and time,&amp;quot; proposes a uniform knowledge representation and reasoning approach for spatio-temporal reasoning, again returning to her conjecture from Chapter 1 that natural language provides the appropriate representation and reasoning mechanism. Susan W. McRoy, Syed S. Ali, and Susan M. Haller, in &amp;quot;Mixed depth representations for dialog processing,&amp;quot; use a uniform knowledge-representation framework to reason about the domain knowledge and the discourse in their tutoring system. The final three papers constitute the second subtheme of Part II. The underlying theme of two of the papers is the use of linguistic knowledge to enrich knowledge representation or knowledge acquisition techniques (the paper by Iwariska, Mata, and Kruger does not use &amp;quot;a priori existing knowledge&amp;quot; [p. 3361 to acquire taxonomic knowledge from texts). Sanda M. Harabagiu and Dan I. Moldovan, in &amp;quot;Enriching the WordNet taxonomy with contextual knowledge acquired from text,&amp;quot; discuss their contextualization of concepts in WordNet, enabling pragmatic inferences such as Gricean implicatures to be derived. Lucja M. Iwatiska, Naveen Mata, and Kellyn Kruger, in &amp;quot;Fully automatic acquisition of taxonomic knowledge from large corpora of texts: Limited-syntax knowledge representation system based on natural language,&amp;quot; employ weak, local-context-based methods to extract taxonomic knowledge from text to be represented using the UNO representation language that was introduced in Iwariska&apos;s first paper in the book. William J. Rapaport and Karen Ehrlich, in &amp;quot;A computational theory of vocabulary acquisition,&amp;quot; use context (surrounding text, grammatical information, and background knowledge) to learn the meaning of new words (or word senses) encountered in text. The papers in this collection, for the most part, describe major projects, some that span one or two decades (and for some, their roots can be traced further). The comments that I make below should not be taken to reflect on the authors&apos; contributions to this book. In the preface, the editors state that this book &amp;quot;contains the most recent theoretical and practical computational approaches to representing and utilizing the meaning of natural language.&amp;quot; Unfortunately, the book seems to have taken some time to be published. This delay is evident in a number of ways: Most of the papers have no references beyond 1997 and the few that do appear are self-references. Also, a table in the preface noting some professional activities associated with the topic of the book ends with a 1997 entry. The editors&apos; claim is also undermined by the fact that two the papers are updated versions of papers from Systems and one is update of a 1992 Intelligence If the 1997 date is not a coincidence, there has been more recent work done. For example, the publication of Blackburn et al. 1998 marks the beginning of the DORIS project at the University of the Saarland (http: //www.coli.uni-sb.de/ —bos/doris I). DRT-oriented and based on classical logics (representation and inference), it would provide a different perspective on this discussion, somewhat akin to the uniform-nonuniform dichotomy in Part II. In addition to the eleven papers, three appendices are included &amp;quot;to make this book (p. xvii). Unfortunately, Appendix A, entitled &amp;quot;Propositional, First- 296 Book Reviews Order and Higher-Order Logics,&amp;quot; gives eight pages each to propositional and firstorder logic (which the editors state are inadequate for natural language) and only half a page to the last topic which is much better suited to the task. Also contributing to the lack of self-containedness (but in a minor way) is the lack of cross-references to the papers contained in the book even though references to prior work by contributors are made (especially in Part I). The editors might have chosen a slightly different set of papers (four of the eleven papers are authored or coauthored by the editors). The DORIS project, mentioned earlier, would be a candidate for Part I. Because all of the bibliographies have been moved to the end of the book, I would have liked to have seen the bibliographic entries in the index. Having a reverse index of the references is appealing when the bibliography is presented in this manner.</abstract>
<note confidence="0.46109725">Reference understanding. Workshop Problem-solving Methodologies with Automated Deduction. Blackburn, Patrick, Johan Bos, Michael Kohlhase, and Hans de Nivelle. 1998. Automated theorem proving for natural Mercer an Associate Professor in the Department of Computer Science at The University of Western Ontario. His interests include nonmonotonic logics and their use in the representation of natural language knowledge and reasoning. Mercer&apos;s address is Department of Computer Science, Middlesex College, The University of Western Ontario, London, Ontario, Canada N6A 5B7; e-mail: mercer@csd.uwo.ca. 297</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Reference Blackburn</author>
<author>Johan Bos Patrick</author>
<author>Michael Kohlhase</author>
<author>Hans de Nivelle</author>
</authors>
<title>Automated theorem proving for natural language understanding.</title>
<date>1998</date>
<booktitle>CADE-15 Workshop Problem-solving Methodologies with Automated Deduction.</booktitle>
<marker>Blackburn, Patrick, Kohlhase, de Nivelle, 1998</marker>
<rawString>Reference Blackburn, Patrick, Johan Bos, Michael Kohlhase, and Hans de Nivelle. 1998. Automated theorem proving for natural language understanding. CADE-15 Workshop Problem-solving Methodologies with Automated Deduction.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>