<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.848287">
On the Role of Lexical Features in Sequence Labeling
</title>
<author confidence="0.964413">
Yoav Goldberg∗ and Michael Elhadad
</author>
<affiliation confidence="0.996785">
Ben Gurion University of the Negev
Department of Computer Science
</affiliation>
<address confidence="0.902971">
POB 653 Be’er Sheva, 84105, Israel
</address>
<email confidence="0.999037">
{yoavg|elhadad}@cs.bgu.ac.il
</email>
<sectionHeader confidence="0.994805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988147826087">
We use the technique of SVM anchoring to
demonstrate that lexical features extracted
from a training corpus are not necessary to
obtain state of the art results on tasks such
as Named Entity Recognition and Chunk-
ing. While standard models require as
many as 100K distinct features, we derive
models with as little as 1K features that
perform as well or better on different do-
mains. These robust reduced models in-
dicate that the way rare lexical features
contribute to classification in NLP is not
fully understood. Contrastive error analy-
sis (with and without lexical features) in-
dicates that lexical features do contribute
to resolving some semantic and complex
syntactic ambiguities – but we find this
contribution does not generalize outside
the training corpus. As a general strat-
egy, we believe lexical features should not
be directly derived from a training corpus
but instead, carefully inferred and selected
from other sources.
</bodyText>
<sectionHeader confidence="0.99888" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999424888888889">
Common NLP tasks, such as Named Entity
Recognition and Chunking, involve the identifi-
cation of spans of words belonging to the same
phrase. These tasks are traditionally reduced to
a tagging task, in which each word is to be clas-
sified as either Beginning a span, Inside a span,
or Outside of a span. The decision is based on
the word to be classified and its neighbors. Fea-
tures supporting the classification usually include
</bodyText>
<footnote confidence="0.893615">
∗Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
</footnote>
<bodyText confidence="0.999900578947369">
the word forms themselves and properties derived
from the word forms, such as prefixes, suffixes,
capitalization information, and parts-of-speech.
While early approaches to the NP-chunking task
(Cardie and Pierce, 1998) relied on part-of-speech
information alone, it is widely accepted that lexi-
cal information (word forms) is crucial for build-
ing accurate systems for these tasks. Indeed,
all the better-performing systems in the CoNLL
shared tasks competitions for Chunking (Sang and
Buchholz, 2000) and Named Entity Recognition
(Tjong Kim Sang, 2002; Tjong Kim Sang and
De Meulder, 2003) make extensive use of such
lexical information.
Is this belief justified? In this paper, we show
that the influence of lexical features on such se-
quence labeling tasks is more complex than is gen-
erally assumed. We find that exact word forms
aren’t necessary for accurate classification. This
observation is important because relying on the
exact word forms that appear in a training corpus
leads to over-fitting, as well as to larger models.
In this work, we focus on learning with Support
Vector Machines (SVMs) (Vapnik, 1995). SVM
classifiers can handle very large feature spaces,
and produce state-of-the-art results for NLP ap-
plications (see e.g. (Kudo and Matsumoto, 2000;
Nivre et al., 2006)). Alas, when trained on pruned
feature sets, in which rare lexical items are re-
moved, SVM models suffer a loss in classifica-
tion accuracy. It would seem that rare lexical
items are indeed crucial for SVM classification
performance. However, in Goldberg and Elhadad
(2007), we suggested that the SVM learner is us-
ing the rare lexical features for singling out hard
cases rather than for learning meaningful general-
izations. We provide further evidence to support
this claim in this paper.
</bodyText>
<page confidence="0.956904">
1142
</page>
<note confidence="0.996592">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142–1151,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999849875">
We show that by using a variant of SVM –
Anchored SVM Learning (Goldberg and Elhadad,
2007) with a polynomial kernel, one can learn
accurate models for English NP-chunking (Mar-
cus and Ramshaw, 1995), base-phrase chunking
(CoNLL 2000), and Dutch Named Entity Recog-
nition (CoNLL 2002), on a heavily pruned feature
space. Our models make use of only a fraction
of the lexical features available in the training set
(less than 1%), and yet provide highly-competitive
accuracies.
For the Chunking and NP-Chunking tasks, the
most heavily pruned experiments, in which we
consider only features appearing at least 100 times
in the training corpus, do show a small but signif-
icant drop in accuracy on the testing corpus com-
pared to the non-pruned models exposed to all
available features in the training data. We pro-
vide detailed error analysis of a development set
in Section 6, revealing the causes for these differ-
ences. We suggest one additional binary feature
in order to account for some of the performance
gap. Moreover, we show that the differences in
accuracy vanish when the lexicalized and unlexi-
calized models are tested on text from slightly dif-
ferent sources than the training corpus (Section 7).
This goes to show that with an appropriate
learning method, orthographic and structural (in
the form of POS tag sequences) information is suf-
ficient for achieving state-of-the-art performance
on these kind of sequence labeling tasks. This
does not mean semantic information is not needed
for these tasks. It does mean that current models
capture only a tiny amount of such semantic in-
formation through rare lexical features, and in a
manner that does not generalize well.
We believe this data motivates a different strat-
egy to incorporate lexical features into classifica-
tion models: instead of collecting the raw lexical
forms appearing in a training corpus, we should at-
tempt to actively construct a feature space includ-
ing lexical features derived from external sources.
The feature representation of (Collobert and We-
ston, 2008) could be a step in that direction. We
also believe that hard cases for sequence labeling
(POS ambiguity, coordination, long syntactic con-
structs) could be directly approached with special-
ized classifiers.
</bodyText>
<sectionHeader confidence="0.558928" genericHeader="introduction">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.999968339622642">
This work complements a similar line of results
from the parsing literature. While it was ini-
tially believed that lexicalization of PCFG parsers
(Collins, 1997; Charniak, 2000) is crucial for
obtaining good parsing results, Gildea (2001)
demonstrated that the lexicalized Model-1 parser
of Collins (1997) does not benefit from bilexical
information when tested on a new text domain,
and only marginally benefits from such informa-
tion when tested on the same text domain as the
training corpora. This was followed by (Bikel,
2004) who showed that bilexical-information is
used in only 1.49% of the decisions in Collins’
Model-2 parser, and that removing this informa-
tion results in “an exceedingly small drop in per-
formance”. However, uni-lexical information was
still considered crucial. Klein and Manning (2003)
bridged the gap between lexicalized and unlexi-
calized parsing performance, providing a compet-
itive unlexicalized parsing model, relying on lex-
ical information for only a few closed-class lex-
ical items. This was recently followed by (Mat-
suzaki et al., 2005; Petrov et al., 2006) who intro-
duce state-of-the-art nearly unlexicalized PCFG
parsers.
Similarly for discriminative dependency pars-
ing, state-of-the-art parsers (McDonald, 2006;
Nivre et al., 2006) are highly lexicalized. How-
ever, the model analysis in (McDonald, 2006)
reveals that bilexical features hardly contribute
to the performance of a discriminative MST-
based dependency parser, while Kawahara and
Uchimoto (2007) demonstrate that minimally-
lexicalized shift-reduce based dependency parsers
can produce near state-of-the-art accuracy.
In this work, we address the same question of
determining the impact of lexical features on a dif-
ferent family of tasks: sequence labeling, as illus-
trated by named entity recognition and chunking.
As discussed above, all state-of-the-art published
methods rely on lexical features for such tasks
(Zhang et al., 2001; Sha and Pereira, 2003; Finkel
et al., 2005; Ratinov and Roth, 2009). Sequence
labeling includes both a structural aspect (bracket-
ing the chunks) and a tagging aspect (classifying
the chunks). While we expect the structural aspect
can benefit from techniques similar to those used
in the parsing literature, it is unclear whether the
tagging component could perform well without
detailed lexical information. We demonstrate in
this work that, indeed, lexical features are not nec-
essary to obtain competitive performance. Our ap-
proach consists in performing a detailed analysis
</bodyText>
<page confidence="0.978428">
1143
</page>
<bodyText confidence="0.9999525">
of the role played by rare lexical features in SVM
models. We distinguish the information brought to
the model by such features from the role they play
in a specific learning method.
</bodyText>
<subsectionHeader confidence="0.706648">
2 Learning with Less Features
</subsectionHeader>
<bodyText confidence="0.999973">
We adopt the common feature representation in
which each data-point is represented as a sparse
D dimensional binary-valued vector f. Each of
the D possible features fi is an indicator func-
tion. The indicator functions look at properties of
the current or neighbouring words. An example
of such function fi is 1 iff the previous
word-form is DOG, 0 otherwise. The
lexical (word-form) features result in extremely
high-dimensional (yet very sparse) feature vectors
– each word-form in the vocabulary of the training
set correspond to (at-least) one indicator function.
Due to the Zipfian distribution of language data,
many of the lexical features are very rare, and ap-
pear only a couple times in the training set. Ide-
ally, we would like our classifiers to learn only
from robust features: consider only features that
appear at least k times in the training data (rare-
feature pruning). These features are more likely to
appear in unseen test data, and thus such features
can support more robust generalization.
However, we find empirically that performing
such feature pruning prior to learning SVM mod-
els hurts the performance of the learned models.
Our intuition is that this sensitivity to rare lexi-
cal features is not explained by the richness of in-
formation such rare features bring to the model.
Instead, we believe that rare lexical features help
the classifier because they make the data artifi-
cially more separable. To demonstrate this claim,
we experiment with anchored SVM, which intro-
duces artificial mechanical anchors into the model
to achieve separability, and make rare lexical fea-
tures unnecessary.
</bodyText>
<sectionHeader confidence="0.94253" genericHeader="method">
3 Learning Method
</sectionHeader>
<bodyText confidence="0.999689509803921">
SVM are discriminative, max-margin, linear clas-
sifiers (Vapnik, 1995), which can be kernelized.
For the formulation of SVMs in the context of
NLP applications, see (Kudo and Matsumoto,
2001). SVMs with a polynomial kernel of degree
2 were shown to provide state-of-the-art perfor-
mance in many NLP application, see for example
(Kudo and Matsumoto, 2000; Nivre et al., 2006;
Isozaki and Kazawa, 2002; Goldberg et al., 2006).
SVMs cope with inseparable data by introduc-
ing a soft-margin – allowing some of the training
instances to be classified incorrectly subject to a
penalty, controlled by a parameter C.
Anchored SVM As we show in Section 5, the
soft-margin heuristic performs sub-optimally for
NLP tasks when the data is inseparable. We use in-
stead the Anchored Learning heuristic, introduced
in (Goldberg and Elhadad, 2007). The idea behind
anchored learning is that some training instances
are inherently ambiguous. This ambiguity stems
from ambiguity in language structure, which can-
not be resolved with a given feature representa-
tion. When a data-point cannot be classified, it
might be due to missing information, which is not
available in the data representation. Instead of al-
lowing ambiguous items to be misclassified during
training, we make the training data artificially sep-
arable. This is achieved by adding a unique feature
to each training example (an anchor). These an-
chor features cause each data-point to be slightly
more similar to itself than to any other data point.
At test time, we remove anchor features.
In terms of kernel-based learning, anchored learn-
ing can be achieved by redefining the dot product
between two vectors to take into account the iden-
tity of the vectors: xi ·anc xj = xi · xj + δij.
The classifier learned over the anchored data
takes into account the fine interactions between
the various inseparable data points. In our ex-
periments, SVM models over anchored data have
many more support vectors than soft-margin SVM
models. However, the anchored models generalize
much better when less features are available.
Relation to L2 SVM The classic soft-margin
SVM formulation uses L1-penalty for misclassi-
fied instances. Specifically, the objective of the
learner is to minimize 12��w��2 + CPiSi subject
to some margin constraints, where w is a weight
vector to be learned and Si is the misclassification
error for instance i. This is equivalent to maximiz-
ing the dual problem:
</bodyText>
<equation confidence="0.959345666666667">
PM P
i=1 αi − 1 i,j αiαjyiyjK(xi, xj)
2
</equation>
<bodyText confidence="0.9954125">
Another variant is L2-penalty SVM (Koshiba
and Abe, 2003), in which there is a quadratic
penalty for misclassified instances.
Here, the learning objective is to minimize:
</bodyText>
<equation confidence="0.801187666666667">
2��w��2 + 1
1 2C Pi S2i or alternatively maximize the
dual: Pi αi − 1 Pi,j αiαjyiyj(K(xi, xj) + δij C ).
</equation>
<page confidence="0.717092">
2
</page>
<bodyText confidence="0.877884">
Interestingly, for the linear kernel, SVM-
</bodyText>
<page confidence="0.974727">
1144
</page>
<bodyText confidence="0.996003875">
anchoring reduces to L2-SVM with C=1. How-
ever, for the case of non-linear kernels, anchored
and L2-SVM produce different results, as the an-
choring is applied prior to the kernel expansion.
Specifically for the case of the second-degree
polynomial kernel, L2-SVM aims to maximize:
Ei αi − 2 Ei,j αiαjyiyj ((xi · xj + 1)2 + δijC ),
while the anchored-SVM variant would maxi-
mizes: Ei αi − 2 Ei,j αiαjyiyj (xi · xj +δij + 1)2.
In our experiments, as discussed in Section
5.4, we find that anchored-SVM and soft-margin
SVM with tuned C value both reach good re-
sults when we reduce the amount of lexical fea-
tures. Anchored-SVM, however, does not require
fine-tuning of the error-parameter C since it in-
sures separability. As a result, we learn anchored-
SVM models quickly (few hours) as opposed to
several days per model for C-tuned soft-margin
SVM. Anchored-SVMs also provide an easy ex-
planation of the role of features in terms of sepa-
rability. Therefore, we use anchored-SVMs in our
experiments as the learning method, but we expect
that other learning methods are capable of learning
with the same reduced feature sets.
</bodyText>
<sectionHeader confidence="0.991633" genericHeader="method">
4 Experiment Setup
</sectionHeader>
<bodyText confidence="0.999911175438597">
How important are the rare lexical features for
learning accurate NLP models? To investigate
this question, we experiment with 3 different NLP
sequence-labeling tasks. For each task, we train a
sequence of polynomial kernel (d=2) SVM classi-
fiers, using both soft-margin (C=1) and anchored
SVM. Each classifier is trained on a pruned fea-
ture set, in which only features appearing at least
k times in the training data are kept. We vary the
pruning parameter k. Pruning is performed over
all the features in the model, but lexical features
are most affected by it.
For all the models, we use the B-I-O represen-
tation, and perform multiclass classification using
pairwise-voting. For our features, we consider
properties of tokens in a 5-token window centered
around the token to be classified, as well as the
two previous classifier predictions. Results are re-
ported as F-measure over labeled identified spans.
Polynomial vs. Linear models The polynomial
kernel of degree 2 allows us to efficiently and im-
plicitly include in our models all feature pairs.
Syntactic structure information as captured by
pairs of POS-tags and Word-POS pairs is certainly
important for such syntactic tasks as Chunking
and NER, as demonstrated by the many systems
described in (Sang and Buchholz, 2000; Tjong
Kim Sang, 2002). By using the polynomial ker-
nel, we can easily make use of this information
without intensive feature-tuning for the most suc-
cessful feature pairs.
L1-SVM, L2-SVM and the choice of the C pa-
rameter Throughout our experiments, we use the
“standard” variant of SVM, L1-penalty soft mar-
gin SVM, as implemented by the TinySVM1 soft-
ware package, with the default C value of 1. This
setting is shown to produce good results for se-
quence labeling tasks in previous work (Kudo and
Matsumoto, 2000), and is what most end-users of
SVM classifiers are likely to use. As we show
in Sect.5.4, fine-tuning the C parameter reaches
better accuracy than L1-SVM with C=1. How-
ever, as this fine-tuning is computationally expen-
sive, we first report the comparison L1-SVM/C=1
vs. anchored-SVM, which consistently reached
the best results, and was the quickest to train.
Feature Pruning vs. Feature Selection Our aim
in this set of experiments is not to find the optimal
set of lexical features, but rather to demonstrate
that most lexical items are not needed for accurate
classification in sequence labeling tasks. To this
end, we perform very crude frequency based fea-
ture pruning. We believe better motivated feature
selection technique taking into account linguistic
(e.g. prune only open-class words) or statistic in-
formation could result in slightly more accurate
models with even fewer lexical items.
</bodyText>
<sectionHeader confidence="0.998386" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.998185">
5.1 Named Entity Recognition (NER)
</subsectionHeader>
<bodyText confidence="0.999293866666667">
We use the Dutch data set from the CoNLL 2002
shared task (Tjong Kim Sang, 2002). The aim is to
identify named entities (persons, locations, orga-
nizations and miscellaneous) in text. The task has
two stages: identification of the entities, and clas-
sification of the identified entities into their corre-
sponding types. We focus here on the identifica-
tion task.
Features: We use the following properties for
each of the relevant tokens: word-form, POS,
ORT, prefix1, prefix2, prefix3, suffix1, suffix2,
suffix3. The ORT feature can take one of the fol-
lowing values: {number, contains-digit, contains-
hyphen, capitalized, all-capitalized, URL, punctu-
ation, regular}.
</bodyText>
<footnote confidence="0.987957">
1http://chasen.org/∼taku/software/TinySVM/
</footnote>
<page confidence="0.971949">
1145
</page>
<table confidence="0.9248258">
PRUNING #FEATURES SOFT-MARGIN ANCHORED
186,421
5,804
1,207
821
</table>
<tableCaption confidence="0.9856895">
Table 1: Named Entity Identification results (F-
score) on dev set, with various pruning thresholds.
</tableCaption>
<bodyText confidence="0.993400076923077">
Results are presented in Table 1. Without fea-
ture pruning, we achieve an F-score of 90.9. This
dataset proved to be quite resilient to feature prun-
ing. Pruning features appearing less than 100
times results in just a slight decrease in F-score.
Extremely aggressive pruning, keeping only fea-
tures appearing more than 1,000 or 1,500 times in
the training data, results in a big drop in F-score
for the soft-margin SVM (from about 91 to 86).
Much less so for the Anchored-SVM. Using An-
chored SVM we achieve an F-score of 90.1 after
pruning with k = 1, 000. This model has 1207 ac-
tive features, and 27 unique active lexical forms.
</bodyText>
<subsectionHeader confidence="0.989839">
5.2 NP Chunking
</subsectionHeader>
<bodyText confidence="0.993253238095238">
The goal of this task (Marcus and Ramshaw, 1995)
is the identification of non-recursive NPs. We use
the data from the CoNLL 2000 shared task: NP
chunks are extracted from Sections 15-18 (train)
and 20 (test) of the Penn WSJ corpus. POS tagged
are automatically assigned by the Brill Tagger.
Features: We consider the POS and word-form of
each token.
Table 2: NP-Chunking results (F-score), with var-
ious pruning thresholds.
Results are presented in Table 2. Without fea-
ture pruning (k = 0), the soft-margin SVM per-
forms slightly better than the Anchored-SVM. Ei-
ther of the results are state-of-the-art for this task.
However, even modest pruning (k = 2) hurts
the soft-margin model significantly. Not so for
the anchored-SVM. Even with relatively aggres-
sive pruning (k = 100), the anchored model still
achieves an impressive F-score of 93.83. Remark-
ably, in that last model, there are only 1,168 active
features, and only 209 unique active lexical forms.
</bodyText>
<subsectionHeader confidence="0.995209">
5.3 Chunking
</subsectionHeader>
<bodyText confidence="0.999890425531915">
The goal of the Chunking task (Sang and Buch-
holz, 2000) is the identification of an assortment
of linguistic base-phrases. We use the data from
the CoNLL 2000 shared task.
Features: We perform two experiments. In the
first experiment, we consider the POS and word-
form of each token. In this setting, feature pruning
resulted in a bigger loss in performance than in
the two previous tasks. Preliminary error analysis
revealed that many errors are due to tagger errors,
especially of the present participle forms. This led
us to the second experiment, in which we added as
features the 2- and 3- letter suffixes for the word to
be classified (but not for the surrounding words).
Results are presented in Tables 3 and 4. In the
first experiment (POS + Word), the non-pruned
soft-margin model is the same system as the top-
performing system in the original shared task,
and yields state-of-the-art results. Unlike the NP-
chunking case, here feature pruning has a rela-
tively large impact on the results even for the an-
chored models. However, the anchored models
are still far more robust than the soft-margin ones.
With k = 100 pruning, the soft-margin model suf-
fers a drop of 2.5 F points, while the anchored
model suffers a drop of only 0.84 F points. Even
after this drop, the anchored k = 100 model still
performs above the top-third system in the CoNLL
2000 shared task. This anchored k = 100 model
has 1,180 active features, and only 209 unique ac-
tive lexical features.
The second experiment (POS + word-form +
suffixes for main word) adds crude morphological
information to the learner, helping it to avoid com-
mon tagger mistakes. This additional information
is helpful: pruning with k = 100 leads to an ac-
curate anchored model (93.12 F) with only 209
unique lexical items. Note that with the addition
of the suffix features, the pruned model k = 20
beats the purely lexical model (no suffix features)
with no pruning (93.51 vs. 93.44) with 10 times
less features. When we combine suffixes and all
lexical forms, we still see a slight advantage to
the lexical model (93.73 vs. 93.12 with pruning
at k = 100).
Even less lexicalization How robust are the suf-
fixes? We performed a third experiment, in which
</bodyText>
<figure confidence="0.992251542857143">
PRUNING #FEATURES
SOFT-MARGIN ANCHORED
94.12 94.08
93.78 94.09
93.58 94.00
93.42 94.01
93.00 93.98
92.48 93.92
92.33 93.96
91.94 93.83
0
1
2
5
10
20
50
100
92,805
46,527
32,583
18,092
10,812
5,952
2,436
1,168
0
100
1000
1500
90.92 90.78
90.73 90.75
88.56 90.10
85.92 89.29
1146
PRUNING #FEATURES SOFT-MARGIN ANCHORED
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0
100
250
93.25 93.23
92.87 93.18
92.40 92.87
19,910
2,563
1,508
92,837
46,557
32,614
18,126
10,834
5,975
2,463
1,180
0
1
2
5
10
20
50
100
93.44 93.40
93.20 93.32
93.10 93.31
92.89 93.29
92.73 93.23
92.18 93.16
91.80 92.89
90.94 92.56
</figure>
<tableCaption confidence="0.947234833333333">
Table 5: Chunking results (F), with various prun-
ing thresholds. Experiment 3. Features: POS ,
Suff2, Suff3 .
Table 3: Chunking results (F), with various prun-
ing thresholds. Experiment 1. Features: POS,
Word.
</tableCaption>
<table confidence="0.932735833333333">
K L1 (C) L2 (C) ANCHORED
0 94.12 (1.0001) 94.09 (2.6128) 94.08
50 93.79 (0.0524) 93.71 (0.0082) 93.96
100 93.72 (0.0567) 93.59 (0.0072) 93.83
PRUNING #FEATURES
SOFT-MARGIN ANCHORED
</table>
<tableCaption confidence="0.595617666666667">
Table 6: NP-Chunking results (F), with various
pruning thresholds K, for L1 and L2 SVMs with
tuned C values
</tableCaption>
<figure confidence="0.99461775">
93.73 93.69
93.56 93.68
93.50 93.64
93.35 93.62
0
1
2
5
10
20
50
100
104,304
72,228
57,578
37,210
23,968
14,060
6,326
3,340
93.26 93.56
92.84 93.51
92.28 93.37
91.83 93.12
</figure>
<figureCaption confidence="0.451310333333333">
Table 4: Chunking results (F), with various prun-
ing thresholds. Experiment 2. Features: POS,
Word, {Suff2, Suff3} of main Word.
</figureCaption>
<bodyText confidence="0.999079">
we replaced any explicit word-forms by 2- and 3-
letter suffixes. This gives us the complete word
form of many function words, and a reasonable
amount of morphological marking. Results are
presented in Table 5. Surprisingly, this infor-
mation proves to be quite robust. Without fea-
ture pruning, both the anchored and soft-margin
model achieve near state-of-the-art performance
of 93.25F. Pruning with k = 100 hurts the re-
sult of the soft-margin model, but the anchored
model remains robust with an F-score of 93.18.
This last model has 2,563 active features. With
further pruning (k = 250), the result of the an-
chored model drops to 92.87F (still 3rd place in
the CoNLL shared task), with only 1,508 active
features in the model.
</bodyText>
<subsectionHeader confidence="0.986695">
5.4 Fine-tuned soft-margin SVMs
</subsectionHeader>
<bodyText confidence="0.999968636363636">
For the sake of completeness, and to serve as a bet-
ter comparison to the soft-margin SVM, we report
results of some experiments with both L1 and L2
SVMs, with tuned C values. NP-chunking perfor-
mance with tuned C values and various pruning
thresholds is presented in Table 6.
For these results, the C parameter was tuned
on a development set using Brent’s 1-dimension
minimization method (Brent, 1973). While tak-
ing about 40 hours of computation to fit, the fi-
nal results catch up with those of the anchored-
SVM but still remain slightly lower. This further
highlights our main point: accurate models can
be achieved also with mostly unlexicalized mod-
els, and the lexical features do not contribute sub-
stantial semantic information, but rather affect the
separability of the data. This is nicely demon-
strated by SVM-anchoring, in which lexical infor-
mation is practically replaced by artificial seman-
tically void indexes, but similar performance can
also be achieved by fine-tuning other learning pa-
rameters.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="method">
6 Error Analysis
</sectionHeader>
<bodyText confidence="0.99998452173913">
Our experiments so far indicate that very aggres-
sive feature pruning hurts performance slightly (by
about 0.5F point). The feature-pruned models are
still accurate, indicating that lexical features con-
tribute little to the classification accuracy. We now
investigate the differences between the lexicalized
and pruned models, in order to characterize the
kind of information that is available to the lexi-
calized models but missing from the pruned ones.
In the next section, we also verify that pruned-
models are more stable than the fully lexicalized
ones when tested over different text genres and do-
mains.
We focus our analysis on the chunking task, which
is a superset of the NP-chunking task. We compare
the fully lexicalized soft-margin SVM model with
the POS+suffix2+suffix3 anchored-SVM model
with k = 100 pruning. We analyze the mod-
els’ respective performance on section 05 of the
WSJ corpus. This dataset is different than the of-
ficial test set. It is, however, part of the same an-
notated corpus as both the training and test sets.
On this dataset, the fully lexicalized SVM model
</bodyText>
<page confidence="0.988017">
1147
</page>
<bodyText confidence="0.998009536231884">
achieves an F-score of 93.24, vs. 92.59 for the
suffix-based pruned anchored-SVM model. (The
pruned anchored-SVM model (k = 100) from ex-
periment 2, achieve a slightly higher F-score of
92.84)
We investigate only those chunks which are
identified correctly by one model but not by the
other. Overall, there are 440 chunks (363 unique)
which are identified correctly only by the lexical-
ized model, and 258 chunks (232 unique) only by
the pruned model.
Where the pruned model is always wrong
Some errors are unique to the pruned model.
Over 45 of the cases that are identified correctly
only in the lexicalized model (more than 10%) are
due to the words “including” (18 cases) and “If”
(9 cases), as well as other -ing forms such as “fol-
lowing”, “according”, “rising” and “suspecting.”
The word “including” appears 80 times in the
training data, always tagged as VBG and func-
tioning as a PP chunk, which is an odd chunk
for VBGs. The lexicalized model easily picked
up on this behaviour, while the pruned model
couldn’t. Similarly, the word “following/VBG”
appears 32 times, 20 of which as PP, and the
word “according/VBG” 53 times, all of them as
PP. The pruned model could not distinguish those
from the rest of the VBGs and tagged them as
VPs. What seems to happen in these cases, is
that certain verbal forms participate in idiomatic
constructions and behave syntactically as preposi-
tions. The POS tagger does not pick this ambigu-
ity in function and contributes only the most likely
tag for the words (VBG). Lexical models learn that
certain VBGs are “becoming” prepositions in the
observed dataset. These words do not appear as
specific features in the pruned models, and hence
these usage shifts are often misclassified. Interest-
ingly, the pruned model did learn that verbal forms
can sometimes be PPs: it made use of that infor-
mation by mis-identifying 11 verbal VBGs and 6
verbal VBNs as PPs.
The word “If/IN”, unlike most prepositions, it
always starts an SBAR rather than a PP chunk in
the corpus. The pruned model learned this be-
haviour correctly for the lower-cased “if/IN”, but
missed the upper-cased version appearing in 79
sentence initial locations in the corpus.
These cases are caused by a mismatch between
the POS tag and the syntactic function observed in
the chunked dataset.
Additional cases include the adverbs (Already,
Nearby, Soon, Maybe, Perhaps, once, Then): they
are sometimes not chunked as ADVP but are left
outside of any chunk. Some one-word ADJP
chunks being chunked as NPs (short, general, sure,
worse, ...) (6 cases) and some are chunked as
ADVPs (hard, British-born, ...) (4 cases).
There are 10 cases where the pruned model
splits an NP into ADVP and NP, such as:
[later] [this week], [roughly][18 more U.S. stores]. In
addition, the pruned model failed to learn the con-
struction “typical of”, resulting in 2 NP chunks
such as: [The more intuitive approach typical].
Some mistakes of the pruned model seem
like mistakes/pecularities of the annotated corpus,
which the lexicalized model found a way to work
around. Consider the following gold-standard
cases from the annotated corpus:
</bodyText>
<figure confidence="0.770812857142857">
- [ VP seems ] [ ADVP rarely ] [ VP to cut ]
- [ ADVP just ] [ PP after ]
- [ VP is ] [ NP anything ] [ O but ] [ VP fixing ]
-
[ ADJP as high ] [ PP as ] [ NP 8.3 % ]
- [ ADJP less ] [ PP than ] [ ADJP rosy ]
- [ NP 40 % ] [ PP to ] [ NP 45 % ]
</figure>
<bodyText confidence="0.990687666666666">
Which were each identified as a single chunk by
the pruned model. It can be argued these are mis-
takes in the tagged dataset.
Where the lexical model is sometimes better
Both models fail on conjunctions, but the lexical-
ized model do slightly better. Conjunction error
types come in two main varieties, either chunking
[x][and][y] instead of [x and y] (pruned: 21 cases,
lex: 14 cases) or chunking [x and y] instead of
[x][and][y] (pruned: 26 cases, lex: 24 cases).
Joining VP and NP into an NP, due
to a verb/adj ambiguity. For exam-
ple chunking [NP fired six executives] in-
stead of [VP fired] [NP six executives],
or [NP keeping viewers] instead of
[VP keeping] [NP viewers]. 12 such cases are
resolved correctly only by the lexicalized model,
and 5 only by the pruned one.
SBAR/PP confusion for words such as:
“as”,“after”,“with”,“since” (both ways). 13 cases
for the pruned model, 6 for lexicalized one.
Where both model are similar
Merging back-to-back NPs: Both models tend
to erroneously join back-to-back NPs to a sin-
gle NP, e.g. : [NP Westinghouse this year], or
[NP themselves fashion enterprises]. No model is bet-
ter than the other on these cases, each model failed
</bodyText>
<page confidence="0.906339">
1148
</page>
<bodyText confidence="0.888814433962264">
on 16 cases the other model succeeded on.
Joining NP and VP into an NP due to
Verb/Noun ambiguity and tagger mistakes:
ates (e.g., “including” used as a preposition).
The main advantage of the fully lexcialized
model is in dealing with:
- [NP the weekend] [VP making] → [NP the weekend making]
- [NP the competition] [VP can] → [NP the competition can]
(lexicalized: 6 errors, pruned: 8 errors)
And splitting some NPs to VP+NP due to the
same reasons:
- [VP operating] [NP profit]
- [VP improved] [NP average yield]
(lexicalized: 5 errors, pruned: 7 errors)
The word “that” is confused between SBAR and
NP (5 mistakes for each model)
Erroneously splitting range NPs, e.g. :
- [about $115][to][$125] (2 cases for each model).
Where the pruned model is better
There are some cases where the pruned models is
doing better than the lexicalized one:
VP wrongly split into VP and ADJP:
- [remains] [banned]
4 mistakes for lexicalized, 1 for pruned
VP wrongly split into VP and VP:
- [were scheduled] [to meet]
- [used] [to complain]
3 mistakes for lexicalized, 1 for pruned
VP wrongly split into ADVP and VP:
- [largly][reflecting]
- [selectively][leaking]
6 mistakes for lexicalized, 1 for pruned
PP and SBAR confusion:
- of, with, As, after
9 mistakes for lex, 5 for pruned
VP chunked as NP due to tagger mistake:
- [NP ruling], [NP drives], [NP cuts]
6 mistakes for lex, 2 for pruned
“that” tagged as NP instead of SBAR:
2 mistakes for lex, 0 for pruned
To conclude
Both the pruned and the fully lexicalized models
have problems dealing with non-local phenomena
such as coordination and relative clauses, as well
as verb/adjective ambiguities and VBG/Noun am-
biguities. They also perform poorly on embeded
syntactic constructions (such as an NP containing
an ADJP), and on identification of back-to-back
NPs, which often requires semantic knowledge.
Both models suffer from tagging mistakes of the
underlying tagger and systematic ambiguity be-
tween the morphological tag assigned by the tag-
ger and the syntactic tag in which the word oper-
</bodyText>
<listItem confidence="0.9776685">
• Some coordinated constructions.
• Some cases of verb/adjective ambiguities.
• Specific function words not seen much in
training.
• Idiomatic usages of some VBG/VBN forms
functioning as prepositions.
</listItem>
<bodyText confidence="0.999951818181818">
The first two items are semantic in nature, and hint
that lexical features do capture some semantic in-
formation. While this might be true on the spe-
cific corpus, we believe that such corpus-derived
semantic knowledge is very restricted, is not gen-
eralizable, and will not transfer well to other cor-
pora, even on the same genre. We provide evi-
dence for this claim in Section 7.
The last two items are syntactic. We address
them by introducing a slightly modified feature
model.
</bodyText>
<subsectionHeader confidence="0.983172">
6.1 Another chunking Experiment
</subsectionHeader>
<bodyText confidence="0.99999">
Based on the observations from the error analy-
sis, we performed another pruned-chunking exper-
iment, with the following features:
</bodyText>
<listItem confidence="0.979538">
• Word and POS for a -2,+2 window around
the current token, and 2-and-3-letter suffixes
of the token to be classified (same as Experi-
ment 2 in Section 5.2 above).
• Features of words appearing as a preposi-
tion (IN) anywhere in the training set are
not pruned (this result in a model with 310
unique lexical items after k = 100 pruning).
• An additional binary feature indicating for
</listItem>
<bodyText confidence="0.98310375">
each token whether it can function as a PP.
The list of possible-PP forms is generated by
considering all tokens seen inside a PP in the
training corpus. It can be easily extended if
additional lexicographic resources are avail-
able, without retraining the model.
This last proposed feature incorporates important
lexical knowledge without relying on features for
specific lexical forms, and is more generalizable.
The accuracy of this new model on the develop-
ment and test set with various pruning thresholds
is presented in Table 7.
The addition of the CanBePrep feature im-
proves the fully-lexicalized model accuracy on the
development set (93.24 to 93.68), and does not af-
fect fully lexicalized result on the test set (93.71
</bodyText>
<page confidence="0.996186">
1149
</page>
<note confidence="0.9038216">
CORPUS SOURCE CONTENT #TOKENS
WSJ 4 articles from wsj.com business Magazine, business 2,671
Jaguar Wikipedia page on Jaguar Well edited text, animals 5,396
FreeWill Wikipedia page on Free Will Well edited text, philosophy 9,428
LJ-Life 4 LiveJournal posts Noisy teenage writing, life 870
</note>
<tableCaption confidence="0.995449">
Table 8: Corpus Variation Text Sources
</tableCaption>
<figure confidence="0.945953214285714">
#FEATURES
SOFT-MARGIN ANCHORED
PRUNING
Dev Set
92,989
4,066
0
100
93.22
Test Set
93.68
0
92,989
93.71
</figure>
<table confidence="0.848221777777778">
TEXT #DIFF PRUNED LEX BOTH
CORRECT CORRECT WRONG
WSJ 13 9 4 0
Jaguar 45 20 20 7
FreeWill 118 51 38 29
LJ-Life 15 8 6 1
93.26
100
4,066
</table>
<tableCaption confidence="0.748333666666667">
Table 7: Chunking results (F), with various prun-
ing thresholds. Experiment 4. Features: POS,
Word, Suff2, Suff3 for main word, CanBePrep .
</tableCaption>
<bodyText confidence="0.9989314">
vs. 93.73). The pruned model performance im-
proves in both cases, more so on the development
set (93.12 to 93.22 on the test set, 92.84 to 93.26
on the development set). The new model helps
bridging the gap between the fully lexicalized and
the pruned model, yet we still observe a lead of
0.4F for the fully lexicalized model. We now turn
to explore how meaningful this difference is in
real-world situation in which one does not operate
on the Penn-WSJ corpus.
</bodyText>
<sectionHeader confidence="0.9716165" genericHeader="method">
7 Corpus Variation and Model
Performance
</sectionHeader>
<bodyText confidence="0.987019967741935">
When tested on the exact same resource as the
models are trained on, the fully lexicalized model
still has a slight edge over the pruned ones. How
well does this lexical knowledge transfer to dif-
ferent text genres? We compare the models’ per-
formance on text from various genres, ranging
from very similar to the training material (re-
cent articles from the WSJ Business section) to a
well-edited but different domain text (“Featured-
content” wikipedia pages) to a non-edited noisy
text (live-journal blog posts from the “life” cate-
gory). As we do not have gold-annotated data for
these text genres, we analyze the few differences
between the models, manually inspecting the in-
stances on which the models disagree.
Table 8 describes our test corpora for this ex-
periment. We applied the fully-lexicalized and
the pruned (k = 100) anchored models described
in Section 6.1 to these texts, and compared the
chunking results. The results are presented in Ta-
ble 9.
When moving outside of the canonic training
Table 9: Comparison of Models’ performance on
different text genres
corpus, the fully lexicalized model have no advan-
tage over the heavily pruned one. On the contrary,
the pruned models seem to have a small advantage
in most cases (though it is hard to tell if the differ-
ences are significant). This is true even for texts
in the very same domain, genre and editing guide-
lines as the training corpus was derived from.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999520964285714">
For all the sequence labeling tasks we analyzed,
the anchored-SVM proved to be robust to feature
pruning. The experiments support the claim that
rare lexical features do not provide substantial in-
formation to the model, but instead play a role in
maintaining separability. When this role is taken
over by anchoring, we can obtain the same level
of performance with very few robust lexical fea-
tures. Yet, we cannot conclude that lexical infor-
mation is not needed. There is a significant differ-
ence between the pruned and non-pruned models
for the chunking task. We showed that this dif-
ference can be bridged to some extent by a binary
feature relating to idiomatic word usage, and that
the difference vanishes when testing outside of the
annotated corpus. The high classification accura-
cies achieved with the heavily pruned anchored-
SVM models sheds new light on the actual role
of lexical features, and indicating that there is still
a lot to be learned regarding the effective incor-
poration of lexical and semantic information into
our models. It is our view that semantic knowl-
edge should not be expected to be learned by in-
spection of raw lexical counts from an annotated
text corpus, but instead collected from sources ex-
ternal to the annotated corpora – either based on
a very large unannotated corpora, or on manually
constructed lexical resources.
</bodyText>
<page confidence="0.988392">
1150
</page>
<sectionHeader confidence="0.993863" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996297375">
Daniel M. Bikel. 2004. Intricacies of collins’ parsing
model. Computational Linguistics, 30(4).
Richard P. Brent, 1973. Algorithms for Minimization
without Derivatives, chapter 4. Prentice-Hall.
Claire Cardie and David Pierce. 1998. Error-driven
pruning of treebank grammars for base noun phrase
identification. In ACL-1998.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc of NAACL.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proc of EACL.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Proc. of
ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc of ACL.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proc of EMNLP.
Yoav Goldberg and Michael Elhadad. 2007. SVM
Model Tampering and Anchored Learning: A Case
Study in Hebrew. NP Chunking. In ACL2007.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2006. Noun Phrase Chunking in Hebrew: Influence
of Lexical and Morphological Features. In COL-
ING/ACL2006.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
Support Vector Classifiers For Named Entity Recog-
nition. In COLING2002.
Daisuke Kawahara and Kiyotaka Uchimoto. 2007.
Miniamlly lexicalized dependency parsing. In Proc
of ACL (Short papers).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. of ACL.
Yoshiaki Koshiba and Shigeo Abe. 2003. Comparison
of L1 and L2 support vector machines. In Proc. of
the International Joint Conference on Neural Net-
works, volume 3.
Taku Kudo and Yuji Matsumoto. 2000. Use of Sup-
port Vector Learning for Chunk Identification. In
CoNLL-2000.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In NAACL ’01.
Mitch P. Marcus and Lance A. Ramshaw. 1995.
Text Chunking Using Transformation-Based Learn-
ing. In 3rd ACL Workshop on Very Large Corpora.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proc of ACL.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006.
MaltParser: A Data-Driven Parser-Generator for
Dependency Parsing. In LREC2006.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc of ACL.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc of CONLL.
Erik F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the CoNLL-2000 shared task: chunking.
In CoNLL-2000.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc of NAACL.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-Independent Named Entity Recog-
nition. In CoNLL-2003.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-Independent
Named Entity Recognition. In CoNLL-2002.
Vladimir Vapnik. 1995. The nature of statistical learn-
ing theory. Springer-Verlag New York, Inc.
Tong Zhang, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Proc of
ACL.
</reference>
<page confidence="0.994516">
1151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.389339">
<title confidence="0.8055895">On the Role of Lexical Features in Sequence Labeling and</title>
<author confidence="0.649432">Ben Gurion University of the</author>
<affiliation confidence="0.961789">Department of Computer</affiliation>
<address confidence="0.802538">POB 653 Be’er Sheva, 84105,</address>
<abstract confidence="0.999108458333334">We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking. While standard models require as many as 100K distinct features, we derive models with as little as 1K features that perform as well or better on different domains. These robust reduced models indicate that the way rare lexical features contribute to classification in NLP is not fully understood. Contrastive error analysis (with and without lexical features) indicates that lexical features do contribute to resolving some semantic and complex syntactic ambiguities – but we find this contribution does not generalize outside the training corpus. As a general strategy, we believe lexical features should not be directly derived from a training corpus but instead, carefully inferred and selected from other sources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Intricacies of collins’ parsing model.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="6421" citStr="Bikel, 2004" startWordPosition="1022" endWordPosition="1023">ructs) could be directly approached with specialized classifiers. 1.1 Related Work This work complements a similar line of results from the parsing literature. While it was initially believed that lexicalization of PCFG parsers (Collins, 1997; Charniak, 2000) is crucial for obtaining good parsing results, Gildea (2001) demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora. This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins’ Model-2 parser, and that removing this information results in “an exceedingly small drop in performance”. However, uni-lexical information was still considered crucial. Klein and Manning (2003) bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items. This was recently followed by (Matsuzaki et al., 2005; Petrov et al., 2006) who introduce state-of-the-art nearly </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of collins’ parsing model. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard P Brent</author>
</authors>
<title>Algorithms for Minimization without Derivatives, chapter 4.</title>
<date>1973</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="24152" citStr="Brent, 1973" startWordPosition="3928" endWordPosition="3929">eatures. With further pruning (k = 250), the result of the anchored model drops to 92.87F (still 3rd place in the CoNLL shared task), with only 1,508 active features in the model. 5.4 Fine-tuned soft-margin SVMs For the sake of completeness, and to serve as a better comparison to the soft-margin SVM, we report results of some experiments with both L1 and L2 SVMs, with tuned C values. NP-chunking performance with tuned C values and various pruning thresholds is presented in Table 6. For these results, the C parameter was tuned on a development set using Brent’s 1-dimension minimization method (Brent, 1973). While taking about 40 hours of computation to fit, the final results catch up with those of the anchoredSVM but still remain slightly lower. This further highlights our main point: accurate models can be achieved also with mostly unlexicalized models, and the lexical features do not contribute substantial semantic information, but rather affect the separability of the data. This is nicely demonstrated by SVM-anchoring, in which lexical information is practically replaced by artificial semantically void indexes, but similar performance can also be achieved by fine-tuning other learning parame</context>
</contexts>
<marker>Brent, 1973</marker>
<rawString>Richard P. Brent, 1973. Algorithms for Minimization without Derivatives, chapter 4. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>David Pierce</author>
</authors>
<title>Error-driven pruning of treebank grammars for base noun phrase identification.</title>
<date>1998</date>
<booktitle>In ACL-1998.</booktitle>
<contexts>
<context position="1918" citStr="Cardie and Pierce, 1998" startWordPosition="299" endWordPosition="302">ing to the same phrase. These tasks are traditionally reduced to a tagging task, in which each word is to be classified as either Beginning a span, Inside a span, or Outside of a span. The decision is based on the word to be classified and its neighbors. Features supporting the classification usually include ∗Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exac</context>
</contexts>
<marker>Cardie, Pierce, 1998</marker>
<rawString>Claire Cardie and David Pierce. 1998. Error-driven pruning of treebank grammars for base noun phrase identification. In ACL-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proc of NAACL.</booktitle>
<contexts>
<context position="6068" citStr="Charniak, 2000" startWordPosition="966" endWordPosition="967">xical forms appearing in a training corpus, we should attempt to actively construct a feature space including lexical features derived from external sources. The feature representation of (Collobert and Weston, 2008) could be a step in that direction. We also believe that hard cases for sequence labeling (POS ambiguity, coordination, long syntactic constructs) could be directly approached with specialized classifiers. 1.1 Related Work This work complements a similar line of results from the parsing literature. While it was initially believed that lexicalization of PCFG parsers (Collins, 1997; Charniak, 2000) is crucial for obtaining good parsing results, Gildea (2001) demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora. This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins’ Model-2 parser, and that removing this information results in “an exceedingly small drop in performance”. However, uni-lexical information was still consider</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proc of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc of EACL.</booktitle>
<contexts>
<context position="6051" citStr="Collins, 1997" startWordPosition="964" endWordPosition="965">ting the raw lexical forms appearing in a training corpus, we should attempt to actively construct a feature space including lexical features derived from external sources. The feature representation of (Collobert and Weston, 2008) could be a step in that direction. We also believe that hard cases for sequence labeling (POS ambiguity, coordination, long syntactic constructs) could be directly approached with specialized classifiers. 1.1 Related Work This work complements a similar line of results from the parsing literature. While it was initially believed that lexicalization of PCFG parsers (Collins, 1997; Charniak, 2000) is crucial for obtaining good parsing results, Gildea (2001) demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora. This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins’ Model-2 parser, and that removing this information results in “an exceedingly small drop in performance”. However, uni-lexical information w</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proc of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="5669" citStr="Collobert and Weston, 2008" startWordPosition="902" endWordPosition="906"> of sequence labeling tasks. This does not mean semantic information is not needed for these tasks. It does mean that current models capture only a tiny amount of such semantic information through rare lexical features, and in a manner that does not generalize well. We believe this data motivates a different strategy to incorporate lexical features into classification models: instead of collecting the raw lexical forms appearing in a training corpus, we should attempt to actively construct a feature space including lexical features derived from external sources. The feature representation of (Collobert and Weston, 2008) could be a step in that direction. We also believe that hard cases for sequence labeling (POS ambiguity, coordination, long syntactic constructs) could be directly approached with specialized classifiers. 1.1 Related Work This work complements a similar line of results from the parsing literature. While it was initially believed that lexicalization of PCFG parsers (Collins, 1997; Charniak, 2000) is crucial for obtaining good parsing results, Gildea (2001) demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text do</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="7863" citStr="Finkel et al., 2005" startWordPosition="1232" endWordPosition="1235"> bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy. In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking. As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). Sequence labeling includes both a structural aspect (bracketing the chunks) and a tagging aspect (classifying the chunks). While we expect the structural aspect can benefit from techniques similar to those used in the parsing literature, it is unclear whether the tagging component could perform well without detailed lexical information. We demonstrate in this work that, indeed, lexical features are not necessary to obtain competitive performance. Our approach consists in performing a detailed analysis 1143 of the role played by rare lexical features in SVM models. We</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proc of EMNLP.</booktitle>
<contexts>
<context position="6129" citStr="Gildea (2001)" startWordPosition="975" endWordPosition="976">o actively construct a feature space including lexical features derived from external sources. The feature representation of (Collobert and Weston, 2008) could be a step in that direction. We also believe that hard cases for sequence labeling (POS ambiguity, coordination, long syntactic constructs) could be directly approached with specialized classifiers. 1.1 Related Work This work complements a similar line of results from the parsing literature. While it was initially believed that lexicalization of PCFG parsers (Collins, 1997; Charniak, 2000) is crucial for obtaining good parsing results, Gildea (2001) demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora. This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins’ Model-2 parser, and that removing this information results in “an exceedingly small drop in performance”. However, uni-lexical information was still considered crucial. Klein and Manning (2003) bridged the gap between </context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proc of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>SVM Model Tampering and Anchored Learning: A Case Study in Hebrew. NP Chunking.</title>
<date>2007</date>
<booktitle>In ACL2007.</booktitle>
<contexts>
<context position="3259" citStr="Goldberg and Elhadad (2007)" startWordPosition="513" endWordPosition="516">act word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142–1151, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP We show that by using a variant of SVM – Anchored SVM Learning (Goldberg and Elhadad, 2007) with a polynomial kernel, one can learn accurate models for English NP-chunking (Marcus and Ramshaw, 1995), base-phrase chunking (Co</context>
<context position="11074" citStr="Goldberg and Elhadad, 2007" startWordPosition="1746" endWordPosition="1749">h a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). SVMs cope with inseparable data by introducing a soft-margin – allowing some of the training instances to be classified incorrectly subject to a penalty, controlled by a parameter C. Anchored SVM As we show in Section 5, the soft-margin heuristic performs sub-optimally for NLP tasks when the data is inseparable. We use instead the Anchored Learning heuristic, introduced in (Goldberg and Elhadad, 2007). The idea behind anchored learning is that some training instances are inherently ambiguous. This ambiguity stems from ambiguity in language structure, which cannot be resolved with a given feature representation. When a data-point cannot be classified, it might be due to missing information, which is not available in the data representation. Instead of allowing ambiguous items to be misclassified during training, we make the training data artificially separable. This is achieved by adding a unique feature to each training example (an anchor). These anchor features cause each data-point to be</context>
</contexts>
<marker>Goldberg, Elhadad, 2007</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2007. SVM Model Tampering and Anchored Learning: A Case Study in Hebrew. NP Chunking. In ACL2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>Noun Phrase Chunking in Hebrew: Influence of Lexical and Morphological Features.</title>
<date>2006</date>
<booktitle>In COLING/ACL2006.</booktitle>
<contexts>
<context position="10668" citStr="Goldberg et al., 2006" startWordPosition="1681" endWordPosition="1684">strate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). SVMs cope with inseparable data by introducing a soft-margin – allowing some of the training instances to be classified incorrectly subject to a penalty, controlled by a parameter C. Anchored SVM As we show in Section 5, the soft-margin heuristic performs sub-optimally for NLP tasks when the data is inseparable. We use instead the Anchored Learning heuristic, introduced in (Goldberg and Elhadad, 2007). The idea behind anchored learning is that some training instances are inherently ambiguous. This ambiguity stems from ambiguity in language structure, which cannot be resolved with a given fea</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2006</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2006. Noun Phrase Chunking in Hebrew: Influence of Lexical and Morphological Features. In COLING/ACL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient Support Vector Classifiers For Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In COLING2002.</booktitle>
<contexts>
<context position="10644" citStr="Isozaki and Kazawa, 2002" startWordPosition="1677" endWordPosition="1680">y more separable. To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). SVMs cope with inseparable data by introducing a soft-margin – allowing some of the training instances to be classified incorrectly subject to a penalty, controlled by a parameter C. Anchored SVM As we show in Section 5, the soft-margin heuristic performs sub-optimally for NLP tasks when the data is inseparable. We use instead the Anchored Learning heuristic, introduced in (Goldberg and Elhadad, 2007). The idea behind anchored learning is that some training instances are inherently ambiguous. This ambiguity stems from ambiguity in language structure, which cannot be r</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2002. Efficient Support Vector Classifiers For Named Entity Recognition. In COLING2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>Miniamlly lexicalized dependency parsing.</title>
<date>2007</date>
<booktitle>In Proc of ACL</booktitle>
<note>(Short papers).</note>
<contexts>
<context position="7383" citStr="Kawahara and Uchimoto (2007)" startWordPosition="1160" endWordPosition="1163">zed parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items. This was recently followed by (Matsuzaki et al., 2005; Petrov et al., 2006) who introduce state-of-the-art nearly unlexicalized PCFG parsers. Similarly for discriminative dependency parsing, state-of-the-art parsers (McDonald, 2006; Nivre et al., 2006) are highly lexicalized. However, the model analysis in (McDonald, 2006) reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy. In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking. As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). Sequence labeling includes both a structural aspect (bracketing the chunks) and a tagging asp</context>
</contexts>
<marker>Kawahara, Uchimoto, 2007</marker>
<rawString>Daisuke Kawahara and Kiyotaka Uchimoto. 2007. Miniamlly lexicalized dependency parsing. In Proc of ACL (Short papers).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="6704" citStr="Klein and Manning (2003)" startWordPosition="1062" endWordPosition="1065">for obtaining good parsing results, Gildea (2001) demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora. This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins’ Model-2 parser, and that removing this information results in “an exceedingly small drop in performance”. However, uni-lexical information was still considered crucial. Klein and Manning (2003) bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items. This was recently followed by (Matsuzaki et al., 2005; Petrov et al., 2006) who introduce state-of-the-art nearly unlexicalized PCFG parsers. Similarly for discriminative dependency parsing, state-of-the-art parsers (McDonald, 2006; Nivre et al., 2006) are highly lexicalized. However, the model analysis in (McDonald, 2006) reveals that bilexical features hardly contribute to the performance of </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshiaki Koshiba</author>
<author>Shigeo Abe</author>
</authors>
<title>Comparison of L1 and L2 support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of the International Joint Conference on Neural Networks,</booktitle>
<volume>3</volume>
<contexts>
<context position="12761" citStr="Koshiba and Abe, 2003" startWordPosition="2024" endWordPosition="2027">ver anchored data have many more support vectors than soft-margin SVM models. However, the anchored models generalize much better when less features are available. Relation to L2 SVM The classic soft-margin SVM formulation uses L1-penalty for misclassified instances. Specifically, the objective of the learner is to minimize 12��w��2 + CPiSi subject to some margin constraints, where w is a weight vector to be learned and Si is the misclassification error for instance i. This is equivalent to maximizing the dual problem: PM P i=1 αi − 1 i,j αiαjyiyjK(xi, xj) 2 Another variant is L2-penalty SVM (Koshiba and Abe, 2003), in which there is a quadratic penalty for misclassified instances. Here, the learning objective is to minimize: 2��w��2 + 1 1 2C Pi S2i or alternatively maximize the dual: Pi αi − 1 Pi,j αiαjyiyj(K(xi, xj) + δij C ). 2 Interestingly, for the linear kernel, SVM1144 anchoring reduces to L2-SVM with C=1. However, for the case of non-linear kernels, anchored and L2-SVM produce different results, as the anchoring is applied prior to the kernel expansion. Specifically for the case of the second-degree polynomial kernel, L2-SVM aims to maximize: Ei αi − 2 Ei,j αiαjyiyj ((xi · xj + 1)2 + δijC ), whi</context>
</contexts>
<marker>Koshiba, Abe, 2003</marker>
<rawString>Yoshiaki Koshiba and Shigeo Abe. 2003. Comparison of L1 and L2 support vector machines. In Proc. of the International Joint Conference on Neural Networks, volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of Support Vector Learning for Chunk Identification.</title>
<date>2000</date>
<booktitle>In CoNLL-2000.</booktitle>
<contexts>
<context position="2966" citStr="Kudo and Matsumoto, 2000" startWordPosition="465" endWordPosition="468">his belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processin</context>
<context position="10598" citStr="Kudo and Matsumoto, 2000" startWordPosition="1669" endWordPosition="1672">ssifier because they make the data artificially more separable. To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). SVMs cope with inseparable data by introducing a soft-margin – allowing some of the training instances to be classified incorrectly subject to a penalty, controlled by a parameter C. Anchored SVM As we show in Section 5, the soft-margin heuristic performs sub-optimally for NLP tasks when the data is inseparable. We use instead the Anchored Learning heuristic, introduced in (Goldberg and Elhadad, 2007). The idea behind anchored learning is that some training instances are inherently ambiguous. This ambiguity stems from ambi</context>
<context position="15955" citStr="Kudo and Matsumoto, 2000" startWordPosition="2561" endWordPosition="2564">ch syntactic tasks as Chunking and NER, as demonstrated by the many systems described in (Sang and Buchholz, 2000; Tjong Kim Sang, 2002). By using the polynomial kernel, we can easily make use of this information without intensive feature-tuning for the most successful feature pairs. L1-SVM, L2-SVM and the choice of the C parameter Throughout our experiments, we use the “standard” variant of SVM, L1-penalty soft margin SVM, as implemented by the TinySVM1 software package, with the default C value of 1. This setting is shown to produce good results for sequence labeling tasks in previous work (Kudo and Matsumoto, 2000), and is what most end-users of SVM classifiers are likely to use. As we show in Sect.5.4, fine-tuning the C parameter reaches better accuracy than L1-SVM with C=1. However, as this fine-tuning is computationally expensive, we first report the comparison L1-SVM/C=1 vs. anchored-SVM, which consistently reached the best results, and was the quickest to train. Feature Pruning vs. Feature Selection Our aim in this set of experiments is not to find the optimal set of lexical features, but rather to demonstrate that most lexical items are not needed for accurate classification in sequence labeling t</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Use of Support Vector Learning for Chunk Identification. In CoNLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In NAACL ’01.</booktitle>
<contexts>
<context position="10437" citStr="Kudo and Matsumoto, 2001" startWordPosition="1643" endWordPosition="1646">exical features is not explained by the richness of information such rare features bring to the model. Instead, we believe that rare lexical features help the classifier because they make the data artificially more separable. To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). SVMs cope with inseparable data by introducing a soft-margin – allowing some of the training instances to be classified incorrectly subject to a penalty, controlled by a parameter C. Anchored SVM As we show in Section 5, the soft-margin heuristic performs sub-optimally for NLP tasks when the data is inseparable. We use instead the Anchored Learning heuristic, intro</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In NAACL ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch P Marcus</author>
<author>Lance A Ramshaw</author>
</authors>
<title>Text Chunking Using Transformation-Based Learning.</title>
<date>1995</date>
<booktitle>In 3rd ACL Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="3833" citStr="Marcus and Ramshaw, 1995" startWordPosition="605" endWordPosition="609">erformance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142–1151, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP We show that by using a variant of SVM – Anchored SVM Learning (Goldberg and Elhadad, 2007) with a polynomial kernel, one can learn accurate models for English NP-chunking (Marcus and Ramshaw, 1995), base-phrase chunking (CoNLL 2000), and Dutch Named Entity Recognition (CoNLL 2002), on a heavily pruned feature space. Our models make use of only a fraction of the lexical features available in the training set (less than 1%), and yet provide highly-competitive accuracies. For the Chunking and NP-Chunking tasks, the most heavily pruned experiments, in which we consider only features appearing at least 100 times in the training corpus, do show a small but significant drop in accuracy on the testing corpus compared to the non-pruned models exposed to all available features in the training dat</context>
<context position="18479" citStr="Marcus and Ramshaw, 1995" startWordPosition="2962" endWordPosition="2965">ve an F-score of 90.9. This dataset proved to be quite resilient to feature pruning. Pruning features appearing less than 100 times results in just a slight decrease in F-score. Extremely aggressive pruning, keeping only features appearing more than 1,000 or 1,500 times in the training data, results in a big drop in F-score for the soft-margin SVM (from about 91 to 86). Much less so for the Anchored-SVM. Using Anchored SVM we achieve an F-score of 90.1 after pruning with k = 1, 000. This model has 1207 active features, and 27 unique active lexical forms. 5.2 NP Chunking The goal of this task (Marcus and Ramshaw, 1995) is the identification of non-recursive NPs. We use the data from the CoNLL 2000 shared task: NP chunks are extracted from Sections 15-18 (train) and 20 (test) of the Penn WSJ corpus. POS tagged are automatically assigned by the Brill Tagger. Features: We consider the POS and word-form of each token. Table 2: NP-Chunking results (F-score), with various pruning thresholds. Results are presented in Table 2. Without feature pruning (k = 0), the soft-margin SVM performs slightly better than the Anchored-SVM. Either of the results are state-of-the-art for this task. However, even modest pruning (k </context>
</contexts>
<marker>Marcus, Ramshaw, 1995</marker>
<rawString>Mitch P. Marcus and Lance A. Ramshaw. 1995. Text Chunking Using Transformation-Based Learning. In 3rd ACL Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic cfg with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="6960" citStr="Matsuzaki et al., 2005" startWordPosition="1101" endWordPosition="1105">on the same text domain as the training corpora. This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins’ Model-2 parser, and that removing this information results in “an exceedingly small drop in performance”. However, uni-lexical information was still considered crucial. Klein and Manning (2003) bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items. This was recently followed by (Matsuzaki et al., 2005; Petrov et al., 2006) who introduce state-of-the-art nearly unlexicalized PCFG parsers. Similarly for discriminative dependency parsing, state-of-the-art parsers (McDonald, 2006; Nivre et al., 2006) are highly lexicalized. However, the model analysis in (McDonald, 2006) reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy. In this work, we address the same question of determinin</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic cfg with latent annotations. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Training and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7138" citStr="McDonald, 2006" startWordPosition="1126" endWordPosition="1127">r, and that removing this information results in “an exceedingly small drop in performance”. However, uni-lexical information was still considered crucial. Klein and Manning (2003) bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items. This was recently followed by (Matsuzaki et al., 2005; Petrov et al., 2006) who introduce state-of-the-art nearly unlexicalized PCFG parsers. Similarly for discriminative dependency parsing, state-of-the-art parsers (McDonald, 2006; Nivre et al., 2006) are highly lexicalized. However, the model analysis in (McDonald, 2006) reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy. In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking. As discussed above, all state-of-the-</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nillson</author>
</authors>
<title>MaltParser: A Data-Driven Parser-Generator for Dependency Parsing.</title>
<date>2006</date>
<booktitle>In LREC2006.</booktitle>
<contexts>
<context position="2987" citStr="Nivre et al., 2006" startWordPosition="469" endWordPosition="472">his paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142–1151, S</context>
<context position="7159" citStr="Nivre et al., 2006" startWordPosition="1128" endWordPosition="1131">ving this information results in “an exceedingly small drop in performance”. However, uni-lexical information was still considered crucial. Klein and Manning (2003) bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items. This was recently followed by (Matsuzaki et al., 2005; Petrov et al., 2006) who introduce state-of-the-art nearly unlexicalized PCFG parsers. Similarly for discriminative dependency parsing, state-of-the-art parsers (McDonald, 2006; Nivre et al., 2006) are highly lexicalized. However, the model analysis in (McDonald, 2006) reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy. In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking. As discussed above, all state-of-the-art published methods</context>
<context position="10618" citStr="Nivre et al., 2006" startWordPosition="1673" endWordPosition="1676">the data artificially more separable. To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). SVMs cope with inseparable data by introducing a soft-margin – allowing some of the training instances to be classified incorrectly subject to a penalty, controlled by a parameter C. Anchored SVM As we show in Section 5, the soft-margin heuristic performs sub-optimally for NLP tasks when the data is inseparable. We use instead the Anchored Learning heuristic, introduced in (Goldberg and Elhadad, 2007). The idea behind anchored learning is that some training instances are inherently ambiguous. This ambiguity stems from ambiguity in language st</context>
</contexts>
<marker>Nivre, Hall, Nillson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nillson. 2006. MaltParser: A Data-Driven Parser-Generator for Dependency Parsing. In LREC2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="6982" citStr="Petrov et al., 2006" startWordPosition="1106" endWordPosition="1109">as the training corpora. This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins’ Model-2 parser, and that removing this information results in “an exceedingly small drop in performance”. However, uni-lexical information was still considered crucial. Klein and Manning (2003) bridged the gap between lexicalized and unlexicalized parsing performance, providing a competitive unlexicalized parsing model, relying on lexical information for only a few closed-class lexical items. This was recently followed by (Matsuzaki et al., 2005; Petrov et al., 2006) who introduce state-of-the-art nearly unlexicalized PCFG parsers. Similarly for discriminative dependency parsing, state-of-the-art parsers (McDonald, 2006; Nivre et al., 2006) are highly lexicalized. However, the model analysis in (McDonald, 2006) reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy. In this work, we address the same question of determining the impact of lexica</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proc of CONLL.</booktitle>
<contexts>
<context position="7888" citStr="Ratinov and Roth, 2009" startWordPosition="1236" endWordPosition="1239">ardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy. In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking. As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). Sequence labeling includes both a structural aspect (bracketing the chunks) and a tagging aspect (classifying the chunks). While we expect the structural aspect can benefit from techniques similar to those used in the parsing literature, it is unclear whether the tagging component could perform well without detailed lexical information. We demonstrate in this work that, indeed, lexical features are not necessary to obtain competitive performance. Our approach consists in performing a detailed analysis 1143 of the role played by rare lexical features in SVM models. We distinguish the informat</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proc of CONLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: chunking.</title>
<date>2000</date>
<booktitle>In CoNLL-2000.</booktitle>
<contexts>
<context position="2200" citStr="Sang and Buchholz, 2000" startWordPosition="341" endWordPosition="344">ssification usually include ∗Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVM</context>
<context position="15443" citStr="Sang and Buchholz, 2000" startWordPosition="2473" endWordPosition="2476">ing pairwise-voting. For our features, we consider properties of tokens in a 5-token window centered around the token to be classified, as well as the two previous classifier predictions. Results are reported as F-measure over labeled identified spans. Polynomial vs. Linear models The polynomial kernel of degree 2 allows us to efficiently and implicitly include in our models all feature pairs. Syntactic structure information as captured by pairs of POS-tags and Word-POS pairs is certainly important for such syntactic tasks as Chunking and NER, as demonstrated by the many systems described in (Sang and Buchholz, 2000; Tjong Kim Sang, 2002). By using the polynomial kernel, we can easily make use of this information without intensive feature-tuning for the most successful feature pairs. L1-SVM, L2-SVM and the choice of the C parameter Throughout our experiments, we use the “standard” variant of SVM, L1-penalty soft margin SVM, as implemented by the TinySVM1 software package, with the default C value of 1. This setting is shown to produce good results for sequence labeling tasks in previous work (Kudo and Matsumoto, 2000), and is what most end-users of SVM classifiers are likely to use. As we show in Sect.5.</context>
<context position="19453" citStr="Sang and Buchholz, 2000" startWordPosition="3123" endWordPosition="3127">ious pruning thresholds. Results are presented in Table 2. Without feature pruning (k = 0), the soft-margin SVM performs slightly better than the Anchored-SVM. Either of the results are state-of-the-art for this task. However, even modest pruning (k = 2) hurts the soft-margin model significantly. Not so for the anchored-SVM. Even with relatively aggressive pruning (k = 100), the anchored model still achieves an impressive F-score of 93.83. Remarkably, in that last model, there are only 1,168 active features, and only 209 unique active lexical forms. 5.3 Chunking The goal of the Chunking task (Sang and Buchholz, 2000) is the identification of an assortment of linguistic base-phrases. We use the data from the CoNLL 2000 shared task. Features: We perform two experiments. In the first experiment, we consider the POS and wordform of each token. In this setting, feature pruning resulted in a bigger loss in performance than in the two previous tasks. Preliminary error analysis revealed that many errors are due to tagger errors, especially of the present participle forms. This led us to the second experiment, in which we added as features the 2- and 3- letter suffixes for the word to be classified (but not for th</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: chunking. In CoNLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc of NAACL.</booktitle>
<contexts>
<context position="7842" citStr="Sha and Pereira, 2003" startWordPosition="1228" endWordPosition="1231">ald, 2006) reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy. In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking. As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). Sequence labeling includes both a structural aspect (bracketing the chunks) and a tagging aspect (classifying the chunks). While we expect the structural aspect can benefit from techniques similar to those used in the parsing literature, it is unclear whether the tagging component could perform well without detailed lexical information. We demonstrate in this work that, indeed, lexical features are not necessary to obtain competitive performance. Our approach consists in performing a detailed analysis 1143 of the role played by rare lexical featu</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proc of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In CoNLL-2003.</booktitle>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In CoNLL-2002.</booktitle>
<contexts>
<context position="2251" citStr="Sang, 2002" startWordPosition="351" endWordPosition="352">ankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very </context>
<context position="15466" citStr="Sang, 2002" startWordPosition="2479" endWordPosition="2480">es, we consider properties of tokens in a 5-token window centered around the token to be classified, as well as the two previous classifier predictions. Results are reported as F-measure over labeled identified spans. Polynomial vs. Linear models The polynomial kernel of degree 2 allows us to efficiently and implicitly include in our models all feature pairs. Syntactic structure information as captured by pairs of POS-tags and Word-POS pairs is certainly important for such syntactic tasks as Chunking and NER, as demonstrated by the many systems described in (Sang and Buchholz, 2000; Tjong Kim Sang, 2002). By using the polynomial kernel, we can easily make use of this information without intensive feature-tuning for the most successful feature pairs. L1-SVM, L2-SVM and the choice of the C parameter Throughout our experiments, we use the “standard” variant of SVM, L1-penalty soft margin SVM, as implemented by the TinySVM1 software package, with the default C value of 1. This setting is shown to produce good results for sequence labeling tasks in previous work (Kudo and Matsumoto, 2000), and is what most end-users of SVM classifiers are likely to use. As we show in Sect.5.4, fine-tuning the C pa</context>
<context position="16994" citStr="Sang, 2002" startWordPosition="2729" endWordPosition="2730">ents is not to find the optimal set of lexical features, but rather to demonstrate that most lexical items are not needed for accurate classification in sequence labeling tasks. To this end, we perform very crude frequency based feature pruning. We believe better motivated feature selection technique taking into account linguistic (e.g. prune only open-class words) or statistic information could result in slightly more accurate models with even fewer lexical items. 5 Experiments and Results 5.1 Named Entity Recognition (NER) We use the Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002). The aim is to identify named entities (persons, locations, organizations and miscellaneous) in text. The task has two stages: identification of the entities, and classification of the identified entities into their corresponding types. We focus here on the identification task. Features: We use the following properties for each of the relevant tokens: word-form, POS, ORT, prefix1, prefix2, prefix3, suffix1, suffix2, suffix3. The ORT feature can take one of the following values: {number, contains-digit, containshyphen, capitalized, all-capitalized, URL, punctuation, regular}. 1http://chasen.or</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition. In CoNLL-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="2817" citStr="Vapnik, 1995" startWordPosition="445" endWordPosition="446"> Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We pro</context>
<context position="10316" citStr="Vapnik, 1995" startWordPosition="1625" endWordPosition="1626">ning SVM models hurts the performance of the learned models. Our intuition is that this sensitivity to rare lexical features is not explained by the richness of information such rare features bring to the model. Instead, we believe that rare lexical features help the classifier because they make the data artificially more separable. To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). SVMs cope with inseparable data by introducing a soft-margin – allowing some of the training instances to be classified incorrectly subject to a penalty, controlled by a parameter C. Anchored SVM As we show in Section 5, the soft-margin heuristic</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Fred Damerau</author>
<author>David Johnson</author>
</authors>
<title>Text chunking using regularized winnow.</title>
<date>2001</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="7819" citStr="Zhang et al., 2001" startWordPosition="1224" endWordPosition="1227">l analysis in (McDonald, 2006) reveals that bilexical features hardly contribute to the performance of a discriminative MSTbased dependency parser, while Kawahara and Uchimoto (2007) demonstrate that minimallylexicalized shift-reduce based dependency parsers can produce near state-of-the-art accuracy. In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking. As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). Sequence labeling includes both a structural aspect (bracketing the chunks) and a tagging aspect (classifying the chunks). While we expect the structural aspect can benefit from techniques similar to those used in the parsing literature, it is unclear whether the tagging component could perform well without detailed lexical information. We demonstrate in this work that, indeed, lexical features are not necessary to obtain competitive performance. Our approach consists in performing a detailed analysis 1143 of the role playe</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2001</marker>
<rawString>Tong Zhang, Fred Damerau, and David Johnson. 2001. Text chunking using regularized winnow. In Proc of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>