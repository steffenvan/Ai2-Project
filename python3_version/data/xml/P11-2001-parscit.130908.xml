<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006146">
<title confidence="0.995015">
Lexicographic Semirings for Exact Automata Encoding of Sequence Models
</title>
<author confidence="0.976922">
Brian Roark, Richard Sproat, and Izhak Shafran
</author>
<email confidence="0.974987">
froark,rws,zakl@cslu.ogi.edu
</email>
<sectionHeader confidence="0.997462" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.990259933333334">
In this paper we introduce a novel use of the
lexicographic semiring and motivate its use
for speech and language processing tasks. We
prove that the semiring allows for exact en-
coding of backoff models with epsilon tran-
sitions. This allows for off-line optimization
of exact models represented as large weighted
finite-state transducers in contrast to implicit
(on-line) failure transition representations. We
present preliminary empirical results demon-
strating that, even in simple intersection sce-
narios amenable to the use of failure transi-
tions, the use of the more powerful lexico-
graphic semiring is competitive in terms of
time of intersection.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999930857142857">
Representing smoothed n-gram language models as
weighted finite-state transducers (WFST) is most
naturally done with a failure transition, which re-
flects the semantics of the “otherwise” formulation
of smoothing (Allauzen et al., 2003). For example,
the typical backoff formulation of the probability of
a word w given a history h is as follows
</bodyText>
<equation confidence="0.986923">
P(w  |h) = { ahp(wI h&apos;) othe(rwi)e 0 (1)
</equation>
<bodyText confidence="0.985672376623377">
where P is an empirical estimate of the probabil-
ity that reserves small finite probability for unseen
n-grams; αh is a backoff weight that ensures nor-
malization; and h&apos; is a backoff history typically
achieved by excising the earliest word in the his-
tory h. The principle benefit of encoding the WFST
in this way is that it only requires explicitly storing
n-gram transitions for observed n-grams, i.e., count
greater than zero, as opposed to all possible n-grams
of the given order which would be infeasible in for
example large vocabulary speech recognition. This
is a massive space savings, and such an approach is
also used for non-probabilistic stochastic language
1
models, such as those trained with the perceptron
algorithm (Roark et al., 2007), as the means to ac-
cess all and exactly those features that should fire
for a particular sequence in a deterministic automa-
ton. Similar issues hold for other finite-state se-
quence processing problems, e.g., tagging, bracket-
ing or segmenting.
Failure transitions, however, are an implicit
method for representing a much larger explicit au-
tomaton – in the case of n-gram models, all pos-
sible n-grams for that order. During composition
with the model, the failure transition must be inter-
preted on the fly, keeping track of those symbols
that have already been found leaving the original
state, and only allowing failure transition traversal
for symbols that have not been found (the semantics
of “otherwise”). This compact implicit representa-
tion cannot generally be preserved when composing
with other models, e.g., when combining a language
model with a pronunciation lexicon as in widely-
used FST approaches to speech recognition (Mohri
et al., 2002). Moving from implicit to explicit repre-
sentation when performing such a composition leads
to an explosion in the size of the resulting trans-
ducer, frequently making the approach intractable.
In practice, an off-line approximation to the model
is made, typically by treating the failure transitions
as epsilon transitions (Mohri et al., 2002; Allauzen
et al., 2003), allowing large transducers to be com-
posed and optimized off-line. These complex ap-
proximate transducers are then used during first-pass
decoding, and the resulting pruned search graphs
(e.g., word lattices) can be rescored with exact lan-
guage models encoded with failure transitions.
Similar problems arise when building, say, POS-
taggers as WFST: not every pos-tag sequence will
have been observed during training, hence failure
transitions will achieve great savings in the size of
models. Yet discriminative models may include
complex features that combine both input stream
(word) and output stream (tag) sequences in a single
feature, yielding complicated transducer topologies
for which effective use of failure transitions may not
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 1–5,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
be possible. An exact encoding using other mecha-
nisms is required in such cases to allow for off-line
representation and optimization.
In this paper, we introduce a novel use of a semir-
ing – the lexicographic semiring (Golan, 1999) –
which permits an exact encoding of these sorts of
models with the same compact topology as with fail-
ure transitions, but using epsilon transitions. Unlike
the standard epsilon approximation, this semiring al-
lows for an exact representation, while also allow-
ing (unlike failure transition approaches) for off-line
composition with other transducers, with all the op-
timizations that such representations provide.
In the next section, we introduce the semiring, fol-
lowed by a proof that its use yields exact represen-
tations. We then conclude with a brief evaluation of
the cost of intersection relative to failure transitions
in comparable situations.
</bodyText>
<sectionHeader confidence="0.969663" genericHeader="method">
2 The Lexicographic Semiring
</sectionHeader>
<bodyText confidence="0.999946476190476">
Weighted automata are automata in which the tran-
sitions carry weight elements of a semiring (Kuich
and Salomaa, 1986). A semiring is a ring that may
lack negation, with two associative operations ⊕ and
⊗ and their respective identity elements 0 and 1. A
common semiring in speech and language process-
ing, and one that we will be using in this paper, is
the tropical semiring (R ∪ {∞}, min, +, ∞, 0), i.e.,
min is the ⊕ of the semiring (with identity ∞) and
+ is the ⊗ of the semiring (with identity 0). This is
appropriate for performing Viterbi search using neg-
ative log probabilities – we add negative logs along
a path and take the min between paths.
A hW1, W2 ... W,,,i-lexicographic weight is a tu-
ple of weights where each of the weight classes
W1, W2 ... W,,,, must observe the path property
(Mohri, 2002). The path property of a semiring K
is defined in terms of the natural order on K such
that: a &lt;K b iff a ⊕ b = a. The tropical semiring
mentioned above is a common example of a semir-
ing that observes the path property, since:
</bodyText>
<equation confidence="0.9998005">
w1 ⊕ w2 = min{w1, w2}
w1 ⊗ w2 = w1 + w2
</equation>
<bodyText confidence="0.999846">
The discussion in this paper will be restricted to
lexicographic weights consisting of a pair of tropi-
cal weights — henceforth the hT, Ti-lexicographic
semiring. For this semiring the operations ⊕ and ⊗
are defined as follows (Golan, 1999, pp. 223–224):
</bodyText>
<equation confidence="0.9343356">
if w1 &lt; w3 or
hw1, w2i (w1 = w3 &amp;
w2 &lt; w4)
hw3, w4i otherwise
hw1, w2i ⊗ hw3, w4i = hw1 + w3, w2 + w4i
</equation>
<bodyText confidence="0.9988735">
The term “lexicographic” is an apt term for this
semiring since the comparison for ⊕ is like the lexi-
cographic comparison of strings, comparing the first
elements, then the second, and so forth.
</bodyText>
<sectionHeader confidence="0.991592" genericHeader="method">
3 Language model encoding
</sectionHeader>
<subsectionHeader confidence="0.999683">
3.1 Standard encoding
</subsectionHeader>
<bodyText confidence="0.999986882352941">
For language model encoding, we will differentiate
between two classes of transitions: backoff arcs (la-
beled with a O for failure, or with E using our new
semiring); and n-gram arcs (everything else, labeled
with the word whose probability is assigned). Each
state in the automaton represents an n-gram history
string h and each n-gram arc is weighted with the
(negative log) conditional probability of the word w
labeling the arc given the history h. For a given his-
tory h and n-gram arc labeled with a word w, the
destination of the arc is the state associated with the
longest suffix of the string hw that is a history in the
model. This will depend on the Markov order of the
n-gram model. For example, consider the trigram
model schematic shown in Figure 1, in which only
history sequences of length 2 are kept in the model.
Thus, from history hz = wz−2wz−1, the word wz
transitions to hz+1 = wz−1wz, which is the longest
suffix of hzwz in the model.
As detailed in the “otherwise” semantics of equa-
tion 1, backoff arcs transition from state h to a state
h&apos;, typically the suffix of h of length |h |− 1, with
weight (− log αh). We call the destination state a
backoff state. This recursive backoff topology ter-
minates at the unigram state, i.e., h = E, no history.
Backoff states of order k may be traversed either
via O-arcs from the higher order n-gram of order k+
1 or via an n-gram arc from a lower order n-gram of
order k −1. This means that no n-gram arc can enter
the zeroeth order state (final backoff), and full-order
states — history strings of length n − 1 for a model
of order n — may have n-gram arcs entering from
other full-order states as well as from backoff states
of history size n − 2.
</bodyText>
<subsectionHeader confidence="0.999645">
3.2 Encoding with lexicographic semiring
</subsectionHeader>
<bodyText confidence="0.999873">
For an LM machine M on the tropical semiring with
failure transitions, which is deterministic and has the
</bodyText>
<equation confidence="0.609783">
hw1, w2i ⊕ hw3, w4i = I
</equation>
<page confidence="0.977245">
2
</page>
<figureCaption confidence="0.9923045">
Figure 1: Deterministic finite-state representation of n-gram
models with negative log probabilities (tropical semiring). The
symbol 0 labels backoff transitions. Modified from Roark and
Sproat (2007), Figure 6.1.
</figureCaption>
<bodyText confidence="0.997988945945946">
path property, we can simulate 0-arcs in a standard
LM topology by a topologically equivalent machine
M0 on the lexicographic (T, T) semiring, where 0
has been replaced with epsilon, as follows. For every
n-gram arc with label w and weight c, source state
si and destination state sj, construct an n-gram arc
with label w, weight (0, c), source state s0i, and des-
tination state s0j. The exit cost of each state is con-
structed as follows. If the state is non-final, (oc, oc).
Otherwise if it final with exit cost c it will be (0, c).
Let n be the length of the longest history string in
the model. For every 0-arc with (backoff) weight
c, source state si, and destination state sj repre-
senting a history of length k, construct an c-arc
with source state s0i, destination state s0j, and weight
(b⊗(n−k), c), where b &gt; 0 and b⊗(n−k) takes b to
the (n − k)th power with the ® operation. In the
tropical semiring, ® is +, so b⊗(n−k) = (n − k)b.
For example, in a trigram model, if we are backing
off from a bigram state h (history length = 1) to a
unigram state, n − k = 2 − 0 = 2, so we set the
backoff weight to (2b, − log αh) for some b &gt; 0.
In order to combine the model with another au-
tomaton or transducer, we would need to also con-
vert those models to the (T, T) semiring. For these
automata, we simply use a default transformation
such that every transition with weight c is assigned
weight (0, c). For example, given a word lattice
L, we convert the lattice to L0 in the lexicographic
semiring using this default transformation, and then
perform the intersection L0 n M0. By removing ep-
silon transitions and determinizing the result, the
low cost path for any given string will be retained
in the result, which will correspond to the path
achieved with 0-arcs. Finally we project the second
dimension of the (T, T) weights to produce a lattice
in the tropical semiring, which is equivalent to the
</bodyText>
<equation confidence="0.767594">
result of L n M, i.e.,
C2(det(eps-rem(L0 n M0))) = L n M
</equation>
<bodyText confidence="0.998961">
where C2 denotes projecting the second-dimension
of the (T, T) weights, det(·) denotes determiniza-
tion, and eps-rem(·) denotes c-removal.
</bodyText>
<sectionHeader confidence="0.998676" genericHeader="method">
4 Proof
</sectionHeader>
<bodyText confidence="0.999198607142857">
We wish to prove that for any machine N,
ShortestPath(M0 n N0) passes through the equiv-
alent states in M0 to those passed through in M for
ShortestPath(M n N). Therefore determinization
of the resulting intersection after c-removal yields
the same topology as intersection with the equiva-
lent 0 machine. Intuitively, since the first dimension
of the (T, T) weights is 0 for n-gram arcs and &gt; 0
for backoff arcs, the shortest path will traverse the
fewest possible backoff arcs; further, since higher-
order backoff arcs cost less in the first dimension of
the (T, T) weights in M0, the shortest path will in-
clude n-gram arcs at their earliest possible point.
We prove this by induction on the state-sequence
of the path p/p0 up to a given state si/s0i in the respec-
tive machines M/M0.
Base case: If p/p0 is of length 0, and therefore the
states si/s0i are the initial states of the respective ma-
chines, the proposition clearly holds.
Inductive step: Now suppose that p/p0 visits
s0...si/s00...s0i and we have therefore reached si/s0i
in the respective machines. Suppose the cumulated
weights of p/p0 are W and (IF, W), respectively. We
wish to show that whichever sj is next visited on p
(i.e., the path becomes s0...sisj) the equivalent state
s0 is visited on p0 (i.e., the path becomes s00...s0is0j).
Let w be the next symbol to be matched leaving
states si and s0i. There are four cases to consider:
</bodyText>
<listItem confidence="0.998701125">
(1) there is an n-gram arc leaving states si and s0i la-
beled with w, but no backoff arc leaving the state;
(2) there is no n-gram arc labeled with w leaving the
states, but there is a backoff arc; (3) there is no n-
gram arc labeled with w and no backoff arc leaving
the states; and (4) there is both an n-gram arc labeled
with w and a backoff arc leaving the states. In cases
(1) and (2), there is only one possible transition to
</listItem>
<bodyText confidence="0.787743666666667">
take in either M or M0, and based on the algorithm
for construction of M0 given in Section 3.2, these
transitions will point to sj and s0j respectively. Case
</bodyText>
<listItem confidence="0.683772">
(3) leads to failure of intersection with either ma-
chine. This leaves case (4) to consider. In M, since
there is a transition leaving state si labeled with w,
</listItem>
<equation confidence="0.99769625">
hi =
wi-2wi-1
wi/-logP(wi|h i)
φ/ log αhi
wi 1
wi/ logP(wi|wi-1)
φ/ log αwi-1
hi+1 =
wi 1wi
wi/-logP(wi)
φ/-log αhi+1
wi
</equation>
<page confidence="0.974089">
3
</page>
<bodyText confidence="0.955212189189189">
the backoff arc, which is a failure transition, can-
not be traversed, hence the destination of the n-gram
arc sj will be the next state in p. However, in M0,
both the n-gram transition labeled with w and the
backoff transition, now labeled with c, can be tra-
versed. What we will now prove is that the shortest
path through M0 cannot include taking the backoff
arc in this case.
In order to emit w by taking the backoff arc out
of state s0i, one or more backoff (c) transitions must
be taken, followed by an n-gram arc labeled with
w. Let k be the order of the history represented
by state s0i, hence the cost of the first backoff arc
is ((n − k)-b, − log(αsz)) in our semiring. If we
traverse m backoff arcs prior to emitting the w,
the first dimension of our accumulated cost will be
m(n − k + m−1
2 )�, based on our algorithm for con-
struction of M0 given in Section 3.2. Let s0l be the
destination state after traversing m backoff arcs fol-
lowed by an n-gram arc labeled with w. Note that,
by definition, m &lt; k, and k − m + 1 is the or-
der of state s0l. Based on the construction algo-
rithm, the state s0l is also reachable by first emit-
ting w from state s0i to reach state s0 j followed by
some number of backoff transitions. The order of
state s0 j is either k (if k is the highest order in the
model) or k + 1 (by extending the history of state
s0i by one word). If it is of order k, then it will re-
quire m − 1 backoff arcs to reach state s0l, one fewer
than the path to state s0l that begins with a back-
off arc, for a total cost of (m − 1)(n − k + m−1
2 )-b
which is less than m(n − k + m−1
2 )�. If state
s0 j is of order k + 1, there will be m backoff
arcs to reach state s0l, but with a total cost of
</bodyText>
<equation confidence="0.786658">
m(n − (k + 1) + m−1
2 )� = m(n − k + m−3
2 )�
which is also less than m(n − k + m−1
</equation>
<bodyText confidence="0.974256">
2 )�. Hence
the state s0l can always be reached from s0i with a
lower cost through state s0 j than by first taking the
backoff arc from s0i. Therefore the shortest path on
M0 must follow s00...s0is0j. E]
This completes the proof.
</bodyText>
<sectionHeader confidence="0.994315" genericHeader="evaluation">
5 Experimental Comparison of c, 0 and
</sectionHeader>
<bodyText confidence="0.998603705882353">
(T, T) encoded language models
For our experiments we used lattices derived from a
very large vocabulary continuous speech recognition
system, which was built for the 2007 GALE Ara-
bic speech recognition task, and used in the work
reported in Lehr and Shafran (2011). The lexico-
graphic semiring was evaluated on the development
set (2.6 hours of broadcast news and conversations;
18K words). The 888 word lattices for the develop-
ment set were generated using a competitive base-
line system with acoustic models trained on about
1000 hrs of Arabic broadcast data and a 4-gram lan-
guage model. The language model consisting of
122M n-grams was estimated by interpolation of 14
components. The vocabulary is relatively large at
737K and the associated dictionary has only single
pronunciations.
The language model was converted to the automa-
ton topology described earlier, and represented in
three ways: first as an approximation of a failure
machine using epsilons instead of failure arcs; sec-
ond as a correct failure machine; and third using the
lexicographic construction derived in this paper.
The three versions of the LM were evaluated by
intersecting them with the 888 lattices of the de-
velopment set. The overall error rate for the sys-
tems was 24.8%—comparable to the state-of-the-
art on this task1. For the shortest paths, the failure
and lexicographic machines always produced iden-
tical lattices (as determined by FST equivalence);
in contrast, 81% of the shortest paths from the ep-
silon approximation are different, at least in terms
of weights, from the shortest paths using the failure
LM. For full lattices, 42 (4.7%) of the lexicographic
outputs differ from the failure LM outputs, due to
small floating point rounding issues; 863 (97%) of
the epsilon approximation outputs differ.
In terms of size, the failure LM, with 5.7 mil-
lion arcs requires 97 Mb. The equivalent (T, T)-
lexicographic LM requires 120 Mb, due to the dou-
bling of the size of the weights.2 To measure speed,
we performed the intersections 1000 times for each
of our 888 lattices on a 2993 MHz Intel ® Xeon®
CPU, and took the mean times for each of our meth-
ods. The 888 lattices were processed with a mean
of 1.62 seconds in total (1.8 msec per lattice) us-
ing the failure LM; using the (T, T)-lexicographic
LM required 1.8 seconds (2.0 msec per lattice), and
is thus about 11% slower. Epsilon approximation,
where the failure arcs are approximated with epsilon
arcs took 1.17 seconds (1.3 msec per lattice). The
</bodyText>
<footnote confidence="0.9841005">
1The error rate is a couple of points higher than in Lehr and
Shafran (2011) since we discarded non-lexical words, which are
absent in maximum likelihood estimated language model and
are typically augmented to the unigram backoff state with an
arbitrary cost, fine-tuned to optimize performance for a given
task.
2If size became an issue, the first dimension of the (T, T)-
weight can be represented by a single byte.
</footnote>
<page confidence="0.99541">
4
</page>
<bodyText confidence="0.99985575">
slightly slower speeds for the exact method using the
failure LM, and (T, T) can be related to the over-
head of computing the failure function at runtime,
and determinization, respectively.
</bodyText>
<sectionHeader confidence="0.999149" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9993285">
In this paper we have introduced a novel applica-
tion of the lexicographic semiring, proved that it
can be used to provide an exact encoding of lan-
guage model topologies with failure arcs, and pro-
vided experimental results that demonstrate its ef-
ficiency. Since the (T, T)-lexicographic semiring
is both left- and right-distributive, other optimiza-
tions such as minimization are possible. The par-
ticular (T, T)-lexicographic semiring we have used
here is but one of many possible lexicographic en-
codings. We are currently exploring the use of a
lexicographic semiring that involves different semir-
ings in the various dimensions, for the integration of
part-of-speech taggers into language models.
An implementation of the lexicographic semir-
ing by the second author is already available as
part of the OpenFst package (Allauzen et al., 2007).
The methods described here are part of the NGram
language-model-training toolkit, soon to be released
at opengrm.org.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999359125">
This research was supported in part by NSF Grant
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA. We thank Maider Lehr for
help in preparing the test data. We also thank the
ACL reviewers for valuable comments.
</bodyText>
<sectionHeader confidence="0.999463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999877388888889">
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 40–47.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Twelfth International
Conference on Implementation and Application of Au-
tomata (CIAA 2007), Lecture Notes in Computer Sci-
ence, volume 4793, pages 11–23, Prague, Czech Re-
public. Springer.
Jonathan Golan. 1999. Semirings and theirApplications.
Kluwer Academic Publishers, Dordrecht.
Werner Kuich and Arto Salomaa. 1986. Semirings,
Automata, Languages. Number 5 in EATCS Mono-
graphs on Theoretical Computer Science. Springer-
Verlag, Berlin, Germany.
Maider Lehr and Izhak Shafran. 2011. Learning a dis-
criminative weighted finite-state transducer for speech
recognition. IEEE Transactions on Audio, Speech, and
Language Processing, July.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69–88.
Mehryar Mohri. 2002. Semiring framework and algo-
rithms for shortest-distance problems. Journal of Au-
tomata, Languages and Combinatorics, 7(3):321–350.
Brian Roark and Richard Sproat. 2007. Computational
Approaches to Morphology and Syntax. Oxford Uni-
versity Press, Oxford.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373–392.
</reference>
<page confidence="0.994387">
5
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.990720">
<title confidence="0.999147">Lexicographic Semirings for Exact Automata Encoding of Sequence Models</title>
<author confidence="0.994318">Brian Roark</author>
<author confidence="0.994318">Richard Sproat</author>
<author confidence="0.994318">Izhak</author>
<abstract confidence="0.9997905625">In this paper we introduce a novel use of the lexicographic semiring and motivate its use for speech and language processing tasks. We prove that the semiring allows for exact encoding of backoff models with epsilon transitions. This allows for off-line optimization of exact models represented as large weighted finite-state transducers in contrast to implicit (on-line) failure transition representations. We present preliminary empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions, the use of the more powerful lexicographic semiring is competitive in terms of time of intersection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1073" citStr="Allauzen et al., 2003" startWordPosition="151" endWordPosition="154">as large weighted finite-state transducers in contrast to implicit (on-line) failure transition representations. We present preliminary empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions, the use of the more powerful lexicographic semiring is competitive in terms of time of intersection. 1 Introduction and Motivation Representing smoothed n-gram language models as weighted finite-state transducers (WFST) is most naturally done with a failure transition, which reflects the semantics of the “otherwise” formulation of smoothing (Allauzen et al., 2003). For example, the typical backoff formulation of the probability of a word w given a history h is as follows P(w |h) = { ahp(wI h&apos;) othe(rwi)e 0 (1) where P is an empirical estimate of the probability that reserves small finite probability for unseen n-grams; αh is a backoff weight that ensures normalization; and h&apos; is a backoff history typically achieved by excising the earliest word in the history h. The principle benefit of encoding the WFST in this way is that it only requires explicitly storing n-gram transitions for observed n-grams, i.e., count greater than zero, as opposed to all poss</context>
<context position="3290" citStr="Allauzen et al., 2003" startWordPosition="511" endWordPosition="514"> “otherwise”). This compact implicit representation cannot generally be preserved when composing with other models, e.g., when combining a language model with a pronunciation lexicon as in widelyused FST approaches to speech recognition (Mohri et al., 2002). Moving from implicit to explicit representation when performing such a composition leads to an explosion in the size of the resulting transducer, frequently making the approach intractable. In practice, an off-line approximation to the model is made, typically by treating the failure transitions as epsilon transitions (Mohri et al., 2002; Allauzen et al., 2003), allowing large transducers to be composed and optimized off-line. These complex approximate transducers are then used during first-pass decoding, and the resulting pruned search graphs (e.g., word lattices) can be rescored with exact language models encoded with failure transitions. Similar problems arise when building, say, POStaggers as WFST: not every pos-tag sequence will have been observed during training, hence failure transitions will achieve great savings in the size of models. Yet discriminative models may include complex features that combine both input stream (word) and output str</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twelfth International Conference on Implementation and Application of Automata (CIAA 2007), Lecture Notes in Computer Science,</booktitle>
<volume>4793</volume>
<pages>11--23</pages>
<publisher>Springer.</publisher>
<location>Prague, Czech Republic.</location>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of the Twelfth International Conference on Implementation and Application of Automata (CIAA 2007), Lecture Notes in Computer Science, volume 4793, pages 11–23, Prague, Czech Republic. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Golan</author>
</authors>
<title>Semirings and theirApplications.</title>
<date>1999</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="4457" citStr="Golan, 1999" startWordPosition="685" endWordPosition="686">mbine both input stream (word) and output stream (tag) sequences in a single feature, yielding complicated transducer topologies for which effective use of failure transitions may not Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 1–5, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics be possible. An exact encoding using other mechanisms is required in such cases to allow for off-line representation and optimization. In this paper, we introduce a novel use of a semiring – the lexicographic semiring (Golan, 1999) – which permits an exact encoding of these sorts of models with the same compact topology as with failure transitions, but using epsilon transitions. Unlike the standard epsilon approximation, this semiring allows for an exact representation, while also allowing (unlike failure transition approaches) for off-line composition with other transducers, with all the optimizations that such representations provide. In the next section, we introduce the semiring, followed by a proof that its use yields exact representations. We then conclude with a brief evaluation of the cost of intersection relati</context>
<context position="6454" citStr="Golan, 1999" startWordPosition="1038" endWordPosition="1039"> of weights where each of the weight classes W1, W2 ... W,,,, must observe the path property (Mohri, 2002). The path property of a semiring K is defined in terms of the natural order on K such that: a &lt;K b iff a ⊕ b = a. The tropical semiring mentioned above is a common example of a semiring that observes the path property, since: w1 ⊕ w2 = min{w1, w2} w1 ⊗ w2 = w1 + w2 The discussion in this paper will be restricted to lexicographic weights consisting of a pair of tropical weights — henceforth the hT, Ti-lexicographic semiring. For this semiring the operations ⊕ and ⊗ are defined as follows (Golan, 1999, pp. 223–224): if w1 &lt; w3 or hw1, w2i (w1 = w3 &amp; w2 &lt; w4) hw3, w4i otherwise hw1, w2i ⊗ hw3, w4i = hw1 + w3, w2 + w4i The term “lexicographic” is an apt term for this semiring since the comparison for ⊕ is like the lexicographic comparison of strings, comparing the first elements, then the second, and so forth. 3 Language model encoding 3.1 Standard encoding For language model encoding, we will differentiate between two classes of transitions: backoff arcs (labeled with a O for failure, or with E using our new semiring); and n-gram arcs (everything else, labeled with the word whose probabilit</context>
</contexts>
<marker>Golan, 1999</marker>
<rawString>Jonathan Golan. 1999. Semirings and theirApplications. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Kuich</author>
<author>Arto Salomaa</author>
</authors>
<date>1986</date>
<journal>Semirings, Automata, Languages. Number</journal>
<booktitle>in EATCS Monographs on Theoretical Computer Science.</booktitle>
<volume>5</volume>
<publisher>SpringerVerlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="5255" citStr="Kuich and Salomaa, 1986" startWordPosition="806" endWordPosition="809">ilon approximation, this semiring allows for an exact representation, while also allowing (unlike failure transition approaches) for off-line composition with other transducers, with all the optimizations that such representations provide. In the next section, we introduce the semiring, followed by a proof that its use yields exact representations. We then conclude with a brief evaluation of the cost of intersection relative to failure transitions in comparable situations. 2 The Lexicographic Semiring Weighted automata are automata in which the transitions carry weight elements of a semiring (Kuich and Salomaa, 1986). A semiring is a ring that may lack negation, with two associative operations ⊕ and ⊗ and their respective identity elements 0 and 1. A common semiring in speech and language processing, and one that we will be using in this paper, is the tropical semiring (R ∪ {∞}, min, +, ∞, 0), i.e., min is the ⊕ of the semiring (with identity ∞) and + is the ⊗ of the semiring (with identity 0). This is appropriate for performing Viterbi search using negative log probabilities – we add negative logs along a path and take the min between paths. A hW1, W2 ... W,,,i-lexicographic weight is a tuple of weights </context>
</contexts>
<marker>Kuich, Salomaa, 1986</marker>
<rawString>Werner Kuich and Arto Salomaa. 1986. Semirings, Automata, Languages. Number 5 in EATCS Monographs on Theoretical Computer Science. SpringerVerlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maider Lehr</author>
<author>Izhak Shafran</author>
</authors>
<title>Learning a discriminative weighted finite-state transducer for speech recognition.</title>
<date>2011</date>
<booktitle>IEEE Transactions on Audio, Speech, and Language Processing,</booktitle>
<contexts>
<context position="15611" citStr="Lehr and Shafran (2011)" startWordPosition="2769" endWordPosition="2772">st of m(n − (k + 1) + m−1 2 )� = m(n − k + m−3 2 )� which is also less than m(n − k + m−1 2 )�. Hence the state s0l can always be reached from s0i with a lower cost through state s0 j than by first taking the backoff arc from s0i. Therefore the shortest path on M0 must follow s00...s0is0j. E] This completes the proof. 5 Experimental Comparison of c, 0 and (T, T) encoded language models For our experiments we used lattices derived from a very large vocabulary continuous speech recognition system, which was built for the 2007 GALE Arabic speech recognition task, and used in the work reported in Lehr and Shafran (2011). The lexicographic semiring was evaluated on the development set (2.6 hours of broadcast news and conversations; 18K words). The 888 word lattices for the development set were generated using a competitive baseline system with acoustic models trained on about 1000 hrs of Arabic broadcast data and a 4-gram language model. The language model consisting of 122M n-grams was estimated by interpolation of 14 components. The vocabulary is relatively large at 737K and the associated dictionary has only single pronunciations. The language model was converted to the automaton topology described earlier</context>
<context position="17911" citStr="Lehr and Shafran (2011)" startWordPosition="3157" endWordPosition="3160">the size of the weights.2 To measure speed, we performed the intersections 1000 times for each of our 888 lattices on a 2993 MHz Intel ® Xeon® CPU, and took the mean times for each of our methods. The 888 lattices were processed with a mean of 1.62 seconds in total (1.8 msec per lattice) using the failure LM; using the (T, T)-lexicographic LM required 1.8 seconds (2.0 msec per lattice), and is thus about 11% slower. Epsilon approximation, where the failure arcs are approximated with epsilon arcs took 1.17 seconds (1.3 msec per lattice). The 1The error rate is a couple of points higher than in Lehr and Shafran (2011) since we discarded non-lexical words, which are absent in maximum likelihood estimated language model and are typically augmented to the unigram backoff state with an arbitrary cost, fine-tuned to optimize performance for a given task. 2If size became an issue, the first dimension of the (T, T)- weight can be represented by a single byte. 4 slightly slower speeds for the exact method using the failure LM, and (T, T) can be related to the overhead of computing the failure function at runtime, and determinization, respectively. 6 Conclusion In this paper we have introduced a novel application o</context>
</contexts>
<marker>Lehr, Shafran, 2011</marker>
<rawString>Maider Lehr and Izhak Shafran. 2011. Learning a discriminative weighted finite-state transducer for speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2925" citStr="Mohri et al., 2002" startWordPosition="455" endWordPosition="458">cit automaton – in the case of n-gram models, all possible n-grams for that order. During composition with the model, the failure transition must be interpreted on the fly, keeping track of those symbols that have already been found leaving the original state, and only allowing failure transition traversal for symbols that have not been found (the semantics of “otherwise”). This compact implicit representation cannot generally be preserved when composing with other models, e.g., when combining a language model with a pronunciation lexicon as in widelyused FST approaches to speech recognition (Mohri et al., 2002). Moving from implicit to explicit representation when performing such a composition leads to an explosion in the size of the resulting transducer, frequently making the approach intractable. In practice, an off-line approximation to the model is made, typically by treating the failure transitions as epsilon transitions (Mohri et al., 2002; Allauzen et al., 2003), allowing large transducers to be composed and optimized off-line. These complex approximate transducers are then used during first-pass decoding, and the resulting pruned search graphs (e.g., word lattices) can be rescored with exact</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Semiring framework and algorithms for shortest-distance problems.</title>
<date>2002</date>
<journal>Journal of Automata, Languages and Combinatorics,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="5949" citStr="Mohri, 2002" startWordPosition="939" endWordPosition="940">nd ⊗ and their respective identity elements 0 and 1. A common semiring in speech and language processing, and one that we will be using in this paper, is the tropical semiring (R ∪ {∞}, min, +, ∞, 0), i.e., min is the ⊕ of the semiring (with identity ∞) and + is the ⊗ of the semiring (with identity 0). This is appropriate for performing Viterbi search using negative log probabilities – we add negative logs along a path and take the min between paths. A hW1, W2 ... W,,,i-lexicographic weight is a tuple of weights where each of the weight classes W1, W2 ... W,,,, must observe the path property (Mohri, 2002). The path property of a semiring K is defined in terms of the natural order on K such that: a &lt;K b iff a ⊕ b = a. The tropical semiring mentioned above is a common example of a semiring that observes the path property, since: w1 ⊕ w2 = min{w1, w2} w1 ⊗ w2 = w1 + w2 The discussion in this paper will be restricted to lexicographic weights consisting of a pair of tropical weights — henceforth the hT, Ti-lexicographic semiring. For this semiring the operations ⊕ and ⊗ are defined as follows (Golan, 1999, pp. 223–224): if w1 &lt; w3 or hw1, w2i (w1 = w3 &amp; w2 &lt; w4) hw3, w4i otherwise hw1, w2i ⊗ hw3, w</context>
</contexts>
<marker>Mohri, 2002</marker>
<rawString>Mehryar Mohri. 2002. Semiring framework and algorithms for shortest-distance problems. Journal of Automata, Languages and Combinatorics, 7(3):321–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>Computational Approaches to Morphology and Syntax.</title>
<date>2007</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="8900" citStr="Roark and Sproat (2007)" startWordPosition="1482" endWordPosition="1485"> arc can enter the zeroeth order state (final backoff), and full-order states — history strings of length n − 1 for a model of order n — may have n-gram arcs entering from other full-order states as well as from backoff states of history size n − 2. 3.2 Encoding with lexicographic semiring For an LM machine M on the tropical semiring with failure transitions, which is deterministic and has the hw1, w2i ⊕ hw3, w4i = I 2 Figure 1: Deterministic finite-state representation of n-gram models with negative log probabilities (tropical semiring). The symbol 0 labels backoff transitions. Modified from Roark and Sproat (2007), Figure 6.1. path property, we can simulate 0-arcs in a standard LM topology by a topologically equivalent machine M0 on the lexicographic (T, T) semiring, where 0 has been replaced with epsilon, as follows. For every n-gram arc with label w and weight c, source state si and destination state sj, construct an n-gram arc with label w, weight (0, c), source state s0i, and destination state s0j. The exit cost of each state is constructed as follows. If the state is non-final, (oc, oc). Otherwise if it final with exit cost c it will be (0, c). Let n be the length of the longest history string in </context>
</contexts>
<marker>Roark, Sproat, 2007</marker>
<rawString>Brian Roark and Richard Sproat. 2007. Computational Approaches to Morphology and Syntax. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Discriminative n-gram language modeling.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1974" citStr="Roark et al., 2007" startWordPosition="304" endWordPosition="307">t that ensures normalization; and h&apos; is a backoff history typically achieved by excising the earliest word in the history h. The principle benefit of encoding the WFST in this way is that it only requires explicitly storing n-gram transitions for observed n-grams, i.e., count greater than zero, as opposed to all possible n-grams of the given order which would be infeasible in for example large vocabulary speech recognition. This is a massive space savings, and such an approach is also used for non-probabilistic stochastic language 1 models, such as those trained with the perceptron algorithm (Roark et al., 2007), as the means to access all and exactly those features that should fire for a particular sequence in a deterministic automaton. Similar issues hold for other finite-state sequence processing problems, e.g., tagging, bracketing or segmenting. Failure transitions, however, are an implicit method for representing a much larger explicit automaton – in the case of n-gram models, all possible n-grams for that order. During composition with the model, the failure transition must be interpreted on the fly, keeping track of those symbols that have already been found leaving the original state, and onl</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer Speech and Language, 21(2):373–392.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>