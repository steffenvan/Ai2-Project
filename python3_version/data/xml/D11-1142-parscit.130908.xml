<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996533">
Identifying Relations for Open Information Extraction
</title>
<author confidence="0.999533">
Anthony Fader, Stephen Soderland, and Oren Etzioni
</author>
<affiliation confidence="0.999774">
University of Washington, Seattle
</affiliation>
<email confidence="0.99939">
{afader,soderlan,etzioni}@cs.washington.edu
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998657263157895">
Open Information Extraction (IE) is the task
of extracting assertions from massive corpora
without requiring a pre-specified vocabulary.
This paper shows that the output of state-of-
the-art Open IE systems is rife with uninfor-
mative and incoherent extractions. To over-
come these problems, we introduce two sim-
ple syntactic and lexical constraints on bi-
nary relations expressed by verbs. We im-
plemented the constraints in the REVERB
Open IE system, which more than doubles the
area under the precision-recall curve relative
to previous extractors such as TEXTRUNNER
and WOEpo3. More than 30% of REVERB’s
extractions are at precision 0.8 or higher—
compared to virtually none for earlier systems.
The paper concludes with a detailed analysis
of REVERB’s errors, suggesting directions for
future work.1
</bodyText>
<sectionHeader confidence="0.995924" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.9990649">
Typically, Information Extraction (IE) systems learn
an extractor for each target relation from la-
beled training examples (Kim and Moldovan, 1993;
Riloff, 1996; Soderland, 1999). This approach to IE
does not scale to corpora where the number of target
relations is very large, or where the target relations
cannot be specified in advance. Open IE solves this
problem by identifying relation phrases—phrases
that denote relations in English sentences (Banko
et al., 2007). The automatic identification of rela-
</bodyText>
<footnote confidence="0.9636305">
1The source code for REVERB is available at http://
reverb.cs.washington.edu/
</footnote>
<bodyText confidence="0.999719147058823">
tion phrases enables the extraction of arbitrary re-
lations from sentences, obviating the restriction to a
pre-specified vocabulary.
Open IE systems have achieved a notable measure
of success on massive, open-domain corpora drawn
from the Web, Wikipedia, and elsewhere. (Banko et
al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The
output of Open IE systems has been used to support
tasks like learning selectional preferences (Ritter et
al., 2010), acquiring common sense knowledge (Lin
et al., 2010), and recognizing entailment (Schoen-
mackers et al., 2010; Berant et al., 2011). In ad-
dition, Open IE extractions have been mapped onto
existing ontologies (Soderland et al., 2010).
We have observed that two types of errors are fre-
quent in the output of Open IE systems such as TEX-
TRUNNER and WOE: incoherent extractions and un-
informative extractions.
Incoherent extractions are cases where the ex-
tracted relation phrase has no meaningful interpre-
tation (see Table 1 for examples). Incoherent ex-
tractions arise because the learned extractor makes a
sequence of decisions about whether to include each
word in the relation phrase, often resulting in incom-
prehensible predictions. To solve this problem, we
introduce a syntactic constraint: every multi-word
relation phrase must begin with a verb, end with a
preposition, and be a contiguous sequence of words
in the sentence. Thus, the identification of a relation
phrase is made in one fell swoop instead of on the
basis of multiple, word-by-word decisions.
Uninformative extractions are extractions that
omit critical information. For example, consider the
sentence “Faust made a deal with the devil.” Previ-
</bodyText>
<page confidence="0.931636">
1535
</page>
<note confidence="0.97264">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.949400783783784">
ous Open IE systems return the uninformative
(Faust, made, a deal)
instead of
(Faust, made a deal with, the devil).
This type of error is caused by improper handling
of relation phrases that are expressed by a combi-
nation of a verb with a noun, such as light verb
constructions (LVCs). An LVC is a multi-word ex-
pression composed of a verb and a noun, with the
noun carrying the semantic content of the predi-
cate (Grefenstette and Teufel, 1995; Stevenson et al.,
2004; Allerton, 2002). Table 2 illustrates the wide
range of relations expressed this way, which are not
captured by existing open extractors. Our syntactic
constraint leads the extractor to include nouns in the
relation phrase, solving this problem.
Although the syntactic constraint significantly re-
duces incoherent and uninformative extractions, it
allows overly-specific relation phrases such as is of-
fering only modest greenhouse gas reduction targets
at. To avoid overly-specific relation phrases, we in-
troduce an intuitive lexical constraint: a binary rela-
tion phrase ought to appear with at least a minimal
number of distinct argument pairs in a large corpus.
In summary, this paper articulates two simple but
surprisingly powerful constraints on how binary re-
lationships are expressed via verbs in English sen-
tences, and implements them in the REVERB Open
IE system. We release REVERB and the data used in
our experiments to the research community.
The rest of the paper is organized as follows. Sec-
tion 2 analyzes previous work. Section 3 defines our
constraints precisely. Section 4 describes REVERB,
our implementation of the constraints. Section 5 re-
ports on our experimental results. Section 6 con-
cludes with a summary and discussion of future
work.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="related work">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.9994">
Open IE systems like TEXTRUNNER (Banko et al.,
2007), WOEPOs, and WOEParse (Wu and Weld, 2010)
focus on extracting binary relations of the form
(arg1, relation phrase, arg2) from text. These sys-
tems all use the following three-step method:
</bodyText>
<footnote confidence="0.35935">
1. Label: Sentences are automatically labeled
with extractions using heuristics or distant su-
pervision.
</footnote>
<note confidence="0.565986375">
Sentence Incoherent Relation
The guide contains dead links contains omits
and omits sites.
The Mark 14 was central to the was central torpedo
torpedo scandal of the fleet.
They recalled that Nungesser recalled began
began his career as a precinct
leader.
</note>
<tableCaption confidence="0.860281142857143">
Table 1: Examples of incoherent extractions. In-
coherent extractions make up approximately 13% of
TEXTRUNNER’s output, 15% of WOEpos’s output, and
30% of WOEparse’s output.
is an album by, is the author of, is a city in
has a population of, has a Ph.D. in, has a cameo in
made a deal with, made a promise to
took place in, took control over, took advantage of
gave birth to, gave a talk at, gave new meaning to
got tickets to, got a deal on, got funding from
Table 2: Examples of uninformative relations (left) and
their completions (right). Uninformative relations oc-
cur in approximately 4% of WOEparse’s output, 6% of
WOEpos’s output, and 7% of TEXTRUNNER’s output.
</tableCaption>
<listItem confidence="0.99989675">
2. Learn: A relation phrase extractor is learned
using a sequence-labeling graphical model
(e.g., CRF).
3. Extract: the system takes a sentence as in-
</listItem>
<bodyText confidence="0.9689019">
put, identifies a candidate pair of NP arguments
(arg1, arg2) from the sentence, and then uses
the learned extractor to label each word be-
tween the two arguments as part of the relation
phrase or not.
The extractor is applied to the successive sentences
in the corpus, and the resulting extractions are col-
lected.
This method faces several challenges. First,
the training phase requires a large number of la-
beled training examples (e.g., 200, 000 heuristically-
labeled sentences for TEXTRUNNER and 300, 000
for WOE). Heuristic labeling of examples obviates
hand labeling but results in noisy labels and distorts
the distribution of examples. Second, the extrac-
tion step is posed as a sequence-labeling problem,
where each word is assigned its own label. Because
each assignment is uncertain, the likelihood that the
extracted relation phrase is flawed increases with
the length of the sequence. Finally, the extractor
</bodyText>
<figure confidence="0.6101755">
is
has
made
took
gave
got
</figure>
<page confidence="0.958732">
1536
</page>
<bodyText confidence="0.999969409638555">
chooses an extraction’s arguments heuristically, and
cannot backtrack over this choice. This is problem-
atic when a word that belongs in the relation phrase
is chosen as an argument (for example, deal from
the “made a deal with” sentence).
Because of the feature sets utilized in previous
work, the learned extractors ignore both “holistic”
aspects of the relation phrase (e.g., is it contiguous?)
as well as lexical aspects (e.g., how many instances
of this relation are there?). Thus, as we show in Sec-
tion 5, systems such as TEXTRUNNER are unable
to learn the constraints embedded in REVERB. Of
course, a learning system, utilizing a different hy-
pothesis space, and an appropriate set of training ex-
amples, could potentially learn and refine the con-
straints in REVERB. This is a topic for future work,
which we consider in Section 6.
The first Open IE system was TEXTRUNNER
(Banko et al., 2007), which used a Naive Bayes
model with unlexicalized POS and NP-chunk fea-
tures, trained using examples heuristically generated
from the Penn Treebank. Subsequent work showed
that utilizing a linear-chain CRF (Banko and Et-
zioni, 2008) or Markov Logic Network (Zhu et al.,
2009) can lead to improved extraction. The WOE
systems introduced by Wu and Weld make use of
Wikipedia as a source of training data for their ex-
tractors, which leads to further improvements over
TEXTRUNNER (Wu and Weld, 2010). Wu and Weld
also show that dependency parse features result in a
dramatic increase in precision and recall over shal-
low linguistic features, but at the cost of extraction
speed.
Other approaches to large-scale IE have included
Preemptive IE (Shinyama and Sekine, 2006), On-
Demand IE (Sekine, 2006), and weak supervision
for IE (Mintz et al., 2009; Hoffmann et al., 2010).
Preemptive IE and On-Demand IE avoid relation-
specific extractors, but rely on document and en-
tity clustering, which is too costly for Web-scale IE.
Weakly supervised methods use an existing ontol-
ogy to generate training data for learning relation-
specific extractors. While this allows for learn-
ing relation-specific extractors at a larger scale than
what was previously possible, the extractions are
still restricted to a specific ontology.
Many systems have used syntactic patterns based
on verbs to extract relation phrases, usually rely-
ing on a full dependency parse of the input sentence
(Lin and Pantel, 2001; Stevenson, 2004; Specia and
Motta, 2006; Kathrin Eichler and Neumann, 2008).
Our work differs from these approaches by focus-
ing on relation phrase patterns expressed in terms
of POS tags and NP chunks, instead of full parse
trees. Banko and Etzioni (Banko and Etzioni, 2008)
showed that a small set of POS-tag patterns cover a
large fraction of relationships in English, but never
incorporated the patterns into an extractor. This pa-
per reports on a substantially improved model of bi-
nary relation phrases, which increases the recall of
the Banko-Etzioni model (see Section 3.3). Further,
while previous work in Open IE has mainly focused
on syntactic patterns for relation extraction, we in-
troduce a lexical constraint that boosts precision and
recall.
Finally, Open IE is closely related to semantic role
labeling (SRL) (Punyakanok et al., 2008; Toutanova
et al., 2008) in that both tasks extract relations and
arguments from sentences. However, SRL systems
traditionally rely on syntactic parsers, which makes
them susceptible to parser errors and substantially
slower than Open IE systems such as REVERB. This
difference is particularly important when operating
on the Web corpus due to its size and heterogeneity.
Finally, SRL requires hand-constructed semantic re-
sources like Propbank and Framenet (Martha and
Palmer, 2002; Baker et al., 1998) as input. In con-
trast, Open IE systems require no relation-specific
training data. ReVerb, in particular, relies on its ex-
plicit lexical and syntactic constraints, which have
no correlate in SRL systems. For a more detailed
comparison of SRL and Open IE, see (Christensen
et al., 2010).
</bodyText>
<sectionHeader confidence="0.957595" genericHeader="method">
3 Constraints on Relation Phrases
</sectionHeader>
<bodyText confidence="0.998430333333333">
In this section we introduce two constraints on re-
lation phrases: a syntactic constraint and a lexical
constraint.
</bodyText>
<subsectionHeader confidence="0.999032">
3.1 Syntactic Constraint
</subsectionHeader>
<bodyText confidence="0.9999816">
The syntactic constraint serves two purposes. First,
it eliminates incoherent extractions, and second, it
reduces uninformative extractions by capturing rela-
tion phrases expressed by a verb-noun combination,
including light verb constructions.
</bodyText>
<page confidence="0.649373">
1537
</page>
<equation confidence="0.98975125">
V  |V P  |V W∗P 3.2 Lexical Constraint
V = verb particle? adv? While the syntactic constraint greatly reduces unin-
W = (noun  |adj  |adv  |pron  |det) formative extractions, it can sometimes match rela-
P = (prep  |particle  |inf. marker) tion phrases that are so specific that they have only a
</equation>
<bodyText confidence="0.991900554216868">
few possible instances, even in a Web-scale corpus.
Consider the sentence:
The Obama administration is offering only modest
greenhouse gas reduction targets at the conference.
The POS pattern will match the phrase:
is offering only modest greenhouse gas reduction targets at
(1)
Thus, there are phrases that satisfy the syntactic con-
straint, but are not relational.
To overcome this limitation, we introduce a lexi-
cal constraint that is used to separate valid relation
phrases from overspecified relation phrases, like the
example in (1). The constraint is based on the in-
tuition that a valid relation phrase should take many
distinct arguments in a large corpus. The phrase in
(1) is specific to the argument pair (Obama admin-
istration, conference), so it is unlikely to represent a
bona fide relation. We describe the implementation
details of the lexical constraint in Section 4.
3.3 Limitations
Our constraints represent an idealized model of re-
lation phrases in English. This raises the question:
How much recall is lost due to the constraints?
To address this question, we analyzed Wu and
Weld’s set of 300 sentences from a set of random
Web pages, manually identifying all verb-based re-
lationships between noun phrase pairs. This resulted
in a set of 327 relation phrases. For each rela-
tion phrase, we checked whether it satisfies our con-
straints. We found that 85% of the relation phrases
do satisfy the constraints. Of the remaining 15%,
we identified some of the common cases where the
constraints were violated, summarized in Table 3.
Many of the example relation phrases shown in
Table 3 involve long-range dependencies between
words in the sentence. These types of dependen-
cies are not easily representable using a pattern over
POS tags. A deeper syntactic analysis of the input
sentence would provide a much more general lan-
guage for modeling relation phrases. For example,
one could create a model of relations expressed in
Figure 1: A simple part-of-speech-based regular expres-
sion reduces the number of incoherent extractions like
was central torpedo and covers relations expressed via
light verb constructions like gave a talk at.
The syntactic constraint requires the relation
phrase to match the POS tag pattern shown in Fig-
ure 1. The pattern limits relation phrases to be either
a verb (e.g., invented), a verb followed immediately
by a preposition (e.g., located in), or a verb followed
by nouns, adjectives, or adverbs ending in a preposi-
tion (e.g., has atomic weight of). If there are multiple
possible matches in a sentence for a single verb, the
longest possible match is chosen. Finally, if the pat-
tern matches multiple adjacent sequences, we merge
them into a single relation phrase (e.g., wants to ex-
tend). This refinement enables the model to readily
handle relation phrases containing multiple verbs. A
consequence of this pattern is that the relation phrase
must be a contiguous span of words in the sentence.
The syntactic constraint eliminates the incoherent
relation phrases returned by existing systems. For
example, given the sentence
Extendicare agreed to buy Arbor Health Care for
about US $432 million in cash and assumed debt.
TEXTRUNNER returns the extraction
(Arbor Health Care, for assumed, debt).
The phrase for assumed is clearly not a valid rela-
tion phrase: it begins with a preposition and splices
together two distant words in the sentence. The syn-
tactic constraint prevents this type of error by sim-
ply restricting relation phrases to match the pattern
in Figure 1.
The syntactic constraint reduces uninformative
extractions by capturing relation phrases expressed
via LVCs. For example, the POS pattern matched
against the sentence “Faust made a deal with the
Devil,” would result in the relation phrase made a
deal with, instead of the uninformative made.
Finally, we require the relation phrase to appear
between its two arguments in the sentence. This is a
common constraint that has been implicitly enforced
in other open extractors.
</bodyText>
<table confidence="0.966989076923077">
1538
Binary Verbal Relation Phrases
85% Satisfy Constraints
8% Non-Contiguous Phrase Structure
Coordination: X is produced and maintained by Y
Multiple Args: X was founded in 1995 by Y
Phrasal Verbs: X turned Y off
4% Relation Phrase Not Between Arguments
Intro. Phrases: Discovered by Y, X ...
Relative Clauses: ... the Y that X discovered
3% Do Not Match POS Pattern
Interrupting Modifiers: X has a lot of faith in Y
Infinitives: X to attack Y
</table>
<tableCaption confidence="0.960213666666667">
Table 3: Approximately 85% of the binary verbal relation
phrases in a sample of Web sentences satisfy our con-
straints.
</tableCaption>
<bodyText confidence="0.999614606060606">
terms of dependency parse features that would cap-
ture the non-contiguous relation phrases in Table 3.
Previous work has shown that dependency paths do
indeed boost the recall of relation extraction systems
(Wu and Weld, 2010; Mintz et al., 2009). While us-
ing dependency path features allows for a more flex-
ible model of relations, it significantly increases pro-
cessing time, which is problematic for Web-scale ex-
traction. Further, we have found that this increased
recall comes at the cost of lower precision on Web
text (see Section 5).
The results in Table 3 are similar to Banko and Et-
zioni’s findings that a set of eight POS patterns cover
a large fraction of binary verbal relation phrases.
However, their analysis was based on a set of sen-
tences known to contain either a company acquisi-
tion or birthplace relationship, while our results are
on a random sample of Web sentences. We applied
Banko and Etzioni’s verbal patterns to our random
sample of 300 Web sentences, and found that they
cover approximately 69% of the relation phrases in
the corpus. The gap in recall between this and the
85% shown in Table 3 is largely due to LVC relation
phrases (made a deal with) and phrases containing
multiple verbs (refuses to return to), which their pat-
terns do not cover.
In sum, our model is by no means complete.
However, we have empirically shown that the ma-
jority of binary verbal relation phrases in a sample
of Web sentences are captured by our model. By
focusing on this subset of language, our model can
be used to perform Open IE at significantly higher
precision than before.
</bodyText>
<sectionHeader confidence="0.999612" genericHeader="method">
4 REVERB
</sectionHeader>
<bodyText confidence="0.999862210526316">
This section introduces REVERB, a novel open ex-
tractor based on the constraints defined in the previ-
ous section. REVERB first identifies relation phrases
that satisfy the syntactic and lexical constraints, and
then finds a pair of NP arguments for each identified
relation phrase. The resulting extractions are then
assigned a confidence score using a logistic regres-
sion classifier.
This algorithm differs in three important ways
from previous methods (Section 2). First, the re-
lation phrase is identified “holistically” rather than
word-by-word. Second, potential phrases are fil-
tered based on statistics over a large corpus (the
implementation of our lexical constraint). Finally,
REVERB is “relation first” rather than “arguments
first”, which enables it to avoid a common error
made by previous methods—confusing a noun in the
relation phrase for an argument, e.g. the noun deal in
made a deal with.
</bodyText>
<subsectionHeader confidence="0.988792">
4.1 Extraction Algorithm
</subsectionHeader>
<bodyText confidence="0.99967775">
REVERB takes as input a POS-tagged and NP-
chunked sentence and returns a set of (x, r, y)
extraction triples.2 Given an input sentence s,
REVERB uses the following extraction algorithm:
</bodyText>
<listItem confidence="0.991132923076923">
1. Relation Extraction: For each verb v in s,
find the longest sequence of words r„ such that
(1) r„ starts at v, (2) r„ satisfies the syntactic
constraint, and (3) r„ satisfies the lexical con-
straint. If any pair of matches are adjacent or
overlap in s, merge them into a single match.
2. Argument Extraction: For each relation
phrase r identified in Step 1, find the nearest
noun phrase x to the left of r in s such that x is
not a relative pronoun, WHO-adverb, or exis-
tential “there”. Find the nearest noun phrase y
to the right of r in s. If such an (x, y) pair could
be found, return (x, r, y) as an extraction.
</listItem>
<bodyText confidence="0.995425666666667">
We check whether a candidate relation phrase
r„ satisfies the syntactic constraint by matching it
against the regular expression in Figure 1.
</bodyText>
<footnote confidence="0.9984125">
2REVERB uses OpenNLP for POS tagging and NP chunk-
ing:http://opennlp.sourceforge.net/
</footnote>
<page confidence="0.994284">
1539
</page>
<bodyText confidence="0.99930703030303">
To determine whether r„ satisfies the lexical con-
straint, we use a large dictionary D of relation
phrases that are known to take many distinct argu-
ments. In an offline step, we construct D by find-
ing all matches of the POS pattern in a corpus of
500 million Web sentences. For each matching re-
lation phrase, we heuristically identify its arguments
(as in Step 2 above). We set D to be the set of all
relation phrases that take at least k distinct argument
pairs in the set of extractions. In order to allow for
minor variations in relation phrases, we normalize
each relation phrase by removing inflection, auxil-
iary verbs, adjectives, and adverbs. Based on ex-
periments on a held-out set of sentences, we found
that a value of k = 20 works well for filtering out
overspecified relations. This results in a set of ap-
proximately 1.7 million distinct normalized relation
phrases, which are stored in memory at extraction
time.
As an example of the extraction algorithm in ac-
tion, consider the following input sentence:
Hudson was born in Hampstead, which is a
suburb of London.
Step 1 of the algorithm identifies three relation
phrases that satisfy the syntactic and lexical con-
straints: was, born in, and is a suburb of. The first
two phrases are adjacent in the sentence, so they are
merged into the single relation phrase was born in.
Step 2 then finds an argument pair for each relation
phrase. For was born in, the nearest NPs are (Hud-
son, Hampstead). For is a suburb of, the extractor
skips over the NP which and chooses the argument
pair (Hampstead, London). The final output is
</bodyText>
<listItem confidence="0.567458">
e1: (Hudson, was born in, Hampstead)
e2: (Hampstead, is a suburb of, London).
</listItem>
<subsectionHeader confidence="0.989974">
4.2 Confidence Function
</subsectionHeader>
<bodyText confidence="0.999774454545455">
The extraction algorithm in the previous section has
high recall, but low precision. Like with previous
open extractors, we want way to trade recall for pre-
cision by tuning a confidence threshold. We use a
logistic regression classifier to assign a confidence
score to each extraction, which uses the features
shown in Table 4. All of these features are efficiently
computable and relation independent. We trained
the confidence function by manually labeling the ex-
tractions from a set of 1, 000 sentences from the Web
and Wikipedia as correct or incorrect.
</bodyText>
<figure confidence="0.91514455">
Weight Feature
1.16 (x, r, y) covers all words in s
0.50 The last preposition in r is for
0.49 The last preposition in r is on
0.46 The last preposition in r is of
0.43 len(s) &lt; 10 words
0.43 There is a WH-word to the left of r
0.42 r matches VW*P from Figure 1
0.39 The last preposition in r is to
0.25 The last preposition in r is in
0.23 10 words &lt; len(s) &lt; 20 words
0.21 s begins with x
0.16 y is a proper noun
0.01 x is a proper noun
-0.30 There is an NP to the left of x in s
-0.43 20 words &lt; len(s)
-0.61 r matches V from Figure 1
-0.65 There is a preposition to the left of x in s
-0.81 There is an NP to the right of y in s
-0.93 Coord. conjunction to the left of r in s
</figure>
<tableCaption confidence="0.769056333333333">
Table 4: REVERB uses these features to assign a confi-
dence score to an extraction (x, r, y) from a sentence s
using a logistic regression classifier.
</tableCaption>
<bodyText confidence="0.9996624">
Previous open extractors require labeled training
data to learn a model of relations, which is then used
to extract relation phrases from text. In contrast,
REVERB uses a specified model of relations for ex-
traction, and requires labeled data only for assigning
confidence scores to its extractions. Learning a con-
fidence function is a much simpler task than learning
a full model of relations, using two orders of magni-
tude fewer training examples than TEXTRUNNER or
WOE.
</bodyText>
<subsectionHeader confidence="0.996399">
4.3 TEXTRUNNER-R
</subsectionHeader>
<bodyText confidence="0.999922846153846">
The model of relation phrases used by REVERB
is specified, but could a TEXTRUNNER-like sys-
tem learn this model from training data? While
it is difficult to answer such a question for all
possible permutations of features sets, training ex-
amples, and learning biases, we demonstrate that
TEXTRUNNER itself cannot learn REVERB’s model
even when re-trained using the output of REVERB
as labeled training data. The resulting system,
TEXTRUNNER-R, uses the same feature representa-
tion as TEXTRUNNER, but different parameters, and
a different set of training examples.
To generate positive instances, we ran REVERB
</bodyText>
<page confidence="0.982946">
1540
</page>
<bodyText confidence="0.999957083333333">
on the Penn Treebank, which is the same dataset
that TEXTRUNNER is trained on. To generate neg-
ative instances from a sentence, we took each noun
phrase pair in the sentence that does not appear as
arguments in a REVERB extraction. This process
resulted in a set of 67, 562 positive instances, and
356,834 negative instances. We then passed these
labeled examples to TEXTRUNNER’s training proce-
dure, which learns a linear-chain CRF using closed-
class features like POS tags, capitalization, punctu-
ation, etc.TEXTRUNNER-R uses the argument-first
extraction algorithm described in Section 2.
</bodyText>
<sectionHeader confidence="0.999774" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.992605">
We compare REVERB to the following systems:
</bodyText>
<listItem confidence="0.970637782608695">
• REVERB—Lex - The REVERB system described
in the previous section, but without the lexical
constraint. REVERB—Lex uses the same confi-
dence function as REVERB.
• TEXTRUNNER - Banko and Etzioni’s 2008 ex-
tractor, which uses a second order linear-chain
CRF trained on extractions heuristically gener-
ated from the Penn Treebank. TEXTRUNNER
uses shallow linguistic features in its CRF,
which come from the same POS tagger and NP-
chunker that REVERB uses.
• TEXTRUNNER-R - Our modification to
TEXTRUNNER, which uses the same extrac-
tion code, but with a model of relations trained
on REVERB extractions.
• WOEpos - Wu and Weld’s modification to
TEXTRUNNER, which uses a model of re-
lations learned from extractions heuristically
generated from Wikipedia.
• WOEparse - Wu and Weld’s parser-based ex-
tractor, which uses a large dictionary of depen-
dency path patterns learned from heuristic ex-
tractions generated from Wikipedia.
</listItem>
<bodyText confidence="0.52297575">
Each system is given a set of sentences as input,
and returns a set of binary extractions as output. We
created a test set of 500 sentences sampled from the
Web, using Yahoo’s random link service.3 After run-
</bodyText>
<footnote confidence="0.516556">
3http://random.yahoo.com/bin/ryl
</footnote>
<figure confidence="0.890054">
5
REVERS REVERS WOE TEXT- WOE TEXT-
-lex parse RUNNER-R pas RUNNER
</figure>
<figureCaption confidence="0.89550225">
Figure 2: REVERB outperforms state-of-the-art open
extractors, with an AUC more than twice that of
TEXTRUNNER or WOEpos, and 38% higher than
WOEparse.
</figureCaption>
<figure confidence="0.976827">
Comparison of REVERB-Based Systems
1 5
Recall
</figure>
<figureCaption confidence="0.98989675">
Figure 3: The lexical constraint gives REVERB
a boost in precision and recall over REVERB¬lex.
TEXTRUNNER-R is unable to learn the model used by
REVERB, which results in lower precision and recall.
</figureCaption>
<bodyText confidence="0.999107777777778">
ning each extractor over the input sentences, two hu-
man judges independently evaluated each extraction
as correct or incorrect. The judges reached agree-
ment on 86% of the extractions, with an agreement
score of n = 0.68. We report results on the subset
of the data where the two judges concur.
The judges labeled uninformative extractions con-
servatively. That is, if critical information was
dropped from the relation phrase but included in the
second argument, it is labeled correct. For example,
both the extractions (Ackerman, is a professor of, bi-
ology) and (Ackerman, is, a professor of biology) are
considered correct.
Each system returns confidence scores for its ex-
tractions. For a given threshold, we can measure
the precision and recall of the output. Precision
is the fraction of returned extractions that are cor-
rect. Recall is the fraction of correct extractions in
</bodyText>
<figure confidence="0.968656727272727">
REVERB
REVERB-le2
TEXTRUNNER-R
Area Under PR Curve
1
1
Precision
1541
Extractions
1 5
Recall
</figure>
<figureCaption confidence="0.981216333333333">
Figure 4: REVERB achieves significantly higher preci-
sion than state-of-the-art Open IE systems, and compara-
ble recall to WOEparse.
</figureCaption>
<figure confidence="0.979957666666667">
Relations Only
1 5
Recall
</figure>
<figureCaption confidence="0.996727666666667">
Figure 5: On the subtask of identifying relations phrases,
REVERB is able to achieve even higher precision and re-
call than other systems.
</figureCaption>
<figure confidence="0.990927583333333">
REVERB
WOEparse
WOEp&amp;quot;
TEXTRUNNER
REVERB
WOEparse
WOEp&amp;quot;
TEXTRUNNER
1
Precision
1
Precision
</figure>
<bodyText confidence="0.996127222222222">
the corpus that are returned. We use the total num-
ber of extractions labeled as correct by the judges
as our measure of recall for the corpus. In order to
avoid double-counting, we treat extractions that dif-
fer superficially (e.g., different punctuation or drop-
ping inessential modifiers) as a single extraction. We
compute a precision-recall curve by varying the con-
fidence threshold, and then compute the area under
the curve (AUC).
</bodyText>
<subsectionHeader confidence="0.769219">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.999688235294118">
Figure 2 shows the AUC of each system. REVERB
achieves an AUC that is 30% higher than WOEparse
and is more than double the AUC of WOEpOs or
TEXTRUNNER. The lexical constraint provides a
significant boost in performance, with REVERB
achieving an AUC 23% higher than REVERB—Lex.
REVERB proves to be a useful source of train-
ing data, with TEXTRUNNER-R having an AUC
71% higher than TEXTRUNNER and performing
on par with WOEpOs. From the training data,
TEXTRUNNER-R was able to learn a model that
predicts contiguous relation phrases, but still re-
turned incoherent relation phrases (e.g., starting with
a preposition) and overspecified relation phrases.
These errors are due to TEXTRUNNER-R overfitting
the training data and not having access to the lexical
constraint.
Figure 3 shows the precision-recall curves of the
systems introduced in this paper. TEXTRUNNER-R
has much lower precision than REVERB and
REVERB—Lex at all levels of recall. The lexi-
cal constraint gives REVERB a boost in precision
over REVERB—Lex, reducing overspecified extrac-
tions from 20% of REVERB—Lex’s output to 1% of
REVERB’s. The lexical constraint also boosts recall
over REVERB—Lex, since REVERB is able to find a
correct relation phrase where REVERB—Lex finds an
overspecified one.
Figure 4 shows the precision-recall curves of
REVERB and the external systems. REVERB has
much higher precision than the other systems at
nearly all levels of recall. In particular, more than
30% of REVERB’s extractions are at precision 0.8
or higher, compared to virtually none for the other
systems. WOEparse achieves a slightly higher recall
than REVERB (0.62 versus 0.64), but at the cost of
lower precision.
In order to highlight the role of the relational
model of each system, we also evaluate their per-
formance on the subtask of extracting just the rela-
tion phrases from the input text. Figure 5 shows the
precision-recall curves for each system on the rela-
tion phrase-only evaluation. In this case, REVERB
has both higher precision and recall than the other
systems.
REVERB’s biggest improvement came from the
elimination of incoherent extractions. Incoher-
ent extractions were a large fraction of the errors
made by previous systems, accounting for approxi-
mately 13% of TEXTRUNNER’s extractions, 15% of
WOEpOs’s, and 30% of WOEparse’s. Uninformative
</bodyText>
<page confidence="0.978691">
1542
</page>
<table confidence="0.993057083333333">
REVERB - Missed Extractions
52% Could not identify correct arguments
23% Relation filtered out by lexical constraint
17% Identified a more specific relation
8% POS/chunking error
REVERB - Incorrect Extractions
65% Correct relation phrase, incorrect arguments
16% N-ary relation
8% Non-contiguous relation phrase
2% Imperative verb
2% Overspecified relation phrase
7% Other, including POS/chunking errors
</table>
<tableCaption confidence="0.863616">
Table 5: The majority of the incorrect extractions re-
turned by REVERB are due to errors in argument extrac-
</tableCaption>
<bodyText confidence="0.998309329411765">
tion.
extractions had a smaller effect on other systems’
precision, accounting for 4% of WOEp&amp;quot;3�’s extrac-
tions, 5% of WOEpo3’s, and 7% of TEXTRUNNER’s,
while only appearing in 1% of REVERB’s extrac-
tions. REVERB’s reduction in uninformative extrac-
tions resulted in a boost in recall, capturing many
LVC relation phrases missed by other systems (like
those shown in Table 2).
To test the systems’ speed, we ran each extrac-
tor on a set of 100, 000 sentences using a Pen-
tium 4 machine with 4GB of RAM. The process-
ing times were 16 minutes for REVERB, 21 min-
utes for TEXTRUNNER, 21 minutes for WOEpo3, and
11 hours for WOEp�&amp;quot;&apos;. The times for REVERB,
TEXTRUNNER, and WOEpo3 are all approximately
the same, since they all use the same POS-tagging
and NP-chunking software. WOEp�.3. processes
each sentence with a dependency parser, resulting
in much longer processing time.
5.2 REVERB Error Analysis
To better understand the limitations of REVERB, we
performed a detailed analysis of its errors in pre-
cision (incorrect extractions returned by REVERB)
and its errors in recall (correct extractions that
REVERB missed).
Table 5 summarizes the types of incorrect extrac-
tions that REVERB returns. We found that 65% of
the incorrect extractions returned by REVERB were
cases where a relation phrase was correctly identi-
fied, but the argument-finding heuristics failed. The
remaining errors were cases where REVERB ex-
tracted an incorrect relation phrase. One common
mistake that REVERB made was extracting a rela-
tion phrase that expresses an n-ary relationship via
a ditransitive verb. For example, given the sentence
Table 6: The majority of extractions that were missed by
REVERB were cases where the correct relation phrase
was found, but the arguments were not correctly identi-
fied.
“I gave him 15 photographs,” REVERB extracts (I,
gave, him). These errors are due to the fact that
REVERB only models binary relations.
Table 6 summarizes the correct extractions that
were extracted by other systems and were not ex-
tracted by REVERB. As with the false positive ex-
tractions, the majority of false negatives (52%) were
due to the argument-finding heuristics choosing the
wrong arguments, or failing to extract all possible ar-
guments (in the case of coordinating conjunctions).
Other sources of failure were due to the lexical con-
straint either failing to filter out an overspecified re-
lation phrase or filtering out a valid relation phrase.
These errors hurt both precision and recall, since
each case results in the extractor overlooking a cor-
rect relation phrase and choosing another.
5.3 Evaluation At Scale
Section 5.1 shows that REVERB outperforms ex-
isting Open IE systems when evaluated on a sam-
ple of sentences. Previous work has shown that
the frequency of an extraction in a large corpus is
useful for assessing the correctness of extractions
(Downey et al., 2005). Thus, it is possible a pri-
ori that REVERB’s gains over previous systems will
diminish when extraction frequency is taken into ac-
count.
In fact, we found that REVERB’s advantage over
TEXTRUNNER when run at scale is qualitatively
similar to its advantage on single sentences. We ran
both REVERB and TEXTRUNNER on Banko and Et-
zioni’s corpus of 500 million Web sentences and ex-
amined the effect of redundancy on precision.
As Downey’s work predicts, precision increased
in both systems for extractions found multiple
times, compared with extractions found only once.
However, REVERB had higher precision than
1543
TEXTRUNNER at all frequency thresholds. In fact,
REVERB’s frequency 1 extractions had a precision
of 0.75, which TEXTRUNNER could not approach
even with frequency 10 extractions, which had a
precision of 0.34. Thus, REVERB is able to return
more correct extractions at a higher precision than
TEXTRUNNER, even when redundancy is taken into
account.
</bodyText>
<sectionHeader confidence="0.998754" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.993445862068965">
The paper’s contributions are as follows:
• We have identified and analyzed the problems
of incoherent and uninformative extractions for
Open IE systems, and shown their prevalence
for systems such as TEXTRUNNER and WOE.
• We articulated general, easy-to-enforce con-
straints on binary, verb-based relation phrases
in English that ameliorate these problems and
yield richer and more informative relations
(see, for example, Table 2).
• Based on these constraints, we designed, im-
plemented, and evaluated the REVERB extrac-
tor, which substantially outperforms previous
Open IE systems in both recall and precision.
• We make REVERB and the data used in our
experiments available to the research commu-
nity.4
In future work, we plan to explore utilizing our
constraints to improve the performance of learned
CRF models. Roth et al. have shown how to incor-
porate constraints into CRF learners (Roth and Yih,
2005). It is natural, then, to consider whether the
combination of heuristically labeled training exam-
ples, CRF learning, and our constraints will result
in superior performance. The error analysis in Sec-
tion 5.2 also suggests natural directions for future
work. For instance, since many of REVERB’s errors
are due to incorrect arguments, improved methods
for argument extraction are in order.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9969485">
We would like to thank Mausam, Dan Weld, Yoav
Artzi, Luke Zettlemoyer, members of the KnowItAll
</bodyText>
<footnote confidence="0.784332">
4http://reverb.cs.washington.edu
</footnote>
<bodyText confidence="0.9997945">
group, and the anonymous reviewers for their help-
ful comments. This research was supported in part
by NSF grant IIS-0803481, ONR grant N00014-08-
1-0431, and DARPA contract FA8750-09-C-0179,
and carried out at the University of Washington’s
Turing Center.
</bodyText>
<sectionHeader confidence="0.998969" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882976744186">
David J. Allerton. 2002. Stretched Verb Constructions in
English. Routledge Studies in Germanic Linguistics.
Routledge (Taylor and Francis), New York.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86–90.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28–36, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In In the Proceedings
of the 20th International Joint Conference on Artificial
Intelligence, pages 2670–2676, January.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2010. Semantic role labeling for
open information extraction. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, FAM-LbR ’10, pages 52–60, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI, pages 1034–1041.
Gregory Grefenstette and Simone Teufel. 1995. Corpus-
based method for automatic identification of support
verbs for nominalizations. In Proceedings of the sev-
enth conference on European chapter of the Associa-
tion for Computational Linguistics, pages 98–103, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ’10, pages 286–
295, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.898667">
1544
</page>
<reference confidence="0.998195979381443">
Holmer Hemsen Kathrin Eichler and Gnter Neu-
mann. 2008. Unsupervised relation extraction
from web documents. In LREC. http://www.lrec-
conf.org/proceedings/lrec2008/.
J. Kim and D. Moldovan. 1993. Acquisition of semantic
patterns for information extraction from corpora. In
Procs. of Ninth IEEE Conference on Artificial Intelli-
gence for Applications, pages 171–176.
Dekang Lin and Patrick Pantel. 2001. DIRT-Discovery
of Inference Rules from Text. In Proceedings of
ACM Conference on Knowledge Discovery and Data
Mining(KDD-01), pages pp. 323–328.
Thomas Lin, Mausam, and Oren Etzioni. 2010. Identify-
ing Functional Relations in Web Text. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1266–1276, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In In Proceedings of LREC-
2002.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP ’09: Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
2, pages 1003–1011, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
E. Riloff. 1996. Automatically constructing extraction
patterns from untagged text. In Procs. of the Thir-
teenth National Conference on Artificial Intelligence
(AAAI-96), pages 1044–1049.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Pref-
erences. In ACL.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd international conference on
Machine learning, ICML ’05, pages 736–743, New
York, NY, USA. ACM.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’10, pages 1088–1098,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 731–738, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive Information Extraction using Unrestricted Rela-
tion Discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 304–311, New York City, USA,
June. Association for Computational Linguistics.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open in-
formation extraction to domain-specific relations. AI
Magazine, 31(3):93–102.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-Structured and Free Text. Machine
Learning, 34(1-3):233–272.
Lucia Specia and Enrico Motta. 2006. M.: A hybrid
approach for extracting semantic relations from texts.
In In. Proceedings of the 2 nd Workshop on Ontology
Learning and Population, pages 57–64.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical measures of the semi-productivity
of light verb constructions. In 2nd ACL Workshop on
Multiword Expressions, pages 1–8.
M. Stevenson. 2004. An unsupervised WordNet-based
algorithm for relation extraction. In Proceedings of
the “Beyond Named Entity” workshop at the Fourth
International Conference on Language Resources and
Evalutaion (LREC’04).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161–
191.
Fei Wu and Daniel S. Weld. 2010. Open information ex-
traction using Wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 118–127, Morristown,
NJ, USA. Association for Computational Linguistics.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. StatSnowball: a statistical ap-
proach to extracting entity relationships. In WWW ’09:
Proceedings of the 18th international conference on
World wide web, pages 101–110, New York, NY, USA.
ACM.
</reference>
<page confidence="0.994017">
1545
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573743">
<title confidence="0.999894">Identifying Relations for Open Information Extraction</title>
<author confidence="0.99956">Anthony Fader</author>
<author confidence="0.99956">Stephen Soderland</author>
<author confidence="0.99956">Oren</author>
<affiliation confidence="0.999792">University of Washington,</affiliation>
<abstract confidence="0.997865">Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We imthe constraints in the Open IE system, which more than doubles the area under the precision-recall curve relative previous extractors such as More than are at precision higher— compared to virtually none for earlier systems. The paper concludes with a detailed analysis</abstract>
<intro confidence="0.598894">errors, suggesting directions for</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David J Allerton</author>
</authors>
<title>Stretched Verb Constructions in English. Routledge Studies in Germanic Linguistics. Routledge (Taylor and Francis),</title>
<date>2002</date>
<location>New York.</location>
<contexts>
<context position="3950" citStr="Allerton, 2002" startWordPosition="604" endWordPosition="605">l Language Processing, pages 1535–1545, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics ous Open IE systems return the uninformative (Faust, made, a deal) instead of (Faust, made a deal with, the devil). This type of error is caused by improper handling of relation phrases that are expressed by a combination of a verb with a noun, such as light verb constructions (LVCs). An LVC is a multi-word expression composed of a verb and a noun, with the noun carrying the semantic content of the predicate (Grefenstette and Teufel, 1995; Stevenson et al., 2004; Allerton, 2002). Table 2 illustrates the wide range of relations expressed this way, which are not captured by existing open extractors. Our syntactic constraint leads the extractor to include nouns in the relation phrase, solving this problem. Although the syntactic constraint significantly reduces incoherent and uninformative extractions, it allows overly-specific relation phrases such as is offering only modest greenhouse gas reduction targets at. To avoid overly-specific relation phrases, we introduce an intuitive lexical constraint: a binary relation phrase ought to appear with at least a minimal number</context>
</contexts>
<marker>Allerton, 2002</marker>
<rawString>David J. Allerton. 2002. Stretched Verb Constructions in English. Routledge Studies in Germanic Linguistics. Routledge (Taylor and Francis), New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="11300" citStr="Baker et al., 1998" startWordPosition="1787" endWordPosition="1790">s precision and recall. Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in that both tasks extract relations and arguments from sentences. However, SRL systems traditionally rely on syntactic parsers, which makes them susceptible to parser errors and substantially slower than Open IE systems such as REVERB. This difference is particularly important when operating on the Web corpus due to its size and heterogeneity. Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input. In contrast, Open IE systems require no relation-specific training data. ReVerb, in particular, relies on its explicit lexical and syntactic constraints, which have no correlate in SRL systems. For a more detailed comparison of SRL and Open IE, see (Christensen et al., 2010). 3 Constraints on Relation Phrases In this section we introduce two constraints on relation phrases: a syntactic constraint and a lexical constraint. 3.1 Syntactic Constraint The syntactic constraint serves two purposes. First, it eliminates incoherent extractions, and second, it reduces uninformative extraction</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics, pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="8700" citStr="Banko and Etzioni, 2008" startWordPosition="1372" endWordPosition="1376">ion 5, systems such as TEXTRUNNER are unable to learn the constraints embedded in REVERB. Of course, a learning system, utilizing a different hypothesis space, and an appropriate set of training examples, could potentially learn and refine the constraints in REVERB. This is a topic for future work, which we consider in Section 6. The first Open IE system was TEXTRUNNER (Banko et al., 2007), which used a Naive Bayes model with unlexicalized POS and NP-chunk features, trained using examples heuristically generated from the Penn Treebank. Subsequent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over TEXTRUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et</context>
<context position="10225" citStr="Banko and Etzioni, 2008" startWordPosition="1621" endWordPosition="1624">. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually relying on a full dependency parse of the input sentence (Lin and Pantel, 2001; Stevenson, 2004; Specia and Motta, 2006; Kathrin Eichler and Neumann, 2008). Our work differs from these approaches by focusing on relation phrase patterns expressed in terms of POS tags and NP chunks, instead of full parse trees. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while previous work in Open IE has mainly focused on syntactic patterns for relation extraction, we introduce a lexical constraint that boosts precision and recall. Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In Proceedings of ACL-08: HLT, pages 28–36, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web. In</title>
<date>2007</date>
<booktitle>In the Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="1491" citStr="Banko et al., 2007" startWordPosition="214" endWordPosition="217">lier systems. The paper concludes with a detailed analysis of REVERB’s errors, suggesting directions for future work.1 1 Introduction and Motivation Typically, Information Extraction (IE) systems learn an extractor for each target relation from labeled training examples (Kim and Moldovan, 1993; Riloff, 1996; Soderland, 1999). This approach to IE does not scale to corpora where the number of target relations is very large, or where the target relations cannot be specified in advance. Open IE solves this problem by identifying relation phrases—phrases that denote relations in English sentences (Banko et al., 2007). The automatic identification of rela1The source code for REVERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (</context>
<context position="5261" citStr="Banko et al., 2007" startWordPosition="808" endWordPosition="811"> but surprisingly powerful constraints on how binary relationships are expressed via verbs in English sentences, and implements them in the REVERB Open IE system. We release REVERB and the data used in our experiments to the research community. The rest of the paper is organized as follows. Section 2 analyzes previous work. Section 3 defines our constraints precisely. Section 4 describes REVERB, our implementation of the constraints. Section 5 reports on our experimental results. Section 6 concludes with a summary and discussion of future work. 2 Previous Work Open IE systems like TEXTRUNNER (Banko et al., 2007), WOEPOs, and WOEParse (Wu and Weld, 2010) focus on extracting binary relations of the form (arg1, relation phrase, arg2) from text. These systems all use the following three-step method: 1. Label: Sentences are automatically labeled with extractions using heuristics or distant supervision. Sentence Incoherent Relation The guide contains dead links contains omits and omits sites. The Mark 14 was central to the was central torpedo torpedo scandal of the fleet. They recalled that Nungesser recalled began began his career as a precinct leader. Table 1: Examples of incoherent extractions. Incohere</context>
<context position="8468" citStr="Banko et al., 2007" startWordPosition="1338" endWordPosition="1341">n previous work, the learned extractors ignore both “holistic” aspects of the relation phrase (e.g., is it contiguous?) as well as lexical aspects (e.g., how many instances of this relation are there?). Thus, as we show in Section 5, systems such as TEXTRUNNER are unable to learn the constraints embedded in REVERB. Of course, a learning system, utilizing a different hypothesis space, and an appropriate set of training examples, could potentially learn and refine the constraints in REVERB. This is a topic for future work, which we consider in Section 6. The first Open IE system was TEXTRUNNER (Banko et al., 2007), which used a Naive Bayes model with unlexicalized POS and NP-chunk features, trained using examples heuristically generated from the Penn Treebank. Subsequent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over TEXTRUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and rec</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In In the Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 2670–2676, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="2186" citStr="Berant et al., 2011" startWordPosition="320" endWordPosition="323">ble at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as TEXTRUNNER and WOE: incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples). Incoherent extractions arise because the learned extractor makes a sequence of decisions about whether to include each word in the relation phrase, often resulting in incomprehensible predictions. </context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Proceedings of ACL, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Semantic role labeling for open information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, FAM-LbR ’10,</booktitle>
<pages>52--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11586" citStr="Christensen et al., 2010" startWordPosition="1834" endWordPosition="1837">es them susceptible to parser errors and substantially slower than Open IE systems such as REVERB. This difference is particularly important when operating on the Web corpus due to its size and heterogeneity. Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input. In contrast, Open IE systems require no relation-specific training data. ReVerb, in particular, relies on its explicit lexical and syntactic constraints, which have no correlate in SRL systems. For a more detailed comparison of SRL and Open IE, see (Christensen et al., 2010). 3 Constraints on Relation Phrases In this section we introduce two constraints on relation phrases: a syntactic constraint and a lexical constraint. 3.1 Syntactic Constraint The syntactic constraint serves two purposes. First, it eliminates incoherent extractions, and second, it reduces uninformative extractions by capturing relation phrases expressed by a verb-noun combination, including light verb constructions. 1537 V |V P |V W∗P 3.2 Lexical Constraint V = verb particle? adv? While the syntactic constraint greatly reduces uninW = (noun |adj |adv |pron |det) formative extractions, it can s</context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2010</marker>
<rawString>Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2010. Semantic role labeling for open information extraction. In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, FAM-LbR ’10, pages 52–60, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Oren Etzioni</author>
<author>Stephen Soderland</author>
</authors>
<title>A probabilistic model of redundancy in information extraction.</title>
<date>2005</date>
<booktitle>In IJCAI,</booktitle>
<pages>1034--1041</pages>
<contexts>
<context position="34345" citStr="Downey et al., 2005" startWordPosition="5595" endWordPosition="5598">dinating conjunctions). Other sources of failure were due to the lexical constraint either failing to filter out an overspecified relation phrase or filtering out a valid relation phrase. These errors hurt both precision and recall, since each case results in the extractor overlooking a correct relation phrase and choosing another. 5.3 Evaluation At Scale Section 5.1 shows that REVERB outperforms existing Open IE systems when evaluated on a sample of sentences. Previous work has shown that the frequency of an extraction in a large corpus is useful for assessing the correctness of extractions (Downey et al., 2005). Thus, it is possible a priori that REVERB’s gains over previous systems will diminish when extraction frequency is taken into account. In fact, we found that REVERB’s advantage over TEXTRUNNER when run at scale is qualitatively similar to its advantage on single sentences. We ran both REVERB and TEXTRUNNER on Banko and Etzioni’s corpus of 500 million Web sentences and examined the effect of redundancy on precision. As Downey’s work predicts, precision increased in both systems for extractions found multiple times, compared with extractions found only once. However, REVERB had higher precisio</context>
</contexts>
<marker>Downey, Etzioni, Soderland, 2005</marker>
<rawString>Doug Downey, Oren Etzioni, and Stephen Soderland. 2005. A probabilistic model of redundancy in information extraction. In IJCAI, pages 1034–1041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
<author>Simone Teufel</author>
</authors>
<title>Corpusbased method for automatic identification of support verbs for nominalizations.</title>
<date>1995</date>
<booktitle>In Proceedings of the seventh conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>98--103</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="3909" citStr="Grefenstette and Teufel, 1995" startWordPosition="596" endWordPosition="599">s of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics ous Open IE systems return the uninformative (Faust, made, a deal) instead of (Faust, made a deal with, the devil). This type of error is caused by improper handling of relation phrases that are expressed by a combination of a verb with a noun, such as light verb constructions (LVCs). An LVC is a multi-word expression composed of a verb and a noun, with the noun carrying the semantic content of the predicate (Grefenstette and Teufel, 1995; Stevenson et al., 2004; Allerton, 2002). Table 2 illustrates the wide range of relations expressed this way, which are not captured by existing open extractors. Our syntactic constraint leads the extractor to include nouns in the relation phrase, solving this problem. Although the syntactic constraint significantly reduces incoherent and uninformative extractions, it allows overly-specific relation phrases such as is offering only modest greenhouse gas reduction targets at. To avoid overly-specific relation phrases, we introduce an intuitive lexical constraint: a binary relation phrase ought</context>
</contexts>
<marker>Grefenstette, Teufel, 1995</marker>
<rawString>Gregory Grefenstette and Simone Teufel. 1995. Corpusbased method for automatic identification of support verbs for nominalizations. In Proceedings of the seventh conference on European chapter of the Association for Computational Linguistics, pages 98–103, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>286--295</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9334" citStr="Hoffmann et al., 2010" startWordPosition="1480" endWordPosition="1483">ogic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over TEXTRUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually relying on a full dependency parse of the input sentence (Lin a</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 286– 295, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holmer Hemsen Kathrin Eichler</author>
<author>Gnter Neumann</author>
</authors>
<title>Unsupervised relation extraction from web documents.</title>
<date>2008</date>
<booktitle>In LREC. http://www.lrecconf.org/proceedings/lrec2008/.</booktitle>
<contexts>
<context position="10026" citStr="Eichler and Neumann, 2008" startWordPosition="1587" endWordPosition="1590">, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually relying on a full dependency parse of the input sentence (Lin and Pantel, 2001; Stevenson, 2004; Specia and Motta, 2006; Kathrin Eichler and Neumann, 2008). Our work differs from these approaches by focusing on relation phrase patterns expressed in terms of POS tags and NP chunks, instead of full parse trees. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while previous work in Open IE has mainly focused on syntactic patterns for relation e</context>
</contexts>
<marker>Eichler, Neumann, 2008</marker>
<rawString>Holmer Hemsen Kathrin Eichler and Gnter Neumann. 2008. Unsupervised relation extraction from web documents. In LREC. http://www.lrecconf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>D Moldovan</author>
</authors>
<title>Acquisition of semantic patterns for information extraction from corpora.</title>
<date>1993</date>
<booktitle>In Procs. of Ninth IEEE Conference on Artificial Intelligence for Applications,</booktitle>
<pages>171--176</pages>
<contexts>
<context position="1166" citStr="Kim and Moldovan, 1993" startWordPosition="163" endWordPosition="166">ary relations expressed by verbs. We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOEpo3. More than 30% of REVERB’s extractions are at precision 0.8 or higher— compared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB’s errors, suggesting directions for future work.1 1 Introduction and Motivation Typically, Information Extraction (IE) systems learn an extractor for each target relation from labeled training examples (Kim and Moldovan, 1993; Riloff, 1996; Soderland, 1999). This approach to IE does not scale to corpora where the number of target relations is very large, or where the target relations cannot be specified in advance. Open IE solves this problem by identifying relation phrases—phrases that denote relations in English sentences (Banko et al., 2007). The automatic identification of rela1The source code for REVERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achiev</context>
</contexts>
<marker>Kim, Moldovan, 1993</marker>
<rawString>J. Kim and D. Moldovan. 1993. Acquisition of semantic patterns for information extraction from corpora. In Procs. of Ninth IEEE Conference on Artificial Intelligence for Applications, pages 171–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT-Discovery of Inference Rules from Text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM Conference on Knowledge Discovery and Data Mining(KDD-01),</booktitle>
<pages>323--328</pages>
<contexts>
<context position="9949" citStr="Lin and Pantel, 2001" startWordPosition="1576" endWordPosition="1579">2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually relying on a full dependency parse of the input sentence (Lin and Pantel, 2001; Stevenson, 2004; Specia and Motta, 2006; Kathrin Eichler and Neumann, 2008). Our work differs from these approaches by focusing on relation phrase patterns expressed in terms of POS tags and NP chunks, instead of full parse trees. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while pre</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT-Discovery of Inference Rules from Text. In Proceedings of ACM Conference on Knowledge Discovery and Data Mining(KDD-01), pages pp. 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying Functional Relations in Web Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1266--1276</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2108" citStr="Lin et al., 2010" startWordPosition="308" endWordPosition="311">. The automatic identification of rela1The source code for REVERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as TEXTRUNNER and WOE: incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples). Incoherent extractions arise because the learned extractor makes a sequence of decisions about whether to include each </context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2010</marker>
<rawString>Thomas Lin, Mausam, and Oren Etzioni. 2010. Identifying Functional Relations in Web Text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1266–1276, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury Martha</author>
<author>Martha Palmer</author>
</authors>
<title>From treebank to propbank. In</title>
<date>2002</date>
<booktitle>In Proceedings of LREC2002.</booktitle>
<contexts>
<context position="11279" citStr="Martha and Palmer, 2002" startWordPosition="1783" endWordPosition="1786">cal constraint that boosts precision and recall. Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in that both tasks extract relations and arguments from sentences. However, SRL systems traditionally rely on syntactic parsers, which makes them susceptible to parser errors and substantially slower than Open IE systems such as REVERB. This difference is particularly important when operating on the Web corpus due to its size and heterogeneity. Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input. In contrast, Open IE systems require no relation-specific training data. ReVerb, in particular, relies on its explicit lexical and syntactic constraints, which have no correlate in SRL systems. For a more detailed comparison of SRL and Open IE, see (Christensen et al., 2010). 3 Constraints on Relation Phrases In this section we introduce two constraints on relation phrases: a syntactic constraint and a lexical constraint. 3.1 Syntactic Constraint The syntactic constraint serves two purposes. First, it eliminates incoherent extractions, and second, it reduces uni</context>
</contexts>
<marker>Martha, Palmer, 2002</marker>
<rawString>Paul Kingsbury Martha and Martha Palmer. 2002. From treebank to propbank. In In Proceedings of LREC2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data. In</title>
<date>2009</date>
<booktitle>ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9310" citStr="Mintz et al., 2009" startWordPosition="1476" endWordPosition="1479">i, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over TEXTRUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually relying on a full dependency parse of t</context>
<context position="17094" citStr="Mintz et al., 2009" startWordPosition="2725" endWordPosition="2728">l Verbs: X turned Y off 4% Relation Phrase Not Between Arguments Intro. Phrases: Discovered by Y, X ... Relative Clauses: ... the Y that X discovered 3% Do Not Match POS Pattern Interrupting Modifiers: X has a lot of faith in Y Infinitives: X to attack Y Table 3: Approximately 85% of the binary verbal relation phrases in a sample of Web sentences satisfy our constraints. terms of dependency parse features that would capture the non-contiguous relation phrases in Table 3. Previous work has shown that dependency paths do indeed boost the recall of relation extraction systems (Wu and Weld, 2010; Mintz et al., 2009). While using dependency path features allows for a more flexible model of relations, it significantly increases processing time, which is problematic for Web-scale extraction. Further, we have found that this increased recall comes at the cost of lower precision on Web text (see Section 5). The results in Table 3 are similar to Banko and Etzioni’s findings that a set of eight POS patterns cover a large fraction of binary verbal relation phrases. However, their analysis was based on a set of sentences known to contain either a company acquisition or birthplace relationship, while our results a</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2, pages 1003–1011, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="10797" citStr="Punyakanok et al., 2008" startWordPosition="1712" endWordPosition="1715"> trees. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while previous work in Open IE has mainly focused on syntactic patterns for relation extraction, we introduce a lexical constraint that boosts precision and recall. Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in that both tasks extract relations and arguments from sentences. However, SRL systems traditionally rely on syntactic parsers, which makes them susceptible to parser errors and substantially slower than Open IE systems such as REVERB. This difference is particularly important when operating on the Web corpus due to its size and heterogeneity. Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input. In contrast, Open IE systems require no relation-specific training data. ReVerb, in pa</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically constructing extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Procs. of the Thirteenth National Conference on Artificial Intelligence (AAAI-96),</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="1180" citStr="Riloff, 1996" startWordPosition="167" endWordPosition="168">by verbs. We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOEpo3. More than 30% of REVERB’s extractions are at precision 0.8 or higher— compared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB’s errors, suggesting directions for future work.1 1 Introduction and Motivation Typically, Information Extraction (IE) systems learn an extractor for each target relation from labeled training examples (Kim and Moldovan, 1993; Riloff, 1996; Soderland, 1999). This approach to IE does not scale to corpora where the number of target relations is very large, or where the target relations cannot be specified in advance. Open IE solves this problem by identifying relation phrases—phrases that denote relations in English sentences (Banko et al., 2007). The automatic identification of rela1The source code for REVERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable m</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically constructing extraction patterns from untagged text. In Procs. of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), pages 1044–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A Latent Dirichlet Allocation Method for Selectional Preferences.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2055" citStr="Ritter et al., 2010" startWordPosition="300" endWordPosition="303">note relations in English sentences (Banko et al., 2007). The automatic identification of rela1The source code for REVERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as TEXTRUNNER and WOE: incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples). Incoherent extractions arise because the learned extractor makes a</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A Latent Dirichlet Allocation Method for Selectional Preferences. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd international conference on Machine learning, ICML ’05,</booktitle>
<pages>736--743</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="36243" citStr="Roth and Yih, 2005" startWordPosition="5895" endWordPosition="5898">rb-based relation phrases in English that ameliorate these problems and yield richer and more informative relations (see, for example, Table 2). • Based on these constraints, we designed, implemented, and evaluated the REVERB extractor, which substantially outperforms previous Open IE systems in both recall and precision. • We make REVERB and the data used in our experiments available to the research community.4 In future work, we plan to explore utilizing our constraints to improve the performance of learned CRF models. Roth et al. have shown how to incorporate constraints into CRF learners (Roth and Yih, 2005). It is natural, then, to consider whether the combination of heuristically labeled training examples, CRF learning, and our constraints will result in superior performance. The error analysis in Section 5.2 also suggests natural directions for future work. For instance, since many of REVERB’s errors are due to incorrect arguments, improved methods for argument extraction are in order. Acknowledgments We would like to thank Mausam, Dan Weld, Yoav Artzi, Luke Zettlemoyer, members of the KnowItAll 4http://reverb.cs.washington.edu group, and the anonymous reviewers for their helpful comments. Thi</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>Dan Roth and Wen-tau Yih. 2005. Integer linear programming inference for conditional random fields. In Proceedings of the 22nd international conference on Machine learning, ICML ’05, pages 736–743, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Jesse Davis</author>
</authors>
<title>Learning first-order horn clauses from web text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1088--1098</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2164" citStr="Schoenmackers et al., 2010" startWordPosition="315" endWordPosition="319">ce code for REVERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as TEXTRUNNER and WOE: incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples). Incoherent extractions arise because the learned extractor makes a sequence of decisions about whether to include each word in the relation phrase, often resulting in incompre</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, Davis, 2010</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld, and Jesse Davis. 2010. Learning first-order horn clauses from web text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1088–1098, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>On-demand information extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>731--738</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9233" citStr="Sekine, 2006" startWordPosition="1464" endWordPosition="1465">bsequent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over TEXTRUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs</context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>Satoshi Sekine. 2006. On-demand information extraction. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 731–738, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive Information Extraction using Unrestricted Relation Discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>304--311</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="9233" citStr="Shinyama and Sekine, 2006" startWordPosition="1462" endWordPosition="1465"> Treebank. Subsequent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over TEXTRUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive Information Extraction using Unrestricted Relation Discovery. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 304–311, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Brendan Roof</author>
<author>Bo Qin</author>
<author>Shi Xu</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Adapting open information extraction to domain-specific relations.</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="2287" citStr="Soderland et al., 2010" startWordPosition="336" endWordPosition="339">s from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as TEXTRUNNER and WOE: incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples). Incoherent extractions arise because the learned extractor makes a sequence of decisions about whether to include each word in the relation phrase, often resulting in incomprehensible predictions. To solve this problem, we introduce a syntactic constraint: every multi-word relation phrase must beg</context>
</contexts>
<marker>Soderland, Roof, Qin, Xu, Mausam, Etzioni, 2010</marker>
<rawString>Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu, Mausam, and Oren Etzioni. 2010. Adapting open information extraction to domain-specific relations. AI Magazine, 31(3):93–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
</authors>
<title>Learning Information Extraction Rules for Semi-Structured and Free Text.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1198" citStr="Soderland, 1999" startWordPosition="169" endWordPosition="170">mplemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOEpo3. More than 30% of REVERB’s extractions are at precision 0.8 or higher— compared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB’s errors, suggesting directions for future work.1 1 Introduction and Motivation Typically, Information Extraction (IE) systems learn an extractor for each target relation from labeled training examples (Kim and Moldovan, 1993; Riloff, 1996; Soderland, 1999). This approach to IE does not scale to corpora where the number of target relations is very large, or where the target relations cannot be specified in advance. Open IE solves this problem by identifying relation phrases—phrases that denote relations in English sentences (Banko et al., 2007). The automatic identification of rela1The source code for REVERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success </context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>S. Soderland. 1999. Learning Information Extraction Rules for Semi-Structured and Free Text. Machine Learning, 34(1-3):233–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Enrico Motta</author>
</authors>
<title>M.: A hybrid approach for extracting semantic relations from texts.</title>
<date>2006</date>
<booktitle>In In. Proceedings of the 2 nd Workshop on Ontology Learning and Population,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="9990" citStr="Specia and Motta, 2006" startWordPosition="1582" endWordPosition="1585">void relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually relying on a full dependency parse of the input sentence (Lin and Pantel, 2001; Stevenson, 2004; Specia and Motta, 2006; Kathrin Eichler and Neumann, 2008). Our work differs from these approaches by focusing on relation phrase patterns expressed in terms of POS tags and NP chunks, instead of full parse trees. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while previous work in Open IE has mainly focused </context>
</contexts>
<marker>Specia, Motta, 2006</marker>
<rawString>Lucia Specia and Enrico Motta. 2006. M.: A hybrid approach for extracting semantic relations from texts. In In. Proceedings of the 2 nd Workshop on Ontology Learning and Population, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Stevenson</author>
<author>Afsaneh Fazly</author>
<author>Ryan North</author>
</authors>
<title>Statistical measures of the semi-productivity of light verb constructions.</title>
<date>2004</date>
<booktitle>In 2nd ACL Workshop on Multiword Expressions,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3933" citStr="Stevenson et al., 2004" startWordPosition="600" endWordPosition="603">irical Methods in Natural Language Processing, pages 1535–1545, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics ous Open IE systems return the uninformative (Faust, made, a deal) instead of (Faust, made a deal with, the devil). This type of error is caused by improper handling of relation phrases that are expressed by a combination of a verb with a noun, such as light verb constructions (LVCs). An LVC is a multi-word expression composed of a verb and a noun, with the noun carrying the semantic content of the predicate (Grefenstette and Teufel, 1995; Stevenson et al., 2004; Allerton, 2002). Table 2 illustrates the wide range of relations expressed this way, which are not captured by existing open extractors. Our syntactic constraint leads the extractor to include nouns in the relation phrase, solving this problem. Although the syntactic constraint significantly reduces incoherent and uninformative extractions, it allows overly-specific relation phrases such as is offering only modest greenhouse gas reduction targets at. To avoid overly-specific relation phrases, we introduce an intuitive lexical constraint: a binary relation phrase ought to appear with at least</context>
</contexts>
<marker>Stevenson, Fazly, North, 2004</marker>
<rawString>Suzanne Stevenson, Afsaneh Fazly, and Ryan North. 2004. Statistical measures of the semi-productivity of light verb constructions. In 2nd ACL Workshop on Multiword Expressions, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
</authors>
<title>An unsupervised WordNet-based algorithm for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the “Beyond Named Entity” workshop at the Fourth International Conference on Language Resources and Evalutaion (LREC’04).</booktitle>
<contexts>
<context position="9966" citStr="Stevenson, 2004" startWordPosition="1580" endWordPosition="1581">nd On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually relying on a full dependency parse of the input sentence (Lin and Pantel, 2001; Stevenson, 2004; Specia and Motta, 2006; Kathrin Eichler and Neumann, 2008). Our work differs from these approaches by focusing on relation phrase patterns expressed in terms of POS tags and NP chunks, instead of full parse trees. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while previous work in Ope</context>
</contexts>
<marker>Stevenson, 2004</marker>
<rawString>M. Stevenson. 2004. An unsupervised WordNet-based algorithm for relation extraction. In Proceedings of the “Beyond Named Entity” workshop at the Fourth International Conference on Language Resources and Evalutaion (LREC’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>191</pages>
<contexts>
<context position="10822" citStr="Toutanova et al., 2008" startWordPosition="1716" endWordPosition="1719"> (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while previous work in Open IE has mainly focused on syntactic patterns for relation extraction, we introduce a lexical constraint that boosts precision and recall. Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in that both tasks extract relations and arguments from sentences. However, SRL systems traditionally rely on syntactic parsers, which makes them susceptible to parser errors and substantially slower than Open IE systems such as REVERB. This difference is particularly important when operating on the Web corpus due to its size and heterogeneity. Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input. In contrast, Open IE systems require no relation-specific training data. ReVerb, in particular, relies on its e</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34(2):161– 191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>118--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1914" citStr="Wu and Weld, 2010" startWordPosition="276" endWordPosition="279"> or where the target relations cannot be specified in advance. Open IE solves this problem by identifying relation phrases—phrases that denote relations in English sentences (Banko et al., 2007). The automatic identification of rela1The source code for REVERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as TEXTRUNNER and WOE: incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted re</context>
<context position="5303" citStr="Wu and Weld, 2010" startWordPosition="815" endWordPosition="818">ow binary relationships are expressed via verbs in English sentences, and implements them in the REVERB Open IE system. We release REVERB and the data used in our experiments to the research community. The rest of the paper is organized as follows. Section 2 analyzes previous work. Section 3 defines our constraints precisely. Section 4 describes REVERB, our implementation of the constraints. Section 5 reports on our experimental results. Section 6 concludes with a summary and discussion of future work. 2 Previous Work Open IE systems like TEXTRUNNER (Banko et al., 2007), WOEPOs, and WOEParse (Wu and Weld, 2010) focus on extracting binary relations of the form (arg1, relation phrase, arg2) from text. These systems all use the following three-step method: 1. Label: Sentences are automatically labeled with extractions using heuristics or distant supervision. Sentence Incoherent Relation The guide contains dead links contains omits and omits sites. The Mark 14 was central to the was central torpedo torpedo scandal of the fleet. They recalled that Nungesser recalled began began his career as a precinct leader. Table 1: Examples of incoherent extractions. Incoherent extractions make up approximately 13% o</context>
<context position="8963" citStr="Wu and Weld, 2010" startWordPosition="1419" endWordPosition="1422">is is a topic for future work, which we consider in Section 6. The first Open IE system was TEXTRUNNER (Banko et al., 2007), which used a Naive Bayes model with unlexicalized POS and NP-chunk features, trained using examples heuristically generated from the Penn Treebank. Subsequent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over TEXTRUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data fo</context>
<context position="17073" citStr="Wu and Weld, 2010" startWordPosition="2721" endWordPosition="2724">in 1995 by Y Phrasal Verbs: X turned Y off 4% Relation Phrase Not Between Arguments Intro. Phrases: Discovered by Y, X ... Relative Clauses: ... the Y that X discovered 3% Do Not Match POS Pattern Interrupting Modifiers: X has a lot of faith in Y Infinitives: X to attack Y Table 3: Approximately 85% of the binary verbal relation phrases in a sample of Web sentences satisfy our constraints. terms of dependency parse features that would capture the non-contiguous relation phrases in Table 3. Previous work has shown that dependency paths do indeed boost the recall of relation extraction systems (Wu and Weld, 2010; Mintz et al., 2009). While using dependency path features allows for a more flexible model of relations, it significantly increases processing time, which is problematic for Web-scale extraction. Further, we have found that this increased recall comes at the cost of lower precision on Web text (see Section 5). The results in Table 3 are similar to Banko and Etzioni’s findings that a set of eight POS patterns cover a large fraction of binary verbal relation phrases. However, their analysis was based on a set of sentences known to contain either a company acquisition or birthplace relationship</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using Wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 118–127, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Zaiqing Nie</author>
<author>Xiaojiang Liu</author>
<author>Bo Zhang</author>
<author>Ji-Rong Wen</author>
</authors>
<title>StatSnowball: a statistical approach to extracting entity relationships.</title>
<date>2009</date>
<booktitle>In WWW ’09: Proceedings of the 18th international conference on World wide web,</booktitle>
<pages>101--110</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1933" citStr="Zhu et al., 2009" startWordPosition="280" endWordPosition="283">t relations cannot be specified in advance. Open IE solves this problem by identifying relation phrases—phrases that denote relations in English sentences (Banko et al., 2007). The automatic identification of rela1The source code for REVERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as TEXTRUNNER and WOE: incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has n</context>
<context position="8743" citStr="Zhu et al., 2009" startWordPosition="1381" endWordPosition="1384">arn the constraints embedded in REVERB. Of course, a learning system, utilizing a different hypothesis space, and an appropriate set of training examples, could potentially learn and refine the constraints in REVERB. This is a topic for future work, which we consider in Section 6. The first Open IE system was TEXTRUNNER (Banko et al., 2007), which used a Naive Bayes model with unlexicalized POS and NP-chunk features, trained using examples heuristically generated from the Penn Treebank. Subsequent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over TEXTRUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preempt</context>
</contexts>
<marker>Zhu, Nie, Liu, Zhang, Wen, 2009</marker>
<rawString>Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. StatSnowball: a statistical approach to extracting entity relationships. In WWW ’09: Proceedings of the 18th international conference on World wide web, pages 101–110, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>