<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.796186">
<title confidence="0.9980125">
Spectral Learning Algorithms for
Natural Language Processing
</title>
<author confidence="0.998587">
Shay Cohen∗, Michael Collins∗, Dean P. Foster§, Karl Stratos∗, Lyle Ungar§
</author>
<affiliation confidence="0.991669">
∗Columbia University
§University of Pennsylvania
</affiliation>
<email confidence="0.970255">
scohen,mcollins,stratos@cs.columbia.edu
dean@foster.net
ungar@cis.upenn.edu
</email>
<sectionHeader confidence="0.999611" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999353571428571">
Recent work in machine learning and NLP has developed spectral algorithms for
many learning tasks involving latent variables. Spectral algorithms rely on sin-
gular value decomposition as a basic operation, usually followed by some simple
estimation method based on the method of moments. From a theoretical point
of view, these methods are appealing in that they offer consistent estimators (and
PAC-style guarantees of sample complexity) for several important latent-variable
models. This is in contrast to the EM algorithm, which is an extremely success-
ful approach, but which only has guarantees of reaching a local maximum of the
likelihood function.
From a practical point of view, the methods (unlike EM) have no need for
careful initialization, and have recently been shown to be highly efficient (as one
example, in work under submission by the authors on learning of latent-variable
PCFGs, a spectral algorithm performs at identical accuracy to EM, but is around
20 times faster).
</bodyText>
<sectionHeader confidence="0.998652" genericHeader="keywords">
2 Outline
</sectionHeader>
<bodyText confidence="0.9999094">
In this tutorial we will aim to give a broad overview of spectral methods, describing
theoretical guarantees, as well as practical issues. We will start by covering the
basics of singular value decomposition and describe efficient methods for doing
singular value decomposition. The SVD operation is at the core of most spectral
algorithms that have been developed.
</bodyText>
<page confidence="0.951884">
13
</page>
<bodyText confidence="0.997784148148148">
Tutorials, NAACL-HLT 2013, pages 13–15,
Atlanta, Georgia, June 9 2013. c�2013 Association for Computational Linguistics
We will then continue to cover canonical correlation analysis (CCA). CCA is an
early method from statistics for dimensionality reduction. With CCA, two or more
views of the data are created, and they are all projected into a lower dimensional
space which maximizes the correlation between the views. We will review the
basic algorithms underlying CCA, give some formal results giving guarantees for
latent-variable models and also describe how they have been applied recently to
learning lexical representations from large quantities of unlabeled data. This idea
of learning lexical representations can be extended further, where unlabeled data is
used to learn underlying representations which are subsequently used as additional
information for supervised training.
We will also cover how spectral algorithms can be used for structured predic-
tion problems with sequences and parse trees. A striking recent result by Hsu,
Kakade and Zhang (2009) shows that HMMs can be learned efficiently using a
spectral algorithm. HMMs are widely used in NLP and speech, and previous al-
gorithms (typically based on EM) were guaranteed to only reach a local maximum
of the likelihood function, so this is a crucial result. We will review the basic me-
chanics of the HMM learning algorithm, describe its formal guarantees, and also
cover practical issues.
Last, we will cover work about spectral algorithms in the context of natural
language parsing. We will show how spectral algorithms can be used to estimate
the parameter models of latent-variable PCFGs, a model which serves as the base
for state-of-the-art parsing models such as the one of Petrov et al. (2007). We will
show what are the practical steps that are needed to be taken in order to make spec-
tral algorithms for L-PCFGs (or other models in general) practical and comparable
to state of the art.
</bodyText>
<sectionHeader confidence="0.992018" genericHeader="introduction">
3 Speaker Bios
</sectionHeader>
<bodyText confidence="0.999573888888889">
Shay Cohen1 is a postdoctoral research scientist in the Department of Computer
Science at Columbia University. He is a computing innovation fellow. His re-
search interests span a range of topics in natural language processing and machine
learning. He is especially interested in developing efficient and scalable parsing
algorithms as well as learning algorithms for probabilistic grammars.
Michael Collins2 is the Vikram S. Pandit Professor of computer science at
Columbia University. His research is focused on topics including statistical pars-
ing, structured prediction problems in machine learning, and applications including
machine translation, dialog systems, and speech recognition. His awards include a
</bodyText>
<footnote confidence="0.999922">
1http://www.cs.columbia.edu/-scohen/
2http://www.cs.columbia.edu/-mcollins/
</footnote>
<page confidence="0.999122">
14
</page>
<bodyText confidence="0.999598789473684">
Sloan fellowship, an NSF career award, and best paper awards at EMNLP (2002,
2004, and 2010), UAI (2004 and 2005), and CoNLL 2008.
Dean P. Foster3 is currently the Marie and Joseph Melone Professor of Statis-
tics at the Wharton School of the University of Pennsylvania. His current research
interests are machine learning, stepwise regression and computational linguistics.
He has been searching for new methods of finding useful features in big data sets.
His current set of hammers revolve around fast matrix methods (which decompose
2nd moments) and tensor methods for decomposing 3rd moments.
Karl Stratos4 is a Ph.D. student in the Department of Computer Science at
Columbia. His research is focused on machine learning and natural language pro-
cessing. His current research efforts are focused on spectral learning of latent-
variable models, or more generally, uncovering latent structure from data.
Lyle Ungar5 is a professor at the Computer and Information Science Depart-
ment at the University of Pennsylvania. His research group develops scalable ma-
chine learning and text mining methods, including clustering, feature selection,
and semi-supervised and multi-task learning for natural language, psychology, and
medical research. Example projects include spectral learning of language models,
multi-view learning for gene expression and MRI data, and mining social media to
better understand personality and well-being.
</bodyText>
<footnote confidence="0.999926">
3http://gosset.wharton.upenn.edu/-foster/index.pl
4http://www.cs.columbia.edu/-stratos/
5http://www.cis.upenn.edu/-ungar/
</footnote>
<page confidence="0.99602">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.353855">
<title confidence="0.9998275">Spectral Learning Algorithms Natural Language Processing</title>
<author confidence="0.999197">Michael Dean P Karl Lyle</author>
<email confidence="0.660609">ofungar@cis.upenn.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>