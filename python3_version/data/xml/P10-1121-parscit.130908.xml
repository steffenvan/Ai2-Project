<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012243">
<title confidence="0.996132">
Complexity Metrics in an Incremental Right-corner Parser
</title>
<author confidence="0.999742">
Stephen Wu Asaf Bachrach† Carlos Cardenas* William Schuler°
</author>
<affiliation confidence="0.9588585">
Department of Computer Science, University of Minnesota
† Unit de Neuroimagerie Cognitive INSERM-CEA
* Department of Brain &amp; Cognitive Sciences, Massachussetts Institute of Technology
° University of Minnesota and The Ohio State University
</affiliation>
<email confidence="0.998525">
swu@cs.umn.edu †asaf@mit.edu ∗cardenas@mit.edu ◦schuler@ling.ohio-state.edu
</email>
<sectionHeader confidence="0.995628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877523809524">
Hierarchical HMM (HHMM) parsers
make promising cognitive models: while
they use a bounded model of working
memory and pursue incremental hypothe-
ses in parallel, they still achieve parsing
accuracies competitive with chart-based
techniques. This paper aims to validate
that a right-corner HHMM parser is also
able to produce complexity metrics, which
quantify a reader’s incremental difficulty
in understanding a sentence. Besides
defining standard metrics in the HHMM
framework, a new metric, embedding
difference, is also proposed, which tests
the hypothesis that HHMM store elements
represents syntactic working memory.
Results show that HHMM surprisal
outperforms all other evaluated metrics
in predicting reading times, and that
embedding difference makes a significant,
independent contribution.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999785557692308">
Since the introduction of a parser-based calcula-
tion for surprisal by Hale (2001), statistical tech-
niques have been become common as models of
reading difficulty and linguistic complexity. Sur-
prisal has received a lot of attention in recent lit-
erature due to nice mathematical properties (Levy,
2008) and predictive ability on eye-tracking move-
ments (Demberg and Keller, 2008; Boston et al.,
2008a). Many other complexity metrics have
been suggested as mutually contributing to reading
difficulty; for example, entropy reduction (Hale,
2006), bigram probabilities (McDonald and Shill-
cock, 2003), and split-syntactic/lexical versions of
other metrics (Roark et al., 2009).
A parser-derived complexity metric such as sur-
prisal can only be as good (empirically) as the
model of language from which it derives (Frank,
2009). Ideally, a psychologically-plausible lan-
guage model would produce a surprisal that would
correlate better with linguistic complexity. There-
fore, the specification of how to encode a syntac-
tic language model is of utmost importance to the
quality of the metric.
However, it is difficult to quantify linguis-
tic complexity and reading difficulty. The two
commonly-used empirical quantifications of read-
ing difficulty are eye-tracking measurements and
word-by-word reading times; this paper uses read-
ing times to find the predictiveness of several
parser-derived complexity metrics. Various fac-
tors (i.e., from syntax, semantics, discourse) are
likely necessary for a full accounting of linguis-
tic complexity, so current computational models
(with some exceptions) narrow the scope to syn-
tactic or lexical complexity.
Three complexity metrics will be calculated in
a Hierarchical Hidden Markov Model (HHMM)
parser that recognizes trees in right-corner form
(the left-right dual of left-corner form). This type
of parser performs competitively on standard pars-
ing tasks (Schuler et al., 2010); also, it reflects
plausible accounts of human language processing
as incremental (Tanenhaus et al., 1995; Brants and
Crocker, 2000), as considering hypotheses proba-
bilistically in parallel (Dahan and Gaskell, 2007),
as bounding memory usage to short-term mem-
ory limits (Cowan, 2001), and as requiring more
memory storage for center-embedding structures
than for right- or left-branching ones (Chomsky
and Miller, 1963; Gibson, 1998). Also, unlike
most other parsers, this parser preserves the arc-
eager/arc-standard ambiguity of Abney and John-
</bodyText>
<page confidence="0.966993">
1189
</page>
<note confidence="0.9425745">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189–1198,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999977905405406">
son (1991). Typical parsing strategies are arc-
standard, keeping all right-descendants open for
subsequent attachment; but since there can be an
unbounded number of such open constituents, this
assumption is not compatible with simple mod-
els of bounded memory. A consistently arc-eager
strategy acknowledges memory bounds, but yields
dead-end parses. Both analyses are considered in
right-corner HHMM parsing.
The purpose of this paper is to determine
whether the language model defined by the
HHMM parser can also predict reading times —
it would be strange if a psychologically plausi-
ble model did not also produce viable complex-
ity metrics. In the course of showing that the
HHMM parser does, in fact, predict reading times,
we will define surprisal and entropy reduction in
the HHMM parser, and introduce a third metric
called embedding difference.
Gibson (1998; 2000) hypothesized two types
of syntactic processing costs: integration cost, in
which incremental input is combined with exist-
ing structures; and memory cost, where unfinished
syntactic constructions may incur some short-term
memory usage. HHMM surprisal and entropy
reduction may be considered forms of integra-
tion cost. Though typical PCFG surprisal has
been considered a forward-looking metric (Dem-
berg and Keller, 2008), the incremental nature of
the right-corner transform causes surprisal and en-
tropy reduction in the HHMM parser to measure
the likelihood of grammatical structures that were
hypothesized before evidence was observed for
them. Therefore, these HHMM metrics resemble
an integration cost encompassing both backward-
looking and forward-looking information.
On the other hand, embedding difference is
designed to model the cost of storing center-
embedded structures in working memory. Chen,
Gibson, and Wolf (2005) showed that sentences
requiring more syntactic memory during sen-
tence processing increased reading times, and it
is widely understood that center-embedding incurs
significant syntactic processing costs (Miller and
Chomsky, 1963; Gibson, 1998). Thus, we would
expect for the usage of the center-embedding
memory store in an HHMM parser to correlate
with reading times (and therefore linguistic com-
plexity).
The HHMM parser processes syntactic con-
structs using a bounded number of store states,
defined to represent short-term memory elements;
additional states are utilized whenever center-
embedded syntactic structures are present. Simi-
lar models such as Crocker and Brants (2000) im-
plicitly allow an infinite memory size, but Schuler
et al. (2008; 2010) showed that a right-corner
HHMM parser can parse most sentences in En-
glish with 4 or fewer center-embedded-depth lev-
els. This behavior is similar to the hypothesized
size of a human short-term memory store (Cowan,
2001). A positive result in predicting reading
times will lend additional validity to the claim
that the HHMM parser’s bounded memory cor-
responds to bounded memory in human sentence
processing.
The rest of this paper is organized as fol-
lows: Section 2 defines the language model of the
HHMM parser, including definitions of the three
complexity metrics. The methodology for evalu-
ating the complexity metrics is described in Sec-
tion 3, with actual results in Section 4. Further dis-
cussion on results, and comparisons to other work,
are in Section 5.
</bodyText>
<sectionHeader confidence="0.990254" genericHeader="method">
2 Parsing Model
</sectionHeader>
<bodyText confidence="0.999980818181818">
This section describes an incremental parser in
which surprisal and entropy reduction are sim-
ple calculations (Section 2.1). The parser uses a
Hierarchical Hidden Markov Model (Section 2.2)
and recognizes trees in a right-corner form (Sec-
tion 2.3 and 2.4). The new complexity metric, em-
bedding difference (Section 2.5), is a natural con-
sequence of this HHMM definition. The model
is equivalent to previous HHMM parsers (Schuler,
2009), but reorganized into 5 cases to clarify the
right-corner structure of the parsed sentences.
</bodyText>
<subsectionHeader confidence="0.993833">
2.1 Surprisal and Entropy in HMMs
</subsectionHeader>
<bodyText confidence="0.999958846153846">
Hidden Markov Models (HMMs) probabilistically
connect sequences of observed states ot and hid-
den states qt at corresponding time steps t. In pars-
ing, observed states are words; hidden states can
be a conglomerate state of linguistic information,
here taken to be syntactic.
The HMM is an incremental, time-series struc-
ture, so one of its by-products is the prefix prob-
ability, which will be used to calculate surprisal.
This is the probability that that words oi..t have
been observed at time t, regardless of which syn-
tactic states qi..t produced them. Bayes’ Law and
Markov independence assumptions allow this to
</bodyText>
<page confidence="0.984162">
1190
</page>
<bodyText confidence="0.999377444444444">
be calculated from two generative probability dis-
tributions.1
Here, probabilities arise from a Transition
Model (OA) between hidden states and an Ob-
servation Model (OB) that generates an observed
state from a hidden state. These models are so
termed for historical reasons (Rabiner, 1990).
Surprisal (Hale, 2001) is then a straightforward
calculation from the prefix probability.
</bodyText>
<equation confidence="0.999717666666667">
Pre(o1..t–1)
Surprisal(t) = l�92 (3)
Pre(o1..t)
</equation>
<bodyText confidence="0.9987194">
This framing of prefix probability and surprisal in
a time-series model is equivalent to Hale’s (2001;
2006), assuming that q1..t E Dt, i.e., that the syn-
tactic states we are considering form derivations
Dt, or partial trees, consistent with the observed
words. We will see that this is the case for our
parser in Sections 2.2–2.4.
Entropy is a measure of uncertainty, defined as
H(x) = −P(x)l092 P(x). Now, the entropy Ht
of a t-word string o1..t in an HMM can be written:
</bodyText>
<equation confidence="0.9528085">
XHt = P(q1..t o1..t) l092 P(q1..t o1..t) (4)
q1..t
</equation>
<bodyText confidence="0.9978025">
and entropy reduction (Hale, 2003; Hale, 2006) at
the tth word is then
</bodyText>
<equation confidence="0.995998">
ER(ot) = max(0, Ht−1 − Ht) (5)
</equation>
<bodyText confidence="0.9996906">
Both of these metrics fall out naturally from the
time-series representation of the language model.
The third complexity metric, embedding differ-
ence, will be discussed after additional back-
ground in Section 2.5.
In the implementation of an HMM, candidate
states at a given time qt are kept in a trel-
lis, with step-by-step backpointers to the highest-
probability q1..t–1.2 Also, the best qt are often kept
in a beam Lit, discarding low-probability states.
</bodyText>
<footnote confidence="0.92124675">
1Technically, a prior distribution over hidden states,
P(q0), is necessary. This q0 is factored and taken to be a de-
terministic constant, and is therefore unimportant as a proba-
bility model.
2Typical tasks in an HMM include finding the most likely
sequence via the Viterbi algorithm, which stores these back-
pointers to maximum-probability previous states and can
uniquely find the most likely sequence.
</footnote>
<bodyText confidence="0.999946888888889">
This mitigates the problems of large state spaces
(e.g., that of all possible grammatical derivations).
Since beams have been shown to perform well
(Brants and Crocker, 2000; Roark, 2001; Boston
et al., 2008b), complexity metrics in this paper
are calculated on a beam rather than over all (un-
bounded) possible derivations Dt. The equations
above, then, will replace the assumption q1..t E Dt
with qt ELit.
</bodyText>
<subsectionHeader confidence="0.999223">
2.2 Hierarchical Hidden Markov Models
</subsectionHeader>
<bodyText confidence="0.999825666666667">
Hidden states q can have internal structure; in Hi-
erarchical HMMs (Fine et al., 1998; Murphy and
Paskin, 2001), this internal structure will be used
to represent syntax trees and looks like several
HMMs stacked on top of each other. As such, qt
is factored into sequences of depth-specific vari-
ables — one for each of D levels in the HMM hi-
erarchy. In addition, an intermediate variable ft is
introduced to interface between the levels.
</bodyText>
<equation confidence="0.909514">
def 1 D
= (qt ... qt ) (6)
def
ft = (f1t ... fDt ) (7)
</equation>
<bodyText confidence="0.9778715">
Transition probabilities POA(qt  |qt–1) over com-
plex hidden states qt are calculated in two phases:
</bodyText>
<listItem confidence="0.9714958">
• Reduce phase. Yields an intermediate
state ft, in which component HMMs may ter-
minate. This ft tells “higher” HMMs to hold
over their information if “lower” levels are in
operation at any time step t, and tells lower
HMMs to signal when they’re done.
• Shift phase. Yields a modeled hidden state qt,
in which unterminated HMMs transition, and
terminated HMMs are re-initialized from
their parent HMMs.
</listItem>
<bodyText confidence="0.966407">
Each phase is factored according to level-
specific reduce and shift models, OF and OQ:
</bodyText>
<equation confidence="0.960451">
POA(qt|qt–1) = X P(ft|qt–1)&apos;P(qt|ft qt–1) (8)
ft
POF(fdt|fd+1
t qd t–1qd–1
t–1 )
&apos; POQ(qdt |fd+1
t fd t qd t–1qd–1
t ) (9)
with fD+1
</equation>
<bodyText confidence="0.995562">
t and qo defined as constants. Note that
only qt is present at the end of the probability cal-
culation. In step t, ft–1 will be unused, so the
marginalization of Equation 9 does not lose any
information.
</bodyText>
<equation confidence="0.9812548125">
XPre(o1..t)=
q1..t
Xdef =
q1..t
P(o1..t q1..t) (1)
POA(qτ  |qτ–1)&apos;POB(oτ  |qτ) (2)
t
Y
τ=1
qt
Xdef =
f1..D
t
D
Y
d=1
</equation>
<page confidence="0.930953">
1191
</page>
<figure confidence="0.998523647887324">
. . .
. . .
. . .
. . .
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8
f it− i
f it
ft i
ft i
q it− i
qt i
qt i
ft
ft
q it
qt
qt
d=1
d=2
d=3
ot− i
ot
word
(a) Dependency structure in the HHMM
parser. Conditional probabilities at a node are
dependent on incoming arcs.
(b) HHMM parser as a store whose elements at each time step are listed
vertically, showing a good hypothesis on a sample sentence out of many
kept in parallel. Variables corresponding to qd are shown.
S
S/NN
NN
trick
S/NN
NN
NN
engineers
VBD/PRT
VBD
pulled
S/VP
NP
NP/NN
DT
the
DT
an
engineering
PRT
off
S/NP
VBD
NP
VP
S
VBD
an
NN
PRT
off
VBD
pulled
DT
NP
DT
the
NN
engineers
NN NN
engineering trick
(c) A sample sentence in CNF. (d) The right-corner transformed version of (c).
</figure>
<figureCaption confidence="0.621781">
Figure 1: Various graphical representations of HHMM parser operation. (a) shows probabilistic depen-
dencies. (b) considers the qd store to be incremental syntactic information. (c)–(d) demonstrate the
right-corner transform, similar to a left-to-right traversal of (c). In ‘NP/NN’ we say that NP is the active
constituent and NN is the awaited.
</figureCaption>
<bodyText confidence="0.98305525">
The Observation Model OB is comparatively
much simpler. It is only dependent on the syntac-
tic state at D (or the deepest active HHMM level).
POB(ot I qt) def = P(ot I qD ) (10)
Figure 1(a) gives a schematic of the dependency
structure of Equations 8–10 for D = 3. Evalua-
tions in this paper are done with D = 4, following
the results of Schuler, et al. (2008).
</bodyText>
<subsectionHeader confidence="0.999937">
2.3 Parsing right-corner trees
</subsectionHeader>
<bodyText confidence="0.991521111111111">
In this HHMM formulation, states and dependen-
cies are optimized for parsing right-corner trees
(Schuler et al., 2008; Schuler et al., 2010). A sam-
ple transformation between CNF and right-corner
trees is in Figures 1(c)–1(d).
Figure 1(b) shows the corresponding store-
element interpretation3 of the right corner tree
in 1(d). These can be used as a case study to
see what kind of operations need to occur in an
</bodyText>
<footnote confidence="0.956022">
3This is technically a pushdown automoton (PDA), where
the store is limited to D elements. When referring to direc-
tions (e.g., up, down), PDAs are typically described opposite
of the one in Figure 1(b); here, we push “up” instead of down.
</footnote>
<bodyText confidence="0.99960547826087">
HHMM when parsing right-corner trees. There
is one unique set of HHMM state values for each
tree, so the operations can be seen on either the
tree or the store elements.
At each time step t, a certain number of el-
ements (maximum D) are kept in memory, i.e.,
in the store. New words are observed input, and
the bottom occupied element (the “frontier” of the
store) is the context; together, they determine what
the store will look like at t+1. We can characterize
the types of store-element changes by when they
happen in Figures 1(b) and 1(d):
Cross-level Expansion (CLE). Occupies a new
store element at a given time step. For exam-
ple, at t =1, a new store element is occupied
which can interact with the observed word,
“the.” At t = 3, an expansion occupies the
second store element.
In-level Reduction (ILR). Completes an active
constituent that is a unary child in the right-
corner tree; always accompanied by an in-
level expansion. At t = 2, “engineers” com-
pletes the active NP constituent; however, the
</bodyText>
<page confidence="0.97964">
1192
</page>
<bodyText confidence="0.999796787878788">
level is not yet complete since the NP is along
the left-branching trunk of the tree.
In-level Expansion (ILE). Starts a new active
constituent at an already-occupied store ele-
ment; always follows an in-level reduction.
With the NP complete in t = 2, a new active
constituent S is produced at t=3.
In-level Transition (ILT). Transitions the store
to a new state in the next time step at the same
level, where the awaited constituent changes
and the active constituent remains the same.
This describes each of the steps from t=4 to
t=8 at d=1 .
Cross-level Reduction (CLR). Vacates a store
element on seeing a complete active con-
stituent. This occurs after t = 4; “off”
completes the active (at depth 2) VBD con-
stituent, and vacates store element 2. This
is accompanied with an in-level transition at
depth 1, producing the store at t=5. It should
be noted that with some probability, complet-
ing the active constituent does not vacate the
store element, and the in-level reduction case
would have to be invoked.
The in-level/cross-level ambiguity occurs in the
expansion as well as the reduction, similar to Ab-
ney and Johnson’s arc-eager/arc-standard compo-
sition strategies (1991). At t = 3, another possible
hypothesis would be to remain on store element
1 using an ILE instead of a CLE. The HHMM
parser, unlike most other parsers, will preserve this
in-level/cross-level ambiguity by considering both
hypotheses in parallel.
</bodyText>
<subsectionHeader confidence="0.989119">
2.4 Reduce and Shift Models
</subsectionHeader>
<bodyText confidence="0.998833666666667">
With the understanding of what operations need to
occur, a formal definition of the language model is
in order. Let us begin with the relevant variables.
A shift variable qdt at depth d and time step t is
a syntactic state that must represent the active and
awaited constituents of right-corner form:
</bodyText>
<equation confidence="0.915685666666667">
def
qd = (gA (11)
t qd t , gW qd t )
</equation>
<bodyText confidence="0.967811166666667">
e.g., in Figure 1(b), q12=(NP,NN)=NP/NN. Each g is
a constituent from the pre-right-corner grammar,
G.
Reduce variables f are then enlisted to ensure
that in-level and cross-level operations are correct.
d def
</bodyText>
<equation confidence="0.793864">
ft = (kfdt ,gfdt ) (12)
</equation>
<bodyText confidence="0.99510225">
First, kfd t is a switching variable that differenti-
ates between ILT, CLE/CLR, and ILE/ILR. This
switching is the most important aspect of fdt , so
regardless of what gfd t is, we will use:
</bodyText>
<listItem confidence="0.999877666666667">
• fdt E F0 when kfdt =0, (ILT/no-op)
• fdt E F1 when kfdt =1, (CLE/CLR)
• fdt E FG when kfdt E G. (ILE/ILR)
</listItem>
<bodyText confidence="0.9999455">
Then, gfd t is used to keep track of a completely-
recognized constituent whenever a reduction oc-
curs (ILR or CLR). For example, in Figure 1(b),
after time step 2, an NP has been completely rec-
ognized and precipitates an ILR. The NP gets
stored in gf13 for use in the ensuing ILE instead
of appearing in the store-elements.
This leads us to a specification of the reduce and
shift probability models. The reduce step happens
first at each time step. True to its name, the re-
duce step handles in-level and cross-level reduc-
tions (the second and third case below):
</bodyText>
<equation confidence="0.9977924">
PΘF(fdt  |fd+1
t qd t−1qd−1
t−1 )def =
�iffd+1
if fd+1
t ∈FG, fdt ∈ F1 : �PΘF-ILR,d(fdt  |qdt�1 qd�1
t�1 ) (13)
if fd+1
t ∈FG, fdt ∈ FG : �PΘF-CLR,d(fdt  |qdt�1 qd�1
t�1 )
</equation>
<bodyText confidence="0.997870588235294">
with edge cases q0t and fD+1 tdefined as appropri-
ate constants. The first case is just store-element
maintenance, in which the variable is not on the
“frontier” and therefore inactive.
Examining OF-ILR,d and OF-CLR,d, we see that
the produced fdt variables are also used in the “if”
statement. These models can be thought of as
picking out a fdt first, finding the matching case,
then applying the probability models that matches.
These models are actually two parts of the same
model when learned from trees.
Probabilities in the shift step are also split into
cases based on the reduce variables. More main-
tenance operations (first case) accompany transi-
tions producing new awaited constituents (second
case below) and expansions producing new active
constituents (third and fourth case):
</bodyText>
<equation confidence="0.99573425">
PΘQ(qd t  |fd+1
t fdt qd t−1qd−1
t )def =
{ if fd+1
t 6∈FG : Qqdt = qdt�1J
t ∈FG, fdt ∈ F0 : �PΘQ-ILT,d(qdt  |fd+1 tqdt�1 qd�1
t )
if fd+1
t )
if fd+1
t ∈FG, fdt ∈ F1 : �PΘQ-ILE,d(qdt  |fdt qdt�1qd�1
t ∈FG, fdt ∈FG : �PΘQ-CLE,d(qdt  |qd�1
if fd+1
t )
(14)
t 6∈FG : Qfdt =0J
</equation>
<page confidence="0.962655">
1193
</page>
<table confidence="0.999390533333333">
FACTOR DESCRIPTION EXPECTED
Word order in For each story, words were indexed. Subjects would tend to read faster later in a story. negative
narrative slope
Reciprocal Log of the reciprocal of the number of letters in each word. A decrease in the reciprocal positive
length (increase in length) might mean longer reading times. slope
Unigram A log-transformed empirical count of word occurrences in the Brown Corpus section of negative
frequency the Penn Treebank. Higher frequency should indicate shorter reading times. slope
Bigram A log-transformed empirical count of two-successive-word occurrences, with Good- negative
probability Turing smoothing on words occuring less than 10 times. slope
Embedding Amount of change in HHMM weighted-average embedding depth. Hypothesized to in- positive
difference crease with larger working memory requirements, which predict longer reading times. slope
Entropy Amount of decrease in the HHMM’s uncertainty about the sentence. Larger reductions positive
reduction in uncertainty are hypothesized to take longer. slope
Surprisal “Surprise value” of a word in the HHMM parser; models were trained on the Wall Street positive
Journal, sections 02–21. More surprising words may take longer to read. slope
</table>
<tableCaption confidence="0.999934">
Table 1: A list of factors hypothesized to contribute to reading times. All data was mean-centered.
</tableCaption>
<bodyText confidence="0.999847">
A final note: the notation �Pp(·  |·) has been used
to indicate probability models that are empirical,
trained directly from frequency counts of right-
corner transformed trees in a large corpus. Alter-
natively, a standard PCFG could be trained on a
corpus (or hand-specified), and then the grammar
itself can be right-corner transformed (Schuler,
2009).
Taken together, Equations 11–14 define the
probabilistic structure of the HHMM for parsing
right-corner trees.
</bodyText>
<subsectionHeader confidence="0.990993">
2.5 Embedding difference in the HHMM
</subsectionHeader>
<bodyText confidence="0.995564363636364">
It should be clear from Figure 1 that at any time
step while parsing depth-bounded right-corner
trees, the candidate hidden state qt will have a
“frontier” depth d(qt). At time t, the beam of
possible hidden states qt stores the syntactic state
(and a backpointer) along with its probability,
P(o1..t q1..t). The average embedding depth at a
time step is then
Eq′t∈Bt P(o1..t q′1..t)
where we have directly used the beam notation.
The embedding difference metric is:
</bodyText>
<equation confidence="0.997069">
EmbDiff(o1..t) = µEMB(o1..t) − µEMB(o1..t−1)
</equation>
<bodyText confidence="0.972815">
There is a strong computational correspondence
between this definition of embedding difference
and the previous definition of surprisal. To see
this, we rewrite Equations 1 and 3:
</bodyText>
<equation confidence="0.99317225">
�Pre(o1..t)= P(o1..t q1..t) (1′)
qt∈Bt
Surprisal(t) = lo92 Pre(o1..t–1) − lo92 Pre(o1..t)
(3′)
</equation>
<bodyText confidence="0.999953434782609">
Both surprisal and embedding difference include
summations over the elements of the beam, and
are calculated as a difference between previous
and current beam states.
Most differences between these metrics are rel-
atively inconsequential. For example, the dif-
ference in order of subtraction only assures that
a positive correlation with reading times is ex-
pected. Also, the presence of a logarithm is rel-
atively minor. Embedding difference weighs the
probabilities with center-embedding depths and
then normalizes the values; since the measure is
a weighted average of embedding depths rather
than a probability distribution, µEMB is not always
less than 1 and the correspondence with Kullback-
Leibler divergence (Levy, 2008) does not hold, so
it does not make sense to take the logs.
Therefore, the inclusion of the embedding
depth, d(qt), is the only significant difference
between the two metrics. The result is a met-
ric that, despite numerical correspondence to sur-
prisal, models the HHMM’s hypotheses about
memory cost.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9996436">
Surprisal, entropy reduction, and embedding dif-
ference from the HHMM parser were evaluated
against a full array of factors (Table 1) on a cor-
pus of word-by-word reading times using a linear
mixed-effects model.
</bodyText>
<equation confidence="0.99958675">
�µEMB(o1..t) =
qt∈Bt
P(o1..t q1..t)
d(qt) ·
</equation>
<page confidence="0.97143">
1194
</page>
<bodyText confidence="0.999958205882353">
The corpus of reading times for 23 native En-
glish speakers was collected on a set of four nar-
ratives (Bachrach et al., 2009), each composed of
sentences that were syntactically complex but con-
structed to appear relatively natural. Using Linger
2.88, words appeared one-by-one on the screen,
and required a button-press in order to advance;
they were displayed in lines with 11.5 words on
average.
Following Roark et al.’s (2009) work on the
same corpus, reading times above 1500 ms (for
diverted attention) or below 150 ms (for button
presses planned before the word appeared) were
discarded. In addition, the first and last word of
each line on the screen were removed; this left
2926 words out of 3540 words in the corpus.
For some tests, a division between open- and
closed-class words was made, with 1450 and 1476
words, respectively. Closed-class words (e.g., de-
terminers or auxiliary verbs) usually play some
kind of syntactic function in a sentence; our evalu-
ations used Roark et al.’s list of stop words. Open
class words (e.g., nouns and other verbs) more
commonly include new words. Thus, one may ex-
pect reading times to differ for these two types of
words.
Linear mixed-effect regression analysis was
used on this data; this entails a set of fixed effects
and another of random effects. Reading times y
were modeled as a linear combination of factors
x, listed in Table 1 (fixed effects); some random
variation in the corpus might also be explained by
groupings according to subject i, word j, or sen-
tence k (random effects).
</bodyText>
<equation confidence="0.974908">
yijk _ N0 + �m Nℓxijkℓ + bi + bj + bk + E (17)
ℓ��
</equation>
<bodyText confidence="0.99970325">
This equation is solved for each of m fixed-
effect coefficients Q with a measure of confidence
(t-value = �Q/SE(�Q), where SE is the standard er-
ror). Qo is the standard intercept to be estimated
along with the rest of the coefficients, to adjust for
affine relationships between the dependent and in-
dependent variables. We report factors as statisti-
cally significant contributors to reading time if the
absolute value of the t-value is greater than 2.
Two more types of comparisons will be made to
see the significance of factors. First, a model of
data with the full list of factors can be compared
to a model with a subset of those factors. This is
done with a likelihood ratio test, producing (for
mixed-effects models) a Xi value and correspond-
ing probability that the smaller model could have
produced the same estimates as the larger model.
A lower probability indicates that the additional
factors in the larger model are significant.
Second, models with different fixed effects can
be compared to each other through various infor-
mation criteria; these trade off between having
a more explanatory model vs. a simpler model,
and can be calculated on any model. Here, we
use Akaike’s Information Criterion (AIC), where
lower values indicate better models.
All these statistics were calculated in R, using
the lme4 package (Bates et al., 2008).
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999956085714286">
Using the full list of factors in Table 1, fixed-effect
coefficients were estimated in Table 2. Fitting the
best model by AIC would actually prune away
some of the factors as relatively insignificant, but
these smaller models largely accord with the sig-
nificance values in the table and are therefore not
presented.
The first data column shows the regression on
all data; the second and third columns divide the
data into open and closed classes, because an eval-
uation (not reported in detail here) showed statis-
tically significant interactions between word class
and 3 of the predictors. Additionally, this facil-
itates comparison with Roark et al. (2009), who
make the same division.
Out of the non-parser-based metrics, word order
and bigram probability are statistically significant
regardless of the data subset; though reciprocal
length and unigram frequency do not reach signif-
icance here, likelihood ratio tests (not shown) con-
firm that they contribute to the model as a whole.
It can be seen that nearly all the slopes have been
estimated with signs as expected, with the excep-
tion of reciprocal length (which is not statistically
significant).
Most notably, HHMM surprisal is seen here to
be a standout predictive measure for reading times
regardless of word class. If the HHMM parser is
a good psycholinguistic model, we would expect
it to at least produce a viable surprisal metric, and
Table 2 attests that this is indeed the case. Though
it seems to be less predictive of open classes, a
surprisal-only model has the best AIC (-7804) out
of any open-class model. Considering the AIC
on the full data, the worst model with surprisal
</bodyText>
<page confidence="0.973977">
1195
</page>
<table confidence="0.9994184">
FULL DATA OPEN CLASS CLOSED CLASS
Coefficient Std. Err. t-value Coefficient Std. Err. t-value Coefficient Std. Err. t-value
(Intcpt) -9.340·10−3 5.347·10 −2 -0.175 -1.237·10−2 5.217·10−2 -0.237 -6.295·10−2 7.930·10−2 -0.794
order -3.746·10−5 7.808·10 −6 -4.797∗ -3.697·10−5 8.002·10−6 -4.621∗ -3.748·10−5 8.854·10−6 -4.232∗
rlength -2.002·10−2 1.635·10 −2 -1.225 9.849·10−3 1.779·10−2 0.554 -2.839·10−2 3.283·10−2 -0.865
unigrm -8.090·10−2 3.690·10 −1 -0.219 -1.047·10−1 2.681·10−1 -0.391 -3.847·10+0 5.976·10+0 -0.644
bigrm -2.074·10+0 8.132·10 −1 -2.551∗ -2.615·10+0 8.050·10−1 -3.248∗ -5.052·10+1 1.910·10+1 -2.645∗
embdiff 9.390·10−3 3.268·10 −3 2.873∗ 2.432·10−3 4.512·10−3 0.539 1.598·10−2 5.185·10−3 3.082∗
etrpyrd 2.753·10−2 6.792·10 −3 4.052∗ 6.634·10−4 1.048·10−2 0.063 4.938·10−2 1.017·10−2 4.857∗
srprsl 3.950·10−3 3.452·10 −4 11.442∗ 2.892·10−3 4.601·10−4 6.285∗ 5.201·10−3 5.601·10−4 9.286∗
</table>
<tableCaption confidence="0.988113">
Table 2: Results of linear mixed-effect modeling. Significance (indicated by ∗) is reported at p &lt; 0.05.
</tableCaption>
<table confidence="0.999831375">
(Intr) order rlngth ungrm bigrm emdiff entrpy
order .000
rlength -.006 -.003
unigrm .049 .000 -.479
bigrm .001 .005 -.006 -.073
emdiff .000 .009 -.049 -.089 .095
etrpyrd .000 .003 .016 -.014 .020 -.010
srprsl .000 -.008 -.033 -.079 .107 .362 .171
</table>
<tableCaption confidence="0.999979">
Table 3: Correlations in the full model.
</tableCaption>
<bodyText confidence="0.999937172413793">
(AIC=-10589) outperformed the best model with-
out it (AIC=-10478), indicating that the HHMM
surprisal is well worth including in the model re-
gardless of the presence of other significant fac-
tors.
HHMM entropy reduction predicts reading
times on the full dataset and on closed-class
words. However, its effect on open-class words is
insignificant; if we compare the model of column
2 against one without entropy reduction, a likeli-
hood ratio test gives Xi = 0.0022,p = 0.9623
(the smaller model could easily generate the same
data).
The HHMM’s average embedding difference
is also significant except in the case of open-
class words — removing embedding difference on
open-class data yields Xi = 0.2739,p = 0.6007.
But what is remarkable is that there is any signifi-
cance for this metric at all. Embedding difference
and surprisal were relatively correlated compared
to other predictors (see Table 3), which is expected
because embedding difference is calculated like
a weighted version of surprisal. Despite this, it
makes an independent contribution to the full-data
and closed-class models. Thus, we can conclude
that the average embedding depth component af-
fects reading times — i.e., the HHMM’s notion of
working memory behaves as we would expect hu-
man working memory to behave.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999797692307692">
As with previous work on large-scale parser-
derived complexity metrics, the linear mixed-
effect models suggest that sentence-level factors
are effective predictors for reading difficulty — in
these evaluations, better than commonly-used lex-
ical and near-neighbor predictors (Pollatsek et al.,
2006; Engbert et al., 2005). The fact that HHMM
surprisal outperforms even n-gram metrics points
to the importance of including a notion of sentence
structure. This is particularly true when the sen-
tence structure is defined in a language model that
is psycholinguistically plausible (here, bounded-
memory right-corner form).
This accords with an understated result of
Boston et al.’s eye-tracking study (2008a): a
richer language model predicts eye movements
during reading better than an oversimplified one.
The comparison there is between phrase struc-
ture surprisal (based on Hale’s (2001) calculation
from an Earley parser), and dependency grammar
surprisal (based on Nivre’s (2007) dependency
parser). Frank (2009) similarly reports improve-
ments in the reading-time predictiveness of unlexi-
calized surprisal when using a language model that
is more plausible than PCFGs.
The difference in predictivity due to word class
is difficult to explain. One theory may be that
closed-class words are less susceptible to random
effects because there is a finite set of them for
any language, making them overall easier to pre-
dict via parser-derived metrics. Or, we could note
that since closed-class words often serve grammat-
ical functions in addition to their lexical content,
they contribute more information to parser-derived
measures than open-class words. Previous work
with complexity metrics on this corpus (Roark et
al., 2009) suggests that these explanations only ac-
count for part of the word-class variation in the
performance of predictors.
</bodyText>
<page confidence="0.976789">
1196
</page>
<bodyText confidence="0.999661256410257">
Further comparsion to Roark et al. will show
other differences, such as the lesser role of word
length and unigram frequency, lower overall cor-
relations between factors, and the greater predic-
tivity of their entropy metric. In addition, their
metrics are different from ours in that they are de-
signed to tease apart lexical and syntactic contri-
butions to reading difficulty. Their notion of en-
tropy, in particular, estimates Hale’s definition of
entropy on whole derivations (2006) by isolating
the predictive entropy; they then proceed to define
separate lexical and syntactic predictive entropies.
Drawing more directly from Hale, our definition
is a whole-derivation metric based on the condi-
tional entropy of the words, given the root. (The
root constituent, though unwritten in our defini-
tions, is always included in the HHMM start state,
q0.)
More generally, the parser used in these evalu-
ations differs from other reported parsers in that
it is not lexicalized. One might expect for this
to be a weakness, allowing distributions of prob-
abilities at each time step in places not licensed
by the observed words, and therefore giving poor
probability-based complexity metrics. However,
we see that this language model performs well
despite its lack of lexicalization. This indicates
that lexicalization is not a requisite part of syntac-
tic parser performance with respect to predicting
linguistic complexity, corroborating the evidence
of Demberg and Keller’s (2008) ‘unlexicalized’
(POS-generating, not word-generating) parser.
Another difference is that previous parsers have
produced useful complexity metrics without main-
taining arc-eager/arc-standard ambiguity. Results
show that including this ambiguity in the HHMM
at least does not invalidate (and may in fact im-
prove) surprisal or entropy reduction as reading-
time predictors.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999983583333333">
The task at hand was to determine whether the
HHMM could consistently be considered a plau-
sible psycholinguistic model, producing viable
complexity metrics while maintaining other char-
acteristics such as bounded memory usage. The
linear mixed-effects models on reading times val-
idate this claim. The HHMM can straightfor-
wardly produce highly-predictive, standard com-
plexity metrics (surprisal and entropy reduction).
HHMM surprisal performs very well in predicting
reading times regardless of word class. Our for-
mulation of entropy reduction is also significant
except in open-class words.
The new metric, embedding difference, uses the
average center-embedding depth of the HHMM
to model syntactic-processing memory cost. This
metric can only be calculated on parsers with an
explicit representation for short-term memory el-
ements like the right-corner HHMM parser. Re-
sults show that embedding difference does predict
reading times except in open-class words, yielding
a significant contribution independent of surprisal
despite the fact that its definition is similar to that
of surprisal.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999593875">
Thanks to Brian Roark for help on the reading
times corpus, Tim Miller for the formulation of
entropy reduction, Mark Holland for statistical in-
sight, and the anonymous reviewers for their input.
This research was supported by National Science
Foundation CAREER/PECASE award 0447685.
The views expressed are not necessarily endorsed
by the sponsors.
</bodyText>
<sectionHeader confidence="0.997206" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995824565217391">
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233–250.
Asaf Bachrach, Brian Roark, Alex Marantz, Susan
Whitfield-Gabrieli, Carlos Cardenas, and John D.E.
Gabrieli. 2009. Incremental prediction in naturalis-
tic language processing: An fMRI study.
Douglas Bates, Martin Maechler, and Bin Dai. 2008.
lme4: Linear mixed-effects models using S4 classes.
R package version 0.999375-31.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
U. Patil, and Shravan Vasishth. 2008a. Parsing costs
as predictors of reading difficulty: An evaluation us-
ing the Potsdam Sentence Corpus. Journal of Eye
Movement Research, 2(1):1–12.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
and Shravan Vasishth. 2008b. Surprising parser ac-
tions and reading difficulty. In Proceedings of ACL-
08: HLT, Short Papers, pages 5–8, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Brants and Matthew Crocker. 2000. Prob-
abilistic parsing and psychological plausibility. In
Proceedings of COLING ’00, pages 111–118.
</reference>
<page confidence="0.872824">
1197
</page>
<reference confidence="0.999879699029126">
Evan Chen, Edward Gibson, and Florian Wolf. 2005.
Online syntactic storage costs in sentence com-
prehension. Journal of Memory and Language,
52(1):144–169.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269–321. Wiley.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87–
185.
Matthew Crocker and Thorsten Brants. 2000. Wide-
coverage probabilistic sentence processing. Journal
of Psycholinguistic Research, 29(6):647–669.
Delphine Dahan and M. Gareth Gaskell. 2007. The
temporal dynamics of ambiguity resolution: Evi-
dence from spoken-word recognition. Journal of
Memory and Language, 57(4):483–501.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.
Ralf Engbert, Antje Nuthmann, Eike M. Richter, and
Reinhold Kliegl. 2005. SWIFT: A dynamical model
of saccade generation during reading. Psychological
Review, 112:777–813.
Shai Fine, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical hidden markov model: Analysis
and applications. Machine Learning, 32(1):41–62.
Stefan L. Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In Proc. Annual Meeting of the
Cognitive Science Society, pages 1139–1144.
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1–
76.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95–126.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
159–166, Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sentence
Processing. Ph.D. thesis, Cognitive Science, The
Johns Hopkins University.
John Hale. 2006. Uncertainty about the rest of the
sentence. Cognitive Science, 30(4):609–642.
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126–1177.
Scott A. McDonald and Richard C. Shillcock. 2003.
Low-level predictive inference in reading: The influ-
ence of transitional probabilities on eye movements.
Vision Research, 43(16):1735–1751.
George Miller and Noam Chomsky. 1963. Finitary
models of language users. In R. Luce, R. Bush,
and E. Galanter, editors, Handbook of Mathematical
Psychology, volume 2, pages 419–491. John Wiley.
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833–840, Vancouver, BC, Canada.
Joakim Nivre. 2007. Inductive dependency parsing.
Computational Linguistics, 33(2).
Alexander Pollatsek, Erik D. Reichle, and Keith
Rayner. 2006. Tests of the EZ Reader model:
Exploring the interface between cognition and eye-
movement control. Cognitive Psychology, 52(1):1–
56.
Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267–296.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
Proceedings of the 2009 Conference on Empirical
Methods in Natural Langauge Processing, pages
324–333.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
William Schuler, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, pages 785–792,
Manchester, UK, August.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics, 36(1).
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ’09), pages
344–352, Boulder, Colorado.
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. In-
tegration of visual and linguistic information in spo-
ken language comprehension. Science, 268:1632–
1634.
</reference>
<page confidence="0.995396">
1198
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.721176">
<title confidence="0.992391">Complexity Metrics in an Incremental Right-corner Parser</title>
<author confidence="0.992293">Wu Asaf William</author>
<affiliation confidence="0.97630025">Department of Computer Science, University of Minnesota de Neuroimagerie Cognitive INSERM-CEA of Brain &amp; Cognitive Sciences, Massachussetts Institute of Technology of Minnesota and The Ohio State University</affiliation>
<abstract confidence="0.990704818181818">Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also to produce which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM a new metric, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
<author>Mark Johnson</author>
</authors>
<title>Memory requirements and local ambiguities of parsing strategies.</title>
<date>1991</date>
<journal>J. Psycholinguistic Research,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>Abney, Johnson, 1991</marker>
<rawString>Steven P. Abney and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. J. Psycholinguistic Research, 20(3):233–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asaf Bachrach</author>
<author>Brian Roark</author>
<author>Alex Marantz</author>
<author>Susan Whitfield-Gabrieli</author>
<author>Carlos Cardenas</author>
<author>John D E Gabrieli</author>
</authors>
<title>Incremental prediction in naturalistic language processing: An fMRI study.</title>
<date>2009</date>
<contexts>
<context position="23607" citStr="Bachrach et al., 2009" startWordPosition="3833" endWordPosition="3836">inclusion of the embedding depth, d(qt), is the only significant difference between the two metrics. The result is a metric that, despite numerical correspondence to surprisal, models the HHMM’s hypotheses about memory cost. 3 Evaluation Surprisal, entropy reduction, and embedding difference from the HHMM parser were evaluated against a full array of factors (Table 1) on a corpus of word-by-word reading times using a linear mixed-effects model. �µEMB(o1..t) = qt∈Bt P(o1..t q1..t) d(qt) · 1194 The corpus of reading times for 23 native English speakers was collected on a set of four narratives (Bachrach et al., 2009), each composed of sentences that were syntactically complex but constructed to appear relatively natural. Using Linger 2.88, words appeared one-by-one on the screen, and required a button-press in order to advance; they were displayed in lines with 11.5 words on average. Following Roark et al.’s (2009) work on the same corpus, reading times above 1500 ms (for diverted attention) or below 150 ms (for button presses planned before the word appeared) were discarded. In addition, the first and last word of each line on the screen were removed; this left 2926 words out of 3540 words in the corpus.</context>
</contexts>
<marker>Bachrach, Roark, Marantz, Whitfield-Gabrieli, Cardenas, Gabrieli, 2009</marker>
<rawString>Asaf Bachrach, Brian Roark, Alex Marantz, Susan Whitfield-Gabrieli, Carlos Cardenas, and John D.E. Gabrieli. 2009. Incremental prediction in naturalistic language processing: An fMRI study.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Bates</author>
<author>Martin Maechler</author>
<author>Bin Dai</author>
</authors>
<title>lme4: Linear mixed-effects models using S4 classes. R package version 0.999375-31.</title>
<date>2008</date>
<contexts>
<context position="26417" citStr="Bates et al., 2008" startWordPosition="4312" endWordPosition="4315">value and corresponding probability that the smaller model could have produced the same estimates as the larger model. A lower probability indicates that the additional factors in the larger model are significant. Second, models with different fixed effects can be compared to each other through various information criteria; these trade off between having a more explanatory model vs. a simpler model, and can be calculated on any model. Here, we use Akaike’s Information Criterion (AIC), where lower values indicate better models. All these statistics were calculated in R, using the lme4 package (Bates et al., 2008). 4 Results Using the full list of factors in Table 1, fixed-effect coefficients were estimated in Table 2. Fitting the best model by AIC would actually prune away some of the factors as relatively insignificant, but these smaller models largely accord with the significance values in the table and are therefore not presented. The first data column shows the regression on all data; the second and third columns divide the data into open and closed classes, because an evaluation (not reported in detail here) showed statistically significant interactions between word class and 3 of the predictors.</context>
</contexts>
<marker>Bates, Maechler, Dai, 2008</marker>
<rawString>Douglas Bates, Martin Maechler, and Bin Dai. 2008. lme4: Linear mixed-effects models using S4 classes. R package version 0.999375-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marisa Ferrara Boston</author>
<author>John T Hale</author>
<author>Reinhold Kliegl</author>
<author>U Patil</author>
<author>Shravan Vasishth</author>
</authors>
<title>Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus.</title>
<date>2008</date>
<journal>Journal of Eye Movement Research,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1654" citStr="Boston et al., 2008" startWordPosition="224" endWordPosition="227">ts syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution. 1 Introduction Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity. Therefore, the specification of how to en</context>
<context position="10538" citStr="Boston et al., 2008" startWordPosition="1588" endWordPosition="1591"> states. 1Technically, a prior distribution over hidden states, P(q0), is necessary. This q0 is factored and taken to be a deterministic constant, and is therefore unimportant as a probability model. 2Typical tasks in an HMM include finding the most likely sequence via the Viterbi algorithm, which stores these backpointers to maximum-probability previous states and can uniquely find the most likely sequence. This mitigates the problems of large state spaces (e.g., that of all possible grammatical derivations). Since beams have been shown to perform well (Brants and Crocker, 2000; Roark, 2001; Boston et al., 2008b), complexity metrics in this paper are calculated on a beam rather than over all (unbounded) possible derivations Dt. The equations above, then, will replace the assumption q1..t E Dt with qt ELit. 2.2 Hierarchical Hidden Markov Models Hidden states q can have internal structure; in Hierarchical HMMs (Fine et al., 1998; Murphy and Paskin, 2001), this internal structure will be used to represent syntax trees and looks like several HMMs stacked on top of each other. As such, qt is factored into sequences of depth-specific variables — one for each of D levels in the HMM hierarchy. In addition, </context>
</contexts>
<marker>Boston, Hale, Kliegl, Patil, Vasishth, 2008</marker>
<rawString>Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl, U. Patil, and Shravan Vasishth. 2008a. Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus. Journal of Eye Movement Research, 2(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marisa Ferrara Boston</author>
<author>John T Hale</author>
<author>Reinhold Kliegl</author>
<author>Shravan Vasishth</author>
</authors>
<title>Surprising parser actions and reading difficulty.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT, Short Papers,</booktitle>
<pages>5--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1654" citStr="Boston et al., 2008" startWordPosition="224" endWordPosition="227">ts syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution. 1 Introduction Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity. Therefore, the specification of how to en</context>
<context position="10538" citStr="Boston et al., 2008" startWordPosition="1588" endWordPosition="1591"> states. 1Technically, a prior distribution over hidden states, P(q0), is necessary. This q0 is factored and taken to be a deterministic constant, and is therefore unimportant as a probability model. 2Typical tasks in an HMM include finding the most likely sequence via the Viterbi algorithm, which stores these backpointers to maximum-probability previous states and can uniquely find the most likely sequence. This mitigates the problems of large state spaces (e.g., that of all possible grammatical derivations). Since beams have been shown to perform well (Brants and Crocker, 2000; Roark, 2001; Boston et al., 2008b), complexity metrics in this paper are calculated on a beam rather than over all (unbounded) possible derivations Dt. The equations above, then, will replace the assumption q1..t E Dt with qt ELit. 2.2 Hierarchical Hidden Markov Models Hidden states q can have internal structure; in Hierarchical HMMs (Fine et al., 1998; Murphy and Paskin, 2001), this internal structure will be used to represent syntax trees and looks like several HMMs stacked on top of each other. As such, qt is factored into sequences of depth-specific variables — one for each of D levels in the HMM hierarchy. In addition, </context>
</contexts>
<marker>Boston, Hale, Kliegl, Vasishth, 2008</marker>
<rawString>Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl, and Shravan Vasishth. 2008b. Surprising parser actions and reading difficulty. In Proceedings of ACL08: HLT, Short Papers, pages 5–8, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Matthew Crocker</author>
</authors>
<title>Probabilistic parsing and psychological plausibility.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING ’00,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="3299" citStr="Brants and Crocker, 2000" startWordPosition="463" endWordPosition="466">(i.e., from syntax, semantics, discourse) are likely necessary for a full accounting of linguistic complexity, so current computational models (with some exceptions) narrow the scope to syntactic or lexical complexity. Three complexity metrics will be calculated in a Hierarchical Hidden Markov Model (HHMM) parser that recognizes trees in right-corner form (the left-right dual of left-corner form). This type of parser performs competitively on standard parsing tasks (Schuler et al., 2010); also, it reflects plausible accounts of human language processing as incremental (Tanenhaus et al., 1995; Brants and Crocker, 2000), as considering hypotheses probabilistically in parallel (Dahan and Gaskell, 2007), as bounding memory usage to short-term memory limits (Cowan, 2001), and as requiring more memory storage for center-embedding structures than for right- or left-branching ones (Chomsky and Miller, 1963; Gibson, 1998). Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of Abney and John1189 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189–1198, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguis</context>
<context position="10504" citStr="Brants and Crocker, 2000" startWordPosition="1582" endWordPosition="1585"> a beam Lit, discarding low-probability states. 1Technically, a prior distribution over hidden states, P(q0), is necessary. This q0 is factored and taken to be a deterministic constant, and is therefore unimportant as a probability model. 2Typical tasks in an HMM include finding the most likely sequence via the Viterbi algorithm, which stores these backpointers to maximum-probability previous states and can uniquely find the most likely sequence. This mitigates the problems of large state spaces (e.g., that of all possible grammatical derivations). Since beams have been shown to perform well (Brants and Crocker, 2000; Roark, 2001; Boston et al., 2008b), complexity metrics in this paper are calculated on a beam rather than over all (unbounded) possible derivations Dt. The equations above, then, will replace the assumption q1..t E Dt with qt ELit. 2.2 Hierarchical Hidden Markov Models Hidden states q can have internal structure; in Hierarchical HMMs (Fine et al., 1998; Murphy and Paskin, 2001), this internal structure will be used to represent syntax trees and looks like several HMMs stacked on top of each other. As such, qt is factored into sequences of depth-specific variables — one for each of D levels i</context>
</contexts>
<marker>Brants, Crocker, 2000</marker>
<rawString>Thorsten Brants and Matthew Crocker. 2000. Probabilistic parsing and psychological plausibility. In Proceedings of COLING ’00, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Chen</author>
<author>Edward Gibson</author>
<author>Florian Wolf</author>
</authors>
<title>Online syntactic storage costs in sentence comprehension.</title>
<date>2005</date>
<journal>Journal of Memory and Language,</journal>
<volume>52</volume>
<issue>1</issue>
<marker>Chen, Gibson, Wolf, 2005</marker>
<rawString>Evan Chen, Edward Gibson, and Florian Wolf. 2005. Online syntactic storage costs in sentence comprehension. Journal of Memory and Language, 52(1):144–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>George A Miller</author>
</authors>
<title>Introduction to the formal analysis of natural languages.</title>
<date>1963</date>
<booktitle>In Handbook of Mathematical Psychology,</booktitle>
<pages>269--321</pages>
<publisher>Wiley.</publisher>
<contexts>
<context position="3585" citStr="Chomsky and Miller, 1963" startWordPosition="504" endWordPosition="507"> Markov Model (HHMM) parser that recognizes trees in right-corner form (the left-right dual of left-corner form). This type of parser performs competitively on standard parsing tasks (Schuler et al., 2010); also, it reflects plausible accounts of human language processing as incremental (Tanenhaus et al., 1995; Brants and Crocker, 2000), as considering hypotheses probabilistically in parallel (Dahan and Gaskell, 2007), as bounding memory usage to short-term memory limits (Cowan, 2001), and as requiring more memory storage for center-embedding structures than for right- or left-branching ones (Chomsky and Miller, 1963; Gibson, 1998). Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of Abney and John1189 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189–1198, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics son (1991). Typical parsing strategies are arcstandard, keeping all right-descendants open for subsequent attachment; but since there can be an unbounded number of such open constituents, this assumption is not compatible with simple models of bounded memory. A consistently arc-ea</context>
</contexts>
<marker>Chomsky, Miller, 1963</marker>
<rawString>Noam Chomsky and George A. Miller. 1963. Introduction to the formal analysis of natural languages. In Handbook of Mathematical Psychology, pages 269–321. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelson Cowan</author>
</authors>
<title>The magical number 4 in shortterm memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences,</title>
<date>2001</date>
<pages>24--87</pages>
<contexts>
<context position="3450" citStr="Cowan, 2001" startWordPosition="487" endWordPosition="488">) narrow the scope to syntactic or lexical complexity. Three complexity metrics will be calculated in a Hierarchical Hidden Markov Model (HHMM) parser that recognizes trees in right-corner form (the left-right dual of left-corner form). This type of parser performs competitively on standard parsing tasks (Schuler et al., 2010); also, it reflects plausible accounts of human language processing as incremental (Tanenhaus et al., 1995; Brants and Crocker, 2000), as considering hypotheses probabilistically in parallel (Dahan and Gaskell, 2007), as bounding memory usage to short-term memory limits (Cowan, 2001), and as requiring more memory storage for center-embedding structures than for right- or left-branching ones (Chomsky and Miller, 1963; Gibson, 1998). Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of Abney and John1189 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189–1198, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics son (1991). Typical parsing strategies are arcstandard, keeping all right-descendants open for subsequent attachment; but since there can be an un</context>
<context position="6676" citStr="Cowan, 2001" startWordPosition="965" endWordPosition="966">times (and therefore linguistic complexity). The HHMM parser processes syntactic constructs using a bounded number of store states, defined to represent short-term memory elements; additional states are utilized whenever centerembedded syntactic structures are present. Similar models such as Crocker and Brants (2000) implicitly allow an infinite memory size, but Schuler et al. (2008; 2010) showed that a right-corner HHMM parser can parse most sentences in English with 4 or fewer center-embedded-depth levels. This behavior is similar to the hypothesized size of a human short-term memory store (Cowan, 2001). A positive result in predicting reading times will lend additional validity to the claim that the HHMM parser’s bounded memory corresponds to bounded memory in human sentence processing. The rest of this paper is organized as follows: Section 2 defines the language model of the HHMM parser, including definitions of the three complexity metrics. The methodology for evaluating the complexity metrics is described in Section 3, with actual results in Section 4. Further discussion on results, and comparisons to other work, are in Section 5. 2 Parsing Model This section describes an incremental pa</context>
</contexts>
<marker>Cowan, 2001</marker>
<rawString>Nelson Cowan. 2001. The magical number 4 in shortterm memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences, 24:87– 185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Crocker</author>
<author>Thorsten Brants</author>
</authors>
<title>Widecoverage probabilistic sentence processing.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>29</volume>
<issue>6</issue>
<contexts>
<context position="6382" citStr="Crocker and Brants (2000)" startWordPosition="914" endWordPosition="917">during sentence processing increased reading times, and it is widely understood that center-embedding incurs significant syntactic processing costs (Miller and Chomsky, 1963; Gibson, 1998). Thus, we would expect for the usage of the center-embedding memory store in an HHMM parser to correlate with reading times (and therefore linguistic complexity). The HHMM parser processes syntactic constructs using a bounded number of store states, defined to represent short-term memory elements; additional states are utilized whenever centerembedded syntactic structures are present. Similar models such as Crocker and Brants (2000) implicitly allow an infinite memory size, but Schuler et al. (2008; 2010) showed that a right-corner HHMM parser can parse most sentences in English with 4 or fewer center-embedded-depth levels. This behavior is similar to the hypothesized size of a human short-term memory store (Cowan, 2001). A positive result in predicting reading times will lend additional validity to the claim that the HHMM parser’s bounded memory corresponds to bounded memory in human sentence processing. The rest of this paper is organized as follows: Section 2 defines the language model of the HHMM parser, including de</context>
</contexts>
<marker>Crocker, Brants, 2000</marker>
<rawString>Matthew Crocker and Thorsten Brants. 2000. Widecoverage probabilistic sentence processing. Journal of Psycholinguistic Research, 29(6):647–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delphine Dahan</author>
<author>M Gareth Gaskell</author>
</authors>
<title>The temporal dynamics of ambiguity resolution: Evidence from spoken-word recognition.</title>
<date>2007</date>
<journal>Journal of Memory and Language,</journal>
<volume>57</volume>
<issue>4</issue>
<contexts>
<context position="3382" citStr="Dahan and Gaskell, 2007" startWordPosition="474" endWordPosition="477"> of linguistic complexity, so current computational models (with some exceptions) narrow the scope to syntactic or lexical complexity. Three complexity metrics will be calculated in a Hierarchical Hidden Markov Model (HHMM) parser that recognizes trees in right-corner form (the left-right dual of left-corner form). This type of parser performs competitively on standard parsing tasks (Schuler et al., 2010); also, it reflects plausible accounts of human language processing as incremental (Tanenhaus et al., 1995; Brants and Crocker, 2000), as considering hypotheses probabilistically in parallel (Dahan and Gaskell, 2007), as bounding memory usage to short-term memory limits (Cowan, 2001), and as requiring more memory storage for center-embedding structures than for right- or left-branching ones (Chomsky and Miller, 1963; Gibson, 1998). Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of Abney and John1189 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189–1198, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics son (1991). Typical parsing strategies are arcstandard, keeping all right-desc</context>
</contexts>
<marker>Dahan, Gaskell, 2007</marker>
<rawString>Delphine Dahan and M. Gareth Gaskell. 2007. The temporal dynamics of ambiguity resolution: Evidence from spoken-word recognition. Journal of Memory and Language, 57(4):483–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>Data from eyetracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>109</volume>
<issue>2</issue>
<contexts>
<context position="1633" citStr="Demberg and Keller, 2008" startWordPosition="220" endWordPosition="223">MM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution. 1 Introduction Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity. Therefore, the speci</context>
<context position="5194" citStr="Demberg and Keller, 2008" startWordPosition="742" endWordPosition="746">ng that the HHMM parser does, in fact, predict reading times, we will define surprisal and entropy reduction in the HHMM parser, and introduce a third metric called embedding difference. Gibson (1998; 2000) hypothesized two types of syntactic processing costs: integration cost, in which incremental input is combined with existing structures; and memory cost, where unfinished syntactic constructions may incur some short-term memory usage. HHMM surprisal and entropy reduction may be considered forms of integration cost. Though typical PCFG surprisal has been considered a forward-looking metric (Demberg and Keller, 2008), the incremental nature of the right-corner transform causes surprisal and entropy reduction in the HHMM parser to measure the likelihood of grammatical structures that were hypothesized before evidence was observed for them. Therefore, these HHMM metrics resemble an integration cost encompassing both backwardlooking and forward-looking information. On the other hand, embedding difference is designed to model the cost of storing centerembedded structures in working memory. Chen, Gibson, and Wolf (2005) showed that sentences requiring more syntactic memory during sentence processing increased </context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Vera Demberg and Frank Keller. 2008. Data from eyetracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Engbert</author>
<author>Antje Nuthmann</author>
<author>Eike M Richter</author>
<author>Reinhold Kliegl</author>
</authors>
<title>SWIFT: A dynamical model of saccade generation during reading. Psychological Review,</title>
<date>2005</date>
<pages>112--777</pages>
<contexts>
<context position="30988" citStr="Engbert et al., 2005" startWordPosition="5011" endWordPosition="5014">risal. Despite this, it makes an independent contribution to the full-data and closed-class models. Thus, we can conclude that the average embedding depth component affects reading times — i.e., the HHMM’s notion of working memory behaves as we would expect human working memory to behave. 5 Discussion As with previous work on large-scale parserderived complexity metrics, the linear mixedeffect models suggest that sentence-level factors are effective predictors for reading difficulty — in these evaluations, better than commonly-used lexical and near-neighbor predictors (Pollatsek et al., 2006; Engbert et al., 2005). The fact that HHMM surprisal outperforms even n-gram metrics points to the importance of including a notion of sentence structure. This is particularly true when the sentence structure is defined in a language model that is psycholinguistically plausible (here, boundedmemory right-corner form). This accords with an understated result of Boston et al.’s eye-tracking study (2008a): a richer language model predicts eye movements during reading better than an oversimplified one. The comparison there is between phrase structure surprisal (based on Hale’s (2001) calculation from an Earley parser),</context>
</contexts>
<marker>Engbert, Nuthmann, Richter, Kliegl, 2005</marker>
<rawString>Ralf Engbert, Antje Nuthmann, Eike M. Richter, and Reinhold Kliegl. 2005. SWIFT: A dynamical model of saccade generation during reading. Psychological Review, 112:777–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Fine</author>
<author>Yoram Singer</author>
<author>Naftali Tishby</author>
</authors>
<title>The hierarchical hidden markov model: Analysis and applications.</title>
<date>1998</date>
<booktitle>Machine Learning,</booktitle>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="10860" citStr="Fine et al., 1998" startWordPosition="1641" endWordPosition="1644"> to maximum-probability previous states and can uniquely find the most likely sequence. This mitigates the problems of large state spaces (e.g., that of all possible grammatical derivations). Since beams have been shown to perform well (Brants and Crocker, 2000; Roark, 2001; Boston et al., 2008b), complexity metrics in this paper are calculated on a beam rather than over all (unbounded) possible derivations Dt. The equations above, then, will replace the assumption q1..t E Dt with qt ELit. 2.2 Hierarchical Hidden Markov Models Hidden states q can have internal structure; in Hierarchical HMMs (Fine et al., 1998; Murphy and Paskin, 2001), this internal structure will be used to represent syntax trees and looks like several HMMs stacked on top of each other. As such, qt is factored into sequences of depth-specific variables — one for each of D levels in the HMM hierarchy. In addition, an intermediate variable ft is introduced to interface between the levels. def 1 D = (qt ... qt ) (6) def ft = (f1t ... fDt ) (7) Transition probabilities POA(qt |qt–1) over complex hidden states qt are calculated in two phases: • Reduce phase. Yields an intermediate state ft, in which component HMMs may terminate. This </context>
</contexts>
<marker>Fine, Singer, Tishby, 1998</marker>
<rawString>Shai Fine, Yoram Singer, and Naftali Tishby. 1998. The hierarchical hidden markov model: Analysis and applications. Machine Learning, 32(1):41–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan L Frank</author>
</authors>
<title>Surprisal-based comparison between a symbolic and a connectionist model of sentence processing.</title>
<date>2009</date>
<booktitle>In Proc. Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>1139--1144</pages>
<contexts>
<context position="2077" citStr="Frank, 2009" startWordPosition="287" endWordPosition="288">ived a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity. Therefore, the specification of how to encode a syntactic language model is of utmost importance to the quality of the metric. However, it is difficult to quantify linguistic complexity and reading difficulty. The two commonly-used empirical quantifications of reading difficulty are eye-tracking measurements and word-by-word reading times; this paper uses reading times to find the predictiveness of several parser-derived complexity metrics. Various factors (i.</context>
<context position="31679" citStr="Frank (2009)" startWordPosition="5114" endWordPosition="5115">portance of including a notion of sentence structure. This is particularly true when the sentence structure is defined in a language model that is psycholinguistically plausible (here, boundedmemory right-corner form). This accords with an understated result of Boston et al.’s eye-tracking study (2008a): a richer language model predicts eye movements during reading better than an oversimplified one. The comparison there is between phrase structure surprisal (based on Hale’s (2001) calculation from an Earley parser), and dependency grammar surprisal (based on Nivre’s (2007) dependency parser). Frank (2009) similarly reports improvements in the reading-time predictiveness of unlexicalized surprisal when using a language model that is more plausible than PCFGs. The difference in predictivity due to word class is difficult to explain. One theory may be that closed-class words are less susceptible to random effects because there is a finite set of them for any language, making them overall easier to predict via parser-derived metrics. Or, we could note that since closed-class words often serve grammatical functions in addition to their lexical content, they contribute more information to parser-der</context>
</contexts>
<marker>Frank, 2009</marker>
<rawString>Stefan L. Frank. 2009. Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. In Proc. Annual Meeting of the Cognitive Science Society, pages 1139–1144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Linguistic complexity: Locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<volume>68</volume>
<issue>1</issue>
<pages>76</pages>
<contexts>
<context position="3600" citStr="Gibson, 1998" startWordPosition="508" endWordPosition="509">r that recognizes trees in right-corner form (the left-right dual of left-corner form). This type of parser performs competitively on standard parsing tasks (Schuler et al., 2010); also, it reflects plausible accounts of human language processing as incremental (Tanenhaus et al., 1995; Brants and Crocker, 2000), as considering hypotheses probabilistically in parallel (Dahan and Gaskell, 2007), as bounding memory usage to short-term memory limits (Cowan, 2001), and as requiring more memory storage for center-embedding structures than for right- or left-branching ones (Chomsky and Miller, 1963; Gibson, 1998). Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of Abney and John1189 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189–1198, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics son (1991). Typical parsing strategies are arcstandard, keeping all right-descendants open for subsequent attachment; but since there can be an unbounded number of such open constituents, this assumption is not compatible with simple models of bounded memory. A consistently arc-eager strategy ac</context>
<context position="5945" citStr="Gibson, 1998" startWordPosition="850" endWordPosition="851"> of grammatical structures that were hypothesized before evidence was observed for them. Therefore, these HHMM metrics resemble an integration cost encompassing both backwardlooking and forward-looking information. On the other hand, embedding difference is designed to model the cost of storing centerembedded structures in working memory. Chen, Gibson, and Wolf (2005) showed that sentences requiring more syntactic memory during sentence processing increased reading times, and it is widely understood that center-embedding incurs significant syntactic processing costs (Miller and Chomsky, 1963; Gibson, 1998). Thus, we would expect for the usage of the center-embedding memory store in an HHMM parser to correlate with reading times (and therefore linguistic complexity). The HHMM parser processes syntactic constructs using a bounded number of store states, defined to represent short-term memory elements; additional states are utilized whenever centerembedded syntactic structures are present. Similar models such as Crocker and Brants (2000) implicitly allow an infinite memory size, but Schuler et al. (2008; 2010) showed that a right-corner HHMM parser can parse most sentences in English with 4 or few</context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>Edward Gibson. 1998. Linguistic complexity: Locality of syntactic dependencies. Cognition, 68(1):1– 76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>The dependency locality theory: A distance-based theory of linguistic complexity. In Image, language, brain: Papers from the first mind articulation project symposium,</title>
<date>2000</date>
<pages>95--126</pages>
<marker>Gibson, 2000</marker>
<rawString>Edward Gibson. 2000. The dependency locality theory: A distance-based theory of linguistic complexity. In Image, language, brain: Papers from the first mind articulation project symposium, pages 95–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>159--166</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1339" citStr="Hale (2001)" startWordPosition="177" endWordPosition="178">r HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution. 1 Introduction Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). A parser-</context>
<context position="8730" citStr="Hale, 2001" startWordPosition="1294" endWordPosition="1295">e, so one of its by-products is the prefix probability, which will be used to calculate surprisal. This is the probability that that words oi..t have been observed at time t, regardless of which syntactic states qi..t produced them. Bayes’ Law and Markov independence assumptions allow this to 1190 be calculated from two generative probability distributions.1 Here, probabilities arise from a Transition Model (OA) between hidden states and an Observation Model (OB) that generates an observed state from a hidden state. These models are so termed for historical reasons (Rabiner, 1990). Surprisal (Hale, 2001) is then a straightforward calculation from the prefix probability. Pre(o1..t–1) Surprisal(t) = l�92 (3) Pre(o1..t) This framing of prefix probability and surprisal in a time-series model is equivalent to Hale’s (2001; 2006), assuming that q1..t E Dt, i.e., that the syntactic states we are considering form derivations Dt, or partial trees, consistent with the observed words. We will see that this is the case for our parser in Sections 2.2–2.4. Entropy is a measure of uncertainty, defined as H(x) = −P(x)l092 P(x). Now, the entropy Ht of a t-word string o1..t in an HMM can be written: XHt = P(q1</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>John Hale. 2001. A probabilistic earley parser as a psycholinguistic model. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, pages 159–166, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>Grammar, Uncertainty and Sentence Processing.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Cognitive Science, The Johns Hopkins University.</institution>
<contexts>
<context position="9404" citStr="Hale, 2003" startWordPosition="1406" endWordPosition="1407">ty. Pre(o1..t–1) Surprisal(t) = l�92 (3) Pre(o1..t) This framing of prefix probability and surprisal in a time-series model is equivalent to Hale’s (2001; 2006), assuming that q1..t E Dt, i.e., that the syntactic states we are considering form derivations Dt, or partial trees, consistent with the observed words. We will see that this is the case for our parser in Sections 2.2–2.4. Entropy is a measure of uncertainty, defined as H(x) = −P(x)l092 P(x). Now, the entropy Ht of a t-word string o1..t in an HMM can be written: XHt = P(q1..t o1..t) l092 P(q1..t o1..t) (4) q1..t and entropy reduction (Hale, 2003; Hale, 2006) at the tth word is then ER(ot) = max(0, Ht−1 − Ht) (5) Both of these metrics fall out naturally from the time-series representation of the language model. The third complexity metric, embedding difference, will be discussed after additional background in Section 2.5. In the implementation of an HMM, candidate states at a given time qt are kept in a trellis, with step-by-step backpointers to the highestprobability q1..t–1.2 Also, the best qt are often kept in a beam Lit, discarding low-probability states. 1Technically, a prior distribution over hidden states, P(q0), is necessary. </context>
</contexts>
<marker>Hale, 2003</marker>
<rawString>John Hale. 2003. Grammar, Uncertainty and Sentence Processing. Ph.D. thesis, Cognitive Science, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>Uncertainty about the rest of the sentence.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1799" citStr="Hale, 2006" startWordPosition="245" endWordPosition="246">ference makes a significant, independent contribution. 1 Introduction Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity. Therefore, the specification of how to encode a syntactic language model is of utmost importance to the quality of the metric. However, it is difficult to quantify linguistic complexity </context>
<context position="9417" citStr="Hale, 2006" startWordPosition="1408" endWordPosition="1409">t–1) Surprisal(t) = l�92 (3) Pre(o1..t) This framing of prefix probability and surprisal in a time-series model is equivalent to Hale’s (2001; 2006), assuming that q1..t E Dt, i.e., that the syntactic states we are considering form derivations Dt, or partial trees, consistent with the observed words. We will see that this is the case for our parser in Sections 2.2–2.4. Entropy is a measure of uncertainty, defined as H(x) = −P(x)l092 P(x). Now, the entropy Ht of a t-word string o1..t in an HMM can be written: XHt = P(q1..t o1..t) l092 P(q1..t o1..t) (4) q1..t and entropy reduction (Hale, 2003; Hale, 2006) at the tth word is then ER(ot) = max(0, Ht−1 − Ht) (5) Both of these metrics fall out naturally from the time-series representation of the language model. The third complexity metric, embedding difference, will be discussed after additional background in Section 2.5. In the implementation of an HMM, candidate states at a given time qt are kept in a trellis, with step-by-step backpointers to the highestprobability q1..t–1.2 Also, the best qt are often kept in a beam Lit, discarding low-probability states. 1Technically, a prior distribution over hidden states, P(q0), is necessary. This q0 is fa</context>
</contexts>
<marker>Hale, 2006</marker>
<rawString>John Hale. 2006. Uncertainty about the rest of the sentence. Cognitive Science, 30(4):609–642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="1558" citStr="Levy, 2008" startWordPosition="211" endWordPosition="212">fference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution. 1 Introduction Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal t</context>
<context position="22910" citStr="Levy, 2008" startWordPosition="3717" endWordPosition="3718">a difference between previous and current beam states. Most differences between these metrics are relatively inconsequential. For example, the difference in order of subtraction only assures that a positive correlation with reading times is expected. Also, the presence of a logarithm is relatively minor. Embedding difference weighs the probabilities with center-embedding depths and then normalizes the values; since the measure is a weighted average of embedding depths rather than a probability distribution, µEMB is not always less than 1 and the correspondence with KullbackLeibler divergence (Levy, 2008) does not hold, so it does not make sense to take the logs. Therefore, the inclusion of the embedding depth, d(qt), is the only significant difference between the two metrics. The result is a metric that, despite numerical correspondence to surprisal, models the HHMM’s hypotheses about memory cost. 3 Evaluation Surprisal, entropy reduction, and embedding difference from the HHMM parser were evaluated against a full array of factors (Table 1) on a corpus of word-by-word reading times using a linear mixed-effects model. �µEMB(o1..t) = qt∈Bt P(o1..t q1..t) d(qt) · 1194 The corpus of reading times</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Roger Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott A McDonald</author>
<author>Richard C Shillcock</author>
</authors>
<title>Low-level predictive inference in reading: The influence of transitional probabilities on eye movements.</title>
<date>2003</date>
<journal>Vision Research,</journal>
<volume>43</volume>
<issue>16</issue>
<contexts>
<context position="1852" citStr="McDonald and Shillcock, 2003" startWordPosition="249" endWordPosition="253">ndent contribution. 1 Introduction Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity. Therefore, the specification of how to encode a syntactic language model is of utmost importance to the quality of the metric. However, it is difficult to quantify linguistic complexity and reading difficulty. The two commonly-used empiric</context>
</contexts>
<marker>McDonald, Shillcock, 2003</marker>
<rawString>Scott A. McDonald and Richard C. Shillcock. 2003. Low-level predictive inference in reading: The influence of transitional probabilities on eye movements. Vision Research, 43(16):1735–1751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Noam Chomsky</author>
</authors>
<title>Finitary models of language users.</title>
<date>1963</date>
<booktitle>Handbook of Mathematical Psychology,</booktitle>
<volume>2</volume>
<pages>419--491</pages>
<editor>In R. Luce, R. Bush, and E. Galanter, editors,</editor>
<publisher>John Wiley.</publisher>
<contexts>
<context position="5930" citStr="Miller and Chomsky, 1963" startWordPosition="846" endWordPosition="849"> to measure the likelihood of grammatical structures that were hypothesized before evidence was observed for them. Therefore, these HHMM metrics resemble an integration cost encompassing both backwardlooking and forward-looking information. On the other hand, embedding difference is designed to model the cost of storing centerembedded structures in working memory. Chen, Gibson, and Wolf (2005) showed that sentences requiring more syntactic memory during sentence processing increased reading times, and it is widely understood that center-embedding incurs significant syntactic processing costs (Miller and Chomsky, 1963; Gibson, 1998). Thus, we would expect for the usage of the center-embedding memory store in an HHMM parser to correlate with reading times (and therefore linguistic complexity). The HHMM parser processes syntactic constructs using a bounded number of store states, defined to represent short-term memory elements; additional states are utilized whenever centerembedded syntactic structures are present. Similar models such as Crocker and Brants (2000) implicitly allow an infinite memory size, but Schuler et al. (2008; 2010) showed that a right-corner HHMM parser can parse most sentences in Englis</context>
</contexts>
<marker>Miller, Chomsky, 1963</marker>
<rawString>George Miller and Noam Chomsky. 1963. Finitary models of language users. In R. Luce, R. Bush, and E. Galanter, editors, Handbook of Mathematical Psychology, volume 2, pages 419–491. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
<author>Mark A Paskin</author>
</authors>
<title>Linear time inference in hierarchical HMMs.</title>
<date>2001</date>
<booktitle>In Proc. NIPS,</booktitle>
<pages>833--840</pages>
<location>Vancouver, BC,</location>
<contexts>
<context position="10886" citStr="Murphy and Paskin, 2001" startWordPosition="1645" endWordPosition="1648">lity previous states and can uniquely find the most likely sequence. This mitigates the problems of large state spaces (e.g., that of all possible grammatical derivations). Since beams have been shown to perform well (Brants and Crocker, 2000; Roark, 2001; Boston et al., 2008b), complexity metrics in this paper are calculated on a beam rather than over all (unbounded) possible derivations Dt. The equations above, then, will replace the assumption q1..t E Dt with qt ELit. 2.2 Hierarchical Hidden Markov Models Hidden states q can have internal structure; in Hierarchical HMMs (Fine et al., 1998; Murphy and Paskin, 2001), this internal structure will be used to represent syntax trees and looks like several HMMs stacked on top of each other. As such, qt is factored into sequences of depth-specific variables — one for each of D levels in the HMM hierarchy. In addition, an intermediate variable ft is introduced to interface between the levels. def 1 D = (qt ... qt ) (6) def ft = (f1t ... fDt ) (7) Transition probabilities POA(qt |qt–1) over complex hidden states qt are calculated in two phases: • Reduce phase. Yields an intermediate state ft, in which component HMMs may terminate. This ft tells “higher” HMMs to </context>
</contexts>
<marker>Murphy, Paskin, 2001</marker>
<rawString>Kevin P. Murphy and Mark A. Paskin. 2001. Linear time inference in hierarchical HMMs. In Proc. NIPS, pages 833–840, Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive dependency parsing.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Nivre, 2007</marker>
<rawString>Joakim Nivre. 2007. Inductive dependency parsing. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pollatsek</author>
<author>Erik D Reichle</author>
<author>Keith Rayner</author>
</authors>
<title>Tests of the EZ Reader model: Exploring the interface between cognition and eyemovement control.</title>
<date>2006</date>
<journal>Cognitive Psychology,</journal>
<volume>52</volume>
<issue>1</issue>
<pages>56</pages>
<contexts>
<context position="30965" citStr="Pollatsek et al., 2006" startWordPosition="5007" endWordPosition="5010">weighted version of surprisal. Despite this, it makes an independent contribution to the full-data and closed-class models. Thus, we can conclude that the average embedding depth component affects reading times — i.e., the HHMM’s notion of working memory behaves as we would expect human working memory to behave. 5 Discussion As with previous work on large-scale parserderived complexity metrics, the linear mixedeffect models suggest that sentence-level factors are effective predictors for reading difficulty — in these evaluations, better than commonly-used lexical and near-neighbor predictors (Pollatsek et al., 2006; Engbert et al., 2005). The fact that HHMM surprisal outperforms even n-gram metrics points to the importance of including a notion of sentence structure. This is particularly true when the sentence structure is defined in a language model that is psycholinguistically plausible (here, boundedmemory right-corner form). This accords with an understated result of Boston et al.’s eye-tracking study (2008a): a richer language model predicts eye movements during reading better than an oversimplified one. The comparison there is between phrase structure surprisal (based on Hale’s (2001) calculation </context>
</contexts>
<marker>Pollatsek, Reichle, Rayner, 2006</marker>
<rawString>Alexander Pollatsek, Erik D. Reichle, and Keith Rayner. 2006. Tests of the EZ Reader model: Exploring the interface between cognition and eyemovement control. Cognitive Psychology, 52(1):1– 56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1990</date>
<booktitle>Readings in speech recognition,</booktitle>
<pages>53--3</pages>
<contexts>
<context position="8706" citStr="Rabiner, 1990" startWordPosition="1291" endWordPosition="1292">ental, time-series structure, so one of its by-products is the prefix probability, which will be used to calculate surprisal. This is the probability that that words oi..t have been observed at time t, regardless of which syntactic states qi..t produced them. Bayes’ Law and Markov independence assumptions allow this to 1190 be calculated from two generative probability distributions.1 Here, probabilities arise from a Transition Model (OA) between hidden states and an Observation Model (OB) that generates an observed state from a hidden state. These models are so termed for historical reasons (Rabiner, 1990). Surprisal (Hale, 2001) is then a straightforward calculation from the prefix probability. Pre(o1..t–1) Surprisal(t) = l�92 (3) Pre(o1..t) This framing of prefix probability and surprisal in a time-series model is equivalent to Hale’s (2001; 2006), assuming that q1..t E Dt, i.e., that the syntactic states we are considering form derivations Dt, or partial trees, consistent with the observed words. We will see that this is the case for our parser in Sections 2.2–2.4. Entropy is a measure of uncertainty, defined as H(x) = −P(x)l092 P(x). Now, the entropy Ht of a t-word string o1..t in an HMM ca</context>
</contexts>
<marker>Rabiner, 1990</marker>
<rawString>Lawrence R. Rabiner. 1990. A tutorial on hidden Markov models and selected applications in speech recognition. Readings in speech recognition, 53(3):267–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Asaf Bachrach</author>
<author>Carlos Cardenas</author>
<author>Christophe Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Langauge Processing,</booktitle>
<pages>324--333</pages>
<contexts>
<context position="1928" citStr="Roark et al., 2009" startWordPosition="260" endWordPosition="263">n for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity. Therefore, the specification of how to encode a syntactic language model is of utmost importance to the quality of the metric. However, it is difficult to quantify linguistic complexity and reading difficulty. The two commonly-used empirical quantifications of reading difficulty are eye-tracking measurements and w</context>
<context position="27084" citStr="Roark et al. (2009)" startWordPosition="4421" endWordPosition="4424">ble 1, fixed-effect coefficients were estimated in Table 2. Fitting the best model by AIC would actually prune away some of the factors as relatively insignificant, but these smaller models largely accord with the significance values in the table and are therefore not presented. The first data column shows the regression on all data; the second and third columns divide the data into open and closed classes, because an evaluation (not reported in detail here) showed statistically significant interactions between word class and 3 of the predictors. Additionally, this facilitates comparison with Roark et al. (2009), who make the same division. Out of the non-parser-based metrics, word order and bigram probability are statistically significant regardless of the data subset; though reciprocal length and unigram frequency do not reach significance here, likelihood ratio tests (not shown) confirm that they contribute to the model as a whole. It can be seen that nearly all the slopes have been estimated with signs as expected, with the exception of reciprocal length (which is not statistically significant). Most notably, HHMM surprisal is seen here to be a standout predictive measure for reading times regard</context>
<context position="32389" citStr="Roark et al., 2009" startWordPosition="5222" endWordPosition="5225">al when using a language model that is more plausible than PCFGs. The difference in predictivity due to word class is difficult to explain. One theory may be that closed-class words are less susceptible to random effects because there is a finite set of them for any language, making them overall easier to predict via parser-derived metrics. Or, we could note that since closed-class words often serve grammatical functions in addition to their lexical content, they contribute more information to parser-derived measures than open-class words. Previous work with complexity metrics on this corpus (Roark et al., 2009) suggests that these explanations only account for part of the word-class variation in the performance of predictors. 1196 Further comparsion to Roark et al. will show other differences, such as the lesser role of word length and unigram frequency, lower overall correlations between factors, and the greater predictivity of their entropy metric. In addition, their metrics are different from ours in that they are designed to tease apart lexical and syntactic contributions to reading difficulty. Their notion of entropy, in particular, estimates Hale’s definition of entropy on whole derivations (2</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Brian Roark, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. Proceedings of the 2009 Conference on Empirical Methods in Natural Langauge Processing, pages 324–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="10517" citStr="Roark, 2001" startWordPosition="1586" endWordPosition="1587">w-probability states. 1Technically, a prior distribution over hidden states, P(q0), is necessary. This q0 is factored and taken to be a deterministic constant, and is therefore unimportant as a probability model. 2Typical tasks in an HMM include finding the most likely sequence via the Viterbi algorithm, which stores these backpointers to maximum-probability previous states and can uniquely find the most likely sequence. This mitigates the problems of large state spaces (e.g., that of all possible grammatical derivations). Since beams have been shown to perform well (Brants and Crocker, 2000; Roark, 2001; Boston et al., 2008b), complexity metrics in this paper are calculated on a beam rather than over all (unbounded) possible derivations Dt. The equations above, then, will replace the assumption q1..t E Dt with qt ELit. 2.2 Hierarchical Hidden Markov Models Hidden states q can have internal structure; in Hierarchical HMMs (Fine et al., 1998; Murphy and Paskin, 2001), this internal structure will be used to represent syntax trees and looks like several HMMs stacked on top of each other. As such, qt is factored into sequences of depth-specific variables — one for each of D levels in the HMM hie</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>Toward a psycholinguistically-motivated model of language.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>785--792</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="6449" citStr="Schuler et al. (2008" startWordPosition="926" endWordPosition="929">erstood that center-embedding incurs significant syntactic processing costs (Miller and Chomsky, 1963; Gibson, 1998). Thus, we would expect for the usage of the center-embedding memory store in an HHMM parser to correlate with reading times (and therefore linguistic complexity). The HHMM parser processes syntactic constructs using a bounded number of store states, defined to represent short-term memory elements; additional states are utilized whenever centerembedded syntactic structures are present. Similar models such as Crocker and Brants (2000) implicitly allow an infinite memory size, but Schuler et al. (2008; 2010) showed that a right-corner HHMM parser can parse most sentences in English with 4 or fewer center-embedded-depth levels. This behavior is similar to the hypothesized size of a human short-term memory store (Cowan, 2001). A positive result in predicting reading times will lend additional validity to the claim that the HHMM parser’s bounded memory corresponds to bounded memory in human sentence processing. The rest of this paper is organized as follows: Section 2 defines the language model of the HHMM parser, including definitions of the three complexity metrics. The methodology for eval</context>
<context position="13746" citStr="Schuler, et al. (2008)" startWordPosition="2183" endWordPosition="2186">probabilistic dependencies. (b) considers the qd store to be incremental syntactic information. (c)–(d) demonstrate the right-corner transform, similar to a left-to-right traversal of (c). In ‘NP/NN’ we say that NP is the active constituent and NN is the awaited. The Observation Model OB is comparatively much simpler. It is only dependent on the syntactic state at D (or the deepest active HHMM level). POB(ot I qt) def = P(ot I qD ) (10) Figure 1(a) gives a schematic of the dependency structure of Equations 8–10 for D = 3. Evaluations in this paper are done with D = 4, following the results of Schuler, et al. (2008). 2.3 Parsing right-corner trees In this HHMM formulation, states and dependencies are optimized for parsing right-corner trees (Schuler et al., 2008; Schuler et al., 2010). A sample transformation between CNF and right-corner trees is in Figures 1(c)–1(d). Figure 1(b) shows the corresponding storeelement interpretation3 of the right corner tree in 1(d). These can be used as a case study to see what kind of operations need to occur in an 3This is technically a pushdown automoton (PDA), where the store is limited to D elements. When referring to directions (e.g., up, down), PDAs are typically d</context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2008</marker>
<rawString>William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2008. Toward a psycholinguistically-motivated model of language. In Proceedings of COLING, pages 785–792, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>Broad-coverage incremental parsing using human-like memory constraints.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="3166" citStr="Schuler et al., 2010" startWordPosition="444" endWordPosition="447">ng times; this paper uses reading times to find the predictiveness of several parser-derived complexity metrics. Various factors (i.e., from syntax, semantics, discourse) are likely necessary for a full accounting of linguistic complexity, so current computational models (with some exceptions) narrow the scope to syntactic or lexical complexity. Three complexity metrics will be calculated in a Hierarchical Hidden Markov Model (HHMM) parser that recognizes trees in right-corner form (the left-right dual of left-corner form). This type of parser performs competitively on standard parsing tasks (Schuler et al., 2010); also, it reflects plausible accounts of human language processing as incremental (Tanenhaus et al., 1995; Brants and Crocker, 2000), as considering hypotheses probabilistically in parallel (Dahan and Gaskell, 2007), as bounding memory usage to short-term memory limits (Cowan, 2001), and as requiring more memory storage for center-embedding structures than for right- or left-branching ones (Chomsky and Miller, 1963; Gibson, 1998). Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of Abney and John1189 Proceedings of the 48th Annual Meeting of the Assoc</context>
<context position="13918" citStr="Schuler et al., 2010" startWordPosition="2209" endWordPosition="2212">aversal of (c). In ‘NP/NN’ we say that NP is the active constituent and NN is the awaited. The Observation Model OB is comparatively much simpler. It is only dependent on the syntactic state at D (or the deepest active HHMM level). POB(ot I qt) def = P(ot I qD ) (10) Figure 1(a) gives a schematic of the dependency structure of Equations 8–10 for D = 3. Evaluations in this paper are done with D = 4, following the results of Schuler, et al. (2008). 2.3 Parsing right-corner trees In this HHMM formulation, states and dependencies are optimized for parsing right-corner trees (Schuler et al., 2008; Schuler et al., 2010). A sample transformation between CNF and right-corner trees is in Figures 1(c)–1(d). Figure 1(b) shows the corresponding storeelement interpretation3 of the right corner tree in 1(d). These can be used as a case study to see what kind of operations need to occur in an 3This is technically a pushdown automoton (PDA), where the store is limited to D elements. When referring to directions (e.g., up, down), PDAs are typically described opposite of the one in Figure 1(b); here, we push “up” instead of down. HHMM when parsing right-corner trees. There is one unique set of HHMM state values for each</context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2010</marker>
<rawString>William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2010. Broad-coverage incremental parsing using human-like memory constraints. Computational Linguistics, 36(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
</authors>
<title>Parsing with a bounded stack using a model-based right-corner transform.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics (NAACL ’09),</booktitle>
<pages>344--352</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="7670" citStr="Schuler, 2009" startWordPosition="1126" endWordPosition="1127">the complexity metrics is described in Section 3, with actual results in Section 4. Further discussion on results, and comparisons to other work, are in Section 5. 2 Parsing Model This section describes an incremental parser in which surprisal and entropy reduction are simple calculations (Section 2.1). The parser uses a Hierarchical Hidden Markov Model (Section 2.2) and recognizes trees in a right-corner form (Section 2.3 and 2.4). The new complexity metric, embedding difference (Section 2.5), is a natural consequence of this HHMM definition. The model is equivalent to previous HHMM parsers (Schuler, 2009), but reorganized into 5 cases to clarify the right-corner structure of the parsed sentences. 2.1 Surprisal and Entropy in HMMs Hidden Markov Models (HMMs) probabilistically connect sequences of observed states ot and hidden states qt at corresponding time steps t. In parsing, observed states are words; hidden states can be a conglomerate state of linguistic information, here taken to be syntactic. The HMM is an incremental, time-series structure, so one of its by-products is the prefix probability, which will be used to calculate surprisal. This is the probability that that words oi..t have b</context>
<context position="21250" citStr="Schuler, 2009" startWordPosition="3465" endWordPosition="3466">e value” of a word in the HHMM parser; models were trained on the Wall Street positive Journal, sections 02–21. More surprising words may take longer to read. slope Table 1: A list of factors hypothesized to contribute to reading times. All data was mean-centered. A final note: the notation �Pp(· |·) has been used to indicate probability models that are empirical, trained directly from frequency counts of rightcorner transformed trees in a large corpus. Alternatively, a standard PCFG could be trained on a corpus (or hand-specified), and then the grammar itself can be right-corner transformed (Schuler, 2009). Taken together, Equations 11–14 define the probabilistic structure of the HHMM for parsing right-corner trees. 2.5 Embedding difference in the HHMM It should be clear from Figure 1 that at any time step while parsing depth-bounded right-corner trees, the candidate hidden state qt will have a “frontier” depth d(qt). At time t, the beam of possible hidden states qt stores the syntactic state (and a backpointer) along with its probability, P(o1..t q1..t). The average embedding depth at a time step is then Eq′t∈Bt P(o1..t q′1..t) where we have directly used the beam notation. The embedding diffe</context>
</contexts>
<marker>Schuler, 2009</marker>
<rawString>William Schuler. 2009. Parsing with a bounded stack using a model-based right-corner transform. In Proceedings of the North American Association for Computational Linguistics (NAACL ’09), pages 344–352, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J Spivey-Knowlton</author>
<author>Kathy M Eberhard</author>
<author>Julie E Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<volume>268</volume>
<pages>1634</pages>
<contexts>
<context position="3272" citStr="Tanenhaus et al., 1995" startWordPosition="459" endWordPosition="462">etrics. Various factors (i.e., from syntax, semantics, discourse) are likely necessary for a full accounting of linguistic complexity, so current computational models (with some exceptions) narrow the scope to syntactic or lexical complexity. Three complexity metrics will be calculated in a Hierarchical Hidden Markov Model (HHMM) parser that recognizes trees in right-corner form (the left-right dual of left-corner form). This type of parser performs competitively on standard parsing tasks (Schuler et al., 2010); also, it reflects plausible accounts of human language processing as incremental (Tanenhaus et al., 1995; Brants and Crocker, 2000), as considering hypotheses probabilistically in parallel (Dahan and Gaskell, 2007), as bounding memory usage to short-term memory limits (Cowan, 2001), and as requiring more memory storage for center-embedding structures than for right- or left-branching ones (Chomsky and Miller, 1963; Gibson, 1998). Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of Abney and John1189 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189–1198, Uppsala, Sweden, 11-16 July 2010. c�2010 Associatio</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Michael K. Tanenhaus, Michael J. Spivey-Knowlton, Kathy M. Eberhard, and Julie E. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632– 1634.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>