<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.979059">
Online Learning for Interactive Statistical Machine Translation
</title>
<author confidence="0.692542">
Daniel Ortiz-Martinez
</author>
<note confidence="0.861037153846154">
Dpto. de Sist. Inf. y Comp.
Univ. Polit´ec. de Valencia
46071 Valencia, Spain
dortiz@dsic.upv.es
Ismael Garcia-Varea
Dpto. de Informitica
Univ. de Castilla-La Mancha
02071 Albacete, Spain
ivarea@info-ab.uclm.es
Francisco Casacuberta
Dpto. de Sist. Inf. y Comp.
Univ. Polit´ec. de Valencia
46071 Valencia, Spain
</note>
<email confidence="0.920145">
fcn@dsic.upv.es
</email>
<sectionHeader confidence="0.993159" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99750168">
State-of-the-art Machine Translation (MT)
systems are still far from being perfect. An al-
ternative is the so-called Interactive Machine
Translation (IMT) framework. In this frame-
work, the knowledge of a human translator is
combined with a MT system. The vast ma-
jority of the existing work on IMT makes use
of the well-known batch learning paradigm.
In the batch learning paradigm, the training of
the IMT system and the interactive translation
process are carried out in separate stages. This
paradigm is not able to take advantage of the
new knowledge produced by the user of the
IMT system. In this paper, we present an ap-
plication of the online learning paradigm to
the IMT framework. In the online learning
paradigm, the training and prediction stages
are no longer separated. This feature is par-
ticularly useful in IMT since it allows the user
feedback to be taken into account. The online
learning techniques proposed here incremen-
tally update the statistical models involved in
the translation process. Empirical results show
the great potential of online learning in the
IMT framework.
</bodyText>
<sectionHeader confidence="0.998875" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.965282">
Information technology advances have led to the
need for more efficient translation methods. Current
MT systems are not able to produce ready-to-use
texts. Indeed, MT systems usually require human
post-editing to achieve high-quality translations.
One way of taking advantage of MT systems is to
combine them with the knowledge of a human trans-
lator in the IMT paradigm, which is a special type of
the computer-assisted translation paradigm (Isabelle
and Church, 1997). An important contribution to
IMT technology was pioneered by the TransType
project (Foster et al., 1997; Langlais et al., 2002)
where data driven MT techniques were adapted for
their use in an interactive translation environment.
Following the TransType ideas, Barrachina et
al. (2009) proposed a new approach to IMT, in which
fully-fledged statistical MT (SMT) systems are used
to produce full target sentence hypotheses, or por-
tions thereof, which can be partially or completely
accepted and amended by a human translator. Each
partial, correct text segment is then used by the
SMT system as additional information to achieve
improved suggestions. Figure 1 illustrates a typical
IMT session.
source(f): Para ver la lista de recursos
reference(6): To view a listing of resources
</bodyText>
<figure confidence="0.956432733333333">
inter.-0 ep To view the resources list
es
inter.-1 ep To view list of resources
k
es
a
inter.-2 ep To view a list resources
k i
es ng
inter.-3 ep To view a listing resources
k
es
o
f
accept ep To view a listing of resources
</figure>
<figureCaption confidence="0.997964">
Figure 1: IMT session to translate a Spanish sen-
</figureCaption>
<bodyText confidence="0.9521606">
tence into English. In interaction-0, the system sug-
gests a translation (es). In interaction-1, the user
moves the mouse to accept the first eight characters
”To view ” and presses the a key (k), then the sys-
tem suggests completing the sentence with ”list of
resources” (a new es). Interactions 2 and 3 are sim-
ilar. In the final interaction, the user accepts the cur-
rent suggestion.
In this paper, we also focus on the IMT frame-
work. Specifically, we present an IMT system that is
able to learn from user feedback. For this purpose,
we apply the online learning paradigm to the IMT
framework. The online learning techniques that we
propose here allow the statistical models involved in
the translation process to be incrementally updated.
</bodyText>
<page confidence="0.974774">
546
</page>
<note confidence="0.7941305">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 546–554,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.961217916666667">
Figure 2 (inspired from (Vidal et al., 2007)) shows
a schematic view of these ideas. Here, f is the in-
put sentence and e is the output derived by the IMT
system from f. By observing f and e, the user inter-
acts with the IMT system until the desired output e� is
produced. The input sentence f and its desired trans-
lation e� can be used to refine the models used by the
system. In general, the model is initially obtained
through a classical batch training process from a pre-
viously given training sequence of pairs (fi,ei) from
the task being considered. Now, the models can be
extended with the use of valuable user feedback.
</bodyText>
<sectionHeader confidence="0.961519" genericHeader="method">
2 Interactive machine translation
</sectionHeader>
<bodyText confidence="0.9379412">
IMT can be seen as an evolution of the SMT frame-
work. Given a sentence f from a source lan-
guage F to be translated into a target sentence e
of a target language £, the fundamental equation of
SMT (Brown et al., 1993) is the following:
</bodyText>
<equation confidence="0.9922145">
e� = argmax {Pr(e  |f)} (1)
e
= argmax {Pr(f  |e) Pr(e)} (2)
e
</equation>
<bodyText confidence="0.9995262">
where Pr(f  |e) is approximated by a translation
model that represents the correlation between the
source and the target sentence and where Pr(e) is
approximated by a language model representing the
well-formedness of the candidate translation e.
State-of-the-art statistical machine translation
systems follow a loglinear approach (Och and Ney,
2002), where direct modelling of the posterior prob-
ability Pr(e  |f) of Equation (1) is used. In this case,
the decision rule is given by the expression:
</bodyText>
<equation confidence="0.99738">
� M �
λmhm(e, f) (3)
m=1
</equation>
<bodyText confidence="0.999901333333333">
where each hm(e, f) is a feature function represent-
ing a statistical model and λm its weight.
Current MT systems are based on the use of
phrase-based models (Koehn et al., 2003) as transla-
tion models. The basic idea of Phrase-based Trans-
lation (PBT) is to segment the source sentence into
phrases, then to translate each source phrase into a
target phrase, and finally to reorder the translated
target phrases in order to compose the target sen-
tence. If we summarize all the decisions made dur-
ing the phrase-based translation process by means of
the hidden variable iiK1 , we obtain the expression:
</bodyText>
<equation confidence="0.9970955">
Pr(f|e) = � Pr( �fK1 ,6K1  |�eK1 ) (4)
K,˜aK
</equation>
<bodyText confidence="0.999787333333333">
where each ak E {1 ... K} denotes the index of the
target phrase e� that is aligned with the k-th source
phrase fk, assuming a segmentation of length K.
According to Equation (4), and following a max-
imum approximation, the problem stated in Equa-
tion (2) can be reframed as:
</bodyText>
<equation confidence="0.8816605">
e� Pz arg max {p(e) · p(f, a  |e)} (5)
e,a
</equation>
<bodyText confidence="0.995270666666667">
In the IMT scenario, we have to find an extension
es for a given prefix ep. To do this we reformulate
Equation (5) as follows:
</bodyText>
<equation confidence="0.904224">
es Pz arg max {p(es  |ep) · p(f, a  |ep, es)} (6)
e3,a
</equation>
<bodyText confidence="0.999324">
where the term p(ep) has been dropped since it does
depend neither on es nor on a.
Thus, the search is restricted to those sentences e
which contain ep as prefix. It is also worth mention-
ing that the similarities between Equation (6) and
Equation (5) (note that epes - e) allow us to use
the same models whenever the search procedures are
adequately modified (Barrachina et al., 2009).
Following the loglinear approach stated in Equa-
tion (3), Equation (6) can be rewriten as:
</bodyText>
<equation confidence="0.974433333333333">
� M
es = argmax 1: λmhm(e, a, f) (7)
e,,a m=1
</equation>
<figure confidence="0.966728526315789">
f
e
feedback/interactions
Interactive
SMT System
f , e
1 1
f , e
2 2
. . .
f
^e
Batch
Learning
Incremental
Models
Online
Learning
f
</figure>
<figureCaption confidence="0.838652">
Figure 2: An Online Interactive SMT system
</figureCaption>
<equation confidence="0.8028642">
k
^
e
e� = argmax
e
</equation>
<page confidence="0.983517">
547
</page>
<bodyText confidence="0.9998725">
which is the approach that we follow in this work.
A common problem in IMT arises when the user
sets a prefix (ep) which cannot be found in the
phrase-based statistical translation model. Differ-
ent solutions have been proposed to deal with this
problem. The use of word translation graphs, as a
compact representation of all possible translations
of a source sentence, is proposed in (Barrachina
et al., 2009). In (Ortiz-Martinez et al., 2009), a
technique based on the generation of partial phrase-
based alignments is described. This last proposal has
also been adopted in this work.
straints. What is more, our system is able to learn
from scratch, that is, without any preexisting model
stored in the system. This is demonstrated empiri-
cally in section 5.
</bodyText>
<sectionHeader confidence="0.998139" genericHeader="method">
4 Online IMT
</sectionHeader>
<bodyText confidence="0.9999758">
In this section we propose an online IMT system.
First, we describe the basic IMT system involved
in the interactive translation process. Then we in-
troduce the required techniques to incrementally up-
date the statistical models used by the system.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.999797393939394">
In this paper we present an application of the online
learning paradigm to the IMT framework. In the on-
line learning setting, models are trained sample by
sample. Our work is also related to model adapta-
tion, although model adaptation and online learning
are not exactly the same thing.
The online learning paradigm has been previ-
ously applied to train discriminative models in
SMT (Liang et al., 2006; Arun and Koehn, 2007;
Watanabe et al., 2007; Chiang et al., 2008). These
works differ from the one presented here in that we
apply online learning techniques to train generative
models instead of discriminative models.
In (Nepveu et al., 2004), dynamic adaptation of
an IMT system via cache-based model extensions to
language and translation models is proposed. The
work by Nepveu et al. (2004) constitutes a domain
adaptation technique and not an online learning
technique, since the proposed cache components re-
quire pre-existent models estimated in batch mode.
In addition to this, their IMT system does not use
state-of-the-art models.
To our knowledge, the only previous work on on-
line learning for IMT is (Cesa-Bianchi et al., 2008),
where a very constrained version of online learn-
ing is presented. This constrained version of online
learning is not able to extend the translation models
due to technical problems with the efficiency of the
learning process. In this paper, we present a purely
statistical IMT system which is able to incrementally
update the parameters of all of the different models
that are used in the system, including the transla-
tion model, breaking with the above mentioned con-
</bodyText>
<equation confidence="0.9796435">
i−n+1) − Dn, 0}
cX(ep(ei|ei−1
i−n+1) = max{cX(ei +
i−1
i−n+1)
Dn N1+(ei−1
i−n+1•) · p(ei|ei−1
i−n+2) (8)
cX(ei−1
i−n+1)
</equation>
<bodyText confidence="0.916112842105263">
where Dn = cn,1 is a fixed discount (cn,1
cn,1+2cn,2
and cn,2 are the number of n-grams with one
and two counts respectively), N1+(ei−1
i−n+1•) is the
number of unique words that follows the history
ei−1
i−n+1 and cX(eii−n+1) is the count of the n-gram
ei i−n+1, where cX(·) can represent true counts
cT(·) or modified counts cm(·). True counts are
used for the higher order n-grams and modified
counts for the lower order n-grams. Given a cer-
tain n-gram, its modified count consists in the
number of different words that precede this n-
gram in the training corpus.
Equation (8) corresponds to the probability given
by an n-gram language model with an interpolated
version of the Kneser-Ney smoothing (Chen and
Goodman, 1996).
</bodyText>
<subsectionHeader confidence="0.985456">
4.1 Basic IMT system
</subsectionHeader>
<bodyText confidence="0.996673">
The basic IMT system that we propose uses a log-
linear model to generate its translations. According
to Equation (7), we introduce a set of seven feature
functions (from h1 to h7):
</bodyText>
<listItem confidence="0.730403">
• n-gram language model (h1)
</listItem>
<equation confidence="0.9816196">
h1(e) = log(�|�|+1
i=1 p(ei|ei−1
i−n+1)), 1 where
p(ei|ei−1
i−n+1) is defined as follows:
</equation>
<bodyText confidence="0.6932485">
1|e |is the length of e, e0 denotes the begin-of-sentence sym-
bol, eIe +1 denotes the end-of-sentence symbol, eji - ei...ej
</bodyText>
<page confidence="0.869931">
548
</page>
<listItem confidence="0.732863">
• target sentence-length model (h2)
</listItem>
<bodyText confidence="0.9624965">
h2(e, f) = log(p(|f   e|)) = log(φ|e|(|f|+0.5)−
φ|e|(|f |− 0.5)), where φ|e|(·) denotes the cumula-
tive distribution function (cdf) for the normal dis-
tribution (the cdf is used here to integrate the nor-
mal density function over an interval of length 1).
We use a specific normal distribution with mean
µ|e |and standard deviation σ|e |for each possible
target sentence length |e|.
</bodyText>
<listItem confidence="0.952400666666667">
• inverse and direct phrase-based models (h3, h4)
h3(e, a, f) = log(HKk=1 p(�fk|�eak)), where
�fk|eak) is defined as follows:
</listItem>
<equation confidence="0.9998925">
p(�fk|�eak) = β · pphr(�fk|�eak) +
(1 − β).phmm(�fk|�eak) (9)
</equation>
<bodyText confidence="0.999790666666667">
In Equation (9), pphr(�fk|�eak) denotes the proba-
bility given by a statistical phrase-based dictionary
used in regular phrase-based models (see (Koehn
et al., 2003) for more details). phmm( �fk|eak) is
the probability given by an HMM-based (intra-
phrase) alignment model (see (Vogel et al., 1996)):
</bodyText>
<equation confidence="0.859638">
p(�fj|�ea;) · p(aj|aj−1, |�e|)
(10)
</equation>
<bodyText confidence="0.8095116">
The HMM-based alignment model probability is
used here for smoothing purposes as described
in (Ortiz-Martinez et al., 2009).
Analogously h4 is defined as:
h4(e, a, f) = log(HKk=1 p(�eak |A))
</bodyText>
<listItem confidence="0.8958605">
• target phrase-length model (h5)
h5(e, a, f) = log(HKk=1 p(|4|)), where p(|4|) =
δ(1 − δ)|6k|. h5 implements a target phrase-length
model by means of a geometric distribution with
probability of success on each trial δ. The use of a
geometric distribution penalizes the length of tar-
get phrases.
• source phrase-length model (h6)
</listItem>
<equation confidence="0.468639">
h6(e, a, f) = log(HKk=1 p(|�fk   �eak|)),
</equation>
<bodyText confidence="0.9615636">
where p( |A   eak|) = δ(1 − δ)abs( |!k|−|e˜�k|) and
abs(·) is the absolute value function. A geometric
distribution is used to model this feature (it penal-
izes the difference between the source and target
phrase lengths).
</bodyText>
<listItem confidence="0.881091">
• distortion model (h7)
</listItem>
<equation confidence="0.9964225">
h7(a) = log(HKk=1 p(�ak|�ak−1)), where
p(iik|�ak−1) = δ(1 − δ)abs(b˜�k−l˜�k−1), bak
</equation>
<bodyText confidence="0.999358">
denotes the beginning position of the source
phrase covered by ak and l4−1 denotes the last
position of the source phrase covered by 4−1.
A geometric distribution is used to model this
feature (it penalizes the reorderings).
The log-linear model, which includes the above
described feature functions, is used to generate the
suffix es given the user-validated prefix ep. Specif-
ically, the IMT system generates a partial phrase-
based alignment between the user prefix ep and a
portion of the source sentence f, and returns the suf-
fix es as the translation of the remaining portion of
f (see (Ortiz-Martinez et al., 2009)).
</bodyText>
<subsectionHeader confidence="0.4300725">
4.2 Extending the IMT system from user
feedback
</subsectionHeader>
<bodyText confidence="0.999883607142857">
After translating a source sentence f, a new sen-
tence pair (f, e) is available to feed the IMT system
(see Figure 1). In this section we describe how the
log-linear model described in section 4.1 is updated
given the new sentence pair. To do this, a set of suf-
ficient statistics that can be incrementally updated is
maintained for each feature function hi(·). A suffi-
cient statistic for a statistical model is a statistic that
captures all the information that is relevant to esti-
mate this model.
Regarding feature function h1 and according to
equation (8), we need to maintain the following data:
ck,1 and ck,2 given any order k, N1+(·), and cX(·)
(see section 4.1 for the meaning of each symbol).
Given a new sentence e, and for each k-gram eii−k+1
of e where 1 ≤ k ≤ n and 1 ≤ i ≤ |e|+1, we mod-
ify the set of sufficient statistics as it is shown in Al-
gorithm 1. The algorithm checks the changes in the
counts of the k-grams to update the set of sufficient
statistics. Sufficient statistics for Dk are updated fol-
lowing the auxiliar procedure shown in Algorithm 2.
Feature function h2 requires the incremental cal-
culation of the mean µ|e |and the standard deviation
σ|e |of the normal distribution associated to a target
sentence length |e|. For this purpose the procedure
described in (Knuth, 1981) can be used. In this pro-
cedure, two quantities are maintained for each nor-
mal distribution: µ|e |and S|e|. Given anew sentence
</bodyText>
<figure confidence="0.9885266">
p(
phmm( f |E) |�f|
= ǫ� H
a |˜f |j=1
1
</figure>
<page confidence="0.94254">
549
</page>
<construct confidence="0.477052666666667">
Algorithm 1: Pseudocode for updating the suf-
ficient statistics of a given k-gram
Algorithm 2: Pseudocode for the updD proce-
dure
pair (f, e), the two quantities are updated using a re-
currence relation:
</construct>
<equation confidence="0.999666">
µ|e |= µ ′ ′
|e |+ (|f |− µ|e|)/c(|e|) (11)
S|e |= S′|e |+ (|f |− µ′|e|)(|f |− µ|e|) (12)
</equation>
<bodyText confidence="0.998447333333333">
where c(|e|) is the count of the number of sentences
of length |e |that have been seen so far, and µ′|e |and
S′|e |are the quantities previously stored (µ|e |is ini-
tialized to the source sentence length of the first sam-
ple and S|e |is initialized to zero). Finally, the stan-
dard deviation can be obtained from S as follows:
</bodyText>
<equation confidence="0.9855515">
,
σ|e |= S|e|/(c(|e|) − 1).
</equation>
<bodyText confidence="0.998857">
Feature functions h3 and h4 implement inverse
and direct smoothed phrase-based models respec-
tively. Since phrase-based models are symmetric
models, only an inverse phrase-based model is main-
tained (direct probabilities can be efficiently ob-
tained using appropriate data structures, see (Ortiz-
Martinez et al., 2008)). The inverse phrase model
probabilities are estimated from the phrase counts:
</bodyText>
<equation confidence="0.991823">
pU�f|�e) = c(
</equation>
<bodyText confidence="0.99993796969697">
According to Equation (13), the set of suffi-
cient statistics to be stored for the inverse phrase
model consists of a set of phrase counts (c(�f, e) and
E f′ c(�f′, e) must be stored separately). Given a
new sentence pair (f, e), the standard phrase-based
model estimation method uses a word alignment ma-
trix between f and e to extract the set of phrase pairs
that are consistent with the word alignment ma-
trix (see (Koehn et al., 2003) for more details). Once
the consistent phrase pairs have been extracted, the
phrase counts are updated. The word alignment ma-
trices required for the extraction of phrase pairs are
generated by means of the HMM-based models used
in the feature functions h3 and h4.
Inverse and direct HMM-based models are used
here for two purposes: to smooth the phrase-based
models via linear interpolation and to generate word
alignment matrices. The weights of the interpola-
tion can be estimated from a development corpus.
Equation (10) shows the expression of the probabil-
ity given by an inverse HMM-based model. The
probability includes lexical probabilities p(fj|ez)
and alignment probabilities p(aj|aj_1, l). Since the
alignment in the HMM-based model is determined
by a hidden variable, the EM algorithm is required
to estimate the parameters of the model (see (Och
and Ney, 2003)). However, the standard EM algo-
rithm is not appropriate to incrementally extend our
HMM-based models because it is designed to work
in batch training scenarios. To solve this problem,
we apply the incremental view of the EM algorithm
described in (Neal and Hinton, 1998). According
to (Och and Ney, 2003), the lexical probability for a
</bodyText>
<equation confidence="0.836504888888889">
input : n (higher order), ei−k+1 (k-gram),
S = {dj(cj,1, cj,2), N1+(&apos;), cX(&apos;)}
(current set of sufficient statistics)
output : S (updated set of sufficient statistics)
begin
if cT(ei−k+1) = 0 then
if k − 1 &gt; 1 then
updD(S,k-1,cM(e�−1
�−k+2),cM(e�−1
�−k+2)+1)
if cM(ei−k−+2) = 0 then
N1+(ei−k+2) = N1+(e�−1
�−k+2) + 1
cM(e�−1
�−k+2) = cM(e�−1
�−k+2) + 1
cM(ei−k+2) = cM(ei−k+2) + 1
if k = n then
N1+(e�−1
�−k+1) = N1+(e�−1
�−k+1) + 1
if k = n then
updD(S,k,cT(e��−k+1),cT (ei−k+1) + 1)
cT(e�−1
�−k+1)=cT(e�−1
�−k+1) + 1
cT(ei−k+1)=cT(ei−k+1) + 1
end
input : S (current set of sufficient statistics),k
(order), c (current count), c′ (new count)
output : (ck,1, ck,2) (updated sufficient statistics)
begin
if c = 0 then
if c′ = 1 then ck,1 = ck,1 + 1
if c′ = 2 then ck,2 = ck,2 + 1
if c = 1 then
ck,1 = ck,1 − 1
if c′ = 2 then ck,2 = ck,2 + 1
if c = 2 then ck,2 = ck,2 − 1
end
E
!′ c(
f, e)
(13)
�f′, �e)
</equation>
<page confidence="0.738869">
550
</page>
<bodyText confidence="0.691527">
pair of words is given by the expression:
</bodyText>
<equation confidence="0.997546666666667">
c(f|e)
p(f|e) = (14)
E f′ c(f′|e)
</equation>
<bodyText confidence="0.999486666666667">
where c(f|e) is the expected number of times that
the word e is aligned to the word f. The alignment
probability is defined in a similar way:
</bodyText>
<equation confidence="0.997928333333333">
c(aj|aj−1, l)
p(aj|aj−1, l) = (15)
Ea′r c(aj′ |aj−1, l)
</equation>
<bodyText confidence="0.999951837837838">
where c(aj|aj−1, l) denotes the expected number of
times that the alignment aj has been seen after the
previous alignment aj−1 given a source sentence
composed of l words.
Given the equations (14) and (15), the set of suf-
ficient statistics for the inverse HMM-based model
consists of a set of expected counts (numerator and
denominator values are stored separately). Given a
new sentence pair (f, e), we execute a new iteration
of the incremental EM algorithm on the new sample
and collect the contributions to the expected counts.
The parameters of the direct HMM-based model
are estimated analogously to those of the inverse
HMM-based model. Once the direct and the inverse
HMM-based model parameters have been modified
due to the presentation of a new sentence pair to the
IMT system, both models are used to obtain word
alignments for the new sentence pair. The resulting
direct and inverse word alignment matrices are com-
bined by means of the symmetrization alignment op-
eration (Och and Ney, 2003) before extracting the
set of consistent phrase pairs.
HMM-based alignment models are used here
because, according to (Och and Ney, 2003)
and (Toutanova et al., 2002), they outperform IBM 1
to IBM 4 alignment models while still allowing the
exact calculation of the likelihood for a given sen-
tence pair.
The δ parameters of the geometric distributions
associated to the feature functions h5, h6 and h7 are
left fixed. Because of this, there are no sufficient
statistics to store for these feature functions.
Finally, the weights of the log-linear combination
are not modified due to the presentation of a new
sentence pair to the system. These weights can be
adjusted off-line by means of a development corpus
and well-known optimization techniques.
</bodyText>
<sectionHeader confidence="0.998634" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9998895">
This section describes the experiments that we car-
ried out to test our online IMT system.
</bodyText>
<subsectionHeader confidence="0.992611">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9999775">
The experiments were performed using the XE-
ROX XRCE corpus (SchlumbergerSema S.A. et
al., 2001), which consists of translations of Xe-
rox printer manuals involving three different pairs
of languages: French-English, Spanish-English, and
German-English. The main features of these cor-
pora are shown in Table 1. Partitions into training,
development and test were performed. This corpus
is used here because it has been extensively used in
the literature on IMT to report results.
IMT experiments were carried out from English
to the other three languages.
</bodyText>
<subsectionHeader confidence="0.999737">
5.2 Assessment criteria
</subsectionHeader>
<bodyText confidence="0.999991714285714">
The evaluation of the techniques presented in this
paper were carried out using the Key-stroke and
mouse-action ratio (KSMR) measure (Barrachina
et al., 2009). This is calculated as the number of
keystrokes plus the number of mouse movements
plus one more count per sentence (aimed at simulat-
ing the user action needed to accept the final transla-
tion), the sum of which is divided by the total num-
ber of reference characters. In addition to this, we
also used the well-known BLEU score (Papineni et
al., 2001) to measure the translation quality of the
first translation hypothesis produced by the IMT sys-
tem for each source sentence (which is automatically
generated without user intervention).
</bodyText>
<subsectionHeader confidence="0.981431">
5.3 Online IMT results
</subsectionHeader>
<bodyText confidence="0.9998205">
To test the techniques proposed in this work, we
carried out experiments in two different scenarios.
In the first one, the first 10 000 sentences extracted
from the training corpora were interactively trans-
lated by means of an IMT system without any pre-
existent model stored in memory. Each time a new
sentence pair was validated, it was used to incremen-
tally train the system. Figures 3a, 3b and 3c show the
evolution of the KSMR with respect to the number
of sentence pairs processed by the IMT system; the
results correspond to the translation from English to
Spanish, French and German, respectively. In addi-
</bodyText>
<page confidence="0.993478">
551
</page>
<table confidence="0.9994348125">
En Sp En Fr En Ge
Train Sent. pairs 55761 52844 49376
Running words
Vocabulary
571960 657172 542762 573170 506877 440682
25627 29565 24958 27399 24899 37338
Dev. Sent. pairs 1012 994 964
Running words
Perplexity (3-grams)
12111 13808 9480 9801 9162 8283
46.2 34.0 96.2 74.1 68.4 124.3
Test Sent. pairs 1125 984 996
Running words
Perplexity (3-grams)
7634 9358 9572 9805 10792 9823
107.0 59.6 192.6 135.4 92.8 169.2
</table>
<tableCaption confidence="0.8801115">
Table 1: XEROX corpus statistics for three different language pairs (from English (En) to Spanish (Sp),
French (Fr) and German (Ge))
</tableCaption>
<bodyText confidence="0.999939111111111">
tion, for each language pair we interactively trans-
lated the original portion of the training corpus and
the same portion of the original corpus after being
randomly shuffled.
As these figures show, the results clearly demon-
strate that the IMT system is able to learn from
scratch. The results were similar for the three lan-
guages. It is also worthy of note that the obtained
results were better in all cases for the original cor-
pora than for the shuffled ones. This is because,
in the original corpora, similar sentences appear
more or less contiguosly (due to the organization of
the contents of the printer manuals). This circum-
stance increases the accuracy of the online learning,
since with the original corpora the number of lat-
eral effects ocurred between the translation of sim-
ilar sentences is decreased. The online learning of
a new sentence pair produces a lateral effect when
the changes in the probability given by the models
not only affect the newly trained sentence pair but
also other sentence pairs. A lateral effect can cause
that the system generates a wrong translation for a
given source sentence due to undesired changes in
the statistical models.
The accuracy were worse for shuffled corpora,
since shuffling increases the number of lateral ef-
fects that may occur between the translation of sim-
ilar sentences (because they no longer appear con-
tiguously). A good way to compare the quality of
different online IMT systems is to determine their
robustness in relation to sentence ordering. How-
ever, it can generally be expected that the sentences
to be translated in an interactive translation session
will be in a non-random order.
Alternatively, we carried out experiments in a dif-
ferent learning scenario. Specifically, the XEROX
</bodyText>
<figure confidence="0.999127">
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
#Sentences
(a) English-Spanish
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
#Sentences
(b) English-French
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
#Sentences
(c) English-German
</figure>
<figureCaption confidence="0.990353">
Figure 3: KSMR evolution translating a portion of
the training corpora
</figureCaption>
<bodyText confidence="0.992418666666667">
test corpora were interactively translated from the
English language to the other three languages, com-
paring the performance of a batch IMT system with
</bodyText>
<figure confidence="0.998446548387097">
KSMR
100
40
90
80
70
60
50
30
original
shuffled
KSMR
100
40
90
80
70
60
50
original
shuffled
KSMR
100
40
90
80
70
60
50
original
shuffled
</figure>
<page confidence="0.990425">
552
</page>
<bodyText confidence="0.999623727272727">
that of an online IMT system. The batch IMT sys-
tem is a conventional IMT system which is not able
to take advantage of user feedback after each transla-
tion while the online IMT system uses the new sen-
tence pairs provided by the user to revise the sta-
tistical models. Both systems were initialized with
a log-linear model trained in batch mode by means
of the XEROX training corpora. The weights of the
log-linear combination were adjusted for the devel-
opment corpora by means of the downhill-simplex
algorithm. Table 2 shows the obtained results. The
table shows the BLEU score and the KSMR for the
batch and the online IMT systems (95% confidence
intervals are shown). The BLEU score was calcu-
lated from the first translation hypothesis produced
by the IMT system for each source sentence. The ta-
ble also shows the average online learning time (LT)
for each new sample presented to the system2. All
the improvements obtained with the online IMT sys-
tem were statistically significant. Also, the average
learning times clearly allow the system to be used in
a real-time scenario.
</bodyText>
<table confidence="0.994866428571428">
IMT system BLEU KSMR LT (s)
batch 55.1± 2.3 18.2± 1.1 -
En-Sp online 60.6± 2.3 15.8± 1.0 0.04
batch 33.7± 2.0 33.9± 1.3 -
En-Fr online 42.2± 2.2 27.9± 1.3 0.09
batch 20.4± 1.8 40.3± 1.2 -
En-Ge online 28.0± 2.0 35.0± 1.3 0.07
</table>
<tableCaption confidence="0.946787">
Table 2: BLEU and KSMR results for the XEROX
</tableCaption>
<bodyText confidence="0.996285642857143">
test corpora using the batch and the online IMT sys-
tems. The average online learning time (LT) in sec-
onds is shown for the online system
Finally, in Table 3 a comparison of the KSMR re-
sults obtained by the online IMT system with state-
of-the-art IMT systems is reported (95% confidence
intervals are shown). We compared our system with
those presented in (Barrachina et al., 2009): the
alignment templates (AT), the stochastic finite-state
transducer (SFST), and the phrase-based (PB) ap-
proaches to IMT. The results were obtained using
the same Xerox training and test sets (see Table 1)
for the four different IMT systems. Our system out-
performed the results obtained by these systems.
</bodyText>
<footnote confidence="0.4078395">
2All the experiments were executed on a PC with a 2.40 Ghz
Intel Xeon processor with 1GB of memory.
</footnote>
<table confidence="0.99802975">
AT PB SFST Online
En-Sp 23.2±1.3 16.7±1.2 21.8±1.4 15.8± 1.0
En-Fr 40.4±1.4 35.8±1.3 43.8±1.6 27.9± 1.3
En-Ge 44.7±1.2 40.1±1.2 45.7±1.4 35.0± 1.3
</table>
<tableCaption confidence="0.970787">
Table 3: KSMR results comparison of our system
and three different state-of-the-art batch systems
</tableCaption>
<sectionHeader confidence="0.998469" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999990034482759">
We have presented an online IMT system. The pro-
posed system is able to incrementally extend the sta-
tistical models involved in the translation process,
breaking technical limitations encountered in other
works. Empirical results show that our techniques
allow the IMT system to learn from scratch or from
previously estimated models.
One key aspect of the proposed system is the use
of HMM-based alignment models trained by means
of the incremental EM algorithm.
The incremental adjustment of the weights of the
log-linear models and other parameters have not
been tackled here. For the future we plan to incor-
porate this functionality into our IMT system.
The incremental techniques proposed here can
also be exploited to extend SMT systems (in fact,
our proposed IMT system is based on an incremen-
tally updateable SMT system). For the near future
we plan to study possible aplications of our tech-
niques in a fully automatic translation scenario.
Finally, it is worthy of note that the main ideas
presented here can be used in other interactive ap-
plications such as Computer Assisted Speech Tran-
scription, Interactive Image Retrieval, etc (see (Vi-
dal et al., 2007) for more information). In conclu-
sion, we think that the online learning techniques
proposed here can be the starting point for a new
generation of interactive pattern recognition systems
that are able to take advantage of user feedback.
</bodyText>
<sectionHeader confidence="0.998683" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.983889555555555">
Work supported by the EC (FEDER/FSE), the
Spanish Government (MEC, MICINN, MITyC,
MAEC, ”Plan E”, under grants MIPRCV ”Con-
solider Ingenio 2010” CSD2007-00018, iTrans2
TIN2009-14511, erudito.com TSI-020110-2009-
439), the Generalitat Valenciana (grant Prome-
teo/2009/014), the Univ. Polit´ecnica de Valencia
(grant 20091027) and the Spanish JCCM (grant
PBI08-0210-7127).
</bodyText>
<page confidence="0.998192">
553
</page>
<sectionHeader confidence="0.993882" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991925">
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of the MT Summit XI,
pages 15–20, Copenhagen, Denmark, September.
S. Barrachina, O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tom´as,
and E. Vidal. 2009. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, 35(1):3–28.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263–311.
N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008.
Online learning algorithms for computer-assisted
translation. Deliverable D4.2, SMART: Stat. Multi-
lingual Analysis for Retrieval and Translation, Mar.
S.F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
of the ACL, pages 310–318, San Francisco.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. ofEMNLP.
George Foster, Pierre Isabelle, and Pierre Plamondon.
1997. Target-text mediated interactive machine trans-
lation. Machine Translation, 12(1):175–194.
P. Isabelle and K. Church. 1997. Special issue on
new tools for human translators. Machine Translation,
12(1–2).
D.E. Knuth. 1981. Seminumerical Algorithms, volume 2
of The Art of Computer Programming. Addison-
Wesley, Massachusetts, 2nd edition.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the HLT/NAACL,
pages 48–54, Edmonton, Canada, May.
P. Langlais, G. Lapalme, and M. Loranger. 2002.
Transtype: Development-evaluation cycles to boost
translator’s productivity. Machine Translation,
15(4):77–98.
P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of the 44th ACL, pages 761–
768, Morristown, NJ, USA.
R.M. Neal and G.E. Hinton. 1998. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In Proceedings of the NATO-ASI on Learning
in graphical models, pages 355–368, Norwell, MA,
USA.
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004.
Adaptive language and translation models for interac-
tive machine translation. In Proc. of EMNLP, pages
190–197, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proc. of the 40th ACL,
pages 295–302, Philadelphia, PA, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51, March.
D. Ortiz-Martinez, I. Garcia-Varea, and Casacuberta F.
2008. The scaling problem in the pattern recognition
approach to machine translation. Pattern Recognition
Letters, 29:1145–1153.
Daniel Ortiz-Martinez, Ismael Garcia-Varea, and Fran-
cisco Casacuberta. 2009. Interactive machine trans-
lation based on partial statistical phrase-based align-
ments. In Proc. of RANLP, Borovets, Bulgaria, sep.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Divi-
sion, Thomas J. Watson Research Center, Yorktown
Heights, NY, September.
SchlumbergerSema S.A., ITI Valencia, RWTH Aachen,
RALI Montreal, Celer Soluciones, Soci´et´e Gamma,
and XRCE. 2001. TT2. TransType2 - computer as-
sisted translation. Project Tech. Rep.
Kristina Toutanova, H. Tolga Ilhan, and Christopher
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In Proc. of EMNLP.
E. Vidal, L. Rodriguez, F. Casacuberta, and I. Garcia-
Varea. 2007. Interactive pattern recognition. In Proc.
of the 4th MLMI, pages 60–71. Brno, Czech Republic,
28-30 June.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. of COLING, pages 836–841, Copen-
hagen, Denmark, August.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP and CoNLL,
pages 764–733, Prage, Czeck Republic.
</reference>
<page confidence="0.998715">
554
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.284695">
<title confidence="0.999915">Online Learning for Interactive Statistical Machine Translation</title>
<author confidence="0.963397">Daniel</author>
<affiliation confidence="0.979432">Dpto. de Sist. Inf. y Univ. Polit´ec. de Valencia</affiliation>
<address confidence="0.999555">46071 Valencia,</address>
<email confidence="0.980019">dortiz@dsic.upv.es</email>
<author confidence="0.432197">Ismael</author>
<affiliation confidence="0.9958155">Dpto. de Informitica Univ. de Castilla-La</affiliation>
<address confidence="0.999609">02071 Albacete, Spain</address>
<email confidence="0.901525">ivarea@info-ab.uclm.es</email>
<author confidence="0.99629">Francisco Casacuberta</author>
<affiliation confidence="0.9934385">Dpto. de Sist. Inf. y Comp. Univ. Polit´ec. de Valencia</affiliation>
<address confidence="0.999739">46071 Valencia, Spain</address>
<email confidence="0.995454">fcn@dsic.upv.es</email>
<abstract confidence="0.992957653846154">State-of-the-art Machine Translation (MT) systems are still far from being perfect. An alternative is the so-called Interactive Machine Translation (IMT) framework. In this framework, the knowledge of a human translator is combined with a MT system. The vast majority of the existing work on IMT makes use the well-known learning In the batch learning paradigm, the training of the IMT system and the interactive translation process are carried out in separate stages. This paradigm is not able to take advantage of the new knowledge produced by the user of the IMT system. In this paper, we present an apof the learning to the IMT framework. In the online learning paradigm, the training and prediction stages are no longer separated. This feature is particularly useful in IMT since it allows the user feedback to be taken into account. The online learning techniques proposed here incrementally update the statistical models involved in the translation process. Empirical results show the great potential of online learning in the IMT framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arun</author>
<author>P Koehn</author>
</authors>
<title>Online learning methods for discriminative training of phrase based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the MT Summit XI,</booktitle>
<pages>15--20</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="8789" citStr="Arun and Koehn, 2007" startWordPosition="1494" endWordPosition="1497">be the basic IMT system involved in the interactive translation process. Then we introduce the required techniques to incrementally update the statistical models used by the system. 3 Related work In this paper we present an application of the online learning paradigm to the IMT framework. In the online learning setting, models are trained sample by sample. Our work is also related to model adaptation, although model adaptation and online learning are not exactly the same thing. The online learning paradigm has been previously applied to train discriminative models in SMT (Liang et al., 2006; Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2008). These works differ from the one presented here in that we apply online learning techniques to train generative models instead of discriminative models. In (Nepveu et al., 2004), dynamic adaptation of an IMT system via cache-based model extensions to language and translation models is proposed. The work by Nepveu et al. (2004) constitutes a domain adaptation technique and not an online learning technique, since the proposed cache components require pre-existent models estimated in batch mode. In addition to this, their IMT system does not use state</context>
</contexts>
<marker>Arun, Koehn, 2007</marker>
<rawString>A. Arun and P. Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. In Proc. of the MT Summit XI, pages 15–20, Copenhagen, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Barrachina</author>
<author>O Bender</author>
<author>F Casacuberta</author>
<author>J Civera</author>
<author>E Cubel</author>
<author>S Khadivi</author>
<author>A Lagarda</author>
<author>H Ney</author>
<author>J Tom´as</author>
<author>E Vidal</author>
</authors>
<title>Statistical approaches to computer-assisted translation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<marker>Barrachina, Bender, Casacuberta, Civera, Cubel, Khadivi, Lagarda, Ney, Tom´as, Vidal, 2009</marker>
<rawString>S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tom´as, and E. Vidal. 2009. Statistical approaches to computer-assisted translation. Computational Linguistics, 35(1):3–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4870" citStr="Brown et al., 1993" startWordPosition="797" endWordPosition="800"> is produced. The input sentence f and its desired translation e� can be used to refine the models used by the system. In general, the model is initially obtained through a classical batch training process from a previously given training sequence of pairs (fi,ei) from the task being considered. Now, the models can be extended with the use of valuable user feedback. 2 Interactive machine translation IMT can be seen as an evolution of the SMT framework. Given a sentence f from a source language F to be translated into a target sentence e of a target language £, the fundamental equation of SMT (Brown et al., 1993) is the following: e� = argmax {Pr(e |f)} (1) e = argmax {Pr(f |e) Pr(e)} (2) e where Pr(f |e) is approximated by a translation model that represents the correlation between the source and the target sentence and where Pr(e) is approximated by a language model representing the well-formedness of the candidate translation e. State-of-the-art statistical machine translation systems follow a loglinear approach (Och and Ney, 2002), where direct modelling of the posterior probability Pr(e |f) of Equation (1) is used. In this case, the decision rule is given by the expression: � M � λmhm(e, f) (3) m</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cesa-Bianchi</author>
<author>G Reverberi</author>
<author>S Szedmak</author>
</authors>
<title>Online learning algorithms for computer-assisted translation. Deliverable D4.2, SMART: Stat. Multilingual Analysis for Retrieval and Translation,</title>
<date>2008</date>
<contexts>
<context position="9507" citStr="Cesa-Bianchi et al., 2008" startWordPosition="1608" endWordPosition="1611">e in that we apply online learning techniques to train generative models instead of discriminative models. In (Nepveu et al., 2004), dynamic adaptation of an IMT system via cache-based model extensions to language and translation models is proposed. The work by Nepveu et al. (2004) constitutes a domain adaptation technique and not an online learning technique, since the proposed cache components require pre-existent models estimated in batch mode. In addition to this, their IMT system does not use state-of-the-art models. To our knowledge, the only previous work on online learning for IMT is (Cesa-Bianchi et al., 2008), where a very constrained version of online learning is presented. This constrained version of online learning is not able to extend the translation models due to technical problems with the efficiency of the learning process. In this paper, we present a purely statistical IMT system which is able to incrementally update the parameters of all of the different models that are used in the system, including the translation model, breaking with the above mentioned coni−n+1) − Dn, 0} cX(ep(ei|ei−1 i−n+1) = max{cX(ei + i−1 i−n+1) Dn N1+(ei−1 i−n+1•) · p(ei|ei−1 i−n+2) (8) cX(ei−1 i−n+1) where Dn = </context>
</contexts>
<marker>Cesa-Bianchi, Reverberi, Szedmak, 2008</marker>
<rawString>N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008. Online learning algorithms for computer-assisted translation. Deliverable D4.2, SMART: Stat. Multilingual Analysis for Retrieval and Translation, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>310--318</pages>
<location>San Francisco.</location>
<contexts>
<context position="10820" citStr="Chen and Goodman, 1996" startWordPosition="1828" endWordPosition="1831">two counts respectively), N1+(ei−1 i−n+1•) is the number of unique words that follows the history ei−1 i−n+1 and cX(eii−n+1) is the count of the n-gram ei i−n+1, where cX(·) can represent true counts cT(·) or modified counts cm(·). True counts are used for the higher order n-grams and modified counts for the lower order n-grams. Given a certain n-gram, its modified count consists in the number of different words that precede this ngram in the training corpus. Equation (8) corresponds to the probability given by an n-gram language model with an interpolated version of the Kneser-Ney smoothing (Chen and Goodman, 1996). 4.1 Basic IMT system The basic IMT system that we propose uses a loglinear model to generate its translations. According to Equation (7), we introduce a set of seven feature functions (from h1 to h7): • n-gram language model (h1) h1(e) = log(�|�|+1 i=1 p(ei|ei−1 i−n+1)), 1 where p(ei|ei−1 i−n+1) is defined as follows: 1|e |is the length of e, e0 denotes the begin-of-sentence symbol, eIe +1 denotes the end-of-sentence symbol, eji - ei...ej 548 • target sentence-length model (h2) h2(e, f) = log(p(|f e|)) = log(φ|e|(|f|+0.5)− φ|e|(|f |− 0.5)), where φ|e|(·) denotes the cumulative distribution f</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S.F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. of the ACL, pages 310–318, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Online largemargin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. ofEMNLP.</booktitle>
<contexts>
<context position="8834" citStr="Chiang et al., 2008" startWordPosition="1502" endWordPosition="1505">active translation process. Then we introduce the required techniques to incrementally update the statistical models used by the system. 3 Related work In this paper we present an application of the online learning paradigm to the IMT framework. In the online learning setting, models are trained sample by sample. Our work is also related to model adaptation, although model adaptation and online learning are not exactly the same thing. The online learning paradigm has been previously applied to train discriminative models in SMT (Liang et al., 2006; Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2008). These works differ from the one presented here in that we apply online learning techniques to train generative models instead of discriminative models. In (Nepveu et al., 2004), dynamic adaptation of an IMT system via cache-based model extensions to language and translation models is proposed. The work by Nepveu et al. (2004) constitutes a domain adaptation technique and not an online learning technique, since the proposed cache components require pre-existent models estimated in batch mode. In addition to this, their IMT system does not use state-of-the-art models. To our knowledge, the onl</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>D. Chiang, Y. Marton, and P. Resnik. 2008. Online largemargin training of syntactic and structural translation features. In Proc. ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Pierre Isabelle</author>
<author>Pierre Plamondon</author>
</authors>
<title>Target-text mediated interactive machine translation.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="2104" citStr="Foster et al., 1997" startWordPosition="317" endWordPosition="320">learning in the IMT framework. 1 Introduction Information technology advances have led to the need for more efficient translation methods. Current MT systems are not able to produce ready-to-use texts. Indeed, MT systems usually require human post-editing to achieve high-quality translations. One way of taking advantage of MT systems is to combine them with the knowledge of a human translator in the IMT paradigm, which is a special type of the computer-assisted translation paradigm (Isabelle and Church, 1997). An important contribution to IMT technology was pioneered by the TransType project (Foster et al., 1997; Langlais et al., 2002) where data driven MT techniques were adapted for their use in an interactive translation environment. Following the TransType ideas, Barrachina et al. (2009) proposed a new approach to IMT, in which fully-fledged statistical MT (SMT) systems are used to produce full target sentence hypotheses, or portions thereof, which can be partially or completely accepted and amended by a human translator. Each partial, correct text segment is then used by the SMT system as additional information to achieve improved suggestions. Figure 1 illustrates a typical IMT session. source(f)</context>
</contexts>
<marker>Foster, Isabelle, Plamondon, 1997</marker>
<rawString>George Foster, Pierre Isabelle, and Pierre Plamondon. 1997. Target-text mediated interactive machine translation. Machine Translation, 12(1):175–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Isabelle</author>
<author>K Church</author>
</authors>
<title>Special issue on new tools for human translators.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<pages>12--1</pages>
<contexts>
<context position="1999" citStr="Isabelle and Church, 1997" startWordPosition="301" endWordPosition="304">he statistical models involved in the translation process. Empirical results show the great potential of online learning in the IMT framework. 1 Introduction Information technology advances have led to the need for more efficient translation methods. Current MT systems are not able to produce ready-to-use texts. Indeed, MT systems usually require human post-editing to achieve high-quality translations. One way of taking advantage of MT systems is to combine them with the knowledge of a human translator in the IMT paradigm, which is a special type of the computer-assisted translation paradigm (Isabelle and Church, 1997). An important contribution to IMT technology was pioneered by the TransType project (Foster et al., 1997; Langlais et al., 2002) where data driven MT techniques were adapted for their use in an interactive translation environment. Following the TransType ideas, Barrachina et al. (2009) proposed a new approach to IMT, in which fully-fledged statistical MT (SMT) systems are used to produce full target sentence hypotheses, or portions thereof, which can be partially or completely accepted and amended by a human translator. Each partial, correct text segment is then used by the SMT system as addi</context>
</contexts>
<marker>Isabelle, Church, 1997</marker>
<rawString>P. Isabelle and K. Church. 1997. Special issue on new tools for human translators. Machine Translation, 12(1–2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>Seminumerical Algorithms,</title>
<date>1981</date>
<booktitle>of The Art of Computer Programming.</booktitle>
<volume>2</volume>
<publisher>AddisonWesley,</publisher>
<location>Massachusetts,</location>
<contexts>
<context position="15065" citStr="Knuth, 1981" startWordPosition="2542" endWordPosition="2543"> symbol). Given a new sentence e, and for each k-gram eii−k+1 of e where 1 ≤ k ≤ n and 1 ≤ i ≤ |e|+1, we modify the set of sufficient statistics as it is shown in Algorithm 1. The algorithm checks the changes in the counts of the k-grams to update the set of sufficient statistics. Sufficient statistics for Dk are updated following the auxiliar procedure shown in Algorithm 2. Feature function h2 requires the incremental calculation of the mean µ|e |and the standard deviation σ|e |of the normal distribution associated to a target sentence length |e|. For this purpose the procedure described in (Knuth, 1981) can be used. In this procedure, two quantities are maintained for each normal distribution: µ|e |and S|e|. Given anew sentence p( phmm( f |E) |�f| = ǫ� H a |˜f |j=1 1 549 Algorithm 1: Pseudocode for updating the sufficient statistics of a given k-gram Algorithm 2: Pseudocode for the updD procedure pair (f, e), the two quantities are updated using a recurrence relation: µ|e |= µ ′ ′ |e |+ (|f |− µ|e|)/c(|e|) (11) S|e |= S′|e |+ (|f |− µ′|e|)(|f |− µ|e|) (12) where c(|e|) is the count of the number of sentences of length |e |that have been seen so far, and µ′|e |and S′|e |are the quantities pre</context>
</contexts>
<marker>Knuth, 1981</marker>
<rawString>D.E. Knuth. 1981. Seminumerical Algorithms, volume 2 of The Art of Computer Programming. AddisonWesley, Massachusetts, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of the HLT/NAACL,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="5650" citStr="Koehn et al., 2003" startWordPosition="929" endWordPosition="932">tion between the source and the target sentence and where Pr(e) is approximated by a language model representing the well-formedness of the candidate translation e. State-of-the-art statistical machine translation systems follow a loglinear approach (Och and Ney, 2002), where direct modelling of the posterior probability Pr(e |f) of Equation (1) is used. In this case, the decision rule is given by the expression: � M � λmhm(e, f) (3) m=1 where each hm(e, f) is a feature function representing a statistical model and λm its weight. Current MT systems are based on the use of phrase-based models (Koehn et al., 2003) as translation models. The basic idea of Phrase-based Translation (PBT) is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally to reorder the translated target phrases in order to compose the target sentence. If we summarize all the decisions made during the phrase-based translation process by means of the hidden variable iiK1 , we obtain the expression: Pr(f|e) = � Pr( �fK1 ,6K1 |�eK1 ) (4) K,˜aK where each ak E {1 ... K} denotes the index of the target phrase e� that is aligned with the k-th source phrase fk, assuming a segment</context>
<context position="12035" citStr="Koehn et al., 2003" startWordPosition="2026" endWordPosition="2029">on function (cdf) for the normal distribution (the cdf is used here to integrate the normal density function over an interval of length 1). We use a specific normal distribution with mean µ|e |and standard deviation σ|e |for each possible target sentence length |e|. • inverse and direct phrase-based models (h3, h4) h3(e, a, f) = log(HKk=1 p(�fk|�eak)), where �fk|eak) is defined as follows: p(�fk|�eak) = β · pphr(�fk|�eak) + (1 − β).phmm(�fk|�eak) (9) In Equation (9), pphr(�fk|�eak) denotes the probability given by a statistical phrase-based dictionary used in regular phrase-based models (see (Koehn et al., 2003) for more details). phmm( �fk|eak) is the probability given by an HMM-based (intraphrase) alignment model (see (Vogel et al., 1996)): p(�fj|�ea;) · p(aj|aj−1, |�e|) (10) The HMM-based alignment model probability is used here for smoothing purposes as described in (Ortiz-Martinez et al., 2009). Analogously h4 is defined as: h4(e, a, f) = log(HKk=1 p(�eak |A)) • target phrase-length model (h5) h5(e, a, f) = log(HKk=1 p(|4|)), where p(|4|) = δ(1 − δ)|6k|. h5 implements a target phrase-length model by means of a geometric distribution with probability of success on each trial δ. The use of a geome</context>
<context position="16723" citStr="Koehn et al., 2003" startWordPosition="2835" endWordPosition="2838">e efficiently obtained using appropriate data structures, see (OrtizMartinez et al., 2008)). The inverse phrase model probabilities are estimated from the phrase counts: pU�f|�e) = c( According to Equation (13), the set of sufficient statistics to be stored for the inverse phrase model consists of a set of phrase counts (c(�f, e) and E f′ c(�f′, e) must be stored separately). Given a new sentence pair (f, e), the standard phrase-based model estimation method uses a word alignment matrix between f and e to extract the set of phrase pairs that are consistent with the word alignment matrix (see (Koehn et al., 2003) for more details). Once the consistent phrase pairs have been extracted, the phrase counts are updated. The word alignment matrices required for the extraction of phrase pairs are generated by means of the HMM-based models used in the feature functions h3 and h4. Inverse and direct HMM-based models are used here for two purposes: to smooth the phrase-based models via linear interpolation and to generate word alignment matrices. The weights of the interpolation can be estimated from a development corpus. Equation (10) shows the expression of the probability given by an inverse HMM-based model.</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of the HLT/NAACL, pages 48–54, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langlais</author>
<author>G Lapalme</author>
<author>M Loranger</author>
</authors>
<title>Transtype: Development-evaluation cycles to boost translator’s productivity.</title>
<date>2002</date>
<journal>Machine Translation,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="2128" citStr="Langlais et al., 2002" startWordPosition="321" endWordPosition="324">ramework. 1 Introduction Information technology advances have led to the need for more efficient translation methods. Current MT systems are not able to produce ready-to-use texts. Indeed, MT systems usually require human post-editing to achieve high-quality translations. One way of taking advantage of MT systems is to combine them with the knowledge of a human translator in the IMT paradigm, which is a special type of the computer-assisted translation paradigm (Isabelle and Church, 1997). An important contribution to IMT technology was pioneered by the TransType project (Foster et al., 1997; Langlais et al., 2002) where data driven MT techniques were adapted for their use in an interactive translation environment. Following the TransType ideas, Barrachina et al. (2009) proposed a new approach to IMT, in which fully-fledged statistical MT (SMT) systems are used to produce full target sentence hypotheses, or portions thereof, which can be partially or completely accepted and amended by a human translator. Each partial, correct text segment is then used by the SMT system as additional information to achieve improved suggestions. Figure 1 illustrates a typical IMT session. source(f): Para ver la lista de r</context>
</contexts>
<marker>Langlais, Lapalme, Loranger, 2002</marker>
<rawString>P. Langlais, G. Lapalme, and M. Loranger. 2002. Transtype: Development-evaluation cycles to boost translator’s productivity. Machine Translation, 15(4):77–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the 44th ACL,</booktitle>
<pages>761--768</pages>
<location>Morristown, NJ, USA.</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. of the 44th ACL, pages 761– 768, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
<author>G E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>In Proceedings of the NATO-ASI on Learning in graphical models,</booktitle>
<pages>355--368</pages>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="17866" citStr="Neal and Hinton, 1998" startWordPosition="3017" endWordPosition="3020">0) shows the expression of the probability given by an inverse HMM-based model. The probability includes lexical probabilities p(fj|ez) and alignment probabilities p(aj|aj_1, l). Since the alignment in the HMM-based model is determined by a hidden variable, the EM algorithm is required to estimate the parameters of the model (see (Och and Ney, 2003)). However, the standard EM algorithm is not appropriate to incrementally extend our HMM-based models because it is designed to work in batch training scenarios. To solve this problem, we apply the incremental view of the EM algorithm described in (Neal and Hinton, 1998). According to (Och and Ney, 2003), the lexical probability for a input : n (higher order), ei−k+1 (k-gram), S = {dj(cj,1, cj,2), N1+(&apos;), cX(&apos;)} (current set of sufficient statistics) output : S (updated set of sufficient statistics) begin if cT(ei−k+1) = 0 then if k − 1 &gt; 1 then updD(S,k-1,cM(e�−1 �−k+2),cM(e�−1 �−k+2)+1) if cM(ei−k−+2) = 0 then N1+(ei−k+2) = N1+(e�−1 �−k+2) + 1 cM(e�−1 �−k+2) = cM(e�−1 �−k+2) + 1 cM(ei−k+2) = cM(ei−k+2) + 1 if k = n then N1+(e�−1 �−k+1) = N1+(e�−1 �−k+1) + 1 if k = n then updD(S,k,cT(e��−k+1),cT (ei−k+1) + 1) cT(e�−1 �−k+1)=cT(e�−1 �−k+1) + 1 cT(ei−k+1)=cT(e</context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>R.M. Neal and G.E. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Proceedings of the NATO-ASI on Learning in graphical models, pages 355–368, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Nepveu</author>
<author>G Lapalme</author>
<author>P Langlais</author>
<author>G Foster</author>
</authors>
<title>Adaptive language and translation models for interactive machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>190--197</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="9012" citStr="Nepveu et al., 2004" startWordPosition="1530" endWordPosition="1533">n application of the online learning paradigm to the IMT framework. In the online learning setting, models are trained sample by sample. Our work is also related to model adaptation, although model adaptation and online learning are not exactly the same thing. The online learning paradigm has been previously applied to train discriminative models in SMT (Liang et al., 2006; Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2008). These works differ from the one presented here in that we apply online learning techniques to train generative models instead of discriminative models. In (Nepveu et al., 2004), dynamic adaptation of an IMT system via cache-based model extensions to language and translation models is proposed. The work by Nepveu et al. (2004) constitutes a domain adaptation technique and not an online learning technique, since the proposed cache components require pre-existent models estimated in batch mode. In addition to this, their IMT system does not use state-of-the-art models. To our knowledge, the only previous work on online learning for IMT is (Cesa-Bianchi et al., 2008), where a very constrained version of online learning is presented. This constrained version of online le</context>
</contexts>
<marker>Nepveu, Lapalme, Langlais, Foster, 2004</marker>
<rawString>L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004. Adaptive language and translation models for interactive machine translation. In Proc. of EMNLP, pages 190–197, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th ACL,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="5300" citStr="Och and Ney, 2002" startWordPosition="864" endWordPosition="867">ion of the SMT framework. Given a sentence f from a source language F to be translated into a target sentence e of a target language £, the fundamental equation of SMT (Brown et al., 1993) is the following: e� = argmax {Pr(e |f)} (1) e = argmax {Pr(f |e) Pr(e)} (2) e where Pr(f |e) is approximated by a translation model that represents the correlation between the source and the target sentence and where Pr(e) is approximated by a language model representing the well-formedness of the candidate translation e. State-of-the-art statistical machine translation systems follow a loglinear approach (Och and Ney, 2002), where direct modelling of the posterior probability Pr(e |f) of Equation (1) is used. In this case, the decision rule is given by the expression: � M � λmhm(e, f) (3) m=1 where each hm(e, f) is a feature function representing a statistical model and λm its weight. Current MT systems are based on the use of phrase-based models (Koehn et al., 2003) as translation models. The basic idea of Phrase-based Translation (PBT) is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally to reorder the translated target phrases in order to compo</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proc. of the 40th ACL, pages 295–302, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17595" citStr="Och and Ney, 2003" startWordPosition="2973" endWordPosition="2976">h3 and h4. Inverse and direct HMM-based models are used here for two purposes: to smooth the phrase-based models via linear interpolation and to generate word alignment matrices. The weights of the interpolation can be estimated from a development corpus. Equation (10) shows the expression of the probability given by an inverse HMM-based model. The probability includes lexical probabilities p(fj|ez) and alignment probabilities p(aj|aj_1, l). Since the alignment in the HMM-based model is determined by a hidden variable, the EM algorithm is required to estimate the parameters of the model (see (Och and Ney, 2003)). However, the standard EM algorithm is not appropriate to incrementally extend our HMM-based models because it is designed to work in batch training scenarios. To solve this problem, we apply the incremental view of the EM algorithm described in (Neal and Hinton, 1998). According to (Och and Ney, 2003), the lexical probability for a input : n (higher order), ei−k+1 (k-gram), S = {dj(cj,1, cj,2), N1+(&apos;), cX(&apos;)} (current set of sufficient statistics) output : S (updated set of sufficient statistics) begin if cT(ei−k+1) = 0 then if k − 1 &gt; 1 then updD(S,k-1,cM(e�−1 �−k+2),cM(e�−1 �−k+2)+1) if c</context>
<context position="20114" citStr="Och and Ney, 2003" startWordPosition="3430" endWordPosition="3433">nce pair (f, e), we execute a new iteration of the incremental EM algorithm on the new sample and collect the contributions to the expected counts. The parameters of the direct HMM-based model are estimated analogously to those of the inverse HMM-based model. Once the direct and the inverse HMM-based model parameters have been modified due to the presentation of a new sentence pair to the IMT system, both models are used to obtain word alignments for the new sentence pair. The resulting direct and inverse word alignment matrices are combined by means of the symmetrization alignment operation (Och and Ney, 2003) before extracting the set of consistent phrase pairs. HMM-based alignment models are used here because, according to (Och and Ney, 2003) and (Toutanova et al., 2002), they outperform IBM 1 to IBM 4 alignment models while still allowing the exact calculation of the likelihood for a given sentence pair. The δ parameters of the geometric distributions associated to the feature functions h5, h6 and h7 are left fixed. Because of this, there are no sufficient statistics to store for these feature functions. Finally, the weights of the log-linear combination are not modified due to the presentation </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ortiz-Martinez</author>
<author>I Garcia-Varea</author>
<author>F Casacuberta</author>
</authors>
<title>The scaling problem in the pattern recognition approach to machine translation.</title>
<date>2008</date>
<journal>Pattern Recognition Letters,</journal>
<pages>29--1145</pages>
<marker>Ortiz-Martinez, Garcia-Varea, Casacuberta, 2008</marker>
<rawString>D. Ortiz-Martinez, I. Garcia-Varea, and Casacuberta F. 2008. The scaling problem in the pattern recognition approach to machine translation. Pattern Recognition Letters, 29:1145–1153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ortiz-Martinez</author>
<author>Ismael Garcia-Varea</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Interactive machine translation based on partial statistical phrase-based alignments.</title>
<date>2009</date>
<booktitle>In Proc. of RANLP,</booktitle>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="7776" citStr="Ortiz-Martinez et al., 2009" startWordPosition="1323" endWordPosition="1326">nteractions Interactive SMT System f , e 1 1 f , e 2 2 . . . f ^e Batch Learning Incremental Models Online Learning f Figure 2: An Online Interactive SMT system k ^ e e� = argmax e 547 which is the approach that we follow in this work. A common problem in IMT arises when the user sets a prefix (ep) which cannot be found in the phrase-based statistical translation model. Different solutions have been proposed to deal with this problem. The use of word translation graphs, as a compact representation of all possible translations of a source sentence, is proposed in (Barrachina et al., 2009). In (Ortiz-Martinez et al., 2009), a technique based on the generation of partial phrasebased alignments is described. This last proposal has also been adopted in this work. straints. What is more, our system is able to learn from scratch, that is, without any preexisting model stored in the system. This is demonstrated empirically in section 5. 4 Online IMT In this section we propose an online IMT system. First, we describe the basic IMT system involved in the interactive translation process. Then we introduce the required techniques to incrementally update the statistical models used by the system. 3 Related work In this pa</context>
<context position="12328" citStr="Ortiz-Martinez et al., 2009" startWordPosition="2070" endWordPosition="2073">ect phrase-based models (h3, h4) h3(e, a, f) = log(HKk=1 p(�fk|�eak)), where �fk|eak) is defined as follows: p(�fk|�eak) = β · pphr(�fk|�eak) + (1 − β).phmm(�fk|�eak) (9) In Equation (9), pphr(�fk|�eak) denotes the probability given by a statistical phrase-based dictionary used in regular phrase-based models (see (Koehn et al., 2003) for more details). phmm( �fk|eak) is the probability given by an HMM-based (intraphrase) alignment model (see (Vogel et al., 1996)): p(�fj|�ea;) · p(aj|aj−1, |�e|) (10) The HMM-based alignment model probability is used here for smoothing purposes as described in (Ortiz-Martinez et al., 2009). Analogously h4 is defined as: h4(e, a, f) = log(HKk=1 p(�eak |A)) • target phrase-length model (h5) h5(e, a, f) = log(HKk=1 p(|4|)), where p(|4|) = δ(1 − δ)|6k|. h5 implements a target phrase-length model by means of a geometric distribution with probability of success on each trial δ. The use of a geometric distribution penalizes the length of target phrases. • source phrase-length model (h6) h6(e, a, f) = log(HKk=1 p(|�fk �eak|)), where p( |A eak|) = δ(1 − δ)abs( |!k|−|e˜�k|) and abs(·) is the absolute value function. A geometric distribution is used to model this feature (it penalizes the</context>
<context position="13713" citStr="Ortiz-Martinez et al., 2009" startWordPosition="2299" endWordPosition="2302">k−1), bak denotes the beginning position of the source phrase covered by ak and l4−1 denotes the last position of the source phrase covered by 4−1. A geometric distribution is used to model this feature (it penalizes the reorderings). The log-linear model, which includes the above described feature functions, is used to generate the suffix es given the user-validated prefix ep. Specifically, the IMT system generates a partial phrasebased alignment between the user prefix ep and a portion of the source sentence f, and returns the suffix es as the translation of the remaining portion of f (see (Ortiz-Martinez et al., 2009)). 4.2 Extending the IMT system from user feedback After translating a source sentence f, a new sentence pair (f, e) is available to feed the IMT system (see Figure 1). In this section we describe how the log-linear model described in section 4.1 is updated given the new sentence pair. To do this, a set of sufficient statistics that can be incrementally updated is maintained for each feature function hi(·). A sufficient statistic for a statistical model is a statistic that captures all the information that is relevant to estimate this model. Regarding feature function h1 and according to equat</context>
</contexts>
<marker>Ortiz-Martinez, Garcia-Varea, Casacuberta, 2009</marker>
<rawString>Daniel Ortiz-Martinez, Ismael Garcia-Varea, and Francisco Casacuberta. 2009. Interactive machine translation based on partial statistical phrase-based alignments. In Proc. of RANLP, Borovets, Bulgaria, sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center, Yorktown Heights, NY,</location>
<contexts>
<context position="22078" citStr="Papineni et al., 2001" startWordPosition="3747" endWordPosition="3750">ort results. IMT experiments were carried out from English to the other three languages. 5.2 Assessment criteria The evaluation of the techniques presented in this paper were carried out using the Key-stroke and mouse-action ratio (KSMR) measure (Barrachina et al., 2009). This is calculated as the number of keystrokes plus the number of mouse movements plus one more count per sentence (aimed at simulating the user action needed to accept the final translation), the sum of which is divided by the total number of reference characters. In addition to this, we also used the well-known BLEU score (Papineni et al., 2001) to measure the translation quality of the first translation hypothesis produced by the IMT system for each source sentence (which is automatically generated without user intervention). 5.3 Online IMT results To test the techniques proposed in this work, we carried out experiments in two different scenarios. In the first one, the first 10 000 sentences extracted from the training corpora were interactively translated by means of an IMT system without any preexistent model stored in memory. Each time a new sentence pair was validated, it was used to incrementally train the system. Figures 3a, 3</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A SchlumbergerSema</author>
<author>ITI Valencia</author>
<author>RWTH Aachen</author>
<author>RALI Montreal</author>
<author>Celer Soluciones</author>
<author>Soci´et´e Gamma</author>
<author>XRCE</author>
</authors>
<title>TT2. TransType2 - computer assisted translation.</title>
<date>2001</date>
<tech>Project Tech. Rep.</tech>
<marker>SchlumbergerSema, Valencia, Aachen, Montreal, Soluciones, Gamma, XRCE, 2001</marker>
<rawString>SchlumbergerSema S.A., ITI Valencia, RWTH Aachen, RALI Montreal, Celer Soluciones, Soci´et´e Gamma, and XRCE. 2001. TT2. TransType2 - computer assisted translation. Project Tech. Rep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>H Tolga Ilhan</author>
<author>Christopher Manning</author>
</authors>
<title>Extensions to hmm-based statistical word alignment models.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="20280" citStr="Toutanova et al., 2002" startWordPosition="3456" endWordPosition="3459">s of the direct HMM-based model are estimated analogously to those of the inverse HMM-based model. Once the direct and the inverse HMM-based model parameters have been modified due to the presentation of a new sentence pair to the IMT system, both models are used to obtain word alignments for the new sentence pair. The resulting direct and inverse word alignment matrices are combined by means of the symmetrization alignment operation (Och and Ney, 2003) before extracting the set of consistent phrase pairs. HMM-based alignment models are used here because, according to (Och and Ney, 2003) and (Toutanova et al., 2002), they outperform IBM 1 to IBM 4 alignment models while still allowing the exact calculation of the likelihood for a given sentence pair. The δ parameters of the geometric distributions associated to the feature functions h5, h6 and h7 are left fixed. Because of this, there are no sufficient statistics to store for these feature functions. Finally, the weights of the log-linear combination are not modified due to the presentation of a new sentence pair to the system. These weights can be adjusted off-line by means of a development corpus and well-known optimization techniques. 5 Experiments Th</context>
</contexts>
<marker>Toutanova, Ilhan, Manning, 2002</marker>
<rawString>Kristina Toutanova, H. Tolga Ilhan, and Christopher Manning. 2002. Extensions to hmm-based statistical word alignment models. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Vidal</author>
<author>L Rodriguez</author>
<author>F Casacuberta</author>
<author>I GarciaVarea</author>
</authors>
<title>Interactive pattern recognition.</title>
<date>2007</date>
<booktitle>In Proc. of the 4th MLMI,</booktitle>
<pages>60--71</pages>
<location>Brno, Czech Republic,</location>
<contexts>
<context position="4038" citStr="Vidal et al., 2007" startWordPosition="641" endWordPosition="644">rent suggestion. In this paper, we also focus on the IMT framework. Specifically, we present an IMT system that is able to learn from user feedback. For this purpose, we apply the online learning paradigm to the IMT framework. The online learning techniques that we propose here allow the statistical models involved in the translation process to be incrementally updated. 546 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 546–554, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Figure 2 (inspired from (Vidal et al., 2007)) shows a schematic view of these ideas. Here, f is the input sentence and e is the output derived by the IMT system from f. By observing f and e, the user interacts with the IMT system until the desired output e� is produced. The input sentence f and its desired translation e� can be used to refine the models used by the system. In general, the model is initially obtained through a classical batch training process from a previously given training sequence of pairs (fi,ei) from the task being considered. Now, the models can be extended with the use of valuable user feedback. 2 Interactive mach</context>
<context position="29381" citStr="Vidal et al., 2007" startWordPosition="4975" endWordPosition="4979">ters have not been tackled here. For the future we plan to incorporate this functionality into our IMT system. The incremental techniques proposed here can also be exploited to extend SMT systems (in fact, our proposed IMT system is based on an incrementally updateable SMT system). For the near future we plan to study possible aplications of our techniques in a fully automatic translation scenario. Finally, it is worthy of note that the main ideas presented here can be used in other interactive applications such as Computer Assisted Speech Transcription, Interactive Image Retrieval, etc (see (Vidal et al., 2007) for more information). In conclusion, we think that the online learning techniques proposed here can be the starting point for a new generation of interactive pattern recognition systems that are able to take advantage of user feedback. Acknowledgments Work supported by the EC (FEDER/FSE), the Spanish Government (MEC, MICINN, MITyC, MAEC, ”Plan E”, under grants MIPRCV ”Consolider Ingenio 2010” CSD2007-00018, iTrans2 TIN2009-14511, erudito.com TSI-020110-2009- 439), the Generalitat Valenciana (grant Prometeo/2009/014), the Univ. Polit´ecnica de Valencia (grant 20091027) and the Spanish JCCM (g</context>
</contexts>
<marker>Vidal, Rodriguez, Casacuberta, GarciaVarea, 2007</marker>
<rawString>E. Vidal, L. Rodriguez, F. Casacuberta, and I. GarciaVarea. 2007. Interactive pattern recognition. In Proc. of the 4th MLMI, pages 60–71. Brno, Czech Republic, 28-30 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="12166" citStr="Vogel et al., 1996" startWordPosition="2047" endWordPosition="2050">ngth 1). We use a specific normal distribution with mean µ|e |and standard deviation σ|e |for each possible target sentence length |e|. • inverse and direct phrase-based models (h3, h4) h3(e, a, f) = log(HKk=1 p(�fk|�eak)), where �fk|eak) is defined as follows: p(�fk|�eak) = β · pphr(�fk|�eak) + (1 − β).phmm(�fk|�eak) (9) In Equation (9), pphr(�fk|�eak) denotes the probability given by a statistical phrase-based dictionary used in regular phrase-based models (see (Koehn et al., 2003) for more details). phmm( �fk|eak) is the probability given by an HMM-based (intraphrase) alignment model (see (Vogel et al., 1996)): p(�fj|�ea;) · p(aj|aj−1, |�e|) (10) The HMM-based alignment model probability is used here for smoothing purposes as described in (Ortiz-Martinez et al., 2009). Analogously h4 is defined as: h4(e, a, f) = log(HKk=1 p(�eak |A)) • target phrase-length model (h5) h5(e, a, f) = log(HKk=1 p(|4|)), where p(|4|) = δ(1 − δ)|6k|. h5 implements a target phrase-length model by means of a geometric distribution with probability of success on each trial δ. The use of a geometric distribution penalizes the length of target phrases. • source phrase-length model (h6) h6(e, a, f) = log(HKk=1 p(|�fk �eak|)),</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proc. of COLING, pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
<author>J Suzuki</author>
<author>H Tsukada</author>
<author>H Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP and CoNLL,</booktitle>
<pages>764--733</pages>
<location>Prage, Czeck Republic.</location>
<contexts>
<context position="8812" citStr="Watanabe et al., 2007" startWordPosition="1498" endWordPosition="1501">m involved in the interactive translation process. Then we introduce the required techniques to incrementally update the statistical models used by the system. 3 Related work In this paper we present an application of the online learning paradigm to the IMT framework. In the online learning setting, models are trained sample by sample. Our work is also related to model adaptation, although model adaptation and online learning are not exactly the same thing. The online learning paradigm has been previously applied to train discriminative models in SMT (Liang et al., 2006; Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2008). These works differ from the one presented here in that we apply online learning techniques to train generative models instead of discriminative models. In (Nepveu et al., 2004), dynamic adaptation of an IMT system via cache-based model extensions to language and translation models is proposed. The work by Nepveu et al. (2004) constitutes a domain adaptation technique and not an online learning technique, since the proposed cache components require pre-existent models estimated in batch mode. In addition to this, their IMT system does not use state-of-the-art models. To </context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. of EMNLP and CoNLL, pages 764–733, Prage, Czeck Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>