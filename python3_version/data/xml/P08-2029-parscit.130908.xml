<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.992693">
Kernels on Linguistic Structures for Answer Extraction
</title>
<author confidence="0.994973">
Alessandro Moschitti and Silvia Quarteroni
</author>
<affiliation confidence="0.995966">
DISI, University of Trento
</affiliation>
<address confidence="0.9188805">
Via Sommarive 14
38100 POVO (TN) - Italy
</address>
<email confidence="0.999197">
{moschitti,silviaq}@disi.unitn.it
</email>
<sectionHeader confidence="0.998557" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994555181818182">
Natural Language Processing (NLP) for Infor-
mation Retrieval has always been an interest-
ing and challenging research area. Despite the
high expectations, most of the results indicate
that successfully using NLP is very complex.
In this paper, we show how Support Vector
Machines along with kernel functions can ef-
fectively represent syntax and semantics. Our
experiments on question/answer classification
show that the above models highly improve on
bag-of-words on a TREC dataset.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99852025">
Question Answering (QA) is an IR task where the
major complexity resides in question processing
and answer extraction (Chen et al., 2006; Collins-
Thompson et al., 2004) rather than document re-
trieval (a step usually carried out by off-the shelf IR
engines). In question processing, useful information
is gathered from the question and a query is created.
This is submitted to an IR module, which provides
a ranked list of relevant documents. From these, the
QA system extracts one or more candidate answers,
which can then be re-ranked following various crite-
ria. Although typical methods are based exclusively
on word similarity between query and answer, recent
work, e.g. (Shen and Lapata, 2007) has shown that
shallow semantic information in the form of predi-
cate argument structures (PASs) improves the auto-
matic detection of correct answers to a target ques-
tion. In (Moschitti et al., 2007), we proposed the
Shallow Semantic Tree Kernel (SSTK) designed to
encode PASs1 in SVMs.
</bodyText>
<footnote confidence="0.96928">
1in PropBank format, (www.cis.upenn.edu/˜ace).
</footnote>
<bodyText confidence="0.999879964285714">
In this paper, similarly to our previous approach,
we design an SVM-based answer extractor, that se-
lects the correct answers from those provided by a
basic QA system by applying tree kernel technol-
ogy. However, we also provide: (i) a new kernel
to process PASs based on the partial tree kernel al-
gorithm (PAS-PTK), which is highly more efficient
and more accurate than the SSTK and (ii) a new ker-
nel called Part of Speech sequence kernel (POSSK),
which proves very accurate to represent shallow syn-
tactic information in the learning algorithm.
To experiment with our models, we built two
different corpora, WEB-QA and TREC-QA by us-
ing the description questions from TREC 2001
(Voorhees, 2001) and annotating the answers re-
trieved from Web resp. TREC data (available at
disi.unitn.it/˜silviaq). Comparative exper-
iments with re-ranking models of increasing com-
plexity show that: (a) PAS-PTK is far more efficient
and effective than SSTK, (b) POSSK provides a re-
markable further improvement on previous models.
Finally, our experiments on the TREC-QA dataset,
un-biased by the presence of typical Web phrasings,
show that BOW is inadequate to learn relations be-
tween questions and answers. This is the reason
why our kernels on linguistic structures improve it
by 63%, which is a remarkable result for an IR task
(Allan, 2000).
</bodyText>
<sectionHeader confidence="0.997757" genericHeader="method">
2 Kernels for Q/A Classification
</sectionHeader>
<bodyText confidence="0.9994858">
The design of an answer extractor basically depends
on the design of a classifier that decides if an an-
swer correctly responds to the target question. We
design a classifier based on SVMs and different ker-
nels applied to several forms of question and answer
</bodyText>
<page confidence="0.992775">
113
</page>
<reference confidence="0.210256">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 113–116,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<figureCaption confidence="0.9578645">
Figure 1: Compact PAS-PTK structures of s1 (a) and s2 (b) and some fragments they have in common as produced by
the PTK (c). Arguments are replaced with their most important word (or semantic head) to reduce data sparseness.
</figureCaption>
<figure confidence="0.9855094">
(c)
(b)
(a)
PAS
rel
characterize
PAS
rel
characterize
A1
inattention
PAS
PAS
rel
characterize
PAS
A1 rel A0
PAS
A1 rel
characterize
rel
A0 characterize
characterize
A1
autism
A0
spectrum
A0
behavior
R-A0
that
A1
disorder
A0
anxiety
rel
characterize
PAS
rel
representations:
</figure>
<listItem confidence="0.9602152">
(1) linear kernels on the bag-of-words (BOW) or
bag-of-POS-tags (POS) features,
(2) the String Kernel (SK) (Shawe-Taylor and Cris-
tianini, 2004) on word sequences (WSK) and POS-
tag sequences (POSSK),
(3) the Syntactic Tree Kernel (STK) (Collins and
Duffy, 2002) on syntactic parse trees (PTs),
(4) the Shallow Semantic Tree Kernel (SSTK) (Mos-
chitti et al., 2007) and the Partial Tree Kernel (PTK)
(Moschitti, 2006) on PASs.
</listItem>
<bodyText confidence="0.9994018">
In particular, POS-tag sequences and PAS trees
used with SK and PTK yield to two innovative ker-
nels, i.e. POSSK and PAS-PTK2. In the next sec-
tions, we describe in more detail the data structures
on which we applied the above kernels.
</bodyText>
<subsectionHeader confidence="0.995558">
2.1 Syntactic Structures
</subsectionHeader>
<bodyText confidence="0.9843364">
The POSSK is obtained by applying the String Ker-
nel on the sequence of POS-tags of a question or
a answer. For example, given sentence s0: What
is autism?, the associated POS sequence is WP
AUX NN ? and some of the substrings extracted by
POSSK are WP NN or WP AUX. A more complete
structure is the full parse tree (PT) of the sentence,
that constitutes the input of the STK. For instance,
the STK accepts the syntactic parse: (SBARQ (WHNP
(WP What))(SQ (VP (AUX is)(NP (NN autism))))(. ?)).
</bodyText>
<subsectionHeader confidence="0.999628">
2.2 Semantic Structures
</subsectionHeader>
<bodyText confidence="0.921855222222222">
The intuition behind our semantic representation is
the idea that when we ignore the answer to a def-
inition question we check whether such answer is
formulated as a “typical” definition and whether an-
swers defining similar concepts are expressed in a
2For example, let PTK(t1,t2) = φ(t1) · φ(t2), where t1
and t2 are two syntactic parse trees. If we map t1 and t2
into two new shallow semantic trees s1 and s2 with a map-
ping φM(·), we obtain: PTK(s1,s2) = φ(s1) · φ(s2) =
</bodyText>
<equation confidence="0.574785666666667">
φ(φM(t1)) · φ(φM(t2)) = φ′(t1) · φ′(t2)=PAS-PTK(t1,t2),
which is a noticeably different kernel induced by the mapping
φ′ = φ ◦ φM.
</equation>
<bodyText confidence="0.996435">
similar way.
To take advantage of semantic representations, we
work with two types of semantic structures; first,
the Word Sequence Kernel applied to both ques-
tion and answer; given s0, sample substrings are:
What is autism, What is, What autism, is autism,
etc. Then, two PAS-based trees: Shallow Seman-
tic Trees for SSTK and Shallow Semantic Trees for
PTK, both based on PropBank structures (Kings-
bury and Palmer, 2002) are automatically generated
by our SRL system (Moschitti et al., 2005). As an
example, let us consider an automatically annotated
sentence from our TREC-QA corpus:
</bodyText>
<construct confidence="0.79972675">
s1: [A1 Autism] is [Tel characterized] [A0 by a broad
spectrum of behavior] [R−A0 that] [Telincludes] [A1 ex-
treme inattention to surroundings and hypersensitivity to
sound and other stimuli].
</construct>
<bodyText confidence="0.985642583333333">
Such annotation can be used to design a shallow se-
mantic representation that can be matched against
other semantically similar sentences, e.g.
s2: [A1 Panic disorder] is [Tel characterized] [A0 by un-
realistic or excessive anxiety].
It can be observed here that, although autism is a
different disease from panic disorder, the structure
of both definitions and the latent semantics they con-
tain (inherent to behavior, disorder, anxiety) are sim-
ilar. So for instance, s2 appears as a definition even
to someone who only knows what the definition of
autism looks like.
The above annotation can be compactly repre-
sented by predicate argument structure trees (PASs)
such as those in Figure 1. Here, we can notice that
the semantic similarity between sentences is explic-
itly visible in terms of common fragments extracted
by the PTK from their respective PASs. Instead,
the similar PAS-SSTK representation in (Moschitti
et al., 2007) does not take argument order into ac-
count, thus it fails to capture the linguistic ratio-
nale expressed above. Moreover, it is much heavier,
causing large memory occupancy and, as shown by
our experiments, much longer processing time.
</bodyText>
<page confidence="0.997656">
114
</page>
<sectionHeader confidence="0.999682" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9996346">
In our experiments we show that (a) the PAS-PTK
shallow semantic tree kernel is more efficient and ef-
fective than the SSTK proposed in (Moschitti et al.,
2007), and (b) our POSSK jointly used with PAS-
PTK and STK greatly improves on BOW.
</bodyText>
<subsectionHeader confidence="0.993603">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999973433333333">
In our experiments, we implemented the BOW and
POS kernels, WSK, POSSK, STK (on syntactic
PTs derived automatically with Charniak’s parser),
SSTK and PTK (on PASs derived automatically with
our SRL system) as well as their combinations in
SVM-light-TK3. Since answers often contain more
than one PAS (see Figure 1), we sum PTK (or SSTK)
applied to all pairs P1 xP2, P1 and P2 being the sets
of PASs of the first two answers.
The experimental datasets were created by sub-
mitting the 138 TREC 2001 test questions labeled as
“description” in (Li and Roth, 2002) to our basic QA
system, YourQA (Quarteroni and Manandhar, 2008)
and by gathering the top 20 answer paragraphs.
YourQA was run on two sources: Web docu-
ments by exploiting Google (code.google.com/
apis/) and the AQUAINT data used for TREC’07
(trec.nist.gov/data/qa) by exploiting Lucene
(lucene.apache.org), yielding two different cor-
pora: WEB-QA and TREC-QA. Each sentence of
the returned paragraphs was manually evaluated
based on whether it contained a correct answer to
the corresponding question. To simplify our task,
we isolated for each paragraph the sentence with the
maximal judgment (such as s1 and s2 in Sec. 2.2)
and labeled it as positive if it answered the question
either concisely or with noise, negative otherwise.
The resulting WEB-QA corpus contains 1309 sen-
tences, 416 of which positive; the TREC-QA corpus
contains 2256 sentences, 261 of which positive.
</bodyText>
<subsectionHeader confidence="0.805342">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.9998006">
In a first experiment, we compared the learning and
classification efficiency of SVMs on PASs by apply-
ing either solely PAS-SSTK or solely PAS-PTK on
the WEB-QA and TREC-QA sets. We divided the
training data in 9 bins of increasing size (with a step
</bodyText>
<footnote confidence="0.9010485">
3Toolkit available at dit.unitn.it/moschitti/, based
on SVM-light (Joachims, 1999)
</footnote>
<figureCaption confidence="0.9996">
Figure 2: Efficiency of PTK and SSTK
</figureCaption>
<figure confidence="0.9965275">
1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0
cost-factor
</figure>
<figureCaption confidence="0.997921">
Figure 3: Impact of different kernels on WEB-QA
</figureCaption>
<figure confidence="0.990181">
4 6 8 10 12 14 16 18 20
cost-factor
</figure>
<figureCaption confidence="0.99999">
Figure 4: Impact of different kernels on TREC-QA
</figureCaption>
<bodyText confidence="0.9982755">
of 200) and measured the training and test time4 for
each bin. Figure 2 shows that in both the test and
training phases, PTK is much faster than SSTK. In
training, PTK is 40 times faster, enabling the exper-
imentation of SVMs with large datasets. This differ-
ence is due to the combination of our lighter seman-
tic structures and the PTK’s ability to extract from
these at least the same information that SSTK de-
rives from much larger structures.
Further interesting experiments regard the accu-
</bodyText>
<footnote confidence="0.86271">
4Processing time in seconds of a Mac-Book Pro 2.4 Ghz.
</footnote>
<figure confidence="0.990129816326531">
240
220
200
180
160
140
120
100
80
PTK (t.lrlrg) PTK (te6t)
60
SSTK (te6t) SSTK (t.lrlrg)
40
20
0
200 400 600 800 1000 1200 1400 1600 1800
T�wlrlrg Set Slze
TIme In Seconds
FS-measure
69
68
67
66
65
64
63
62
61
60
P +WSK+PAS(P K) P
P +BOW P +POS
P +WSK WSK
BOW P +WSK+PAS(SS K)
FS-measure
40
38
36
34
32
30
28
26
24
22
20
PT POS+PT
POSSK+PT POSSK+PT+PAS-PTK
BOW+PT BOW+POS+PT
BOW POSSK+PT+PAS-SSTK
</figure>
<page confidence="0.995023">
115
</page>
<bodyText confidence="0.999978771428572">
racy tests of different kernels and some of their most
promising combinations. As a kernel operator, we
applied the sum between kernels5 that yields the
joint feature space of the individual kernels (Shawe-
Taylor and Cristianini, 2004).
Figure 3 shows the F1-plots of several kernels ac-
cording to different cost-factor values (i.e. different
Precision/Recall rates). Each F1 value is the average
of 5 fold cross-validation. We note that (a) BOW
achieves very high accuracy, comparable to the one
produced by PT; (b) the BOW+PT combination im-
proves on both single models; (c) WSK improves on
BOW and it is enhanced by WSK+PT, demonstrat-
ing that word sequences and PTs are very relevant
for this task; (d) both PAS-SSTK and PAS-PTK im-
prove on previous models yielding the highest result.
The high accuracy of BOW is surprising as sup-
port vectors are compared with test examples which
are in general different (there are no questions
shared between training and test set). The explana-
tion resides in the fact that WEB-QA contains com-
mon BOW patterns due to typical Web phrasings,
e.g. Learn more about X, that facilitate the de-
tection of incorrect answers.
Hence, to have un-biased results, we experi-
mented with the TREC corpus which is cleaner from
a linguistic viewpoint and also more complex from
a QA perspective. A comparative analysis of Fig-
ure 4 suggests that: (a) the F1 of all models is much
lower than for the WEB-QA dataset; (b) BOW de-
notes the lowest accuracy; (c) POS combined with
PT improves on PT; (d) POSSK+PT improves on
POS+PT; (f) finally, PAS adds further information
as the best model is POSSK+PT+PAS-PTK (or PAS-
SSTK).
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999876875">
With respect to our previous findings, experimenting
with TREC-QA allowed us to show that BOW is not
relevant to learn re-ranking functions from exam-
ples; indeed, while it is useful to establish an initial
ranking by measuring the similarity between ques-
tion and answer, BOW is almost irrelevant to grasp
typical rules that suggest if a description is valid or
not. Moreover, using the new POSSK and PAS-PTK
</bodyText>
<footnote confidence="0.576336">
5All adding kernels are normalized to have a similarity score
</footnote>
<bodyText confidence="0.522141">
between 0 and 1, i.e. K′(X1, X2) = K(X1,X2)
</bodyText>
<equation confidence="0.660065">
√K(X1 ,X1) × K(X2,X2)
</equation>
<bodyText confidence="0.999355666666667">
kernels provides an improvement of 5 absolute per-
cent points wrt our previous work.
Finally, error analysis revealed that PAS-PTK can
provide patterns like A1(X) R-A1(that) rel(result)
A1(Y) and A1(X) rel(characterize) A0(Y), where X
and Y need not necessarily be matched.
</bodyText>
<sectionHeader confidence="0.999124" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.53373975">
This work was partly supported by the FP6 IST LUNA
project (contract No. 33549) and by the European
Commission Marie Curie Excellence Grant for the
ADAMACH project (contract No. 022593).
</reference>
<sectionHeader confidence="0.979548" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852052631579">
J. Allan. 2000. Natural language processing for informa-
tion retrieval. In Proceedings of NAACL/ANLP (tuto-
rial notes).
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models. In
ACL’06.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL’02.
K. Collins-Thompson, J. Callan, E. Terra, and C. L.A.
Clarke. 2004. The effect of document retrieval quality
on factoid QA performance. In SIGIR’04.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In LREC’02.
X. Li and D. Roth. 2002. Learning question classifiers.
In ACL’02.
A. Moschitti, B. Coppola, A. Giuglea, and R. Basili.
2005. Hierarchical semantic role labeling. In CoNLL
2005 shared task.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-
har. 2007. Exploiting syntactic and shallow semantic
kernels for question/answer classification. In ACL’07.
A. Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In
ECML’06.
S. Quarteroni and S. Manandhar. 2008. Designing an
interactive open domain question answering system.
Journ. ofNat. Lang. Eng. (in press).
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In EMNLP-CoNLL.
E. M. Voorhees. 2001. Overview of the TREC 2001
Question Answering Track. In TREC’01.
</reference>
<page confidence="0.999025">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931037">
<title confidence="0.999571">Kernels on Linguistic Structures for Answer Extraction</title>
<author confidence="0.998183">Moschitti Quarteroni</author>
<affiliation confidence="0.999947">DISI, University of Trento</affiliation>
<address confidence="0.981445">Via Sommarive 14 38100 POVO (TN) - Italy</address>
<abstract confidence="0.99731325">Natural Language Processing (NLP) for Information Retrieval has always been an interesting and challenging research area. Despite the high expectations, most of the results indicate that successfully using NLP is very complex. In this paper, we show how Support Vector Machines along with kernel functions can effectively represent syntax and semantics. Our experiments on question/answer classification show that the above models highly improve on bag-of-words on a TREC dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>113--116</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 113–116,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<title>c�2008 Association for Computational Linguistics This work was partly supported by the FP6 IST LUNA project (contract No. 33549) and by the European Commission Marie Curie Excellence Grant for the ADAMACH project (contract</title>
<date>2008</date>
<tech>No. 022593).</tech>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics This work was partly supported by the FP6 IST LUNA project (contract No. 33549) and by the European Commission Marie Curie Excellence Grant for the ADAMACH project (contract No. 022593).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>Natural language processing for information retrieval.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL/ANLP (tutorial notes).</booktitle>
<marker>Allan, 2000</marker>
<rawString>J. Allan. 2000. Natural language processing for information retrieval. In Proceedings of NAACL/ANLP (tutorial notes).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>M Zhou</author>
<author>S Wang</author>
</authors>
<title>Reranking answers from definitional QA using language models.</title>
<date>2006</date>
<booktitle>In ACL’06.</booktitle>
<contexts>
<context position="841" citStr="Chen et al., 2006" startWordPosition="118" endWordPosition="121">e Processing (NLP) for Information Retrieval has always been an interesting and challenging research area. Despite the high expectations, most of the results indicate that successfully using NLP is very complex. In this paper, we show how Support Vector Machines along with kernel functions can effectively represent syntax and semantics. Our experiments on question/answer classification show that the above models highly improve on bag-of-words on a TREC dataset. 1 Introduction Question Answering (QA) is an IR task where the major complexity resides in question processing and answer extraction (Chen et al., 2006; CollinsThompson et al., 2004) rather than document retrieval (a step usually carried out by off-the shelf IR engines). In question processing, useful information is gathered from the question and a query is created. This is submitted to an IR module, which provides a ranked list of relevant documents. From these, the QA system extracts one or more candidate answers, which can then be re-ranked following various criteria. Although typical methods are based exclusively on word similarity between query and answer, recent work, e.g. (Shen and Lapata, 2007) has shown that shallow semantic informa</context>
</contexts>
<marker>Chen, Zhou, Wang, 2006</marker>
<rawString>Y. Chen, M. Zhou, and S. Wang. 2006. Reranking answers from definitional QA using language models. In ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL’02.</booktitle>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>E Terra</author>
<author>C L A Clarke</author>
</authors>
<title>The effect of document retrieval quality on factoid QA performance.</title>
<date>2004</date>
<booktitle>In SIGIR’04.</booktitle>
<marker>Collins-Thompson, Callan, Terra, Clarke, 2004</marker>
<rawString>K. Collins-Thompson, J. Callan, E. Terra, and C. L.A. Clarke. 2004. The effect of document retrieval quality on factoid QA performance. In SIGIR’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Scholkopf, C. Burges, and A. Smola, editors,</editor>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Scholkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In LREC’02.</booktitle>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>P. Kingsbury and M. Palmer. 2002. From Treebank to PropBank. In LREC’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In ACL’02.</booktitle>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning question classifiers. In ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>B Coppola</author>
<author>A Giuglea</author>
<author>R Basili</author>
</authors>
<title>Hierarchical semantic role labeling.</title>
<date>2005</date>
<booktitle>In CoNLL</booktitle>
<note>shared task.</note>
<marker>Moschitti, Coppola, Giuglea, Basili, 2005</marker>
<rawString>A. Moschitti, B. Coppola, A. Giuglea, and R. Basili. 2005. Hierarchical semantic role labeling. In CoNLL 2005 shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
<author>R Basili</author>
<author>S Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question/answer classification.</title>
<date>2007</date>
<booktitle>In ACL’07.</booktitle>
<contexts>
<context position="1599" citStr="Moschitti et al., 2007" startWordPosition="241" endWordPosition="244">ocessing, useful information is gathered from the question and a query is created. This is submitted to an IR module, which provides a ranked list of relevant documents. From these, the QA system extracts one or more candidate answers, which can then be re-ranked following various criteria. Although typical methods are based exclusively on word similarity between query and answer, recent work, e.g. (Shen and Lapata, 2007) has shown that shallow semantic information in the form of predicate argument structures (PASs) improves the automatic detection of correct answers to a target question. In (Moschitti et al., 2007), we proposed the Shallow Semantic Tree Kernel (SSTK) designed to encode PASs1 in SVMs. 1in PropBank format, (www.cis.upenn.edu/˜ace). In this paper, similarly to our previous approach, we design an SVM-based answer extractor, that selects the correct answers from those provided by a basic QA system by applying tree kernel technology. However, we also provide: (i) a new kernel to process PASs based on the partial tree kernel algorithm (PAS-PTK), which is highly more efficient and more accurate than the SSTK and (ii) a new kernel called Part of Speech sequence kernel (POSSK), which proves very </context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In ACL’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML’06.</booktitle>
<marker>Moschitti, 2006</marker>
<rawString>A. Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Quarteroni</author>
<author>S Manandhar</author>
</authors>
<title>Designing an interactive open domain question answering system.</title>
<date>2008</date>
<journal>Journ. ofNat. Lang. Eng.</journal>
<note>(in press).</note>
<marker>Quarteroni, Manandhar, 2008</marker>
<rawString>S. Quarteroni and S. Manandhar. 2008. Designing an interactive open domain question answering system. Journ. ofNat. Lang. Eng. (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<publisher>Cambridge University</publisher>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press. D. Shen and M. Lapata. 2007. Using semantic roles to improve question answering. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Question Answering Track.</title>
<date>2001</date>
<journal>Overview of the TREC</journal>
<booktitle>In TREC’01.</booktitle>
<contexts>
<context position="2426" citStr="Voorhees, 2001" startWordPosition="377" endWordPosition="378">wer extractor, that selects the correct answers from those provided by a basic QA system by applying tree kernel technology. However, we also provide: (i) a new kernel to process PASs based on the partial tree kernel algorithm (PAS-PTK), which is highly more efficient and more accurate than the SSTK and (ii) a new kernel called Part of Speech sequence kernel (POSSK), which proves very accurate to represent shallow syntactic information in the learning algorithm. To experiment with our models, we built two different corpora, WEB-QA and TREC-QA by using the description questions from TREC 2001 (Voorhees, 2001) and annotating the answers retrieved from Web resp. TREC data (available at disi.unitn.it/˜silviaq). Comparative experiments with re-ranking models of increasing complexity show that: (a) PAS-PTK is far more efficient and effective than SSTK, (b) POSSK provides a remarkable further improvement on previous models. Finally, our experiments on the TREC-QA dataset, un-biased by the presence of typical Web phrasings, show that BOW is inadequate to learn relations between questions and answers. This is the reason why our kernels on linguistic structures improve it by 63%, which is a remarkable resu</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>E. M. Voorhees. 2001. Overview of the TREC 2001 Question Answering Track. In TREC’01.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>