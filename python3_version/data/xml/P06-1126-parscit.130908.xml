<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9992315">
Discriminative Pruning of Language Models for
Chinese Word Segmentation
</title>
<author confidence="0.999111">
Jianfeng Li Haifeng Wang Dengjun Ren Guohua Li
</author>
<affiliation confidence="0.990389">
Toshiba (China) Research and Development Center
</affiliation>
<address confidence="0.901586333333333">
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District
Beijing, 100738, China
{lijianfeng, wanghaifeng, rendengjun,
</address>
<email confidence="0.996857">
liguohua}@rdc.toshiba.com.cn
</email>
<sectionHeader confidence="0.995623" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838590909091">
This paper presents a discriminative
pruning method of n-gram language
model for Chinese word segmentation.
To reduce the size of the language model
that is used in a Chinese word segmenta-
tion system, importance of each bigram is
computed in terms of discriminative
pruning criterion that is related to the per-
formance loss caused by pruning the bi-
gram. Then we propose a step-by-step
growing algorithm to build the language
model of desired size. Experimental re-
sults show that the discriminative pruning
method leads to a much smaller model
compared with the model pruned using
the state-of-the-art method. At the same
Chinese word segmentation F-measure,
the number of bigrams in the model can
be reduced by up to 90%. Correlation be-
tween language model perplexity and
word segmentation performance is also
discussed.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991832">
Chinese word segmentation is the initial stage of
many Chinese language processing tasks, and
has received a lot of attention in the literature
(Sproat et al., 1996; Sun and Tsou, 2001; Zhang
et al., 2003; Peng et al., 2004). In Gao et al.
(2003), an approach based on source-channel
model for Chinese word segmentation was pro-
posed. Gao et al. (2005) further developed it to a
linear mixture model. In these statistical models,
language models are essential for word segmen-
tation disambiguation. However, an uncom-
pressed language model is usually too large for
practical use since all realistic applications have
memory constraints. Therefore, language model
pruning techniques are used to produce smaller
models. Pruning a language model is to eliminate
a number of parameters explicitly stored in it,
according to some pruning criteria. The goal of
research for language model pruning is to find
criteria or methods, using which the model size
could be reduced effectively, while the perform-
ance loss is kept as small as possible.
A few criteria have been presented for lan-
guage model pruning, including count cut-off
(Jelinek, 1990), weighted difference factor
(Seymore and Rosenfeld, 1996), Kullback-
Leibler distance (Stolcke, 1998), rank and en-
tropy (Gao and Zhang, 2002). These criteria are
general for language model pruning, and are not
optimized according to the performance of lan-
guage model in specific tasks.
In recent years, discriminative training has
been introduced to natural language processing
applications such as parsing (Collins, 2000), ma-
chine translation (Och and Ney, 2002) and lan-
guage model building (Kuo et al., 2002; Roark et
al., 2004). To the best of our knowledge, it has
not been applied to language model pruning.
In this paper, we propose a discriminative
pruning method of n-gram language model for
Chinese word segmentation. It differentiates
from the previous pruning approaches in two
respects. First, the pruning criterion is based on
performance variation of word segmentation.
Second, the model of desired size is achieved by
adding valuable bigrams to a base model, instead
of by pruning bigrams from an unpruned model.
We define a misclassification function that
approximately represents the likelihood that a
sentence will be incorrectly segmented. The
</bodyText>
<page confidence="0.905006">
1001
</page>
<note confidence="0.5205925">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1001–1008,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999884333333333">
variation value of the misclassification function
caused by adding a parameter to the base model
is used as the criterion for model pruning. We
also suggest a step-by-step growing algorithm
that can generate models of any reasonably de-
sired size. We take the pruning method based on
Kullback-Leibler distance as the baseline. Ex-
perimental results show that our method outper-
forms the baseline significantly with small model
size. With the F-Measure of 96.33%, number of
bigrams decreases by up to 90%. In addition, by
combining the discriminative pruning method
with the baseline method, we obtain models that
achieve better performance for any model size.
Correlation between language model perplexity
and system performance is also discussed.
The remainder of the paper is organized as fol-
lows. Section 2 briefly discusses the related work
on language model pruning. Section 3 proposes
our discriminative pruning method for Chinese
word segmentation. Section 4 describes the ex-
perimental settings and results. Result analysis
and discussions are also presented in this section.
We draw the conclusions in section 5.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999860625">
A simple way to reduce the size of an n-gram
language model is to exclude those n-grams oc-
curring infrequently in training corpus. It is
named as count cut-off method (Jelinek, 1990).
Because counts are always integers, the size of
the model can only be reduced to discrete values.
Gao and Lee (2000) proposed a distribution-
based pruning. Instead of pruning n-grams that
are infrequent in training data, they prune n-
grams that are likely to be infrequent in a new
document. Experimental results show that it is
better than traditional count cut-off method.
Seymore and Rosenfeld (1996) proposed a
method to measure the difference of the models
before and after pruning each n-gram, and the
difference is computed as:
</bodyText>
<subsectionHeader confidence="0.858274">
−N(hj,wi)x [log P′(wi |hj)− log P(wi |hj)] (1)
</subsectionHeader>
<bodyText confidence="0.996146833333333">
Where P(wi|hj) denotes the conditional prob-
abilities assigned by the original model, and
P′(wi|hj) denotes the probabilities in the pruned
model. N(hj, wi) is the discounted frequency of n-
gram event hjwi. Seymore and Rosenfeld (1996)
showed that this method is more effective than
the traditional cut-off method.
Stolcke (1998) presented a more sound crite-
rion for computing the difference of models be-
fore and after pruning each n-gram, which is
called relative entropy or Kullback-Leibler dis-
tance. It is computed as:
</bodyText>
<equation confidence="0.995438">
− E P(wi, hj )[log P′(wi  |hj) −
wi,hj
</equation>
<bodyText confidence="0.9989938">
The sum is over all words wi and histories hj.
This criterion removes some of the approxima-
tions employed in Seymore and Rosenfeld
(1996). In addition, Stolcke (1998) presented a
method for efficient computation of the Kull-
back-Leibler distance of each n-gram.
In Gao and Zhang (2002), three measures are
studied for the purpose of language model prun-
ing. They are probability, rank, and entropy.
Among them, probability is very similar to that
proposed by Seymore and Rosenfeld (1996). Gao
and Zhang (2002) also presented a method of
combining two criteria, and showed the combi-
nation of rank and entropy achieved the smallest
models.
</bodyText>
<sectionHeader confidence="0.8631825" genericHeader="method">
3 Discriminative Pruning for Chinese
Word Segmentation
</sectionHeader>
<subsectionHeader confidence="0.999825">
3.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.99248375">
In this paper, discussions are restricted to bigram
language model P(wy|wx). In a bigram model,
three kinds of parameters are involved: bigram
probability Pm(wy|wx) for seen bigram wxwy in
training corpus, unigram probability Pm(w) and
backoff coefficient αm(w) for any word w. For
any wx and wy in the vocabulary, bigram prob-
ability P(wy|wx) is computed as:
</bodyText>
<equation confidence="0.996853">
P w w
(  |) if c w w
( , )
m y x x y
P(wy  |wO= tαm(wx)x Pm(wy) if c(wx,wy
</equation>
<bodyText confidence="0.999967833333333">
As equation (3) shows, the probability of an
unseen bigram is computed by the product of the
unigram probability and the corresponding back-
off coefficient. If we remove a seen bigram from
the model, we can still yield a bigram probability
for it, by regarding it as an unseen bigram. Thus,
we can reduce the number of bigram probabili-
ties explicitly stored in the model. By doing this,
model size decreases. This is the foundation for
bigram model pruning.
The research issue is to find an effective crite-
rion to compute &amp;quot;importance&amp;quot; of each bigram.
Here, &amp;quot;importance&amp;quot; indicates the performance
loss caused by pruning the bigram. Generally,
given a target model size, the method for lan-
guage model pruning is described in Figure 1.
In fact, deciding which bigrams should be ex-
cluded from the model is equivalent to deciding
</bodyText>
<equation confidence="0.636312">
j
)] (2)
log P(wi  |h
�
)
0
1
0
(3)
=
</equation>
<page confidence="0.97581">
1002
</page>
<bodyText confidence="0.999934066666667">
which bigrams should be included in the model.
Hence, we suggest a growing algorithm through
which a model of desired size can also be
achieved. It is illustrated in Figure 2. Here, two
terms are introduced. Full-bigram model is the
unpruned model containing all seen bigrams in
training corpus. And base model is currently the
unigram model.
For the discriminative pruning method sug-
gested in this paper, growing algorithm instead
of pruning algorithm is applied to generate the
model of desired size. In addition, &amp;quot;importance&amp;quot;
of each bigram indicates the performance im-
provement caused by adding a bigram into the
base model.
</bodyText>
<listItem confidence="0.9985329">
1. Given the desired model size, compute
the number of bigrams that should be
pruned. The number is denoted as m;
2. Compute &amp;quot;importance&amp;quot; of each bigram;
3. Sort all bigrams in the language model,
according to their &amp;quot;importance&amp;quot;;
4. Remove m most &amp;quot;unimportant&amp;quot; bigrams
from the model;
5. Re-compute backoff coefficients in the
model.
</listItem>
<figureCaption confidence="0.747248">
Figure 1. Language Model Pruning Algorithm
</figureCaption>
<listItem confidence="0.995192846153846">
1. Given the desired model size, compute
the number of bigrams that should be
added into the base model. The number
is denoted as n;
2. Compute &amp;quot;importance&amp;quot; of each bigram
included in the full-bigram model but
excluded from the base model;
3. Sort the bigrams according to their &amp;quot;im-
portance&amp;quot;;
4. Add n most &amp;quot;important&amp;quot; bigrams into
the base model;
5. Re-compute backoff coefficients in the
base model.
</listItem>
<figureCaption confidence="0.842603">
Figure 2. Growing Algorithm for Language
Model Pruning
</figureCaption>
<subsectionHeader confidence="0.994167">
3.2 Discriminative Pruning Criterion
</subsectionHeader>
<bodyText confidence="0.999676333333333">
Given a Chinese character string S, a word seg-
mentation system chooses a sequence of words
W* as the segmentation result, satisfying:
</bodyText>
<equation confidence="0.996038">
W* argmax(log ( ) log (  |))
= P W + P S W (4)
W
</equation>
<bodyText confidence="0.7043185">
The sum of the two logarithm probabilities in
equation (4) is called discriminant function:
</bodyText>
<equation confidence="0.947485">
g S W A ]F = P W + P S W
( , ; , ) log ( ) log (  |) (5)
</equation>
<bodyText confidence="0.999267125">
Where F denotes a language model that is
used to compute P(W), and A denotes a genera-
tive model that is used to compute P(S|W). In
language model pruning, A is an invariable.
The discriminative pruning criterion is in-
spired by the comparison of segmented sentences
using full-bigram model FF and using base model
FB. Given a sentence S, full-bigram model
</bodyText>
<equation confidence="0.896745833333333">
B
chooses as the segmentation result, and base
WF *
model chooses as the segmentation result,
WB *
satisfying:
WF =
* arg max g(S, W; A, ]FF) (6)
W
WB =
* arg max g(S, W; A, ]FB) (7)
W
</equation>
<bodyText confidence="0.984427666666667">
Here, given a language model F, we define a
misclassification function representing the differ-
ence between discriminant functions of and
</bodyText>
<equation confidence="0.776972857142857">
WF *
WB *
d S A ]F =g S W B * A ]F −g S W F * A ]F
( ; , ) ( , ; , ) ( , ; , ) (8)
The misclassification function reflects which
one of and is inclined to be chosen as
WF * WB *
</equation>
<bodyText confidence="0.922262714285714">
the segmentation result. If WF # WB , we may
extract some hints from the comparison of them,
and select a few valuable bigrams. By adding
these bigrams to base model, we should make the
model choose the correct answer between WF *
*
and . If
</bodyText>
<equation confidence="0.912">
WB * WF = WB
</equation>
<bodyText confidence="0.602863">
* , no hints can be extracted.
Let W0 be the known correct word sequence.
</bodyText>
<equation confidence="0.81092575">
* *
Under the precondition W F # W B , we describe
our method in the following three cases.
Case 1: WF * = W0 and WB * # W0
</equation>
<bodyText confidence="0.999973083333333">
Here, full-bigram model chooses the correct
answer, while base model does not. Based on
equation (6), (7) and (8), we know that d(S;A,FB)
&gt; 0 and d(S;A,FF) &lt; 0. It implies that adding bi-
grams into base model may lead the misclassifi-
cation function from positive to negative. Which
bigram should be added depends on the variation
of misclassification function caused by adding it.
If adding a bigram makes the misclassification
function become smaller, it should be added with
higher priority.
We add each bigram individually to FB, and
</bodyText>
<subsectionHeader confidence="0.245252">
B
</subsectionHeader>
<bodyText confidence="0.923937">
then compute the variation of the misclassifica-
tion function. Let F′ denotes the model after add-
:
</bodyText>
<page confidence="0.648695">
1003
</page>
<bodyText confidence="0.996231333333333">
ing bigram wxwy into I&apos;BB. According to equation
(5) and (8), we can write the misclassification
function using I&apos;B B and I&apos;′ separately:
</bodyText>
<equation confidence="0.958743">
(WB)+logPΛ(S  |WB)
−log PB (WF )—log PΛ(S  |WF )
</equation>
<bodyText confidence="0.999279166666667">
Where BPB(.), P′(.), PA(.) represent probabilities
in base model, model I&apos;′ and model A separately.
The variation of the misclassification function is
computed as:
provement caused by adding wxwy. Thus, &amp;quot;impor-
tance&amp;quot; of bigram wxwy on S is computed as:
</bodyText>
<equation confidence="0.996715333333333">
imp(wxwy; S) = Δd (S; wxwy) (14)
Case 2: WF* ≠ W0 and WB * = W0
Here, it is just contrary to case 1. In this way,
we have:
imp(wxwy;S)=−Δd(S;wxwy) (15)
Case 3: WF ≠W0 ≠WB
</equation>
<bodyText confidence="0.93703375">
In case 1 and 2, bigrams are added so that dis-
criminant function of correct word sequence be-
comes bigger, and that of incorrect word se-
quence becomes smaller. In case 3, both and
</bodyText>
<equation confidence="0.793000714285714">
WF *
d
(S; Λ, ΓB) = log PB
d(S; , ) log (WB)+logPΛ(S|WB)
Λ Γ′ = P′
logP′(WF)−logPΛ(S  |WF)
( ; ) ( ; , ) ( ; , )
w w d S
= Λ Γ − Λ Γ′
d S
x y B
Δ d S
[log ( ) log ( )]
P W * *
′ P W
−
F B F
are incorrect. Thus, the misclassification
=
(11) WB *
P W
( ) log ( )]
* *
′ P W
−
B B
−
[log
</equation>
<bodyText confidence="0.909837">
Because the only difference between base
model and model I&apos;′ is that model I&apos;′ involves the
bigram probability P′(wy|wx), we have:
</bodyText>
<equation confidence="0.992053714285714">
(WF)−logPB(WF)
= ∑[logP′(wF(i)  |wF(i−1)) − logPB (wF(i)  |wF(i−1) ]
i
= wy ) [logp (wy  |wO −logPB ( )
n (WF, wx wy
− logαB ( )]
w x
</equation>
<bodyText confidence="0.955703676470588">
Where n WF wxwy
( * , ) denotes the number of
times the bigram wxwy appears in sequence WF .
Note that in equation (12), base model is treated
as a bigram model instead of a unigram model.
The reason lies in two respects. First, the uni-
gram model can be regarded as a particular bi-
gram model by setting all backoff coefficients to
1. Second, the base model is not always a uni-
gram model during the step-by-step growing al-
gorithm, which will be discussed in the next sub-
section.
In fact, bigram probability P′(wy|wx) is ex-
tracted from full-bigram model, so P′(wy|wx) =
PF(wy|wx). In addition, similar deductions can be
conducted to the second bracket in equation (11).
Thus, we have:
(S; wxwy) = [n(WF, wxwy) − n(WB , wxwy )
×[logPF(wy |wx)−logPB(wy)−logαB(wx)
Note that d(S;A,I&apos;) approximately indicates the
likelihood that S will be incorrectly segmented,
so Ad(S;wxwy) represents the performance im-
function in equation (8) does not represent the
likelihood that S will be incorrectly segmented.
Therefore, variation of the misclassification
function in equation (13) can not be used to
measure the &amp;quot;importance&amp;quot; of a bigram. Here, sen-
tence S is ignored, and the &amp;quot;importance&amp;quot; of all
bigrams on S are zero.
The above three cases are designed for one
sentence. The &amp;quot;importance&amp;quot; of each bigram on
the whole training corpus is the sum of its &amp;quot;im-
portance&amp;quot; on each single sentence, as equation
(16) shows.
</bodyText>
<equation confidence="0.920301333333333">
imp(wxwy ) imp(wxwy; S)
= ∑ (16)
S
</equation>
<bodyText confidence="0.9891265">
To sum up, the &amp;quot;importance&amp;quot; of each bigram is
computed as Figure 3 shows.
</bodyText>
<listItem confidence="0.9972915">
1. For each wxwy, set imp(wxwy) = 0;
2. For each sentence in training corpus:
</listItem>
<equation confidence="0.878482">
For each wxwy:
if WF = W0 and WB ≠ W0:
imp(wxwy) += Ad(S;wxwy);
else if WF ≠ *W0 and WB = *W0 :
imp(wxwy) −= Ad(S;wxwy);
</equation>
<figureCaption confidence="0.984234">
Figure 3. Calculation of &amp;quot;Importance&amp;quot;
of Bigrams
</figureCaption>
<bodyText confidence="0.99882075">
We illustrate the process of computing &amp;quot;im-
portance&amp;quot; of bigrams with a simple example.
Suppose S is &amp;quot; 这 (zhe4) 样 (yang4) 才 (cai2) 能
(neng2) 更 (geng4) 方 (fang1) 便 (bian4)&amp;quot;. The
segmented result using full-bigram model is &amp;quot;这
样(zhe4yang4)/才(cai2)/能(neng2)/更(geng4)/方
便(fang1bian4)&amp;quot;, which is the correct word se-
quence. The segmented result using base model
</bodyText>
<figure confidence="0.871036285714286">
lo
g P ′
(12)
Δd
]
]
(13)
</figure>
<page confidence="0.872794">
1004
</page>
<equation confidence="0.64504">
is &amp;quot; 这 样 (zhe4yang4)/ 才 能 (cai2neng2)/ 更
(geng4)/ 方 便 (fang1bian4)&amp;quot;. Obviously, it
matches case 1. For bigram &amp;quot;这样(zhe4yang4)才
(cai2)&amp;quot;, it occurs in once, and does not occur
WF *
</equation>
<bodyText confidence="0.9915435">
in WB. According to equation (13), its &amp;quot;impor-
tance&amp;quot; on sentence S is:
</bodyText>
<equation confidence="0.990640333333333">
imp(这样(zhe4yang4)才(cai2);S)
= logPF(才(cai2)|这样(zhe4yang4)) −
B [logPB(才(cai2)) + logαBB(这样(zhe4yang4))]
</equation>
<bodyText confidence="0.844663333333333">
For bigram &amp;quot; 更 (geng4) 方 便 (fang1bian4)&amp;quot;,
since it occurs once both in WF and WB* , its
&amp;quot;importance&amp;quot; on S is zero.
</bodyText>
<subsectionHeader confidence="0.992879">
3.3 Step-by-step Growing
</subsectionHeader>
<bodyText confidence="0.986662157894737">
Given the target model size, we can add exact
number of bigrams to the base model at one time
by using the growing algorithm illustrated in
Figure 2. But it is more suitable to adopt a step-
by-step growing algorithm illustrated in Figure 4.
As shown in equation (13), the &amp;quot;importance&amp;quot;
of each bigram depends on the base model. Ini-
tially, the base model is set to the unigram model.
With bigrams added in, it becomes a growing
bigram model. Thus, and
WB * log αB (wx) will
change. So, the added bigrams will affect the
calculation of &amp;quot;importance&amp;quot; of bigrams to be
added. Generally, adding more bigrams at one
time will lead to more negative impacts. Thus, it
is expected that models produced by step-by-step
growing algorithm may achieve better perform-
ance than growing algorithm, and smaller step
size will lead to even better performance.
</bodyText>
<listItem confidence="0.982678875">
1. Given step size s;
2. Set the base model to be the unigram
model;
3. Segment corpus with full-bigram model;
4. Segment corpus with base model;
5. Compute &amp;quot;importance&amp;quot; of each bigram
included in the full-bigram model but ex-
cluded from the base model;
6. Sort the bigrams according to their &amp;quot;im-
portance&amp;quot;;
7. Add s bigrams with the biggest &amp;quot;impor-
tance&amp;quot; to the base model;
8. Re-compute backoff coefficients in the
base model;
9. If the base model is still smaller than the
desired size, go to step 4; otherwise, stop.
</listItem>
<figureCaption confidence="0.976172">
Figure 4. Step-by-step Growing Algorithm
</figureCaption>
<sectionHeader confidence="0.997509" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999237">
4.1 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.99996016">
The training corpus comes from People&apos;s daily
2000, containing about 25 million Chinese char-
acters. It is manually segmented into word se-
quences, according to the word segmentation
specification of Peking University (Yu et al.,
2003). The testing text that is provided by Peking
University comes from the second international
Chinese word segmentation bakeoff organized
by SIGHAN. The testing text is a part of Peo-
ple&apos;s daily 2001, consisting of about 170K Chi-
nese characters.
The vocabulary is automatically extracted
from the training corpus, and the words occur-
ring only once are removed. Finally, about 67K
words are included in the vocabulary. The full-
bigram model and the unigram model are trained
by CMU language model toolkit (Clarkson and
Rosenfeld, 1997). Without any count cut-off, the
full-bigram model contains about 2 million bi-
grams.
The word segmentation system is developed
based on a source-channel model similar to that
described in (Gao et al., 2003). Viterbi algorithm
is applied to find the best word segmentation
path.
</bodyText>
<subsectionHeader confidence="0.969379">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999078">
The language models built in our experiments
are evaluated by two metrics. One is F-Measure
of the word segmentation result; the other is lan-
guage model perplexity.
For F-Measure evaluation, we firstly segment
the raw testing text using the model to be evalu-
ated. Then, the segmented result is evaluated by
comparing with the gold standard set. The
evaluation tool is also from the word segmenta-
tion bakeoff. F-Measure is calculated as:
</bodyText>
<equation confidence="0.879176333333333">
F-Measure = 2 × Precision × Recall (17)
Precision Recall
+
</equation>
<bodyText confidence="0.9996766">
For perplexity evaluation, the language model
to be evaluated is used to provide the bigram
probabilities for each word in the testing text.
The perplexity is the mean logarithm probability
as shown in equation (18):
</bodyText>
<equation confidence="0.991173">
−PP(M) = 2 N∑N1log2P(wi|wi−1) (18)
</equation>
<subsectionHeader confidence="0.987292">
4.3 Comparison of Pruning Methods
</subsectionHeader>
<bodyText confidence="0.9420545">
The Kullback-Leibler Distance (KLD) based
method is the state-of-the-art method, and is
</bodyText>
<page confidence="0.982999">
1005
</page>
<bodyText confidence="0.996440709677419">
taken as the baseline1. Pruning algorithm illus-
trated in Figure 1 is used for KLD based pruning.
Growing algorithms illustrated in Figure 2 and
Figure 4 are used for discriminative pruning
method. Growing algorithms are not applied to
KLD based pruning, because the computation of
KLD is independent of the base model.
At step 1 for KLD based pruning, m is set to
produce ten models containing 10K, 20K, ...,
100K bigrams. We apply each of the models to
the word segmentation system, and evaluate the
segmented results with the evaluation tool. The
F-Measures of the ten models are illustrated in
Figure 5, denoted by &amp;quot;KLD&amp;quot;.
For the discriminative pruning criterion, the
growing algorithm illustrated in Figure 2 is
firstly used. Unigram model acts as the base
model. At step 1, n is set to 10K, 20K, ..., 100K
separately. At step 2, &amp;quot;importance&amp;quot; of each bi-
gram is computed following Figure 3. Ten mod-
els are produced and evaluated. The F-Measures
are also illustrated in Figure 5, denoted by &amp;quot;Dis-
crim&amp;quot;.
By adding bigrams step by step as illustrated
in Figure 4, and setting step size to 10K, 5K, and
2K separately, we obtain other three series of
models, denoted by &amp;quot;Step-10K&amp;quot;, &amp;quot;Step-5K&amp;quot; and
&amp;quot;Step-2K&amp;quot; in Figure 5.
We also include in Figure 5 the performance
of the count cut-off method. Obviously, it is infe-
rior to other methods.
</bodyText>
<figure confidence="0.7219685">
1 2 3 4 5 6 7 8 9 10
Bigram Num(10K)
</figure>
<figureCaption confidence="0.887232">
Figure 5. Performance Comparison of Different
Pruning Methods
</figureCaption>
<bodyText confidence="0.978702">
First, we compare the performance of &amp;quot;KLD&amp;quot;
and &amp;quot;Discrim&amp;quot;. When the model size is small,
</bodyText>
<footnote confidence="0.986163333333333">
1 Our pilot study shows that the method based on Kullback-
Leibler distance outperforms methods based on other crite-
ria introduced in section 2.
</footnote>
<bodyText confidence="0.999578866666667">
such as those models containing less than 70K
bigrams, the performance of &amp;quot;Discrim&amp;quot; is better
than &amp;quot;KLD&amp;quot;. For the models containing more
than 70K bigrams, &amp;quot;KLD&amp;quot; gets better perform-
ance than &amp;quot;Discrim&amp;quot;. The reason is that the added
bigrams affect the calculation of &amp;quot;importance&amp;quot; of
bigrams to be added, which has been discussed
in section 3.3.
If we add the bigrams step by step, better per-
formance is achieved. From Figure 5, it can be
seen that all of the models generated by step-by-
step growing algorithm outperform &amp;quot;KLD&amp;quot; and
&amp;quot;Discrim&amp;quot; consistently. Compared with the base-
line KLD based method, step-by-step growing
methods result in at least 0.2 percent improve-
ment for each model size.
Comparing &amp;quot;Step-10K&amp;quot;, &amp;quot;Step-5K&amp;quot; and &amp;quot;Step-
2K&amp;quot;, they perform differently before the 60K-
bigram point, and perform almost the same after
that. The reason is that they are approaching their
saturation states, which will be discussed in sec-
tion 4.5. Before 60K-bigram point, smaller step
size yields better performance.
An example of detailed comparison result is
shown in Table 1, where the F-Measure is
96.33%. The last column shows the relative
model sizes with respect to the KLD pruned
model. It shows that with the F-Measure of
96.33%, number of bigrams decreases by up to
90%.
</bodyText>
<table confidence="0.9970192">
# of bigrams % of KLD
KLD 100,000 100%
Step-10K 25,000 25%
Step-5K 15,000 15%
Step-2K 10,000 10%
</table>
<tableCaption confidence="0.9886605">
Table 1. Comparison of Number of Bigrams
at F-Measure 96.33%
</tableCaption>
<subsectionHeader confidence="0.993003">
4.4 Correlation between Perplexity and F-
Measure
</subsectionHeader>
<bodyText confidence="0.999945692307692">
Perplexities of the models built above are evalu-
ated over the gold standard set. Figure 6 shows
how the perplexities vary with the bigram num-
bers in models. Here, we notice that the KLD
models achieve the lowest perplexities. It is not a
surprising result, because the goal of KLD based
pruning is to minimize the Kullback-Leibler dis-
tance that can be interpreted as a relative change
of perplexity (Stolcke, 1998).
Now we compare Figure 5 and Figure 6. Per-
plexities of KLD models are much lower than
that of the other models, but their F-Measures are
much worse than that of step-by-step growing
</bodyText>
<figure confidence="0.994990090909091">
F-Measure(%)
96.6
96.5
96.4
96.3
96.2
96.1
96.0
KLD Discrim
Step-10K Step-5K
Step-2K Cut-off
</figure>
<page confidence="0.987317">
1006
</page>
<bodyText confidence="0.9995845">
models. It implies that lower perplexity does not
always lead to higher F-Measure.
However, when the comparison is restricted in
a single pruning method, the case is different.
For each pruning method, as more bigrams are
included in the model, the perplexity curve falls,
and the F-Measure curve rises. It implies there
are correlations between them. We compute the
Pearson product-moment correlation coefficient
for each pruning method, as listed in Table 2. It
shows that the correlation between perplexity
and F-Measure is very strong.
To sum up, the correlation between language
model perplexity and system performance (here
represented by F-Measure) depends on whether
the models come from the same pruning method.
If so, the correlation is strong. Otherwise, the
correlation is weak.
</bodyText>
<equation confidence="0.7939455">
1 2 3 4 5 6 7 8 9 10
Bigram Num(10K)
</equation>
<bodyText confidence="0.99062528125">
the mis-aligned part of the segmented corpus,
where ≠ . It is likely that not all bigrams
WF * WB *
have the opportunity. As more and more bigrams
are added into the base model, the segmented
training corpus using the current base model ap-
proaches to that using the full-bigram model.
Gradually, none bigram can be added into the
current base model. At that time, the model stops
growing, and reaches its saturation state. The
model that reaches its saturation state is named
as saturated model. In our experiments, three
step-by-step growing models reach their satura-
tion states when about 100K bigrams are added
in.
By combining with the baseline KLD based
method, we obtain models that outperform the
baseline for any model size. We combine them
as follows. If the desired model size is smaller
than that of the saturated model, step-by-step
growing is applied. Otherwise, Kullback-Leibler
distance is used for further growing over the
saturated model. For instance, by growing over
the saturated model of &amp;quot;Step-2K&amp;quot;, we obtain
combined models containing from 100K to 2
million bigrams. The performance of the com-
bined models and that of the baseline KLD mod-
els are illustrated in Figure 7. It shows that the
combined model performs consistently better
than KLD model over all of bigram numbers.
Finally, the two curves converge at the perform-
ance of the full-bigram model.
</bodyText>
<figure confidence="0.999136">
Perplexity
650
600
550
500
450
400
350
300
700
KLD Discrim
Step-10K Step-5K
Step-2K Cut-off
Pruning Method Correlation
Cut-off -0.990
KLD -0.991
Discrim -0.979
Step-10K -0.985
Step-5K -0.974
Step-2K -0.995
</figure>
<figureCaption confidence="0.649644">
Figure 6. Perplexity Comparison of Different
</figureCaption>
<figure confidence="0.982078565217391">
Pruning Methods
10
30
50
70
90
110
130
150
170
190
207
F-Measure(%)
97.0
96.9
96.8
96.7
96.6
96.5
96.4
96.3
KLD
Combined Model
</figure>
<tableCaption confidence="0.709907">
Table 2. Correlation between Perplexity
and F-Measure
</tableCaption>
<subsectionHeader confidence="0.497058">
4.5 Combination of Saturated Model and
KLD
</subsectionHeader>
<bodyText confidence="0.999977333333333">
The above experimental results show that step-
by-step growing models achieve the best per-
formance when less than 100K bigrams are
added in. Unfortunately, they can not grow up
into any desired size. A bigram has no chance to
be added into the base model, unless it appears in
</bodyText>
<subsectionHeader confidence="0.350971">
Bigram Num(10K)
</subsectionHeader>
<bodyText confidence="0.315596">
Figure 7. Performance Comparison of Combined
Model and KLD Model
</bodyText>
<sectionHeader confidence="0.997458" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999983833333333">
A discriminative pruning criterion of n-gram lan-
guage model for Chinese word segmentation was
proposed in this paper, and a step-by-step grow-
ing algorithm was suggested to generate the
model of desired size based on a full-bigram
model and a base model. Experimental results
</bodyText>
<page confidence="0.977791">
1007
</page>
<bodyText confidence="0.999990863636364">
showed that the discriminative pruning method
achieves significant improvements over the base-
line KLD based method. At the same F-measure,
the number of bigrams can be reduced by up to
90%. By combining the saturated model and the
baseline KLD based method, we achieved better
performance for any model size. Analysis shows
that, if the models come from the same pruning
method, the correlation between perplexity and
performance is strong. Otherwise, the correlation
is weak.
The pruning methods discussed in this paper
focus on bigram pruning, keeping unigram prob-
abilities unchanged. The future work will attempt
to prune bigrams and unigrams simultaneously,
according to a same discriminative pruning crite-
rion. And we will try to improve the efficiency of
the step-by-step growing algorithm. In addition,
the method described in this paper can be ex-
tended to other applications, such as IME and
speech recognition, where language models are
applied in a similar way.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987888607595">
Philip Clarkson and Ronald Rosenfeld. 1997. Statisti-
cal Language Modeling Using the CMU-
Cambridge Toolkit. In Proc. of the 5th European
Conference on Speech Communication and Tech-
nology (Eurospeech-1997), pages 2707-2710.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Machine Learning:
Proc. of 17th International Conference (ICML-
2000), pages 175-182.
Jianfeng Gao and Kai-Fu Lee. 2000. Distribution-
based pruning of backoff language models. In Proc.
of the 38th Annual Meeting of Association for Com-
putational Linguistics (ACL-2000), pages 579-585.
Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003.
Improved Source-channel Models for Chinese
Word Segmentation. In Proc. of the 41st Annual
Meeting of Association for Computational Linguis-
tics (ACL-2003), pages 272-279.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese Word Segmentation and
Named Entity Recognition: A Pragmatic Approach.
Computational Linguistics, 31(4): 531-574.
Jianfeng Gao and Min Zhang. 2002. Improving Lan-
guage Model Size Reduction using Better Pruning
Criteria. In Proc. of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL-
2002), pages 176-182.
Fredrick Jelinek. 1990. Self-organized language mod-
eling for speech recognition. In Alexander Waibel
and Kai-Fu Lee (Eds.), Readings in Speech Recog-
nition, pages 450-506.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang,
and Chin-Hui Lee. 2002. Discriminative Training
of Language Models for Speech Recognition. In
Proc. of the 27th International Conference On
Acoustics, Speech and Signal Processing (ICASSP-
2002), pages 325-328.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native Training and Maximum Entropy Models for
Statistical Machine Translation. In Proc. of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2002), pages 295-302.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese Segmentation and New Word De-
tection using Conditional Random Fields. In Proc.
of the 20th International Conference on Computa-
tional Linguistics (COLING-2004), pages 562-568.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative Language
Modeling with Conditional Random Fields and the
Perceptron Algorithm. In Proc. of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2004), pages 47-54.
Kristie Seymore and Ronald Rosenfeld. 1996. Scal-
able Backoff Language Models. In Proc. of the 4th
International Conference on Spoken Language
Processing (ICSLP-1996), pages. 232-235.
Richard Sproat, Chilin Shih, William Gale, and
Nancy Chang. 1996. A Stochastic Finite-state
Word-segmentation Algorithm for Chinese. Com-
putational Linguistics, 22(3): 377-404.
Andreas Stolcke. 1998. Entropy-based Pruning of
Backoff Language Models. In Proc. of DARPA
News Transcription and Understanding Workshop,
pages 270-274.
Maosong Sun and Benjamin K. Tsou. 2001. A Re-
view and Evaluation on Automatic Segmentation
of Chinese. Contemporary Linguistics, 3(1): 22-32.
Shiwen Yu, Huiming Duan, Xuefeng Zhu, Bin Swen,
and Baobao Chang. 2003. Specification for Corpus
Processing at Peking University: Word Segmenta-
tion, POS Tagging and Phonetic Notation. Journal
of Chinese Language and Computing, 13(2): 121-
158.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and
Qun Liu. 2003. HHMM-based Chinese Lexical
Analyzer ICTCLAS, In Proc. of the ACL-2003
Workshop on Chinese Language Processing
(SIGHAN), pages 184-187.
</reference>
<page confidence="0.991356">
1008
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.791519">
<title confidence="0.997526">Discriminative Pruning of Language Models for Chinese Word Segmentation</title>
<author confidence="0.997327">Jianfeng Li Haifeng Wang Dengjun Ren Guohua Li</author>
<affiliation confidence="0.994071">Toshiba (China) Research and Development Center</affiliation>
<address confidence="0.993881">5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District Beijing, 100738, China</address>
<email confidence="0.9459105">lijianfeng@rdc.toshiba.com.cn</email>
<email confidence="0.9459105">wanghaifeng@rdc.toshiba.com.cn</email>
<email confidence="0.9459105">rendengjun@rdc.toshiba.com.cn</email>
<email confidence="0.9459105">liguohua@rdc.toshiba.com.cn</email>
<abstract confidence="0.995392434782609">This paper presents a discriminative method of language model for Chinese word segmentation. To reduce the size of the language model that is used in a Chinese word segmentation system, importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram. Then we propose a step-by-step growing algorithm to build the language model of desired size. Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method. At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%. Correlation between language model perplexity and word segmentation performance is also discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical Language Modeling Using the CMUCambridge Toolkit.</title>
<date>1997</date>
<booktitle>In Proc. of the 5th European Conference on Speech Communication and Technology (Eurospeech-1997),</booktitle>
<pages>2707--2710</pages>
<contexts>
<context position="18094" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="3119" endWordPosition="3122">sequences, according to the word segmentation specification of Peking University (Yu et al., 2003). The testing text that is provided by Peking University comes from the second international Chinese word segmentation bakeoff organized by SIGHAN. The testing text is a part of People&apos;s daily 2001, consisting of about 170K Chinese characters. The vocabulary is automatically extracted from the training corpus, and the words occurring only once are removed. Finally, about 67K words are included in the vocabulary. The fullbigram model and the unigram model are trained by CMU language model toolkit (Clarkson and Rosenfeld, 1997). Without any count cut-off, the full-bigram model contains about 2 million bigrams. The word segmentation system is developed based on a source-channel model similar to that described in (Gao et al., 2003). Viterbi algorithm is applied to find the best word segmentation path. 4.2 Evaluation Metrics The language models built in our experiments are evaluated by two metrics. One is F-Measure of the word segmentation result; the other is language model perplexity. For F-Measure evaluation, we firstly segment the raw testing text using the model to be evaluated. Then, the segmented result is evalu</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip Clarkson and Ronald Rosenfeld. 1997. Statistical Language Modeling Using the CMUCambridge Toolkit. In Proc. of the 5th European Conference on Speech Communication and Technology (Eurospeech-1997), pages 2707-2710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Reranking for Natural Language Parsing.</title>
<date>2000</date>
<booktitle>In Machine Learning: Proc. of 17th International Conference (ICML2000),</booktitle>
<pages>175--182</pages>
<contexts>
<context position="2737" citStr="Collins, 2000" startWordPosition="420" endWordPosition="421">ize could be reduced effectively, while the performance loss is kept as small as possible. A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation. It differentiates from the previous pruning approaches in two respects. First, the pruning criterion is based on performance variation of word segmentation. Second, the model of desired size is achieved by adding valuable bigrams to a base model, instead of by pruning bigrams from an unpruned m</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative Reranking for Natural Language Parsing. In Machine Learning: Proc. of 17th International Conference (ICML2000), pages 175-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Kai-Fu Lee</author>
</authors>
<title>Distributionbased pruning of backoff language models.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of Association for Computational Linguistics (ACL-2000),</booktitle>
<pages>579--585</pages>
<contexts>
<context position="5115" citStr="Gao and Lee (2000)" startWordPosition="791" endWordPosition="794">discusses the related work on language model pruning. Section 3 proposes our discriminative pruning method for Chinese word segmentation. Section 4 describes the experimental settings and results. Result analysis and discussions are also presented in this section. We draw the conclusions in section 5. 2 Related Work A simple way to reduce the size of an n-gram language model is to exclude those n-grams occurring infrequently in training corpus. It is named as count cut-off method (Jelinek, 1990). Because counts are always integers, the size of the model can only be reduced to discrete values. Gao and Lee (2000) proposed a distributionbased pruning. Instead of pruning n-grams that are infrequent in training data, they prune ngrams that are likely to be infrequent in a new document. Experimental results show that it is better than traditional count cut-off method. Seymore and Rosenfeld (1996) proposed a method to measure the difference of the models before and after pruning each n-gram, and the difference is computed as: −N(hj,wi)x [log P′(wi |hj)− log P(wi |hj)] (1) Where P(wi|hj) denotes the conditional probabilities assigned by the original model, and P′(wi|hj) denotes the probabilities in the prun</context>
</contexts>
<marker>Gao, Lee, 2000</marker>
<rawString>Jianfeng Gao and Kai-Fu Lee. 2000. Distributionbased pruning of backoff language models. In Proc. of the 38th Annual Meeting of Association for Computational Linguistics (ACL-2000), pages 579-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Improved Source-channel Models for Chinese Word Segmentation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>272--279</pages>
<contexts>
<context position="1426" citStr="Gao et al. (2003)" startWordPosition="218" endWordPosition="221">lts show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method. At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%. Correlation between language model perplexity and word segmentation performance is also discussed. 1 Introduction Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004). In Gao et al. (2003), an approach based on source-channel model for Chinese word segmentation was proposed. Gao et al. (2005) further developed it to a linear mixture model. In these statistical models, language models are essential for word segmentation disambiguation. However, an uncompressed language model is usually too large for practical use since all realistic applications have memory constraints. Therefore, language model pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pruning criteria. The go</context>
<context position="18300" citStr="Gao et al., 2003" startWordPosition="3152" endWordPosition="3155">akeoff organized by SIGHAN. The testing text is a part of People&apos;s daily 2001, consisting of about 170K Chinese characters. The vocabulary is automatically extracted from the training corpus, and the words occurring only once are removed. Finally, about 67K words are included in the vocabulary. The fullbigram model and the unigram model are trained by CMU language model toolkit (Clarkson and Rosenfeld, 1997). Without any count cut-off, the full-bigram model contains about 2 million bigrams. The word segmentation system is developed based on a source-channel model similar to that described in (Gao et al., 2003). Viterbi algorithm is applied to find the best word segmentation path. 4.2 Evaluation Metrics The language models built in our experiments are evaluated by two metrics. One is F-Measure of the word segmentation result; the other is language model perplexity. For F-Measure evaluation, we firstly segment the raw testing text using the model to be evaluated. Then, the segmented result is evaluated by comparing with the gold standard set. The evaluation tool is also from the word segmentation bakeoff. F-Measure is calculated as: F-Measure = 2 × Precision × Recall (17) Precision Recall + For perpl</context>
</contexts>
<marker>Gao, Li, Huang, 2003</marker>
<rawString>Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003. Improved Source-channel Models for Chinese Word Segmentation. In Proc. of the 41st Annual Meeting of Association for Computational Linguistics (ACL-2003), pages 272-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Andi Wu</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<pages>531--574</pages>
<contexts>
<context position="1531" citStr="Gao et al. (2005)" startWordPosition="235" endWordPosition="238">ned using the state-of-the-art method. At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%. Correlation between language model perplexity and word segmentation performance is also discussed. 1 Introduction Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004). In Gao et al. (2003), an approach based on source-channel model for Chinese word segmentation was proposed. Gao et al. (2005) further developed it to a linear mixture model. In these statistical models, language models are essential for word segmentation disambiguation. However, an uncompressed language model is usually too large for practical use since all realistic applications have memory constraints. Therefore, language model pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pruning criteria. The goal of research for language model pruning is to find criteria or methods, using which the model size coul</context>
</contexts>
<marker>Gao, Li, Wu, Huang, 2005</marker>
<rawString>Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang. 2005. Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach. Computational Linguistics, 31(4): 531-574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Min Zhang</author>
</authors>
<title>Improving Language Model Size Reduction using Better Pruning Criteria.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002),</booktitle>
<pages>176--182</pages>
<contexts>
<context position="2456" citStr="Gao and Zhang, 2002" startWordPosition="377" endWordPosition="380">del pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pruning criteria. The goal of research for language model pruning is to find criteria or methods, using which the model size could be reduced effectively, while the performance loss is kept as small as possible. A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation. It differentia</context>
<context position="6421" citStr="Gao and Zhang (2002)" startWordPosition="1004" endWordPosition="1007">feld (1996) showed that this method is more effective than the traditional cut-off method. Stolcke (1998) presented a more sound criterion for computing the difference of models before and after pruning each n-gram, which is called relative entropy or Kullback-Leibler distance. It is computed as: − E P(wi, hj )[log P′(wi |hj) − wi,hj The sum is over all words wi and histories hj. This criterion removes some of the approximations employed in Seymore and Rosenfeld (1996). In addition, Stolcke (1998) presented a method for efficient computation of the Kullback-Leibler distance of each n-gram. In Gao and Zhang (2002), three measures are studied for the purpose of language model pruning. They are probability, rank, and entropy. Among them, probability is very similar to that proposed by Seymore and Rosenfeld (1996). Gao and Zhang (2002) also presented a method of combining two criteria, and showed the combination of rank and entropy achieved the smallest models. 3 Discriminative Pruning for Chinese Word Segmentation 3.1 Problem Definition In this paper, discussions are restricted to bigram language model P(wy|wx). In a bigram model, three kinds of parameters are involved: bigram probability Pm(wy|wx) for s</context>
</contexts>
<marker>Gao, Zhang, 2002</marker>
<rawString>Jianfeng Gao and Min Zhang. 2002. Improving Language Model Size Reduction using Better Pruning Criteria. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002), pages 176-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredrick Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1990</date>
<booktitle>In Alexander Waibel and Kai-Fu Lee (Eds.), Readings in Speech Recognition,</booktitle>
<pages>450--506</pages>
<contexts>
<context position="2316" citStr="Jelinek, 1990" startWordPosition="359" endWordPosition="360">language model is usually too large for practical use since all realistic applications have memory constraints. Therefore, language model pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pruning criteria. The goal of research for language model pruning is to find criteria or methods, using which the model size could be reduced effectively, while the performance loss is kept as small as possible. A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language mod</context>
<context position="4997" citStr="Jelinek, 1990" startWordPosition="772" endWordPosition="773">y and system performance is also discussed. The remainder of the paper is organized as follows. Section 2 briefly discusses the related work on language model pruning. Section 3 proposes our discriminative pruning method for Chinese word segmentation. Section 4 describes the experimental settings and results. Result analysis and discussions are also presented in this section. We draw the conclusions in section 5. 2 Related Work A simple way to reduce the size of an n-gram language model is to exclude those n-grams occurring infrequently in training corpus. It is named as count cut-off method (Jelinek, 1990). Because counts are always integers, the size of the model can only be reduced to discrete values. Gao and Lee (2000) proposed a distributionbased pruning. Instead of pruning n-grams that are infrequent in training data, they prune ngrams that are likely to be infrequent in a new document. Experimental results show that it is better than traditional count cut-off method. Seymore and Rosenfeld (1996) proposed a method to measure the difference of the models before and after pruning each n-gram, and the difference is computed as: −N(hj,wi)x [log P′(wi |hj)− log P(wi |hj)] (1) Where P(wi|hj) den</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>Fredrick Jelinek. 1990. Self-organized language modeling for speech recognition. In Alexander Waibel and Kai-Fu Lee (Eds.), Readings in Speech Recognition, pages 450-506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong-Kwang Jeff Kuo</author>
<author>Eric Fosler-Lussier</author>
<author>Hui Jiang</author>
<author>Chin-Hui Lee</author>
</authors>
<title>Discriminative Training of Language Models for Speech Recognition.</title>
<date>2002</date>
<booktitle>In Proc. of the 27th International Conference On Acoustics, Speech and Signal Processing (ICASSP2002),</booktitle>
<pages>325--328</pages>
<contexts>
<context position="2824" citStr="Kuo et al., 2002" startWordPosition="434" endWordPosition="437">ible. A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation. It differentiates from the previous pruning approaches in two respects. First, the pruning criterion is based on performance variation of word segmentation. Second, the model of desired size is achieved by adding valuable bigrams to a base model, instead of by pruning bigrams from an unpruned model. We define a misclassification function that approximately represents the likeliho</context>
</contexts>
<marker>Kuo, Fosler-Lussier, Jiang, Lee, 2002</marker>
<rawString>Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, and Chin-Hui Lee. 2002. Discriminative Training of Language Models for Speech Recognition. In Proc. of the 27th International Conference On Acoustics, Speech and Signal Processing (ICASSP2002), pages 325-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<pages>295--302</pages>
<contexts>
<context position="2778" citStr="Och and Ney, 2002" startWordPosition="425" endWordPosition="428">le the performance loss is kept as small as possible. A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation. It differentiates from the previous pruning approaches in two respects. First, the pruning criterion is based on performance variation of word segmentation. Second, the model of desired size is achieved by adding valuable bigrams to a base model, instead of by pruning bigrams from an unpruned model. We define a misclassification funct</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), pages 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese Segmentation and New Word Detection using Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th International Conference on Computational Linguistics (COLING-2004),</booktitle>
<pages>562--568</pages>
<contexts>
<context position="1404" citStr="Peng et al., 2004" startWordPosition="213" endWordPosition="216">size. Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method. At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%. Correlation between language model perplexity and word segmentation performance is also discussed. 1 Introduction Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004). In Gao et al. (2003), an approach based on source-channel model for Chinese word segmentation was proposed. Gao et al. (2005) further developed it to a linear mixture model. In these statistical models, language models are essential for word segmentation disambiguation. However, an uncompressed language model is usually too large for practical use since all realistic applications have memory constraints. Therefore, language model pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pr</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese Segmentation and New Word Detection using Conditional Random Fields. In Proc. of the 20th International Conference on Computational Linguistics (COLING-2004), pages 562-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-2004),</booktitle>
<pages>47--54</pages>
<contexts>
<context position="2845" citStr="Roark et al., 2004" startWordPosition="438" endWordPosition="441">ia have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation. It differentiates from the previous pruning approaches in two respects. First, the pruning criterion is based on performance variation of word segmentation. Second, the model of desired size is achieved by adding valuable bigrams to a base model, instead of by pruning bigrams from an unpruned model. We define a misclassification function that approximately represents the likelihood that a sentence wi</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm. In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-2004), pages 47-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Scalable Backoff Language Models.</title>
<date>1996</date>
<booktitle>In Proc. of the 4th International Conference on Spoken Language Processing (ICSLP-1996),</booktitle>
<pages>232--235</pages>
<contexts>
<context position="2374" citStr="Seymore and Rosenfeld, 1996" startWordPosition="364" endWordPosition="367">tical use since all realistic applications have memory constraints. Therefore, language model pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pruning criteria. The goal of research for language model pruning is to find criteria or methods, using which the model size could be reduced effectively, while the performance loss is kept as small as possible. A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pru</context>
<context position="5400" citStr="Seymore and Rosenfeld (1996)" startWordPosition="836" endWordPosition="839">e conclusions in section 5. 2 Related Work A simple way to reduce the size of an n-gram language model is to exclude those n-grams occurring infrequently in training corpus. It is named as count cut-off method (Jelinek, 1990). Because counts are always integers, the size of the model can only be reduced to discrete values. Gao and Lee (2000) proposed a distributionbased pruning. Instead of pruning n-grams that are infrequent in training data, they prune ngrams that are likely to be infrequent in a new document. Experimental results show that it is better than traditional count cut-off method. Seymore and Rosenfeld (1996) proposed a method to measure the difference of the models before and after pruning each n-gram, and the difference is computed as: −N(hj,wi)x [log P′(wi |hj)− log P(wi |hj)] (1) Where P(wi|hj) denotes the conditional probabilities assigned by the original model, and P′(wi|hj) denotes the probabilities in the pruned model. N(hj, wi) is the discounted frequency of ngram event hjwi. Seymore and Rosenfeld (1996) showed that this method is more effective than the traditional cut-off method. Stolcke (1998) presented a more sound criterion for computing the difference of models before and after prun</context>
</contexts>
<marker>Seymore, Rosenfeld, 1996</marker>
<rawString>Kristie Seymore and Ronald Rosenfeld. 1996. Scalable Backoff Language Models. In Proc. of the 4th International Conference on Spoken Language Processing (ICSLP-1996), pages. 232-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A Stochastic Finite-state Word-segmentation Algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<pages>377--404</pages>
<contexts>
<context position="1344" citStr="Sproat et al., 1996" startWordPosition="201" endWordPosition="204">tep growing algorithm to build the language model of desired size. Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method. At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%. Correlation between language model perplexity and word segmentation performance is also discussed. 1 Introduction Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004). In Gao et al. (2003), an approach based on source-channel model for Chinese word segmentation was proposed. Gao et al. (2005) further developed it to a linear mixture model. In these statistical models, language models are essential for word segmentation disambiguation. However, an uncompressed language model is usually too large for practical use since all realistic applications have memory constraints. Therefore, language model pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A Stochastic Finite-state Word-segmentation Algorithm for Chinese. Computational Linguistics, 22(3): 377-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based Pruning of Backoff Language Models.</title>
<date>1998</date>
<booktitle>In Proc. of DARPA News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="2416" citStr="Stolcke, 1998" startWordPosition="371" endWordPosition="372">onstraints. Therefore, language model pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pruning criteria. The goal of research for language model pruning is to find criteria or methods, using which the model size could be reduced effectively, while the performance loss is kept as small as possible. A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for C</context>
<context position="5906" citStr="Stolcke (1998)" startWordPosition="918" endWordPosition="919">erimental results show that it is better than traditional count cut-off method. Seymore and Rosenfeld (1996) proposed a method to measure the difference of the models before and after pruning each n-gram, and the difference is computed as: −N(hj,wi)x [log P′(wi |hj)− log P(wi |hj)] (1) Where P(wi|hj) denotes the conditional probabilities assigned by the original model, and P′(wi|hj) denotes the probabilities in the pruned model. N(hj, wi) is the discounted frequency of ngram event hjwi. Seymore and Rosenfeld (1996) showed that this method is more effective than the traditional cut-off method. Stolcke (1998) presented a more sound criterion for computing the difference of models before and after pruning each n-gram, which is called relative entropy or Kullback-Leibler distance. It is computed as: − E P(wi, hj )[log P′(wi |hj) − wi,hj The sum is over all words wi and histories hj. This criterion removes some of the approximations employed in Seymore and Rosenfeld (1996). In addition, Stolcke (1998) presented a method for efficient computation of the Kullback-Leibler distance of each n-gram. In Gao and Zhang (2002), three measures are studied for the purpose of language model pruning. They are prob</context>
<context position="22820" citStr="Stolcke, 1998" startWordPosition="3914" endWordPosition="3915">igrams % of KLD KLD 100,000 100% Step-10K 25,000 25% Step-5K 15,000 15% Step-2K 10,000 10% Table 1. Comparison of Number of Bigrams at F-Measure 96.33% 4.4 Correlation between Perplexity and FMeasure Perplexities of the models built above are evaluated over the gold standard set. Figure 6 shows how the perplexities vary with the bigram numbers in models. Here, we notice that the KLD models achieve the lowest perplexities. It is not a surprising result, because the goal of KLD based pruning is to minimize the Kullback-Leibler distance that can be interpreted as a relative change of perplexity (Stolcke, 1998). Now we compare Figure 5 and Figure 6. Perplexities of KLD models are much lower than that of the other models, but their F-Measures are much worse than that of step-by-step growing F-Measure(%) 96.6 96.5 96.4 96.3 96.2 96.1 96.0 KLD Discrim Step-10K Step-5K Step-2K Cut-off 1006 models. It implies that lower perplexity does not always lead to higher F-Measure. However, when the comparison is restricted in a single pruning method, the case is different. For each pruning method, as more bigrams are included in the model, the perplexity curve falls, and the F-Measure curve rises. It implies ther</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based Pruning of Backoff Language Models. In Proc. of DARPA News Transcription and Understanding Workshop, pages 270-274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
<author>Benjamin K Tsou</author>
</authors>
<title>A Review and Evaluation on Automatic Segmentation of Chinese.</title>
<date>2001</date>
<journal>Contemporary Linguistics,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>22--32</pages>
<contexts>
<context position="1364" citStr="Sun and Tsou, 2001" startWordPosition="205" endWordPosition="208"> to build the language model of desired size. Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method. At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%. Correlation between language model perplexity and word segmentation performance is also discussed. 1 Introduction Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004). In Gao et al. (2003), an approach based on source-channel model for Chinese word segmentation was proposed. Gao et al. (2005) further developed it to a linear mixture model. In these statistical models, language models are essential for word segmentation disambiguation. However, an uncompressed language model is usually too large for practical use since all realistic applications have memory constraints. Therefore, language model pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters expli</context>
</contexts>
<marker>Sun, Tsou, 2001</marker>
<rawString>Maosong Sun and Benjamin K. Tsou. 2001. A Review and Evaluation on Automatic Segmentation of Chinese. Contemporary Linguistics, 3(1): 22-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
<author>Huiming Duan</author>
<author>Xuefeng Zhu</author>
<author>Bin Swen</author>
<author>Baobao Chang</author>
</authors>
<title>Specification for Corpus Processing at Peking University: Word Segmentation, POS Tagging and Phonetic Notation.</title>
<date>2003</date>
<journal>Journal of Chinese Language and Computing,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>121--158</pages>
<contexts>
<context position="17563" citStr="Yu et al., 2003" startWordPosition="3034" endWordPosition="3037">odel but excluded from the base model; 6. Sort the bigrams according to their &amp;quot;importance&amp;quot;; 7. Add s bigrams with the biggest &amp;quot;importance&amp;quot; to the base model; 8. Re-compute backoff coefficients in the base model; 9. If the base model is still smaller than the desired size, go to step 4; otherwise, stop. Figure 4. Step-by-step Growing Algorithm 4 Experiments 4.1 Experiment Settings The training corpus comes from People&apos;s daily 2000, containing about 25 million Chinese characters. It is manually segmented into word sequences, according to the word segmentation specification of Peking University (Yu et al., 2003). The testing text that is provided by Peking University comes from the second international Chinese word segmentation bakeoff organized by SIGHAN. The testing text is a part of People&apos;s daily 2001, consisting of about 170K Chinese characters. The vocabulary is automatically extracted from the training corpus, and the words occurring only once are removed. Finally, about 67K words are included in the vocabulary. The fullbigram model and the unigram model are trained by CMU language model toolkit (Clarkson and Rosenfeld, 1997). Without any count cut-off, the full-bigram model contains about 2 m</context>
</contexts>
<marker>Yu, Duan, Zhu, Swen, Chang, 2003</marker>
<rawString>Shiwen Yu, Huiming Duan, Xuefeng Zhu, Bin Swen, and Baobao Chang. 2003. Specification for Corpus Processing at Peking University: Word Segmentation, POS Tagging and Phonetic Notation. Journal of Chinese Language and Computing, 13(2): 121-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua-Ping Zhang</author>
<author>Hong-Kui Yu</author>
<author>De-Yi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>HHMM-based Chinese Lexical Analyzer ICTCLAS,</title>
<date>2003</date>
<booktitle>In Proc. of the ACL-2003 Workshop on Chinese Language Processing (SIGHAN),</booktitle>
<pages>184--187</pages>
<contexts>
<context position="1384" citStr="Zhang et al., 2003" startWordPosition="209" endWordPosition="212">ge model of desired size. Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method. At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%. Correlation between language model perplexity and word segmentation performance is also discussed. 1 Introduction Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004). In Gao et al. (2003), an approach based on source-channel model for Chinese word segmentation was proposed. Gao et al. (2005) further developed it to a linear mixture model. In these statistical models, language models are essential for word segmentation disambiguation. However, an uncompressed language model is usually too large for practical use since all realistic applications have memory constraints. Therefore, language model pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters explicitly stored in it, </context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun Liu. 2003. HHMM-based Chinese Lexical Analyzer ICTCLAS, In Proc. of the ACL-2003 Workshop on Chinese Language Processing (SIGHAN), pages 184-187.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>