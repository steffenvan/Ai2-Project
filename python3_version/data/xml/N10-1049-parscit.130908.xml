<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.062776">
<title confidence="0.961209">
The Simple Truth about Dependency and Phrase Structure Representations
An Opinion Piece
</title>
<author confidence="0.997259">
Owen Rambow
</author>
<affiliation confidence="0.992573">
CCLS, Columbia University
</affiliation>
<address confidence="0.912622">
New York, NY, USA
</address>
<email confidence="0.999535">
rambow@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993915" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999825875">
There are many misconceptions about de-
pendency representations and phrase structure
representations for syntax. They are partly due
to terminological confusion, partly due to a
lack of meta-scientific clarity about the roles
of representations and linguistic theories. This
opinion piece argues for a simple but clear
view of syntactic representation.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9944226">
To the machine learning community, treebanks are
just collections of data, like pixels with captions,
structural and behavioral facts about genes, or ob-
servations about wild boar populations. In contrast,
to us computational linguists, treebanks are not nat-
urally occurring data at all: they are the result of
a very complex annotation process. While the text
that is annotated (usually) is naturally occurring, the
annotation itself is already the result of a scientific
activity. This opinion piece argues that the level of
discourse about treebanks often found in our com-
munity does not reflect this fact (presumably due
to the influence of the brute machine learning per-
spective). We, as a community of computational lin-
guists, need to be very precise when talking about
treebanks and syntactic representations in general.
So let’s start with three very important concepts
which we must always distinguish. The representa-
tion type: what type of mathematical object is used
to represent syntactic facts? In this opinion piece,
I only consider dependency trees (DTs) and phrase
structure trees (PSTs) (Section 2). The represented
syntactic content: the morphological and syntactic
facts of the analyzed sentence (Section 3). The syn-
tactic theory: it explains how syntactic content is
represented in the chosen representation type (Sec-
tion 4).
A crucial confusing factor is the fact that the terms
dependency and phrase structure both have both a
mathematical and a linguistic meaning. The math-
ematical meaning refers representation types. The
linguistic meaning refers to syntactic content. I dis-
cuss this issue in Section 3. I discuss the issue of
converting between DTs and PSTs in Section 5, as
an example of how my proposed conceptualization
of syntactic representation throws light on a compu-
tational problem.
This opinion piece will be a success if after read-
ing it, the reader concludes that actually he or she
knew this all along. In fact, this opinion piece does
not advocate for a controversial position; its mission
is to make its readers be more precise when talking
about syntactic representations. This opinion piece
is intentionally polemical for rhetorical reasons.
2 DTs and PSTs as Representation Types
Assume we have two disjoint symbol sets: a set of
terminal symbols which contains the words of the
language we are describing; and a set of nontermi-
nal symbols. A Dependency Tree (DT) is a tree
in which all nodes are labeled with words (elements
of the set of terminal symbols) or empty strings. A
Phrase Structure Tree (PST) is a tree in which all
and only the leaf nodes are labeled with words or
empty strings, and internal nodes are labeled with
nonterminal symbols. There is nothing more to the
</bodyText>
<page confidence="0.971397">
337
</page>
<subsubsectionHeader confidence="0.578544">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 337–340,
</subsubsectionHeader>
<subsectionHeader confidence="0.276792">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.9998502">
definitions. Trees of both types can have many other
properties which are not part of the two definitions,
and which do not follow from the definitions. I men-
tion some such properties.
Unordered trees. DTs and PSTs can be ordered or
unordered. For example, the Prague Theory (Sgall
et al., 1986) uses unordered DTs at the deeper level
of representation and ordered DTs at a more surfacy
level. GPSG (Gazdar et al., 1985) uses unordered
trees (or at any rate context-free rules whose right-
hand side is ordered by a separate component of the
grammar), as does current Chomskyan theory (the
PST at spell-out may be unordered).
Empty categories. Empty categories can be empty
pronouns, or traces, which are co-indexed with a
word elsewhere in the tree. Empty pronouns are
widely used in both DT- and PST-based represen-
tations. While most DT-based approaches do not
use traces, Lombardo and Lesmo (1998) do; and
while traces are commonly found in PST-based ap-
proaches, there are many that do not use them, such
as the c-structure of LFG.
Discontinuous Constituents or Non-Projectivity.
Both types of trees can be used with or without dis-
continuous constituents; PSTs are more likely to use
traces to avoid discontinuous constituents, but lin-
guistic proposals for PSTs with discontinuous con-
stituents have been made (work by McCawley, or
(Becker et al., 1991)).
Labeled Arcs. In DTs, arcs often have labels; arcs
in PSTs usually do not, but we can of course label
PST arcs as well, as is done in the German TIGER
corpus.I note that in both DTs and PSTs we can rep-
resent the arc label as a feature on the daughter node,
or as a separate node.
</bodyText>
<sectionHeader confidence="0.99551" genericHeader="method">
3 Syntactic Content
</sectionHeader>
<bodyText confidence="0.999991808510638">
While there is lots of disagreement about the proper
representation type for syntax, there is actually a
broad consensus among theoretical and descriptive
syntacticians of all persuasions about the range of
syntactic phenomena that exist. What exactly is this
content, then? It is not a theory-neutral representa-
tion of syntax (Section 4). Rather, it is the empirical
matter which linguistic theory attempts to represent
or explain. We cannot represent it without a theory,
but we can refer to it without a theory, using names
such as control constructions or transitive verb. In
the same manner, we use the word light and physi-
cists will agree on what the phenomenon is, but we
cannot represent light within a theory without choos-
ing a representation as either particles or wave.
Note that in linguistics, the terms dependency and
phrase structure refer to syntactic content, i.e., syn-
tactic facts we can represent. Syntactic depen-
dency is direct relation between words. Usually,
this relation is labeled (or typed), and is identical
to (or subsumes) the notion of grammatical func-
tion, which covers relations such as SUBJECT, OB-
JECT, TEMPORAL-ADJUNCT and so forth. Syn-
tactic phrase structure, also known as syntactic
constituency structure is recursive representation
using sets of one or more linguistic units (words
and empty strings), such that at each level, each
set (constituent) acts as a unit syntactically. Lin-
guistic phrase structure is most conveniently ex-
pressed in a phrase structure tree, while linguis-
tic dependency is most conveniently expressed in
a dependency tree. However, we can express the
same content in either type of tree! For exam-
ple, the English Penn Treebank (PTB) encodes the
predicate-argument structure of English using struc-
tural conventions and special nonterminal labels
(“dashtags”), such as NP-SBJ. And a dependency
tree represents constituency: each node can be in-
terpreted both as a preterminal node (X°) and as a
node heading a constituent containing all terminals
included in the subtree it heads (the XP). Of course,
what is more complex to encode in a DT are inter-
mediate projections, such as VP. I leave a fuller dis-
cussion aside for lack of space, but I claim that the
syntactic content which is expressed in intermediate
projections can also be expressed in a DT, through
the use of features and arc labels.
</bodyText>
<sectionHeader confidence="0.992946" genericHeader="method">
4 Syntactic Theory
</sectionHeader>
<bodyText confidence="0.999692333333333">
The choice of representation type does not deter-
mine the representation for a given sentence. This
is obvious, but it needs to be repeated; I have heard
“What is the DT for this sentence?” one too many
times. There are many possible DTs and PSTs, pro-
posed by serious syntacticians, for even simple sen-
</bodyText>
<page confidence="0.99489">
338
</page>
<bodyText confidence="0.999994431034483">
tences, even when the syntacticians agree on what
the syntactic content (a transitive verb with SVO or-
der, for example) of the analysis should be! What is
going on?
In order to make sense of this, we need a third player
in addition to the representation type and the con-
tent. This is the syntactic theory. A linguistic the-
ory chooses a representation type and then defines
a coherent mapping for a well-defined set of con-
tent to the chosen representation type. Here, “coher-
ent representation” means that the different choices
made for conceptually independent content are also
representationally independent, so that we can com-
pose representational choices. Note that a theory
can decide to omit some content; for example, we
can have a theory which does not distinguish raising
from control (the English PTB does not).
There are different types of syntactic theories. A
descriptive theory is an account of the syntax of
one language. Examples of descriptive grammars
include works such as Quirk for English, or the an-
notation manuals of monolingual treebanks, such
as (Marcus et al., 1994; Maamouri et al., 2003).
The annotation manual serves two purposes: it tells
the annotators how to represent a syntactic phe-
nomenon, and it tells the users of the treebank (us!)
how to interpret the annotation. A treebank without
manual is meaningless. And an arborescent struc-
ture does not mean the same thing in all treebanks
(for example, a “flat NP” indicates an unannotated
constituent in the English ATB but a fully annotated
construction in the Arabic Treebank is).
An explanatory theory is a theory which attempts
to account for the syntax of all languages, for exam-
ple by reducing their diversity to a set of principles
and finite-valued parameters. Linguistic theories
(and explanatory theories in particular) often take
the form of a one-to-many mapping from a simple
representation of syntactic dependency (predicate-
argument structure) to a structural representation
that determines surface word order. The linguistic
theory itself is formulated as a (computational) de-
vice that relates the deeper level to the more surfacy
level. LFG has a very pure expression of this ap-
proach, with the deeper level expressed using a DT
(actually, dependency directed acyclic graphs, but
the distinction is not relevant here), and the surfacy
level expressed using a PST. But the Chomskyan ap-
proaches fit the same paradigm, as do many other
theories of syntax.
Therefore, there is no theory-neutral representation
of a sentence or a set of sentences, because every
representation needs a theory for us to extract its
meaning! Often what is meant by “theory-neutral
tree” is a tree which is interpreted using some no-
tion of consensus theory, perhaps a stripped-down
representation which omits much content for which
there is no consensus on how to represent it.
</bodyText>
<sectionHeader confidence="0.595247" genericHeader="method">
5 Converting Between DTs and PSTs
</sectionHeader>
<bodyText confidence="0.999636">
Converting a set of DS annotations to PS or vice
versa means that we want to obtain a representa-
tion which expresses exactly the same content. This
is frequently done these days as interest in depen-
dency parsing grows but many languages only have
PS treebanks. However, this process is often not un-
derstood.
To start, I observe that uninterpreted structures (i.e.,
structures without a syntactic theory, or trees from
a treebank without a manual) cannot be converted
from or into, as we do not know what they mean
and we cannot know if we are preserving the same
content or not.
Now, my central claim about the possibility of au-
tomatically converting between PSTs and DTs is the
following. If we have an interpretation for the source
representation and the goal representation (as we
must in order for this task to be meaningful), then
we can convert any facts that are represented in the
source structure, and we cannot convert any facts
that are not represented in the source structure. It
is that simple. If we are converting from a source
which contains less information than the target, then
we cannot succeed. For example, if we are convert-
ing from a PS treebank that does not distinguish par-
ticles from prepositions to a DS treebank that does,
then we will fail. General claims about the possi-
bility of conversion (“it is easier to convert PS to
DS than DS to PS”) are therefore meaningless. It
only matters what is represented, not how it is rep-
resented.
There is, however, no guarantee that there is a sim-
ple algorithm for conversion, such as a parametrized
</bodyText>
<page confidence="0.998044">
339
</page>
<bodyText confidence="0.999972575757576">
head percolation algorithm passed down from re-
searcher to researcher like a sorcerer’s incantation.
In general, if the two representations are indepen-
dently devised and both are linguistically motivated,
then we have no reason to believe that the conversion
can be done using a specific simple approach, or us-
ing conversion rules which have some fixed property
(say, the depth of the trees in the rules templates). In
the general case, the only way to write an automatic
converter between two representations is to study the
two annotation manuals and to create a case-by-case
converter, covering all linguistic phenomena repre-
sented in the target representation.
Machine learning-based conversion (for example,
(Xia and Palmer, 2001)) is an interesting exercise,
but it does not give us any general insights into de-
pendency or phrase structure. Suppose the source
contains all the information that the target should
contain. Then if machine learning-based conversion
fails or does not perform completely correctly, the
exercise merely shows that the machine learning is
not adequate. Now suppose that the source does
not contain all the information that the target should
contain. Then no fancy machine learning can ever
provide a completely correct conversion. Also, note
that unlike, for example, parsers which are based
on machine learning and which learn about a natu-
ral phenomenon (language use), machine learning of
conversion merely learns an artificial phenomenon:
the relation between the two syntactic theories in
question, which are created by researchers. (Of
course, in practice, machine learning of automatic
conversion between DT to PSTs is useful.)
</bodyText>
<sectionHeader confidence="0.999248" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999926636363637">
I have argued that when talking about dependency
and phrase structure representations, one should al-
ways distinguish the type of representation (depen-
dency or phrase structure) from the content of the
representation, and one needs to understand (and
make explicit if it is implicit) the linguistic the-
ory that relates content to representation. Machine
learning researchers have the luxury of treating syn-
tactic representations as mere fodder for their mills;
we as computational linguists do not, since this is
our area of expertise.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999984125">
I would like to thank my colleagues on the Hindi-
Urdu treebank project (Bhatt et al., 2009) (NSF
grant CNS-0751089) for spirited discussions about
the issues discussed here. I would like to thank Syl-
vain Kahane, Yoav Goldberg, and Joakim Nivre for
comments that have helped me improve this paper.
The expressed opinions have been influenced by far
too many people to thank individually here.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997684">
Tilman Becker, Aravind Joshi, and Owen Rambow.
1991. Long distance scrambling and tree adjoining gram-
mars. In Fifth Conference of the European Chapter of the
Association for Computational Linguistics (EACL’91),
pages 21–26. ACL.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Sharma, and Fei Xia. 2009.
A multi-representational and multi-layered treebank for
hindi/urdu. In Proceedings of the Third Linguistic Anno-
tation Workshop, pages 186–189, Suntec, Singapore.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan
Sag. 1985. Generalized Phrase Structure Grammar.
Harvard University Press, Cambridge, Mass.
Vincenzo Lombardo and Leonardo Lesmo. 1998. For-
mal aspects and parsing issue of dependency theory. In
36th Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Compu-
tational Linguistics (COLING-ACL’98), pages 787–793,
Montr´eal, Canada.
Mohamed Maamouri, Ann Bies, Hubert Jin, and Tim
Buckwalter. 2003. Arabic treebank: Part 1 v 2.0. Dis-
tributed by the Linguistic Data Consortium. LDC Cata-
log No.: LDC2003T06.
Mohamed Maamouri, Ann Bies, and Tim Buckwalter.
2004. The Penn Arabic Treebank: Building a large-
scale annotated arabic corpus. In NEMLAR Conference
on Arabic Language Resources and Tools, Cairo, Egypt.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn Treebank: Annotating predicate argu-
ment structure. In Proceedings of the ARPA Human Lan-
guage Technology Workshop.
Igor A. Mel’ˇcuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press, New York.
P. Sgall, E. Hajiˇcov´a, and J. Panevov´a. 1986. The mean-
ing of the sentence and its semantic and pragmatic as-
pects. Reidel, Dordrecht.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structure to phrase structures. In hlt2001, pages
61–65.
</reference>
<page confidence="0.998285">
340
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.306149">
<title confidence="0.994178">The Simple Truth about Dependency and Phrase Structure Representations An Opinion Piece</title>
<author confidence="0.971159">Owen</author>
<address confidence="0.638192">CCLS, Columbia</address>
<author confidence="0.424208">New York</author>
<author confidence="0.424208">NY</author>
<email confidence="0.999787">rambow@ccls.columbia.edu</email>
<abstract confidence="0.998919111111111">There are many misconceptions about dependency representations and phrase structure representations for syntax. They are partly due to terminological confusion, partly due to a lack of meta-scientific clarity about the roles of representations and linguistic theories. This opinion piece argues for a simple but clear view of syntactic representation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tilman Becker</author>
<author>Aravind Joshi</author>
<author>Owen Rambow</author>
</authors>
<title>Long distance scrambling and tree adjoining grammars.</title>
<date>1991</date>
<booktitle>In Fifth Conference of the European Chapter of the Association for Computational Linguistics (EACL’91),</booktitle>
<pages>21--26</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4823" citStr="Becker et al., 1991" startWordPosition="767" endWordPosition="770">re in the tree. Empty pronouns are widely used in both DT- and PST-based representations. While most DT-based approaches do not use traces, Lombardo and Lesmo (1998) do; and while traces are commonly found in PST-based approaches, there are many that do not use them, such as the c-structure of LFG. Discontinuous Constituents or Non-Projectivity. Both types of trees can be used with or without discontinuous constituents; PSTs are more likely to use traces to avoid discontinuous constituents, but linguistic proposals for PSTs with discontinuous constituents have been made (work by McCawley, or (Becker et al., 1991)). Labeled Arcs. In DTs, arcs often have labels; arcs in PSTs usually do not, but we can of course label PST arcs as well, as is done in the German TIGER corpus.I note that in both DTs and PSTs we can represent the arc label as a feature on the daughter node, or as a separate node. 3 Syntactic Content While there is lots of disagreement about the proper representation type for syntax, there is actually a broad consensus among theoretical and descriptive syntacticians of all persuasions about the range of syntactic phenomena that exist. What exactly is this content, then? It is not a theory-neu</context>
</contexts>
<marker>Becker, Joshi, Rambow, 1991</marker>
<rawString>Tilman Becker, Aravind Joshi, and Owen Rambow. 1991. Long distance scrambling and tree adjoining grammars. In Fifth Conference of the European Chapter of the Association for Computational Linguistics (EACL’91), pages 21–26. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rajesh Bhatt</author>
<author>Bhuvana Narasimhan</author>
<author>Martha Palmer</author>
<author>Owen Rambow</author>
<author>Dipti Sharma</author>
<author>Fei Xia</author>
</authors>
<title>A multi-representational and multi-layered treebank for hindi/urdu.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Linguistic Annotation Workshop,</booktitle>
<pages>186--189</pages>
<publisher>Harvard University Press,</publisher>
<location>Suntec, Singapore. Gerald Gazdar, Ewan</location>
<marker>Bhatt, Narasimhan, Palmer, Rambow, Sharma, Xia, 2009</marker>
<rawString>Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer, Owen Rambow, Dipti Sharma, and Fei Xia. 2009. A multi-representational and multi-layered treebank for hindi/urdu. In Proceedings of the Third Linguistic Annotation Workshop, pages 186–189, Suntec, Singapore. Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincenzo Lombardo</author>
<author>Leonardo Lesmo</author>
</authors>
<title>Formal aspects and parsing issue of dependency theory.</title>
<date>1998</date>
<booktitle>In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL’98),</booktitle>
<pages>787--793</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="4368" citStr="Lombardo and Lesmo (1998)" startWordPosition="693" endWordPosition="696">y (Sgall et al., 1986) uses unordered DTs at the deeper level of representation and ordered DTs at a more surfacy level. GPSG (Gazdar et al., 1985) uses unordered trees (or at any rate context-free rules whose righthand side is ordered by a separate component of the grammar), as does current Chomskyan theory (the PST at spell-out may be unordered). Empty categories. Empty categories can be empty pronouns, or traces, which are co-indexed with a word elsewhere in the tree. Empty pronouns are widely used in both DT- and PST-based representations. While most DT-based approaches do not use traces, Lombardo and Lesmo (1998) do; and while traces are commonly found in PST-based approaches, there are many that do not use them, such as the c-structure of LFG. Discontinuous Constituents or Non-Projectivity. Both types of trees can be used with or without discontinuous constituents; PSTs are more likely to use traces to avoid discontinuous constituents, but linguistic proposals for PSTs with discontinuous constituents have been made (work by McCawley, or (Becker et al., 1991)). Labeled Arcs. In DTs, arcs often have labels; arcs in PSTs usually do not, but we can of course label PST arcs as well, as is done in the Germ</context>
</contexts>
<marker>Lombardo, Lesmo, 1998</marker>
<rawString>Vincenzo Lombardo and Leonardo Lesmo. 1998. Formal aspects and parsing issue of dependency theory. In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL’98), pages 787–793, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Hubert Jin</author>
<author>Tim Buckwalter</author>
</authors>
<date>2003</date>
<booktitle>Arabic treebank: Part 1 v 2.0. Distributed by the Linguistic Data Consortium. LDC Catalog No.: LDC2003T06.</booktitle>
<contexts>
<context position="8919" citStr="Maamouri et al., 2003" startWordPosition="1454" endWordPosition="1457">” means that the different choices made for conceptually independent content are also representationally independent, so that we can compose representational choices. Note that a theory can decide to omit some content; for example, we can have a theory which does not distinguish raising from control (the English PTB does not). There are different types of syntactic theories. A descriptive theory is an account of the syntax of one language. Examples of descriptive grammars include works such as Quirk for English, or the annotation manuals of monolingual treebanks, such as (Marcus et al., 1994; Maamouri et al., 2003). The annotation manual serves two purposes: it tells the annotators how to represent a syntactic phenomenon, and it tells the users of the treebank (us!) how to interpret the annotation. A treebank without manual is meaningless. And an arborescent structure does not mean the same thing in all treebanks (for example, a “flat NP” indicates an unannotated constituent in the English ATB but a fully annotated construction in the Arabic Treebank is). An explanatory theory is a theory which attempts to account for the syntax of all languages, for example by reducing their diversity to a set of princ</context>
</contexts>
<marker>Maamouri, Bies, Jin, Buckwalter, 2003</marker>
<rawString>Mohamed Maamouri, Ann Bies, Hubert Jin, and Tim Buckwalter. 2003. Arabic treebank: Part 1 v 2.0. Distributed by the Linguistic Data Consortium. LDC Catalog No.: LDC2003T06.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
</authors>
<title>The Penn Arabic Treebank: Building a largescale annotated arabic corpus.</title>
<date>2004</date>
<booktitle>In NEMLAR Conference on Arabic Language Resources</booktitle>
<marker>Maamouri, Bies, Buckwalter, 2004</marker>
<rawString>Mohamed Maamouri, Ann Bies, and Tim Buckwalter. 2004. The Penn Arabic Treebank: Building a largescale annotated arabic corpus. In NEMLAR Conference on Arabic Language Resources and Tools, Cairo, Egypt. M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>Press,</publisher>
<institution>State University of New York</institution>
<location>New</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Igor A. Mel’ˇcuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press, New York. P. Sgall, E. Hajiˇcov´a, and J. Panevov´a. 1986. The meaning of the sentence and its semantic and pragmatic aspects. Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
</authors>
<title>Converting dependency structure to phrase structures.</title>
<date>2001</date>
<booktitle>In hlt2001,</booktitle>
<pages>61--65</pages>
<contexts>
<context position="12994" citStr="Xia and Palmer, 2001" startWordPosition="2129" endWordPosition="2132">f the two representations are independently devised and both are linguistically motivated, then we have no reason to believe that the conversion can be done using a specific simple approach, or using conversion rules which have some fixed property (say, the depth of the trees in the rules templates). In the general case, the only way to write an automatic converter between two representations is to study the two annotation manuals and to create a case-by-case converter, covering all linguistic phenomena represented in the target representation. Machine learning-based conversion (for example, (Xia and Palmer, 2001)) is an interesting exercise, but it does not give us any general insights into dependency or phrase structure. Suppose the source contains all the information that the target should contain. Then if machine learning-based conversion fails or does not perform completely correctly, the exercise merely shows that the machine learning is not adequate. Now suppose that the source does not contain all the information that the target should contain. Then no fancy machine learning can ever provide a completely correct conversion. Also, note that unlike, for example, parsers which are based on machine</context>
</contexts>
<marker>Xia, Palmer, 2001</marker>
<rawString>Fei Xia and Martha Palmer. 2001. Converting dependency structure to phrase structures. In hlt2001, pages 61–65.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>