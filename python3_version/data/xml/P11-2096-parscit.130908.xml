<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018404">
<title confidence="0.996366">
An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques
</title>
<author confidence="0.994928">
Donald Metzler
</author>
<affiliation confidence="0.995091">
Information Sciences Institute
Univ. of Southern California
</affiliation>
<address confidence="0.534707">
Marina del Rey, CA, USA
</address>
<email confidence="0.998662">
metzler@isi.edu
</email>
<author confidence="0.989793">
Eduard Hovy
</author>
<affiliation confidence="0.9942255">
Information Sciences Institute
Univ. of Southern California
</affiliation>
<address confidence="0.534523">
Marina del Rey, CA, USA
</address>
<email confidence="0.998324">
hovy@isi.edu
</email>
<author confidence="0.996228">
Chunliang Zhang
</author>
<affiliation confidence="0.9958945">
Information Sciences Institute
Univ. of Southern California
</affiliation>
<address confidence="0.534831">
Marina del Rey, CA, USA
</address>
<email confidence="0.998841">
czheng@isi.edu
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999664111111111">
Paraphrase generation is an important task
that has received a great deal of interest re-
cently. Proposed data-driven solutions to the
problem have ranged from simple approaches
that make minimal use of NLP tools to more
complex approaches that rely on numerous
language-dependent resources. Despite all of
the attention, there have been very few direct
empirical evaluations comparing the merits of
the different approaches. This paper empiri-
cally examines the tradeoffs between simple
and sophisticated paraphrase harvesting ap-
proaches to help shed light on their strengths
and weaknesses. Our evaluation reveals that
very simple approaches fare surprisingly well
and have a number of distinct advantages, in-
cluding strong precision, good coverage, and
low redundancy.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99883075">
A popular idiom states that “variety is the spice of
life”. As with life, variety also adds spice and appeal
to language. Paraphrases make it possible to express
the same meaning in an almost unbounded number
of ways. While variety prevents language from be-
ing overly rigid and boring, it also makes it difficult
to algorithmically determine if two phrases or sen-
tences express the same meaning. In an attempt to
address this problem, a great deal of recent research
has focused on identifying, generating, and harvest-
ing phrase- and sentence-level paraphrases (Barzi-
lay and McKeown, 2001; Bhagat and Ravichan-
dran, 2008; Barzilay and Lee, 2003; Bannard and
Callison-Burch, 2005; Callison-Burch, 2008; Lin
and Pantel, 2001; Pang et al., 2003; Pasca and Di-
enes, 2005)
Many data-driven approaches to the paraphrase
problem have been proposed. The approaches vastly
differ in their complexity and the amount of NLP re-
sources that they rely on. At one end of the spec-
trum are approaches that generate paraphrases from
a large monolingual corpus and minimally rely on
NLP tools. Such approaches typically make use
of statistical co-occurrences, which act as a rather
crude proxy for semantics. At the other end of
the spectrum are more complex approaches that re-
quire access to bilingual parallel corpora and may
also rely on part-of-speech (POS) taggers, chunkers,
parsers, and statistical machine translation tools.
Constructing large comparable and bilingual cor-
pora is expensive and, in some cases, impossible.
Despite all of the previous research, there have
not been any evaluations comparing the quality of
simple and sophisticated data-driven approaches for
generating paraphrases. Evaluation is not only im-
portant from a practical perspective, but also from
a methodological standpoint, as well, since it is of-
ten more fruitful to devote attention to building upon
the current state-of-the-art as opposed to improv-
ing upon less effective approaches. Although the
more sophisticated approaches have garnered con-
siderably more attention from researchers, from a
practical perspective, simplicity, quality, and flexi-
bility are the most important properties. But are sim-
ple methods adequate enough for the task?
The primary goal of this paper is to take a small
step towards addressing the lack of comparative
evaluations. To achieve this goal, we empirically
</bodyText>
<page confidence="0.977312">
546
</page>
<note confidence="0.5834285">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 546–551,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999918090909091">
evaluate three previously proposed paraphrase gen-
eration techniques, which range from very simple
approaches that make use of little-to-no NLP or
language-dependent resources to more sophisticated
ones that heavily rely on such resources. Our eval-
uation helps develop a better understanding of the
strengths and weaknesses of each type of approach.
The evaluation also brings to light additional proper-
ties, including the number of redundant paraphrases
generated, that future approaches and evaluations
may want to consider more carefully.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999930434782608">
Instead of exhaustively covering the entire spectrum
of previously proposed paraphrasing techniques, our
evaluation focuses on two families of data-driven ap-
proaches that are widely studied and used. More
comprehensive surveys of data-driven paraphrasing
techniques can be found in Androutsopoulos and
Malakasiotis (2010) and Madnani and Dorr (2010).
The first family of approaches that we consider
harvests paraphrases from monolingual corpora us-
ing distributional similarity. The DIRT algorithm,
proposed by Lin and Pantel (2001), uses parse tree
paths as contexts for computing distributional sim-
ilarity. In this way, two phrases were considered
similar if they occurred in similar contexts within
many sentences. Although parse tree paths serve as
rich representations, they are costly to construct and
yield sparse representations. The approach proposed
by Pasca and Dienes (2005) avoided the costs asso-
ciated with parsing by using n-gram contexts. Given
the simplicity of the approach, the authors were able
to harvest paraphrases from a very large collection
of news articles. Bhagat and Ravichandran (2008)
proposed a similar approach that used noun phrase
chunks as contexts and locality sensitive hashing
to reduce the dimensionality of the context vectors.
Despite their simplicity, such techniques are suscep-
tible to a number of issues stemming from the distri-
butional assumption. For example, such approaches
have a propensity to assign large scores to antonyms
and other semantically irrelevant phrases.
The second line of research uses comparable or
bilingual corpora as the ‘pivot’ that binds para-
phrases together (Barzilay and McKeown, 2001;
Barzilay and Lee, 2003; Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Pang et al.,
2003). Amongst the most effective recent work,
Bannard and Callison-Burch (2005) show how dif-
ferent English translations of the same entry in a
statistically-derived translation table can be viewed
as paraphrases. The recent work by Zhao et al.
(Zhao et al., 2009) uses a generalization of DIRT-
style patterns to generate paraphrases from a bilin-
gual parallel corpus. The primary drawback of these
type of approaches is that they require a consider-
able amount of resource engineering that may not be
available for all languages, domains, or applications.
</bodyText>
<sectionHeader confidence="0.997819" genericHeader="method">
3 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999984615384615">
The goal of our experimental evaluation is to ana-
lyze the effectiveness of a variety of paraphrase gen-
eration techniques, ranging from simple to sophis-
ticated. Our evaluation focuses on generating para-
phrases for verb phrases, which tend to exhibit more
variation than other types of phrases. Furthermore,
our interest in paraphrase generation was initially
inspired by challenges encountered during research
related to machine reading (Barker et al., 2007). In-
formation extraction systems, which are key compo-
nent of machine reading systems, can use paraphrase
technology to automatically expand seed sets of re-
lation triggers, which are commonly verb phrases.
</bodyText>
<subsectionHeader confidence="0.995862">
3.1 Systems
</subsectionHeader>
<bodyText confidence="0.9999413125">
Our evaluation compares the effectiveness of the
following paraphrase harvesting approaches:
PD: The basic distributional similarity-inspired
approach proposed by Pasca and Dienes (2005)
that uses variable-length n-gram contexts and
overlap-based scoring. The context of a phrase
is defined as the concatenation of the n-grams
immediately to the left and right of the phrase. We
set the minimum length of an n-gram context to be
2 and the maximum length to be 3. The maximum
length of a phrase is set to 5.
BR: The distributional similarity approach proposed
by Bhagat and Ravichandran (2008) that uses noun
phrase chunks as contexts and locality sensitive
hashing to reduce the dimensionality of the contex-
tual vectors.
</bodyText>
<page confidence="0.984355">
547
</page>
<bodyText confidence="0.999820647058823">
BCB-S: An extension of the Bannard Callison-
Burch (Bannard and Callison-Burch, 2005)
approach that constrains the paraphrases to have the
same syntactic type as the original phrase (Callison-
Burch, 2008). We constrained all paraphrases to be
verb phrases.
We chose these three particular systems because
they span the spectrum of paraphrase approaches, in
that the PD approach is simple and does not rely on
any NLP resources while the BCB-S approach is so-
phisticated and makes heavy use of NLP resources.
For the two distributional similarity approaches
(PD and BR), paraphrases were harvested from the
English Gigaword Fourth Edition corpus and scored
using the cosine similarity between PMI weighted
contextual vectors. For the BCB-S approach, we
made use of a publicly available implementation1.
</bodyText>
<subsectionHeader confidence="0.997896">
3.2 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.994170153846154">
We randomly sampled 50 verb phrases from 1000
news articles about terrorism and another 50 verb
phrases from 500 news articles about American
football. Individual occurrences of verb phrases
were sampled, which means that more common verb
phrases were more likely to be selected and that a
given phrase could be selected multiple times. This
sampling strategy was used to evaluate the systems
across a realistic sample of phrases. To obtain a
richer class of phrases beyond basic verb groups, we
defined verb phrases to be contiguous sequences of
tokens that matched the following POS tag pattern:
(TO I IN I RB I MD I VB)+.
Following the methodology used in previous
paraphrase evaluations (Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Kok and Brock-
ett, 2010), we presented annotators with two sen-
tences. The first sentence was randomly selected
from amongst all of the sentences in the evaluation
corpus that contain the original phrase. The second
sentence was the same as the first, except the orig-
inal phrase is replaced with the system generated
paraphrase. Annotators were given the following
options, which were adopted from those described
by Kok and Brockett (2010), for each sentence pair:
0) Different meaning; 1) Same meaning; revised is
</bodyText>
<footnote confidence="0.989177">
1Available at http://www.cs.jhu.edu/˜ccb/.
</footnote>
<bodyText confidence="0.999652363636363">
grammatically incorrect; and 2) Same meaning; re-
vised is grammatically correct. Table 1 shows three
example sentence pairs and their corresponding an-
notations according to the guidelines just described.
Amazon’s Mechanical Turk service was used to
collect crowdsourced annotations. For each para-
phrase system, we retrieve (up to) 10 paraphrases
for each phrase in the evaluation set. This yields
a total of 6,465 unique (phrase, paraphrase) pairs
after pooling results from all systems. Each Me-
chanical Turk HIT consisted of 12 sentence pairs.
To ensure high quality annotations and help iden-
tify spammers, 2 of the 12 sentence pairs per HIT
were actually “hidden tests” for which the correct
answer was known by us. We automatically rejected
any HITs where the worker failed either of these hid-
den tests. We also rejected all work from annotators
who failed at least 25% of their hidden tests. We
collected a total of 51,680 annotations. We rejected
65% of the annotations based on the hidden test fil-
tering just described, leaving 18,150 annotations for
our evaluation. Each sentence pair received a mini-
mum of 1, a median of 3, and maximum of 6 anno-
tations. The raw agreement of the annotators (after
filtering) was 77% and the Fleiss’ Kappa was 0.43,
which signifies moderate agreement (Fleiss, 1971;
Landis and Koch, 1977).
The systems were evaluated in terms of coverage
and expected precision at k. Coverage is defined
as the percentage of phrases for which the system
returned at least one paraphrase. Expected precision
at k is the expected number of correct paraphrases
amongst the top k returned, and is computed as:
</bodyText>
<equation confidence="0.996557">
k
1
E[i@k] = k
</equation>
<bodyText confidence="0.999883545454545">
where pi is the proportion of positive annotations
for item i. When computing the mean expected
precision over a set of input phrases, only those
phrases that generate one or more paraphrases is
considered in the mean. Hence, if precision were
to be averaged over all 100 phrases, then systems
with poor coverage would perform significantly
worse. Thus, one should take a holistic view of the
results, rather than focus on coverage or precision
in isolation, but consider them, and their respective
tradeoffs, together.
</bodyText>
<equation confidence="0.951657">
pi
i=1
</equation>
<page confidence="0.988191">
548
</page>
<note confidence="0.453661">
Sentence Pair Annotation
</note>
<bodyText confidence="0.884191916666667">
A five-man presidential council for the independent state newly proclaimed in south Yemen 0
was named overnight Saturday, it was officially announced in Aden.
A five-man presidential council for the independent state newly proclaimed in south Yemen
was named overnight Saturday, it was cancelled in Aden.
Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem to 1
protest against the killing of Sharif.
Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem in
protest of against the killing of Sharif.
It says that foreign companies have no greater right to compensation – establishing debts at a 2
1/1 ratio of the dollar to the peso – than Argentine citizens do.
It says that foreign companies have no greater right to compensation – setting debts at a 1/1
ratio of the dollar to the peso – than Argentine citizens do.
</bodyText>
<tableCaption confidence="0.9988335">
Table 1: Example annotated sentence pairs. In each pair, the first sentence is the original and the second has a system-
generated paraphrase filled in (denoted by the bold text).
</tableCaption>
<table confidence="0.999892">
Method C Lenient Strict
P1 P5 P10 P1 P5 P10
PD 86 .48 .42 .36 .25 .22 .19
BR 84 .83 .65 .52 .16 .17 .15
BCB-S 62 .63 .45 .34 .22 .17 .13
</table>
<tableCaption confidence="0.993557">
Table 2: Coverage (C) and expected precision at k (Pk)
under lenient and strict evaluation criteria.
</tableCaption>
<table confidence="0.9997518">
Method Lenient Strict
P1 P5 P10 P1 P5 P10
PD .26 .22 .20 .19 .16 .15
BR .05 .10 .11 .04 .05 .05
BCB-S .24 .25 .20 .17 .14 .10
</table>
<tableCaption confidence="0.980771">
Table 3: Expected precision at k (Pk) when considering
redundancy under lenient and strict evaluation criteria.
</tableCaption>
<bodyText confidence="0.9982915">
Two binarized evaluation criteria are reported.
The lenient criterion allows for grammatical er-
rors in the paraphrased sentence, while the strict
criterion does not.
</bodyText>
<subsectionHeader confidence="0.999583">
3.3 Basic Results
</subsectionHeader>
<bodyText confidence="0.999979">
Table 2 summarizes the results of our evaluation.
For this evaluation, all 100 verb phrases were run
through each system. The paraphrases returned by
the systems were then ranked (ordered) in descend-
ing order of their score, thus placing the highest
scoring item at rank 1. Bolded values represent the
best result for a given metric.
As expected, the results show that the systems
perform significantly worse under the strict evalu-
ation criteria, which requires the paraphrased sen-
tences to be grammatically correct. None of the ap-
proaches tested used any information from the eval-
uation sentences (other than the fact a verb phrase
was to be filled in). Recent work showed that us-
ing language models and/or syntactic clues from the
evaluation sentence can improve the grammatical-
ity of the paraphrased sentences (Callison-Burch,
2008). Such approaches could likely be used to im-
prove the quality of all of the approaches under the
strict evaluation criteria.
In terms of coverage, the distributional similarity
approaches performed the best. In another set of ex-
periments, we used the PD method to harvest para-
phrases from a large Web corpus, and found that the
coverage was 98%. Achieving similar coverage with
resource-dependent approaches would likely require
more human and machine effort.
</bodyText>
<subsectionHeader confidence="0.971149">
3.4 Redundancy
</subsectionHeader>
<bodyText confidence="0.999984333333333">
After manually inspecting the results returned by the
various paraphrase systems, we noticed that some
approaches returned highly redundant paraphrases
that were of limited practical use. For example,
for the phrase “were losing”, the BR system re-
turned “are losing”, “have been losing”, “have lost”,
“lose”, “might lose”, “had lost”, “stand to lose”,
“who have lost” and “would lose” within the top 10
paraphrases. All of these are simple variants that
contain different forms of the verb “lose”. Under
the lenient evaluation criterion almost all of these
paraphrases would be marked as correct, since the
</bodyText>
<page confidence="0.996495">
549
</page>
<bodyText confidence="0.999879181818182">
same verb is being returned with some grammati-
cal modifications. While highly redundant output
of this form may be useful for some tasks, for oth-
ers (such as information extraction) is it more useful
to identify paraphrases that contain a diverse, non-
redundant set of verbs.
Therefore, we carried out another evaluation
aimed at penalizing highly redundant outputs. For
each approach, we manually identified all of the
paraphrases that contained the same verb as the
main verb in the original phrase. During evalua-
tion, these “redundant” paraphrases were regarded
as non-related.
The results from this experiment are provided in
Table 3. The results are dramatically different com-
pared to those in Table 2, suggesting that evaluations
that do not consider this type of redundancy may
over-estimate actual system quality. The percent-
age of results marked as redundant for the BCB-S,
BR, and PD approaches were 22.6%, 52.5%, and
22.9%, respectively. Thus, the BR system, which
appeared to have excellent (lenient) precision in our
initial evaluation, returns a very large number of re-
dundant paraphrases. This remarkably reduces the
lenient P1 from 0.83 in our initial evaluation to just
0.05 in our redundancy-based evaluation. The BCB-
S and PD approaches return a comparable number of
redundant results. As with our previous evaluation,
the BCB-S approach tends to perform better under
the lenient evaluation, while PD is better under the
strict evaluation. Estimated 95% confidence inter-
vals show all differences between BCB-S and PD
are statistically significant, except for lenient P10.
Of course, existing paraphrasing approaches do
not explicitly account for redundancy, and hence this
evaluation is not completely fair. However, these
findings suggest that redundancy may be an impor-
tant issue to consider when developing and evalu-
ating data-driven paraphrase approaches. There are
likely other characteristics, beyond redundancy, that
may also be important for developing robust, effec-
tive paraphrasing techniques. Exploring the space
of such characteristics in a task-dependent manner
is an important direction of future work.
</bodyText>
<subsectionHeader confidence="0.788198">
3.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999930388888889">
In all of our evaluations, we found that the simple
approaches are surprisingly effective in terms of pre-
cision, coverage, and redundancy, making them a
reasonable choice for an “out of the box” approach
for this particular task. However, additional task-
dependent comparative evaluations are necessary to
develop even deeper insights into the pros and cons
of the different types of approaches.
From a high level perspective, it is also important
to note that the precision of these widely used, com-
monly studied paraphrase generation approaches is
still extremely poor. After accounting for redun-
dancy, the best approaches achieve a precision at 1
of less than 20% using the strict criteria and less than
26% when using the lenient criteria. This suggests
that there is still substantial work left to be done be-
fore the output of these systems can reliably be used
to support other tasks.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99998405882353">
This paper examined the tradeoffs between simple
paraphrasing approaches that do not make use of any
NLP resources and more sophisticated approaches
that use a variety of such resources. Our evaluation
demonstrated that simple harvesting approaches fare
well against more sophisticated approaches, achiev-
ing state-of-the-art precision, good coverage, and
relatively low redundancy.
In the future, we would like to see more em-
pirical evaluations and detailed studies comparing
the practical merits of various paraphrase genera-
tion techniques. As Madnani and Dorr (Madnani
and Dorr, 2010) suggested, it would be beneficial
to the research community to develop a standard,
shared evaluation that would act to catalyze further
advances and encourage more meaningful compara-
tive evaluations of such approaches moving forward.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999365">
The authors gratefully acknowledge the support of
the DARPA Machine Reading Program under AFRL
prime contract no. FA8750-09-C-3705. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government. We would
also like to thank the anonymous reviewers for their
valuable feedback and the Mechanical Turk workers
for their efforts.
</bodyText>
<page confidence="0.994073">
550
</page>
<sectionHeader confidence="0.990228" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855825">
I. Androutsopoulos and P. Malakasiotis. 2010. A survey
of paraphrasing and textual entailment methods. Jour-
nal of Artificial Intelligence Research, 38:135–187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ’05, pages 597–604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw, James
Fan, Noah Friedland, Michael Glass, Jerry Hobbs,
Eduard Hovy, David Israel, Doo Soon Kim, Rutu
Mulkar-Mehta, Sourabh Patwardhan, Bruce Porter,
Dan Tecuci, and Peter Yeh. 2007. Learning by read-
ing: a prototype system, performance baseline and
lessons learned. In Proceedings of the 22nd national
conference on Artificial intelligence - Volume 1, pages
280–286. AAAI Press.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ’03, pages 16–
23, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, ACL ’01, pages 50–57,
Morristown, NJ, USA. Association for Computational
Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT, pages 674–
682, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ’08, pages
196–205, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Joseph L. Fleiss. 1971. Measuring Nominal Scale
Agreement Among Many Raters. Psychological Bul-
letin, 76(5):378–382.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ’10, pages 145–153, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159–174, March.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Nat. Lang. Eng.,
7:343–360, December.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist., 36:341–387.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: ex-
tracting paraphrases and generating new sentences.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1, NAACL ’03, pages 102–109, Morristown,
NJ, USA. Association for Computational Linguistics.
Marius Pasca and Pter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the web.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 119–130. Springer Berlin / Heidelberg.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting paraphrase patterns from bilin-
gual parallel corpora. Natural Language Engineering,
15(Special Issue 04):503–526.
</reference>
<page confidence="0.998052">
551
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.233198">
<title confidence="0.999806">An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</title>
<author confidence="0.960936">Donald</author>
<affiliation confidence="0.9957295">Information Sciences Univ. of Southern</affiliation>
<author confidence="0.95585">Marina del Rey</author>
<author confidence="0.95585">CA</author>
<email confidence="0.999359">metzler@isi.edu</email>
<author confidence="0.612013">Eduard</author>
<affiliation confidence="0.996843">Information Sciences Univ. of Southern</affiliation>
<author confidence="0.932891">Marina del Rey</author>
<author confidence="0.932891">CA</author>
<email confidence="0.99872">hovy@isi.edu</email>
<author confidence="0.488247">Chunliang</author>
<affiliation confidence="0.9881005">Information Sciences Univ. of Southern</affiliation>
<author confidence="0.943397">Marina del Rey</author>
<author confidence="0.943397">CA</author>
<email confidence="0.999436">czheng@isi.edu</email>
<abstract confidence="0.998616052631579">Paraphrase generation is an important task that has received a great deal of interest recently. Proposed data-driven solutions to the problem have ranged from simple approaches that make minimal use of NLP tools to more complex approaches that rely on numerous language-dependent resources. Despite all of the attention, there have been very few direct empirical evaluations comparing the merits of the different approaches. This paper empirically examines the tradeoffs between simple and sophisticated paraphrase harvesting approaches to help shed light on their strengths and weaknesses. Our evaluation reveals that very simple approaches fare surprisingly well and have a number of distinct advantages, including strong precision, good coverage, and low redundancy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Androutsopoulos</author>
<author>P Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>38--135</pages>
<contexts>
<context position="4646" citStr="Androutsopoulos and Malakasiotis (2010)" startWordPosition="685" endWordPosition="688"> Our evaluation helps develop a better understanding of the strengths and weaknesses of each type of approach. The evaluation also brings to light additional properties, including the number of redundant paraphrases generated, that future approaches and evaluations may want to consider more carefully. 2 Related Work Instead of exhaustively covering the entire spectrum of previously proposed paraphrasing techniques, our evaluation focuses on two families of data-driven approaches that are widely studied and used. More comprehensive surveys of data-driven paraphrasing techniques can be found in Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). The first family of approaches that we consider harvests paraphrases from monolingual corpora using distributional similarity. The DIRT algorithm, proposed by Lin and Pantel (2001), uses parse tree paths as contexts for computing distributional similarity. In this way, two phrases were considered similar if they occurred in similar contexts within many sentences. Although parse tree paths serve as rich representations, they are costly to construct and yield sparse representations. The approach proposed by Pasca and Dienes (2005) avoided the costs associated with p</context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>I. Androutsopoulos and P. Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. Journal of Artificial Intelligence Research, 38:135–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>597--604</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1882" citStr="Bannard and Callison-Burch, 2005" startWordPosition="276" endWordPosition="279">pice of life”. As with life, variety also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex approaches that require access to bilin</context>
<context position="6156" citStr="Bannard and Callison-Burch (2005)" startWordPosition="912" endWordPosition="915">ensitive hashing to reduce the dimensionality of the context vectors. Despite their simplicity, such techniques are susceptible to a number of issues stemming from the distributional assumption. For example, such approaches have a propensity to assign large scores to antonyms and other semantically irrelevant phrases. The second line of research uses comparable or bilingual corpora as the ‘pivot’ that binds paraphrases together (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Pang et al., 2003). Amongst the most effective recent work, Bannard and Callison-Burch (2005) show how different English translations of the same entry in a statistically-derived translation table can be viewed as paraphrases. The recent work by Zhao et al. (Zhao et al., 2009) uses a generalization of DIRTstyle patterns to generate paraphrases from a bilingual parallel corpus. The primary drawback of these type of approaches is that they require a considerable amount of resource engineering that may not be available for all languages, domains, or applications. 3 Experimental Evaluation The goal of our experimental evaluation is to analyze the effectiveness of a variety of paraphrase g</context>
<context position="8138" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1220" endWordPosition="1223">enes (2005) that uses variable-length n-gram contexts and overlap-based scoring. The context of a phrase is defined as the concatenation of the n-grams immediately to the left and right of the phrase. We set the minimum length of an n-gram context to be 2 and the maximum length to be 3. The maximum length of a phrase is set to 5. BR: The distributional similarity approach proposed by Bhagat and Ravichandran (2008) that uses noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the contextual vectors. 547 BCB-S: An extension of the Bannard CallisonBurch (Bannard and Callison-Burch, 2005) approach that constrains the paraphrases to have the same syntactic type as the original phrase (CallisonBurch, 2008). We constrained all paraphrases to be verb phrases. We chose these three particular systems because they span the spectrum of paraphrase approaches, in that the PD approach is simple and does not rely on any NLP resources while the BCB-S approach is sophisticated and makes heavy use of NLP resources. For the two distributional similarity approaches (PD and BR), paraphrases were harvested from the English Gigaword Fourth Edition corpus and scored using the cosine similarity bet</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 597–604, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ken Barker</author>
<author>Bhalchandra Agashe</author>
<author>Shaw-Yi Chaw</author>
<author>James Fan</author>
<author>Noah Friedland</author>
<author>Michael Glass</author>
<author>Jerry Hobbs</author>
<author>Eduard Hovy</author>
<author>David Israel</author>
<author>Doo Soon Kim</author>
<author>Rutu Mulkar-Mehta</author>
<author>Sourabh Patwardhan</author>
<author>Bruce Porter</author>
<author>Dan Tecuci</author>
<author>Peter Yeh</author>
</authors>
<title>Learning by reading: a prototype system, performance baseline and lessons learned.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd national conference on Artificial intelligence -</booktitle>
<volume>1</volume>
<pages>280--286</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="7113" citStr="Barker et al., 2007" startWordPosition="1061" endWordPosition="1064">s that they require a considerable amount of resource engineering that may not be available for all languages, domains, or applications. 3 Experimental Evaluation The goal of our experimental evaluation is to analyze the effectiveness of a variety of paraphrase generation techniques, ranging from simple to sophisticated. Our evaluation focuses on generating paraphrases for verb phrases, which tend to exhibit more variation than other types of phrases. Furthermore, our interest in paraphrase generation was initially inspired by challenges encountered during research related to machine reading (Barker et al., 2007). Information extraction systems, which are key component of machine reading systems, can use paraphrase technology to automatically expand seed sets of relation triggers, which are commonly verb phrases. 3.1 Systems Our evaluation compares the effectiveness of the following paraphrase harvesting approaches: PD: The basic distributional similarity-inspired approach proposed by Pasca and Dienes (2005) that uses variable-length n-gram contexts and overlap-based scoring. The context of a phrase is defined as the concatenation of the n-grams immediately to the left and right of the phrase. We set </context>
</contexts>
<marker>Barker, Agashe, Chaw, Fan, Friedland, Glass, Hobbs, Hovy, Israel, Kim, Mulkar-Mehta, Patwardhan, Porter, Tecuci, Yeh, 2007</marker>
<rawString>Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw, James Fan, Noah Friedland, Michael Glass, Jerry Hobbs, Eduard Hovy, David Israel, Doo Soon Kim, Rutu Mulkar-Mehta, Sourabh Patwardhan, Bruce Porter, Dan Tecuci, and Peter Yeh. 2007. Learning by reading: a prototype system, performance baseline and lessons learned. In Proceedings of the 22nd national conference on Artificial intelligence - Volume 1, pages 280–286. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1848" citStr="Barzilay and Lee, 2003" startWordPosition="272" endWordPosition="275">s that “variety is the spice of life”. As with life, variety also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex appro</context>
<context position="6006" citStr="Barzilay and Lee, 2003" startWordPosition="891" endWordPosition="894">ection of news articles. Bhagat and Ravichandran (2008) proposed a similar approach that used noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the context vectors. Despite their simplicity, such techniques are susceptible to a number of issues stemming from the distributional assumption. For example, such approaches have a propensity to assign large scores to antonyms and other semantically irrelevant phrases. The second line of research uses comparable or bilingual corpora as the ‘pivot’ that binds paraphrases together (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Pang et al., 2003). Amongst the most effective recent work, Bannard and Callison-Burch (2005) show how different English translations of the same entry in a statistically-derived translation table can be viewed as paraphrases. The recent work by Zhao et al. (Zhao et al., 2009) uses a generalization of DIRTstyle patterns to generate paraphrases from a bilingual parallel corpus. The primary drawback of these type of approaches is that they require a considerable amount of resource engineering that may not be available for all languages, do</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiplesequence alignment. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 16– 23, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01,</booktitle>
<pages>50--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1793" citStr="Barzilay and McKeown, 2001" startWordPosition="262" endWordPosition="266">e, and low redundancy. 1 Introduction A popular idiom states that “variety is the spice of life”. As with life, variety also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. </context>
<context position="5982" citStr="Barzilay and McKeown, 2001" startWordPosition="887" endWordPosition="890">rases from a very large collection of news articles. Bhagat and Ravichandran (2008) proposed a similar approach that used noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the context vectors. Despite their simplicity, such techniques are susceptible to a number of issues stemming from the distributional assumption. For example, such approaches have a propensity to assign large scores to antonyms and other semantically irrelevant phrases. The second line of research uses comparable or bilingual corpora as the ‘pivot’ that binds paraphrases together (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Pang et al., 2003). Amongst the most effective recent work, Bannard and Callison-Burch (2005) show how different English translations of the same entry in a statistically-derived translation table can be viewed as paraphrases. The recent work by Zhao et al. (Zhao et al., 2009) uses a generalization of DIRTstyle patterns to generate paraphrases from a bilingual parallel corpus. The primary drawback of these type of approaches is that they require a considerable amount of resource engineering that may not be availab</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01, pages 50–57, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Large scale acquisition of paraphrases for learning surface patterns.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>674--682</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1824" citStr="Bhagat and Ravichandran, 2008" startWordPosition="267" endWordPosition="271">roduction A popular idiom states that “variety is the spice of life”. As with life, variety also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectru</context>
<context position="5439" citStr="Bhagat and Ravichandran (2008)" startWordPosition="805" endWordPosition="808">RT algorithm, proposed by Lin and Pantel (2001), uses parse tree paths as contexts for computing distributional similarity. In this way, two phrases were considered similar if they occurred in similar contexts within many sentences. Although parse tree paths serve as rich representations, they are costly to construct and yield sparse representations. The approach proposed by Pasca and Dienes (2005) avoided the costs associated with parsing by using n-gram contexts. Given the simplicity of the approach, the authors were able to harvest paraphrases from a very large collection of news articles. Bhagat and Ravichandran (2008) proposed a similar approach that used noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the context vectors. Despite their simplicity, such techniques are susceptible to a number of issues stemming from the distributional assumption. For example, such approaches have a propensity to assign large scores to antonyms and other semantically irrelevant phrases. The second line of research uses comparable or bilingual corpora as the ‘pivot’ that binds paraphrases together (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Bannard and CallisonBurch, 2005</context>
<context position="7922" citStr="Bhagat and Ravichandran (2008)" startWordPosition="1187" endWordPosition="1190">re commonly verb phrases. 3.1 Systems Our evaluation compares the effectiveness of the following paraphrase harvesting approaches: PD: The basic distributional similarity-inspired approach proposed by Pasca and Dienes (2005) that uses variable-length n-gram contexts and overlap-based scoring. The context of a phrase is defined as the concatenation of the n-grams immediately to the left and right of the phrase. We set the minimum length of an n-gram context to be 2 and the maximum length to be 3. The maximum length of a phrase is set to 5. BR: The distributional similarity approach proposed by Bhagat and Ravichandran (2008) that uses noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the contextual vectors. 547 BCB-S: An extension of the Bannard CallisonBurch (Bannard and Callison-Burch, 2005) approach that constrains the paraphrases to have the same syntactic type as the original phrase (CallisonBurch, 2008). We constrained all paraphrases to be verb phrases. We chose these three particular systems because they span the spectrum of paraphrase approaches, in that the PD approach is simple and does not rely on any NLP resources while the BCB-S approach is sophisticated a</context>
</contexts>
<marker>Bhagat, Ravichandran, 2008</marker>
<rawString>Rahul Bhagat and Deepak Ravichandran. 2008. Large scale acquisition of paraphrases for learning surface patterns. In Proceedings of ACL-08: HLT, pages 674– 682, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>196--205</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1904" citStr="Callison-Burch, 2008" startWordPosition="280" endWordPosition="281">ty also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex approaches that require access to bilingual parallel corpora </context>
<context position="6061" citStr="Callison-Burch, 2008" startWordPosition="900" endWordPosition="901">roposed a similar approach that used noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the context vectors. Despite their simplicity, such techniques are susceptible to a number of issues stemming from the distributional assumption. For example, such approaches have a propensity to assign large scores to antonyms and other semantically irrelevant phrases. The second line of research uses comparable or bilingual corpora as the ‘pivot’ that binds paraphrases together (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Pang et al., 2003). Amongst the most effective recent work, Bannard and Callison-Burch (2005) show how different English translations of the same entry in a statistically-derived translation table can be viewed as paraphrases. The recent work by Zhao et al. (Zhao et al., 2009) uses a generalization of DIRTstyle patterns to generate paraphrases from a bilingual parallel corpus. The primary drawback of these type of approaches is that they require a considerable amount of resource engineering that may not be available for all languages, domains, or applications. 3 Experimental Evaluation The g</context>
<context position="9625" citStr="Callison-Burch, 2008" startWordPosition="1457" endWordPosition="1458">s about American football. Individual occurrences of verb phrases were sampled, which means that more common verb phrases were more likely to be selected and that a given phrase could be selected multiple times. This sampling strategy was used to evaluate the systems across a realistic sample of phrases. To obtain a richer class of phrases beyond basic verb groups, we defined verb phrases to be contiguous sequences of tokens that matched the following POS tag pattern: (TO I IN I RB I MD I VB)+. Following the methodology used in previous paraphrase evaluations (Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Kok and Brockett, 2010), we presented annotators with two sentences. The first sentence was randomly selected from amongst all of the sentences in the evaluation corpus that contain the original phrase. The second sentence was the same as the first, except the original phrase is replaced with the system generated paraphrase. Annotators were given the following options, which were adopted from those described by Kok and Brockett (2010), for each sentence pair: 0) Different meaning; 1) Same meaning; revised is 1Available at http://www.cs.jhu.edu/˜ccb/. grammatically incorrect; and 2) Same mean</context>
<context position="14941" citStr="Callison-Burch, 2008" startWordPosition="2345" endWordPosition="2346">eir score, thus placing the highest scoring item at rank 1. Bolded values represent the best result for a given metric. As expected, the results show that the systems perform significantly worse under the strict evaluation criteria, which requires the paraphrased sentences to be grammatically correct. None of the approaches tested used any information from the evaluation sentences (other than the fact a verb phrase was to be filled in). Recent work showed that using language models and/or syntactic clues from the evaluation sentence can improve the grammaticality of the paraphrased sentences (Callison-Burch, 2008). Such approaches could likely be used to improve the quality of all of the approaches under the strict evaluation criteria. In terms of coverage, the distributional similarity approaches performed the best. In another set of experiments, we used the PD method to harvest paraphrases from a large Web corpus, and found that the coverage was 98%. Achieving similar coverage with resource-dependent approaches would likely require more human and machine effort. 3.4 Redundancy After manually inspecting the results returned by the various paraphrase systems, we noticed that some approaches returned hi</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 196–205, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring Nominal Scale Agreement Among Many Raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="11487" citStr="Fleiss, 1971" startWordPosition="1757" endWordPosition="1758">t answer was known by us. We automatically rejected any HITs where the worker failed either of these hidden tests. We also rejected all work from annotators who failed at least 25% of their hidden tests. We collected a total of 51,680 annotations. We rejected 65% of the annotations based on the hidden test filtering just described, leaving 18,150 annotations for our evaluation. Each sentence pair received a minimum of 1, a median of 3, and maximum of 6 annotations. The raw agreement of the annotators (after filtering) was 77% and the Fleiss’ Kappa was 0.43, which signifies moderate agreement (Fleiss, 1971; Landis and Koch, 1977). The systems were evaluated in terms of coverage and expected precision at k. Coverage is defined as the percentage of phrases for which the system returned at least one paraphrase. Expected precision at k is the expected number of correct paraphrases amongst the top k returned, and is computed as: k 1 E[i@k] = k where pi is the proportion of positive annotations for item i. When computing the mean expected precision over a set of input phrases, only those phrases that generate one or more paraphrases is considered in the mean. Hence, if precision were to be averaged o</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring Nominal Scale Agreement Among Many Raters. Psychological Bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Chris Brockett</author>
</authors>
<title>Hitting the right paraphrases in good time.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>145--153</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9650" citStr="Kok and Brockett, 2010" startWordPosition="1459" endWordPosition="1463">all. Individual occurrences of verb phrases were sampled, which means that more common verb phrases were more likely to be selected and that a given phrase could be selected multiple times. This sampling strategy was used to evaluate the systems across a realistic sample of phrases. To obtain a richer class of phrases beyond basic verb groups, we defined verb phrases to be contiguous sequences of tokens that matched the following POS tag pattern: (TO I IN I RB I MD I VB)+. Following the methodology used in previous paraphrase evaluations (Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Kok and Brockett, 2010), we presented annotators with two sentences. The first sentence was randomly selected from amongst all of the sentences in the evaluation corpus that contain the original phrase. The second sentence was the same as the first, except the original phrase is replaced with the system generated paraphrase. Annotators were given the following options, which were adopted from those described by Kok and Brockett (2010), for each sentence pair: 0) Different meaning; 1) Same meaning; revised is 1Available at http://www.cs.jhu.edu/˜ccb/. grammatically incorrect; and 2) Same meaning; revised is grammatic</context>
</contexts>
<marker>Kok, Brockett, 2010</marker>
<rawString>Stanley Kok and Chris Brockett. 2010. Hitting the right paraphrases in good time. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 145–153, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="11511" citStr="Landis and Koch, 1977" startWordPosition="1759" endWordPosition="1762">nown by us. We automatically rejected any HITs where the worker failed either of these hidden tests. We also rejected all work from annotators who failed at least 25% of their hidden tests. We collected a total of 51,680 annotations. We rejected 65% of the annotations based on the hidden test filtering just described, leaving 18,150 annotations for our evaluation. Each sentence pair received a minimum of 1, a median of 3, and maximum of 6 annotations. The raw agreement of the annotators (after filtering) was 77% and the Fleiss’ Kappa was 0.43, which signifies moderate agreement (Fleiss, 1971; Landis and Koch, 1977). The systems were evaluated in terms of coverage and expected precision at k. Coverage is defined as the percentage of phrases for which the system returned at least one paraphrase. Expected precision at k is the expected number of correct paraphrases amongst the top k returned, and is computed as: k 1 E[i@k] = k where pi is the proportion of positive annotations for item i. When computing the mean expected precision over a set of input phrases, only those phrases that generate one or more paraphrases is considered in the mean. Hence, if precision were to be averaged over all 100 phrases, the</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159–174, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question-answering.</title>
<date>2001</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>7</volume>
<contexts>
<context position="1926" citStr="Lin and Pantel, 2001" startWordPosition="282" endWordPosition="285"> appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex approaches that require access to bilingual parallel corpora and may also rely on p</context>
<context position="4856" citStr="Lin and Pantel (2001)" startWordPosition="716" endWordPosition="719">hat future approaches and evaluations may want to consider more carefully. 2 Related Work Instead of exhaustively covering the entire spectrum of previously proposed paraphrasing techniques, our evaluation focuses on two families of data-driven approaches that are widely studied and used. More comprehensive surveys of data-driven paraphrasing techniques can be found in Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). The first family of approaches that we consider harvests paraphrases from monolingual corpora using distributional similarity. The DIRT algorithm, proposed by Lin and Pantel (2001), uses parse tree paths as contexts for computing distributional similarity. In this way, two phrases were considered similar if they occurred in similar contexts within many sentences. Although parse tree paths serve as rich representations, they are costly to construct and yield sparse representations. The approach proposed by Pasca and Dienes (2005) avoided the costs associated with parsing by using n-gram contexts. Given the simplicity of the approach, the authors were able to harvest paraphrases from a very large collection of news articles. Bhagat and Ravichandran (2008) proposed a simil</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question-answering. Nat. Lang. Eng., 7:343–360, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of datadriven methods.</title>
<date>2010</date>
<journal>Comput. Linguist.,</journal>
<pages>36--341</pages>
<contexts>
<context position="4674" citStr="Madnani and Dorr (2010)" startWordPosition="690" endWordPosition="693">standing of the strengths and weaknesses of each type of approach. The evaluation also brings to light additional properties, including the number of redundant paraphrases generated, that future approaches and evaluations may want to consider more carefully. 2 Related Work Instead of exhaustively covering the entire spectrum of previously proposed paraphrasing techniques, our evaluation focuses on two families of data-driven approaches that are widely studied and used. More comprehensive surveys of data-driven paraphrasing techniques can be found in Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). The first family of approaches that we consider harvests paraphrases from monolingual corpora using distributional similarity. The DIRT algorithm, proposed by Lin and Pantel (2001), uses parse tree paths as contexts for computing distributional similarity. In this way, two phrases were considered similar if they occurred in similar contexts within many sentences. Although parse tree paths serve as rich representations, they are costly to construct and yield sparse representations. The approach proposed by Pasca and Dienes (2005) avoided the costs associated with parsing by using n-gram conte</context>
<context position="19682" citStr="Madnani and Dorr, 2010" startWordPosition="3079" endWordPosition="3082">tasks. 4 Conclusions and Future Work This paper examined the tradeoffs between simple paraphrasing approaches that do not make use of any NLP resources and more sophisticated approaches that use a variety of such resources. Our evaluation demonstrated that simple harvesting approaches fare well against more sophisticated approaches, achieving state-of-the-art precision, good coverage, and relatively low redundancy. In the future, we would like to see more empirical evaluations and detailed studies comparing the practical merits of various paraphrase generation techniques. As Madnani and Dorr (Madnani and Dorr, 2010) suggested, it would be beneficial to the research community to develop a standard, shared evaluation that would act to catalyze further advances and encourage more meaningful comparative evaluations of such approaches moving forward. Acknowledgments The authors gratefully acknowledge the support of the DARPA Machine Reading Program under AFRL prime contract no. FA8750-09-C-3705. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government. We would also like to tha</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of datadriven methods. Comput. Linguist., 36:341–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -Volume 1, NAACL ’03,</booktitle>
<pages>102--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1945" citStr="Pang et al., 2003" startWordPosition="286" endWordPosition="289">araphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex approaches that require access to bilingual parallel corpora and may also rely on part-of-speech (POS)</context>
<context position="6081" citStr="Pang et al., 2003" startWordPosition="902" endWordPosition="905">oach that used noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the context vectors. Despite their simplicity, such techniques are susceptible to a number of issues stemming from the distributional assumption. For example, such approaches have a propensity to assign large scores to antonyms and other semantically irrelevant phrases. The second line of research uses comparable or bilingual corpora as the ‘pivot’ that binds paraphrases together (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Pang et al., 2003). Amongst the most effective recent work, Bannard and Callison-Burch (2005) show how different English translations of the same entry in a statistically-derived translation table can be viewed as paraphrases. The recent work by Zhao et al. (Zhao et al., 2009) uses a generalization of DIRTstyle patterns to generate paraphrases from a bilingual parallel corpus. The primary drawback of these type of approaches is that they require a considerable amount of resource engineering that may not be available for all languages, domains, or applications. 3 Experimental Evaluation The goal of our experimen</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -Volume 1, NAACL ’03, pages 102–109, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>Pter Dienes</author>
</authors>
<title>Aligning needles in a haystack: Paraphrase acquisition across the web. In</title>
<date>2005</date>
<booktitle>Natural Language Processing IJCNLP 2005,</booktitle>
<volume>3651</volume>
<pages>119--130</pages>
<editor>Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="1970" citStr="Pasca and Dienes, 2005" startWordPosition="290" endWordPosition="294">possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex approaches that require access to bilingual parallel corpora and may also rely on part-of-speech (POS) taggers, chunkers, parse</context>
<context position="5210" citStr="Pasca and Dienes (2005)" startWordPosition="769" endWordPosition="772">ues can be found in Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). The first family of approaches that we consider harvests paraphrases from monolingual corpora using distributional similarity. The DIRT algorithm, proposed by Lin and Pantel (2001), uses parse tree paths as contexts for computing distributional similarity. In this way, two phrases were considered similar if they occurred in similar contexts within many sentences. Although parse tree paths serve as rich representations, they are costly to construct and yield sparse representations. The approach proposed by Pasca and Dienes (2005) avoided the costs associated with parsing by using n-gram contexts. Given the simplicity of the approach, the authors were able to harvest paraphrases from a very large collection of news articles. Bhagat and Ravichandran (2008) proposed a similar approach that used noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the context vectors. Despite their simplicity, such techniques are susceptible to a number of issues stemming from the distributional assumption. For example, such approaches have a propensity to assign large scores to antonyms and other </context>
<context position="7516" citStr="Pasca and Dienes (2005)" startWordPosition="1117" endWordPosition="1120"> to exhibit more variation than other types of phrases. Furthermore, our interest in paraphrase generation was initially inspired by challenges encountered during research related to machine reading (Barker et al., 2007). Information extraction systems, which are key component of machine reading systems, can use paraphrase technology to automatically expand seed sets of relation triggers, which are commonly verb phrases. 3.1 Systems Our evaluation compares the effectiveness of the following paraphrase harvesting approaches: PD: The basic distributional similarity-inspired approach proposed by Pasca and Dienes (2005) that uses variable-length n-gram contexts and overlap-based scoring. The context of a phrase is defined as the concatenation of the n-grams immediately to the left and right of the phrase. We set the minimum length of an n-gram context to be 2 and the maximum length to be 3. The maximum length of a phrase is set to 5. BR: The distributional similarity approach proposed by Bhagat and Ravichandran (2008) that uses noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the contextual vectors. 547 BCB-S: An extension of the Bannard CallisonBurch (Bannard and</context>
</contexts>
<marker>Pasca, Dienes, 2005</marker>
<rawString>Marius Pasca and Pter Dienes. 2005. Aligning needles in a haystack: Paraphrase acquisition across the web. In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors, Natural Language Processing IJCNLP 2005, volume 3651 of Lecture Notes in Computer Science, pages 119–130. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Extracting paraphrase patterns from bilingual parallel corpora.</title>
<date>2009</date>
<journal>Natural Language Engineering, 15(Special Issue</journal>
<pages>04--503</pages>
<contexts>
<context position="6340" citStr="Zhao et al., 2009" startWordPosition="943" endWordPosition="946">example, such approaches have a propensity to assign large scores to antonyms and other semantically irrelevant phrases. The second line of research uses comparable or bilingual corpora as the ‘pivot’ that binds paraphrases together (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Pang et al., 2003). Amongst the most effective recent work, Bannard and Callison-Burch (2005) show how different English translations of the same entry in a statistically-derived translation table can be viewed as paraphrases. The recent work by Zhao et al. (Zhao et al., 2009) uses a generalization of DIRTstyle patterns to generate paraphrases from a bilingual parallel corpus. The primary drawback of these type of approaches is that they require a considerable amount of resource engineering that may not be available for all languages, domains, or applications. 3 Experimental Evaluation The goal of our experimental evaluation is to analyze the effectiveness of a variety of paraphrase generation techniques, ranging from simple to sophisticated. Our evaluation focuses on generating paraphrases for verb phrases, which tend to exhibit more variation than other types of </context>
</contexts>
<marker>Zhao, Wang, Liu, Li, 2009</marker>
<rawString>Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2009. Extracting paraphrase patterns from bilingual parallel corpora. Natural Language Engineering, 15(Special Issue 04):503–526.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>