<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003030">
<title confidence="0.985848">
Word Sense Induction for Novel Sense Detection
</title>
<author confidence="0.915805">
Jey Han Lau,&apos;° Paul Cook,° Diana McCarthy, �
David Newman,* and Timothy Baldwin&apos;°
</author>
<affiliation confidence="0.827383333333333">
4 NICTA Victoria Research Laboratory
♡ Dept of Computer Science and Software Engineering, University of Melbourne
Q Dept of Computer Science, University of California Irvine
</affiliation>
<address confidence="0.821537">
46 Lexical Computing
</address>
<email confidence="0.9596365">
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, newman@uci.edu, tb@ldwin.net
</email>
<sectionHeader confidence="0.993829" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986225">
We apply topic modelling to automatically
induce word senses of a target word, and
demonstrate that our word sense induction
method can be used to automatically de-
tect words with emergent novel senses, as
well as token occurrences of those senses.
We start by exploring the utility of stan-
dard topic models for word sense induction
(WSI), with a pre-determined number of
topics (=senses). We next demonstrate that
a non-parametric formulation that learns an
appropriate number of senses per word ac-
tually performs better at the WSI task. We
go on to establish state-of-the-art results
over two WSI datasets, and apply the pro-
posed model to a novel sense detection task.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998642">
Word sense induction (WSI) is the task of auto-
matically inducing the different senses of a given
word, generally in the form of an unsupervised
learning task with senses represented as clusters
of token instances. It contrasts with word sense
disambiguation (WSD), where a fixed sense in-
ventory is assumed to exist, and token instances
of a given word are disambiguated relative to the
sense inventory. While WSI is intuitively appeal-
ing as a task, there have been no real examples of
WSI being successfully deployed in end-user ap-
plications, other than work by Schutze (1998) and
Navigli and Crisafulli (2010) in an information re-
trieval context. A key contribution of this paper
is the successful application of WSI to the lexico-
graphical task of novel sense detection, i.e. identi-
fying words which have taken on new senses over
time.
One of the key challenges in WSI is learning
the appropriate sense granularity for a given word,
i.e. the number of senses that best captures the
token occurrences of that word. Building on the
work of Brody and Lapata (2009) and others, we
approach WSI via topic modelling — using La-
tent Dirichlet Allocation (LDA: Blei et al. (2003))
and derivative approaches — and use the topic
model to determine the appropriate sense gran-
ularity. Topic modelling is an unsupervised ap-
proach to jointly learn topics — in the form of
multinomial probability distributions over words
— and per-document topic assignments — in the
form of multinomial probability distributions over
topics. LDA is appealing for WSI as it both as-
signs senses to words (in the form of topic alloca-
tion), and outputs a representation of each sense
as a weighted list of words. LDA offers a solu-
tion to the question of sense granularity determi-
nation via non-parametric formulations, such as
a Hierarchical Dirichlet Process (HDP: Teh et al.
(2006), Yao and Durme (2011)).
Our contributions in this paper are as follows.
We first establish the effectiveness of HDP for
WSI over both the SemEval-2007 and SemEval-
2010 WSI datasets (Agirre and Soroa, 2007; Man-
andhar et al., 2010), and show that the non-
parametric formulation is superior to a standard
LDA formulation with oracle determination of
sense granularity for a given word. We next
demonstrate that our interpretation of HDP-based
WSI is superior to other topic model-based ap-
proaches to WSI, and indeed, better than the best-
published results for both SemEval datasets. Fi-
nally, we apply our method to the novel sense de-
tection task based on a dataset developed in this
research, and achieve highly encouraging results.
</bodyText>
<sectionHeader confidence="0.998111" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.9927085">
In topic modelling, documents are assumed to ex-
hibit multiple topics, with each document having
</bodyText>
<page confidence="0.97254">
591
</page>
<note confidence="0.976528">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 591–601,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999795">
its own distribution over topics. Words are gen-
erated in each document by first sampling a topic
from the document’s topic distribution, then sam-
pling a word from that topic. In this work we
use the topic models’s probabilistic assignment of
topics to words for the WSI task.
</bodyText>
<subsectionHeader confidence="0.997771">
2.1 Data Representation and Pre-processing
</subsectionHeader>
<bodyText confidence="0.999965793103448">
In the context of WSI, topics form our sense rep-
resentation, and words in a sentence are gener-
ated conditioned on a particular sense of the target
word. The “document” in the WSI case is a sin-
gle sentence or a short document fragment con-
taining the target word, as we would not expect
to be able to generate a full document from the
sense of a single target word.1 In the case of the
SemEval datasets, we use the word contexts pro-
vided in the dataset, while in our novel sense de-
tection experiments, we use a context window of
three sentences, one sentence to either side of the
token occurrence of the target word.
As our baseline representation, we use a bag of
words, where word frequency is kept but not word
order. All words are lemmatised, and stopwords
and low frequency terms are removed.
We also experiment with the addition of po-
sitional context word information, as commonly
used in WSI. That is, we introduce an additional
word feature for each of the three words to the left
and right of the target word.
Pad´o and Lapata (2007) demonstrated the im-
portance of syntactic dependency relations in the
construction of semantic space models, e.g. for
WSD. Based on these findings, we include depen-
dency relations as additional features in our topic
models,2 but just for dependency relations that in-
volve the target word.
</bodyText>
<subsectionHeader confidence="0.991245">
2.2 Topic Modelling
</subsectionHeader>
<bodyText confidence="0.999983285714286">
Topic models learn a probability distribution over
topics for each document, by simply aggregating
the distributions over topics for each word in the
document. In WSI terms, we take this distribu-
tion over topics for each target word (“instance”
in WSI parlance) as our distribution over senses
for that word.
</bodyText>
<footnote confidence="0.9916912">
1Notwithstanding the one sense per discourse heuristic
(Gale et al., 1992).
2We use the Stanford Parser to do part of speech tagging
and to extract the dependency relations (Klein and Manning,
2003; De Marneffe et al., 2006).
</footnote>
<bodyText confidence="0.998726777777778">
In our initial experiments, we use LDA topic
modelling, which requires us to set T, the num-
ber of topics to be learned by the model. The
LDA generative process is: (1) draw a latent
topic z from a document-specific topic distribu-
tion P(t = z|d) then; (2) draw a word w from
the chosen topic P(w|t = z). Thus, the probabil-
ity of producing a single copy of word w given a
document d is given by:
</bodyText>
<equation confidence="0.9985285">
P(w|d) = ∑T P(w|t = z)P(t = z|d).
z=1
</equation>
<bodyText confidence="0.999749181818182">
In standard LDA, the user needs to specify the
number of topics T. In non-parametric variants of
LDA, the model dynamically learns the number of
topics as part of the topic modelling. The particu-
lar implementation of non-parametric topic model
we experiment with is Hierarchical Dirichlet Pro-
cess (HDP: Teh et al. (2006)),3 where, for each
document, a distribution of mixture components
P(t|d) is sampled from a base distribution G0
as follows: (1) choose a base distribution G0 ∼
DP(-y, H); (2) for each document d, generate dis-
tribution P(t|d) ∼ DP(α0, G0); (3) draw a la-
tent topic z from the document’s mixture compo-
nent distribution P(t|d), in the same manner as
for LDA; and (4) draw a word w from the chosen
topic P(w|t = z).4
For both LDA and HDP, we individually topic
model each target word, and determine the sense
assignment z for a given instance by aggregating
over the topic assignments for each word in the
instance and selecting the sense with the highest
aggregated probability, arg maxz P(t = z|d).
</bodyText>
<sectionHeader confidence="0.983291" genericHeader="method">
3 SemEval Experiments
</sectionHeader>
<bodyText confidence="0.99946425">
To facilitate comparison of our proposed method
for WSI with previous approaches, we use the
dataset from the SemEval-2007 and SemEval-
2010 word sense induction tasks (Agirre and
</bodyText>
<footnote confidence="0.929093">
3We use the C++ implementation of HDP
(http://www.cs.princeton.edu/˜blei/
topicmodeling.html) in our experiments.
4The two HDP parameters γ and α0 control the variabil-
ity of senses in the documents. In particular, γ controls the
degree of sharing of topics across documents — a high γ
value leads to more topics, as topics for different documents
are more dissimilar. α0, on the other hand, controls the de-
gree of mixing of topics within a document — a high α0 gen-
erates fewer topics, as topics are less homogeneous within a
document.
</footnote>
<page confidence="0.996964">
592
</page>
<bodyText confidence="0.99841675">
Soroa, 2007; Manandhar et al., 2010). We first
experiment with the SemEval-2010 dataset, as it
includes explicit training and test data for each
target word and utilises a more robust evaluation
methodology. We then return to experiment with
the SemEval-2007 dataset, for comparison pur-
poses with other published results for topic mod-
elling approaches to WSI.
</bodyText>
<subsectionHeader confidence="0.950344">
3.1 Sen Eval-2010
3.1.1 Dataset and Methodology
</subsectionHeader>
<bodyText confidence="0.999986333333333">
Our primary WSI evaluation is based on
the dataset provided by the SemEval-2010 WSI
shared task (Manandhar et al., 2010). The dataset
contains 100 target words: 50 nouns and 50 verbs.
For each target word, a fixed set of training and
test instances are supplied, typically 1 to 3 sen-
tences in length, each containing the target word.
The default approach to evaluation for the
SemEval-2010 WSI task is in the form of WSD
over the test data, based on the senses that have
been automatically induced from the training
data. Because the induced senses will likely vary
in number and nature between systems, the WSD
evaluation has to incorporate a sense alignment
step, which it performs by splitting the test in-
stances into two sets: a mapping set and an eval-
uation set. The optimal mapping from induced
senses to gold-standard senses is learned from the
mapping set, and the resulting sense alignment is
used to map the predictions of the WSI system to
pre-defined senses for the evaluation set. The par-
ticular split we use to calculate WSD effective-
ness in this paper is 80%/20% (mapping/test), av-
eraged across 5 random splits.5
The SemEval-2010 training data consists of ap-
proximately 163K training instances for the 100
target words, all taken from the web. The test
data is approximately 9K instances taken from a
variety of news sources. Following the standard
approach used by the participating systems in the
SemEval-2010 task, we induce senses only from
the training instances, and use the learned model
to assign senses to the test instances.
</bodyText>
<footnote confidence="0.944964875">
5A 60%/40% split is also provided as part of the task
setup, but the results are almost identical to those for the
80%/20% split, and so are omitted from this paper. The orig-
inal task also made use of V-measure and Paired F-score to
evaluate the induced word sense clusters, but have degen-
erate behaviour in correlating strongly with the number of
senses induced by the method (Manandhar et al., 2010), and
are hence omitted from this paper.
</footnote>
<bodyText confidence="0.998968">
In our original experiments with LDA, we set
the number of topics (T) for each target word to
the number of senses represented in the test data
for that word (varying T for each target word).
This is based on the unreasonable assumption that
we will have access to gold-standard information
on sense granularity for each target word, and is
done to establish an upper bound score for LDA.
We then relax the assumption, and use a fixed T
setting for each of sets of nouns (T = 7) and
verbs (T = 3), based on the average number of
senses from the test data in each case. Finally,
we introduce positional context features for LDA,
once again using the fixed T values for nouns and
verbs.
We next apply HDP to the WSI task, using
positional features, but learning the number of
senses automatically for each target word via the
model. Finally, we experiment with adding de-
pendency features to the model.
To summarise, we provide results for the fol-
lowing models:
</bodyText>
<listItem confidence="0.925501307692308">
1. LDA+Variable T: LDA with variable T
for each target word based on the number of
gold-standard senses.
2. LDA+Fixed T: LDA with fixed T for each
of nouns and verbs.
3. LDA+Fixed T+Position: LDA with fixed
T and extra positional word features.
4. HDP+Position: HDP (which automatically
learns T), with extra positional word fea-
tures.
5. HDP+Position+Dependency: HDP with
both positional word and dependency fea-
tures.
</listItem>
<bodyText confidence="0.9998974">
We compare our models with two baselines
from the SemEval-2010 task: (1) Baseline Ran-
dom — randomly assign each test instance to one
of four senses; (2) Baseline MFS — most fre-
quent sense baseline, assigning all test instances
to one sense; and also a benchmark system
(UoY), in the form of the University of York sys-
tem (Korkontzelos and Manandhar, 2010), which
achieved the best overall WSD results in the orig-
inal SemEval-2010 task.
</bodyText>
<subsectionHeader confidence="0.999756">
3.2 Sen Eval-2010 Results
</subsectionHeader>
<bodyText confidence="0.9993305">
The results of our experiments over the SemEval-
2010 dataset are summarised in Table 1.
</bodyText>
<page confidence="0.996742">
593
</page>
<table confidence="0.999643571428571">
System WSD (80%/20%)
All Verbs Nouns
Baselines
Baseline Random 0.57 0.66 0.51
Baseline MFS 0.59 0.67 0.53
LDA
Variable T 0.64 0.69 0.60
Fixed T 0.63 0.68 0.59
Fixed T +Position 0.63 0.68 0.60
HDP
+Position 0.68 0.72 0.65
+Position+Dependency 0.68 0.72 0.65
Benchmark 0.62 0.67 0.59
UoY
</table>
<tableCaption confidence="0.999873">
Table 1: WSD F-score over the SemEval-2010 dataset
</tableCaption>
<bodyText confidence="0.99187250617284">
Looking first at the results for LDA, we see
that the first LDA approach (variable T) is very
competitive, outperforming the benchmark sys-
tem. In this approach, however, we assume per-
fect knowledge of the number of gold senses of
each target word, meaning that the method isn’t
truly unsupervised. When we fixed T for each
of the nouns and verbs, we see a small drop in
F-score, but encouragingly the method still per-
forms above the benchmark. Adding positional
word features improves the results very slightly
for nouns.
When we relax the assumption on the number
of word senses in moving to HDP, we observe a
marked improvement in F-score over LDA. This
is highly encouraging and somewhat surprising,
as in hiding information about sense granularity
from the model, we have actually improved our
results. We return to discuss this effect below.
For the final feature, we add dependency features
to the HDP model (in addition to retaining the
positional word features), but see no movement
in the results.6 While the dependency features
didn’t reduce F-score, their utility is questionable
as the generation of the features from the Stanford
parser is computationally expensive.
To better understand these results, we present
the top-10 terms for each of the senses induced for
the word cheat in Table 2. These senses are learnt
using HDP with both positional word features
(e.g. husband #-1, indicating the lemma husband
to the immediate left of the target word) and de-
pendency features (e.g. cheat#prep on#wife). The
first observation to make is that senses 7, 8 and
9 are “junk” senses, in that the top-10 terms do
6An identical result was observed for LDA.
not convey a coherent sense. These topics are an
artifact of HDP: they are learnt at a much later
stage of the iterative process of Gibbs sampling
and are often smaller than other topics (i.e. have
more zero-probability terms). We notice that they
are assigned as topics to instances very rarely (al-
though they are certainly used to assign topics to
non-target words in the instances), and as such,
they do not present a real issue when assigning
the sense to an instance, as they are likely to be
overshadowed by the dominant senses.7 This con-
clusion is born out when we experimented with
manually filtering out these topics when assign-
ing instance to senses: there was no perceptible
change in the results, reinforcing our suggestion
that these topics do not impact on target word
sense assignment.
Comparing the results for HDP back to those
for LDA, HDP tends to learn almost double the
number of senses per target word as are in the
gold-standard (and hence are used for the “Vari-
able T” version of LDA). Far from hurting our
WSD F-score, however, the extra topics are dom-
inated by junk topics, and boost WSD F-score for
the “genuine” topics. Based on this insight, we
ran LDA once again with variable T (and posi-
tional and dependency features), but this time set-
ting T to the value learned by HDP, to give LDA
the facility to use junk topics. This resulted in an
F-score of 0.66 across all word classes (verbs =
0.71, nouns = 0.62), demonstrating that, surpris-
ingly, even for the same T setting, HDP achieves
superior results to LDA. I.e., not only does HDP
learn T automatically, but the topic model learned
for a given T is superior to that for LDA.
Looking at the other senses discovered for
cheat, we notice that the model has induced a
myriad of senses: the relationship sense of cheat
(senses 1, 3 and 4, e.g. husband cheats); the exam
usage of cheat (sense 2); the competition/game
usage of cheat (sense 5); and cheating in the po-
litical domain (sense 6). Although the senses are
possibly “split” a little more than desirable (e.g.
senses 1, 3 and 4 arguably describe the same
sense), the overall quality of the produced senses
</bodyText>
<footnote confidence="0.964196285714286">
7In the WSD evaluation, the alignment of induced senses
to the gold senses is learnt automatically based on the map-
ping instances. E.g. if all instances that are assigned sense
a have gold sense x, then sense a is mapped to gold sense
x. Therefore, if the proportion of junk senses in the map-
ping instances is low, their influence on WSD results will be
negligible.
</footnote>
<page confidence="0.993187">
594
</page>
<figure confidence="0.9401344">
Sense Num Top-10 Terms
1 cheat think want ... love feel tell guy cheat#nsubj#include find
2 cheat student cheating test game school cheat#aux#to teacher exam study
3 husband wife cheat wife #1 tiger husband #-1 cheat#prep on#wife ... woman cheat#nsubj#husband
4 cheat woman relationship cheating partner reason cheat#nsubj#man woman #-1 cheat#aux#to spouse
5 cheat game play player cheating poker cheat#aux#to card cheated money
6 cheat exchange china chinese foreign cheat #-2 cheat #2 china #-1 cheat#aux#to team
7 tina bette kirk walk accuse mon pok symkyn nick star
8 fat jones ashley pen body taste weight expectation parent able
9 euro goal luck fair france irish single 2000 cheat#prep at#point complain
</figure>
<tableCaption confidence="0.988214">
Table 2: The top-10 terms for each of the senses induced for the verb cheat by the HDP model (with positional
word and dependency features)
</tableCaption>
<bodyText confidence="0.997764">
is encouraging. Also, we observe a spin-off ben-
efit of topic modelling approaches to WSI: the
high-ranking words in each topic can be used to
gist the sense, and anecdotally confirm the impact
of the different feature types (i.e. the positional
word and dependency features).
</bodyText>
<subsectionHeader confidence="0.965871">
3.3 Comparison with other Topic Modelling
Approaches to WSI
</subsectionHeader>
<bodyText confidence="0.999496379310345">
The idea of applying topic modelling to WSI is
not entirely new. Brody and Lapata (2009) pro-
posed an LDA-based model which assigns differ-
ent weights to different feature sets (e.g. unigram
tokens vs. dependency relations), using a “lay-
ered” feature representation. They carry out ex-
tensive parameter optimisation of both the (fixed)
number of senses, number of layers, and size of
the context window.
Separately, Yao and Durme (2011) proposed
the use of non-parametric topic models in WSI.
The authors preprocess the instances slightly dif-
ferently, opting to remove the target word from
each instance and stem the tokens. They also
tuned the hyperparameters of the topic model to
optimise the WSI effectiveness over the evalua-
tion set, and didn’t use positional or dependency
features.
Both of these papers were evaluated over
only the SemEval-2007 WSI dataset (Agirre and
Soroa, 2007), so we similarly apply our HDP
method to this dataset for direct comparability. In
the remainder of this section, we refer to Brody
and Lapata (2009) as BL, and Yao and Durme
(2011) as YVD.
The SemEval-2007 dataset consists of roughly
27K instances, for 65 target verbs and 35 target
nouns. BL report on results only over the noun
instances, so we similarly restrict our attention to
</bodyText>
<table confidence="0.998415333333333">
System F-Score
BL 0.855
YVD 0.857
SemEval Best (I2R) 0.868
Our method (default parameters) 0.842
Our method (tuned parameters) 0.869
</table>
<tableCaption confidence="0.9549745">
Table 3: F-score for the SemEval-2007 WSI task, for
our HDP method with default and tuned parameter set-
tings, as compared to competitor topic modelling and
other approaches to WSI
</tableCaption>
<bodyText confidence="0.9999265">
the nouns in this paper. Training data was not pro-
vided as part of the original dataset, so we fol-
low the approach of BL and YVD in construct-
ing our own training dataset for each target word
from instances extracted from the British National
Corpus (BNC: Burnard (2000)).8 Both BL and
YVD separately report slightly higher in-domain
results from training on WSJ data (the SemEval-
2007 data was taken from the WSJ). For the pur-
poses of model comparison under identical train-
ing settings, however, it is appropriate to report on
results for only the BNC.
We experiment with both our original method
(with both positional word and dependency fea-
tures, and default parameter settings for HDP)
without any parameter tuning, and the same
method with the tuned parameter settings of
YVD, for direct comparability. We present the re-
sults in Table 3, including the results for the best-
performing system in the original SemEval-2007
task (I2R: Niu et al. (2007)).
The results are enlightening: with default pa-
rameter settings, our methodology is slightly be-
low the results of the other three models. Bear
</bodyText>
<footnote confidence="0.9874605">
8In creating the training dataset, each instance is made
up of the sentence the target word occurs in, as we as one
sentence to either side of that sentence, i.e. 3 sentences in
total per instance.
</footnote>
<page confidence="0.99643">
595
</page>
<bodyText confidence="0.999950214285714">
in mind, however, that the two topic modelling-
based approaches were tuned extensively to the
dataset. When we use the tuned hyperparame-
ter settings of YVD, our results rise around 2.5%
to surpass both topic modelling approaches, and
marginally outperform the I2R system from the
original task. Recall that both BL and YVD report
higher results again using in-domain training data,
so we would expect to see further gains again over
the I2R system in following this path.
Overall, these results agree with our findings
over the SemEval-2010 dataset (Section 3.2), un-
derlining the viability of topic modelling to auto-
mated word sense induction.
</bodyText>
<subsectionHeader confidence="0.547779">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999995451612903">
As part of our preprocessing, we remove all stop-
words (other than for the positional word and de-
pendency features), as described in Section 2.1.
We separately experimented with not removing
stopwords, based on the intuition that prepositions
such as to and on can be informative in determin-
ing word sense based on local context. The results
were markedly worse, however. We also tried ap-
pending part of speech information to each word
lemma, but the resulting data sparseness meant
that results dropped marginally.
When determining the sense for an instance, we
aggregate the sense assignments for each word in
the instance (not just the target word). An alter-
nate strategy is to use only the target word topic
assignment, but again, the results for this strategy
were inferior to the aggregate method.
In the SemEval-2007 experiments (Sec-
tion 3.3), we found that YVD’s hyperparameter
settings yielded better results than the default
settings. We experimented with parameter tuning
over the SemEval-2010 dataset (including YVD’s
optimal setting on the 2007 dataset), but found
that the default setting achieved the best overall
results: although the WSD F-score improved a
little for nouns, it worsened for verbs. This obser-
vation is not unexpected: as the hyperparameters
were optimised for nouns in their experiments,
the settings might not be appropriate for verbs.
This also suggests that their results may be due in
part to overfitting the SemEval-2007 data.
</bodyText>
<sectionHeader confidence="0.99506" genericHeader="method">
4 Identifying Novel Senses
</sectionHeader>
<bodyText confidence="0.9997574375">
Having established the effectiveness of our ap-
proach at WSI, we next turn to an application of
WSI, in identifying words which have taken on
novel senses over time, based on analysis of di-
achronic data. Our topic modelling approach is
particularly attractive for this task as, not only
does it jointly perform type-level WSI, and token-
level WSD based on the induced senses (in as-
signing topics to each instance), but it is possible
to gist the induced senses via the contents of the
topic (typically using the topic words with highest
marginal probability).
The meanings of words can change over time;
in particular, words can take on new senses. Con-
temporary examples of new word-senses include
the meanings of swag and tweet as used below:
</bodyText>
<listItem confidence="0.742702">
1. We all know Frankie is adorable, but does he
have swag? [swag = ‘style’]
2. The alleged victim gave a description of the
man on Twitter and tweeted that she thought
she could identify him. [tweet = ‘send a mes-
sage on Twitter’]
</listItem>
<bodyText confidence="0.999447566666667">
These senses of swag and tweet are not included
in many dictionaries or computational lexicons —
e.g., neither of these senses is listed in Wordnet
3.0 (Fellbaum, 1998) — yet appear to be in regu-
lar usage, particularly in text related to pop culture
and online media.
The manual identification of such new word-
senses is a challenge in lexicography over and
above identifying new words themselves, and
is essential to keeping dictionaries up-to-date.
Moreover, lexicons that better reflect contempo-
rary usage could benefit NLP applications that use
sense inventories.
The challenge of identifying changes in word
sense has only recently been considered in com-
putational linguistics. For example, Sagi et al.
(2009), Cook and Stevenson (2010), and Gulor-
dava and Baroni (2011) propose type-based mod-
els of semantic change. Such models do not
account for polysemy, and appear best-suited to
identifying changes in predominant sense. Bam-
man and Crane (2011) use a parallel Latin-
English corpus to induce word senses and build
a WSD system, which they then apply to study
diachronic variation in word senses. Crucially, in
this token-based approach there is a clear connec-
tion between word senses and tokens, making it
possible to identify usages of a specific sense.
Based on the findings in Section 3.2, here we
apply the HDP method for WSI to the task of
</bodyText>
<page confidence="0.997065">
596
</page>
<bodyText confidence="0.999682333333333">
identifying new word-senses. In contrast to Bam-
man and Crane (2011) our token-based approach
does not require parallel text to induce senses.
</bodyText>
<subsectionHeader confidence="0.997718">
4.1 Method
</subsectionHeader>
<bodyText confidence="0.999728055555555">
Given two corpora — a reference corpus which
we take to represent standard usage, and a second
corpus of newer texts — we identify senses that
are novel to the second corpus compared to the
reference corpus. For a given word w, we pool
all usages of w in the reference corpus and sec-
ond corpus, and run the HDP WSI method on this
super-corpus to induce the senses of w. We then
tag all usages of w in both corpora with their sin-
gle most-likely automatically-induced sense.
Intuitively, if a word w is used in some sense
s in the second corpus, and w is never used in
that sense in the reference corpus, then w has ac-
quired a new sense, namely s. We capture this
intuition into a novelty score (“Nov”) that indi-
cates whether a given word w has a new sense in
the second corpus, s, compared to the reference
corpus, r, as below:
</bodyText>
<equation confidence="0.99425575">
({ps(ti) − pr(ti) })
Nov(w) = max : ti E T
pr(ti)
(1)
</equation>
<bodyText confidence="0.999982625">
where ps(ti) and pr(ti) are the probability of
sense ti in the second corpus and reference cor-
pus, respectively, calculated using smoothed max-
imum likelihood estimates, and T is the set of
senses induced for w. Novelty is high if there is
some sense t that has much higher relative fre-
quency in s than r and that is also relatively infre-
quent in r.
</bodyText>
<subsectionHeader confidence="0.976115">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.999898863636364">
Because we are interested in the identification of
novel word-senses for applications such as lexi-
con maintenance, we focus on relatively newly-
coined word-senses. In particular, we take the
written portion of the BNC — consisting primar-
ily of British English text from the late 20th cen-
tury — as our reference corpus, and a similarly-
sized random sample of documents from the
ukWaC (Ferraresi et al., 2008) — a Web corpus
built from the .uk domain in 2007 which in-
cludes a wide range of text types — as our sec-
ond corpus. Text genres are represented to dif-
ferent extents in these corpora with, for example,
text types related to the Internet being much more
common in the ukWaC. Such differences are a
noted challenge for approaches to identifying lex-
ical semantic differences between corpora (Peirs-
man et al., 2010), but are difficult to avoid given
the corpora that are available. We use TreeTagger
(Schmid, 1994) to tokenise and lemmatise both
corpora.
Evaluating approaches to identifying seman-
tic change is a challenge, particularly due to the
lack of appropriate evaluation resources; indeed,
most previous approaches have used very small
datasets (Sagi et al., 2009; Cook and Stevenson,
2010; Bamman and Crane, 2011). Because this
is a preliminary attempt at applying WSI tech-
niques to identifying new word-senses, our evalu-
ation will also be based on a rather small dataset.
We require a set of words that are known to
have acquired a new sense between the late 20th
and early 21st centuries. The Concise Oxford
English Dictionary aims to document contempo-
rary usage, and has been published in numerous
editions including Thompson (1995, COD95) and
Soanes and Stevenson (2008, COD08). Although
some of the entries have been substantially re-
vised between editions, many have not, enabling
us to easily identify new senses amongst the en-
tries in COD08 relative to COD95. A manual lin-
ear search through the entries in these dictionaries
would be very time consuming, but by exploit-
ing the observation that new words often corre-
spond to concepts that are culturally salient (Ayto,
2006), we can quickly identify some candidates
for words that have taken on a new sense.
Between the time periods of our two corpora,
computers and the Internet have become much
more mainstream in society. We therefore ex-
tracted all entries from COD08 containing the
word computing (which is often used as a topic la-
bel in this dictionary) that have a token frequency
of at least 1000 in the BNC. We then read the
entries for these 87 lexical items in COD95 and
COD08 and identified those which have a clear
computing sense in COD08 that was not present
in COD95. In total we found 22 such items. This
process, along with all the annotation in this sec-
tion, is carried out by a native English-speaking
author of this paper.
To ensure that the words identified from the
dictionaries do in fact have a new sense in the
ukWaC sample compared to the BNC, we exam-
ine the usage of these words in the corpora. We
extract a random sample of 100 usages of each
</bodyText>
<page confidence="0.993922">
597
</page>
<bodyText confidence="0.999971365853659">
lemma from the BNC and ukWaC sample and
annotate these usages as to whether they corre-
spond to the novel sense or not. This binary dis-
tinction is easier than fine-grained sense annota-
tion, and since we do not use these annotations
for formal evaluation — only for selecting items
for our dataset — we do not carry out an inter-
annotator agreement study here. We eliminate any
lemma for which we find evidence of the novel
sense in the BNC, or for which we do not find
evidence of the novel sense in the ukWaC sam-
ple.9 We further check word sketches (Kilgarriff
and Tugwell, 2002)10 for each of these lemmas
in the BNC and ukWaC for collocates that likely
correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel
sense in the BNC, or fail to find evidence of the
novel sense in the ukWaC sample. At the end
of this process we have identified the following
5 lemmas that have the indicated novel senses in
the ukWaC compared to the BNC: domain (n) “In-
ternet domain”; export (v) “export data”; mirror
(n) “mirror website”; poster (n) “one who posts
online”; and worm (n) “malicious program”. For
each of the 5 lemmas with novel senses, a sec-
ond annotator — also a native English-speaking
author of this paper — annotated the sample of
100 usages from the ukWaC. The observed agree-
ment and unweighted Kappa between the two an-
notators is 97.2% and 0.92, respectively, indicat-
ing that this is indeed a relatively easy annotation
task. The annotators discussed the small number
of disagreements to reach consensus.
For our dataset we also require items that have
not acquired a novel sense in the ukWaC sample.
For each of the above 5 lemmas we identified a
distractor lemma of the same part-of-speech that
has a similar frequency in the BNC, and that has
not undergone sense change between COD95 and
COD08. The 5 distractors are: cinema (n); guess
(v); symptom (n); founder (n); and racism (n).
</bodyText>
<subsectionHeader confidence="0.943036">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.989759">
We compute novelty (“Nov”, Equation 1) for all
10 items in our dataset, based on the output of the
</bodyText>
<footnote confidence="0.9915205">
9We use the IMS Open Corpus Workbench (http://
cwb.sourceforge.net/) to extract the usages of our
target lemmas from the corpora. This extraction process fails
in some cases, and so we also eliminate such items from our
dataset.
10http://www.sketchengine.co.uk/
</footnote>
<table confidence="0.999065545454545">
Lemma Novelty Freq. ratio Novel sense freq.
domain (n) 116.2 2.60 41
worm (n) 68.4 1.04 30
mirror (n) 38.4 0.53 10
guess (v) 16.5 0.93 –
export (v) 13.8 0.88 28
founder (n) 11.0 1.20 –
cinema (n) 9.7 1.30 –
poster (n) 7.9 1.83 4
racism (n) 2.4 0.98 –
symptom (n) 2.1 1.16 –
</table>
<tableCaption confidence="0.99888">
Table 4: Novelty score (“Nov”), ratio of frequency in
</tableCaption>
<bodyText confidence="0.990784075">
the ukWaC sample and BNC, and frequency of the
novel sense in the manually-annotated 100 instances
from the ukWaC sample (where applicable), for all
lemmas in our dataset. Lemmas shown in boldface
have a novel sense in the ukWaC sample compared to
the BNC.
topic modelling. The results are shown in column
“Novelty” in Table 4. The lemmas with a novel
sense have higher novelty scores than the distrac-
tors according to a one-sided Wilcoxon rank sum
test (p &lt; .05).
When a lemma takes on a new sense, it might
also increase in frequency. We therefore also con-
sider a baseline in which we rank the lemmas by
the ratio of their frequency in the second and ref-
erence corpora. These results are shown in col-
umn “Freq. ratio” in Table 4. The difference be-
tween the frequency ratios for the lemmas with a
novel sense, and the distractors, is not significant
(p &gt; .05).
Examining the frequency of the novel senses —
shown in column “Novel sense freq.” in Table 4
— we see that the lowest-ranked lemma with a
novel sense, poster, is also the lemma with the
least-frequent novel sense. This result is unsur-
prising as our novelty score will be higher for
higher-frequency novel senses. The identification
of infrequent novel senses remains a challenge.
The top-ranked topic words for the sense cor-
responding to the maximum in Equation 1 for
the highest-ranked distractor, guess, are the fol-
lowing: @card@, post, ..., n’t, comment, think,
subject, forum, view, guess. This sense seems
to correspond to usages of guess in the context
of online forums, which are better represented
in the ukWaC sample than the BNC. Because of
the challenges posed by such differences between
corpora (discussed in Section 4.2) we are unsur-
prised to see such an error, but this could be ad-
dressed in the future by building comparable cor-
</bodyText>
<page confidence="0.992594">
598
</page>
<table confidence="0.999812384615385">
Lemma Topic Selection Methodology
Nov Oracle (single topic) Oracle (multiple topics)
Precision Recall F-score Precision Recall F-score Precision Recall F-score
domain (n)
export (v)
mirror (n)
poster (n)
worm (n)
1.00 0.29 0.45 1.00 0.56 0.72 0.97 0.88 0.92
0.93 0.96 0.95 0.93 0.96 0.95 0.90 1.00 0.95
0.67 1.00 0.80 0.67 1.00 0.80 0.67 1.00 0.80
0.00 0.00 0.00 0.44 1.00 0.62 0.44 1.00 0.62
0.93 0.90 0.92 0.93 0.90 0.92 0.86 1.00 0.92
</table>
<tableCaption confidence="0.994406">
Table 5: Results for identifying the gold-standard novel senses based on the three topic selection methodologies
of: (1) Nov; (2) oracle selection of a single topic; and (3) oracle selection of multiple topics.
</tableCaption>
<bodyText confidence="0.999144838709678">
pora for use in this application.
Having demonstrated that our method for iden-
tifying novel senses can distinguish lemmas that
have a novel sense in one corpus compared to an-
other from those that do not, we now consider
whether this method can also automatically iden-
tify the usages of the induced novel sense.
For each lemma with a gold-standard novel
sense, we define the automatically-induced novel
sense to be the single sense corresponding to the
maximum in Equation 1. We then compute the
precision, recall, and F-score of this novel sense
with respect to the gold-standard novel sense,
based on the 100 annotated tokens for each of
the 5 lemmas with a novel sense. The results are
shown in the first three numeric columns of Ta-
ble 5.
In the case of export and worm the results are
remarkably good, with precision and recall both
over 0.90. For domain, the low recall is a result of
the majority of usages of the gold-standard novel
sense (“Internet domain”) being split across two
induced senses — the top-two highest ranked in-
duced senses according to Equation 1. The poor
performance for poster is unsurprising due to the
very low frequency of this lemma’s gold-standard
novel sense.
These results are based on our novelty rank-
ing method (“Nov”), and the assumption that
the novel sense will be represented in a single
topic. To evaluate the theoretical upper-bound
for a topic-ranking method which uses our HDP-
based WSI method and selects a single topic to
capture the novel sense, we next evaluate an op-
timal topic selection approach. In the middle
three numeric columns of Table 5, we present re-
sults for an experimental setup in which the sin-
gle best induced sense — in terms of F-score —
is selected as the novel sense by an oracle. We
see big improvements in F-score for domain and
poster. This encouraging result suggests refining
the sense selection heuristic could theoretically
improve our method for identifying novel senses,
and that the topic modelling approach proposed
in this paper has considerable promise for auto-
matic novel sense detection. Of particular note is
the result for poster: although the gold-standard
novel sense of poster is rare, all of its usages are
grouped into a single topic.
Finally, we consider whether an oracle which
can select the best subset of induced senses — in
terms of F-score — as the novel sense could of-
fer further improvements. In this case — results
shown in the final three columns of Table 5 —
we again see an increase in F-score to 0.92 for
domain. For this lemma the gold-standard novel
sense usages were split across multiple induced
topics, and so we are unsurprised to find that a
method which is able to select multiple topics as
the novel sense performs well. Based on these
findings, in future work we plan to consider alter-
native formulations of novelty.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999904470588235">
We propose the application of topic modelling
to the task of word sense induction (WSI), start-
ing with a simple LDA-based methodology with
a fixed number of senses, and culminating in
a nonparametric method based on a Hierarchi-
cal Dirichlet Process (HDP), which automatically
learns the number of senses for a given target
word. Our HDP-based method outperforms all
methods over the SemEval-2010 WSI dataset, and
is also superior to other topic modelling-based
approaches to WSI based on the SemEval-2007
dataset. We applied the proposed WSI model to
the task of identifying words which have taken on
new senses, including identifying the token oc-
currences of the new word sense. Over a small
dataset developed in this research, we achieved
highly encouraging results.
</bodyText>
<page confidence="0.99812">
599
</page>
<sectionHeader confidence="0.995869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871144144144">
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7–12, Prague, Czech Re-
public.
John Ayto. 2006. Movers and Shakers: A Chronology
of Words that Shaped our Age. Oxford University
Press, Oxford.
David Bamman and Gregory Crane. 2011. Measur-
ing historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Dig-
ital Libraries (JCDL 2011), pages 1–10, Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993–1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. pages 103–111, Athens, Greece.
Lou Burnard. 2000. The British National Corpus
Users Reference Guide. Oxford University Com-
puting Services.
Paul Cook and Suzanne Stevenson. 2010. Automat-
ically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2010), pages 28–34, Valletta,
Malta.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
Genoa, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47–54, Mar-
rakech, Morocco.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. pages
233–237.
Kristina Gulordava and Marco Baroni. 2011. A dis-
tributional similarity approach to the detection of
semantic change in the Google Books Ngram cor-
pus. In Proceedings of the GEMS 2011 Workshop
on GEometrical Models of Natural Language Se-
mantics, pages 67–71, Edinburgh, Scotland.
Adam Kilgarriff and David Tugwell. 2002. Sketch-
ing words. In Marie-H´el`ene Corr´eard, editor, Lex-
icography and Natural Language Processing: A
Festschrift in Honour of B. T. S. Atkins, pages 125–
137. Euralex, Grenoble, France.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), pages 3–
10, Whistler, Canada.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
Uoy: Graphs of unambiguous vertices for word
sense induction and disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 355–358, Uppsala, Sweden.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dli-
gach, and Sameer Pradhan. 2010. SemEval-2010
Task 14: Word sense induction &amp; disambiguation.
In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63–68, Uppsala,
Sweden.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing word senses to improve web search result
clustering. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 116–126, Cambridge, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2007. I2R: Three systems for word sense discrimi-
nation, chinese word sense disambiguation, and en-
glish word sense disambiguation. In Proceedings
of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 177–182,
Prague, Czech Republic.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33:161–199.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469–491.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104–
111, Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44–49, Manchester, UK.
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
Catherine Soanes and Angus Stevenson, editors. 2008.
The Concise Oxford English Dictionary. Oxford
University Press, eleventh (revised) edition. Oxford
Reference Online.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101:1566–
1581.
</reference>
<page confidence="0.960757">
600
</page>
<reference confidence="0.99951825">
Della Thompson, editor. 1995. The Concise Oxford
Dictionary of Current English. Oxford University
Press, Oxford, ninth edition.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10–14,
Portland, Oregon.
</reference>
<page confidence="0.998235">
601
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.719832">
<title confidence="0.999886">Word Sense Induction for Novel Sense Detection</title>
<author confidence="0.996712">Han Paul Diana McCarthy</author>
<affiliation confidence="0.886484">Victoria Research of Computer Science and Software Engineering, University of of Computer Science, University of California</affiliation>
<abstract confidence="0.998572647058824">We apply topic modelling to automatically senses a target word, and demonstrate that our word sense induction method can be used to automatically detect words with emergent novel senses, as well as token occurrences of those senses. We start by exploring the utility of standard topic models for word sense induction (WSI), with a pre-determined number of topics (=senses). We next demonstrate that a non-parametric formulation that learns an appropriate number of senses per word actually performs better at the WSI task. We go on to establish state-of-the-art results over two WSI datasets, and apply the proposed model to a novel sense detection task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>SemEval-2007 Task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3179" citStr="Agirre and Soroa, 2007" startWordPosition="508" endWordPosition="511"> assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple </context>
<context position="19331" citStr="Agirre and Soroa, 2007" startWordPosition="3255" endWordPosition="3258">n. They carry out extensive parameter optimisation of both the (fixed) number of senses, number of layers, and size of the context window. Separately, Yao and Durme (2011) proposed the use of non-parametric topic models in WSI. The authors preprocess the instances slightly differently, opting to remove the target word from each instance and stem the tokens. They also tuned the hyperparameters of the topic model to optimise the WSI effectiveness over the evaluation set, and didn’t use positional or dependency features. Both of these papers were evaluated over only the SemEval-2007 WSI dataset (Agirre and Soroa, 2007), so we similarly apply our HDP method to this dataset for direct comparability. In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. The SemEval-2007 dataset consists of roughly 27K instances, for 65 target verbs and 35 target nouns. BL report on results only over the noun instances, so we similarly restrict our attention to System F-Score BL 0.855 YVD 0.857 SemEval Best (I2R) 0.868 Our method (default parameters) 0.842 Our method (tuned parameters) 0.869 Table 3: F-score for the SemEval-2007 WSI task, for our HDP method with default an</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 Task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 7–12, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Ayto</author>
</authors>
<title>Movers and Shakers: A Chronology of Words that Shaped our Age.</title>
<date>2006</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="29282" citStr="Ayto, 2006" startWordPosition="4946" endWordPosition="4947">early 21st centuries. The Concise Oxford English Dictionary aims to document contemporary usage, and has been published in numerous editions including Thompson (1995, COD95) and Soanes and Stevenson (2008, COD08). Although some of the entries have been substantially revised between editions, many have not, enabling us to easily identify new senses amongst the entries in COD08 relative to COD95. A manual linear search through the entries in these dictionaries would be very time consuming, but by exploiting the observation that new words often correspond to concepts that are culturally salient (Ayto, 2006), we can quickly identify some candidates for words that have taken on a new sense. Between the time periods of our two corpora, computers and the Internet have become much more mainstream in society. We therefore extracted all entries from COD08 containing the word computing (which is often used as a topic label in this dictionary) that have a token frequency of at least 1000 in the BNC. We then read the entries for these 87 lexical items in COD95 and COD08 and identified those which have a clear computing sense in COD08 that was not present in COD95. In total we found 22 such items. This pro</context>
</contexts>
<marker>Ayto, 2006</marker>
<rawString>John Ayto. 2006. Movers and Shakers: A Chronology of Words that Shaped our Age. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Gregory Crane</author>
</authors>
<title>Measuring historical word sense variation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Joint International Conference on Digital Libraries (JCDL 2011),</booktitle>
<pages>1--10</pages>
<location>Ottawa, Canada.</location>
<contexts>
<context position="25398" citStr="Bamman and Crane (2011)" startWordPosition="4259" endWordPosition="4263">ge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel LatinEnglish corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to the task of 596 identifying new word-senses. In contrast to Bamman and Crane (2011) our token-based approach does not require parallel text to induce senses. 4.1 Method Given two corpora — a reference corpus which</context>
<context position="28415" citStr="Bamman and Crane, 2011" startWordPosition="4799" endWordPosition="4802">ample, text types related to the Internet being much more common in the ukWaC. Such differences are a noted challenge for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010), but are difficult to avoid given the corpora that are available. We use TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora. Evaluating approaches to identifying semantic change is a challenge, particularly due to the lack of appropriate evaluation resources; indeed, most previous approaches have used very small datasets (Sagi et al., 2009; Cook and Stevenson, 2010; Bamman and Crane, 2011). Because this is a preliminary attempt at applying WSI techniques to identifying new word-senses, our evaluation will also be based on a rather small dataset. We require a set of words that are known to have acquired a new sense between the late 20th and early 21st centuries. The Concise Oxford English Dictionary aims to document contemporary usage, and has been published in numerous editions including Thompson (1995, COD95) and Soanes and Stevenson (2008, COD08). Although some of the entries have been substantially revised between editions, many have not, enabling us to easily identify new s</context>
</contexts>
<marker>Bamman, Crane, 2011</marker>
<rawString>David Bamman and Gregory Crane. 2011. Measuring historical word sense variation. In Proceedings of the 2011 Joint International Conference on Digital Libraries (JCDL 2011), pages 1–10, Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2297" citStr="Blei et al. (2003)" startWordPosition="362" endWordPosition="365"> work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use the topic model to determine the appropriate sense granularity. Topic modelling is an unsupervised approach to jointly learn topics — in the form of multinomial probability distributions over words — and per-document topic assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>M Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<pages>103--111</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="2188" citStr="Brody and Lapata (2009)" startWordPosition="343" endWordPosition="346">s a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use the topic model to determine the appropriate sense granularity. Topic modelling is an unsupervised approach to jointly learn topics — in the form of multinomial probability distributions over words — and per-document topic assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighte</context>
<context position="18534" citStr="Brody and Lapata (2009)" startWordPosition="3129" endWordPosition="3132">fair france irish single 2000 cheat#prep at#point complain Table 2: The top-10 terms for each of the senses induced for the verb cheat by the HDP model (with positional word and dependency features) is encouraging. Also, we observe a spin-off benefit of topic modelling approaches to WSI: the high-ranking words in each topic can be used to gist the sense, and anecdotally confirm the impact of the different feature types (i.e. the positional word and dependency features). 3.3 Comparison with other Topic Modelling Approaches to WSI The idea of applying topic modelling to WSI is not entirely new. Brody and Lapata (2009) proposed an LDA-based model which assigns different weights to different feature sets (e.g. unigram tokens vs. dependency relations), using a “layered” feature representation. They carry out extensive parameter optimisation of both the (fixed) number of senses, number of layers, and size of the context window. Separately, Yao and Durme (2011) proposed the use of non-parametric topic models in WSI. The authors preprocess the instances slightly differently, opting to remove the target word from each instance and stem the tokens. They also tuned the hyperparameters of the topic model to optimise</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>S. Brody and M. Lapata. 2009. Bayesian word sense induction. pages 103–111, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>The British National Corpus Users Reference Guide.</title>
<date>2000</date>
<institution>Oxford University Computing Services.</institution>
<contexts>
<context position="20298" citStr="Burnard (2000)" startWordPosition="3423" endWordPosition="3424">larly restrict our attention to System F-Score BL 0.855 YVD 0.857 SemEval Best (I2R) 0.868 Our method (default parameters) 0.842 Our method (tuned parameters) 0.869 Table 3: F-score for the SemEval-2007 WSI task, for our HDP method with default and tuned parameter settings, as compared to competitor topic modelling and other approaches to WSI the nouns in this paper. Training data was not provided as part of the original dataset, so we follow the approach of BL and YVD in constructing our own training dataset for each target word from instances extracted from the British National Corpus (BNC: Burnard (2000)).8 Both BL and YVD separately report slightly higher in-domain results from training on WSJ data (the SemEval2007 data was taken from the WSJ). For the purposes of model comparison under identical training settings, however, it is appropriate to report on results for only the BNC. We experiment with both our original method (with both positional word and dependency features, and default parameter settings for HDP) without any parameter tuning, and the same method with the tuned parameter settings of YVD, for direct comparability. We present the results in Table 3, including the results for th</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. The British National Corpus Users Reference Guide. Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatically identifying changes in the semantic orientation of words.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC 2010),</booktitle>
<pages>28--34</pages>
<location>Valletta,</location>
<contexts>
<context position="25186" citStr="Cook and Stevenson (2010)" startWordPosition="4226" endWordPosition="4229">hese senses is listed in Wordnet 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel LatinEnglish corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to t</context>
<context position="28390" citStr="Cook and Stevenson, 2010" startWordPosition="4795" endWordPosition="4798">these corpora with, for example, text types related to the Internet being much more common in the ukWaC. Such differences are a noted challenge for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010), but are difficult to avoid given the corpora that are available. We use TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora. Evaluating approaches to identifying semantic change is a challenge, particularly due to the lack of appropriate evaluation resources; indeed, most previous approaches have used very small datasets (Sagi et al., 2009; Cook and Stevenson, 2010; Bamman and Crane, 2011). Because this is a preliminary attempt at applying WSI techniques to identifying new word-senses, our evaluation will also be based on a rather small dataset. We require a set of words that are known to have acquired a new sense between the late 20th and early 21st centuries. The Concise Oxford English Dictionary aims to document contemporary usage, and has been published in numerous editions including Thompson (1995, COD95) and Soanes and Stevenson (2008, COD08). Although some of the entries have been substantially revised between editions, many have not, enabling us</context>
</contexts>
<marker>Cook, Stevenson, 2010</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2010. Automatically identifying changes in the semantic orientation of words. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC 2010), pages 28–34, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<location>Genoa, Italy.</location>
<marker>De Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1702" citStr="(1998)" startWordPosition="262" endWordPosition="262">etection task. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukwac, a very large web-derived corpus of english.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop: Can we beat Google,</booktitle>
<pages>47--54</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="27598" citStr="Ferraresi et al., 2008" startWordPosition="4663" endWordPosition="4666">aximum likelihood estimates, and T is the set of senses induced for w. Novelty is high if there is some sense t that has much higher relative frequency in s than r and that is also relatively infrequent in r. 4.2 Data Because we are interested in the identification of novel word-senses for applications such as lexicon maintenance, we focus on relatively newlycoined word-senses. In particular, we take the written portion of the BNC — consisting primarily of British English text from the late 20th century — as our reference corpus, and a similarlysized random sample of documents from the ukWaC (Ferraresi et al., 2008) — a Web corpus built from the .uk domain in 2007 which includes a wide range of text types — as our second corpus. Text genres are represented to different extents in these corpora with, for example, text types related to the Internet being much more common in the ukWaC. Such differences are a noted challenge for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010), but are difficult to avoid given the corpora that are available. We use TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora. Evaluating approaches to identifying semantic chan</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukwac, a very large web-derived corpus of english. In Proceedings of the 4th Web as Corpus Workshop: Can we beat Google, pages 47–54, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<pages>233--237</pages>
<contexts>
<context position="6079" citStr="Gale et al., 1992" startWordPosition="994" endWordPosition="997">truction of semantic space models, e.g. for WSD. Based on these findings, we include dependency relations as additional features in our topic models,2 but just for dependency relations that involve the target word. 2.2 Topic Modelling Topic models learn a probability distribution over topics for each document, by simply aggregating the distributions over topics for each word in the document. In WSI terms, we take this distribution over topics for each target word (“instance” in WSI parlance) as our distribution over senses for that word. 1Notwithstanding the one sense per discourse heuristic (Gale et al., 1992). 2We use the Stanford Parser to do part of speech tagging and to extract the dependency relations (Klein and Manning, 2003; De Marneffe et al., 2006). In our initial experiments, we use LDA topic modelling, which requires us to set T, the number of topics to be learned by the model. The LDA generative process is: (1) draw a latent topic z from a document-specific topic distribution P(t = z|d) then; (2) draw a word w from the chosen topic P(w|t = z). Thus, the probability of producing a single copy of word w given a document d is given by: P(w|d) = ∑T P(w|t = z)P(t = z|d). z=1 In standard LDA,</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Gulordava</author>
<author>Marco Baroni</author>
</authors>
<title>A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>67--71</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="25219" citStr="Gulordava and Baroni (2011)" startWordPosition="4231" endWordPosition="4235">t 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel LatinEnglish corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to the task of 596 identifying new wo</context>
</contexts>
<marker>Gulordava, Baroni, 2011</marker>
<rawString>Kristina Gulordava and Marco Baroni. 2011. A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 67–71, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>David Tugwell</author>
</authors>
<title>Sketching words.</title>
<date>2002</date>
<booktitle>In Marie-H´el`ene Corr´eard, editor, Lexicography and Natural Language Processing: A Festschrift in Honour of B.</booktitle>
<pages>125--137</pages>
<editor>T. S. Atkins,</editor>
<publisher>Euralex,</publisher>
<location>Grenoble, France.</location>
<contexts>
<context position="30811" citStr="Kilgarriff and Tugwell, 2002" startWordPosition="5227" endWordPosition="5230"> extract a random sample of 100 usages of each 597 lemma from the BNC and ukWaC sample and annotate these usages as to whether they correspond to the novel sense or not. This binary distinction is easier than fine-grained sense annotation, and since we do not use these annotations for formal evaluation — only for selecting items for our dataset — we do not carry out an interannotator agreement study here. We eliminate any lemma for which we find evidence of the novel sense in the BNC, or for which we do not find evidence of the novel sense in the ukWaC sample.9 We further check word sketches (Kilgarriff and Tugwell, 2002)10 for each of these lemmas in the BNC and ukWaC for collocates that likely correspond to the novel sense; we exclude any lemma for which we find evidence of the novel sense in the BNC, or fail to find evidence of the novel sense in the ukWaC sample. At the end of this process we have identified the following 5 lemmas that have the indicated novel senses in the ukWaC compared to the BNC: domain (n) “Internet domain”; export (v) “export data”; mirror (n) “mirror website”; poster (n) “one who posts online”; and worm (n) “malicious program”. For each of the 5 lemmas with novel senses, a second an</context>
</contexts>
<marker>Kilgarriff, Tugwell, 2002</marker>
<rawString>Adam Kilgarriff and David Tugwell. 2002. Sketching words. In Marie-H´el`ene Corr´eard, editor, Lexicography and Natural Language Processing: A Festschrift in Honour of B. T. S. Atkins, pages 125– 137. Euralex, Grenoble, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS 2002),</booktitle>
<pages>3--10</pages>
<location>Whistler, Canada.</location>
<contexts>
<context position="6202" citStr="Klein and Manning, 2003" startWordPosition="1015" endWordPosition="1018">l features in our topic models,2 but just for dependency relations that involve the target word. 2.2 Topic Modelling Topic models learn a probability distribution over topics for each document, by simply aggregating the distributions over topics for each word in the document. In WSI terms, we take this distribution over topics for each target word (“instance” in WSI parlance) as our distribution over senses for that word. 1Notwithstanding the one sense per discourse heuristic (Gale et al., 1992). 2We use the Stanford Parser to do part of speech tagging and to extract the dependency relations (Klein and Manning, 2003; De Marneffe et al., 2006). In our initial experiments, we use LDA topic modelling, which requires us to set T, the number of topics to be learned by the model. The LDA generative process is: (1) draw a latent topic z from a document-specific topic distribution P(t = z|d) then; (2) draw a word w from the chosen topic P(w|t = z). Thus, the probability of producing a single copy of word w given a document d is given by: P(w|d) = ∑T P(w|t = z)P(t = z|d). z=1 In standard LDA, the user needs to specify the number of topics T. In non-parametric variants of LDA, the model dynamically learns the numb</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15 (NIPS 2002), pages 3– 10, Whistler, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Korkontzelos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>355--358</pages>
<location>Uppsala,</location>
<contexts>
<context position="12551" citStr="Korkontzelos and Manandhar, 2010" startWordPosition="2109" endWordPosition="2112">r each of nouns and verbs. 3. LDA+Fixed T+Position: LDA with fixed T and extra positional word features. 4. HDP+Position: HDP (which automatically learns T), with extra positional word features. 5. HDP+Position+Dependency: HDP with both positional word and dependency features. We compare our models with two baselines from the SemEval-2010 task: (1) Baseline Random — randomly assign each test instance to one of four senses; (2) Baseline MFS — most frequent sense baseline, assigning all test instances to one sense; and also a benchmark system (UoY), in the form of the University of York system (Korkontzelos and Manandhar, 2010), which achieved the best overall WSD results in the original SemEval-2010 task. 3.2 Sen Eval-2010 Results The results of our experiments over the SemEval2010 dataset are summarised in Table 1. 593 System WSD (80%/20%) All Verbs Nouns Baselines Baseline Random 0.57 0.66 0.51 Baseline MFS 0.59 0.67 0.53 LDA Variable T 0.64 0.69 0.60 Fixed T 0.63 0.68 0.59 Fixed T +Position 0.63 0.68 0.60 HDP +Position 0.68 0.72 0.65 +Position+Dependency 0.68 0.72 0.65 Benchmark 0.62 0.67 0.59 UoY Table 1: WSD F-score over the SemEval-2010 dataset Looking first at the results for LDA, we see that the first LDA a</context>
</contexts>
<marker>Korkontzelos, Manandhar, 2010</marker>
<rawString>Ioannis Korkontzelos and Suresh Manandhar. 2010. Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 355–358, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer Pradhan</author>
</authors>
<title>SemEval-2010 Task 14: Word sense induction &amp; disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>63--68</pages>
<location>Uppsala,</location>
<contexts>
<context position="3204" citStr="Manandhar et al., 2010" startWordPosition="512" endWordPosition="516">rm of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple topics, with each documen</context>
<context position="8455" citStr="Manandhar et al., 2010" startWordPosition="1410" endWordPosition="1413">Eval2010 word sense induction tasks (Agirre and 3We use the C++ implementation of HDP (http://www.cs.princeton.edu/˜blei/ topicmodeling.html) in our experiments. 4The two HDP parameters γ and α0 control the variability of senses in the documents. In particular, γ controls the degree of sharing of topics across documents — a high γ value leads to more topics, as topics for different documents are more dissimilar. α0, on the other hand, controls the degree of mixing of topics within a document — a high α0 generates fewer topics, as topics are less homogeneous within a document. 592 Soroa, 2007; Manandhar et al., 2010). We first experiment with the SemEval-2010 dataset, as it includes explicit training and test data for each target word and utilises a more robust evaluation methodology. We then return to experiment with the SemEval-2007 dataset, for comparison purposes with other published results for topic modelling approaches to WSI. 3.1 Sen Eval-2010 3.1.1 Dataset and Methodology Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al., 2010). The dataset contains 100 target words: 50 nouns and 50 verbs. For each target word, a fixed set of trainin</context>
<context position="10778" citStr="Manandhar et al., 2010" startWordPosition="1800" endWordPosition="1803">ews sources. Following the standard approach used by the participating systems in the SemEval-2010 task, we induce senses only from the training instances, and use the learned model to assign senses to the test instances. 5A 60%/40% split is also provided as part of the task setup, but the results are almost identical to those for the 80%/20% split, and so are omitted from this paper. The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al., 2010), and are hence omitted from this paper. In our original experiments with LDA, we set the number of topics (T) for each target word to the number of senses represented in the test data for that word (varying T for each target word). This is based on the unreasonable assumption that we will have access to gold-standard information on sense granularity for each target word, and is done to establish an upper bound score for LDA. We then relax the assumption, and use a fixed T setting for each of sets of nouns (T = 7) and verbs (T = 3), based on the average number of senses from the test data in e</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach, and Sameer Pradhan. 2010. SemEval-2010 Task 14: Word sense induction &amp; disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63–68, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Giuseppe Crisafulli</author>
</authors>
<title>Inducing word senses to improve web search result clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>116--126</pages>
<location>Cambridge, USA.</location>
<contexts>
<context position="1736" citStr="Navigli and Crisafulli (2010)" startWordPosition="264" endWordPosition="267">sk. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use t</context>
</contexts>
<marker>Navigli, Crisafulli, 2010</marker>
<rawString>Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing word senses to improve web search result clustering. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 116–126, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Dong-Hong Ji</author>
<author>Chew-Lim Tan</author>
</authors>
<title>I2R: Three systems for word sense discrimination, chinese word sense disambiguation, and english word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>177--182</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="20979" citStr="Niu et al. (2007)" startWordPosition="3534" endWordPosition="3537">results from training on WSJ data (the SemEval2007 data was taken from the WSJ). For the purposes of model comparison under identical training settings, however, it is appropriate to report on results for only the BNC. We experiment with both our original method (with both positional word and dependency features, and default parameter settings for HDP) without any parameter tuning, and the same method with the tuned parameter settings of YVD, for direct comparability. We present the results in Table 3, including the results for the bestperforming system in the original SemEval-2007 task (I2R: Niu et al. (2007)). The results are enlightening: with default parameter settings, our methodology is slightly below the results of the other three models. Bear 8In creating the training dataset, each instance is made up of the sentence the target word occurs in, as we as one sentence to either side of that sentence, i.e. 3 sentences in total per instance. 595 in mind, however, that the two topic modellingbased approaches were tuned extensively to the dataset. When we use the tuned hyperparameter settings of YVD, our results rise around 2.5% to surpass both topic modelling approaches, and marginally outperform</context>
</contexts>
<marker>Niu, Ji, Tan, 2007</marker>
<rawString>Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2R: Three systems for word sense discrimination, chinese word sense disambiguation, and english word sense disambiguation. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 177–182, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<pages>33--161</pages>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Comput. Linguist., 33:161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Dirk Geeraerts</author>
<author>Dirk Speelman</author>
</authors>
<title>The automatic identification of lexical variation between language varieties.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="28008" citStr="Peirsman et al., 2010" startWordPosition="4736" endWordPosition="4740">tten portion of the BNC — consisting primarily of British English text from the late 20th century — as our reference corpus, and a similarlysized random sample of documents from the ukWaC (Ferraresi et al., 2008) — a Web corpus built from the .uk domain in 2007 which includes a wide range of text types — as our second corpus. Text genres are represented to different extents in these corpora with, for example, text types related to the Internet being much more common in the ukWaC. Such differences are a noted challenge for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010), but are difficult to avoid given the corpora that are available. We use TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora. Evaluating approaches to identifying semantic change is a challenge, particularly due to the lack of appropriate evaluation resources; indeed, most previous approaches have used very small datasets (Sagi et al., 2009; Cook and Stevenson, 2010; Bamman and Crane, 2011). Because this is a preliminary attempt at applying WSI techniques to identifying new word-senses, our evaluation will also be based on a rather small dataset. We require a set of words that ar</context>
</contexts>
<marker>Peirsman, Geeraerts, Speelman, 2010</marker>
<rawString>Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation between language varieties. Natural Language Engineering, 16(4):469–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Sagi</author>
<author>Stefan Kaufmann</author>
<author>Brady Clark</author>
</authors>
<title>Semantic density analysis: Comparing word meaning across time and space.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical Models of Natural Language Semantics,</booktitle>
<pages>104--111</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="25159" citStr="Sagi et al. (2009)" startWordPosition="4222" endWordPosition="4225">— e.g., neither of these senses is listed in Wordnet 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel LatinEnglish corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply </context>
<context position="28364" citStr="Sagi et al., 2009" startWordPosition="4791" endWordPosition="4794">fferent extents in these corpora with, for example, text types related to the Internet being much more common in the ukWaC. Such differences are a noted challenge for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010), but are difficult to avoid given the corpora that are available. We use TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora. Evaluating approaches to identifying semantic change is a challenge, particularly due to the lack of appropriate evaluation resources; indeed, most previous approaches have used very small datasets (Sagi et al., 2009; Cook and Stevenson, 2010; Bamman and Crane, 2011). Because this is a preliminary attempt at applying WSI techniques to identifying new word-senses, our evaluation will also be based on a rather small dataset. We require a set of words that are known to have acquired a new sense between the late 20th and early 21st centuries. The Concise Oxford English Dictionary aims to document contemporary usage, and has been published in numerous editions including Thompson (1995, COD95) and Soanes and Stevenson (2008, COD08). Although some of the entries have been substantially revised between editions, </context>
</contexts>
<marker>Sagi, Kaufmann, Clark, 2009</marker>
<rawString>Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009. Semantic density analysis: Comparing word meaning across time and space. In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical Models of Natural Language Semantics, pages 104– 111, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="28107" citStr="Schmid, 1994" startWordPosition="4755" endWordPosition="4756">eference corpus, and a similarlysized random sample of documents from the ukWaC (Ferraresi et al., 2008) — a Web corpus built from the .uk domain in 2007 which includes a wide range of text types — as our second corpus. Text genres are represented to different extents in these corpora with, for example, text types related to the Internet being much more common in the ukWaC. Such differences are a noted challenge for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010), but are difficult to avoid given the corpora that are available. We use TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora. Evaluating approaches to identifying semantic change is a challenge, particularly due to the lack of appropriate evaluation resources; indeed, most previous approaches have used very small datasets (Sagi et al., 2009; Cook and Stevenson, 2010; Bamman and Crane, 2011). Because this is a preliminary attempt at applying WSI techniques to identifying new word-senses, our evaluation will also be based on a rather small dataset. We require a set of words that are known to have acquired a new sense between the late 20th and early 21st centuries. The Concise Ox</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<contexts>
<context position="1702" citStr="Schutze (1998)" startWordPosition="261" endWordPosition="262"> sense detection task. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>Hinrich Schutze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<title>The Concise Oxford English Dictionary.</title>
<date>2008</date>
<editor>Catherine Soanes and Angus Stevenson, editors.</editor>
<publisher>Oxford University Press,</publisher>
<marker>2008</marker>
<rawString>Catherine Soanes and Angus Stevenson, editors. 2008. The Concise Oxford English Dictionary. Oxford University Press, eleventh (revised) edition. Oxford Reference Online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<pages>1581</pages>
<contexts>
<context position="2975" citStr="Teh et al. (2006)" startWordPosition="474" endWordPosition="477">rmine the appropriate sense granularity. Topic modelling is an unsupervised approach to jointly learn topics — in the form of multinomial probability distributions over words — and per-document topic assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our </context>
<context position="6983" citStr="Teh et al. (2006)" startWordPosition="1160" endWordPosition="1163">e LDA generative process is: (1) draw a latent topic z from a document-specific topic distribution P(t = z|d) then; (2) draw a word w from the chosen topic P(w|t = z). Thus, the probability of producing a single copy of word w given a document d is given by: P(w|d) = ∑T P(w|t = z)P(t = z|d). z=1 In standard LDA, the user needs to specify the number of topics T. In non-parametric variants of LDA, the model dynamically learns the number of topics as part of the topic modelling. The particular implementation of non-parametric topic model we experiment with is Hierarchical Dirichlet Process (HDP: Teh et al. (2006)),3 where, for each document, a distribution of mixture components P(t|d) is sampled from a base distribution G0 as follows: (1) choose a base distribution G0 ∼ DP(-y, H); (2) for each document d, generate distribution P(t|d) ∼ DP(α0, G0); (3) draw a latent topic z from the document’s mixture component distribution P(t|d), in the same manner as for LDA; and (4) draw a word w from the chosen topic P(w|t = z).4 For both LDA and HDP, we individually topic model each target word, and determine the sense assignment z for a given instance by aggregating over the topic assignments for each word in th</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566– 1581.</rawString>
</citation>
<citation valid="true">
<title>The Concise Oxford Dictionary of Current English.</title>
<date>1995</date>
<editor>Della Thompson, editor.</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford,</location>
<note>ninth edition.</note>
<marker>1995</marker>
<rawString>Della Thompson, editor. 1995. The Concise Oxford Dictionary of Current English. Oxford University Press, Oxford, ninth edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Nonparametric bayesian word sense induction.</title>
<date>2011</date>
<booktitle>In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing,</booktitle>
<pages>10--14</pages>
<location>Portland, Oregon.</location>
<marker>Yao, Van Durme, 2011</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2011. Nonparametric bayesian word sense induction. In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing, pages 10–14, Portland, Oregon.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>