<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019710">
<title confidence="0.977564">
KELVIN: a tool for automated knowledge base construction
</title>
<author confidence="0.998247">
Paul McNamee, James Mayfield Tim Finin, Tim Oates
</author>
<affiliation confidence="0.9877545">
Johns Hopkins University University of Maryland
Human Language Technology Center of Excellence Baltimore County
</affiliation>
<author confidence="0.993476">
Dawn Lawrie
</author>
<affiliation confidence="0.993149">
Loyola University Maryland
</affiliation>
<sectionHeader confidence="0.978613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999581166666667">
We present KELVIN, an automated system for
processing a large text corpus and distilling a
knowledge base about persons, organizations,
and locations. We have tested the KELVIN
system on several corpora, including: (a) the
TAC KBP 2012 Cold Start corpus which con-
sists of public Web pages from the University
of Pennsylvania, and (b) a subset of 26k news
articles taken from English Gigaword 5th edi-
tion.
Our NAACL HLT 2013 demonstration per-
mits a user to interact with a set of search-
able HTML pages, which are automatically
generated from the knowledge base. Each
page contains information analogous to the
semi-structured details about an entity that are
present in Wikipedia Infoboxes, along with
hyperlink citations to supporting text.
</bodyText>
<sectionHeader confidence="0.99895" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992221909090909">
The Text Analysis Conference (TAC) Knowledge
Base Population (KBP) Cold Start task1 requires
systems to take set of documents and produce a
comprehensive set of &lt;Subject, Predicate, Object&gt;
triples that encode relationships between and at-
tributes of the named-entities that are mentioned in
the corpus. Systems are evaluated based on the fi-
delity of the constructed knowledge base. For the
2012 evaluation, a fixed schema of 42 relations (or
slots), and their logical inverses was provided, for
example:
</bodyText>
<listItem confidence="0.979344">
• X:Organization employs Y:Person
</listItem>
<footnote confidence="0.9899855">
1See details at http://www.nist.gov/tac/2012/
KBP/task_guidelines/index.html
</footnote>
<page confidence="0.99457">
32
</page>
<author confidence="0.992596">
Tan Xu, Douglas W. Oard
</author>
<affiliation confidence="0.7863115">
University of Maryland
College Park
</affiliation>
<listItem confidence="0.9994945">
• X:Person has-job-title title
• X:Organization headquartered-in Y:Location
</listItem>
<bodyText confidence="0.999069705882353">
Multiple layers of NLP software are required for
this undertaking, including at the least: detection of
named-entities, intra-document co-reference resolu-
tion, relation extraction, and entity disambiguation.
To help prevent a bias towards learning about
prominent entities at the expense of generality,
KELVIN refrains from mining facts from sources
such as documents obtained through Web search,
Wikipedia2, or DBpedia.3 Only facts that are as-
serted in and gleaned from the source documents are
posited.
Other systems that create large-scale knowledge
bases from general text include the Never-Ending
Language Learning (NELL) system at Carnegie
Mellon University (Carlson et al., 2010), and the
TextRunner system developed at the University of
Washington (Etzioni et al., 2008).
</bodyText>
<sectionHeader confidence="0.899807" genericHeader="method">
2 Washington Post KB
</sectionHeader>
<bodyText confidence="0.9998675">
No gold-standard KBs were available to us to assist
during the development of KELVIN, so we relied on
qualitative assessment to gauge the effectiveness of
our extracted relations – by manually examining ten
random samples for each relations, we ascertained
that most relations were between 30-80% accurate.
Although the TAC KBP 2012 Cold Start task was a
pilot evaluation of a new task using a novel evalua-
tion methodology, the KELVIN system did attain the
highest reported F1 scores.4
</bodyText>
<footnote confidence="0.916048">
2http://en.wikipedia.org/
3http://www.dbpedia.org/
40.497 0-hop &amp; 0.363 all-hops, as reported in the prelimi-
nary TAC 2012 Evaluation Results.
</footnote>
<subsubsectionHeader confidence="0.349561">
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 32–35,
</subsubsectionHeader>
<bodyText confidence="0.952813428571428">
Atlanta, Georgia, 10-12 June 2013. c�2013 Association for Computational Linguistics
During our initial development we worked with
a 26,143 document collection of 2010 Washington
Post articles and the system discovered 194,059 re-
lations about 57,847 named entities. KELVIN learns
some interesting, but rather dubious relations from
the Washington Post articles5
</bodyText>
<listItem confidence="0.995356904761905">
• Sen. Harry Reid is an employee of the “Repub-
lican Party.” Sen. Reid is also an employee of
the “Democratic Party.”
• Big Foot is an employee of Starbucks.
• MacBook Air is a subsidiary of Apple Inc.
• Jill Biden is married to Jill Biden.
However, KELVIN also learns quite a number of
correct facts, including:
• Warren Buffett owns shares of Berkshire Hath-
away, Burlington Northern Santa Fe, the Wash-
ington Post Co., and four other stocks.
• Jared Fogle is an employee of Subway.
• Freeman Hrabowski works for UMBC,
founded the Meyerhoff Scholars Program, and
graduated from Hampton University and the
University of Illinois.
• Supreme Court Justice Elena Kagan attended
Oxford, Harvard, and Princeton.
• Southwest Airlines is headquartered in Texas.
• Ian Soboroff is a computer scientist6 employed
by NIST.7
</listItem>
<sectionHeader confidence="0.98896" genericHeader="method">
3 Pipeline Components
</sectionHeader>
<subsectionHeader confidence="0.981639">
3.1 SERIF
</subsectionHeader>
<bodyText confidence="0.999972">
BBN’s SERIF tool8 (Boschee et al., 2005) provides
a considerable suite of document annotations that
are an excellent basis for building a knowledge base.
The functions SERIF can provide are based largely
</bodyText>
<footnote confidence="0.979039428571429">
5All 2010 Washington Post articles from English Gigaword
5th ed. (LDC2011T07).
6Ian is the sole computer scientist discovered in processing
a year of news. In contrast, KELVIN found 52 lobbyists.
7From Washington Post article (WPB ENG 20100506.0012
in LDC2011T07).
8Statistical Entity &amp; Relation Information Finding.
</footnote>
<table confidence="0.999843727272727">
Slotname Count
per:employee of 60,690
org:employees 44,663
gpe:employees 16,027
per:member of 14,613
org:membership 14,613
org:city of headquarters 12,598
gpe:headquarters in city 12,598
org:parents 6,526
org:country of headquarters 4,503
gpe:headquarters in country 4,503
</table>
<tableCaption confidence="0.7312115">
Table 1: Most prevalent slots extracted by SERIF from
the Washington Post texts.
</tableCaption>
<table confidence="0.999785454545454">
Slotname Count
per:title 44,896
per:employee of 39,101
per:member of 20,735
per:countries of residence 8,192
per:origin 4,187
per:statesorprovinces of residence 3,376
per:cities of residence 3,376
per:country of birth 1,577
per:age 1,233
per:spouse 1,057
</table>
<tableCaption confidence="0.94977">
Table 2: Most prevalent slots extracted by FACETS from
the Washington Post texts.
</tableCaption>
<bodyText confidence="0.9999347">
on the NIST ACE specification,9 and include: (a)
identifying named-entities and classifying them by
type and subtype; (b) performing intra-document
co-reference analysis, including named mentions,
as well as co-referential nominal and pronominal
mentions; (c) parsing sentences and extracting intra-
sentential relations between entities; and, (d) detect-
ing certain types of events.
In Table 1 we list the most common slots SERIF
extracts from the Washington Post articles.
</bodyText>
<subsectionHeader confidence="0.919255">
3.2 FACETS
</subsectionHeader>
<bodyText confidence="0.99926275">
FACETS, another BBN tool, is an add-on pack-
age that takes SERIF output and produces role and
argument annotations about person noun phrases.
FACETS is implemented using a conditional-
</bodyText>
<footnote confidence="0.9445916">
9The principal types of ACE named-entities are per-
sons, organizations, and geo-political entities (GPEs).
GPEs are inhabited locations with a government. See
http://www.itl.nist.gov/iad/mig/tests/ace/
2008/doc/ace08-evalplan.v1.2d.pdf.
</footnote>
<page confidence="0.99883">
33
</page>
<figureCaption confidence="0.974826666666667">
Figure 1: Simple rendering of KB page about former Florida congressman Joe Scarborough. Many facts are correct
– he lived in and was employed by the State of Florida; he has a brother George; he was a member of the Republican
House of Representatives; and, he is employed by MSNBC.
</figureCaption>
<bodyText confidence="0.963874125">
exponential learner trained on broadcast news. The
attributes FACETS can recognize include general at-
tributes like religion and age (which anyone might
have), as well as role-specific attributes, such as
medical specialty for physicians, or academic insti-
tution for someone associated with an university.
In Table 2 we report the most prevalent slots
FACETS extracts from the Washington Post.10
</bodyText>
<subsectionHeader confidence="0.994523">
3.3 CUNY toolkit
</subsectionHeader>
<bodyText confidence="0.999845823529412">
To increase our coverage of relations we also in-
tegrated the KBP Slot Filling Toolkit (Chen et al.,
2011) developed at the CUNY BLENDER Lab.
Given that the KBP toolkit was designed for the tra-
ditional slot filling task at TAC, this primarily in-
volved creating the queries that the tool expected as
input and parallelizing the toolkit to handle the vast
number of queries issued in the cold start scenarios.
To informally gauge the accuracy of slots
extracted from the CUNY tool, some coarse as-
sessment was done over a small collection of 807
New York Times articles that include the string
“University of Kansas.” From this collection, 4264
slots were identified. Nine different types of slots
were filled in order of frequency: per:title (37%),
per:employee of (23%), per:cities of residence
(17%), per:stateorprovinces of residence (6%),
</bodyText>
<footnote confidence="0.9073425">
10Note FACETS can independently extract some slots that
SERIF is capable of discovering (e.g., employment relations).
</footnote>
<bodyText confidence="0.9597692">
org:top members/employees (6%), org:member of
(6%), per:countries of residence (2%), per:spouse
(2%), and per:member of (1%). We randomly sam-
pled 10 slot-fills of each type, and found accuracy
to vary from 20-70%.
</bodyText>
<subsectionHeader confidence="0.98304">
3.4 Coreference
</subsectionHeader>
<bodyText confidence="0.999788333333333">
We used two methods for entity coreference. Un-
der the theory that name ambiguity may not be a
huge problem, we adopted a baseline approach of
merging entities across different documents if their
canonical mentions were an exact string match af-
ter some basic normalizations, such as removing
punctuation and conversion to lower-case charac-
ters. However we also used the JHU HLTCOE
CALE system (Stoyanov et al., 2012), which maps
named-entity mentions to the TAC-KBP reference
KB, which was derived from a 2008 snapshot of En-
glish Wikipedia. For entities that are not found in the
KB, we reverted to exact string match. CALE entity
linking proved to be the more effective approach for
the Cold Start task.
</bodyText>
<subsectionHeader confidence="0.826516">
3.5 Timex2 Normalization
</subsectionHeader>
<bodyText confidence="0.998321666666667">
SERIF recognizes, but does not normalize, temporal
expressions, so we used the Stanford SUTime pack-
age, to normalize date values.
</bodyText>
<page confidence="0.997755">
34
</page>
<figureCaption confidence="0.9871925">
Figure 2: Supporting text for some assertions about Mr. Scarborough. Source documents are also viewable by
following hyperlinks.
</figureCaption>
<subsectionHeader confidence="0.985861">
3.6 Lightweight Inference
</subsectionHeader>
<bodyText confidence="0.999858857142857">
We performed a small amount of light inference to
fill some slots. For example, if we identified that
a person P worked for organization O, and we also
extracted a job title T for P, and if T matched a set
of titles such as president or minister we asserted
that the tuple &lt;O, org:top members employees, P&gt;
relation also held.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="method">
4 Ongoing Work
</sectionHeader>
<bodyText confidence="0.9998745">
There are a number of improvements that we are un-
dertaking, including: scaling to much larger corpora,
detecting contradictions, expanding the use of infer-
ence, exploiting the confidence of extracted infor-
mation, and applying KELVIN to various genres of
text.
</bodyText>
<sectionHeader confidence="0.991886" genericHeader="conclusions">
5 Script Outline
</sectionHeader>
<bodyText confidence="0.99939175">
The KB generated by KELVIN is best explored us-
ing a Wikipedia metaphor. Thus our demonstration
consists of a web browser that starts with a list of
moderately prominent named-entities that the user
can choose to examine (e.g., investor Warren Buf-
fett, Supreme Court Justice Elena Kagan, Southwest
Airlines Co., the state of Florida). Selecting any
entity takes one to a page displaying its known at-
tributes and relations, with links to documents that
serve as provenance for each assertion. On every
page, each entity is hyperlinked to its own canon-
ical page; therefore the user is able to browse the
KB much as one browses Wikipedia by simply fol-
lowing links. A sample generated page is shown in
Figure 1 and text that supports some of the learned
assertions in the figure is shown in Figure 2. We
also provide a search interface to support jump-
ing to a desired entity and can demonstrate access-
ing the data encoded in the semantic web language
RDF (World Wide Web Consortium, 2013), which
supports ontology browsing and executing complex
SPARQL queries (Prud’Hommeaux and Seaborne,
2008) such as “List the employers of people living
in Nebraska or Kansas who are older than 40.”
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999574625">
E. Boschee, R. Weischedel, and A. Zamanian. 2005. Au-
tomatic information extraction. In Proceedings of the
2005 International Conference on Intelligence Analy-
sis, McLean, VA, pages 2–4.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Z. Chen, S. Tamang, A. Lee, X. Li, and H. Ji. 2011.
Knowledge Base Population (KBP) Toolkit @ CUNY
BLENDER LAB Manual.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68–74, Decem-
ber.
E Prud’Hommeaux and A. Seaborne. 2008. SPARQL
query language for RDF. Technical report, World
Wide Web Consortium, January.
Veselin Stoyanov, James Mayfield, Tan Xu, Douglas W.
Oard, Dawn Lawrie, Tim Oates, and Tim Finin. 2012.
A context-aware approach to entity linking. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge Ex-
traction, AKBC-WEKEX ’12, pages 62–67, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
World Wide Web Consortium. 2013. Resource Descrip-
tion Framework Specification. ”http://http://
www.w3.org/RDF/. ”[Online; accessed 8 April,
2013]”.
</reference>
<page confidence="0.999323">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.761549">
<title confidence="0.997888">KELVIN: a tool for automated knowledge base construction</title>
<author confidence="0.996784">Paul McNamee</author>
<author confidence="0.996784">James Mayfield Tim Finin</author>
<author confidence="0.996784">Tim Oates</author>
<affiliation confidence="0.964623">Johns Hopkins University University of Maryland</affiliation>
<title confidence="0.808604">Human Language Technology Center of Excellence Baltimore County</title>
<author confidence="0.982057">Dawn Lawrie</author>
<affiliation confidence="0.999382">Loyola University Maryland</affiliation>
<abstract confidence="0.99871">We present KELVIN, an automated system for processing a large text corpus and distilling a knowledge base about persons, organizations, and locations. We have tested the KELVIN system on several corpora, including: (a) the TAC KBP 2012 Cold Start corpus which consists of public Web pages from the University of Pennsylvania, and (b) a subset of 26k news articles taken from English Gigaword 5th edition. Our NAACL HLT 2013 demonstration permits a user to interact with a set of searchable HTML pages, which are automatically generated from the knowledge base. Each page contains information analogous to the semi-structured details about an entity that are present in Wikipedia Infoboxes, along with hyperlink citations to supporting text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Boschee</author>
<author>R Weischedel</author>
<author>A Zamanian</author>
</authors>
<title>Automatic information extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 International Conference on Intelligence Analysis,</booktitle>
<pages>2--4</pages>
<location>McLean, VA,</location>
<contexts>
<context position="4521" citStr="Boschee et al., 2005" startWordPosition="680" endWordPosition="683">arns quite a number of correct facts, including: • Warren Buffett owns shares of Berkshire Hathaway, Burlington Northern Santa Fe, the Washington Post Co., and four other stocks. • Jared Fogle is an employee of Subway. • Freeman Hrabowski works for UMBC, founded the Meyerhoff Scholars Program, and graduated from Hampton University and the University of Illinois. • Supreme Court Justice Elena Kagan attended Oxford, Harvard, and Princeton. • Southwest Airlines is headquartered in Texas. • Ian Soboroff is a computer scientist6 employed by NIST.7 3 Pipeline Components 3.1 SERIF BBN’s SERIF tool8 (Boschee et al., 2005) provides a considerable suite of document annotations that are an excellent basis for building a knowledge base. The functions SERIF can provide are based largely 5All 2010 Washington Post articles from English Gigaword 5th ed. (LDC2011T07). 6Ian is the sole computer scientist discovered in processing a year of news. In contrast, KELVIN found 52 lobbyists. 7From Washington Post article (WPB ENG 20100506.0012 in LDC2011T07). 8Statistical Entity &amp; Relation Information Finding. Slotname Count per:employee of 60,690 org:employees 44,663 gpe:employees 16,027 per:member of 14,613 org:membership 14,</context>
</contexts>
<marker>Boschee, Weischedel, Zamanian, 2005</marker>
<rawString>E. Boschee, R. Weischedel, and A. Zamanian. 2005. Automatic information extraction. In Proceedings of the 2005 International Conference on Intelligence Analysis, McLean, VA, pages 2–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI</booktitle>
<contexts>
<context position="2463" citStr="Carlson et al., 2010" startWordPosition="357" endWordPosition="360">, including at the least: detection of named-entities, intra-document co-reference resolution, relation extraction, and entity disambiguation. To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia2, or DBpedia.3 Only facts that are asserted in and gleaned from the source documents are posited. Other systems that create large-scale knowledge bases from general text include the Never-Ending Language Learning (NELL) system at Carnegie Mellon University (Carlson et al., 2010), and the TextRunner system developed at the University of Washington (Etzioni et al., 2008). 2 Washington Post KB No gold-standard KBs were available to us to assist during the development of KELVIN, so we relied on qualitative assessment to gauge the effectiveness of our extracted relations – by manually examining ten random samples for each relations, we ascertained that most relations were between 30-80% accurate. Although the TAC KBP 2012 Cold Start task was a pilot evaluation of a new task using a novel evaluation methodology, the KELVIN system did attain the highest reported F1 scores.4</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chen</author>
<author>S Tamang</author>
<author>A Lee</author>
<author>X Li</author>
<author>H Ji</author>
</authors>
<date>2011</date>
<journal>Knowledge Base Population (KBP) Toolkit @ CUNY BLENDER LAB Manual.</journal>
<contexts>
<context position="7396" citStr="Chen et al., 2011" startWordPosition="1094" endWordPosition="1097">er George; he was a member of the Republican House of Representatives; and, he is employed by MSNBC. exponential learner trained on broadcast news. The attributes FACETS can recognize include general attributes like religion and age (which anyone might have), as well as role-specific attributes, such as medical specialty for physicians, or academic institution for someone associated with an university. In Table 2 we report the most prevalent slots FACETS extracts from the Washington Post.10 3.3 CUNY toolkit To increase our coverage of relations we also integrated the KBP Slot Filling Toolkit (Chen et al., 2011) developed at the CUNY BLENDER Lab. Given that the KBP toolkit was designed for the traditional slot filling task at TAC, this primarily involved creating the queries that the tool expected as input and parallelizing the toolkit to handle the vast number of queries issued in the cold start scenarios. To informally gauge the accuracy of slots extracted from the CUNY tool, some coarse assessment was done over a small collection of 807 New York Times articles that include the string “University of Kansas.” From this collection, 4264 slots were identified. Nine different types of slots were filled</context>
</contexts>
<marker>Chen, Tamang, Lee, Li, Ji, 2011</marker>
<rawString>Z. Chen, S. Tamang, A. Lee, X. Li, and H. Ji. 2011. Knowledge Base Population (KBP) Toolkit @ CUNY BLENDER LAB Manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2008</date>
<journal>Commun. ACM,</journal>
<volume>51</volume>
<issue>12</issue>
<contexts>
<context position="2555" citStr="Etzioni et al., 2008" startWordPosition="371" endWordPosition="374">n, relation extraction, and entity disambiguation. To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia2, or DBpedia.3 Only facts that are asserted in and gleaned from the source documents are posited. Other systems that create large-scale knowledge bases from general text include the Never-Ending Language Learning (NELL) system at Carnegie Mellon University (Carlson et al., 2010), and the TextRunner system developed at the University of Washington (Etzioni et al., 2008). 2 Washington Post KB No gold-standard KBs were available to us to assist during the development of KELVIN, so we relied on qualitative assessment to gauge the effectiveness of our extracted relations – by manually examining ten random samples for each relations, we ascertained that most relations were between 30-80% accurate. Although the TAC KBP 2012 Cold Start task was a pilot evaluation of a new task using a novel evaluation methodology, the KELVIN system did attain the highest reported F1 scores.4 2http://en.wikipedia.org/ 3http://www.dbpedia.org/ 40.497 0-hop &amp; 0.363 all-hops, as report</context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Commun. ACM, 51(12):68–74, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Prud’Hommeaux</author>
<author>A Seaborne</author>
</authors>
<title>SPARQL query language for RDF.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>World Wide Web Consortium,</institution>
<marker>Prud’Hommeaux, Seaborne, 2008</marker>
<rawString>E Prud’Hommeaux and A. Seaborne. 2008. SPARQL query language for RDF. Technical report, World Wide Web Consortium, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>James Mayfield</author>
<author>Tan Xu</author>
<author>Douglas W Oard</author>
<author>Dawn Lawrie</author>
<author>Tim Oates</author>
<author>Tim Finin</author>
</authors>
<title>A context-aware approach to entity linking.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX ’12,</booktitle>
<pages>62--67</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8894" citStr="Stoyanov et al., 2012" startWordPosition="1330" endWordPosition="1333">es (6%), org:member of (6%), per:countries of residence (2%), per:spouse (2%), and per:member of (1%). We randomly sampled 10 slot-fills of each type, and found accuracy to vary from 20-70%. 3.4 Coreference We used two methods for entity coreference. Under the theory that name ambiguity may not be a huge problem, we adopted a baseline approach of merging entities across different documents if their canonical mentions were an exact string match after some basic normalizations, such as removing punctuation and conversion to lower-case characters. However we also used the JHU HLTCOE CALE system (Stoyanov et al., 2012), which maps named-entity mentions to the TAC-KBP reference KB, which was derived from a 2008 snapshot of English Wikipedia. For entities that are not found in the KB, we reverted to exact string match. CALE entity linking proved to be the more effective approach for the Cold Start task. 3.5 Timex2 Normalization SERIF recognizes, but does not normalize, temporal expressions, so we used the Stanford SUTime package, to normalize date values. 34 Figure 2: Supporting text for some assertions about Mr. Scarborough. Source documents are also viewable by following hyperlinks. 3.6 Lightweight Inferenc</context>
</contexts>
<marker>Stoyanov, Mayfield, Xu, Oard, Lawrie, Oates, Finin, 2012</marker>
<rawString>Veselin Stoyanov, James Mayfield, Tan Xu, Douglas W. Oard, Dawn Lawrie, Tim Oates, and Tim Finin. 2012. A context-aware approach to entity linking. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX ’12, pages 62–67, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>World Wide Web Consortium</author>
</authors>
<title>Resource Description Framework Specification. ”http://http:// www.w3.org/RDF/.</title>
<date>2013</date>
<journal>Online; accessed</journal>
<volume>8</volume>
<marker>Consortium, 2013</marker>
<rawString>World Wide Web Consortium. 2013. Resource Description Framework Specification. ”http://http:// www.w3.org/RDF/. ”[Online; accessed 8 April, 2013]”.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>