<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014704">
<title confidence="0.998182">
MemeTube: A Sentiment-based Audiovisual System
for Analyzing and Displaying Microblog Messages
</title>
<author confidence="0.993261">
Cheng-Te Li1 Chien-Yuan Wang2 Chien-Lin Tseng2 Shou-De Lin1,2
</author>
<affiliation confidence="0.999047">
1 Graduate Institute of Networking and Multimedia
2 Department of Computer Science and Information Engineering
National Taiwan University, Taipei, Taiwan
</affiliation>
<email confidence="0.972391">
{d98944005, sdlin}@csie.ntu.edu.tw {gagedark, moonspirit.wcy}@gmail.com
</email>
<sectionHeader confidence="0.997068" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999468142857143">
Micro-blogging services provide platforms
for users to share their feelings and ideas
on the move. In this paper, we present a
search-based demonstration system, called
MemeTube, to summarize the sentiments
of microblog messages in an audiovisual
manner. MemeTube provides three main
functions: (1) recognizing the sentiments of
messages (2) generating music melody au-
tomatically based on detected sentiments,
and (3) produce an animation of real-time
piano playing for audiovisual display. Our
MemeTube system can be accessed via:
http://mslab.csie.ntu.edu.tw/memetube/ .
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994">
Micro-blogging services such as Twitter1, Plurk2,
and Jaiku3, are platforms that allow users to share
immediate but short messages with friends. Gener-
ally, the micro-blogging services possess some
signature properties that differentiate them from
conventional weblogs and forum. First, microblogs
deal with almost real-time messaging, including
instant information, expression of feelings, and
immediate ideas. It also provides a source of crowd
intelligence that can be used to investigate com-
mon feelings or potential trends about certain news
or concepts. However, this real-time property can
lead to the production of an enormous number of
messages that recipients must digest. Second, mi-
cro-blogging is time-traceable. The temporal in-
formation is crucial because contextual posts that
appear close together are, to some extent, correlat-
ed. Third, the style of micro-blogging posts tends
to be conversation-based with a sequence of re-
</bodyText>
<footnote confidence="0.999177666666667">
1 http://www.twitter.com
2 http://www.plurk.com
3 http://www.jaiku.com/
</footnote>
<page confidence="0.995243">
32
</page>
<bodyText confidence="0.999232961538462">
sponses. This phenomenon indicates that the posts
and their responses are highly correlated in many
respects. Fourth, micro-blogging is friendship-
influenced. Posts from a particular user can also be
viewed by his/her friends and might have an im-
pact on them (e.g. the empathy effect) implicitly or
explicitly. Therefore, posts from friends in the
same period may be correlated sentiment-wise as
well as content-wise.
We leverage the above properties to develop an
automatic and intuitive Web application, Me-
meTube, to analyze and display the sentiments be-
hind messages in microblogs. Our system can be
regarded as a sentiment-driven, music-based sum-
marization framework as well as a novel audiovis-
ual presentation of art. MemeTube is designed as a
search-based tool. The system flow is as shown in
Figure 1. Given a query (either a keyword or a user
id), the system first extracts a series of relevant
posts and replies based on keyword matching.
Then sentiment analysis is applied to determine the
sentiment of the posts. Next a piece of music is
composed to reflect the detected sentiments. Final-
ly, the messages and music are fed into the anima-
tion generation model, which displays a piano
keyboard that plays automatically.
</bodyText>
<figureCaption confidence="0.999598">
Figure 1: The system flow of our MemeTube.
</figureCaption>
<bodyText confidence="0.885352636363636">
The contributions of this work can be viewed
from three different perspectives.
 From system perspective of view, we demo a
novel Web-based system, MemeTube, as a kind
of search-based sentiment presentation, musi-
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 32–37,
Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics
calization, and visualization tool for microblog
messages. It can serve as a real-time sentiment
detector or an interactive microblog audio-
visual presentation system.
</bodyText>
<listItem confidence="0.845759">
• Technically, we integrate a language-model-
based classifier approach with a Markov-
transition model to exploit three kinds of in-
formation (i.e., contextual, response, and
friendship information) for sentiment recogni-
tion. We also integrate the sentiment-detection
system with a real-time rule-based harmonic
music and animation generator to display
streams of messages in an audiovisual format.
• Conceptually, our system demonstrates that,
instead of simply using textual tags to express
sentiments, it is possible to exploit audio (i.e.,
music) and visual (i.e., animation) cues to pre-
sent microblog users’ feelings and experiences.
In this respect, the system can also serve as a
Web-based art piece that uses NLP-
technologies to concretize and portray senti-
ments.
</listItem>
<sectionHeader confidence="0.999041" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.995677379310345">
Related works can be divided into two parts: sen-
timent classification in microblogs, and sentiment-
based audiovisual presentation for social media.
For the first part, most of related literatures focus
on exploiting different classification methods to
separate positive and negative sentiments by a va-
riety of textual and linguistics features, as shown in
Table 1. Their accuracy ranges from 60%~85%
depending on different setups. The major differ-
ence between our work and existing approaches is
that our model considers three kinds of additional
information (i.e., contextual, response and friend-
ship information) for sentiment recognition.
In recent years, a number of studies have inves-
tigated integrating emotions and music in certain
media applications. For example, Ishizuka and
Onisawa (2006) generated variations of theme mu-
sic to fit the impressions of story scenes represent-
ed by textual content or pictures. Kaminskas (2009)
aligned music with user-selected points of interests
for recommendation. Li and Shan (2007) produced
painting slideshows with musical accompaniment.
Hua et al. (2004) proposed a Photo2Video system
that allows users to specify incident music that ex-
presses their feelings about the photos. To the best
of our knowledge, MemeTube is the first attempt
to exploit AI techniques to create harmonic audio-
visual experiences and interactive emotion-based
summarization for microblogs.
</bodyText>
<tableCaption confidence="0.9583775">
Table 1: Summary of related works that
detect sentiments in microblogs.
</tableCaption>
<table confidence="0.984170269230769">
Features Methods
Pak and Paroubek statistic counting of Naive Bayes
2010 adjectives
Chen et al. 2008 POS tags, emoticons SVM
Lewin and Pribu- smileys, keywords Maximum Entropy,
la 2009 SVM
Riley 2009 n-grams, smileys, Naive Bayes,
hashtags, replies,
URLs, usernames,
emoticons
Prasad 2010 n-grams Naïve Bayes
Go et al. 2009 usernames, sequential Naive Bayes, Max-
patterns of keywords, imum Entropy,
POS tags, n-grams SVM
Li et al. 2009 several dictionaries Keyword Matching
about different kinds of
keywords
Barbosa and Feng retweets, hashtag, re- SVM
2010 plies, URLs, emoticons,
upper cases
Sun et al. 2010 keyword counting and Naive Bayes, SVM
Chinese dictionaries
Davidov et al. n-grams, word patterns, k-Nearest Neighbor
2010 punctuation information
Bermingham and n-grams and POS tags Binary Classifica-
Smeaton 2010 tion
</table>
<sectionHeader confidence="0.931335" genericHeader="method">
3 Sentiment Analysis of Microblog Posts
</sectionHeader>
<bodyText confidence="0.999984">
First, we develop a classification model as our
basic sentiment recognition mechanism. Given a
training corpus of posts and responses annotated
with sentiment labels, we train an n-gram language
model for each sentiment. Then, we use such mod-
el to calculate the probability that a post expresses
the sentiment s associated with that model:
</bodyText>
<equation confidence="0.66263525">
ܲݎሺ݌|ݏሻ
௠
ൌ ܲݎሺݓଵ, ⋯ , ݓ௠|ݏሻ ෑ ܲݎ൫ݓ௜หݓ௜ିሺ௡ିଵሻ, ⋯ , ݓ௜ିଵ, ݏ൯,
௜ୀଵ
</equation>
<bodyText confidence="0.952865875">
where w is the sequence of words in the post. We
also use the common Laplace smoothing method.
For each post p and each sentiment sS, our
classifier calculates the probability that such post
expresses the sentiment ܲݎሺݏ|݌ሻ using Bayes rule:
ܲݎሺݏ|݌ሻ ൌ ܲݎሺݏሻܲݎሺ݌|ݏሻ
ܲݎሺݏሻ is estimated directly by counting, while
ܲݎሺ݌|ݏሻ can be derived by using the learned lan-
</bodyText>
<page confidence="0.994821">
33
</page>
<bodyText confidence="0.9999879">
guage models. This allow us to produce a distribu-
tion of sentiments for a given post p, denoted as ܵ௣.
However, the major challenge in the microblog
sentiment detection task is that the length of each
post is limited (i.e., posts on Twitter are limited to
140 characters). Consequently, there might not be
enough information for a sentiment detection sys-
tem to exploit. To solve this problem, we propose
to utilize the three types of information mentioned
earlier. We discuss each type in detail below.
</bodyText>
<subsectionHeader confidence="0.998855">
3.1 Response Factor
</subsectionHeader>
<bodyText confidence="0.9916376">
We believe the sentiment of a post is highly corre-
lated with (but not necessary similar to) that of re-
sponses to the post. For example, an angry post
usually triggers angry responses, but a sad post
usually solicits supportive responses. We propose
to learn the correlation patterns of sentiments from
the data and use them to improve the recognition.
To achieve such goal, from the data, we learn
the probability ܲሺܵ݁݊ݐ݅݉݁݊ݐ௣௢௦௧|ܵ݁݊ݐ݅݉݁݊ݐ௥௘௦௣௢௡௦௘ሻ,
which represents the conditional probability of a
post given responses. Then we use such probability
to construct a transition matrix ܯ௥ , where ܯ௥೔ೕ
=ܲሺܵ݁݊ݐ݅݉݁݊ݐ௣௢௦௧ ൌ ݆  |ܵ݁݊ݐ݅݉݁݊ݐ௥௘௦௣௢௡௦௘ ൌ ݅ሻ.
With ܯ௥, we can generate the adjusted sentiment dis-
tribution of the post ܵ′௣ as:
</bodyText>
<equation confidence="0.9700012">
∑ܹ௥೔ܵ௥೔ܯ௥
௞
௜ୀଵ
ܵ′௣ ൌ α ൈ ൅ ሺ1 െ αሻܵ௣ ,
݇
</equation>
<bodyText confidence="0.999996454545454">
where ܵ௣ denotes the original sentiment distribu-
tion of the post, and ܵ௥೔ is the sentiment distribu-
tion of the ݅௧௛ response determined by the
abovementioned language model approach. In ad-
dition, ܹ௥೔ ൌ 1⁄ሺݐ௥௘௦௣௢௡௦௘೔ െ ݐ௣௢௦௧ሻ represents the
weight of the response since it is preferable to as-
sign higher weights to closer responses. There is
also a global parameter a that determines how
much the system should trust the information de-
rived from the responses to the post. If there is no
response to a post, we simply assign ܵ′௣ ൌ ܵ௣.
</bodyText>
<subsectionHeader confidence="0.999492">
3.2 Context Factor
</subsectionHeader>
<bodyText confidence="0.9994592">
It is assumed that the sentiment of a microblog
post is correlated with the author’s previous posts
(i.e., the ‘context’ of the post). We also assume
that, for each person, there is a sentiment transition
matrix ܯ௖ that represents how his/her sentiments
change over time. The ሺ݅, ݆ሻ௧௛ element in ܯ௖ repre-
sents the conditional probability from the senti-
ment of the previous post to that of the current post:
ܲሺܵ݁݊ݐ݅݉݁݊ݐሺܲ௧ሻ ൌ ݆  |ܵ݁݊ݐ݅݉݁݊ݐሺܲ௧ିଵሻ ൌ ݅ሻ.
The diagonal elements stand for the consistency
of the emotion state of a person. Conceivably, a
capricious person’s diagnostic ܯ௖೔೔ values will be
lower than those of a calm person. The matrix ܯ௖
can be learned directly from the annotated data.
Let ܵ௧ represent the detected sentiment distribu-
tion of an existing post at time t. We want to adjust
ܵ௧ based on the previous posts from ݐ െ 4ݐ to ݐ,
where 4ݐ is a given temporal threshold. The sys-
tem first extracts a set of posts from the same au-
thor posted from time ݐ െ 4ݐ to ݐ and determines
their sentiment distributions ሼܵ௧భ, ܵ௧మ, ... ,ܵ௧ೖሽ ,
where ݐ െ 4ݐ ൏ ݐଵ, ݐଶ, ... , ݐ௞ ൏ ݐ using the same
classifier. Then, the system utilizes the following
update equation to obtain an adjusted sentiment
distribution ܵ′௧:
</bodyText>
<equation confidence="0.9720152">
∑ ܹ௧౟ܵ௧౟ܯ௖
௞
௜ୀଵ
ܵ′௧ ൌ α ൈ ൅ ሺ1 െ αሻܵ௧ ,
݇
</equation>
<bodyText confidence="0.99970675">
where ܹ௧೔ ൌ 1/ሺݐ െ ݐ௜ሻ. The parameters ܹ௧೔, ݇, a
are defined similar to the previous case. If there is
no post in the defined interval, the system will
leave ܵ௧ unchanged.
</bodyText>
<subsectionHeader confidence="0.997865">
3.3 Friendship Factor
</subsectionHeader>
<bodyText confidence="0.999920615384615">
We also assume that the friends’ emotions are cor-
related with each other. This is because friends
affect each other, and they are more likely to be in
the same circumstances, and thus enjoy/suffer sim-
ilarly. Our hypothesis is that the sentiment of a
post and the sentiments of the author’s friends’
recent posts might be correlated. Therefore, we can
treat the friends’ recent posts in the same way as
the recent posts of the author, and learn the transi-
tion matrixܯ௙ , where ܯ௙೔ೕ ൌ ܲሺܵ݁݊ݐ݅݉݁݊ݐݑݏ݁ݎሺܲ௧ሻ ൌ
݆  |ܵ݁݊ݐ݅݉݁݊ݐݑݏ݁ݎ′ݏ ݂ݎ݅݁݊݀ሺܲ௧ିଵሻ ൌ ݅ሻ, and apply the tech-
nique proposed in the previous section to improve
the recognition accuracy.
However, it is not necessarily true that all
friends have similar emotional patterns. One’s sen-
timent transition matrix ܯ௖ might be very different
from that of the other, so we need to be careful
when using such information to adjust our recogni-
tion outcomes. We propose to only consider posts
from friends with similar emotional patterns.
To achieve our goal, we first learn every user’s
contextual sentiment transition matrix ܯ௖ from the
data. In ܯ௖, each row represents a distribution that
sums to one; therefore, we can compare two ma-
trixes ܯ௖భ and ܯ௖మ by averaging the symmetric
KL-divergence of each row. That is,
</bodyText>
<page confidence="0.954316">
34
</page>
<equation confidence="0.857599">
Similarity(Ml, MZ)
=
Average!��
� KL(Row(Ml, i), Row(MZ, i)).
</equation>
<bodyText confidence="0.999812375">
Two persons are considered as having similar
emotion pattern if their contextual sentiment transi-
tion matrixes are similar. After a set of similar
friends are identified, their recent posts (i.e., from
t — t to t) are treated in the same way as the
posts by the author, and we use the method pro-
posed previously to fine-tune the recogni-
tion outcomes.
</bodyText>
<sectionHeader confidence="0.989696" genericHeader="method">
4 Music Generation
</sectionHeader>
<bodyText confidence="0.999941346153846">
For each microblog post retrieved according to the
query, we can derive its sentiment distribution (as
a vector of probabilities) by using the above meth-
od. Next, the system transforms every sentiment
distribution into an affective vector comprised of a
valence value and an arousal value. The valence
value represents the positive-to-negative sentiment,
while the arousal value represents the intense-to-
silent level.
We exploit the mapping from each type of sen-
timent to a two-dimensional affective vector based
on the two-dimensional emotion model of Russell
(1980). Using the model we extract the affective
score vectors of the six emotions (see Table 2)
used in our experiments. The mapping enables us
to transform a sentiment distribution Sp into an
affective score vector by weighted sum approach.
For example, given a distribution of (Anger=20%,
Surprise=20%,Disgust=10%, Fear=10%, Joy=10%,
Sadness=30%), the two-dimensional affective vec-
tor can be computed as 0.2*(-0.25, 1) + 0.2*(0.5,
0.75) + 0.1*(-0.75, -0.5) + 0.1*(-0.75, 0.5) +
0.1*(1, 0.25) + 0.3*(-1, -0.25). Finally, the affec-
tive vector of each post will be summed to repre-
sent the sentiment of the given query in terms of
the valence and arousal values.
</bodyText>
<tableCaption confidence="0.998607">
Table 2: Affective score vector for each sentiment label.
</tableCaption>
<table confidence="0.981514142857143">
Sentiment Label Affective Score Vector
Anger (-0.25, 1)
Surprise (0.5, 0.75)
Disgust (-0.75, -0.5)
Fear (-0.75, 0.5)
Joy (1, 0.25)
Sadness (-1, -0.25)
</table>
<bodyText confidence="0.999514">
Next the system transforms the affective vector
into music elements through chord set selection
(based on the valence value) and rhythm determi-
nation (based on the arousal value). For chord set
selection, we design nine basic chord sets as {A,
Am, Bm, C, D, Dm, Em, F, G}, where each chord
set consists of some basic notes. The chord sets are
used to compose twenty chord sequences. Half of
the chord sequences are used for weakly positive to
strongly positive sentiments and the other half are
used for weakly negative to strongly negative sen-
timents. The valence value is therefore divided into
twenty levels, and gradually shifts from strongly
positive to strongly negative. The chord sets ensure
that the resulting auditory presentation is in har-
mony (Hewitt 2008). For rhythm determination,
we divide the arousal values into five levels to de-
cide the tempo/speed of the music. Higher arousal
values generate music with a faster tempo while
lower ones lead to slow and easy-listening music.
</bodyText>
<figureCaption confidence="0.999633">
Figure 2: A snapshot of the proposed MemeTube.
Figure 3: The animation with automatic piano playing.
</figureCaption>
<sectionHeader confidence="0.809598" genericHeader="method">
5 Animation Generation
</sectionHeader>
<bodyText confidence="0.999987">
In this final stage, our system produces real-time
animation for visualization. The streams of mes-
sages are designed to flow as if they were playing a
piece of a piano melody. We associate each mes-
sage with a note in the generated music. When a
post message flows from right to left and touches a
piano key, the key itself blinks once and the corre-
sponding tone of the key is produced. The message
flow and the chord/rhythm have to be synchro-
nized so that it looks as if the messages themselves
are playing the piano. The system also allows users
to highlight the body of a message by moving the
</bodyText>
<page confidence="0.9983">
35
</page>
<bodyText confidence="0.999969666666667">
cursor over the flowing message. A snapshot is
shown in Figure 2 and the sequential snapshots of
the animation are shown in Figure 3.
</bodyText>
<sectionHeader confidence="0.967972" genericHeader="method">
6 Evaluations on Sentiment Detection
</sectionHeader>
<bodyText confidence="0.99992648">
We collect the posts and responses from every ef-
fective user, users with more than 10 messages, of
Plurk from January 31st to May 23rd, 2009. In order
to create the diversity for the music generation sys-
tem, we decide to use six different sentiments, as
shown in Table 2, rather than using only three sen-
timent types, positive, negative and neutral, as
most of the systems in Table 1 have used. The sen-
timent of each sentence is labeled automatically
using the emoticons. This is similar to what many
people have proposed for evaluation (Davidov et al.
2010; Sun et al. 2010; Bifet and Frank 2010; Go et
al. 2009; Pak and Paroubek 2010; Chen et al.
2010). We use data from January 31st to April 30th
as training set, May 1st to 23rd as testing data. For
the purpose of observing the result of using the
three factors, we filter the users without friends,
the posts without responses, and the posts without
previous post in 24 hour in testing data. We also
manually label the sentiments on the testing data
(totally 1200 posts, 200 posts for each sentiment).
We use three metrics to evaluate our model: ac-
curacy, Root-Mean-Square Error for valence (de-
noted by RMSE(V)) and RMSE for arousal
(denoted by RMSE(A)). The RMSE values are
generated by comparing the affective vector of the
predicted sentiment distribution with the affective
vector of the answer. Our basic model reaches
33.8% in accuracy, 0.78 in the RMSE(V) and 0.64
in RMSE(A). Note that RMSE0.5 means that
there is roughly one quarter (25%) error in the va-
lence/arousal values as they range from [-1,1].
Note that the main reason the accuracy is not ex-
tremely high is that we are dealing with 6 classes.
When we combine angry, disgust, fear, and sad-
ness into one negative sense and the rest as posi-
tive senses, our system reaches 78.7% in accuracy,
which is competitive to the state-of-the-art algo-
rithms as shown in the related work section. How-
ever, doing such generalization will lose the
flexibility of producing more fruitful and diverse
pieces of music. Therefore we choose more fine-
grained classes for our experiment.
Figure 3 shows the results of exploiting the re-
sponse, context, and friendship. Note RMSE0.5
means that there is roughly one quarter (25%) error
in the valence/arousal values as they range from [-
1,1]. The results show that considering all three
additional factors can achieve the best results and
decent improvement over the basic LM model.
</bodyText>
<tableCaption confidence="0.941145">
Table 3: The results after adding addition info
note that for RMSE, the lower value the better
</tableCaption>
<table confidence="0.9973955">
LM Response Context Friend Combine
Accuracy 33.8% 34.7% 34.8% 35.1% 36.5%
RMSE(V) 0.784 0.683 0.684 0.703 0.679
RMSE(A) 0.640 0.522 0.516 0.538 0.514
</table>
<sectionHeader confidence="0.994838" genericHeader="method">
7 System Demo
</sectionHeader>
<bodyText confidence="0.9999908">
We create video clips of five different queries for
demonstration, which is downloadable from:
http://mslab.csie.ntu.edu.tw/memetube/demo/. This
demo page contains the resulting clips of four
keyword queries (including football, volcano,
Monday, big bang) and a user id query mstcgeek.
Here we briefly describe each case. (1) The video
for query term, football, was recorded on February
7th 2011, results in a relatively positive and ex-
tremely intense atmosphere. It is reasonable be-
cause the NFL Super Bowl was played on
February 6th, 2011. The valence value is not as
high as the arousal value because some fans might
not be very happy to see their favorite team losing
the game. (2) The query, volcano, was also record-
ed on February 7th 2011. The resulting video ex-
presses negative valence and neutral arousal. After
checking the posts, we have learned that it is be-
cause the Japanese volcano Mount Asama has con-
tinued to erupt. Some users are worried and
discussed about the potential follow-up disasters.
(3) The query Monday was performed on February
6th 2011, which is a Sunday night. The negative
valence reflects the “blue Monday” phenomenon,
which leads to some heavy, less smooth melody. (4)
The term big bang turns out to be very positive on
both valence and arousal, mainly because, besides
its relatively neutral meaning in physics, this term
also refers to a famous comic show that some peo-
ple in Plurk love to watch. We also use one user id
as query: the user-id mstcgeek is the official ac-
count of Microsoft Taiwan. This user often uses
cheery texts to share some videos about their prod-
ucts or provide some discounts of their product,
which leads to relatively hyped music.
</bodyText>
<page confidence="0.997533">
36
</page>
<sectionHeader confidence="0.998705" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99997625">
Microblog, as a daily journey and social network-
ing service, generally captures the dynamics of the
change of feelings over time of the authors and
their friends. In MemeTube, the affective vector is
generated by aggregating the sentiment distribution
of each post; thus, it represents the majority’s opin-
ion (or sentiment) about a topic. In this sense, our
system can be regarded as providing users with an
audiovisual experience to learn collective opinion
of a particular topic. It also shows how NLP tech-
niques can be integrated with knowledge about
music and visualization to create a piece of inter-
esting network art work. Note that MemeTube can
be regarded as a flexible framework as well since
each component can be further refined inde-
pendently. Therefore, our future works are three-
fold: For sentiment analysis, we will consider more
sophisticated ways to improve the baseline accura-
cy and to aggregate individual posts into a collec-
tive consensus. For music generation, we plan to
add more instruments and exploit learning ap-
proaches to improve the selection of chords. For
visualization, we plan to add more interactions be-
tween music, sentiments, and users.
</bodyText>
<sectionHeader confidence="0.997163" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.907937333333333">
This work was supported by National Science Council, Na-
tional Taiwan University and Intel Corporation under Grants
NSC99-2911-I-002-001, 99R70600, and 10R80800.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928070422535">
Barbosa, L., and Feng, J. 2010. Robust Sentiment Detec-
tion on Twitter from Biased and Noisy Data. In Pro-
ceedings of International Conference on Computational
Linguistics (COLING’10), 36–44.
Bermingham, A., and Smeaton, A. F. 2010. Classifying
Sentiment in Microblogs: is Brevity an Advantage? In
Proceedings of ACM International Conference on In-
formation and Knowledge Management (CIKM’10),
1183–1186.
Chen, M. Y.; Lin, H. N.; Shih, C. A.; Hsu, Y. C.; Hsu, P.
Y.; and Hewitt, M. 2008. Music Theory for Computer
Musicians. Delmar.
Hsieh, S. K. 2010. Classifying Mood in Plurks. In Proceed-
ings of Conference on Computational Linguistics and
Speech Processing (ROCLING 2010), 172–183.
Davidov, D.; Tsur, O.; and Rappoport, A. 2010. Enhanced
Sentiment Learning Using Twitter Hashtags and Smi-
leys. In Proceedings of International Conference on
Computational Linguistics (COLING’10), 241–249.
Go, A.; Bhayani, R.; and Huang, L. 2009. Twitter Senti-
ment ClassiÞcation using Distant Supervision. Technical
Report, Stanford University.
Hua, X. S.; Lu, L.; and Zhang, H. J. 2004. Photo2Video -
A System for Automatically Converting Photographic
Series into Video. In Proceedings of ACM International
Conference on Multimedia (MM’04), 708–715.
Ishizuka, K., and Onisawa, T. 2006. Generation of Varia-
tions on Theme Music Based on Impressions of Story
Scenes. In Proceedings of ACM International Confer-
ence on Game Research and Development, 129–136.
Kaminskas, M. 2009. Matching Information Content with
Music. In Proceedings of ACM International Confer-
ence on Recommendation System (RecSys’09), 405–
408.
Lewin, J. S., and Pribula, A. 2009. Extracting Emotion
from Twitter. Technical Report, Stanford University.
Li, C. T., and Shan, M. K. 2007. Emotion-based Impres-
sionism Slideshow with Automatic Music Accompani-
ment. In Proceedings of ACM International Conference
on Multimedia (MM’07), 839–842.
Li, S.; Zheng, L.; Ren, X.; and Cheng, X. 2009. Emotion
Mining Research on Micro-blog. In Proceedings of
IEEE Symposium on Web Society, 71–75.
Pak, A., and Paroubek, P. 2010. Twitter Based System:
Using Twitter for Disambiguating Sentiment Ambigu-
ous Adjectives. In Proceedings of International Work-
shop on Semantic Evaluation, (ACL’10), 436–439.
Pak, A., and Paroubek, P. 2010. Twitter as a Corpus for
Sentiment Analysis and Opinion Mining. In Proceedings
of International Conference on Language Resources and
Evaluation (LREC’10), 1320–1326.
Prasad, S. 2010. Micro-blogging Sentiment Analysis Using
Bayesian Classification Methods. Technical Report,
Stanford University.
Riley, C. 2009. Emotional Classification of Twitter Mes-
sages. Technical Report, UC Berkeley.
Russell, J. A. 1980. Circumplex Model of Affect. Journal
of Personality and Social Psychology, 39(6):1161–1178.
Strapparava, C., and Valitutti, A. 2004. Wordnet-affect: an
Affective extension of wordnet. In Proceedings of Inter-
national Conference on Language Resources and Evalu-
ation, 1083–1086.
Sun, Y. T.; Chen, C. L.; Liu, C. C.; Liu, C. L.; and Soo, V.
W. 2010. Sentiment Classification of Short Chinese
Sentences. In Proceedings of Conference on Computa-
tional Linguistics and Speech Processing
(ROCLING’10), 184–198.
Yang, C.; Lin, K. H. Y.; and Chen, H. H. 2007. Emotion
Classification Using Web Blog Corpora. In Proceedings
of IEEE/WIC/ACM International Conference on Web
Intelligence (WI’07), 275–278.
</reference>
<page confidence="0.999611">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.581374">
<title confidence="0.9994315">MemeTube: A Sentiment-based Audiovisual for Analyzing and Displaying Microblog Messages</title>
<author confidence="0.983342">Chien-Yuan Chien-Lin Shou-De</author>
<affiliation confidence="0.998241666666667">1Graduate Institute of Networking and 2Department of Computer Science and Information National Taiwan University, Taipei,</affiliation>
<email confidence="0.969437">d98944005@gmail.com</email>
<email confidence="0.969437">sdlin}@csie.ntu.edu.tw{gagedark@gmail.com</email>
<email confidence="0.969437">moonspirit.wcy@gmail.com</email>
<abstract confidence="0.995615642857143">Micro-blogging services provide platforms for users to share their feelings and ideas on the move. In this paper, we present a search-based demonstration system, called MemeTube, to summarize the sentiments of microblog messages in an audiovisual manner. MemeTube provides three main functions: (1) recognizing the sentiments of messages (2) generating music melody automatically based on detected sentiments, and (3) produce an animation of real-time piano playing for audiovisual display. Our MemeTube system can be accessed via:</abstract>
<web confidence="0.66202">http://mslab.csie.ntu.edu.tw/memetube/.</web>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Barbosa</author>
<author>J Feng</author>
</authors>
<title>Robust Sentiment Detection on Twitter from Biased and Noisy Data. In</title>
<date>2010</date>
<booktitle>Proceedings of International Conference on Computational Linguistics (COLING’10),</booktitle>
<pages>36--44</pages>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Barbosa, L., and Feng, J. 2010. Robust Sentiment Detection on Twitter from Biased and Noisy Data. In Proceedings of International Conference on Computational Linguistics (COLING’10), 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bermingham</author>
<author>A F Smeaton</author>
</authors>
<title>Classifying Sentiment in Microblogs: is Brevity an Advantage?</title>
<date>2010</date>
<booktitle>In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM’10),</booktitle>
<pages>1183--1186</pages>
<marker>Bermingham, Smeaton, 2010</marker>
<rawString>Bermingham, A., and Smeaton, A. F. 2010. Classifying Sentiment in Microblogs: is Brevity an Advantage? In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM’10), 1183–1186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Y Chen</author>
<author>H N Lin</author>
<author>C A Shih</author>
<author>Y C Hsu</author>
<author>P Y Hsu</author>
<author>M Hewitt</author>
</authors>
<title>Music Theory for Computer Musicians.</title>
<date>2008</date>
<publisher>Delmar.</publisher>
<contexts>
<context position="6165" citStr="Chen et al. 2008" startWordPosition="901" endWordPosition="904">points of interests for recommendation. Li and Shan (2007) produced painting slideshows with musical accompaniment. Hua et al. (2004) proposed a Photo2Video system that allows users to specify incident music that expresses their feelings about the photos. To the best of our knowledge, MemeTube is the first attempt to exploit AI techniques to create harmonic audiovisual experiences and interactive emotion-based summarization for microblogs. Table 1: Summary of related works that detect sentiments in microblogs. Features Methods Pak and Paroubek statistic counting of Naive Bayes 2010 adjectives Chen et al. 2008 POS tags, emoticons SVM Lewin and Pribu- smileys, keywords Maximum Entropy, la 2009 SVM Riley 2009 n-grams, smileys, Naive Bayes, hashtags, replies, URLs, usernames, emoticons Prasad 2010 n-grams Naïve Bayes Go et al. 2009 usernames, sequential Naive Bayes, Maxpatterns of keywords, imum Entropy, POS tags, n-grams SVM Li et al. 2009 several dictionaries Keyword Matching about different kinds of keywords Barbosa and Feng retweets, hashtag, re- SVM 2010 plies, URLs, emoticons, upper cases Sun et al. 2010 keyword counting and Naive Bayes, SVM Chinese dictionaries Davidov et al. n-grams, word patt</context>
</contexts>
<marker>Chen, Lin, Shih, Hsu, Hsu, Hewitt, 2008</marker>
<rawString>Chen, M. Y.; Lin, H. N.; Shih, C. A.; Hsu, Y. C.; Hsu, P. Y.; and Hewitt, M. 2008. Music Theory for Computer Musicians. Delmar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Hsieh</author>
</authors>
<title>Classifying Mood in Plurks.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference on Computational Linguistics and Speech Processing (ROCLING 2010),</booktitle>
<pages>172--183</pages>
<marker>Hsieh, 2010</marker>
<rawString>Hsieh, S. K. 2010. Classifying Mood in Plurks. In Proceedings of Conference on Computational Linguistics and Speech Processing (ROCLING 2010), 172–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>O Tsur</author>
<author>A Rappoport</author>
</authors>
<title>Enhanced Sentiment Learning Using Twitter Hashtags and Smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics (COLING’10),</booktitle>
<pages>241--249</pages>
<contexts>
<context position="16564" citStr="Davidov et al. 2010" startWordPosition="2660" endWordPosition="2663">n are shown in Figure 3. 6 Evaluations on Sentiment Detection We collect the posts and responses from every effective user, users with more than 10 messages, of Plurk from January 31st to May 23rd, 2009. In order to create the diversity for the music generation system, we decide to use six different sentiments, as shown in Table 2, rather than using only three sentiment types, positive, negative and neutral, as most of the systems in Table 1 have used. The sentiment of each sentence is labeled automatically using the emoticons. This is similar to what many people have proposed for evaluation (Davidov et al. 2010; Sun et al. 2010; Bifet and Frank 2010; Go et al. 2009; Pak and Paroubek 2010; Chen et al. 2010). We use data from January 31st to April 30th as training set, May 1st to 23rd as testing data. For the purpose of observing the result of using the three factors, we filter the users without friends, the posts without responses, and the posts without previous post in 24 hour in testing data. We also manually label the sentiments on the testing data (totally 1200 posts, 200 posts for each sentiment). We use three metrics to evaluate our model: accuracy, Root-Mean-Square Error for valence (denoted b</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Davidov, D.; Tsur, O.; and Rappoport, A. 2010. Enhanced Sentiment Learning Using Twitter Hashtags and Smileys. In Proceedings of International Conference on Computational Linguistics (COLING’10), 241–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Go</author>
<author>R Bhayani</author>
<author>L Huang</author>
</authors>
<title>Twitter Sentiment ClassiÞcation using Distant Supervision.</title>
<date>2009</date>
<tech>Technical Report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="6388" citStr="Go et al. 2009" startWordPosition="935" endWordPosition="938">eir feelings about the photos. To the best of our knowledge, MemeTube is the first attempt to exploit AI techniques to create harmonic audiovisual experiences and interactive emotion-based summarization for microblogs. Table 1: Summary of related works that detect sentiments in microblogs. Features Methods Pak and Paroubek statistic counting of Naive Bayes 2010 adjectives Chen et al. 2008 POS tags, emoticons SVM Lewin and Pribu- smileys, keywords Maximum Entropy, la 2009 SVM Riley 2009 n-grams, smileys, Naive Bayes, hashtags, replies, URLs, usernames, emoticons Prasad 2010 n-grams Naïve Bayes Go et al. 2009 usernames, sequential Naive Bayes, Maxpatterns of keywords, imum Entropy, POS tags, n-grams SVM Li et al. 2009 several dictionaries Keyword Matching about different kinds of keywords Barbosa and Feng retweets, hashtag, re- SVM 2010 plies, URLs, emoticons, upper cases Sun et al. 2010 keyword counting and Naive Bayes, SVM Chinese dictionaries Davidov et al. n-grams, word patterns, k-Nearest Neighbor 2010 punctuation information Bermingham and n-grams and POS tags Binary ClassificaSmeaton 2010 tion 3 Sentiment Analysis of Microblog Posts First, we develop a classification model as our basic sent</context>
<context position="16619" citStr="Go et al. 2009" startWordPosition="2672" endWordPosition="2675">n We collect the posts and responses from every effective user, users with more than 10 messages, of Plurk from January 31st to May 23rd, 2009. In order to create the diversity for the music generation system, we decide to use six different sentiments, as shown in Table 2, rather than using only three sentiment types, positive, negative and neutral, as most of the systems in Table 1 have used. The sentiment of each sentence is labeled automatically using the emoticons. This is similar to what many people have proposed for evaluation (Davidov et al. 2010; Sun et al. 2010; Bifet and Frank 2010; Go et al. 2009; Pak and Paroubek 2010; Chen et al. 2010). We use data from January 31st to April 30th as training set, May 1st to 23rd as testing data. For the purpose of observing the result of using the three factors, we filter the users without friends, the posts without responses, and the posts without previous post in 24 hour in testing data. We also manually label the sentiments on the testing data (totally 1200 posts, 200 posts for each sentiment). We use three metrics to evaluate our model: accuracy, Root-Mean-Square Error for valence (denoted by RMSE(V)) and RMSE for arousal (denoted by RMSE(A)). T</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Go, A.; Bhayani, R.; and Huang, L. 2009. Twitter Sentiment ClassiÞcation using Distant Supervision. Technical Report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X S Hua</author>
<author>L Lu</author>
<author>H J Zhang</author>
</authors>
<title>Photo2Video -A System for Automatically Converting Photographic Series into Video.</title>
<date>2004</date>
<booktitle>In Proceedings of ACM International Conference on Multimedia (MM’04),</booktitle>
<pages>708--715</pages>
<contexts>
<context position="5682" citStr="Hua et al. (2004)" startWordPosition="828" endWordPosition="831">roaches is that our model considers three kinds of additional information (i.e., contextual, response and friendship information) for sentiment recognition. In recent years, a number of studies have investigated integrating emotions and music in certain media applications. For example, Ishizuka and Onisawa (2006) generated variations of theme music to fit the impressions of story scenes represented by textual content or pictures. Kaminskas (2009) aligned music with user-selected points of interests for recommendation. Li and Shan (2007) produced painting slideshows with musical accompaniment. Hua et al. (2004) proposed a Photo2Video system that allows users to specify incident music that expresses their feelings about the photos. To the best of our knowledge, MemeTube is the first attempt to exploit AI techniques to create harmonic audiovisual experiences and interactive emotion-based summarization for microblogs. Table 1: Summary of related works that detect sentiments in microblogs. Features Methods Pak and Paroubek statistic counting of Naive Bayes 2010 adjectives Chen et al. 2008 POS tags, emoticons SVM Lewin and Pribu- smileys, keywords Maximum Entropy, la 2009 SVM Riley 2009 n-grams, smileys,</context>
</contexts>
<marker>Hua, Lu, Zhang, 2004</marker>
<rawString>Hua, X. S.; Lu, L.; and Zhang, H. J. 2004. Photo2Video -A System for Automatically Converting Photographic Series into Video. In Proceedings of ACM International Conference on Multimedia (MM’04), 708–715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ishizuka</author>
<author>T Onisawa</author>
</authors>
<title>Generation of Variations on Theme Music Based on Impressions of Story Scenes.</title>
<date>2006</date>
<booktitle>In Proceedings of ACM International Conference on Game Research and Development,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="5379" citStr="Ishizuka and Onisawa (2006)" startWordPosition="783" endWordPosition="786"> of related literatures focus on exploiting different classification methods to separate positive and negative sentiments by a variety of textual and linguistics features, as shown in Table 1. Their accuracy ranges from 60%~85% depending on different setups. The major difference between our work and existing approaches is that our model considers three kinds of additional information (i.e., contextual, response and friendship information) for sentiment recognition. In recent years, a number of studies have investigated integrating emotions and music in certain media applications. For example, Ishizuka and Onisawa (2006) generated variations of theme music to fit the impressions of story scenes represented by textual content or pictures. Kaminskas (2009) aligned music with user-selected points of interests for recommendation. Li and Shan (2007) produced painting slideshows with musical accompaniment. Hua et al. (2004) proposed a Photo2Video system that allows users to specify incident music that expresses their feelings about the photos. To the best of our knowledge, MemeTube is the first attempt to exploit AI techniques to create harmonic audiovisual experiences and interactive emotion-based summarization fo</context>
</contexts>
<marker>Ishizuka, Onisawa, 2006</marker>
<rawString>Ishizuka, K., and Onisawa, T. 2006. Generation of Variations on Theme Music Based on Impressions of Story Scenes. In Proceedings of ACM International Conference on Game Research and Development, 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaminskas</author>
</authors>
<title>Matching Information Content with Music.</title>
<date>2009</date>
<booktitle>In Proceedings of ACM International Conference on Recommendation System (RecSys’09),</booktitle>
<pages>405--408</pages>
<contexts>
<context position="5515" citStr="Kaminskas (2009)" startWordPosition="807" endWordPosition="808">nd linguistics features, as shown in Table 1. Their accuracy ranges from 60%~85% depending on different setups. The major difference between our work and existing approaches is that our model considers three kinds of additional information (i.e., contextual, response and friendship information) for sentiment recognition. In recent years, a number of studies have investigated integrating emotions and music in certain media applications. For example, Ishizuka and Onisawa (2006) generated variations of theme music to fit the impressions of story scenes represented by textual content or pictures. Kaminskas (2009) aligned music with user-selected points of interests for recommendation. Li and Shan (2007) produced painting slideshows with musical accompaniment. Hua et al. (2004) proposed a Photo2Video system that allows users to specify incident music that expresses their feelings about the photos. To the best of our knowledge, MemeTube is the first attempt to exploit AI techniques to create harmonic audiovisual experiences and interactive emotion-based summarization for microblogs. Table 1: Summary of related works that detect sentiments in microblogs. Features Methods Pak and Paroubek statistic counti</context>
</contexts>
<marker>Kaminskas, 2009</marker>
<rawString>Kaminskas, M. 2009. Matching Information Content with Music. In Proceedings of ACM International Conference on Recommendation System (RecSys’09), 405– 408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Lewin</author>
<author>A Pribula</author>
</authors>
<title>Extracting Emotion from Twitter.</title>
<date>2009</date>
<tech>Technical Report,</tech>
<institution>Stanford University.</institution>
<marker>Lewin, Pribula, 2009</marker>
<rawString>Lewin, J. S., and Pribula, A. 2009. Extracting Emotion from Twitter. Technical Report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Li</author>
<author>M K Shan</author>
</authors>
<title>Emotion-based Impressionism Slideshow with Automatic Music Accompaniment.</title>
<date>2007</date>
<booktitle>In Proceedings of ACM International Conference on Multimedia (MM’07),</booktitle>
<pages>839--842</pages>
<contexts>
<context position="5607" citStr="Li and Shan (2007)" startWordPosition="818" endWordPosition="821"> on different setups. The major difference between our work and existing approaches is that our model considers three kinds of additional information (i.e., contextual, response and friendship information) for sentiment recognition. In recent years, a number of studies have investigated integrating emotions and music in certain media applications. For example, Ishizuka and Onisawa (2006) generated variations of theme music to fit the impressions of story scenes represented by textual content or pictures. Kaminskas (2009) aligned music with user-selected points of interests for recommendation. Li and Shan (2007) produced painting slideshows with musical accompaniment. Hua et al. (2004) proposed a Photo2Video system that allows users to specify incident music that expresses their feelings about the photos. To the best of our knowledge, MemeTube is the first attempt to exploit AI techniques to create harmonic audiovisual experiences and interactive emotion-based summarization for microblogs. Table 1: Summary of related works that detect sentiments in microblogs. Features Methods Pak and Paroubek statistic counting of Naive Bayes 2010 adjectives Chen et al. 2008 POS tags, emoticons SVM Lewin and Pribu- </context>
</contexts>
<marker>Li, Shan, 2007</marker>
<rawString>Li, C. T., and Shan, M. K. 2007. Emotion-based Impressionism Slideshow with Automatic Music Accompaniment. In Proceedings of ACM International Conference on Multimedia (MM’07), 839–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>L Zheng</author>
<author>X Ren</author>
<author>X Cheng</author>
</authors>
<title>Emotion Mining Research on Micro-blog.</title>
<date>2009</date>
<booktitle>In Proceedings of IEEE Symposium on Web Society,</booktitle>
<pages>71--75</pages>
<contexts>
<context position="6499" citStr="Li et al. 2009" startWordPosition="953" endWordPosition="956">ques to create harmonic audiovisual experiences and interactive emotion-based summarization for microblogs. Table 1: Summary of related works that detect sentiments in microblogs. Features Methods Pak and Paroubek statistic counting of Naive Bayes 2010 adjectives Chen et al. 2008 POS tags, emoticons SVM Lewin and Pribu- smileys, keywords Maximum Entropy, la 2009 SVM Riley 2009 n-grams, smileys, Naive Bayes, hashtags, replies, URLs, usernames, emoticons Prasad 2010 n-grams Naïve Bayes Go et al. 2009 usernames, sequential Naive Bayes, Maxpatterns of keywords, imum Entropy, POS tags, n-grams SVM Li et al. 2009 several dictionaries Keyword Matching about different kinds of keywords Barbosa and Feng retweets, hashtag, re- SVM 2010 plies, URLs, emoticons, upper cases Sun et al. 2010 keyword counting and Naive Bayes, SVM Chinese dictionaries Davidov et al. n-grams, word patterns, k-Nearest Neighbor 2010 punctuation information Bermingham and n-grams and POS tags Binary ClassificaSmeaton 2010 tion 3 Sentiment Analysis of Microblog Posts First, we develop a classification model as our basic sentiment recognition mechanism. Given a training corpus of posts and responses annotated with sentiment labels, we</context>
</contexts>
<marker>Li, Zheng, Ren, Cheng, 2009</marker>
<rawString>Li, S.; Zheng, L.; Ren, X.; and Cheng, X. 2009. Emotion Mining Research on Micro-blog. In Proceedings of IEEE Symposium on Web Society, 71–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pak</author>
<author>P Paroubek</author>
</authors>
<title>Twitter Based System: Using Twitter for Disambiguating Sentiment Ambiguous Adjectives.</title>
<date>2010</date>
<booktitle>In Proceedings of International Workshop on Semantic Evaluation, (ACL’10),</booktitle>
<pages>436--439</pages>
<contexts>
<context position="16642" citStr="Pak and Paroubek 2010" startWordPosition="2676" endWordPosition="2679"> posts and responses from every effective user, users with more than 10 messages, of Plurk from January 31st to May 23rd, 2009. In order to create the diversity for the music generation system, we decide to use six different sentiments, as shown in Table 2, rather than using only three sentiment types, positive, negative and neutral, as most of the systems in Table 1 have used. The sentiment of each sentence is labeled automatically using the emoticons. This is similar to what many people have proposed for evaluation (Davidov et al. 2010; Sun et al. 2010; Bifet and Frank 2010; Go et al. 2009; Pak and Paroubek 2010; Chen et al. 2010). We use data from January 31st to April 30th as training set, May 1st to 23rd as testing data. For the purpose of observing the result of using the three factors, we filter the users without friends, the posts without responses, and the posts without previous post in 24 hour in testing data. We also manually label the sentiments on the testing data (totally 1200 posts, 200 posts for each sentiment). We use three metrics to evaluate our model: accuracy, Root-Mean-Square Error for valence (denoted by RMSE(V)) and RMSE for arousal (denoted by RMSE(A)). The RMSE values are gene</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Pak, A., and Paroubek, P. 2010. Twitter Based System: Using Twitter for Disambiguating Sentiment Ambiguous Adjectives. In Proceedings of International Workshop on Semantic Evaluation, (ACL’10), 436–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pak</author>
<author>P Paroubek</author>
</authors>
<title>Twitter as a Corpus for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<pages>1320--1326</pages>
<contexts>
<context position="16642" citStr="Pak and Paroubek 2010" startWordPosition="2676" endWordPosition="2679"> posts and responses from every effective user, users with more than 10 messages, of Plurk from January 31st to May 23rd, 2009. In order to create the diversity for the music generation system, we decide to use six different sentiments, as shown in Table 2, rather than using only three sentiment types, positive, negative and neutral, as most of the systems in Table 1 have used. The sentiment of each sentence is labeled automatically using the emoticons. This is similar to what many people have proposed for evaluation (Davidov et al. 2010; Sun et al. 2010; Bifet and Frank 2010; Go et al. 2009; Pak and Paroubek 2010; Chen et al. 2010). We use data from January 31st to April 30th as training set, May 1st to 23rd as testing data. For the purpose of observing the result of using the three factors, we filter the users without friends, the posts without responses, and the posts without previous post in 24 hour in testing data. We also manually label the sentiments on the testing data (totally 1200 posts, 200 posts for each sentiment). We use three metrics to evaluate our model: accuracy, Root-Mean-Square Error for valence (denoted by RMSE(V)) and RMSE for arousal (denoted by RMSE(A)). The RMSE values are gene</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Pak, A., and Paroubek, P. 2010. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. In Proceedings of International Conference on Language Resources and Evaluation (LREC’10), 1320–1326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Prasad</author>
</authors>
<title>Micro-blogging Sentiment Analysis Using Bayesian Classification Methods.</title>
<date>2010</date>
<tech>Technical Report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="6353" citStr="Prasad 2010" startWordPosition="930" endWordPosition="931">incident music that expresses their feelings about the photos. To the best of our knowledge, MemeTube is the first attempt to exploit AI techniques to create harmonic audiovisual experiences and interactive emotion-based summarization for microblogs. Table 1: Summary of related works that detect sentiments in microblogs. Features Methods Pak and Paroubek statistic counting of Naive Bayes 2010 adjectives Chen et al. 2008 POS tags, emoticons SVM Lewin and Pribu- smileys, keywords Maximum Entropy, la 2009 SVM Riley 2009 n-grams, smileys, Naive Bayes, hashtags, replies, URLs, usernames, emoticons Prasad 2010 n-grams Naïve Bayes Go et al. 2009 usernames, sequential Naive Bayes, Maxpatterns of keywords, imum Entropy, POS tags, n-grams SVM Li et al. 2009 several dictionaries Keyword Matching about different kinds of keywords Barbosa and Feng retweets, hashtag, re- SVM 2010 plies, URLs, emoticons, upper cases Sun et al. 2010 keyword counting and Naive Bayes, SVM Chinese dictionaries Davidov et al. n-grams, word patterns, k-Nearest Neighbor 2010 punctuation information Bermingham and n-grams and POS tags Binary ClassificaSmeaton 2010 tion 3 Sentiment Analysis of Microblog Posts First, we develop a cla</context>
</contexts>
<marker>Prasad, 2010</marker>
<rawString>Prasad, S. 2010. Micro-blogging Sentiment Analysis Using Bayesian Classification Methods. Technical Report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Riley</author>
</authors>
<title>Emotional Classification of Twitter Messages.</title>
<date>2009</date>
<tech>Technical Report,</tech>
<institution>UC Berkeley.</institution>
<contexts>
<context position="6264" citStr="Riley 2009" startWordPosition="919" endWordPosition="920">mpaniment. Hua et al. (2004) proposed a Photo2Video system that allows users to specify incident music that expresses their feelings about the photos. To the best of our knowledge, MemeTube is the first attempt to exploit AI techniques to create harmonic audiovisual experiences and interactive emotion-based summarization for microblogs. Table 1: Summary of related works that detect sentiments in microblogs. Features Methods Pak and Paroubek statistic counting of Naive Bayes 2010 adjectives Chen et al. 2008 POS tags, emoticons SVM Lewin and Pribu- smileys, keywords Maximum Entropy, la 2009 SVM Riley 2009 n-grams, smileys, Naive Bayes, hashtags, replies, URLs, usernames, emoticons Prasad 2010 n-grams Naïve Bayes Go et al. 2009 usernames, sequential Naive Bayes, Maxpatterns of keywords, imum Entropy, POS tags, n-grams SVM Li et al. 2009 several dictionaries Keyword Matching about different kinds of keywords Barbosa and Feng retweets, hashtag, re- SVM 2010 plies, URLs, emoticons, upper cases Sun et al. 2010 keyword counting and Naive Bayes, SVM Chinese dictionaries Davidov et al. n-grams, word patterns, k-Nearest Neighbor 2010 punctuation information Bermingham and n-grams and POS tags Binary Cl</context>
</contexts>
<marker>Riley, 2009</marker>
<rawString>Riley, C. 2009. Emotional Classification of Twitter Messages. Technical Report, UC Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Russell</author>
</authors>
<title>Circumplex Model of Affect.</title>
<date>1980</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>39</volume>
<issue>6</issue>
<contexts>
<context position="13253" citStr="Russell (1980)" startWordPosition="2102" endWordPosition="2103">e the recognition outcomes. 4 Music Generation For each microblog post retrieved according to the query, we can derive its sentiment distribution (as a vector of probabilities) by using the above method. Next, the system transforms every sentiment distribution into an affective vector comprised of a valence value and an arousal value. The valence value represents the positive-to-negative sentiment, while the arousal value represents the intense-tosilent level. We exploit the mapping from each type of sentiment to a two-dimensional affective vector based on the two-dimensional emotion model of Russell (1980). Using the model we extract the affective score vectors of the six emotions (see Table 2) used in our experiments. The mapping enables us to transform a sentiment distribution Sp into an affective score vector by weighted sum approach. For example, given a distribution of (Anger=20%, Surprise=20%,Disgust=10%, Fear=10%, Joy=10%, Sadness=30%), the two-dimensional affective vector can be computed as 0.2*(-0.25, 1) + 0.2*(0.5, 0.75) + 0.1*(-0.75, -0.5) + 0.1*(-0.75, 0.5) + 0.1*(1, 0.25) + 0.3*(-1, -0.25). Finally, the affective vector of each post will be summed to represent the sentiment of the </context>
</contexts>
<marker>Russell, 1980</marker>
<rawString>Russell, J. A. 1980. Circumplex Model of Affect. Journal of Personality and Social Psychology, 39(6):1161–1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Valitutti</author>
</authors>
<title>Wordnet-affect: an Affective extension of wordnet.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Language Resources and Evaluation,</booktitle>
<pages>1083--1086</pages>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>Strapparava, C., and Valitutti, A. 2004. Wordnet-affect: an Affective extension of wordnet. In Proceedings of International Conference on Language Resources and Evaluation, 1083–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y T Sun</author>
<author>C L Chen</author>
<author>C C Liu</author>
<author>C L Liu</author>
<author>V W Soo</author>
</authors>
<title>Sentiment Classification of Short Chinese Sentences.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference on Computational Linguistics and Speech Processing (ROCLING’10),</booktitle>
<pages>184--198</pages>
<contexts>
<context position="6672" citStr="Sun et al. 2010" startWordPosition="979" endWordPosition="982">oblogs. Features Methods Pak and Paroubek statistic counting of Naive Bayes 2010 adjectives Chen et al. 2008 POS tags, emoticons SVM Lewin and Pribu- smileys, keywords Maximum Entropy, la 2009 SVM Riley 2009 n-grams, smileys, Naive Bayes, hashtags, replies, URLs, usernames, emoticons Prasad 2010 n-grams Naïve Bayes Go et al. 2009 usernames, sequential Naive Bayes, Maxpatterns of keywords, imum Entropy, POS tags, n-grams SVM Li et al. 2009 several dictionaries Keyword Matching about different kinds of keywords Barbosa and Feng retweets, hashtag, re- SVM 2010 plies, URLs, emoticons, upper cases Sun et al. 2010 keyword counting and Naive Bayes, SVM Chinese dictionaries Davidov et al. n-grams, word patterns, k-Nearest Neighbor 2010 punctuation information Bermingham and n-grams and POS tags Binary ClassificaSmeaton 2010 tion 3 Sentiment Analysis of Microblog Posts First, we develop a classification model as our basic sentiment recognition mechanism. Given a training corpus of posts and responses annotated with sentiment labels, we train an n-gram language model for each sentiment. Then, we use such model to calculate the probability that a post expresses the sentiment s associated with that model: ܲݎ</context>
<context position="16581" citStr="Sun et al. 2010" startWordPosition="2664" endWordPosition="2667"> 3. 6 Evaluations on Sentiment Detection We collect the posts and responses from every effective user, users with more than 10 messages, of Plurk from January 31st to May 23rd, 2009. In order to create the diversity for the music generation system, we decide to use six different sentiments, as shown in Table 2, rather than using only three sentiment types, positive, negative and neutral, as most of the systems in Table 1 have used. The sentiment of each sentence is labeled automatically using the emoticons. This is similar to what many people have proposed for evaluation (Davidov et al. 2010; Sun et al. 2010; Bifet and Frank 2010; Go et al. 2009; Pak and Paroubek 2010; Chen et al. 2010). We use data from January 31st to April 30th as training set, May 1st to 23rd as testing data. For the purpose of observing the result of using the three factors, we filter the users without friends, the posts without responses, and the posts without previous post in 24 hour in testing data. We also manually label the sentiments on the testing data (totally 1200 posts, 200 posts for each sentiment). We use three metrics to evaluate our model: accuracy, Root-Mean-Square Error for valence (denoted by RMSE(V)) and RM</context>
</contexts>
<marker>Sun, Chen, Liu, Liu, Soo, 2010</marker>
<rawString>Sun, Y. T.; Chen, C. L.; Liu, C. C.; Liu, C. L.; and Soo, V. W. 2010. Sentiment Classification of Short Chinese Sentences. In Proceedings of Conference on Computational Linguistics and Speech Processing (ROCLING’10), 184–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yang</author>
<author>K H Y Lin</author>
<author>H H Chen</author>
</authors>
<title>Emotion Classification Using Web Blog Corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of IEEE/WIC/ACM International Conference on Web Intelligence (WI’07),</booktitle>
<pages>275--278</pages>
<marker>Yang, Lin, Chen, 2007</marker>
<rawString>Yang, C.; Lin, K. H. Y.; and Chen, H. H. 2007. Emotion Classification Using Web Blog Corpora. In Proceedings of IEEE/WIC/ACM International Conference on Web Intelligence (WI’07), 275–278.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>