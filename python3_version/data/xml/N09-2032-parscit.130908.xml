<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.072742">
<title confidence="0.995814">
Domain Adaptation with Artificial Data for Semantic Parsing of Speech
</title>
<author confidence="0.929025">
Lonneke van der Plas James Henderson Paola Merlo
</author>
<affiliation confidence="0.983143">
Department of Linguistics Department of Computer Science Department of Linguistics
University of Geneva University of Geneva University of Geneva
Geneva, Switzerland Geneva, Switzerland Geneva, Switzerland
</affiliation>
<email confidence="0.99691">
{Lonneke.vanderPlas,James.Henderson,Paola.Merlo}@unige.ch
</email>
<sectionHeader confidence="0.994746" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995962">
We adapt a semantic role parser to the do-
main of goal-directed speech by creating an
artificial treebank from an existing text tree-
bank. We use a three-component model that
includes distributional models from both tar-
get and source domains. We show that we im-
prove the parser’s performance on utterances
collected from human-machine dialogues by
training on the artificially created data without
loss of performance on the text treebank.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942186440678">
As the quality of natural language parsing improves
and the sophistication of natural language under-
standing applications increases, there are several do-
mains where parsing, and especially semantic pars-
ing, could be useful. This is particularly true in
adaptive systems for spoken language understand-
ing, where complex utterances need to be translated
into shallow semantic representation, such as dia-
logue acts.
The domain on which we are working is goal-
directed system-driven dialogues, where a system
helps the user to fulfil a certain goal, e.g. booking a
hotel room. Typically, users respond with short an-
swers to questions posed by the system. For exam-
ple In the South is an answer to the question Where
would you like the hotel to be? Parsing helps iden-
tifying the components (In the South is a PP) and
semantic roles identify the PP as a locative, yield-
ing the following slot-value pair for the dialogue act:
area=South. A PP such as in time is not identified as
a locative, whereas keyword-spotting techniques as
those currently used in dialogue systems may pro-
duce area=South and area=time indifferently.
Statistical syntactic and semantic parsers need
treebanks. Current available data is lacking in one or
more respects: Syntactic/semantic treebanks are de-
veloped on text, while treebanks of speech corpora
are not semantically annotated (e.g. Switchboard).
Moreover, the available human-human speech tree-
banks do not exhibit the same properties as the
system-driven speech on which we are focusing, in
particular in their proportion of non-sentential utter-
ances (NSUs), utterances that are not full sentences.
In a corpus study of a subset of the human-human
dialogues in the BNC, Fern´andez (2006) found that
only 9% of the total utterances are NSUs, whereas
we find 44% in our system-driven data.
We illustrate a technique to adapt an exist-
ing semantic parser trained on merged Penn Tree-
bank/PropBank data to goal-directed system-driven
dialogue by artificial data generation. Our main con-
tribution lies in the framework used to generate ar-
tificial data for domain adaptation. We mimic the
distributions over parse structures in the target do-
main by combining the text treebank data and the
artificially created NSUs, using a three-component
model. The first component is a hand-crafted model
of NSUs. The second component describes the dis-
tribution over full sentences and types of NSUs as
found in a minimally annotated subset of the target
domain. The third component describes the distribu-
tion over the internal parse structure of the generated
data and is taken from the source domain.
Our approach differs from most approaches to do-
main adaptation, which require some training on
fully annotated target data (Nivre et al., 2007),
whereas we use minimally annotated target data
only to help determine the distributions in the ar-
tificially created data. It also differs from previ-
</bodyText>
<page confidence="0.981192">
125
</page>
<note confidence="0.358712">
Proceedings of NAACL HLT 2009: Short Papers, pages 125–128,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.979797888888889">
ous work in domain adaptation by Foster (2007),
where similar proportions of ungrammatical and
grammatical data are combined to train a parser
on ungrammatical written text, and by Weilhammer
et al. (2006), who use interpolation between two
separately trained models, one on an artificial cor-
pus of user utterances generated by a hand-coded
domain-specific grammar and one on available cor-
pora. Whereas much previous work on parsing
speech has focused on speech repairs, e.g. Charniak
and Johnson (2001), we focus on parsing NSUs.
2 The first component: a model of NSUs
To construct a model of NSUs we studied a subset of
the data under consideration: TownInfo. This small
corpus of transcribed spoken human-machine dia-
logues in the domain of hotel/restaurant/bar search
is gathered using the TownInfo tourist information
system (Lemon et al., 2006).
The NSUs we find in our data are mainly of the
type answers, according to the classification given
in Fern´andez (2006). More specifically, we find
short answers, plain and repeated affirmative an-
swers, plain and helpful rejections, but also greet-
ings.
Current linguistic theory provides several ap-
proaches to dealing with NSUs (Merchant, 2004;
Progovac et al., 2006; Fern´andez, 2006). Follow-
ing the linguistic analysis of NSUs as non-sentential
small clauses (Progovac et al., 2006) that do not have
tense or agreement functional nodes, we make the
assumption that they are phrasal projections. There-
fore, we reason, we can create an artificial data set
of NSUs by extracting phrasal projections from an
annotated treebank.
In the example given in the introduction, we saw
a PP fragment, but fragments can be NPs, APs, etc.
We define different types of NSUs based on the root
label of the phrasal projection and define rules that
allow us to extract NSUs (partial parse trees) from
the source corpus.1 Because the target corpus also
contains full sentences, we allow full sentences to
be taken without modification from the source tree-
bank.
1Not all of these rules are simple extractions of phrasal pro-
jections, as described in section 4.
</bodyText>
<sectionHeader confidence="0.920664" genericHeader="method">
3 The two distributional components
</sectionHeader>
<bodyText confidence="0.999992666666667">
The distributional model consists of two compo-
nents. By applying the extraction rules to the source
corpus we build a large collection of both full sen-
tences and NSUs. The distributions in this collec-
tion follow the distributions of trees in the source do-
main (first distributional component). We then sam-
ple from this collection to generate our artificial cor-
pus following distributions from the target domain
(second distributional component).
The probability of an artificial tree P (fi(cj)) gen-
erated with an extraction rule fi applied to a con-
stituent from the source corpus cj is defined as
</bodyText>
<equation confidence="0.998007">
P(fi(cj)) = P(fi)P(cj|fi) Pz Pt(fi)Ps(cj|fi)
</equation>
<bodyText confidence="0.999974263157895">
The first distributional component originates from
the source domain. It is responsible for the internal
structure of the NSUs and full sentences extracted.
Ps(cj|fi) is the probability of the constituent taken
from the source treebank (cj), given that the rule fi
is applicable to that constituent.
Sampling is done according to distributions of
NSUs and full sentences found in the target corpus
(Pt(fi)). As explained in section 2, there are several
types of NSUs found in the target domain. This sec-
ond component describes the distributions of types
of NSUs (or full sentences) found in the target do-
main. It determines, for example, the proportion of
NP NSUs that will be added to the artificial corpus.
To determine the target distribution we classified
171 (approximately 5%) randomly selected utter-
ances from the TownInfo data, that were used as a
development set.2 In Table 1 we can see that 15.2 %
of the trees in the artificial corpus will be NP NSUs.3
</bodyText>
<sectionHeader confidence="0.987504" genericHeader="method">
4 Data generation
</sectionHeader>
<bodyText confidence="0.999833666666667">
We constructed our artificial corpus from sections
2 to 21 of the Wall Street Journal (WSJ) section
of the Penn Treebank corpus (Marcus et al., 1993)
</bodyText>
<footnote confidence="0.99242575">
2We discarded very short utterances (yes, no, and greetings)
since they don’t need parsing. We also do not consider incom-
plete NSUs resulting from interruptions or recording problems.
3Because NSUs can be interpreted only in context, the same
NSU can correspond to several syntactic categories: South for
example, can be an noun, an adverb, or an adjective. In case of
ambiguity, we divided the score up for the several possible tags.
This accounts for the fractional counts.
</footnote>
<page confidence="0.970059">
126
</page>
<table confidence="0.999852">
Category # Occ. Perc. Category # Occ. Perc.
NP 19.0 15.2 RB 1.7 1.3
JJ 12.7 10.1 DT 1.0 0.8
PP 12.0 9.6 CD 1.0 0.8
NN 11.7 9.3 Total frag. 70.0 56.0
VP 11.0 8.8 Full sents 55.0 44.0
</table>
<tableCaption confidence="0.995916">
Table 1: Distribution of types of NSUs and full sentences
in the TownInfo development set.
</tableCaption>
<bodyText confidence="0.999675888888889">
merged with PropBank labels (Palmer et al., 2005).
We included all the sentences from this dataset in
our artificial corpus, giving us 39,832 full sentences.
In accordance with the target distribution we added
50,699 NSUs extracted from the same dataset. We
sampled NSUs according to the distribution given in
Table 1. After the extraction we added a root FRAG
node to the extracted NSUs4 and we capitalised the
first letter of each NSU to form an utterance.
There are two additional pre-processing steps.
First, for some types of NSUs maximal projections
are added. For example, in the subset from the tar-
get source we saw many occurrences of nouns with-
out determiners, such as Hotel or Bar. These types
of NSUs would be missed if we just extracted NPs
from the source data, since we assume that NSUs are
maximal projections. Therefore, we extracted single
nouns as well and we added the NP phrasal projec-
tions to these nouns in the constructed trees. Sec-
ond, not all extracted NSUs can keep their semantic
roles. Extracting part of the sentence often severs
the semantic role from the predicate of which it was
originally an argument. An exception to this are VP
NSUs and prepositional phrases that are modifiers,
such as locative PPs, which are not dependent on the
verb. Hence, we removed the semantic roles from
the generated NSUs except for VPs and modifiers.
</bodyText>
<sectionHeader confidence="0.999365" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999977666666667">
We trained three parsing models on both the original
non-augmented merged Penn Treebank/Propbank
corpus and the artificially generated augmented tree-
bank including NSUs. We ran a contrastive ex-
periment to examine the usefulness of the three-
component model by training two versions of the
</bodyText>
<footnote confidence="0.948791666666667">
4The node FRAG exists in the Penn Treebank. Our annota-
tion does not introduce new labels, but only changes their dis-
tribution.
</footnote>
<bodyText confidence="0.998296363636364">
augmented model: One with and one without the
target component.5
These models were tested on two test sets: a small
corpus of 150 transcribed utterances taken from the
TownInfo corpus, annotated with gold syntactic and
semantic annotation by two of the authors6: the
TownInfo test set. The second test set is used to
compare the performance of the parser on WSJ-style
sentences and consists of section 23 of the merged
Penn Treebank/Propbank corpus. We will refer to
this test set as the non-augmented test set.
</bodyText>
<subsectionHeader confidence="0.999282">
5.1 The statistical parser
</subsectionHeader>
<bodyText confidence="0.999914695652174">
The parsing model is the one proposed in Merlo
and Musillo (2008), which extends the syntactic
parser of Henderson (2003) and Titov and Hender-
son (2007) with annotations which identify seman-
tic role labels, and has competitive performance.
The parser uses a generative history-based proba-
bility model for a binarised left-corner derivation.
The probabilities of derivation decisions are mod-
elled using the neural network approximation (Hen-
derson, 2003) to a type of dynamic Bayesian Net-
work called an Incremental Sigmoid Belief Network
(ISBN) (Titov and Henderson, 2007).
The ISBN models the derivation history with a
vector of binary latent variables. These latent vari-
ables learn to represent features of the parse history
which are useful for making the current and subse-
quent derivation decisions. Induction of these fea-
tures is biased towards features which are local in
the parse tree, but can find features which are passed
arbitrarily far through the tree. This flexible mecha-
nism for feature induction allows the model to adapt
to the parsing of NSUs without requiring any design
changes or feature engineering.
</bodyText>
<subsectionHeader confidence="0.963352">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999915666666667">
In Table 2, we report labelled constituent recall, pre-
cision, and F-measure for the three trained parsers
(rows) on the two test sets (columns).7 These mea-
</bodyText>
<footnote confidence="0.998543875">
5The model without the target distribution has a uniform dis-
tribution over full sentences and NSUs and within NSUs a uni-
form distribution over the 8 types.
6This test set was constructed separately and is completely
different from the development set used to determine the distri-
butions in the target data.
7Statistical significance is determined using a stratified shuf-
fling method, using software available at http://www.cis.
</footnote>
<page confidence="0.979943">
127
</page>
<table confidence="0.999781">
Training TownInfo Testing
PTB nonaug
Rec Prec F Rec Prec F
PTB nonaug 69.4 76.7 72.9 81.4 82.1 81.7
PTB aug(+t) 81.4 77.8 79.5 81.3 82.0 81.7
PTB aug(−t) 62.6 64.3 63.4 81.2 81.9 81.6
</table>
<tableCaption confidence="0.966975333333333">
Table 2: Recall, precision, and F-measure for the two test
sets, trained on non-augmented data and data augmented
with and without the target distribution component.
</tableCaption>
<bodyText confidence="0.999088083333333">
sures include both syntactic labels and semantic role
labels.
The results in the first two lines of the columns
headed TownInfo indicate the performance on the
real data to which we are trying to adapt our parser:
spoken data from human-machine dialogues. The
parser does much better when trained on the aug-
mented data. The differences between training on
newspaper text and newspaper texts augmented with
artificially created data are statistically significant
(p &lt; 0.001) and particularly large for recall: almost
12%.
The columns headed PTB nonaug show that the
performance on parsing WSJ texts is not hurt by
training on data augmented with artificially cre-
ated NSUs (first vs. second line). The difference
in performance compared to training on the non-
augmented data is not statistically significant.
The last two rows of the TownInfo data show the
results of our contrastive experiment. It is clear
that the three-component model and in particular our
careful characterisation of the target distribution is
indispensable. The F-measure drops from 79.5% to
63.4% when we disregard the target distribution.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.993886933333333">
We have shown how a three-component model that
consists of a model of the phenomenon being stud-
ied and two distributional components, one from the
source data and one from the target data, allows
one to create data artificially for training a seman-
tic parser. Specifically, analysis and minimal anno-
tation of only a small subset of utterances from the
target domain of spoken dialogue systems suffices
to determine a model of NSUs as well as the nec-
essary target distribution. Following this framework
upenn.edu/˜dbikel/software.html.
we were able to improve the performance of a statis-
tical parser on goal-directed spoken data extracted
from human-machine dialogues without degrading
the performance on full sentences.
</bodyText>
<sectionHeader confidence="0.990993" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9991895">
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLASSIC
project: www.classic-project.org).
</bodyText>
<sectionHeader confidence="0.998894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999945025641026">
E. Charniak and M. Johnson. 2001. Edit detection and
parsing for transcribed speech. In Procs. NAACL.
R. Fern´andez. 2006. Non-sentential utterances in dia-
logue: classification resolution and use. Ph.D. thesis,
University of London.
J. Foster. 2007. Treebanks gone bad: Parser evaluation
and retraining using a treebank of ungrammatical sen-
tences. International Journal of Document Analysis
and Recognition, 10:1–16.
J. Henderson. 2003. Inducing history representations for
broad-coverage statistical parsing. In Procs. NAACL-
HLT.
O. Lemon, K. Georgila, J. Henderson, and M. Stuttle.
2006. An ISU dialogue system exhibiting reinforce-
ment learning of dialogue policies: generic slot-filling
in the TALK in-car system. In Procs. EACL.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
the Penn Treebank. Comp. Ling., 19:313–330.
J. Merchant. 2004. Fragments and ellipsis. Linguistics
and Philosophy, 27:661–738.
P. Merlo and G. Musillo. 2008. Semantic parsing
for high-precision semantic role labelling. In Procs.
CONLL.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Procs. EMNLP-
CoNLL.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Comp. Ling., 31:71–105.
L. Progovac, K. Paesani, E. Casielles, and E. Barton.
2006. The Syntax of Nonsententials:Multidisciplinary
Perspectives. John Benjamins.
I Titov and J Henderson. 2007. Constituent parsing with
Incremental Sigmoid Belief Networks. In Procs. ACL.
K. Weilhammer, M. Stuttle, and S. Young. 2006. Boot-
strapping language models for dialogue systems. In
Procs. Conf. on Spoken Language Processing.
</reference>
<page confidence="0.996748">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987768">
<title confidence="0.999278">Domain Adaptation with Artificial Data for Semantic Parsing of Speech</title>
<author confidence="0.999364">Lonneke van_der_Plas James Henderson Paola Merlo</author>
<affiliation confidence="0.9999645">Department of Linguistics Department of Computer Science Department of Linguistics University of Geneva University of Geneva University of Geneva</affiliation>
<address confidence="0.995195">Geneva, Switzerland Geneva, Switzerland Geneva, Switzerland</address>
<abstract confidence="0.999425636363636">We adapt a semantic role parser to the domain of goal-directed speech by creating an artificial treebank from an existing text treebank. We use a three-component model that includes distributional models from both target and source domains. We show that we improve the parser’s performance on utterances collected from human-machine dialogues by training on the artificially created data without loss of performance on the text treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Procs. NAACL.</booktitle>
<contexts>
<context position="4390" citStr="Charniak and Johnson (2001)" startWordPosition="674" endWordPosition="677">NAACL HLT 2009: Short Papers, pages 125–128, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ous work in domain adaptation by Foster (2007), where similar proportions of ungrammatical and grammatical data are combined to train a parser on ungrammatical written text, and by Weilhammer et al. (2006), who use interpolation between two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings.</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>E. Charniak and M. Johnson. 2001. Edit detection and parsing for transcribed speech. In Procs. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fern´andez</author>
</authors>
<title>Non-sentential utterances in dialogue: classification resolution and use.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of London.</institution>
<marker>Fern´andez, 2006</marker>
<rawString>R. Fern´andez. 2006. Non-sentential utterances in dialogue: classification resolution and use. Ph.D. thesis, University of London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Foster</author>
</authors>
<title>Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences.</title>
<date>2007</date>
<journal>International Journal of Document Analysis and Recognition,</journal>
<pages>10--1</pages>
<contexts>
<context position="3933" citStr="Foster (2007)" startWordPosition="606" endWordPosition="607"> component describes the distribution over the internal parse structure of the generated data and is taken from the source domain. Our approach differs from most approaches to domain adaptation, which require some training on fully annotated target data (Nivre et al., 2007), whereas we use minimally annotated target data only to help determine the distributions in the artificially created data. It also differs from previ125 Proceedings of NAACL HLT 2009: Short Papers, pages 125–128, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ous work in domain adaptation by Foster (2007), where similar proportions of ungrammatical and grammatical data are combined to train a parser on ungrammatical written text, and by Weilhammer et al. (2006), who use interpolation between two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under considera</context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>J. Foster. 2007. Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences. International Journal of Document Analysis and Recognition, 10:1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
</authors>
<title>Inducing history representations for broad-coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Procs. NAACLHLT.</booktitle>
<contexts>
<context position="10997" citStr="Henderson (2003)" startWordPosition="1776" endWordPosition="1777">e target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6: the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The statistical parser The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent variables learn to represent features of the parse hist</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>J. Henderson. 2003. Inducing history representations for broad-coverage statistical parsing. In Procs. NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>K Georgila</author>
<author>J Henderson</author>
<author>M Stuttle</author>
</authors>
<title>An ISU dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the TALK in-car system.</title>
<date>2006</date>
<booktitle>In Procs. EACL.</booktitle>
<contexts>
<context position="4736" citStr="Lemon et al., 2006" startWordPosition="730" endWordPosition="733">etween two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings. Current linguistic theory provides several approaches to dealing with NSUs (Merchant, 2004; Progovac et al., 2006; Fern´andez, 2006). Following the linguistic analysis of NSUs as non-sentential small clauses (Progovac et al., 2006) that do not have tense or agreement functional nodes, we make the assumption that they are phrasal projections. T</context>
</contexts>
<marker>Lemon, Georgila, Henderson, Stuttle, 2006</marker>
<rawString>O. Lemon, K. Georgila, J. Henderson, and M. Stuttle. 2006. An ISU dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the TALK in-car system. In Procs. EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<location>Comp. Ling.,</location>
<contexts>
<context position="7788" citStr="Marcus et al., 1993" startWordPosition="1231" endWordPosition="1234">onent describes the distributions of types of NSUs (or full sentences) found in the target domain. It determines, for example, the proportion of NP NSUs that will be added to the artificial corpus. To determine the target distribution we classified 171 (approximately 5%) randomly selected utterances from the TownInfo data, that were used as a development set.2 In Table 1 we can see that 15.2 % of the trees in the artificial corpus will be NP NSUs.3 4 Data generation We constructed our artificial corpus from sections 2 to 21 of the Wall Street Journal (WSJ) section of the Penn Treebank corpus (Marcus et al., 1993) 2We discarded very short utterances (yes, no, and greetings) since they don’t need parsing. We also do not consider incomplete NSUs resulting from interruptions or recording problems. 3Because NSUs can be interpreted only in context, the same NSU can correspond to several syntactic categories: South for example, can be an noun, an adverb, or an adjective. In case of ambiguity, we divided the score up for the several possible tags. This accounts for the fractional counts. 126 Category # Occ. Perc. Category # Occ. Perc. NP 19.0 15.2 RB 1.7 1.3 JJ 12.7 10.1 DT 1.0 0.8 PP 12.0 9.6 CD 1.0 0.8 NN 1</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Comp. Ling., 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Merchant</author>
</authors>
<title>Fragments and ellipsis.</title>
<date>2004</date>
<journal>Linguistics and Philosophy,</journal>
<pages>27--661</pages>
<contexts>
<context position="5081" citStr="Merchant, 2004" startWordPosition="786" endWordPosition="787">ct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings. Current linguistic theory provides several approaches to dealing with NSUs (Merchant, 2004; Progovac et al., 2006; Fern´andez, 2006). Following the linguistic analysis of NSUs as non-sentential small clauses (Progovac et al., 2006) that do not have tense or agreement functional nodes, we make the assumption that they are phrasal projections. Therefore, we reason, we can create an artificial data set of NSUs by extracting phrasal projections from an annotated treebank. In the example given in the introduction, we saw a PP fragment, but fragments can be NPs, APs, etc. We define different types of NSUs based on the root label of the phrasal projection and define rules that allow us to</context>
</contexts>
<marker>Merchant, 2004</marker>
<rawString>J. Merchant. 2004. Fragments and ellipsis. Linguistics and Philosophy, 27:661–738.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>G Musillo</author>
</authors>
<title>Semantic parsing for high-precision semantic role labelling.</title>
<date>2008</date>
<booktitle>In Procs. CONLL.</booktitle>
<contexts>
<context position="10941" citStr="Merlo and Musillo (2008)" startWordPosition="1766" endWordPosition="1769">their distribution. augmented model: One with and one without the target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6: the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The statistical parser The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent</context>
</contexts>
<marker>Merlo, Musillo, 2008</marker>
<rawString>P. Merlo and G. Musillo. 2008. Semantic parsing for high-precision semantic role labelling. In Procs. CONLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Procs. EMNLPCoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Procs. EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Comp. Ling.,</journal>
<pages>31--71</pages>
<contexts>
<context position="8591" citStr="Palmer et al., 2005" startWordPosition="1372" endWordPosition="1375">. 3Because NSUs can be interpreted only in context, the same NSU can correspond to several syntactic categories: South for example, can be an noun, an adverb, or an adjective. In case of ambiguity, we divided the score up for the several possible tags. This accounts for the fractional counts. 126 Category # Occ. Perc. Category # Occ. Perc. NP 19.0 15.2 RB 1.7 1.3 JJ 12.7 10.1 DT 1.0 0.8 PP 12.0 9.6 CD 1.0 0.8 NN 11.7 9.3 Total frag. 70.0 56.0 VP 11.0 8.8 Full sents 55.0 44.0 Table 1: Distribution of types of NSUs and full sentences in the TownInfo development set. merged with PropBank labels (Palmer et al., 2005). We included all the sentences from this dataset in our artificial corpus, giving us 39,832 full sentences. In accordance with the target distribution we added 50,699 NSUs extracted from the same dataset. We sampled NSUs according to the distribution given in Table 1. After the extraction we added a root FRAG node to the extracted NSUs4 and we capitalised the first letter of each NSU to form an utterance. There are two additional pre-processing steps. First, for some types of NSUs maximal projections are added. For example, in the subset from the target source we saw many occurrences of nouns</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Comp. Ling., 31:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Progovac</author>
<author>K Paesani</author>
<author>E Casielles</author>
<author>E Barton</author>
</authors>
<title>The Syntax of Nonsententials:Multidisciplinary Perspectives.</title>
<date>2006</date>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="5104" citStr="Progovac et al., 2006" startWordPosition="788" endWordPosition="791">Us we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings. Current linguistic theory provides several approaches to dealing with NSUs (Merchant, 2004; Progovac et al., 2006; Fern´andez, 2006). Following the linguistic analysis of NSUs as non-sentential small clauses (Progovac et al., 2006) that do not have tense or agreement functional nodes, we make the assumption that they are phrasal projections. Therefore, we reason, we can create an artificial data set of NSUs by extracting phrasal projections from an annotated treebank. In the example given in the introduction, we saw a PP fragment, but fragments can be NPs, APs, etc. We define different types of NSUs based on the root label of the phrasal projection and define rules that allow us to extract NSUs (partial </context>
</contexts>
<marker>Progovac, Paesani, Casielles, Barton, 2006</marker>
<rawString>L. Progovac, K. Paesani, E. Casielles, and E. Barton. 2006. The Syntax of Nonsententials:Multidisciplinary Perspectives. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>Constituent parsing with Incremental Sigmoid Belief Networks.</title>
<date>2007</date>
<booktitle>In Procs. ACL.</booktitle>
<contexts>
<context position="11028" citStr="Titov and Henderson (2007)" startWordPosition="1779" endWordPosition="1783">These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6: the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The statistical parser The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent variables learn to represent features of the parse history which are useful for making</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>I Titov and J Henderson. 2007. Constituent parsing with Incremental Sigmoid Belief Networks. In Procs. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Weilhammer</author>
<author>M Stuttle</author>
<author>S Young</author>
</authors>
<title>Bootstrapping language models for dialogue systems.</title>
<date>2006</date>
<booktitle>In Procs. Conf. on Spoken Language Processing.</booktitle>
<contexts>
<context position="4092" citStr="Weilhammer et al. (2006)" startWordPosition="628" endWordPosition="631">ers from most approaches to domain adaptation, which require some training on fully annotated target data (Nivre et al., 2007), whereas we use minimally annotated target data only to help determine the distributions in the artificially created data. It also differs from previ125 Proceedings of NAACL HLT 2009: Short Papers, pages 125–128, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ous work in domain adaptation by Foster (2007), where similar proportions of ungrammatical and grammatical data are combined to train a parser on ungrammatical written text, and by Weilhammer et al. (2006), who use interpolation between two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tou</context>
</contexts>
<marker>Weilhammer, Stuttle, Young, 2006</marker>
<rawString>K. Weilhammer, M. Stuttle, and S. Young. 2006. Bootstrapping language models for dialogue systems. In Procs. Conf. on Spoken Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>